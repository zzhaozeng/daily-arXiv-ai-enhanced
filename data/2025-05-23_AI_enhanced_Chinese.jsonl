{"id": "2505.16397", "pdf": "https://arxiv.org/pdf/2505.16397", "abs": "https://arxiv.org/abs/2505.16397", "authors": ["Koki Nagakura", "Tatsuki Fushimi", "Ayaka Tsutsui", "Yoichi Ochiai"], "title": "Dynamic Caustics by Ultrasonically Modulated Liquid Surface", "categories": ["cs.GR", "cs.HC"], "comment": null, "summary": "This paper presents a method for generating dynamic caustic patterns by\nutilising dual-optimised holographic fields with Phased Array Transducer (PAT).\nBuilding on previous research in static caustic optimisation and ultrasonic\nmanipulation, this approach employs computational techniques to dynamically\nshape fluid surfaces, thereby creating controllable and real-time caustic\nimages. The system employs a Digital Twin framework, which enables iterative\nfeedback and refinement, thereby improving the accuracy and quality of the\ncaustic patterns produced. This paper extends the foundational work in caustic\ngeneration by integrating liquid surfaces as refractive media. This concept has\npreviously been explored in simulations but not fully realised in practical\napplications. The utilisation of ultrasound to directly manipulate these\nsurfaces enables the generation of dynamic caustics with a high degree of\nflexibility. The Digital Twin approach further enhances this process by\nallowing for precise adjustments and optimisation based on real-time feedback.\nExperimental results demonstrate the technique's capacity to generate\ncontinuous animations and complex caustic patterns at high frequencies.\nAlthough there are limitations in contrast and resolution compared to\nsolid-surface methods, this approach offers advantages in terms of real-time\nadaptability and scalability. This technique has the potential to be applied in\na number of areas, including interactive displays, artistic installations and\neducational tools. This research builds upon the work of previous researchers\nin the fields of caustics optimisation, ultrasonic manipulation, and\ncomputational displays. Future research will concentrate on enhancing the\nresolution and intricacy of the generated patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u53cc\u4f18\u5316\u5168\u606f\u573a\u548c\u76f8\u63a7\u9635\u6362\u80fd\u5668\uff08PAT\uff09\u751f\u6210\u52a8\u6001\u7126\u6563\u56fe\u6848\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u5b9e\u73b0\u5b9e\u65f6\u53cd\u9988\u548c\u4f18\u5316\u3002", "motivation": "\u6269\u5c55\u9759\u6001\u7126\u6563\u4f18\u5316\u548c\u8d85\u58f0\u6ce2\u64cd\u63a7\u7684\u7814\u7a76\uff0c\u63a2\u7d22\u6db2\u4f53\u8868\u9762\u4f5c\u4e3a\u6298\u5c04\u4ecb\u8d28\u7684\u52a8\u6001\u7126\u6563\u751f\u6210\uff0c\u586b\u8865\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u6280\u672f\u52a8\u6001\u5851\u9020\u6d41\u4f53\u8868\u9762\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u8fdb\u884c\u8fed\u4ee3\u53cd\u9988\u548c\u4f18\u5316\uff0c\u5229\u7528\u8d85\u58f0\u6ce2\u76f4\u63a5\u64cd\u63a7\u6db2\u4f53\u8868\u9762\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u9891\u8fde\u7eed\u52a8\u753b\u548c\u590d\u6742\u7126\u6563\u56fe\u6848\uff0c\u867d\u5bf9\u6bd4\u5ea6\u548c\u5206\u8fa8\u7387\u4e0d\u53ca\u56fa\u4f53\u8868\u9762\u65b9\u6cd5\uff0c\u4f46\u5177\u6709\u5b9e\u65f6\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ea4\u4e92\u663e\u793a\u3001\u827a\u672f\u88c5\u7f6e\u548c\u6559\u80b2\u5de5\u5177\u7b49\u9886\u57df\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5c06\u805a\u7126\u4e8e\u63d0\u5347\u56fe\u6848\u5206\u8fa8\u7387\u548c\u590d\u6742\u5ea6\u3002"}}
{"id": "2505.16951", "pdf": "https://arxiv.org/pdf/2505.16951", "abs": "https://arxiv.org/abs/2505.16951", "authors": ["Santiago Berrezueta-Guzman", "Andrei Koshelev", "Stefan Wagner"], "title": "From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development", "categories": ["cs.GR"], "comment": "Paper accepted in the IEEE Gaming, Entertainment and Media conference", "summary": "Photogrammetry is transforming digital content creation by enabling the rapid\nconversion of real-world objects into highly detailed 3D models. This paper\nevaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in\ngame development of Virtual Reality (VR). We assess its efficiency,\nreconstruction accuracy, and integration with Unreal Engine, comparing its\nadvantages and limitations against traditional modeling workflows.\nAdditionally, we examined user preferences between designed 3D assets and\nphotogrammetry-generated models. The results revealed that while photogrammetry\nenhances realism and interactivity, users slightly preferred manually designed\nmodels for small, manipulable elements because of the level of detail. However,\nfrom a developer perspective, RealityCapture significantly reduces development\ntime while maintaining geometric precision and photorealistic textures. Despite\nits reliance on high-performance hardware, its automation, scalability, and\nseamless integration with real-time rendering engines make it a valuable tool\nfor game developers and VR creators. Future improvements in AI-driven\noptimization and cloud-based processing could enhance accessibility, broadening\nits applications in gaming, cultural heritage preservation, and simulation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86RealityCapture\u5728VR\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u6548\u7387\u3001\u7cbe\u5ea6\u548c\u96c6\u6210\u4f18\u52bf\uff0c\u53d1\u73b0\u7528\u6237\u504f\u597d\u624b\u52a8\u8bbe\u8ba1\u6a21\u578b\uff0c\u4f46\u5f00\u53d1\u8005\u8ba4\u4e3a\u5176\u8282\u7701\u65f6\u95f4\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76RealityCapture\u5728VR\u6e38\u620f\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u5176\u4e0e\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u7684\u4f18\u52a3\u3002", "method": "\u8bc4\u4f30RealityCapture\u7684\u6548\u7387\u3001\u91cd\u5efa\u7cbe\u5ea6\u53ca\u4e0eUnreal Engine\u7684\u96c6\u6210\uff0c\u5e76\u6bd4\u8f83\u7528\u6237\u5bf9\u8bbe\u8ba1\u6a21\u578b\u4e0e\u6444\u5f71\u6d4b\u91cf\u751f\u6210\u6a21\u578b\u7684\u504f\u597d\u3002", "result": "\u6444\u5f71\u6d4b\u91cf\u63d0\u5347\u771f\u5b9e\u611f\u4f46\u7528\u6237\u504f\u597d\u624b\u52a8\u8bbe\u8ba1\u5c0f\u7269\u4ef6\uff1bRealityCapture\u663e\u8457\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u4e14\u4fdd\u6301\u9ad8\u8d28\u91cf\u3002", "conclusion": "RealityCapture\u662fVR\u5f00\u53d1\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u672a\u6765AI\u4f18\u5316\u548c\u4e91\u5904\u7406\u53ef\u6269\u5c55\u5176\u5e94\u7528\u3002"}}
{"id": "2505.15916", "pdf": "https://arxiv.org/pdf/2505.15916", "abs": "https://arxiv.org/abs/2505.15916", "authors": ["Juvenal Domingos J\u00fanior", "Augusto Faria", "E. Seiti de Oliveira", "Erick de Brito", "Matheus Teotonio", "Andre Assump\u00e7\u00e3o", "Diedre Carmo", "Roberto Lotufo", "Jayr Pereira"], "title": "BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents BR-TaxQA-R, a novel dataset designed to support question\nanswering with references in the context of Brazilian personal income tax law.\nThe dataset contains 715 questions from the 2024 official Q\\&A document\npublished by Brazil's Internal Revenue Service, enriched with statutory norms\nand administrative rulings from the Conselho Administrativo de Recursos Fiscais\n(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using\nOpenAI embeddings for searching and GPT-4o-mini for answer generation. We\ncompare different text segmentation strategies and benchmark our system against\ncommercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.\nResults show that our custom RAG pipeline outperforms commercial systems in\nResponse Relevancy, indicating stronger alignment with user queries, while\ncommercial models achieve higher scores in Factual Correctness and fluency.\nThese findings highlight a trade-off between legally grounded generation and\nlinguistic fluency. Crucially, we argue that human expert evaluation remains\nessential to ensure the legal validity of AI-generated answers in high-stakes\ndomains such as taxation. BR-TaxQA-R is publicly available at\nhttps://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.", "AI": {"tldr": "BR-TaxQA-R\u662f\u4e00\u4e2a\u652f\u6301\u5df4\u897f\u4e2a\u4eba\u6240\u5f97\u7a0e\u6cd5\u95ee\u7b54\u7684\u65b0\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u6027\u80fd\u4f18\u4e8e\u5546\u4e1a\u5de5\u5177\uff0c\u4f46\u9700\u4e13\u5bb6\u9a8c\u8bc1\u6cd5\u5f8b\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u5df4\u897f\u4e2a\u4eba\u6240\u5f97\u7a0e\u6cd5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u95ee\u7b54\u652f\u6301\uff0c\u7ed3\u5408\u6cd5\u5f8b\u6587\u672c\u548cAI\u6280\u672f\u3002", "method": "\u4f7f\u7528RAG\u7ba1\u9053\uff08OpenAI\u5d4c\u5165\u641c\u7d22\u548cGPT-4o-mini\u751f\u6210\uff09\uff0c\u6bd4\u8f83\u4e0d\u540c\u6587\u672c\u5206\u5272\u7b56\u7565\uff0c\u5e76\u57fa\u4e8eRAGAS\u6307\u6807\u8bc4\u4f30\u3002", "result": "\u81ea\u5b9a\u4e49RAG\u5728\u54cd\u5e94\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u5546\u4e1a\u7cfb\u7edf\uff0c\u5546\u4e1a\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u6cd5\u5f8b\u9886\u57dfAI\u751f\u6210\u7b54\u6848\u9700\u4e13\u5bb6\u9a8c\u8bc1\uff0cBR-TaxQA-R\u6570\u636e\u96c6\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2505.15825", "pdf": "https://arxiv.org/pdf/2505.15825", "abs": "https://arxiv.org/abs/2505.15825", "authors": ["Ammar Chouchane", "Mohcene Bessaoudi", "Hamza Kheddar", "Abdelmalik Ouamane", "Tiago Vieira", "Mahmoud Hassaballah"], "title": "Multilinear subspace learning for person re-identification based fusion of high order tensor features", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Video surveillance image analysis and processing is a challenging field in\ncomputer vision, with one of its most difficult tasks being Person\nRe-Identification (PRe-ID). PRe-ID aims to identify and track target\nindividuals who have already been detected in a network of cameras, using a\nrobust description of their pedestrian images. The success of recent research\nin person PRe-ID is largely due to effective feature extraction and\nrepresentation, as well as the powerful learning of these features to reliably\ndiscriminate between pedestrian images. To this end, two powerful features,\nConvolutional Neural Networks (CNN) and Local Maximal Occurrence (LOMO), are\nmodeled on multidimensional data using the proposed method, High-Dimensional\nFeature Fusion (HDFF). Specifically, a new tensor fusion scheme is introduced\nto leverage and combine these two types of features in a single tensor, even\nthough their dimensions are not identical. To enhance the system's accuracy, we\nemploy Tensor Cross-View Quadratic Analysis (TXQDA) for multilinear subspace\nlearning, followed by cosine similarity for matching. TXQDA efficiently\nfacilitates learning while reducing the high dimensionality inherent in\nhigh-order tensor data. The effectiveness of our approach is verified through\nexperiments on three widely-used PRe-ID datasets: VIPeR, GRID, and PRID450S.\nExtensive experiments demonstrate that our approach outperforms recent\nstate-of-the-art methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7ef4\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff08HDFF\uff09\uff0c\u7ed3\u5408CNN\u548cLOMO\u7279\u5f81\uff0c\u901a\u8fc7\u5f20\u91cf\u878d\u5408\u548cTXQDA\u5b66\u4e60\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\uff08PRe-ID\uff09\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u884c\u4eba\u91cd\u8bc6\u522b\u662f\u89c6\u9891\u76d1\u63a7\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u548c\u8868\u793a\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u91c7\u7528HDFF\u65b9\u6cd5\u878d\u5408CNN\u548cLOMO\u7279\u5f81\uff0c\u5f15\u5165\u5f20\u91cf\u878d\u5408\u65b9\u6848\uff0c\u7ed3\u5408TXQDA\u8fdb\u884c\u591a\u7ebf\u6027\u5b50\u7a7a\u95f4\u5b66\u4e60\uff0c\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\u3002", "result": "\u5728VIPeR\u3001GRID\u548cPRID450S\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HDFF\u548cTXQDA\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u884c\u4eba\u91cd\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.15918", "pdf": "https://arxiv.org/pdf/2505.15918", "abs": "https://arxiv.org/abs/2505.15918", "authors": ["Aliakbar Nafar", "Kristen Brent Venable", "Zijun Cui", "Parisa Kordjamshidi"], "title": "Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated potential as factual knowledge\nbases; however, their capability to generate probabilistic knowledge about\nreal-world events remains understudied. This paper investigates using\nprobabilistic knowledge inherent in LLMs to derive probability estimates for\nstatements concerning events and their interrelationships captured via a\nBayesian Network (BN). Using LLMs in this context allows for the\nparameterization of BNs, enabling probabilistic modeling within specific\ndomains. Experiments on eighty publicly available Bayesian Networks, from\nhealthcare to finance, demonstrate that querying LLMs about the conditional\nprobabilities of events provides meaningful results when compared to baselines,\nincluding random and uniform distributions, as well as approaches based on\nnext-token generation probabilities. We explore how these LLM-derived\ndistributions can serve as expert priors to refine distributions extracted from\nminimal data, significantly reducing systematic biases. Overall, this work\nintroduces a promising strategy for automatically constructing Bayesian\nNetworks by combining probabilistic knowledge extracted from LLMs with small\namounts of real-world data. Additionally, we evaluate several prompting\nstrategies for eliciting probabilistic knowledge from LLMs and establish the\nfirst comprehensive baseline for assessing LLM performance in extracting\nprobabilistic knowledge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u6982\u7387\u77e5\u8bc6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\uff08BN\uff09\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u751f\u6210\u6982\u7387\u4f30\u8ba1\uff0c\u5e76\u63a2\u7d22\u4e86LLMs\u5728\u53c2\u6570\u5316BN\u548c\u51cf\u5c11\u7cfb\u7edf\u504f\u5dee\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "LLMs\u4f5c\u4e3a\u4e8b\u5b9e\u77e5\u8bc6\u5e93\u7684\u6f5c\u529b\u5df2\u88ab\u8bc1\u5b9e\uff0c\u4f46\u5176\u751f\u6210\u5173\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u7684\u6982\u7387\u77e5\u8bc6\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u67e5\u8be2LLMs\u83b7\u53d6\u4e8b\u4ef6\u7684\u6761\u4ef6\u6982\u7387\uff0c\u5c06\u5176\u7528\u4e8e\u53c2\u6570\u5316\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u5e76\u4e0e\u968f\u673a\u3001\u5747\u5300\u5206\u5e03\u7b49\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u63d0\u4f9b\u7684\u6982\u7387\u4f30\u8ba1\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u533b\u7597\u548c\u91d1\u878d\uff09\u4e2d\u5177\u6709\u610f\u4e49\uff0c\u5e76\u80fd\u4f5c\u4e3a\u4e13\u5bb6\u5148\u9a8c\u4f18\u5316\u6570\u636e\u7a00\u7f3a\u65f6\u7684\u5206\u5e03\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLMs\u6982\u7387\u77e5\u8bc6\u4e0e\u5c11\u91cf\u771f\u5b9e\u6570\u636e\u81ea\u52a8\u6784\u5efa\u8d1d\u53f6\u65af\u7f51\u7edc\u7684\u7b56\u7565\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc4\u4f30LLMs\u63d0\u53d6\u6982\u7387\u77e5\u8bc6\u6027\u80fd\u7684\u57fa\u7ebf\u3002"}}
{"id": "2505.15863", "pdf": "https://arxiv.org/pdf/2505.15863", "abs": "https://arxiv.org/abs/2505.15863", "authors": ["Katharina Winter", "Abhishek Vivekanandan", "Rupert Polley", "Yinzhe Shen", "Christian Schlauch", "Mohamed-Khalil Bouzidi", "Bojan Derajic", "Natalie Grabowsky", "Annajoyce Mariani", "Dennis Rochau", "Giovanni Lucente", "Harsh Yadav", "Firas Mualla", "Adam Molin", "Sebastian Bernhard", "Christian Wirth", "\u00d6mer \u015eahin Ta\u015f", "Nadja Klein", "Fabian B. Flohr", "Hanno Gottschalk"], "title": "Generative AI for Autonomous Driving: A Review", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Generative AI (GenAI) is rapidly advancing the field of Autonomous Driving\n(AD), extending beyond traditional applications in text, image, and video\ngeneration. We explore how generative models can enhance automotive tasks, such\nas static map creation, dynamic scenario generation, trajectory forecasting,\nand vehicle motion planning. By examining multiple generative approaches\nranging from Variational Autoencoder (VAEs) over Generative Adversarial\nNetworks (GANs) and Invertible Neural Networks (INNs) to Generative\nTransformers (GTs) and Diffusion Models (DMs), we highlight and compare their\ncapabilities and limitations for AD-specific applications. Additionally, we\ndiscuss hybrid methods integrating conventional techniques with generative\napproaches, and emphasize their improved adaptability and robustness. We also\nidentify relevant datasets and outline open research questions to guide future\ndevelopments in GenAI. Finally, we discuss three core challenges: safety,\ninterpretability, and realtime capabilities, and present recommendations for\nimage generation, dynamic scenario generation, and planning.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u9886\u57df\u7684\u5e94\u7528\u6269\u5c55\u4e86\u4f20\u7edf\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u7684\u8303\u56f4\uff0c\u6d89\u53ca\u9759\u6001\u5730\u56fe\u521b\u5efa\u3001\u52a8\u6001\u573a\u666f\u751f\u6210\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u8f66\u8f86\u8fd0\u52a8\u89c4\u5212\u7b49\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22\u751f\u6210\u5f0f\u6a21\u578b\u5982\u4f55\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u3002", "method": "\u5206\u6790\u591a\u79cd\u751f\u6210\u65b9\u6cd5\uff08\u5982VAEs\u3001GANs\u3001INNs\u3001GTs\u548cDMs\uff09\u53ca\u5176\u6df7\u5408\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u76f8\u5173\u6570\u636e\u96c6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u751f\u6210\u5f0fAI\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5b9e\u65f6\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2505.15922", "pdf": "https://arxiv.org/pdf/2505.15922", "abs": "https://arxiv.org/abs/2505.15922", "authors": ["Dong Won Lee", "Hae Won Park", "Cynthia Breazeal", "Louis-Philippe Morency"], "title": "Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 3 tables", "summary": "We propose a large language model based reward decomposition framework for\naligning dialogue agents using only a single session-level feedback signal. We\nleverage the reasoning capabilities of a frozen, pretrained large language\nmodel (LLM) to infer fine-grained local implicit rewards by decomposing global,\nsession-level feedback. Our first text-only variant prompts the LLM to perform\nreward decomposition using only the dialogue transcript. The second multimodal\nvariant incorporates additional behavioral cues, such as pitch, gaze, and\nfacial affect, expressed as natural language descriptions. These inferred\nturn-level rewards are distilled into a lightweight reward model, which we\nutilize for RL-based fine-tuning for dialogue generation. We evaluate both\ntext-only and multimodal variants against state-of-the-art reward decomposition\nmethods and demonstrate notable improvements in human evaluations of\nconversation quality, suggesting that LLMs are strong reward decomposers that\nobviate the need for manual reward shaping and granular human feedback.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u5206\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u4f1a\u8bdd\u7ea7\u53cd\u9988\u4fe1\u53f7\u5bf9\u9f50\u5bf9\u8bdd\u4ee3\u7406\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7ec6\u7c92\u5ea6\u5c40\u90e8\u5956\u52b1\uff0c\u5e76\u84b8\u998f\u4e3a\u8f7b\u91cf\u7ea7\u5956\u52b1\u6a21\u578b\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u5956\u52b1\u548c\u7ec6\u7c92\u5ea6\u4eba\u7c7b\u53cd\u9988\u7684\u95ee\u9898\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5b9e\u73b0\u81ea\u52a8\u5956\u52b1\u5206\u89e3\u3002", "method": "\u63d0\u51fa\u6587\u672c\u548c\u591a\u6a21\u6001\u4e24\u79cd\u53d8\u4f53\uff0c\u5206\u522b\u57fa\u4e8e\u5bf9\u8bdd\u6587\u672c\u548c\u884c\u4e3a\u7ebf\u7d22\uff08\u5982\u97f3\u8c03\u3001\u6ce8\u89c6\u548c\u9762\u90e8\u8868\u60c5\uff09\u5206\u89e3\u5956\u52b1\uff0c\u5e76\u84b8\u998f\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u3002", "result": "\u5728\u5bf9\u8bdd\u8d28\u91cf\u7684\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u5956\u52b1\u5206\u89e3\u65b9\u6cd5\uff0c\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5f3a\u5927\u7684\u5956\u52b1\u5206\u89e3\u5de5\u5177\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u6709\u6548\u66ff\u4ee3\u624b\u52a8\u5956\u52b1\u8bbe\u8ba1\u548c\u7ec6\u7c92\u5ea6\u53cd\u9988\uff0c\u63d0\u5347\u5bf9\u8bdd\u4ee3\u7406\u5bf9\u9f50\u6548\u679c\u3002"}}
{"id": "2505.15865", "pdf": "https://arxiv.org/pdf/2505.15865", "abs": "https://arxiv.org/abs/2505.15865", "authors": ["Ingeol Baek", "Hwan Chang", "Sunghyun Ryu", "Hwanhee Lee"], "title": "How Do Large Vision-Language Models See Text in Image? Unveiling the Distinctive Role of OCR Heads", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in Large Vision Language Models (LVLMs), a\ngap remains, particularly regarding their interpretability and how they locate\nand interpret textual information within images. In this paper, we explore\nvarious LVLMs to identify the specific heads responsible for recognizing text\nfrom images, which we term the Optical Character Recognition Head (OCR Head).\nOur findings regarding these heads are as follows: (1) Less Sparse: Unlike\nprevious retrieval heads, a large number of heads are activated to extract\ntextual information from images. (2) Qualitatively Distinct: OCR heads possess\nproperties that differ significantly from general retrieval heads, exhibiting\nlow similarity in their characteristics. (3) Statically Activated: The\nfrequency of activation for these heads closely aligns with their OCR scores.\nWe validate our findings in downstream tasks by applying Chain-of-Thought (CoT)\nto both OCR and conventional retrieval heads and by masking these heads. We\nalso demonstrate that redistributing sink-token values within the OCR heads\nimproves performance. These insights provide a deeper understanding of the\ninternal mechanisms LVLMs employ in processing embedded textual information in\nimages.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u8d1f\u8d23\u8bc6\u522b\u56fe\u50cf\u6587\u672c\u7684\u7279\u5b9a\u5934\uff08OCR Head\uff09\uff0c\u53d1\u73b0\u5176\u5177\u6709\u7a00\u758f\u6027\u4f4e\u3001\u6027\u8d28\u72ec\u7279\u548c\u9759\u6001\u6fc0\u6d3b\u7684\u7279\u70b9\uff0c\u5e76\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u548c\u89e3\u91ca\u6587\u672c\u4fe1\u606f\u7684\u53ef\u89e3\u91ca\u6027\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63a2\u7d22\u4e86\u591a\u79cdLVLMs\uff0c\u8bc6\u522b\u51fa\u8d1f\u8d23\u6587\u672c\u8bc6\u522b\u7684OCR Head\uff0c\u5e76\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u548c\u63a9\u7801\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u7279\u6027\u3002", "result": "OCR Head\u5177\u6709\u4f4e\u7a00\u758f\u6027\u3001\u72ec\u7279\u6027\u8d28\u548c\u9759\u6001\u6fc0\u6d3b\u7684\u7279\u70b9\uff0c\u4e14\u901a\u8fc7\u8c03\u6574\u5176sink-token\u503c\u53ef\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LVLMs\u5904\u7406\u56fe\u50cf\u4e2d\u5d4c\u5165\u6587\u672c\u4fe1\u606f\u7684\u5185\u90e8\u673a\u5236\uff0c\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.15948", "pdf": "https://arxiv.org/pdf/2505.15948", "abs": "https://arxiv.org/abs/2505.15948", "authors": ["Parth Sarin", "Juan Pablo Alperin"], "title": "Citation Parsing and Analysis with Language Models", "categories": ["cs.CL", "cs.DL", "cs.SI"], "comment": "Presented at the Workshop on Open Citations & Open Scholarly Metadata\n  2025", "summary": "A key type of resource needed to address global inequalities in knowledge\nproduction and dissemination is a tool that can support journals in\nunderstanding how knowledge circulates. The absence of such a tool has resulted\nin comparatively less information about networks of knowledge sharing in the\nGlobal South. In turn, this gap authorizes the exclusion of researchers and\nscholars from the South in indexing services, reinforcing colonial arrangements\nthat de-center and minoritize those scholars. In order to support citation\nnetwork tracking on a global scale, we investigate the capacity of open-weight\nlanguage models to mark up manuscript citations in an indexable format. We\nassembled a dataset of matched plaintext and annotated citations from preprints\nand published research papers. Then, we evaluated a number of open-weight\nlanguage models on the annotation task. We find that, even out of the box,\ntoday's language models achieve high levels of accuracy on identifying the\nconstituent components of each citation, outperforming state-of-the-art\nmethods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all\nfields with high accuracy in $2^5$ passes, suggesting that post-training is\nlikely to be effective in producing small, robust citation parsing models. Such\na tool could greatly improve the fidelity of citation networks and thus\nmeaningfully improve research indexing and discovery, as well as further\nmetascientific research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u89e3\u6790\u548c\u6807\u8bb0\u6587\u732e\u5f15\u7528\uff0c\u4ee5\u6539\u5584\u5168\u7403\u77e5\u8bc6\u5171\u4eab\u7f51\u7edc\u7684\u4e0d\u5e73\u7b49\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5168\u7403\u77e5\u8bc6\u751f\u4ea7\u548c\u4f20\u64ad\u4e2d\u7684\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5168\u7403\u5357\u65b9\u5b66\u8005\u5728\u7d22\u5f15\u670d\u52a1\u4e2d\u7684\u8fb9\u7f18\u5316\u73b0\u8c61\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u5bf9\u6587\u732e\u5f15\u7528\u8fdb\u884c\u6807\u8bb0\u548c\u89e3\u6790\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u5728\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u6790\u5f15\u7528\u65f6\u5177\u6709\u9ad8\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5c0f\u6a21\u578bQwen3-0.6B\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u5de5\u5177\u53ef\u663e\u8457\u63d0\u5347\u5f15\u7528\u7f51\u7edc\u7684\u51c6\u786e\u6027\uff0c\u6539\u5584\u7814\u7a76\u7d22\u5f15\u548c\u53d1\u73b0\uff0c\u63a8\u52a8\u5143\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2505.15867", "pdf": "https://arxiv.org/pdf/2505.15867", "abs": "https://arxiv.org/abs/2505.15867", "authors": ["Nikolaos Chaidos", "Angeliki Dimitriou", "Maria Lymperaiou", "Giorgos Stamou"], "title": "SCENIR: Visual Semantic Clarity through Unsupervised Scene Graph Retrieval", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Despite the dominance of convolutional and transformer-based architectures in\nimage-to-image retrieval, these models are prone to biases arising from\nlow-level visual features, such as color. Recognizing the lack of semantic\nunderstanding as a key limitation, we propose a novel scene graph-based\nretrieval framework that emphasizes semantic content over superficial image\ncharacteristics. Prior approaches to scene graph retrieval predominantly rely\non supervised Graph Neural Networks (GNNs), which require ground truth graph\npairs driven from image captions. However, the inconsistency of caption-based\nsupervision stemming from variable text encodings undermine retrieval\nreliability. To address these, we present SCENIR, a Graph Autoencoder-based\nunsupervised retrieval framework, which eliminates the dependence on labeled\ntraining data. Our model demonstrates superior performance across metrics and\nruntime efficiency, outperforming existing vision-based, multimodal, and\nsupervised GNN approaches. We further advocate for Graph Edit Distance (GED) as\na deterministic and robust ground truth measure for scene graph similarity,\nreplacing the inconsistent caption-based alternatives for the first time in\nimage-to-image retrieval evaluation. Finally, we validate the generalizability\nof our method by applying it to unannotated datasets via automated scene graph\ngeneration, while substantially contributing in advancing state-of-the-art in\ncounterfactual image retrieval.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u56fe\u50cf\u68c0\u7d22\u6846\u67b6SCENIR\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u7684\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u548c\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5377\u79ef\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u56fe\u50cf\u68c0\u7d22\u4e2d\u6613\u53d7\u4f4e\u5c42\u6b21\u89c6\u89c9\u7279\u5f81\uff08\u5982\u989c\u8272\uff09\u7684\u504f\u89c1\u5f71\u54cd\uff0c\u4e14\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u573a\u666f\u56fe\u68c0\u7d22\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e0d\u4e00\u81f4\u7684\u6807\u6ce8\u6570\u636e\u3002", "method": "\u63d0\u51faSCENIR\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65e0\u76d1\u7763\u68c0\u7d22\u6846\u67b6\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u9996\u6b21\u5f15\u5165\u56fe\u7f16\u8f91\u8ddd\u79bb\uff08GED\uff09\u4f5c\u4e3a\u573a\u666f\u56fe\u76f8\u4f3c\u6027\u7684\u786e\u5b9a\u6027\u5ea6\u91cf\u3002", "result": "SCENIR\u5728\u6027\u80fd\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u3001\u591a\u6a21\u6001\u548c\u76d1\u7763GNN\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316\u573a\u666f\u56fe\u751f\u6210\u9a8c\u8bc1\u4e86\u5176\u5728\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SCENIR\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u548cGED\u5ea6\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u68c0\u7d22\u7684\u8bed\u4e49\u7406\u89e3\u548c\u53ef\u9760\u6027\uff0c\u63a8\u52a8\u4e86\u53cd\u4e8b\u5b9e\u56fe\u50cf\u68c0\u7d22\u7684\u5148\u8fdb\u6280\u672f\u3002"}}
{"id": "2505.15960", "pdf": "https://arxiv.org/pdf/2505.15960", "abs": "https://arxiv.org/abs/2505.15960", "authors": ["Ryo Kamoi", "Yusen Zhang", "Nan Zhang", "Sarkar Snigdha Sarathi Das", "Rui Zhang"], "title": "Training Step-Level Reasoning Verifiers with Formal Verification Tools", "categories": ["cs.CL"], "comment": "Datasets, models, and code are provided at\n  https://github.com/psunlpgroup/FoVer. Please also refer to our project\n  website at https://fover-prm.github.io/", "summary": "Process Reward Models (PRMs), which provide step-by-step feedback on the\nreasoning generated by Large Language Models (LLMs), are receiving increasing\nattention. However, two key research gaps remain: collecting accurate\nstep-level error labels for training typically requires costly human\nannotation, and existing PRMs are limited to math reasoning problems. In\nresponse to these gaps, this paper aims to address the challenges of automatic\ndataset creation and the generalization of PRMs to diverse reasoning tasks. To\nachieve this goal, we propose FoVer, an approach for training PRMs on\nstep-level error labels automatically annotated by formal verification tools,\nsuch as Z3 for formal logic and Isabelle for theorem proof, which provide\nautomatic and accurate verification for symbolic tasks. Using this approach, we\nsynthesize a training dataset with error labels on LLM responses for formal\nlogic and theorem proof tasks without human annotation. Although this data\nsynthesis is feasible only for tasks compatible with formal verification, we\nobserve that LLM-based PRMs trained on our dataset exhibit cross-task\ngeneralization, improving verification across diverse reasoning tasks.\nSpecifically, PRMs trained with FoVer significantly outperform baseline PRMs\nbased on the original LLMs and achieve competitive or superior results compared\nto state-of-the-art PRMs trained on labels annotated by humans or stronger\nmodels, as measured by step-level verification on ProcessBench and Best-of-K\nperformance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,\nand BBH. The datasets, models, and code are provided at\nhttps://github.com/psunlpgroup/FoVer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFoVer\u65b9\u6cd5\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u81ea\u52a8\u6807\u6ce8\u6b65\u9aa4\u7ea7\u9519\u8bef\u6807\u7b7e\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\uff0c\u89e3\u51b3\u4e86\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u548c\u4efb\u52a1\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PRMs\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u4e14\u4ec5\u9002\u7528\u4e8e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\uff0c\u9700\u89e3\u51b3\u81ea\u52a8\u6570\u636e\u96c6\u521b\u5efa\u548c\u4efb\u52a1\u6cdb\u5316\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\uff08\u5982Z3\u548cIsabelle\uff09\u81ea\u52a8\u6807\u6ce8\u6b65\u9aa4\u7ea7\u9519\u8bef\u6807\u7b7e\uff0c\u5408\u6210\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u8bad\u7ec3LLM-based PRMs\u3002", "result": "FoVer\u8bad\u7ec3\u7684PRMs\u5728\u591a\u6837\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\u6216\u8d85\u8d8a\u3002", "conclusion": "FoVer\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u548c\u4efb\u52a1\u6cdb\u5316\u6027\u6269\u5c55\u4e86PRMs\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.15870", "pdf": "https://arxiv.org/pdf/2505.15870", "abs": "https://arxiv.org/abs/2505.15870", "authors": ["Can Rong", "Xin Zhang", "Yanxin Xi", "Hongjie Sui", "Jingtao Ding", "Yong Li"], "title": "Satellites Reveal Mobility: A Commuting Origin-destination Flow Generator for Global Cities", "categories": ["cs.CV", "cs.CY", "eess.IV"], "comment": "26 pages, 8 figures", "summary": "Commuting Origin-destination~(OD) flows, capturing daily population mobility\nof citizens, are vital for sustainable development across cities around the\nworld. However, it is challenging to obtain the data due to the high cost of\ntravel surveys and privacy concerns. Surprisingly, we find that satellite\nimagery, publicly available across the globe, contains rich urban semantic\nsignals to support high-quality OD flow generation, with over 98\\%\nexpressiveness of traditional multisource hard-to-collect urban\nsociodemographic, economics, land use, and point of interest data. This\ninspires us to design a novel data generator, GlODGen, which can generate OD\nflow data for any cities of interest around the world. Specifically, GlODGen\nfirst leverages Vision-Language Geo-Foundation Models to extract urban semantic\nsignals related to human mobility from satellite imagery. These features are\nthen combined with population data to form region-level representations, which\nare used to generate OD flows via graph diffusion models. Extensive experiments\non 4 continents and 6 representative cities show that GlODGen has great\ngeneralizability across diverse urban environments on different continents and\ncan generate OD flow data for global cities highly consistent with real-world\nmobility data. We implement GlODGen as an automated tool, seamlessly\nintegrating data acquisition and curation, urban semantic feature extraction,\nand OD flow generation together. It has been released at\nhttps://github.com/tsinghua-fib-lab/generate-od-pubtools.", "AI": {"tldr": "GlODGen\u5229\u7528\u536b\u661f\u56fe\u50cf\u548c\u4eba\u53e3\u6570\u636e\u751f\u6210\u5168\u7403\u57ce\u5e02\u7684OD\u6d41\u91cf\u6570\u636e\uff0c\u66ff\u4ee3\u4f20\u7edf\u9ad8\u6210\u672c\u8c03\u67e5\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u83b7\u53d6OD\u6d41\u91cf\u6570\u636e\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6d89\u53ca\u9690\u79c1\u95ee\u9898\uff0c\u800c\u536b\u661f\u56fe\u50cf\u53ef\u63d0\u4f9b\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u3002", "method": "GlODGen\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u5730\u7406\u57fa\u7840\u6a21\u578b\u548c\u56fe\u6269\u6563\u6a21\u578b\uff0c\u4ece\u536b\u661f\u56fe\u50cf\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u5e76\u751f\u6210OD\u6d41\u91cf\u3002", "result": "\u5728\u516d\u5927\u6d32\u516d\u4e2a\u57ce\u5e02\u7684\u5b9e\u9a8c\u4e2d\uff0cGlODGen\u751f\u6210\u7684OD\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "GlODGen\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684OD\u6570\u636e\u751f\u6210\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5168\u7403\u57ce\u5e02\u3002"}}
{"id": "2505.15962", "pdf": "https://arxiv.org/pdf/2505.15962", "abs": "https://arxiv.org/abs/2505.15962", "authors": ["Linxi Zhao", "Sofian Zalouk", "Christian K. Belardi", "Justin Lovelace", "Jin Peng Zhou", "Kilian Q. Weinberger", "Yoav Artzi", "Jennifer J. Sun"], "title": "Pre-training Large Memory Language Models with Internal and External Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Neural language models are black-boxes -- both linguistic patterns and\nfactual knowledge are distributed across billions of opaque parameters. This\nentangled encoding makes it difficult to reliably inspect, verify, or update\nspecific facts. We propose a new class of language models, Large Memory\nLanguage Models (LMLM) with a pre-training recipe that stores factual knowledge\nin both internal weights and an external database. Our approach strategically\nmasks externally retrieved factual values from the training loss, thereby\nteaching the model to perform targeted lookups rather than relying on\nmemorization in model weights. Our experiments demonstrate that LMLMs achieve\ncompetitive performance compared to significantly larger, knowledge-dense LLMs\non standard benchmarks, while offering the advantages of explicit, editable,\nand verifiable knowledge bases. This work represents a fundamental shift in how\nlanguage models interact with and manage factual knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8bed\u8a00\u6a21\u578bLMLM\uff0c\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u6743\u91cd\u548c\u5916\u90e8\u6570\u636e\u5e93\u5b58\u50a8\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u51cf\u5c11\u5bf9\u6a21\u578b\u8bb0\u5fc6\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u53ef\u7f16\u8f91\u548c\u53ef\u9a8c\u8bc1\u7684\u77e5\u8bc6\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u4e2d\u77e5\u8bc6\u5206\u5e03\u4e0d\u900f\u660e\u3001\u96be\u4ee5\u68c0\u67e5\u548c\u66f4\u65b0\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLMLM\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\u5c06\u4e8b\u5b9e\u77e5\u8bc6\u5b58\u50a8\u5728\u5185\u90e8\u6743\u91cd\u548c\u5916\u90e8\u6570\u636e\u5e93\u4e2d\uff0c\u5e76\u5c4f\u853d\u5916\u90e8\u68c0\u7d22\u7684\u4e8b\u5b9e\u503c\u4ee5\u51cf\u5c11\u8bb0\u5fc6\u4f9d\u8d56\u3002", "result": "LMLM\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u66f4\u5927\u89c4\u6a21\u7684\u77e5\u8bc6\u5bc6\u96c6\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u5177\u5907\u663e\u5f0f\u3001\u53ef\u7f16\u8f91\u548c\u53ef\u9a8c\u8bc1\u7684\u77e5\u8bc6\u5e93\u4f18\u52bf\u3002", "conclusion": "LMLM\u4ee3\u8868\u4e86\u8bed\u8a00\u6a21\u578b\u4e0e\u4e8b\u5b9e\u77e5\u8bc6\u4ea4\u4e92\u548c\u7ba1\u7406\u65b9\u5f0f\u7684\u6839\u672c\u6027\u8f6c\u53d8\u3002"}}
{"id": "2505.15875", "pdf": "https://arxiv.org/pdf/2505.15875", "abs": "https://arxiv.org/abs/2505.15875", "authors": ["Shenghe Zheng", "Hongzhi Wang", "Chenyu Huang", "Xiaohui Wang", "Tao Chen", "Jiayuan Fan", "Shuyue Hu", "Peng Ye"], "title": "Decouple and Orthogonalize: A Data-Free Framework for LoRA Merging", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages, 5 figures", "summary": "With more open-source models available for diverse tasks, model merging has\ngained attention by combining models into one, reducing training, storage, and\ninference costs. Current research mainly focuses on model merging for full\nfine-tuning, overlooking the popular LoRA. However, our empirical analysis\nreveals that: a) existing merging methods designed for full fine-tuning perform\npoorly on LoRA; b) LoRA modules show much larger parameter magnitude variance\nthan full fine-tuned weights; c) greater parameter magnitude variance\ncorrelates with worse merging performance. Considering that large magnitude\nvariances cause deviations in the distribution of the merged parameters,\nresulting in information loss and performance degradation, we propose a\nDecoupled and Orthogonal merging approach(DO-Merging). By separating parameters\ninto magnitude and direction components and merging them independently, we\nreduce the impact of magnitude differences on the directional alignment of the\nmerged models, thereby preserving task information. Furthermore, we introduce a\ndata-free, layer-wise gradient descent method with orthogonal constraints to\nmitigate interference during the merging of direction components. We provide\ntheoretical guarantees for both the decoupling and orthogonal components. And\nwe validate through extensive experiments across vision, language, and\nmulti-modal domains that our proposed DO-Merging can achieve significantly\nhigher performance than existing merging methods at a minimal cost. Notably,\neach component can be flexibly integrated with existing methods, offering near\nfree-lunch improvements across tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LoRA\u6a21\u578b\u7684\u89e3\u8026\u6b63\u4ea4\u5408\u5e76\u65b9\u6cd5\uff08DO-Merging\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5408\u5e76\u65b9\u6cd5\u5728LoRA\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u53c2\u6570\u7684\u5927\u5c0f\u548c\u65b9\u5411\u5206\u91cf\u5e76\u72ec\u7acb\u5408\u5e76\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u5e76\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u6a21\u578b\u7684\u589e\u591a\uff0c\u6a21\u578b\u5408\u5e76\u6210\u4e3a\u964d\u4f4e\u8bad\u7ec3\u3001\u5b58\u50a8\u548c\u63a8\u7406\u6210\u672c\u7684\u91cd\u8981\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5168\u5fae\u8c03\u6a21\u578b\u7684\u5408\u5e76\uff0c\u5ffd\u89c6\u4e86\u6d41\u884c\u7684LoRA\u6a21\u578b\u3002\u5b9e\u8bc1\u5206\u6790\u8868\u660e\uff0c\u73b0\u6709\u65b9\u6cd5\u5728LoRA\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u53c2\u6570\u5e45\u5ea6\u5dee\u5f02\u8f83\u5927\u5bfc\u81f4\u5408\u5e76\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDO-Merging\u65b9\u6cd5\uff0c\u5c06\u53c2\u6570\u89e3\u8026\u4e3a\u5e45\u5ea6\u548c\u65b9\u5411\u5206\u91cf\u5e76\u72ec\u7acb\u5408\u5e76\uff0c\u540c\u65f6\u5f15\u5165\u65e0\u6570\u636e\u3001\u5206\u5c42\u68af\u5ea6\u4e0b\u964d\u548c\u6b63\u4ea4\u7ea6\u675f\u4ee5\u51cf\u5c11\u65b9\u5411\u5206\u91cf\u7684\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDO-Merging\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u4ee5\u6781\u4f4e\u6210\u672c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DO-Merging\u901a\u8fc7\u89e3\u8026\u548c\u6b63\u4ea4\u5408\u5e76\u6709\u6548\u89e3\u51b3\u4e86LoRA\u6a21\u578b\u5408\u5e76\u7684\u95ee\u9898\uff0c\u5404\u7ec4\u4ef6\u53ef\u7075\u6d3b\u96c6\u6210\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4efb\u52a1\u63d0\u4f9b\u8fd1\u4e4e\u514d\u8d39\u7684\u6539\u8fdb\u3002"}}
{"id": "2505.15993", "pdf": "https://arxiv.org/pdf/2505.15993", "abs": "https://arxiv.org/abs/2505.15993", "authors": ["Anirudh Maiya", "Razan Alghamdi", "Maria Leonor Pacheco", "Ashutosh Trivedi", "Fabio Somenzi"], "title": "Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku", "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "The success of Large Language Models (LLMs) in human-AI collaborative\ndecision-making hinges on their ability to provide trustworthy, gradual, and\ntailored explanations. Solving complex puzzles, such as Sudoku, offers a\ncanonical example of this collaboration, where clear and customized\nexplanations often hold greater importance than the final solution. In this\nstudy, we evaluate the performance of five LLMs in solving and explaining\n\\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving\npuzzles, none can explain the solution process in a manner that reflects\nstrategic reasoning or intuitive problem-solving. These findings underscore\nsignificant challenges that must be addressed before LLMs can become effective\npartners in human-AI collaborative decision-making.", "AI": {"tldr": "\u8bc4\u4f30\u4e94\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u548c\u89e3\u91ca\u6570\u72ec\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u89e3\u91ca\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u4eba\u7c7b-AI\u534f\u4f5c\u51b3\u7b56\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3001\u9010\u6b65\u6027\u548c\u5b9a\u5236\u5316\u89e3\u91ca\u80fd\u529b\uff0c\u4ee5\u6570\u72ec\u4e3a\u4f8b\u3002", "method": "\u8bc4\u4f30\u4e94\u79cdLLMs\u5728\u89e3\u51b3\u548c\u89e3\u91ca\u516d\u516d\u6570\u72ec\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e00\u79cdLLM\u80fd\u6709\u9650\u89e3\u51b3\u6570\u72ec\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5747\u65e0\u6cd5\u63d0\u4f9b\u53cd\u6620\u6218\u7565\u63a8\u7406\u6216\u76f4\u89c2\u95ee\u9898\u89e3\u51b3\u7684\u89e3\u91ca\u3002", "conclusion": "LLMs\u5728\u6210\u4e3a\u6709\u6548\u534f\u4f5c\u4f19\u4f34\u524d\u9700\u89e3\u51b3\u89e3\u91ca\u80fd\u529b\u7684\u91cd\u5927\u6311\u6218\u3002"}}
{"id": "2505.15877", "pdf": "https://arxiv.org/pdf/2505.15877", "abs": "https://arxiv.org/abs/2505.15877", "authors": ["Siting Li", "Xiang Gao", "Simon Shaolei Du"], "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "25 pages, 5 figures", "summary": "While an image is worth more than a thousand words, only a few provide\ncrucial information for a given task and thus should be focused on. In light of\nthis, ideal text-to-image (T2I) retrievers should prioritize specific visual\nattributes relevant to queries. To evaluate current retrievers on handling\nattribute-focused queries, we build COCO-Facet, a COCO-based benchmark with\n9,112 queries about diverse attributes of interest. We find that CLIP-like\nretrievers, which are widely adopted due to their efficiency and zero-shot\nability, have poor and imbalanced performance, possibly because their image\nembeddings focus on global semantics and subjects while leaving out other\ndetails. Notably, we reveal that even recent Multimodal Large Language Model\n(MLLM)-based, stronger retrievers with a larger output dimension struggle with\nthis limitation. Hence, we hypothesize that retrieving with general image\nembeddings is suboptimal for performing such queries. As a solution, we propose\nto use promptable image embeddings enabled by these multimodal retrievers,\nwhich boost performance by highlighting required attributes. Our pipeline for\nderiving such embeddings generalizes across query types, image pools, and base\nretriever architectures. To enhance real-world applicability, we offer two\nacceleration strategies: Pre-processing promptable embeddings and using linear\napproximations. We show that the former yields a 15% improvement in Recall@5\nwhen prompts are predefined, while the latter achieves an 8% improvement when\nprompts are only available during inference.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCOCO-Facet\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u5668\u5728\u5c5e\u6027\u805a\u7126\u67e5\u8be2\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\uff08\u5982CLIP\u548cMLLM\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u5d4c\u5165\u65b9\u6cd5\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u5668\u5728\u5c5e\u6027\u805a\u7126\u67e5\u8be2\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662fCLIP\u548cMLLM\u7b49\u65b9\u6cd5\uff0c\u56e0\u5176\u56fe\u50cf\u5d4c\u5165\u5173\u6ce8\u5168\u5c40\u8bed\u4e49\u800c\u5ffd\u7565\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u63d0\u793a\u6027\u56fe\u50cf\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u68c0\u7d22\u5668\u751f\u6210\u53ef\u63d0\u793a\u7684\u56fe\u50cf\u5d4c\u5165\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u52a0\u901f\u7b56\u7565\uff1a\u9884\u5904\u7406\u63d0\u793a\u5d4c\u5165\u548c\u7ebf\u6027\u8fd1\u4f3c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9884\u5904\u7406\u7684\u63d0\u793a\u5d4c\u5165\u5728\u9884\u5b9a\u4e49\u63d0\u793a\u4e0bRecall@5\u63d0\u534715%\uff0c\u800c\u7ebf\u6027\u8fd1\u4f3c\u5728\u63a8\u7406\u65f6\u63d0\u793a\u53ef\u7528\u65f6\u63d0\u53478%\u3002", "conclusion": "\u63d0\u793a\u6027\u56fe\u50cf\u5d4c\u5165\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5c5e\u6027\u805a\u7126\u67e5\u8be2\u7684\u6027\u80fd\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.16000", "pdf": "https://arxiv.org/pdf/2505.16000", "abs": "https://arxiv.org/abs/2505.16000", "authors": ["Mehrdad ghassabi", "Pedram Rostami", "Hamidreza Baradaran Kashani", "Amirhossein Poursina", "Zahra Kazemi", "Milad Tavakoli"], "title": "Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 4 figures", "summary": "The rapid advancement of language models has demonstrated the potential of\nartificial intelligence in the healthcare industry. However, small language\nmodels struggle with specialized domains in low-resource languages like\nPersian. While numerous medical-domain websites exist in Persian, no curated\ndataset or corpus has been available making ours the first of its kind. This\nstudy explores the enhancement of medical knowledge in a small language model\nby leveraging accessible online data, including a crawled corpus from medical\nmagazines and a dataset of real doctor-patient QA pairs. We fine-tuned a\nbaseline model using our curated data to improve its medical knowledge.\nBenchmark evaluations demonstrate that the fine-tuned model achieves improved\naccuracy in medical question answering and provides better responses compared\nto its baseline. This work highlights the potential of leveraging open-access\nonline data to enrich small language models in medical fields, providing a\nnovel solution for Persian medical AI applications suitable for\nresource-constrained environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5229\u7528\u6ce2\u65af\u8bed\u5728\u7ebf\u533b\u7597\u6570\u636e\uff08\u5305\u62ec\u533b\u5b66\u6742\u5fd7\u548c\u533b\u60a3\u95ee\u7b54\u5bf9\uff09\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u5176\u5728\u533b\u7597\u9886\u57df\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u533b\u7597\u9886\u57df\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u586b\u8865\u4e86\u6ce2\u65af\u8bed\u533b\u7597\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u722c\u53d6\u6ce2\u65af\u8bed\u533b\u5b66\u6742\u5fd7\u548c\u533b\u60a3\u95ee\u7b54\u5bf9\u6570\u636e\uff0c\u6784\u5efa\u9996\u4e2a\u6ce2\u65af\u8bed\u533b\u7597\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u5fae\u8c03\u57fa\u7ebf\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u5f00\u653e\u83b7\u53d6\u7684\u5728\u7ebf\u6570\u636e\u53ef\u4ee5\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u8868\u73b0\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6ce2\u65af\u8bed\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848\u3002"}}
{"id": "2505.15879", "pdf": "https://arxiv.org/pdf/2505.15879", "abs": "https://arxiv.org/abs/2505.15879", "authors": ["Yue Fan", "Xuehai He", "Diji Yang", "Kaizhi Zheng", "Ching-Chen Kuo", "Yuting Zheng", "Sravana Jyothi Narayanaraju", "Xinze Guan", "Xin Eric Wang"], "title": "GRIT: Teaching MLLMs to Think with Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning\n(RL) in building reasoning models that articulate chains of thoughts prior to\nproducing final answers. However, despite ongoing advances that aim at enabling\nreasoning for vision-language tasks, existing open-source visual reasoning\nmodels typically generate reasoning content with pure natural language, lacking\nexplicit integration of visual information. This limits their ability to\nproduce clearly articulated and visually grounded reasoning chains. To this\nend, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method\nfor training MLLMs to think with images. GRIT introduces a grounded reasoning\nparadigm, in which models generate reasoning chains that interleave natural\nlanguage and explicit bounding box coordinates. These coordinates point to\nregions of the input image that the model consults during its reasoning\nprocess. Additionally, GRIT is equipped with a reinforcement learning approach,\nGRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused\non the final answer accuracy and format of the grounded reasoning output, which\neliminates the need for data with reasoning chain annotations or explicit\nbounding box labels. As a result, GRIT achieves exceptional data efficiency,\nrequiring as few as 20 image-question-answer triplets from existing datasets.\nComprehensive evaluations demonstrate that GRIT effectively trains MLLMs to\nproduce coherent and visually grounded reasoning chains, showing a successful\nunification of reasoning and grounding abilities.", "AI": {"tldr": "GRIT\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u4fe1\u606f\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89c6\u89c9\u63a5\u5730\u7684\u63a8\u7406\u94fe\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u4ec5\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u751f\u6210\u63a8\u7406\u5185\u5bb9\uff0c\u7f3a\u4e4f\u89c6\u89c9\u4fe1\u606f\u7684\u663e\u5f0f\u6574\u5408\uff0c\u9650\u5236\u4e86\u5176\u751f\u6210\u6e05\u6670\u4e14\u89c6\u89c9\u63a5\u5730\u7684\u63a8\u7406\u94fe\u7684\u80fd\u529b\u3002", "method": "GRIT\u5f15\u5165\u4e86\u4e00\u79cd\u63a5\u5730\u7684\u63a8\u7406\u8303\u5f0f\uff0c\u6a21\u578b\u751f\u6210\u4ea4\u66ff\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u548c\u663e\u5f0f\u8fb9\u754c\u6846\u5750\u6807\u7684\u63a8\u7406\u94fe\uff0c\u5e76\u7ed3\u5408GRPO-GR\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4f18\u5316\u8bad\u7ec3\u3002", "result": "GRIT\u4ec5\u9700\u5c11\u91cf\u6570\u636e\uff0820\u4e2a\u56fe\u50cf-\u95ee\u9898-\u7b54\u6848\u4e09\u5143\u7ec4\uff09\u5373\u53ef\u9ad8\u6548\u8bad\u7ec3\u6a21\u578b\uff0c\u751f\u6210\u8fde\u8d2f\u4e14\u89c6\u89c9\u63a5\u5730\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "GRIT\u6210\u529f\u7edf\u4e00\u4e86\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16002", "pdf": "https://arxiv.org/pdf/2505.16002", "abs": "https://arxiv.org/abs/2505.16002", "authors": ["Sasha Boguraev", "Christopher Potts", "Kyle Mahowald"], "title": "Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 19 figures, 11 tables", "summary": "Large Language Models (LLMs) have emerged as powerful sources of evidence for\nlinguists seeking to develop theories of syntax. In this paper, we argue that\ncausal interpretability methods, applied to LLMs, can greatly enhance the value\nof such evidence by helping us characterize the abstract mechanisms that LLMs\nlearn to use. Our empirical focus is a set of English filler-gap dependency\nconstructions (e.g., questions, relative clauses). Linguistic theories largely\nagree that these constructions share many properties. Using experiments based\nin Distributed Interchange Interventions, we show that LLMs converge on similar\nabstract analyses of these constructions. These analyses also reveal previously\noverlooked factors -- relating to frequency, filler type, and surrounding\ncontext -- that could motivate changes to standard linguistic theory. Overall,\nthese results suggest that mechanistic, internal analyses of LLMs can push\nlinguistic theory forward.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u56e0\u679c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u4ee5\u63ed\u793a\u5176\u5b66\u4e60\u5230\u7684\u62bd\u8c61\u673a\u5236\uff0c\u4ece\u800c\u63a8\u52a8\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u53d1\u5c55\u3002", "motivation": "\u8bed\u8a00\u5b66\u7406\u8bba\u9700\u8981\u66f4\u6df1\u5165\u7684\u8bc1\u636e\u6765\u7406\u89e3\u53e5\u6cd5\u7ed3\u6784\uff0c\u800cLLMs\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6f5c\u5728\u8bc1\u636e\u3002\u901a\u8fc7\u5206\u6790LLMs\u7684\u5185\u90e8\u673a\u5236\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5176\u5982\u4f55\u5904\u7406\u82f1\u8bed\u586b\u5145-\u7a7a\u7f3a\u4f9d\u8d56\u7ed3\u6784\uff08\u5982\u7591\u95ee\u53e5\u3001\u5173\u7cfb\u4ece\u53e5\uff09\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u4ea4\u6362\u5e72\u9884\uff08Distributed Interchange Interventions\uff09\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5206\u6790LLMs\u5bf9\u586b\u5145-\u7a7a\u7f3a\u4f9d\u8d56\u7ed3\u6784\u7684\u62bd\u8c61\u5904\u7406\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5bf9\u8fd9\u4e9b\u7ed3\u6784\u91c7\u7528\u4e86\u76f8\u4f3c\u7684\u62bd\u8c61\u5206\u6790\uff0c\u5e76\u63ed\u793a\u4e86\u9891\u7387\u3001\u586b\u5145\u7c7b\u578b\u548c\u4e0a\u4e0b\u6587\u7b49\u88ab\u5ffd\u89c6\u7684\u56e0\u7d20\uff0c\u8fd9\u4e9b\u53d1\u73b0\u53ef\u80fd\u63a8\u52a8\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u4fee\u6b63\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u5316\u5206\u6790LLMs\u7684\u5185\u90e8\u673a\u5236\uff0c\u53ef\u4ee5\u4e3a\u8bed\u8a00\u5b66\u7406\u8bba\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\u548c\u63a8\u52a8\u529b\u3002"}}
{"id": "2505.15880", "pdf": "https://arxiv.org/pdf/2505.15880", "abs": "https://arxiv.org/abs/2505.15880", "authors": ["Zhiyuan Xu", "Bohan Li", "Huan-ang Gao", "Mingju Gao", "Yong Chen", "Ming Liu", "Chenxu Yan", "Hang Zhao", "Shuo Feng", "Hao Zhao"], "title": "Challenger: Affordable Adversarial Driving Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://pixtella.github.io/Challenger/", "summary": "Generating photorealistic driving videos has seen significant progress\nrecently, but current methods largely focus on ordinary, non-adversarial\nscenarios. Meanwhile, efforts to generate adversarial driving scenarios often\noperate on abstract trajectory or BEV representations, falling short of\ndelivering realistic sensor data that can truly stress-test autonomous driving\n(AD) systems. In this work, we introduce Challenger, a framework that produces\nphysically plausible yet photorealistic adversarial driving videos. Generating\nsuch videos poses a fundamental challenge: it requires jointly optimizing over\nthe space of traffic interactions and high-fidelity sensor observations.\nChallenger makes this affordable through two techniques: (1) a physics-aware\nmulti-round trajectory refinement process that narrows down candidate\nadversarial maneuvers, and (2) a tailored trajectory scoring function that\nencourages realistic yet adversarial behavior while maintaining compatibility\nwith downstream video synthesis. As tested on the nuScenes dataset, Challenger\ngenerates a diverse range of aggressive driving scenarios-including cut-ins,\nsudden lane changes, tailgating, and blind spot intrusions-and renders them\ninto multiview photorealistic videos. Extensive evaluations show that these\nscenarios significantly increase the collision rate of state-of-the-art\nend-to-end AD models (UniAD, VAD, SparseDrive, and DiffusionDrive), and\nimportantly, adversarial behaviors discovered for one model often transfer to\nothers.", "AI": {"tldr": "Challenger\u6846\u67b6\u751f\u6210\u7269\u7406\u5408\u7406\u4e14\u903c\u771f\u7684\u5bf9\u6297\u6027\u9a7e\u9a76\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u78b0\u649e\u7387\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u666e\u901a\u9a7e\u9a76\u573a\u666f\uff0c\u7f3a\u4e4f\u903c\u771f\u7684\u5bf9\u6297\u6027\u4f20\u611f\u5668\u6570\u636e\u4ee5\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u7269\u7406\u611f\u77e5\u7684\u591a\u8f6e\u8f68\u8ff9\u4f18\u5316\u548c\u5b9a\u5236\u8f68\u8ff9\u8bc4\u5206\u51fd\u6570\uff0c\u751f\u6210\u903c\u771f\u5bf9\u6297\u6027\u9a7e\u9a76\u89c6\u9891\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u751f\u6210\u591a\u6837\u5316\u5bf9\u6297\u573a\u666f\uff0c\u663e\u8457\u63d0\u9ad8\u591a\u4e2a\u5148\u8fdb\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u78b0\u649e\u7387\u3002", "conclusion": "Challenger\u80fd\u6709\u6548\u751f\u6210\u903c\u771f\u5bf9\u6297\u6027\u9a7e\u9a76\u89c6\u9891\uff0c\u4e14\u5bf9\u6297\u884c\u4e3a\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u3002"}}
{"id": "2505.16003", "pdf": "https://arxiv.org/pdf/2505.16003", "abs": "https://arxiv.org/abs/2505.16003", "authors": ["Roland Daynauth", "Christopher Clarke", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "title": "SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for\nevaluating language models. Although several calibration techniques have been\nproposed to better align these evaluators with human judgment, prior studies\nfocus primarily on narrow, well-structured benchmarks. As a result, it remains\nunclear whether such calibrations generalize to real-world, open-ended tasks.\n  In this work, we show that SOTA calibrated evaluators often fail in these\nsettings, exhibiting weak or even negative correlation with human judgments. To\naddress this, we propose SLMEval, a novel and efficient calibration method\nbased on entropy maximization over a small amount of human preference data. By\nestimating a latent distribution over model quality and reweighting evaluator\nscores accordingly, SLMEval achieves strong correlation with human evaluations\nacross two real-world production use cases and the public benchmark. For\nexample, on one such task, SLMEval achieves a Spearman correlation of 0.57 with\nhuman judgments, while G-Eval yields a negative correlation. In addition,\nSLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated\nevaluators such as G-eval.", "AI": {"tldr": "SLMEval\u662f\u4e00\u79cd\u57fa\u4e8e\u71b5\u6700\u5927\u5316\u7684\u65b0\u578b\u6821\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u6280\u672f\u5728\u5f00\u653e\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u5f31\u751a\u81f3\u8d1f\u76f8\u5173\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSLMEval\uff0c\u901a\u8fc7\u71b5\u6700\u5927\u5316\u548c\u5c0f\u89c4\u6a21\u4eba\u7c7b\u504f\u597d\u6570\u636e\u4f30\u8ba1\u6f5c\u5728\u8d28\u91cf\u5206\u5e03\uff0c\u91cd\u65b0\u52a0\u6743\u8bc4\u4f30\u5206\u6570\u3002", "result": "SLMEval\u5728\u771f\u5b9e\u4efb\u52a1\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347\uff08\u5982Spearman\u76f8\u5173\u7cfb\u65700.57\uff09\uff0c\u4e14\u6210\u672c\u964d\u4f4e5-30\u500d\u3002", "conclusion": "SLMEval\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6821\u51c6\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u4efb\u52a1\u8bc4\u4f30\u3002"}}
{"id": "2505.15928", "pdf": "https://arxiv.org/pdf/2505.15928", "abs": "https://arxiv.org/abs/2505.15928", "authors": ["Tony Montes", "Fernando Lozano"], "title": "ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation", "categories": ["cs.CV", "cs.CL", "I.4.8"], "comment": null, "summary": "Recent advancements in Video Question Answering (VideoQA) have introduced\nLLM-based agents, modular frameworks, and procedural solutions, yielding\npromising results. These systems use dynamic agents and memory-based mechanisms\nto break down complex tasks and refine answers. However, significant\nimprovements remain in tracking objects for grounding over time and\ndecision-making based on reasoning to better align object references with\nlanguage model outputs, as newer models get better at both tasks. This work\npresents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)\nthat combines a Chain-of-Thought framework with grounding reasoning alongside\nYOLO-World to enhance object tracking and alignment. This approach establishes\na new state-of-the-art in VideoQA and Video Understanding, showing enhanced\nperformance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also\nenables cross-checking of grounding timeframes, improving accuracy and\nproviding valuable support for verification and increased output reliability\nacross multiple video domains. The code is available at\nhttps://github.com/t-montes/viqagent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Chain-of-Thought\u6846\u67b6\u548cYOLO-World\u7684LLM-brained\u4ee3\u7406\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u89c6\u9891\u95ee\u7b54\uff08VideoQA\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u8ddf\u8e2a\u548c\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVideoQA\u7cfb\u7edf\u5728\u5bf9\u8c61\u8ddf\u8e2a\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u51b3\u7b56\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8c61\u5f15\u7528\u4e0e\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u5bf9\u9f50\u4e0a\u3002", "method": "\u91c7\u7528LLM-brained\u4ee3\u7406\uff0c\u7ed3\u5408Chain-of-Thought\u6846\u67b6\u548cYOLO-World\uff0c\u589e\u5f3a\u5bf9\u8c61\u8ddf\u8e2a\u548c\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5728NExT-QA\u3001iVQA\u548cActivityNet-QA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u8f93\u51fa\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aVideoQA\u548c\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8de8\u9886\u57df\u9a8c\u8bc1\u548c\u53ef\u9760\u6027\u63d0\u5347\u3002"}}
{"id": "2505.16008", "pdf": "https://arxiv.org/pdf/2505.16008", "abs": "https://arxiv.org/abs/2505.16008", "authors": ["Wenrui Yu", "Yiyi Chen", "Johannes Bjerva", "Sokol Kosta", "Qiongxiu Li"], "title": "LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "We propose LAGO - Language Similarity-Aware Graph Optimization - a novel\napproach for few-shot cross-lingual embedding inversion attacks, addressing\ncritical privacy vulnerabilities in multilingual NLP systems. Unlike prior work\nin embedding inversion attacks that treat languages independently, LAGO\nexplicitly models linguistic relationships through a graph-based constrained\ndistributed optimization framework. By integrating syntactic and lexical\nsimilarity as edge constraints, our method enables collaborative parameter\nlearning across related languages. Theoretically, we show this formulation\ngeneralizes prior approaches, such as ALGEN, which emerges as a special case\nwhen similarity constraints are relaxed. Our framework uniquely combines\nFrobenius-norm regularization with linear inequality or total variation\nconstraints, ensuring robust alignment of cross-lingual embedding spaces even\nwith extremely limited data (as few as 10 samples per language). Extensive\nexperiments across multiple languages and embedding models demonstrate that\nLAGO substantially improves the transferability of attacks with 10-20% increase\nin Rouge-L score over baselines. This work establishes language similarity as a\ncritical factor in inversion attack transferability, urging renewed focus on\nlanguage-aware privacy-preserving multilingual embeddings.", "AI": {"tldr": "LAGO\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u76f8\u4f3c\u6027\u7684\u56fe\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u8de8\u8bed\u8a00\u5d4c\u5165\u53cd\u8f6c\u653b\u51fb\uff0c\u663e\u8457\u63d0\u5347\u653b\u51fb\u7684\u8fc1\u79fb\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00NLP\u7cfb\u7edf\u4e2d\u56e0\u8bed\u8a00\u72ec\u7acb\u6027\u5047\u8bbe\u5bfc\u81f4\u7684\u9690\u79c1\u6f0f\u6d1e\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56fe\u7ea6\u675f\u5206\u5e03\u5f0f\u4f18\u5316\u6846\u67b6\u5efa\u6a21\u8bed\u8a00\u5173\u7cfb\uff0c\u7ed3\u5408\u53e5\u6cd5\u548c\u8bcd\u6c47\u76f8\u4f3c\u6027\u4f5c\u4e3a\u8fb9\u7ea6\u675f\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u53c2\u6570\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLAGO\u5728\u6781\u5c11\u91cf\u6570\u636e\u4e0b\uff08\u6bcf\u79cd\u8bed\u8a00\u4ec510\u6837\u672c\uff09\u663e\u8457\u63d0\u5347\u653b\u51fb\u8fc1\u79fb\u6027\uff0cRouge-L\u5206\u6570\u63d0\u9ad810-20%\u3002", "conclusion": "\u8bed\u8a00\u76f8\u4f3c\u6027\u662f\u53cd\u8f6c\u653b\u51fb\u8fc1\u79fb\u6027\u7684\u5173\u952e\u56e0\u7d20\uff0c\u547c\u5401\u5173\u6ce8\u8bed\u8a00\u611f\u77e5\u7684\u9690\u79c1\u4fdd\u62a4\u591a\u8bed\u8a00\u5d4c\u5165\u65b9\u6cd5\u3002"}}
{"id": "2505.15952", "pdf": "https://arxiv.org/pdf/2505.15952", "abs": "https://arxiv.org/abs/2505.15952", "authors": ["Mohammad Reza Taesiri", "Abhijay Ghildyal", "Saman Zadtootaghaj", "Nabajeet Barman", "Cor-Paul Bezemer"], "title": "VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance", "categories": ["cs.CV", "cs.AI"], "comment": "Project website with code and data:\n  https://asgaardlab.github.io/videogameqa-bench/", "summary": "With video games now generating the highest revenues in the entertainment\nindustry, optimizing game development workflows has become essential for the\nsector's sustained growth. Recent advancements in Vision-Language Models (VLMs)\noffer considerable potential to automate and enhance various aspects of game\ndevelopment, particularly Quality Assurance (QA), which remains one of the\nindustry's most labor-intensive processes with limited automation options. To\naccurately evaluate the performance of VLMs in video game QA tasks and\ndetermine their effectiveness in handling real-world scenarios, there is a\nclear need for standardized benchmarks, as existing benchmarks are insufficient\nto address the specific requirements of this domain. To bridge this gap, we\nintroduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array\nof game QA activities, including visual unit testing, visual regression\ntesting, needle-in-a-haystack tasks, glitch detection, and bug report\ngeneration for both images and videos of various games. Code and data are\navailable at: https://asgaardlab.github.io/videogameqa-bench/", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86VideoGameQA-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6e38\u620fQA\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u7efc\u5408\u57fa\u51c6\u3002", "motivation": "\u6e38\u620f\u884c\u4e1a\u6536\u5165\u9ad8\uff0c\u4f46QA\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\u4e14\u81ea\u52a8\u5316\u9009\u9879\u6709\u9650\uff0c\u9700\u8981\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30VLMs\u5728\u6e38\u620fQA\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faVideoGameQA-Bench\uff0c\u6db5\u76d6\u591a\u79cd\u6e38\u620fQA\u4efb\u52a1\uff0c\u5982\u89c6\u89c9\u5355\u5143\u6d4b\u8bd5\u3001\u56de\u5f52\u6d4b\u8bd5\u3001\u6545\u969c\u68c0\u6d4b\u7b49\u3002", "result": "\u57fa\u51c6\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u56fe\u50cf\u548c\u89c6\u9891\u7684\u6e38\u620fQA\u4efb\u52a1\u3002", "conclusion": "VideoGameQA-Bench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\uff0c\u4e3a\u6e38\u620fQA\u9886\u57df\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\u3002"}}
{"id": "2505.16014", "pdf": "https://arxiv.org/pdf/2505.16014", "abs": "https://arxiv.org/abs/2505.16014", "authors": ["Yash Saxena", "Anpur Padia", "Mandar S Chaudhary", "Kalpa Gunaratna", "Srinivasan Parthasarathy", "Manas Gaur"], "title": "Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains", "categories": ["cs.CL"], "comment": null, "summary": "Traditional Retrieval-Augmented Generation (RAG) pipelines rely on\nsimilarity-based retrieval and re-ranking, which depend on heuristics such as\ntop-k, and lack explainability, interpretability, and robustness against\nadversarial content. To address this gap, we propose a novel method METEORA\nthat replaces re-ranking in RAG with a rationale-driven selection approach.\nMETEORA operates in two stages. First, a general-purpose LLM is\npreference-tuned to generate rationales conditioned on the input query using\ndirect preference optimization. These rationales guide the evidence chunk\nselection engine, which selects relevant chunks in three stages: pairing\nindividual rationales with corresponding retrieved chunks for local relevance,\nglobal selection with elbow detection for adaptive cutoff, and context\nexpansion via neighboring chunks. This process eliminates the need for top-k\nheuristics. The rationales are also used for consistency check using a Verifier\nLLM to detect and filter poisoned or misleading content for safe generation.\nThe framework provides explainable and interpretable evidence flow by using\nrationales consistently across both selection and verification. Our evaluation\nacross six datasets spanning legal, financial, and academic research domains\nshows that METEORA improves generation accuracy by 33.34% while using\napproximately 50% fewer chunks than state-of-the-art re-ranking methods. In\nadversarial settings, METEORA significantly improves the F1 score from 0.10 to\n0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating\nstrong resilience to poisoning attacks. Code available at:\nhttps://anonymous.4open.science/r/METEORA-DC46/README.md", "AI": {"tldr": "METEORA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u7406\u6027\u7684\u9009\u62e9\u66ff\u4ee3\u4f20\u7edf\u91cd\u6392\u5e8f\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u51c6\u786e\u6027\u3001\u89e3\u91ca\u6027\u548c\u5bf9\u6297\u6027\u5185\u5bb9\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfRAG\u4f9d\u8d56\u76f8\u4f3c\u6027\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u5bf9\u6297\u6027\u5185\u5bb9\u7684\u9c81\u68d2\u6027\u3002", "method": "METEORA\u5206\u4e24\u9636\u6bb5\uff1a1) \u4f7f\u7528\u504f\u597d\u8c03\u6574\u7684LLM\u751f\u6210\u7406\u6027\uff1b2) \u57fa\u4e8e\u7406\u6027\u9009\u62e9\u8bc1\u636e\u5757\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668\u8fc7\u6ee4\u8bef\u5bfc\u5185\u5bb9\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cMETEORA\u751f\u6210\u51c6\u786e\u6027\u63d0\u9ad833.34%\uff0c\u4f7f\u7528\u5757\u6570\u51cf\u5c1150%\uff0c\u5bf9\u6297\u6027F1\u5206\u6570\u4ece0.10\u63d0\u5347\u81f30.44\u3002", "conclusion": "METEORA\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.15961", "pdf": "https://arxiv.org/pdf/2505.15961", "abs": "https://arxiv.org/abs/2505.15961", "authors": ["Gabby Litterio", "Juan-David Lizarazo-Ferro", "Pedro Felzenszwalb", "Rashid Zia"], "title": "Super-Resolution with Structured Motion", "categories": ["cs.CV", "I.4.1; I.4.3"], "comment": null, "summary": "We consider the limits of super-resolution using imaging constraints. Due to\nvarious theoretical and practical limitations, reconstruction-based methods\nhave been largely restricted to small increases in resolution. In addition,\nmotion-blur is usually seen as a nuisance that impedes super-resolution. We\nshow that by using high-precision motion information, sparse image priors, and\nconvex optimization, it is possible to increase resolution by large factors. A\nkey operation in super-resolution is deconvolution with a box. In general,\nconvolution with a box is not invertible. However, we obtain perfect\nreconstructions of sparse signals using convex optimization. We also show that\nmotion blur can be helpful for super-resolution. We demonstrate that using\npseudo-random motion it is possible to reconstruct a high-resolution target\nusing a single low-resolution image. We present numerical experiments with\nsimulated data and results with real data captured by a camera mounted on a\ncomputer controlled stage.", "AI": {"tldr": "\u901a\u8fc7\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u4fe1\u606f\u3001\u7a00\u758f\u56fe\u50cf\u5148\u9a8c\u548c\u51f8\u4f18\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5927\u5e45\u63d0\u9ad8\u5206\u8fa8\u7387\uff0c\u5e76\u8bc1\u660e\u8fd0\u52a8\u6a21\u7cca\u6709\u52a9\u4e8e\u8d85\u5206\u8fa8\u7387\u3002", "motivation": "\u63a2\u8ba8\u8d85\u5206\u8fa8\u7387\u7684\u7406\u8bba\u9650\u5236\uff0c\u5c24\u5176\u662f\u8fd0\u52a8\u6a21\u7cca\u5bf9\u5206\u8fa8\u7387\u63d0\u5347\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u4fe1\u606f\u3001\u7a00\u758f\u56fe\u50cf\u5148\u9a8c\u548c\u51f8\u4f18\u5316\u6280\u672f\uff0c\u7ed3\u5408\u4f2a\u968f\u673a\u8fd0\u52a8\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u5f20\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u76ee\u6807\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8fd0\u52a8\u6a21\u7cca\u53ef\u4ee5\u4f5c\u4e3a\u8d85\u5206\u8fa8\u7387\u7684\u52a9\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u65b9\u6cd5\u5b9e\u73b0\u5927\u5e45\u5206\u8fa8\u7387\u63d0\u5347\u3002"}}
{"id": "2505.16022", "pdf": "https://arxiv.org/pdf/2505.16022", "abs": "https://arxiv.org/abs/2505.16022", "authors": ["Wei Liu", "Siya Qi", "Xinyu Wang", "Chen Qian", "Yali Du", "Yulan He"], "title": "NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 5 tables, 12 figures", "summary": "Recent advances such as DeepSeek R1-Zero highlight the effectiveness of\nincentive training, a reinforcement learning paradigm that computes rewards\nsolely based on the final answer part of a language model's output, thereby\nencouraging the generation of intermediate reasoning steps. However, these\nmethods fundamentally rely on external verifiers, which limits their\napplicability to domains like mathematics and coding where such verifiers are\nreadily available. Although reward models can serve as verifiers, they require\nhigh-quality annotated data and are costly to train. In this work, we propose\nNOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning\nframework that requires only standard supervised fine-tuning data with no need\nfor an external verifier. NOVER enables incentive training across a wide range\nof text-to-text tasks and outperforms the model of the same size distilled from\nlarge reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the\nflexibility of NOVER enables new possibilities for optimizing large language\nmodels, such as inverse incentive training.", "AI": {"tldr": "NOVER\u662f\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u9700\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u6570\u636e\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u6587\u672c\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\uff0c\u4e14\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faNOVER\u6846\u67b6\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\uff0c\u4ec5\u9700\u6807\u51c6\u76d1\u7763\u5fae\u8c03\u6570\u636e\u3002", "result": "NOVER\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\uff08\u5982DeepSeek R1 671B\uff097.7%\u3002", "conclusion": "NOVER\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u5982\u9006\u5411\u6fc0\u52b1\u8bad\u7ec3\u3002"}}
{"id": "2505.15963", "pdf": "https://arxiv.org/pdf/2505.15963", "abs": "https://arxiv.org/abs/2505.15963", "authors": ["Shujun Liu", "Siyuan Wang", "Zejun Li", "Jianxiang Wang", "Cheng Zeng", "Zhongyu Wei"], "title": "OViP: Online Vision-Language Preference Learning", "categories": ["cs.CV", "cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Large vision-language models (LVLMs) remain vulnerable to hallucination,\noften generating content misaligned with visual inputs. While recent approaches\nadvance multi-modal Direct Preference Optimization (DPO) to mitigate\nhallucination, they typically rely on predefined or randomly edited negative\nsamples that fail to reflect actual model errors, limiting training efficacy.\nIn this work, we propose an Online Vision-language Preference Learning (OViP)\nframework that dynamically constructs contrastive training data based on the\nmodel's own hallucinated outputs. By identifying semantic differences between\nsampled response pairs and synthesizing negative images using a diffusion\nmodel, OViP generates more relevant supervision signals in real time. This\nfailure-driven training enables adaptive alignment of both textual and visual\npreferences. Moreover, we refine existing evaluation protocols to better\ncapture the trade-off between hallucination suppression and expressiveness.\nExperiments on hallucination and general benchmarks demonstrate that OViP\neffectively reduces hallucinations while preserving core multi-modal\ncapabilities.", "AI": {"tldr": "OViP\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5bf9\u6bd4\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u6a21\u6001\u80fd\u529b\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u6613\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u7b26\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u968f\u673a\u7f16\u8f91\u7684\u8d1f\u6837\u672c\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u89c6\u89c9\u8bed\u8a00\u504f\u597d\u5b66\u4e60\uff08OViP\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u6a21\u578b\u81ea\u8eab\u5e7b\u89c9\u8f93\u51fa\u52a8\u6001\u6784\u5efa\u5bf9\u6bd4\u6570\u636e\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5408\u6210\u8d1f\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOViP\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u591a\u6a21\u6001\u80fd\u529b\u3002", "conclusion": "OViP\u901a\u8fc7\u5931\u8d25\u9a71\u52a8\u7684\u8bad\u7ec3\u548c\u52a8\u6001\u6570\u636e\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2505.16023", "pdf": "https://arxiv.org/pdf/2505.16023", "abs": "https://arxiv.org/abs/2505.16023", "authors": ["Sheshera Mysore", "Debarati Das", "Hancheng Cao", "Bahareh Sarrafzadeh"], "title": "Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild", "categories": ["cs.CL", "cs.HC"], "comment": "Pre-print under-review", "summary": "As large language models (LLMs) are used in complex writing workflows, users\nengage in multi-turn interactions to steer generations to better fit their\nneeds. Rather than passively accepting output, users actively refine, explore,\nand co-construct text. We conduct a large-scale analysis of this collaborative\nbehavior for users engaged in writing tasks in the wild with two popular AI\nassistants, Bing Copilot and WildChat. Our analysis goes beyond simple task\nclassification or satisfaction estimation common in prior work and instead\ncharacterizes how users interact with LLMs through the course of a session. We\nidentify prototypical behaviors in how users interact with LLMs in prompts\nfollowing their original request. We refer to these as Prototypical Human-AI\nCollaboration Behaviors (PATHs) and find that a small group of PATHs explain a\nmajority of the variation seen in user-LLM interaction. These PATHs span users\nrevising intents, exploring texts, posing questions, adjusting style or\ninjecting new content. Next, we find statistically significant correlations\nbetween specific writing intents and PATHs, revealing how users' intents shape\ntheir collaboration behaviors. We conclude by discussing the implications of\nour findings on LLM alignment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7528\u6237\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u534f\u4f5c\u884c\u4e3a\uff0c\u63d0\u51fa\u4e86\u539f\u578b\u4eba\u673a\u534f\u4f5c\u884c\u4e3a\uff08PATHs\uff09\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u884c\u4e3a\u4e0e\u7528\u6237\u5199\u4f5c\u610f\u56fe\u7684\u5173\u8054\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u5728\u590d\u6742\u5199\u4f5c\u4efb\u52a1\u4e2d\u5982\u4f55\u4e3b\u52a8\u5f15\u5bfcLLM\u751f\u6210\u5185\u5bb9\uff0c\u800c\u975e\u88ab\u52a8\u63a5\u53d7\u8f93\u51fa\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7528\u6237\u4e0eBing Copilot\u548cWildChat\u7684\u4ea4\u4e92\u6570\u636e\uff0c\u8bc6\u522b\u539f\u578b\u534f\u4f5c\u884c\u4e3a\uff08PATHs\uff09\uff0c\u5e76\u7edf\u8ba1\u5176\u4e0e\u5199\u4f5c\u610f\u56fe\u7684\u5173\u8054\u3002", "result": "\u53d1\u73b0\u5c11\u6570PATHs\u80fd\u89e3\u91ca\u5927\u90e8\u5206\u4ea4\u4e92\u884c\u4e3a\uff0c\u4e14\u7279\u5b9a\u5199\u4f5c\u610f\u56fe\u4e0e\u7279\u5b9aPATHs\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9LLM\u7684\u4f18\u5316\u548c\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2505.15966", "pdf": "https://arxiv.org/pdf/2505.15966", "abs": "https://arxiv.org/abs/2505.15966", "authors": ["Alex Su", "Haozhe Wang", "Weimin Ren", "Fangzhen Lin", "Wenhu Chen"], "title": "Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Haozhe Wang and Alex Su contributed equally and listed alphabetically", "summary": "Chain-of-thought reasoning has significantly improved the performance of\nLarge Language Models (LLMs) across various domains. However, this reasoning\nprocess has been confined exclusively to textual space, limiting its\neffectiveness in visually intensive tasks. To address this limitation, we\nintroduce the concept of reasoning in the pixel-space. Within this novel\nframework, Vision-Language Models (VLMs) are equipped with a suite of visual\nreasoning operations, such as zoom-in and select-frame. These operations enable\nVLMs to directly inspect, interrogate, and infer from visual evidences, thereby\nenhancing reasoning fidelity for visual tasks. Cultivating such pixel-space\nreasoning capabilities in VLMs presents notable challenges, including the\nmodel's initially imbalanced competence and its reluctance to adopt the newly\nintroduced pixel-space operations. We address these challenges through a\ntwo-phase training approach. The first phase employs instruction tuning on\nsynthesized reasoning traces to familiarize the model with the novel visual\noperations. Following this, a reinforcement learning (RL) phase leverages a\ncuriosity-driven reward scheme to balance exploration between pixel-space\nreasoning and textual reasoning. With these visual operations, VLMs can\ninteract with complex visual inputs, such as information-rich images or videos\nto proactively gather necessary information. We demonstrate that this approach\nsignificantly improves VLM performance across diverse visual reasoning\nbenchmarks. Our 7B model, \\model, achieves 84\\% on V* bench, 74\\% on\nTallyQA-Complex, and 84\\% on InfographicsVQA, marking the highest accuracy\nachieved by any open-source model to date. These results highlight the\nimportance of pixel-space reasoning and the effectiveness of our framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u50cf\u7d20\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u64cd\u4f5c\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u4ec5\u9650\u4e8e\u6587\u672c\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u63a8\u7406\u80fd\u529b\u5230\u50cf\u7d20\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u6307\u4ee4\u8c03\u4f18\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165\u89c6\u89c9\u64cd\u4f5c\uff08\u5982\u653e\u5927\u3001\u9009\u62e9\u5e27\uff09\u4ee5\u589e\u5f3aVLM\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "result": "7B\u6a21\u578b\u5728\u591a\u4e2a\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982V* bench\uff0884%\uff09\u3001TallyQA-Complex\uff0874%\uff09\u548cInfographicsVQA\uff0884%\uff09\u3002", "conclusion": "\u50cf\u7d20\u7a7a\u95f4\u63a8\u7406\u5bf9\u63d0\u5347VLM\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u63a8\u7406\u7684\u6311\u6218\u3002"}}
{"id": "2505.16036", "pdf": "https://arxiv.org/pdf/2505.16036", "abs": "https://arxiv.org/abs/2505.16036", "authors": ["Burak Erin\u00e7 \u00c7etin", "Y\u0131ld\u0131r\u0131m \u00d6zen", "Elif Naz Demiry\u0131lmaz", "Kaan Eng\u00fcr", "Cagri Toraman"], "title": "OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Generative large language models present significant potential but also raise\ncritical ethical concerns. Most studies focus on narrow ethical dimensions, and\nalso limited diversity of languages and models. To address these gaps, we\nconduct a broad ethical evaluation of 29 recent open-source large language\nmodels using a novel data collection including four ethical aspects:\nRobustness, reliability, safety, and fairness. We analyze model behavior in\nboth a commonly used language, English, and a low-resource language, Turkish.\nOur aim is to provide a comprehensive ethical assessment and guide safer model\ndevelopment by filling existing gaps in evaluation breadth, language coverage,\nand model diversity. Our experimental results, based on LLM-as-a-Judge, reveal\nthat optimization efforts for many open-source models appear to have\nprioritized safety and fairness, and demonstrated good robustness while\nreliability remains a concern. We demonstrate that ethical evaluation can be\neffectively conducted independently of the language used. In addition, models\nwith larger parameter counts tend to exhibit better ethical performance, with\nGemma and Qwen models demonstrating the most ethical behavior among those\nevaluated.", "AI": {"tldr": "\u8bba\u6587\u5bf929\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4f26\u7406\u8bc4\u4f30\uff0c\u6db5\u76d6\u9c81\u68d2\u6027\u3001\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\uff0c\u586b\u8865\u4e86\u8bc4\u4f30\u5e7f\u5ea6\u3001\u8bed\u8a00\u8986\u76d6\u548c\u6a21\u578b\u591a\u6837\u6027\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u72ed\u7a84\u7684\u4f26\u7406\u7ef4\u5ea6\uff0c\u4e14\u8bed\u8a00\u548c\u6a21\u578b\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9700\u66f4\u5168\u9762\u7684\u4f26\u7406\u8bc4\u4f30\u4ee5\u6307\u5bfc\u66f4\u5b89\u5168\u7684\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u4f7f\u7528LLM-as-a-Judge\u65b9\u6cd5\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u82f1\u8bed\u548c\u571f\u8033\u5176\u8bed\u4e2d\u7684\u884c\u4e3a\uff0c\u5206\u6790\u56db\u4e2a\u4f26\u7406\u65b9\u9762\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u5728\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u9c81\u68d2\u6027\u826f\u597d\uff0c\u4f46\u53ef\u9760\u6027\u5b58\u7591\uff1b\u53c2\u6570\u66f4\u591a\u7684\u6a21\u578b\u4f26\u7406\u8868\u73b0\u66f4\u4f18\uff0cGemma\u548cQwen\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u4f26\u7406\u8bc4\u4f30\u53ef\u72ec\u7acb\u4e8e\u8bed\u8a00\u8fdb\u884c\uff0c\u5927\u53c2\u6570\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2505.15970", "pdf": "https://arxiv.org/pdf/2505.15970", "abs": "https://arxiv.org/abs/2505.15970", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.LG"], "comment": "(Oral) CVPR 2025 Workshop on Mechanistic Interpretability for Vision.\n  Authors 1 and 2 contributed equally", "summary": "The ImageNet hierarchy provides a structured taxonomy of object categories,\noffering a valuable lens through which to analyze the representations learned\nby deep vision models. In this work, we conduct a comprehensive analysis of how\nvision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders\n(SAEs) to probe their internal representations. SAEs have been widely used as\nan explanation tool for large language models (LLMs), where they enable the\ndiscovery of semantically meaningful features. Here, we extend their use to\nvision models to investigate whether learned representations align with the\nontological structure defined by the ImageNet taxonomy. Our results show that\nSAEs uncover hierarchical relationships in model activations, revealing an\nimplicit encoding of taxonomic structure. We analyze the consistency of these\nrepresentations across different layers of the popular vision foundation model\nDINOv2 and provide insights into how deep vision models internalize\nhierarchical category information by increasing information in the class token\nthrough each layer. Our study establishes a framework for systematic\nhierarchical analysis of vision model representations and highlights the\npotential of SAEs as a tool for probing semantic structure in deep networks.", "AI": {"tldr": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u6790\u89c6\u89c9\u6a21\u578b\u5982\u4f55\u7f16\u7801ImageNet\u5c42\u6b21\u7ed3\u6784\uff0c\u53d1\u73b0\u6a21\u578b\u6fc0\u6d3b\u4e2d\u5b58\u5728\u9690\u542b\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u4e86DINOv2\u6a21\u578b\u4e2d\u4e0d\u540c\u5c42\u7684\u8868\u793a\u4e00\u81f4\u6027\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u6a21\u578b\u662f\u5426\u80fd\u591f\u5b66\u4e60\u5e76\u7f16\u7801ImageNet\u5206\u7c7b\u5b66\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u4ee5\u53ca\u5982\u4f55\u5229\u7528SAEs\u63ed\u793a\u8fd9\u79cd\u7ed3\u6784\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5206\u6790\u89c6\u89c9\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u7279\u522b\u5173\u6ce8DINOv2\u6a21\u578b\u7684\u4e0d\u540c\u5c42\u3002", "result": "SAEs\u63ed\u793a\u4e86\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u5c42\u6b21\u5173\u7cfb\uff0c\u8868\u660e\u89c6\u89c9\u6a21\u578b\u9690\u542b\u5730\u7f16\u7801\u4e86\u5206\u7c7b\u5b66\u7ed3\u6784\uff0c\u4e14\u7c7b\u522b\u4fe1\u606f\u901a\u8fc7\u6bcf\u4e00\u5c42\u7684\u7c7b\u4ee4\u724c\u9010\u6b65\u589e\u52a0\u3002", "conclusion": "\u7814\u7a76\u4e3a\u89c6\u89c9\u6a21\u578b\u8868\u793a\u7684\u5c42\u6b21\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86SAEs\u5728\u63ed\u793a\u6df1\u5ea6\u7f51\u7edc\u8bed\u4e49\u7ed3\u6784\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16061", "pdf": "https://arxiv.org/pdf/2505.16061", "abs": "https://arxiv.org/abs/2505.16061", "authors": ["Yu Zhang"], "title": "Internal and External Impacts of Natural Language Processing Papers", "categories": ["cs.CL", "cs.DL"], "comment": "7 pages; Accepted to ACL 2025", "summary": "We investigate the impacts of NLP research published in top-tier conferences\n(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from\nresearch articles and external sources such as patents, media, and policy\ndocuments, we examine how different NLP topics are consumed both within the\nacademic community and by the broader public. Our findings reveal that language\nmodeling has the widest internal and external influence, while linguistic\nfoundations have lower impacts. We also observe that internal and external\nimpacts generally align, but topics like ethics, bias, and fairness show\nsignificant attention in policy documents with much fewer academic citations.\nAdditionally, external domains exhibit distinct preferences, with patents\nfocusing on practical NLP applications and media and policy documents engaging\nmore with the societal implications of NLP models.", "AI": {"tldr": "\u5206\u6790\u4e861979\u81f32024\u5e74\u9876\u7ea7NLP\u4f1a\u8bae\u8bba\u6587\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u754c\u548c\u516c\u4f17\u4e2d\u5f71\u54cd\u6700\u5e7f\uff0c\u800c\u8bed\u8a00\u5b66\u57fa\u7840\u5f71\u54cd\u8f83\u4f4e\u3002\u4f26\u7406\u548c\u516c\u5e73\u8bdd\u9898\u5728\u653f\u7b56\u6587\u4ef6\u4e2d\u53d7\u5173\u6ce8\u4f46\u5b66\u672f\u5f15\u7528\u8f83\u5c11\u3002", "motivation": "\u7814\u7a76NLP\u9886\u57df\u7684\u7814\u7a76\u6210\u679c\u5982\u4f55\u88ab\u5b66\u672f\u754c\u548c\u66f4\u5e7f\u6cdb\u7684\u793e\u4f1a\u9886\u57df\uff08\u5982\u4e13\u5229\u3001\u5a92\u4f53\u3001\u653f\u7b56\uff09\u6240\u91c7\u7528\u548c\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7814\u7a76\u6587\u7ae0\u548c\u5916\u90e8\u6765\u6e90\uff08\u4e13\u5229\u3001\u5a92\u4f53\u3001\u653f\u7b56\u6587\u4ef6\uff09\u7684\u5f15\u7528\u6570\u636e\uff0c\u8bc4\u4f30\u4e0d\u540cNLP\u4e3b\u9898\u7684\u5f71\u54cd\u529b\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5f71\u54cd\u529b\u6700\u5e7f\uff0c\u8bed\u8a00\u5b66\u57fa\u7840\u5f71\u54cd\u8f83\u4f4e\uff1b\u4f26\u7406\u548c\u516c\u5e73\u8bdd\u9898\u5728\u653f\u7b56\u6587\u4ef6\u4e2d\u53d7\u5173\u6ce8\u4f46\u5b66\u672f\u5f15\u7528\u8f83\u5c11\uff1b\u5916\u90e8\u9886\u57df\u5bf9NLP\u7684\u5e94\u7528\u548c\u793e\u4f1a\u5f71\u54cd\u6709\u4e0d\u540c\u504f\u597d\u3002", "conclusion": "NLP\u7814\u7a76\u7684\u5f71\u54cd\u529b\u5728\u5b66\u672f\u754c\u548c\u5916\u90e8\u9886\u57df\u5b58\u5728\u5dee\u5f02\uff0c\u8bed\u8a00\u6a21\u578b\u548c\u4f26\u7406\u8bdd\u9898\u662f\u91cd\u70b9\u65b9\u5411\u3002"}}
{"id": "2505.15997", "pdf": "https://arxiv.org/pdf/2505.15997", "abs": "https://arxiv.org/abs/2505.15997", "authors": ["Mehran Zoravar", "Shadi Alijani", "Homayoun Najjaran"], "title": "Domain Adaptive Skin Lesion Classification via Conformal Ensemble of Vision Transformers", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "5 pages, 4 figures, conference (ccece 2025)", "summary": "Exploring the trustworthiness of deep learning models is crucial, especially\nin critical domains such as medical imaging decision support systems. Conformal\nprediction has emerged as a rigorous means of providing deep learning models\nwith reliable uncertainty estimates and safety guarantees. However, conformal\nprediction results face challenges due to the backbone model's struggles in\ndomain-shifted scenarios, such as variations in different sources. To aim this\nchallenge, this paper proposes a novel framework termed Conformal Ensemble of\nVision Transformers (CE-ViTs) designed to enhance image classification\nperformance by prioritizing domain adaptation and model robustness, while\naccounting for uncertainty. The proposed method leverages an ensemble of vision\ntransformer models in the backbone, trained on diverse datasets including\nHAM10000, Dermofit, and Skin Cancer ISIC datasets. This ensemble learning\napproach, calibrated through the combined mentioned datasets, aims to enhance\ndomain adaptation through conformal learning. Experimental results underscore\nthat the framework achieves a high coverage rate of 90.38\\%, representing an\nimprovement of 9.95\\% compared to the HAM10000 model. This indicates a strong\nlikelihood that the prediction set includes the true label compared to singular\nmodels. Ensemble learning in CE-ViTs significantly improves conformal\nprediction performance, increasing the average prediction set size for\nchallenging misclassified samples from 1.86 to 3.075.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCE-ViTs\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u53d8\u6362\u5668\u6a21\u578b\u548c\u96c6\u6210\u5b66\u4e60\uff0c\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u57df\u9002\u5e94\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u9762\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u5173\u952e\u9886\u57df\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u57df\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5b66\u4e60\u7684\u89c6\u89c9\u53d8\u6362\u5668\u6a21\u578b\uff0c\u7ed3\u5408HAM10000\u3001Dermofit\u548cISIC\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4fdd\u5f62\u5b66\u4e60\u589e\u5f3a\u57df\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6846\u67b6\u7684\u8986\u76d6\u7387\u8fbe\u523090.38%\uff0c\u6bd4\u5355\u4e00\u6a21\u578b\u63d0\u53479.95%\uff0c\u4e14\u5bf9\u96be\u5206\u7c7b\u6837\u672c\u7684\u9884\u6d4b\u96c6\u5927\u5c0f\u4ece1.86\u589e\u52a0\u52303.075\u3002", "conclusion": "CE-ViTs\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u548c\u4fdd\u5f62\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u57df\u9002\u5e94\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5173\u952e\u9886\u57df\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u3002"}}
{"id": "2505.16078", "pdf": "https://arxiv.org/pdf/2505.16078", "abs": "https://arxiv.org/abs/2505.16078", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "title": "Small Language Models in the Real World: Insights from Industrial Text Classification", "categories": ["cs.CL"], "comment": null, "summary": "With the emergence of ChatGPT, Transformer models have significantly advanced\ntext classification and related tasks. Decoder-only models such as Llama\nexhibit strong performance and flexibility, yet they suffer from inefficiency\non inference due to token-by-token generation, and their effectiveness in text\nclassification tasks heavily depends on prompt quality. Moreover, their\nsubstantial GPU resource requirements often limit widespread adoption. Thus,\nthe question of whether smaller language models are capable of effectively\nhandling text classification tasks emerges as a topic of significant interest.\nHowever, the selection of appropriate models and methodologies remains largely\nunderexplored. In this paper, we conduct a comprehensive evaluation of prompt\nengineering and supervised fine-tuning methods for transformer-based text\nclassification. Specifically, we focus on practical industrial scenarios,\nincluding email classification, legal document categorization, and the\nclassification of extremely long academic texts. We examine the strengths and\nlimitations of smaller models, with particular attention to both their\nperformance and their efficiency in Video Random-Access Memory (VRAM)\nutilization, thereby providing valuable insights for the local deployment and\napplication of compact models in industrial settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u548c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49Transformer\u6a21\u578b\u7684\u5174\u8d77\uff0c\u89e3\u7801\u5668\u6a21\u578b\uff08\u5982Llama\uff09\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a8\u7406\u6548\u7387\u4f4e\u3001\u4f9d\u8d56\u63d0\u793a\u8d28\u91cf\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u56e0\u6b64\u7814\u7a76\u5c0f\u578b\u6a21\u578b\u7684\u6709\u6548\u6027\u6210\u4e3a\u91cd\u8981\u8bfe\u9898\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u63d0\u793a\u5de5\u7a0b\u548c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u5de5\u4e1a\u573a\u666f\uff08\u5982\u90ae\u4ef6\u5206\u7c7b\u3001\u6cd5\u5f8b\u6587\u6863\u5206\u7c7b\u548c\u957f\u6587\u672c\u5206\u7c7b\uff09\uff0c\u5206\u6790\u5c0f\u578b\u6a21\u578b\u7684\u6027\u80fd\u4e0eVRAM\u6548\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c0f\u578b\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e14\u8d44\u6e90\u5229\u7528\u7387\u9ad8\uff0c\u9002\u5408\u672c\u5730\u90e8\u7f72\u3002", "conclusion": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u6a21\u578b\u9009\u62e9\u548c\u65b9\u6cd5\u4f18\u5316\u3002"}}
{"id": "2505.16001", "pdf": "https://arxiv.org/pdf/2505.16001", "abs": "https://arxiv.org/abs/2505.16001", "authors": ["Qiang Zhu", "Kuan Lu", "Menghao Huo", "Yuxiao Li"], "title": "Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Image-to-image translation aims to learn a mapping between a source and a\ntarget domain, enabling tasks such as style transfer, appearance\ntransformation, and domain adaptation. In this work, we explore a\ndiffusion-based framework for image-to-image translation by adapting Diffusion\nTransformers (DiT), which combine the denoising capabilities of diffusion\nmodels with the global modeling power of transformers. To guide the translation\nprocess, we condition the model on image embeddings extracted from a\npre-trained CLIP encoder, allowing for fine-grained and structurally consistent\ntranslations without relying on text or class labels. We incorporate both a\nCLIP similarity loss to enforce semantic consistency and an LPIPS perceptual\nloss to enhance visual fidelity during training. We validate our approach on\ntwo benchmark datasets: face2comics, which translates real human faces to\ncomic-style illustrations, and edges2shoes, which translates edge maps to\nrealistic shoe images. Experimental results demonstrate that DiT, combined with\nCLIP-based conditioning and perceptual similarity objectives, achieves\nhigh-quality, semantically faithful translations, offering a promising\nalternative to GAN-based models for paired image-to-image translation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\uff0c\u7ed3\u5408CLIP\u7f16\u7801\u5668\u5f15\u5bfc\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u8f6c\u6362\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cTransformer\u7684\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684GAN\u6a21\u578b\uff0c\u540c\u65f6\u5229\u7528CLIP\u7f16\u7801\u5668\u5b9e\u73b0\u65e0\u9700\u6587\u672c\u6216\u6807\u7b7e\u7684\u7ec6\u7c92\u5ea6\u7ffb\u8bd1\u3002", "method": "\u91c7\u7528Diffusion Transformers\uff08DiT\uff09\u6846\u67b6\uff0c\u7ed3\u5408CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u5e76\u4f7f\u7528CLIP\u76f8\u4f3c\u6027\u635f\u5931\u548cLPIPS\u611f\u77e5\u635f\u5931\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728face2comics\u548cedges2shoes\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4e14\u8bed\u4e49\u4e00\u81f4\u7684\u56fe\u50cf\u7ffb\u8bd1\u3002", "conclusion": "DiT\u7ed3\u5408CLIP\u6761\u4ef6\u548c\u611f\u77e5\u76f8\u4f3c\u6027\u76ee\u6807\uff0c\u4e3a\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.16081", "pdf": "https://arxiv.org/pdf/2505.16081", "abs": "https://arxiv.org/abs/2505.16081", "authors": ["KMA Solaiman"], "title": "BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators", "categories": ["cs.CL"], "comment": "Under review", "summary": "We present BiasLab, a dataset of 300 political news articles annotated for\nperceived ideological bias. These articles were selected from a curated\n900-document pool covering diverse political events and source biases. Each\narticle is labeled by crowdworkers along two independent scales, assessing\nsentiment toward the Democratic and Republican parties, and enriched with\nrationale indicators. The annotation pipeline incorporates targeted worker\nqualification and was refined through pilot-phase analysis. We quantify\ninter-annotator agreement, analyze misalignment with source-level outlet bias,\nand organize the resulting labels into interpretable subsets. Additionally, we\nsimulate annotation using schema-constrained GPT-4o, enabling direct comparison\nto human labels and revealing mirrored asymmetries, especially in\nmisclassifying subtly right-leaning content. We define two modeling tasks:\nperception drift prediction and rationale type classification, and report\nbaseline performance to illustrate the challenge of explainable bias detection.\nBiasLab's rich rationale annotations provide actionable interpretations that\nfacilitate explainable modeling of political bias, supporting the development\nof transparent, socially aware NLP systems. We release the dataset, annotation\nschema, and modeling code to encourage research on human-in-the-loop\ninterpretability and the evaluation of explanation effectiveness in real-world\nsettings.", "AI": {"tldr": "BiasLab\u662f\u4e00\u4e2a\u5305\u542b300\u7bc7\u653f\u6cbb\u65b0\u95fb\u6587\u7ae0\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u611f\u77e5\u5230\u7684\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7406\u7531\u6ce8\u91ca\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u7684\u653f\u6cbb\u504f\u89c1\u5efa\u6a21\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u652f\u6301\u900f\u660e\u3001\u793e\u4f1a\u610f\u8bc6NLP\u7cfb\u7edf\u7684\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u5bf9\u504f\u89c1\u68c0\u6d4b\u548c\u89e3\u91ca\u6709\u6548\u6027\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4f17\u5305\u6807\u6ce8\u6587\u7ae0\u5bf9\u6c11\u4e3b\u515a\u548c\u5171\u548c\u515a\u7684\u60c5\u611f\uff0c\u7ed3\u5408\u5de5\u4eba\u8d44\u683c\u7b5b\u9009\u548c\u8bd5\u70b9\u5206\u6790\u4f18\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u5e76\u5229\u7528GPT-4\u6a21\u62df\u6807\u6ce8\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u91cf\u5316\u4e86\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u5206\u6790\u4e86\u4e0e\u6765\u6e90\u504f\u89c1\u7684\u504f\u5dee\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u7ebf\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u53ef\u89e3\u91ca\u504f\u89c1\u68c0\u6d4b\u7684\u6311\u6218\u3002", "conclusion": "BiasLab\u4e3a\u53ef\u89e3\u91ca\u7684\u653f\u6cbb\u504f\u89c1\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.16007", "pdf": "https://arxiv.org/pdf/2505.16007", "abs": "https://arxiv.org/abs/2505.16007", "authors": ["Jinjin Gu"], "title": "Position: Agentic Systems Constitute a Key Component of Next-Generation Intelligent Image Processing", "categories": ["cs.CV"], "comment": null, "summary": "This position paper argues that the image processing community should broaden\nits focus from purely model-centric development to include agentic system\ndesign as an essential complementary paradigm. While deep learning has\nsignificantly advanced capabilities for specific image processing tasks,\ncurrent approaches face critical limitations in generalization, adaptability,\nand real-world problem-solving flexibility. We propose that developing\nintelligent agentic systems, capable of dynamically selecting, combining, and\noptimizing existing image processing tools, represents the next evolutionary\nstep for the field. Such systems would emulate human experts' ability to\nstrategically orchestrate different tools to solve complex problems, overcoming\nthe brittleness of monolithic models. The paper analyzes key limitations of\nmodel-centric paradigms, establishes design principles for agentic image\nprocessing systems, and outlines different capability levels for such agents.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u56fe\u50cf\u5904\u7406\u9886\u57df\u5e94\u4ece\u7eaf\u6a21\u578b\u4e2d\u5fc3\u5f00\u53d1\u6269\u5c55\u5230\u5305\u542b\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf\u5904\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6cdb\u5316\u6027\u3001\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5f00\u53d1\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u52a8\u6001\u9009\u62e9\u548c\u4f18\u5316\u73b0\u6709\u5de5\u5177\uff0c\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u7b56\u7565\u6027\u64cd\u4f5c\u3002", "result": "\u5206\u6790\u4e86\u6a21\u578b\u4e2d\u5fc3\u8303\u5f0f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u786e\u7acb\u4e86\u4ee3\u7406\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u80fd\u529b\u7b49\u7ea7\u3002", "conclusion": "\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u662f\u56fe\u50cf\u5904\u7406\u9886\u57df\u7684\u4e0b\u4e00\u6b65\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.16088", "pdf": "https://arxiv.org/pdf/2505.16088", "abs": "https://arxiv.org/abs/2505.16088", "authors": ["Gagan Bhatia", "Maxime Peyrard", "Wei Zhao"], "title": "Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Modern BPE tokenizers often split calendar dates into meaningless fragments,\ne.g., 20250312 $\\rightarrow$ 202, 503, 12, inflating token counts and obscuring\nthe inherent structure needed for robust temporal reasoning. In this work, we\n(1) introduce a simple yet interpretable metric, termed date fragmentation\nratio, that measures how faithfully a tokenizer preserves multi-digit date\ncomponents; (2) release DateAugBench, a suite of 6500 examples spanning three\ntemporal reasoning tasks: context-based date resolution, format-invariance\npuzzles, and date arithmetic across historical, contemporary, and future\nregimes; and (3) through layer-wise probing and causal attention-hop analyses,\nuncover an emergent date-abstraction mechanism whereby large language models\nstitch together the fragments of month, day, and year components for temporal\nreasoning. Our experiments show that excessive fragmentation correlates with\naccuracy drops of up to 10 points on uncommon dates like historical and\nfuturistic dates. Further, we find that the larger the model, the faster the\nemergent date abstraction that heals date fragments is accomplished. Lastly, we\nobserve a reasoning path that LLMs follow to assemble date fragments, typically\ndiffering from human interpretation (year $\\rightarrow$ month $\\rightarrow$\nday).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cfBPE\u5206\u8bcd\u5668\u5bf9\u65e5\u671f\u5206\u5272\u6548\u679c\u7684\u6307\u6807DateAugBench\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u8fc7\u5ea6\u5206\u5272\u4f1a\u5f71\u54cd\u6a21\u578b\u5bf9\u4e0d\u5e38\u89c1\u65e5\u671f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86LLM\u5728\u65e5\u671f\u62bd\u8c61\u4e0a\u7684\u673a\u5236\u3002", "motivation": "\u73b0\u4ee3BPE\u5206\u8bcd\u5668\u5e38\u5c06\u65e5\u671f\u5206\u5272\u6210\u65e0\u610f\u4e49\u7684\u7247\u6bb5\uff0c\u5f71\u54cd\u65f6\u95f4\u63a8\u7406\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u6b64\u9700\u8981\u91cf\u5316\u8fd9\u79cd\u5206\u5272\u6548\u679c\u5e76\u63a2\u7d22LLM\u5982\u4f55\u5904\u7406\u65e5\u671f\u7247\u6bb5\u3002", "method": "1. \u63d0\u51fa\u65e5\u671f\u5206\u5272\u6bd4\u6307\u6807\uff1b2. \u53d1\u5e03DateAugBench\u6d4b\u8bd5\u96c6\uff1b3. \u901a\u8fc7\u5206\u5c42\u63a2\u6d4b\u548c\u6ce8\u610f\u529b\u5206\u6790\u63ed\u793aLLM\u7684\u65e5\u671f\u62bd\u8c61\u673a\u5236\u3002", "result": "\u8fc7\u5ea6\u5206\u5272\u5bfc\u81f4\u4e0d\u5e38\u89c1\u65e5\u671f\uff08\u5982\u5386\u53f2\u6216\u672a\u6765\u65e5\u671f\uff09\u7684\u51c6\u786e\u7387\u4e0b\u964d10%\uff1b\u6a21\u578b\u8d8a\u5927\uff0c\u65e5\u671f\u62bd\u8c61\u673a\u5236\u8d8a\u65e9\u51fa\u73b0\uff1bLLM\u7684\u65e5\u671f\u63a8\u7406\u8def\u5f84\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "conclusion": "\u65e5\u671f\u5206\u5272\u5bf9\u65f6\u95f4\u63a8\u7406\u6709\u663e\u8457\u5f71\u54cd\uff0cLLM\u901a\u8fc7\u7279\u5b9a\u673a\u5236\u4fee\u590d\u5206\u5272\u7247\u6bb5\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u6548\u679c\u8d8a\u597d\u3002"}}
{"id": "2505.16025", "pdf": "https://arxiv.org/pdf/2505.16025", "abs": "https://arxiv.org/abs/2505.16025", "authors": ["Wen Wen", "Yaohong Wu", "Yue Sheng", "Neil Birkbeck", "Balu Adsumilli", "Yilin Wang"], "title": "CP-LLM: Context and Pixel Aware Large Language Model for Video Quality Assessment", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Under review", "summary": "Video quality assessment (VQA) is a challenging research topic with broad\napplications. Effective VQA necessitates sensitivity to pixel-level distortions\nand a comprehensive understanding of video context to accurately determine the\nperceptual impact of distortions. Traditional hand-crafted and learning-based\nVQA models mainly focus on pixel-level distortions and lack contextual\nunderstanding, while recent LLM-based models struggle with sensitivity to small\ndistortions or handle quality scoring and description as separate tasks. To\naddress these shortcomings, we introduce CP-LLM: a Context and Pixel aware\nLarge Language Model. CP-LLM is a novel multimodal LLM architecture featuring\ndual vision encoders designed to independently analyze perceptual quality at\nboth high-level (video context) and low-level (pixel distortion) granularity,\nalong with a language decoder subsequently reasons about the interplay between\nthese aspects. This design enables CP-LLM to simultaneously produce robust\nquality scores and interpretable quality descriptions, with enhanced\nsensitivity to pixel distortions (e.g. compression artifacts). The model is\ntrained via a multi-task pipeline optimizing for score prediction, description\ngeneration, and pairwise comparisons. Experiment results demonstrate that\nCP-LLM achieves state-of-the-art cross-dataset performance on established VQA\nbenchmarks and superior robustness to pixel distortions, confirming its\nefficacy for comprehensive and practical video quality assessment in real-world\nscenarios.", "AI": {"tldr": "CP-LLM\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u5206\u522b\u5206\u6790\u89c6\u9891\u4e0a\u4e0b\u6587\u548c\u50cf\u7d20\u7ea7\u5931\u771f\uff0c\u7ed3\u5408\u8bed\u8a00\u89e3\u7801\u5668\u5b9e\u73b0\u8d28\u91cf\u8bc4\u5206\u548c\u63cf\u8ff0\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfVQA\u6a21\u578b\u7f3a\u4e4f\u5bf9\u89c6\u9891\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u800c\u73b0\u6709LLM\u6a21\u578b\u5bf9\u5c0f\u5931\u771f\u4e0d\u654f\u611f\u6216\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u8d28\u91cf\u8bc4\u5206\u548c\u63cf\u8ff0\u3002CP-LLM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "CP-LLM\u91c7\u7528\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u5206\u522b\u5904\u7406\u9ad8/\u4f4e\u7c92\u5ea6\u4fe1\u606f\uff0c\u7ed3\u5408\u8bed\u8a00\u89e3\u7801\u5668\u8fdb\u884c\u63a8\u7406\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8bad\u7ec3\u4f18\u5316\u8bc4\u5206\u3001\u63cf\u8ff0\u751f\u6210\u548c\u6210\u5bf9\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCP-LLM\u5728VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5bf9\u5c0f\u5931\u771f\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CP-LLM\u4e3a\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u9762\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16102", "pdf": "https://arxiv.org/pdf/2505.16102", "abs": "https://arxiv.org/abs/2505.16102", "authors": ["Yash Kumar Atri", "Thomas H Shin", "Thomas Hartvigsen"], "title": "Continually Self-Improving Language Models for Bariatric Surgery Question--Answering", "categories": ["cs.CL"], "comment": null, "summary": "While bariatric and metabolic surgery (MBS) is considered the gold standard\ntreatment for severe and morbid obesity, its therapeutic efficacy hinges upon\nactive and longitudinal engagement with multidisciplinary providers, including\nsurgeons, dietitians/nutritionists, psychologists, and endocrinologists. This\nengagement spans the entire patient journey, from preoperative preparation to\nlong-term postoperative management. However, this process is often hindered by\nnumerous healthcare disparities, such as logistical and access barriers, which\nimpair easy patient access to timely, evidence-based, clinician-endorsed\ninformation. To address these gaps, we introduce bRAGgen, a novel adaptive\nretrieval-augmented generation (RAG)-based model that autonomously integrates\nreal-time medical evidence when response confidence dips below dynamic\nthresholds. This self-updating architecture ensures that responses remain\ncurrent and accurate, reducing the risk of misinformation. Additionally, we\npresent bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,\nvalidated by an expert bariatric surgeon. bRAGq constitutes the first\nlarge-scale, domain-specific benchmark for comprehensive MBS care. In a\ntwo-phase evaluation, bRAGgen is benchmarked against state-of-the-art models\nusing both large language model (LLM)--based metrics and expert surgeon review.\nAcross all evaluation dimensions, bRAGgen demonstrates substantially superior\nperformance in generating clinically accurate and relevant responses.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86bRAGgen\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u81ea\u9002\u5e94\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u51cf\u80a5\u548c\u4ee3\u8c22\u624b\u672f\uff08MBS\uff09\u60a3\u8005\u83b7\u53d6\u4fe1\u606f\u7684\u969c\u788d\uff0c\u5e76\u901a\u8fc7bRAGq\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u51cf\u80a5\u548c\u4ee3\u8c22\u624b\u672f\uff08MBS\uff09\u7684\u591a\u5b66\u79d1\u534f\u4f5c\u6cbb\u7597\u5e38\u56e0\u533b\u7597\u8d44\u6e90\u4e0d\u5747\u800c\u53d7\u963b\uff0c\u60a3\u8005\u96be\u4ee5\u53ca\u65f6\u83b7\u53d6\u51c6\u786e\u4fe1\u606f\u3002", "method": "\u5f00\u53d1\u4e86bRAGgen\u6a21\u578b\uff0c\u52a8\u6001\u6574\u5408\u5b9e\u65f6\u533b\u5b66\u8bc1\u636e\uff0c\u5e76\u5f15\u5165bRAGq\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "bRAGgen\u5728\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u548c\u76f8\u5173\u54cd\u5e94\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "bRAGgen\u4e3aMBS\u62a4\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u4fe1\u606f\u652f\u6301\uff0c\u6709\u671b\u6539\u5584\u60a3\u8005\u6cbb\u7597\u4f53\u9a8c\u3002"}}
{"id": "2505.16029", "pdf": "https://arxiv.org/pdf/2505.16029", "abs": "https://arxiv.org/abs/2505.16029", "authors": ["Shichao Li", "Peiliang Li", "Qing Lian", "Peng Yun", "Xiaozhi Chen"], "title": "Learning better representations for crowded pedestrians in offboard LiDAR-camera 3D tracking-by-detection", "categories": ["cs.CV"], "comment": null, "summary": "Perceiving pedestrians in highly crowded urban environments is a difficult\nlong-tail problem for learning-based autonomous perception. Speeding up 3D\nground truth generation for such challenging scenes is performance-critical yet\nvery challenging. The difficulties include the sparsity of the captured\npedestrian point cloud and a lack of suitable benchmarks for a specific system\ndesign study. To tackle the challenges, we first collect a new multi-view\nLiDAR-camera 3D multiple-object-tracking benchmark of highly crowded\npedestrians for in-depth analysis. We then build an offboard auto-labeling\nsystem that reconstructs pedestrian trajectories from LiDAR point cloud and\nmulti-view images. To improve the generalization power for crowded scenes and\nthe performance for small objects, we propose to learn high-resolution\nrepresentations that are density-aware and relationship-aware. Extensive\nexperiments validate that our approach significantly improves the 3D pedestrian\ntracking performance towards higher auto-labeling efficiency. The code will be\npublicly available at this HTTP URL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u5bc6\u5ea6\u884c\u4eba\u573a\u666f\u76843D\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u89c6\u89d2LiDAR\u548c\u76f8\u673a\u6570\u636e\u91cd\u5efa\u884c\u4eba\u8f68\u8ff9\uff0c\u5e76\u5b66\u4e60\u9ad8\u5206\u8fa8\u7387\u8868\u793a\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5bc6\u5ea6\u57ce\u5e02\u73af\u5883\u4e2d\u884c\u4eba\u611f\u77e5\u7684\u96be\u9898\uff0c\u5c24\u5176\u662f3D\u5730\u9762\u771f\u5b9e\u6570\u636e\u751f\u6210\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "method": "\u6536\u96c6\u591a\u89c6\u89d2LiDAR-\u76f8\u673a3D\u591a\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\uff0c\u6784\u5efa\u79bb\u7ebf\u81ea\u52a8\u6807\u6ce8\u7cfb\u7edf\uff0c\u5b66\u4e60\u5bc6\u5ea6\u548c\u5173\u7cfb\u611f\u77e5\u7684\u9ad8\u5206\u8fa8\u7387\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u884c\u4eba\u8ddf\u8e2a\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u6807\u6ce8\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5bc6\u5ea6\u884c\u4eba\u573a\u666f\u76843D\u6807\u6ce8\u95ee\u9898\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2505.16104", "pdf": "https://arxiv.org/pdf/2505.16104", "abs": "https://arxiv.org/abs/2505.16104", "authors": ["Yue Li", "Xin Yi", "Dongsheng Shi", "Gerard de Melo", "Xiaoling Wang", "Linlin Wang"], "title": "Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "With the increasing size of Large Vision-Language Models (LVLMs), network\npruning techniques aimed at compressing models for deployment in\nresource-constrained environments have garnered significant attention. However,\nwe observe that pruning often leads to a degradation in safety performance. To\naddress this issue, we present a novel and lightweight approach, termed\nHierarchical Safety Realignment (HSR). HSR operates by first quantifying the\ncontribution of each attention head to safety, identifying the most critical\nones, and then selectively restoring neurons directly within these attention\nheads that play a pivotal role in maintaining safety. This process\nhierarchically realigns the safety of pruned LVLMs, progressing from the\nattention head level to the neuron level. We validate HSR across various models\nand pruning strategies, consistently achieving notable improvements in safety\nperformance. To our knowledge, this is the first work explicitly focused on\nrestoring safety in LVLMs post-pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHSR\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5bf9\u9f50\u6ce8\u610f\u529b\u5934\u548c\u795e\u7ecf\u5143\uff0c\u6062\u590d\u526a\u679d\u540e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u526a\u679d\u6280\u672f\u867d\u7136\u80fd\u538b\u7f29\u6a21\u578b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u5b89\u5168\u6027\u4e0b\u964d\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HSR\u901a\u8fc7\u91cf\u5316\u6ce8\u610f\u529b\u5934\u5bf9\u5b89\u5168\u6027\u7684\u8d21\u732e\uff0c\u8bc6\u522b\u5173\u952e\u5934\u5e76\u9009\u62e9\u6027\u6062\u590d\u795e\u7ecf\u5143\uff0c\u5206\u5c42\u5bf9\u9f50\u5b89\u5168\u6027\u3002", "result": "\u5728\u4e0d\u540c\u6a21\u578b\u548c\u526a\u679d\u7b56\u7565\u4e0b\uff0cHSR\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u8868\u73b0\u3002", "conclusion": "HSR\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u526a\u679d\u540e\u6062\u590dLVLM\u5b89\u5168\u6027\u7684\u5de5\u4f5c\uff0c\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2505.16033", "pdf": "https://arxiv.org/pdf/2505.16033", "abs": "https://arxiv.org/abs/2505.16033", "authors": ["Faika Fairuj Preotee", "Shuvashis Sarker", "Shamim Rahim Refat", "Tashreef Muhammad", "Shifat Islam"], "title": "An Approach Towards Identifying Bangladeshi Leaf Diseases through Transfer Learning and XAI", "categories": ["cs.CV"], "comment": "Accepted for publication in 2024 27th International Conference on\n  Computer and Information Technology (ICCIT)", "summary": "Leaf diseases are harmful conditions that affect the health, appearance and\nproductivity of plants, leading to significant plant loss and negatively\nimpacting farmers' livelihoods. These diseases cause visible symptoms such as\nlesions, color changes, and texture variations, making it difficult for farmers\nto manage plant health, especially in large or remote farms where expert\nknowledge is limited. The main motivation of this study is to provide an\nefficient and accessible solution for identifying plant leaf diseases in\nBangladesh, where agriculture plays a critical role in food security. The\nobjective of our research is to classify 21 distinct leaf diseases across six\nplants using deep learning models, improving disease detection accuracy while\nreducing the need for expert involvement. Deep Learning (DL) techniques,\nincluding CNN and Transfer Learning (TL) models like VGG16, VGG19, MobileNetV2,\nInceptionV3, ResNet50V2 and Xception are used. VGG19 and Xception achieve the\nhighest accuracies, with 98.90% and 98.66% respectively. Additionally,\nExplainable AI (XAI) techniques such as GradCAM, GradCAM++, LayerCAM, ScoreCAM\nand FasterScoreCAM are used to enhance transparency by highlighting the regions\nof the models focused on during disease classification. This transparency\nensures that farmers can understand the model's predictions and take necessary\naction. This approach not only improves disease management but also supports\nfarmers in making informed decisions, leading to better plant protection and\nincreased agricultural productivity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982VGG19\u548cXception\uff09\u548c\u53ef\u89e3\u91caAI\u6280\u672f\uff08\u5982GradCAM\uff09\u9ad8\u6548\u8bc6\u522b\u5b5f\u52a0\u62c9\u56fd\u7684\u690d\u7269\u53f6\u7247\u75c5\u5bb3\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe98.90%\uff0c\u5e2e\u52a9\u519c\u6c11\u7ba1\u7406\u4f5c\u7269\u5065\u5eb7\u3002", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u56fd\u519c\u6c11\u56e0\u7f3a\u4e4f\u4e13\u5bb6\u77e5\u8bc6\u800c\u96be\u4ee5\u8bc6\u522b\u548c\u7ba1\u7406\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u519c\u4e1a\u751f\u4ea7\u529b\u3002", "method": "\u4f7f\u7528CNN\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08VGG16\u3001VGG19\u7b49\uff09\u5206\u7c7b21\u79cd\u53f6\u7247\u75c5\u5bb3\uff0c\u5e76\u7ed3\u5408XAI\u6280\u672f\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u3002", "result": "VGG19\u548cXception\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a98.90%\u548c98.66%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u75c5\u5bb3\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u8fd8\u901a\u8fc7\u900f\u660e\u5316\u6a21\u578b\u51b3\u7b56\u652f\u6301\u519c\u6c11\u505a\u51fa\u66f4\u660e\u667a\u7684\u7ba1\u7406\u51b3\u7b56\uff0c\u4fc3\u8fdb\u519c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2505.16107", "pdf": "https://arxiv.org/pdf/2505.16107", "abs": "https://arxiv.org/abs/2505.16107", "authors": ["Bo Li", "Gexiang Fang", "Wei Ye", "Zhenghua Xu", "Jinglei Zhang", "Hao Cheng", "Shikun Zhang"], "title": "MPL: Multiple Programming Languages with Large Language Models for Information Extraction", "categories": ["cs.CL"], "comment": "Findings of ACL2025", "summary": "Recent research in information extraction (IE) focuses on utilizing\ncode-style inputs to enhance structured output generation. The intuition behind\nthis is that the programming languages (PLs) inherently exhibit greater\nstructural organization than natural languages (NLs). This structural advantage\nmakes PLs particularly suited for IE tasks. Nevertheless, existing research\nprimarily focuses on Python for code-style simulation, overlooking the\npotential of other widely-used PLs (e.g., C++ and Java) during the supervised\nfine-tuning (SFT) phase. In this research, we propose \\textbf{M}ultiple\n\\textbf{P}rogramming \\textbf{L}anguages with large language models for\ninformation extraction (abbreviated as \\textbf{MPL}), a novel framework that\nexplores the potential of incorporating different PLs in the SFT phase.\nAdditionally, we introduce \\texttt{function-prompt} with virtual running to\nsimulate code-style inputs more effectively and efficiently. Experimental\nresults on a wide range of datasets demonstrate the effectiveness of MPL.\nFurthermore, we conduct extensive experiments to provide a comprehensive\nanalysis. We have released our code for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPL\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff08\u5982C++\u548cJava\uff09\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u63d0\u5347\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u7684\u6548\u679c\uff0c\u5e76\u5f15\u5165function-prompt\u548c\u865a\u62df\u8fd0\u884c\u6280\u672f\u4f18\u5316\u4ee3\u7801\u5f0f\u8f93\u5165\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8Python\u6a21\u62df\u4ee3\u7801\u5f0f\u8f93\u5165\uff0c\u5ffd\u89c6\u4e86\u5176\u4ed6\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f16\u7a0b\u8bed\u8a00\uff08\u5982C++\u548cJava\uff09\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faMPL\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u5e76\u5f15\u5165function-prompt\u4e0e\u865a\u62df\u8fd0\u884c\u6280\u672f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86MPL\u7684\u6709\u6548\u6027\u3002", "conclusion": "MPL\u6846\u67b6\u901a\u8fc7\u591a\u7f16\u7a0b\u8bed\u8a00\u7ed3\u5408\u548c\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.16039", "pdf": "https://arxiv.org/pdf/2505.16039", "abs": "https://arxiv.org/abs/2505.16039", "authors": ["Shuvashis Sarker", "Shamim Rahim Refat", "Faika Fairuj Preotee", "Shifat Islam", "Tashreef Muhammad", "Mohammad Ashraful Hoque"], "title": "An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection", "categories": ["cs.CV"], "comment": "Accepted for publication in 2024 27th International Conference on\n  Computer and Information Technology (ICCIT)", "summary": "The brain is a highly complex organ that manages many important tasks,\nincluding movement, memory and thinking. Brain-related conditions, like tumors\nand degenerative disorders, can be hard to diagnose and treat. Magnetic\nResonance Imaging (MRI) serves as a key tool for identifying these conditions,\noffering high-resolution images of brain structures. Despite this, interpreting\nMRI scans can be complicated. This study tackles this challenge by conducting a\ncomparative analysis of Vision Transformer (ViT) and Transfer Learning (TL)\nmodels such as VGG16, VGG19, Resnet50V2, MobilenetV2 for classifying brain\ndiseases using MRI data from Bangladesh based dataset. ViT, known for their\nability to capture global relationships in images, are particularly effective\nfor medical imaging tasks. Transfer learning helps to mitigate data constraints\nby fine-tuning pre-trained models. Furthermore, Explainable AI (XAI) methods\nsuch as GradCAM, GradCAM++, LayerCAM, ScoreCAM, and Faster-ScoreCAM are\nemployed to interpret model predictions. The results demonstrate that ViT\nsurpasses transfer learning models, achieving a classification accuracy of\n94.39%. The integration of XAI methods enhances model transparency, offering\ncrucial insights to aid medical professionals in diagnosing brain diseases with\ngreater precision.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86Vision Transformer\uff08ViT\uff09\u548c\u591a\u79cd\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08\u5982VGG16\u3001VGG19\u7b49\uff09\u5728MRI\u6570\u636e\u4e0a\u5bf9\u8111\u90e8\u75be\u75c5\u7684\u5206\u7c7b\u6548\u679c\uff0cViT\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe94.39%\uff0c\u5e76\u5229\u7528XAI\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8111\u90e8\u75be\u75c5\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u590d\u6742\uff0cMRI\u56fe\u50cf\u89e3\u8bfb\u56f0\u96be\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5148\u8fdb\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63d0\u9ad8\u8bca\u65ad\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528ViT\u548c\u591a\u79cd\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u5bf9\u5b5f\u52a0\u62c9\u56fdMRI\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408XAI\u65b9\u6cd5\uff08\u5982GradCAM\u7b49\uff09\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u3002", "result": "ViT\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe94.39%\uff0cXAI\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "ViT\u7ed3\u5408XAI\u65b9\u6cd5\u5728\u8111\u90e8\u75be\u75c5\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u533b\u5b66\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u7684\u5de5\u5177\u3002"}}
{"id": "2505.16118", "pdf": "https://arxiv.org/pdf/2505.16118", "abs": "https://arxiv.org/abs/2505.16118", "authors": ["Haotian Lan", "Yao Gao", "Yujun Cheng", "Wei Yuan", "Kun Wang"], "title": "Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics", "categories": ["cs.CL", "stat.AP"], "comment": "33 pages, 6 figures", "summary": "Social media's rise establishes user-generated content (UGC) as pivotal for\ntravel decisions, yet analytical methods lack scalability. This study\nintroduces a dual-method LLM framework: unsupervised expectation extraction\nfrom UGC paired with survey-informed supervised fine-tuning. Findings reveal\nleisure/social expectations drive engagement more than foundational\nnatural/emotional factors. By establishing LLMs as precision tools for\nexpectation quantification, we advance tourism analytics methodology and\npropose targeted strategies for experience personalization and social travel\npromotion. The framework's adaptability extends to consumer behavior research,\ndemonstrating computational social science's transformative potential in\nmarketing optimization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u7528\u6237\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u65c5\u6e38\u671f\u671b\uff0c\u53d1\u73b0\u4f11\u95f2/\u793e\u4ea4\u671f\u671b\u6bd4\u81ea\u7136/\u60c5\u611f\u56e0\u7d20\u66f4\u80fd\u9a71\u52a8\u7528\u6237\u53c2\u4e0e\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u751f\u6210\u5185\u5bb9\u5bf9\u65c5\u6e38\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u53cc\u65b9\u6cd5LLM\u6846\u67b6\uff1a\u65e0\u76d1\u7763\u671f\u671b\u63d0\u53d6\u4e0e\u8c03\u67e5\u76d1\u7763\u5fae\u8c03\u76f8\u7ed3\u5408\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4f11\u95f2/\u793e\u4ea4\u671f\u671b\u6bd4\u81ea\u7136/\u60c5\u611f\u56e0\u7d20\u66f4\u80fd\u9a71\u52a8\u7528\u6237\u53c2\u4e0e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65c5\u6e38\u5206\u6790\u63d0\u4f9b\u4e86\u7cbe\u51c6\u5de5\u5177\uff0c\u5e76\u5c55\u793a\u4e86\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5728\u8425\u9500\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16144", "pdf": "https://arxiv.org/pdf/2505.16144", "abs": "https://arxiv.org/abs/2505.16144", "authors": ["Ming Yang", "Haoran Li"], "title": "GMatch: Geometry-Constrained Feature Matching for RGB-D Object Pose Estimation", "categories": ["cs.CV"], "comment": "9 pages + 3 pages references + 2 pages appendix; 6 figures; 1 table", "summary": "We present GMatch, a learning-free feature matcher designed for robust 6DoF\nobject pose estimation, addressing common local ambiguities in sparse feature\nmatching. Unlike traditional methods that rely solely on descriptor similarity,\nGMatch performs a guided, incremental search, enforcing SE(3)-invariant\ngeometric consistency throughout the matching process. It leverages a provably\ncomplete set of geometric features that uniquely determine 3D keypoint\nconfigurations, ensuring globally consistent correspondences without the need\nfor training or GPU support. When combined with classical descriptors such as\nSIFT, GMatch-SIFT forms a general-purpose pose estimation pipeline that offers\nstrong interpretability and generalization across diverse objects and scenes.\nExperiments on the HOPE dataset show that GMatch outperforms both traditional\nand learning-based matchers, with GMatch-SIFT achieving or surpassing the\nperformance of instance-level pose networks. On the YCB-Video dataset,\nGMatch-SIFT demonstrates high accuracy and low variance on texture-rich\nobjects. These results not only validate the effectiveness of GMatch-SIFT for\nobject pose estimation but also highlight the broader applicability of GMatch\nas a general-purpose feature matcher. Code will be released upon acceptance.", "AI": {"tldr": "GMatch\u662f\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u7684\u7279\u5f81\u5339\u914d\u65b9\u6cd5\uff0c\u7528\u4e8e\u9c81\u68d2\u76846DoF\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u6027\u89e3\u51b3\u7a00\u758f\u7279\u5f81\u5339\u914d\u4e2d\u7684\u5c40\u90e8\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u63cf\u8ff0\u7b26\u76f8\u4f3c\u6027\uff0c\u5bb9\u6613\u53d7\u5230\u5c40\u90e8\u6a21\u7cca\u6027\u7684\u5f71\u54cd\uff0cGMatch\u65e8\u5728\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u6027\u63d0\u5347\u5339\u914d\u7684\u9c81\u68d2\u6027\u3002", "method": "GMatch\u91c7\u7528\u589e\u91cf\u641c\u7d22\u548cSE(3)\u4e0d\u53d8\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u5229\u7528\u51e0\u4f55\u7279\u5f81\u552f\u4e00\u786e\u5b9a3D\u5173\u952e\u70b9\u914d\u7f6e\uff0c\u65e0\u9700\u8bad\u7ec3\u6216GPU\u652f\u6301\u3002", "result": "\u5728HOPE\u548cYCB-Video\u6570\u636e\u96c6\u4e0a\uff0cGMatch-SIFT\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u5b9e\u4f8b\u7ea7\u59ff\u6001\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "GMatch-SIFT\u4e0d\u4ec5\u9a8c\u8bc1\u4e86\u5176\u5728\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8fd8\u5c55\u793a\u4e86\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u5339\u914d\u5668\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.16125", "pdf": "https://arxiv.org/pdf/2505.16125", "abs": "https://arxiv.org/abs/2505.16125", "authors": ["Hyopil Shin", "Sangah Lee", "Dongjun Jang", "Wooseok Song", "Jaeyoon Kim", "Chaeyoung Oh", "Hyemi Jo", "Youngchae Ahn", "Sihyun Oh", "Hyohyeong Chang", "Sunkyoung Kim", "Jinsik Lee"], "title": "KoBALT: Korean Benchmark For Advanced Linguistic Tasks", "categories": ["cs.CL"], "comment": "Under Reveiw", "summary": "We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a\ncomprehensive linguistically-motivated benchmark comprising 700 multiple-choice\nquestions spanning 24 phenomena across five linguistic domains: syntax,\nsemantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed\nto advance the evaluation of large language models (LLMs) in Korean, a\nmorphologically rich language, by addressing the limitations of conventional\nbenchmarks that often lack linguistic depth and typological grounding. It\nintroduces a suite of expert-curated, linguistically motivated questions with\nminimal n-gram overlap with standard Korean corpora, substantially mitigating\nthe risk of data contamination and allowing a more robust assessment of true\nlanguage understanding. Our evaluation of 20 contemporary LLMs reveals\nsignificant performance disparities, with the highest-performing model\nachieving 61\\% general accuracy but showing substantial variation across\nlinguistic domains - from stronger performance in semantics (66\\%) to\nconsiderable weaknesses in phonology (31\\%) and morphology (36\\%). Through\nhuman preference evaluation with 95 annotators, we demonstrate a strong\ncorrelation between KoBALT scores and human judgments, validating our\nbenchmark's effectiveness as a discriminative measure of Korean language\nunderstanding. KoBALT addresses critical gaps in linguistic evaluation for\ntypologically diverse languages and provides a robust framework for assessing\ngenuine linguistic competence in Korean language models.", "AI": {"tldr": "KoBALT\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u7684\u7efc\u5408\u6027\u8bed\u8a00\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b700\u9053\u9009\u62e9\u9898\uff0c\u8986\u76d65\u4e2a\u8bed\u8a00\u5b66\u9886\u57df\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u8bed\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u8bed\u8a00\u5b66\u6df1\u5ea6\u548c\u7c7b\u578b\u5b66\u57fa\u7840\uff0cKoBALT\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u97e9\u8bed\u8fd9\u79cd\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "KoBALT\u901a\u8fc7\u4e13\u5bb6\u7b56\u5212\u7684700\u9053\u8bed\u8a00\u5b66\u95ee\u9898\uff0c\u51cf\u5c11\u4e0e\u6807\u51c6\u97e9\u8bed\u8bed\u6599\u5e93\u7684n-gram\u91cd\u53e0\uff0c\u964d\u4f4e\u6570\u636e\u6c61\u67d3\u98ce\u9669\u3002", "result": "\u8bc4\u4f3020\u4e2a\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u793a\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u4e3a61%\uff0c\u4f46\u5404\u9886\u57df\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u8bed\u4e49\u9886\u57df\u8868\u73b0\u6700\u4f73\uff0866%\uff09\uff0c\u97f3\u7cfb\uff0831%\uff09\u548c\u5f62\u6001\u5b66\uff0836%\uff09\u8868\u73b0\u8f83\u5f31\u3002\u4eba\u7c7b\u504f\u597d\u8bc4\u4f30\u9a8c\u8bc1\u4e86KoBALT\u7684\u6709\u6548\u6027\u3002", "conclusion": "KoBALT\u4e3a\u97e9\u8bed\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u586b\u8865\u4e86\u8bed\u8a00\u5b66\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2505.16146", "pdf": "https://arxiv.org/pdf/2505.16146", "abs": "https://arxiv.org/abs/2505.16146", "authors": ["Zhenglin Hua", "Jinghan He", "Zijun Yao", "Tianxu Han", "Haiyun Guo", "Yuheng Jia", "Junfeng Fang"], "title": "Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large vision-language models (LVLMs) have achieved remarkable performance on\nmultimodal tasks such as visual question answering (VQA) and image captioning.\nHowever, they still suffer from hallucinations, generating text inconsistent\nwith visual input, posing significant risks in real-world applications.\nExisting approaches to address this issue focus on incorporating external\nknowledge bases, alignment training, or decoding strategies, all of which\nrequire substantial computational cost and time. Recent works try to explore\nmore efficient alternatives by adjusting LVLMs' internal representations.\nAlthough promising, these methods may cause hallucinations to be insufficiently\nsuppressed or lead to excessive interventions that negatively affect normal\nsemantics. In this work, we leverage sparse autoencoders (SAEs) to identify\nsemantic directions closely associated with either hallucinations or actuality,\nrealizing more precise and direct hallucination-related representations. Our\nanalysis demonstrates that interventions along the faithful direction we\nidentified can mitigate hallucinations, while those along the hallucinatory\ndirection can exacerbate them. Building on these insights, we propose Steering\nLVLMs via SAE Latent Directions (SSL), a training-free method based on\nSAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive\nexperiments demonstrate that SSL significantly outperforms existing decoding\napproaches in mitigating hallucinations, while maintaining transferability\nacross different model architectures with negligible additional time overhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u65b9\u6cd5\uff08SSL\uff09\uff0c\u901a\u8fc7\u8bc6\u522b\u5e7b\u89c9\u548c\u5b9e\u9645\u8bed\u4e49\u7684\u65b9\u5411\uff0c\u6709\u6548\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u751f\u6210\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u4e00\u81f4\u7684\u6587\u672c\u3002\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u4e0e\u5e7b\u89c9\u6216\u5b9e\u9645\u8bed\u4e49\u76f8\u5173\u7684\u65b9\u5411\uff0c\u63d0\u51faSSL\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u8fd9\u4e9b\u65b9\u5411\u6765\u6291\u5236\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSL\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u8de8\u6a21\u578b\u67b6\u6784\u7684\u8fc1\u79fb\u6027\u548c\u4f4e\u65f6\u95f4\u5f00\u9500\u3002", "conclusion": "SSL\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u51cf\u5c11LVLM\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002"}}
{"id": "2505.16128", "pdf": "https://arxiv.org/pdf/2505.16128", "abs": "https://arxiv.org/abs/2505.16128", "authors": ["Yue Zhou", "Barbara Di Eugenio"], "title": "Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Despite LLMs' explicit alignment against demographic stereotypes, they have\nbeen shown to exhibit biases under various social contexts. In this work, we\nfind that LLMs exhibit concerning biases in how they associate solution\nveracity with demographics. Through experiments across five human value-aligned\nLLMs on mathematics, coding, commonsense, and writing problems, we reveal two\nforms of such veracity biases: Attribution Bias, where models\ndisproportionately attribute correct solutions to certain demographic groups,\nand Evaluation Bias, where models' assessment of identical solutions varies\nbased on perceived demographic authorship. Our results show pervasive biases:\nLLMs consistently attribute fewer correct solutions and more incorrect ones to\nAfrican-American groups in math and coding, while Asian authorships are least\npreferred in writing evaluation. In additional studies, we show LLMs\nautomatically assign racially stereotypical colors to demographic groups in\nvisualization code, suggesting these biases are deeply embedded in models'\nreasoning processes. Our findings indicate that demographic bias extends beyond\nsurface-level stereotypes and social context provocations, raising concerns\nabout LLMs' deployment in educational and evaluation settings.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLMs\u5728\u8868\u9762\u4e0a\u907f\u514d\u4eba\u53e3\u7edf\u8ba1\u523b\u677f\u5370\u8c61\uff0c\u4f46\u5728\u89e3\u51b3\u6570\u5b66\u3001\u7f16\u7a0b\u3001\u5e38\u8bc6\u548c\u5199\u4f5c\u95ee\u9898\u65f6\uff0c\u4ecd\u8868\u73b0\u51fa\u5bf9\u7279\u5b9a\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u5305\u62ec\u5f52\u56e0\u504f\u89c1\u548c\u8bc4\u4f30\u504f\u89c1\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5df2\u660e\u786e\u907f\u514d\u4eba\u53e3\u7edf\u8ba1\u523b\u677f\u5370\u8c61\uff0c\u4f46\u5728\u4e0d\u540c\u793e\u4f1a\u80cc\u666f\u4e0b\u4ecd\u8868\u73b0\u51fa\u504f\u89c1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793aLLMs\u5728\u89e3\u51b3\u65b9\u6848\u771f\u5b9e\u6027\u65b9\u9762\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e94\u79cd\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684LLMs\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u3001\u5e38\u8bc6\u548c\u5199\u4f5c\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u5f52\u56e0\u504f\u89c1\u548c\u8bc4\u4f30\u504f\u89c1\u3002", "result": "LLMs\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4e2d\u66f4\u5c11\u5c06\u6b63\u786e\u7b54\u6848\u5f52\u56e0\u4e8e\u975e\u88d4\u7f8e\u56fd\u4eba\u7fa4\u4f53\uff0c\u800c\u5728\u5199\u4f5c\u8bc4\u4f30\u4e2d\u6700\u4e0d\u504f\u597d\u4e9a\u6d32\u4f5c\u8005\u3002\u6b64\u5916\uff0cLLMs\u5728\u53ef\u89c6\u5316\u4ee3\u7801\u4e2d\u81ea\u52a8\u4e3a\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u5206\u914d\u523b\u677f\u989c\u8272\u3002", "conclusion": "\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u4e0d\u4ec5\u9650\u4e8e\u8868\u9762\u523b\u677f\u5370\u8c61\uff0c\u8fd8\u6df1\u5165\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bf9\u5176\u5728\u6559\u80b2\u4e0e\u8bc4\u4f30\u9886\u57df\u7684\u5e94\u7528\u63d0\u51fa\u8b66\u793a\u3002"}}
{"id": "2505.16149", "pdf": "https://arxiv.org/pdf/2505.16149", "abs": "https://arxiv.org/abs/2505.16149", "authors": ["Zirui Pang", "Haosheng Tan", "Yuhan Pu", "Zhijie Deng", "Zhouan Shen", "Keyu Hu", "Jiaheng Wei"], "title": "When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet\nserve as critical tools for model evaluation. However, despite the cleaning\nefforts, these datasets still suffer from pervasive noisy labels and often\ncontain missing labels due to the co-existing image pattern where multiple\nclasses appear in an image sample. This results in misleading model comparisons\nand unfair evaluations. Existing label cleaning methods focus primarily on\nnoisy labels, but the issue of missing labels remains largely overlooked.\nMotivated by these challenges, we present a comprehensive framework named\nREVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,\nLLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods\n(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and\nmissing label detection in widely-used image classification test sets. REVEAL\ndetects potential noisy labels and omissions, aggregates predictions from\nvarious methods, and refines label accuracy through confidence-informed\npredictions and consensus-based filtering. Additionally, we provide a thorough\nanalysis of state-of-the-art vision-language models and pre-trained image\nclassifiers, highlighting their strengths and limitations within the context of\ndataset renovation by revealing 10 observations. Our method effectively reveals\nmissing labels from public datasets and provides soft-labeled results with\nlikelihoods. Through human verifications, REVEAL significantly improves the\nquality of 6 benchmark test sets, highly aligning to human judgments and\nenabling more accurate and meaningful comparisons in image classification.", "AI": {"tldr": "REVEAL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6807\u7b7e\u6e05\u7406\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u89e3\u51b3\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e2d\u566a\u58f0\u6807\u7b7e\u548c\u7f3a\u5931\u6807\u7b7e\u95ee\u9898\uff0c\u663e\u8457\u63d0\u53476\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6807\u7b7e\u6e05\u7406\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u566a\u58f0\u6807\u7b7e\uff0c\u800c\u7f3a\u5931\u6807\u7b7e\u95ee\u9898\u88ab\u5ffd\u89c6\uff0c\u5bfc\u81f4\u6a21\u578b\u8bc4\u4f30\u4e0d\u51c6\u786e\u3002", "method": "\u6574\u5408\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaVA\u3001BLIP\uff09\u4e0e\u6807\u7b7e\u6e05\u7406\u65b9\u6cd5\uff08\u5982Cleanlab\uff09\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u548c\u5171\u8bc6\u8fc7\u6ee4\u68c0\u6d4b\u566a\u58f0\u548c\u7f3a\u5931\u6807\u7b7e\u3002", "result": "REVEAL\u663e\u8457\u63d0\u5347\u4e866\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u6807\u7b7e\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u9ad8\u5ea6\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002", "conclusion": "REVEAL\u4e3a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u6807\u7b7e\uff0c\u652f\u6301\u66f4\u516c\u5e73\u7684\u6a21\u578b\u8bc4\u4f30\u3002"}}
{"id": "2505.16129", "pdf": "https://arxiv.org/pdf/2505.16129", "abs": "https://arxiv.org/abs/2505.16129", "authors": ["Hyang Cui"], "title": "LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods", "categories": ["cs.CL", "I.2.7"], "comment": "5 pages, 2 figures, 2 tables. Conforms to the ACL Rolling Review\n  (ARR) short paper track. Code and data available at:\n  https://github.com/CuiNiki/LLMs-Are-Not-Scorers", "summary": "Recent studies have applied large language models (LLMs) to machine\ntranslation quality estimation (MTQE) by prompting models to assign numeric\nscores. Nonetheless, these direct scoring methods tend to show low\nsegment-level correlation with human judgments. In this paper, we propose a\ngeneration-based evaluation paradigm that leverages decoder-only LLMs to\nproduce high-quality references, followed by semantic similarity scoring using\nsentence embeddings. We conduct the most extensive evaluation to date in MTQE,\ncovering 8 LLMs and 8 language pairs. Empirical results show that our method\noutperforms both intra-LLM direct scoring baselines and external non-LLM\nreference-free metrics from MTME. These findings demonstrate the strength of\ngeneration-based evaluation and support a shift toward hybrid approaches that\ncombine fluent generation with accurate semantic assessment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u89e3\u7801\u5668LLMs\u751f\u6210\u9ad8\u8d28\u91cf\u53c2\u8003\u8bd1\u6587\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u5206\u63d0\u5347\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u76f4\u63a5\u8bc4\u5206\u65b9\u6cd5\u5728\u7247\u6bb5\u7ea7\u522b\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u9700\u6539\u8fdb\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u4f7f\u7528\u89e3\u7801\u5668LLMs\u751f\u6210\u53c2\u8003\u8bd1\u6587\uff0c\u7ed3\u5408\u53e5\u5b50\u5d4c\u5165\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u5206\u3002", "result": "\u57288\u79cdLLMs\u548c8\u79cd\u8bed\u8a00\u5bf9\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u4f18\u4e8e\u76f4\u63a5\u8bc4\u5206\u548c\u975eLLM\u53c2\u8003\u65e0\u5173\u6307\u6807\u3002", "conclusion": "\u751f\u6210\u5f0f\u8bc4\u4f30\u7ed3\u5408\u8bed\u4e49\u5206\u6790\u662f\u672a\u6765\u65b9\u5411\uff0c\u652f\u6301\u6df7\u5408\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16151", "pdf": "https://arxiv.org/pdf/2505.16151", "abs": "https://arxiv.org/abs/2505.16151", "authors": ["Hongchen Wei", "Zhenzhong Chen"], "title": "Training-Free Reasoning and Reflection in MLLMs", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have\nshowcased impressive reasoning capabilities via reinforcement learning.\nHowever, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by\nthe prohibitive costs of retraining and the scarcity of high-quality,\nverifiable multimodal reasoning datasets. This paper introduces FRANK Model, a\ntraining-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning\nand reflection abilities, without any gradient updates or extra supervision.\nOur key insight is to decouple perception and reasoning across MLLM decoder\nlayers. Specifically, we observe that compared to the deeper decoder layers,\nthe shallow decoder layers allocate more attention to visual tokens, while the\ndeeper decoder layers concentrate on textual semantics. This observation\nmotivates a hierarchical weight merging approach that combines a\nvisual-pretrained MLLM with a reasoning-specialized LLM. To this end, we\npropose a layer-wise, Taylor-derived closed-form fusion mechanism that\nintegrates reasoning capacity into deep decoder layers while preserving visual\ngrounding in shallow decoder layers. Extensive experiments on challenging\nmultimodal reasoning benchmarks demonstrate the effectiveness of our approach.\nOn the MMMU benchmark, our model FRANK-38B achieves an accuracy of 69.2,\noutperforming the strongest baseline InternVL2.5-38B by +5.3, and even\nsurpasses the proprietary GPT-4o model. Our project homepage is at:\nhttp://iip.whu.edu.cn/frank/index.html", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFRANK\u6a21\u578b\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684MLLM\uff0c\u901a\u8fc7\u5206\u5c42\u6743\u91cd\u5408\u5e76\u65b9\u6cd5\u5c06\u89c6\u89c9\u9884\u8bad\u7ec3MLLM\u4e0e\u63a8\u7406\u4e13\u7528LLM\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u6269\u5c55\u63a8\u7406LLMs\u5230\u591a\u6a21\u6001LLMs\uff08MLLMs\uff09\u9762\u4e34\u9ad8\u6602\u7684\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u548c\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6743\u91cd\u5408\u5e76\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9\u9884\u8bad\u7ec3MLLM\u548c\u63a8\u7406\u4e13\u7528LLM\uff0c\u63d0\u51fa\u6cf0\u52d2\u5bfc\u51fa\u7684\u95ed\u5f0f\u878d\u5408\u673a\u5236\u3002", "result": "\u5728MMMU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFRANK-38B\u51c6\u786e\u7387\u8fbe69.2\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578bInternVL2.5-38B\uff08+5.3\uff09\uff0c\u751a\u81f3\u8d85\u8fc7GPT-4o\u3002", "conclusion": "FRANK\u6a21\u578b\u901a\u8fc7\u5206\u5c42\u878d\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u76d1\u7763\u3002"}}
{"id": "2505.16134", "pdf": "https://arxiv.org/pdf/2505.16134", "abs": "https://arxiv.org/abs/2505.16134", "authors": ["Menschikov Mikhail", "Alexander Kharitonov", "Maiia Kotyga", "Vadim Porvatov", "Anna Zhukovskaya", "David Kagramanyan", "Egor Shvetsov", "Evgeny Burnaev"], "title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models exhibit positional bias -- systematic neglect of\ninformation at specific context positions -- yet its interplay with linguistic\ndiversity remains poorly understood. We present a cross-linguistic study across\nfive typologically distinct languages (English, Russian, German, Hindi,\nVietnamese), examining how positional bias interacts with model uncertainty,\nsyntax, and prompting. Key findings: (1) Positional bias is model-driven, with\nlanguage-specific variations -- Qwen2.5-7B favors late positions, challenging\nassumptions of early-token bias; (2) Explicit positional guidance (e.g.,\ncorrect context is at position X) reduces accuracy across languages,\nundermining prompt-engineering practices; (3) Aligning context with positional\nbias increases entropy, yet minimal entropy does not predict accuracy. (4) We\nfurther uncover that LLMs differently impose dominant word order in\nfree-word-order languages like Hindi.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4f4d\u7f6e\u504f\u89c1\uff0c\u4e14\u4e0e\u8bed\u8a00\u591a\u6837\u6027\u3001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u53e5\u6cd5\u76f8\u5173\u3002\u901a\u8fc7\u4e94\u79cd\u8bed\u8a00\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u9a71\u52a8\u7684\u504f\u89c1\u7279\u5f81\u53ca\u5176\u5bf9\u63d0\u793a\u5de5\u7a0b\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4f4d\u7f6e\u504f\u89c1\u4e0e\u8bed\u8a00\u591a\u6837\u6027\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u5176\u5bf9\u6a21\u578b\u884c\u4e3a\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u5f71\u54cd\u3002", "method": "\u8de8\u8bed\u8a00\u7814\u7a76\uff08\u82f1\u8bed\u3001\u4fc4\u8bed\u3001\u5fb7\u8bed\u3001\u5370\u5730\u8bed\u3001\u8d8a\u5357\u8bed\uff09\uff0c\u5206\u6790\u4f4d\u7f6e\u504f\u89c1\u4e0e\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u53e5\u6cd5\u548c\u63d0\u793a\u7684\u5173\u7cfb\u3002", "result": "1. \u4f4d\u7f6e\u504f\u89c1\u662f\u6a21\u578b\u9a71\u52a8\u7684\uff0c\u8bed\u8a00\u95f4\u5b58\u5728\u5dee\u5f02\uff1b2. \u663e\u5f0f\u4f4d\u7f6e\u6307\u5bfc\u964d\u4f4e\u51c6\u786e\u6027\uff1b3. \u5bf9\u9f50\u4f4d\u7f6e\u504f\u89c1\u589e\u52a0\u71b5\uff0c\u4f46\u6700\u5c0f\u71b5\u4e0d\u9884\u6d4b\u51c6\u786e\u6027\uff1b4. \u81ea\u7531\u8bed\u5e8f\u8bed\u8a00\u4e2dLLM\u5bf9\u8bcd\u5e8f\u7684\u5f3a\u52a0\u3002", "conclusion": "\u4f4d\u7f6e\u504f\u89c1\u662f\u6a21\u578b\u56fa\u6709\u7279\u6027\uff0c\u63d0\u793a\u5de5\u7a0b\u9700\u8003\u8651\u8bed\u8a00\u591a\u6837\u6027\uff0c\u663e\u5f0f\u4f4d\u7f6e\u6307\u5bfc\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002"}}
{"id": "2505.16154", "pdf": "https://arxiv.org/pdf/2505.16154", "abs": "https://arxiv.org/abs/2505.16154", "authors": ["Ji Guo", "Long Zhou", "Zhijin Wang", "Jiaming He", "Qiyang Song", "Aiguo Chen", "Wenbo Jiang"], "title": "BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, deep learning-based Monocular Depth Estimation (MDE) models\nhave been widely applied in fields such as autonomous driving and robotics.\nHowever, their vulnerability to backdoor attacks remains unexplored. To fill\nthe gap in this area, we conduct a comprehensive investigation of backdoor\nattacks against MDE models. Typically, existing backdoor attack methods can not\nbe applied to MDE models. This is because the label used in MDE is in the form\nof a depth map. To address this, we propose BadDepth, the first backdoor attack\ntargeting MDE models. BadDepth overcomes this limitation by selectively\nmanipulating the target object's depth using an image segmentation model and\nrestoring the surrounding areas via depth completion, thereby generating\npoisoned datasets for object-level backdoor attacks. To improve robustness in\nphysical world scenarios, we further introduce digital-to-physical augmentation\nto adapt to the domain gap between the physical world and the digital domain.\nExtensive experiments on multiple models validate the effectiveness of BadDepth\nin both the digital domain and the physical world, without being affected by\nenvironmental factors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BadDepth\uff0c\u9996\u4e2a\u9488\u5bf9\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\uff08MDE\uff09\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u64cd\u7eb5\u76ee\u6807\u5bf9\u8c61\u6df1\u5ea6\u5e76\u751f\u6210\u6bd2\u5316\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5e94\u7528\u4e8eMDE\u7684\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1MDE\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u7814\u7a76\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faBadDepth\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u5206\u5272\u6a21\u578b\u9009\u62e9\u6027\u64cd\u7eb5\u76ee\u6807\u5bf9\u8c61\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u6062\u590d\u5468\u56f4\u533a\u57df\uff0c\u751f\u6210\u6bd2\u5316\u6570\u636e\u96c6\u3002\u5f15\u5165\u6570\u5b57\u5230\u7269\u7406\u589e\u5f3a\u4ee5\u63d0\u9ad8\u7269\u7406\u4e16\u754c\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86BadDepth\u5728\u6570\u5b57\u548c\u7269\u7406\u4e16\u754c\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e14\u4e0d\u53d7\u73af\u5883\u56e0\u7d20\u5f71\u54cd\u3002", "conclusion": "BadDepth\u662f\u9996\u4e2a\u9488\u5bf9MDE\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2505.16142", "pdf": "https://arxiv.org/pdf/2505.16142", "abs": "https://arxiv.org/abs/2505.16142", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRLKD\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u7ed3\u6784\u5956\u52b1\u6a21\u578b\uff08GSRM\uff09\u6539\u8fdb\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfSFT\u65b9\u6cd5\u53ea\u80fd\u6355\u6349\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8def\u5f84\u7684\u8868\u9762\u7279\u5f81\uff0c\u65e0\u6cd5\u4f20\u9012\u5176\u9690\u542b\u7684\u591a\u5206\u652f\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "method": "\u63d0\u51faRLKD\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548cGSRM\u6a21\u578b\uff0c\u5c06\u63a8\u7406\u8def\u5f84\u5206\u89e3\u4e3a\u5143\u63a8\u7406\u4e0e\u6c42\u89e3\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u673a\u5236\u8861\u91cf\u5b66\u751f\u4e0e\u6559\u5e08\u63a8\u7406\u7ed3\u6784\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRLKD\u5728\u4ec5\u4f7f\u75280.1%\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u4f18\u4e8e\u4f20\u7edfSFT-RL\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u6f5c\u529b\u3002", "conclusion": "RLKD\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u4f20\u9012\u6559\u5e08\u6a21\u578b\u7684\u9690\u542b\u63a8\u7406\u7ed3\u6784\uff0c\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16157", "pdf": "https://arxiv.org/pdf/2505.16157", "abs": "https://arxiv.org/abs/2505.16157", "authors": ["Yuang Ai", "Huaibo Huang", "Tao Wu", "Qihang Fan", "Ran He"], "title": "Breaking Complexity Barriers: High-Resolution Image Restoration with Rank Enhanced Linear Attention", "categories": ["cs.CV"], "comment": "13 pages, 7 figures, 12 tables", "summary": "Transformer-based models have made remarkable progress in image restoration\n(IR) tasks. However, the quadratic complexity of self-attention in Transformer\nhinders its applicability to high-resolution images. Existing methods mitigate\nthis issue with sparse or window-based attention, yet inherently limit global\ncontext modeling. Linear attention, a variant of softmax attention,\ndemonstrates promise in global context modeling while maintaining linear\ncomplexity, offering a potential solution to the above challenge. Despite its\nefficiency benefits, vanilla linear attention suffers from a significant\nperformance drop in IR, largely due to the low-rank nature of its attention\nmap. To counter this, we propose Rank Enhanced Linear Attention (RELA), a\nsimple yet effective method that enriches feature representations by\nintegrating a lightweight depthwise convolution. Building upon RELA, we propose\nan efficient and effective image restoration Transformer, named LAformer.\nLAformer achieves effective global perception by integrating linear attention\nand channel attention, while also enhancing local fitting capabilities through\na convolutional gated feed-forward network. Notably, LAformer eliminates\nhardware-inefficient operations such as softmax and window shifting, enabling\nefficient processing of high-resolution images. Extensive experiments across 7\nIR tasks and 21 benchmarks demonstrate that LAformer outperforms SOTA methods\nand offers significant computational advantages.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRELA\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86\u9ad8\u6548\u56fe\u50cf\u4fee\u590dTransformer\u6a21\u578bLAformer\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\u3002", "motivation": "Transformer\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u7a00\u758f\u6216\u7a97\u53e3\u6ce8\u610f\u529b\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u727a\u7272\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u63d0\u51faRank Enhanced Linear Attention (RELA)\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5377\u79ef\u589e\u5f3a\u7279\u5f81\u8868\u793a\uff1b\u57fa\u4e8eRELA\u6784\u5efaLAformer\u6a21\u578b\uff0c\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u901a\u9053\u6ce8\u610f\u529b\u5b9e\u73b0\u5168\u5c40\u611f\u77e5\uff0c\u5e76\u901a\u8fc7\u5377\u79ef\u95e8\u63a7\u524d\u9988\u7f51\u7edc\u589e\u5f3a\u5c40\u90e8\u62df\u5408\u80fd\u529b\u3002", "result": "\u57287\u4e2a\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u548c21\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAformer\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "LAformer\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u5377\u79ef\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3002"}}
{"id": "2505.16160", "pdf": "https://arxiv.org/pdf/2505.16160", "abs": "https://arxiv.org/abs/2505.16160", "authors": ["Bin Xu", "Yu Bai", "Huashan Sun", "Yiguan Lin", "Siming Liu", "Xinyue Liang", "Yaolin Li", "Yang Gao", "Heyan Huang"], "title": "EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios", "categories": ["cs.CL"], "comment": null, "summary": "As large language models continue to advance, their application in\neducational contexts remains underexplored and under-optimized. In this paper,\nwe address this gap by introducing the first diverse benchmark tailored for\neducational scenarios, incorporating synthetic data containing 9 major\nscenarios and over 4,000 distinct educational contexts. To enable comprehensive\nassessment, we propose a set of multi-dimensional evaluation metrics that cover\n12 critical aspects relevant to both teachers and students. We further apply\nhuman annotation to ensure the effectiveness of the model-generated evaluation\nresponses. Additionally, we succeed to train a relatively small-scale model on\nour constructed dataset and demonstrate that it can achieve performance\ncomparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on\nthe test set. Overall, this work provides a practical foundation for the\ndevelopment and evaluation of education-oriented language models. Code and data\nare released at https://github.com/ybai-nlp/EduBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u6559\u80b2\u573a\u666f\u7684\u591a\u6837\u5316\u57fa\u51c6EduBench\uff0c\u5305\u542b9\u5927\u573a\u666f\u548c4000\u591a\u4e2a\u6559\u80b2\u60c5\u5883\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\u3002\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u6a21\u578b\u751f\u6210\u6548\u679c\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5c0f\u89c4\u6a21\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1SOTA\u5927\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u548c\u4f18\u5316\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u591a\u6837\u5316\u6559\u80b2\u573a\u666f\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u7ef4\u8bc4\u4f30\u6307\u6807\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\u6a21\u578b\u6548\u679c\uff0c\u5e76\u8bad\u7ec3\u5c0f\u89c4\u6a21\u6a21\u578b\u3002", "result": "\u8bad\u7ec3\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6027\u80fd\u63a5\u8fd1SOTA\u5927\u6a21\u578b\uff08\u5982Deepseek V3\u3001Qwen Max\uff09\u3002", "conclusion": "\u4e3a\u6559\u80b2\u5bfc\u5411\u7684\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.16161", "pdf": "https://arxiv.org/pdf/2505.16161", "abs": "https://arxiv.org/abs/2505.16161", "authors": ["Liyan Wang", "Weixiang Zhou", "Cong Wang", "Kin-Man Lam", "Zhixun Su", "Jinshan Pan"], "title": "Deep Learning-Driven Ultra-High-Definition Image Restoration: A Survey", "categories": ["cs.CV"], "comment": "20 papers, 12 figures", "summary": "Ultra-high-definition (UHD) image restoration aims to specifically solve the\nproblem of quality degradation in ultra-high-resolution images. Recent\nadvancements in this field are predominantly driven by deep learning-based\ninnovations, including enhancements in dataset construction, network\narchitecture, sampling strategies, prior knowledge integration, and loss\nfunctions. In this paper, we systematically review recent progress in UHD image\nrestoration, covering various aspects ranging from dataset construction to\nalgorithm design. This serves as a valuable resource for understanding\nstate-of-the-art developments in the field. We begin by summarizing degradation\nmodels for various image restoration subproblems, such as super-resolution,\nlow-light enhancement, deblurring, dehazing, deraining, and desnowing, and\nemphasizing the unique challenges of their application to UHD image\nrestoration. We then highlight existing UHD benchmark datasets and organize the\nliterature according to degradation types and dataset construction methods.\nFollowing this, we showcase major milestones in deep learning-driven UHD image\nrestoration, reviewing the progression of restoration tasks, technological\ndevelopments, and evaluations of existing methods. We further propose a\nclassification framework based on network architectures and sampling\nstrategies, helping to clearly organize existing methods. Finally, we share\ninsights into the current research landscape and propose directions for further\nadvancements. A related repository is available at\nhttps://github.com/wlydlut/UHD-Image-Restoration-Survey.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u8d85\u9ad8\u6e05\uff08UHD\uff09\u56fe\u50cf\u4fee\u590d\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u6570\u636e\u96c6\u6784\u5efa\u3001\u7b97\u6cd5\u8bbe\u8ba1\u7b49\u65b9\u9762\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u8d85\u9ad8\u6e05\u56fe\u50cf\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u603b\u7ed3\u6df1\u5ea6\u5b66\u4e60\u5728\u8be5\u9886\u57df\u7684\u521b\u65b0\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u8d44\u6e90\u3002", "method": "\u603b\u7ed3\u9000\u5316\u6a21\u578b\u3001\u6574\u7406\u6570\u636e\u96c6\u3001\u5206\u7c7b\u7f51\u7edc\u67b6\u6784\u548c\u91c7\u6837\u7b56\u7565\uff0c\u5e76\u5206\u6790\u6280\u672f\u53d1\u5c55\u3002", "result": "\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u6574\u7406\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "UHD\u56fe\u50cf\u4fee\u590d\u9886\u57df\u4ecd\u6709\u53d1\u5c55\u7a7a\u95f4\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2505.16162", "pdf": "https://arxiv.org/pdf/2505.16162", "abs": "https://arxiv.org/abs/2505.16162", "authors": ["Mingbo Song", "Heming Xia", "Jun Zhang", "Chak Tou Leong", "Qiancheng Xu", "Wenjie Li", "Sujian Li"], "title": "KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate\nthe inference of large language models (LLMs) without compromising generation\nquality. It works by efficiently drafting multiple tokens using a compact model\nand then verifying them in parallel using the target LLM. Notably,\nSelf-Speculative Decoding proposes skipping certain layers to construct the\ndraft model, which eliminates the need for additional parameters or training.\nDespite its strengths, we observe in this work that drafting with layer\nskipping exhibits significant sensitivity to domain shifts, leading to a\nsubstantial drop in acceleration performance. To enhance the domain\ngeneralizability of this paradigm, we introduce KNN-SSD, an algorithm that\nleverages K-Nearest Neighbor (KNN) search to match different skipped layers\nwith various domain inputs. We evaluated our algorithm in various models and\nmultiple tasks, observing that its application leads to 1.3x-1.6x speedup in\nLLM inference.", "AI": {"tldr": "KNN-SSD\u7b97\u6cd5\u901a\u8fc7KNN\u641c\u7d22\u5339\u914d\u4e0d\u540c\u8df3\u5c42\u4e0e\u9886\u57df\u8f93\u5165\uff0c\u63d0\u5347\u81ea\u63a8\u6d4b\u89e3\u7801\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0LLM\u63a8\u74061.3x-1.6x\u52a0\u901f\u3002", "motivation": "\u81ea\u63a8\u6d4b\u89e3\u7801\u5728\u9886\u57df\u8f6c\u79fb\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u63d0\u5347\u5176\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faKNN-SSD\u7b97\u6cd5\uff0c\u5229\u7528KNN\u641c\u7d22\u52a8\u6001\u5339\u914d\u8df3\u5c42\u4e0e\u8f93\u5165\u9886\u57df\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\uff0cKNN-SSD\u5b9e\u73b01.3x-1.6x\u7684\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "KNN-SSD\u6709\u6548\u63d0\u5347\u81ea\u63a8\u6d4b\u89e3\u7801\u7684\u9886\u57df\u9002\u5e94\u6027\uff0c\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\u3002"}}
{"id": "2505.16165", "pdf": "https://arxiv.org/pdf/2505.16165", "abs": "https://arxiv.org/abs/2505.16165", "authors": ["Yechan Park", "Gyuhyeon Pak", "Euntai Kim"], "title": "RE-TRIP : Reflectivity Instance Augmented Triangle Descriptor for 3D Place Recognition", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "While most people associate LiDAR primarily with its ability to measure\ndistances and provide geometric information about the environment (via point\nclouds), LiDAR also captures additional data, including reflectivity or\nintensity values. Unfortunately, when LiDAR is applied to Place Recognition\n(PR) in mobile robotics, most previous works on LiDAR-based PR rely only on\ngeometric measurements, neglecting the additional reflectivity information that\nLiDAR provides. In this paper, we propose a novel descriptor for 3D PR, named\nRE-TRIP (REflectivity-instance augmented TRIangle descriPtor). This new\ndescriptor leverages both geometric measurements and reflectivity to enhance\nrobustness in challenging scenarios such as geometric degeneracy, high\ngeometric similarity, and the presence of dynamic objects. To implement RE-TRIP\nin real-world applications, we further propose (1) a keypoint extraction\nmethod, (2) a key instance segmentation method, (3) a RE-TRIP matching method,\nand (4) a reflectivity-combined loop verification method. Finally, we conduct a\nseries of experiments to demonstrate the effectiveness of RE-TRIP. Applied to\npublic datasets (i.e., HELIPR, FusionPortable) containing diverse scenarios\nsuch as long corridors, bridges, large-scale urban areas, and highly dynamic\nenvironments -- our experimental results show that the proposed method\noutperforms existing state-of-the-art methods in terms of Scan Context,\nIntensity Scan Context, and STD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76843D\u4f4d\u7f6e\u8bc6\u522b\u63cf\u8ff0\u7b26RE-TRIP\uff0c\u7ed3\u5408\u51e0\u4f55\u6d4b\u91cf\u548c\u53cd\u5c04\u7387\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709LiDAR\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u51e0\u4f55\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u53cd\u5c04\u7387\u6570\u636e\uff0c\u800cRE-TRIP\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u989d\u5916\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faRE-TRIP\u63cf\u8ff0\u7b26\uff0c\u5305\u62ec\u5173\u952e\u70b9\u63d0\u53d6\u3001\u5b9e\u4f8b\u5206\u5272\u3001\u5339\u914d\u65b9\u6cd5\u548c\u53cd\u5c04\u7387\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRE-TRIP\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982Scan Context\u7b49\uff09\u3002", "conclusion": "RE-TRIP\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u548c\u53cd\u5c04\u7387\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4d\u7f6e\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2505.16164", "pdf": "https://arxiv.org/pdf/2505.16164", "abs": "https://arxiv.org/abs/2505.16164", "authors": ["Mengyang Qiu", "Zoe Brisebois", "Siena Sun"], "title": "Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly explored as substitutes for\nhuman participants in cognitive tasks, but their ability to simulate human\nbehavioral variability remains unclear. This study examines whether LLMs can\napproximate individual differences in the phonemic fluency task, where\nparticipants generate words beginning with a target letter. We evaluated 34\nmodel configurations, varying prompt specificity, sampling temperature, and\nmodel type, and compared outputs to responses from 106 human participants.\nWhile some configurations, especially Claude 3.7 Sonnet, matched human averages\nand lexical preferences, none reproduced the scope of human variability. LLM\noutputs were consistently less diverse and structurally rigid, and LLM\nensembles failed to increase diversity. Network analyses further revealed\nfundamental differences in retrieval structure between humans and models. These\nresults highlight key limitations in using LLMs to simulate human cognition and\nbehavior.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u5728\u97f3\u7d20\u6d41\u7545\u6027\u4efb\u52a1\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\uff0c\u53d1\u73b0LLMs\u867d\u80fd\u5339\u914d\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0c\u4f46\u65e0\u6cd5\u590d\u73b0\u4eba\u7c7b\u884c\u4e3a\u7684\u591a\u6837\u6027\u3002", "motivation": "\u9a8c\u8bc1LLMs\u662f\u5426\u80fd\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u6a21\u62df\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u4e2a\u4f53\u5dee\u5f02\u884c\u4e3a\u3002", "method": "\u8bc4\u4f30\u4e8634\u79cd\u6a21\u578b\u914d\u7f6e\uff0c\u6bd4\u8f83\u4e86LLMs\u4e0e106\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u5728\u97f3\u7d20\u6d41\u7545\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u80fd\u5339\u914d\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\u548c\u8bcd\u6c47\u504f\u597d\uff0c\u4f46\u65e0\u6cd5\u590d\u73b0\u4eba\u7c7b\u884c\u4e3a\u7684\u591a\u6837\u6027\u548c\u7ed3\u6784\u7075\u6d3b\u6027\u3002", "conclusion": "LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u548c\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2505.16166", "pdf": "https://arxiv.org/pdf/2505.16166", "abs": "https://arxiv.org/abs/2505.16166", "authors": ["Yuhao Xue", "Zhifei Zhang", "Xinyang Jiang", "Yifei Shen", "Junyao Gao", "Wentao Gu", "Jiale Zhao", "Miaojing Shi", "Cairong Zhao"], "title": "TRAIL: Transferable Robust Adversarial Images via Latent diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial attacks exploiting unrestricted natural perturbations present\nsevere security risks to deep learning systems, yet their transferability\nacross models remains limited due to distribution mismatches between generated\nadversarial features and real-world data. While recent works utilize\npre-trained diffusion models as adversarial priors, they still encounter\nchallenges due to the distribution shift between the distribution of ideal\nadversarial samples and the natural image distribution learned by the diffusion\nmodel. To address the challenge, we propose Transferable Robust Adversarial\nImages via Latent Diffusion (TRAIL), a test-time adaptation framework that\nenables the model to generate images from a distribution of images with\nadversarial features and closely resembles the target images. To mitigate the\ndistribution shift, during attacks, TRAIL updates the diffusion U-Net's weights\nby combining adversarial objectives (to mislead victim models) and perceptual\nconstraints (to preserve image realism). The adapted model then generates\nadversarial samples through iterative noise injection and denoising guided by\nthese objectives. Experiments demonstrate that TRAIL significantly outperforms\nstate-of-the-art methods in cross-model attack transferability, validating that\ndistribution-aligned adversarial feature synthesis is critical for practical\nblack-box attacks.", "AI": {"tldr": "TRAIL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6297\u76ee\u6807\u548c\u611f\u77e5\u7ea6\u675f\u751f\u6210\u5206\u5e03\u5bf9\u9f50\u7684\u5bf9\u6297\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u578b\u653b\u51fb\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u56e0\u751f\u6210\u7684\u5bf9\u6297\u7279\u5f81\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u8de8\u6a21\u578b\u53ef\u8fc1\u79fb\u6027\u53d7\u9650\u3002", "method": "TRAIL\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5728\u653b\u51fb\u65f6\u66f4\u65b0U-Net\u6743\u91cd\uff0c\u7ed3\u5408\u5bf9\u6297\u76ee\u6807\u548c\u611f\u77e5\u7ea6\u675f\u751f\u6210\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTRAIL\u5728\u8de8\u6a21\u578b\u653b\u51fb\u53ef\u8fc1\u79fb\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5206\u5e03\u5bf9\u9f50\u7684\u5bf9\u6297\u7279\u5f81\u5408\u6210\u5bf9\u5b9e\u9645\u9ed1\u76d2\u653b\u51fb\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.16170", "pdf": "https://arxiv.org/pdf/2505.16170", "abs": "https://arxiv.org/abs/2505.16170", "authors": ["Yuqing Yang", "Robin Jia"], "title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction", "categories": ["cs.CL"], "comment": null, "summary": "Can large language models (LLMs) admit their mistakes when they should know\nbetter? In this work, we define the behavior of acknowledging errors in\npreviously generated answers as \"retraction\" and aim to understand when and why\nLLMs choose to retract. We first construct model-specific datasets to evaluate\nwhether a model will retract an incorrect answer that contradicts its own\nparametric knowledge. While LLMs are capable of retraction, they do so only\ninfrequently. We demonstrate that retraction is closely tied to previously\nidentified indicators of models' internal belief: models fail to retract wrong\nanswers that they \"believe\" to be factually correct. Steering experiments\nfurther demonstrate that internal belief causally influences model retraction.\nIn particular, when the model does not believe its answer, this not only\nencourages the model to attempt to verify the answer, but also alters attention\nbehavior during self-verification. Finally, we demonstrate that simple\nsupervised fine-tuning significantly improves retraction performance by helping\nthe model learn more accurate internal beliefs. Code and datasets are available\non https://github.com/ayyyq/llm-retraction.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5426\u4f1a\u5728\u77e5\u9053\u9519\u8bef\u65f6\u627f\u8ba4\u9519\u8bef\uff08\u79f0\u4e3a\u201c\u64a4\u56de\u201d\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u64a4\u56de\u7684\u6761\u4ef6\u548c\u539f\u56e0\u3002\u7814\u7a76\u53d1\u73b0LLM\u80fd\u591f\u64a4\u56de\u9519\u8bef\u7b54\u6848\uff0c\u4f46\u9891\u7387\u8f83\u4f4e\uff0c\u4e14\u64a4\u56de\u884c\u4e3a\u4e0e\u6a21\u578b\u7684\u5185\u90e8\u4fe1\u5ff5\u5bc6\u5207\u76f8\u5173\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5185\u90e8\u4fe1\u5ff5\u5bf9\u64a4\u56de\u884c\u4e3a\u6709\u56e0\u679c\u5f71\u54cd\uff0c\u4e14\u7b80\u5355\u7684\u76d1\u7763\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u64a4\u56de\u6027\u80fd\u3002", "motivation": "\u7406\u89e3LLM\u5728\u4f55\u65f6\u53ca\u4e3a\u4f55\u4f1a\u64a4\u56de\u5176\u9519\u8bef\u7b54\u6848\uff0c\u4ee5\u63ed\u793a\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u6784\u5efa\u6a21\u578b\u7279\u5b9a\u6570\u636e\u96c6\u8bc4\u4f30LLM\u662f\u5426\u4f1a\u64a4\u56de\u4e0e\u5176\u53c2\u6570\u77e5\u8bc6\u77db\u76fe\u7684\u9519\u8bef\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5185\u90e8\u4fe1\u5ff5\u5bf9\u64a4\u56de\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "LLM\u80fd\u591f\u64a4\u56de\u9519\u8bef\u7b54\u6848\u4f46\u9891\u7387\u8f83\u4f4e\uff0c\u64a4\u56de\u884c\u4e3a\u4e0e\u5185\u90e8\u4fe1\u5ff5\u76f8\u5173\uff1b\u76d1\u7763\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u64a4\u56de\u6027\u80fd\u3002", "conclusion": "LLM\u7684\u64a4\u56de\u884c\u4e3a\u53d7\u5185\u90e8\u4fe1\u5ff5\u9a71\u52a8\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u53ef\u4f18\u5316\u5176\u64a4\u56de\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16174", "pdf": "https://arxiv.org/pdf/2505.16174", "abs": "https://arxiv.org/abs/2505.16174", "authors": ["Ping Liu", "Chi Zhang"], "title": "Erased or Dormant? Rethinking Concept Erasure Through Reversibility", "categories": ["cs.CV"], "comment": "Dr. Chi Zhang is the corresponding author", "summary": "To what extent does concept erasure eliminate generative capacity in\ndiffusion models? While prior evaluations have primarily focused on measuring\nconcept suppression under specific textual prompts, we explore a complementary\nand fundamental question: do current concept erasure techniques genuinely\nremove the ability to generate targeted concepts, or do they merely achieve\nsuperficial, prompt-specific suppression? We systematically evaluate the\nrobustness and reversibility of two representative concept erasure methods,\nUnified Concept Editing and Erased Stable Diffusion, by probing their ability\nto eliminate targeted generative behaviors in text-to-image models. These\nmethods attempt to suppress undesired semantic concepts by modifying internal\nmodel parameters, either through targeted attention edits or model-level\nfine-tuning strategies. To rigorously assess whether these techniques truly\nerase generative capacity, we propose an instance-level evaluation strategy\nthat employs lightweight fine-tuning to explicitly test the reactivation\npotential of erased concepts. Through quantitative metrics and qualitative\nanalyses, we show that erased concepts often reemerge with substantial visual\nfidelity after minimal adaptation, indicating that current methods suppress\nlatent generative representations without fully eliminating them. Our findings\nreveal critical limitations in existing concept erasure approaches and\nhighlight the need for deeper, representation-level interventions and more\nrigorous evaluation standards to ensure genuine, irreversible removal of\nconcepts from generative models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f53\u524d\u6982\u5ff5\u64e6\u9664\u6280\u672f\u662f\u5426\u771f\u6b63\u6d88\u9664\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u76ee\u6807\u6982\u5ff5\u7684\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u5b9e\u73b0\u4e86\u8868\u9762\u7684\u3001\u7279\u5b9a\u63d0\u793a\u7684\u6291\u5236\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e24\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9006\u6027\uff0c\u53d1\u73b0\u64e6\u9664\u7684\u6982\u5ff5\u5e38\u901a\u8fc7\u8f7b\u5fae\u9002\u5e94\u91cd\u65b0\u51fa\u73b0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u6982\u5ff5\u64e6\u9664\u6280\u672f\u662f\u5426\u771f\u6b63\u6d88\u9664\u4e86\u751f\u6210\u76ee\u6807\u6982\u5ff5\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u5b9e\u73b0\u7279\u5b9a\u63d0\u793a\u4e0b\u7684\u8868\u9762\u6291\u5236\u3002", "method": "\u91c7\u7528\u5b9e\u4f8b\u7ea7\u8bc4\u4f30\u7b56\u7565\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u6d4b\u8bd5\u64e6\u9664\u6982\u5ff5\u7684\u518d\u6fc0\u6d3b\u6f5c\u529b\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u4e24\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff08Unified Concept Editing\u548cErased Stable Diffusion\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\u64e6\u9664\u7684\u6982\u5ff5\u5e38\u901a\u8fc7\u8f7b\u5fae\u9002\u5e94\u91cd\u65b0\u51fa\u73b0\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u4ec5\u6291\u5236\u4e86\u6f5c\u5728\u751f\u6210\u8868\u793a\uff0c\u672a\u5b8c\u5168\u6d88\u9664\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff0c\u9700\u66f4\u6df1\u5c42\u6b21\u7684\u8868\u793a\u7ea7\u5e72\u9884\u548c\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4ee5\u5b9e\u73b0\u771f\u6b63\u4e0d\u53ef\u9006\u7684\u6982\u5ff5\u79fb\u9664\u3002"}}
{"id": "2505.16172", "pdf": "https://arxiv.org/pdf/2505.16172", "abs": "https://arxiv.org/abs/2505.16172", "authors": ["Abhay Kumara Sri Krishna Nandiraju", "Gondy Leroy", "David Kauchak", "Arif Ahmed"], "title": "Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding health information is essential in achieving and maintaining a\nhealthy life. We focus on simplifying health information for better\nunderstanding. With the availability of generative AI, the simplification\nprocess has become efficient and of reasonable quality, however, the algorithms\nremove information that may be crucial for comprehension. In this study, we\ncompare generative AI to detect missing information in simplified text,\nevaluate its importance, and fix the text with the missing information. We\ncollected 50 health information texts and simplified them using gpt-4-0613. We\ncompare five approaches to identify missing elements and regenerate the text by\ninserting the missing elements. These five approaches involve adding missing\nentities and missing words in various ways: 1) adding all the missing entities,\n2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,\nand 4, 5) serving as controls for comparison, adding randomly chosen entities.\nWe use cosine similarity and ROUGE scores to evaluate the semantic similarity\nand content overlap between the original, simplified, and reconstructed\nsimplified text. We do this for both summaries and full text. Overall, we find\nthat adding missing entities improves the text. Adding all the missing entities\nresulted in better text regeneration, which was better than adding the\ntop-ranked entities or words, or random words. Current tools can identify these\nentities, but are not valuable in ranking them.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u751f\u6210\u5f0fAI\u5728\u7b80\u5316\u5065\u5eb7\u4fe1\u606f\u65f6\u7f3a\u5931\u5185\u5bb9\u7684\u60c5\u51b5\uff0c\u5e76\u63d0\u51fa\u4e94\u79cd\u65b9\u6cd5\u8865\u5168\u7f3a\u5931\u4fe1\u606f\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8865\u5168\u7f3a\u5931\u5b9e\u4f53\u80fd\u663e\u8457\u63d0\u5347\u6587\u672c\u8d28\u91cf\u3002", "motivation": "\u7b80\u5316\u5065\u5eb7\u4fe1\u606f\u6709\u52a9\u4e8e\u7406\u89e3\uff0c\u4f46\u751f\u6210\u5f0fAI\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u5185\u5bb9\uff0c\u9700\u8bc4\u4f30\u548c\u4fee\u590d\u8fd9\u4e9b\u7f3a\u5931\u3002", "method": "\u6536\u96c650\u4efd\u5065\u5eb7\u4fe1\u606f\u6587\u672c\uff0c\u7528GPT-4\u7b80\u5316\uff0c\u6bd4\u8f83\u4e94\u79cd\u8865\u5168\u65b9\u6cd5\uff08\u5b9e\u4f53\u3001\u8bcd\u6c47\u3001\u6392\u540d\u5b9e\u4f53\u7b49\uff09\uff0c\u5e76\u7528\u76f8\u4f3c\u6027\u6307\u6807\u8bc4\u4f30\u3002", "result": "\u8865\u5168\u6240\u6709\u7f3a\u5931\u5b9e\u4f53\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73\uff0c\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u5f53\u524d\u5de5\u5177\u80fd\u8bc6\u522b\u5b9e\u4f53\u4f46\u65e0\u6cd5\u6709\u6548\u6392\u5e8f\u3002", "conclusion": "\u8865\u5168\u7f3a\u5931\u5b9e\u4f53\u80fd\u6709\u6548\u63d0\u5347\u7b80\u5316\u6587\u672c\u8d28\u91cf\uff0c\u4f46\u9700\u6539\u8fdb\u5b9e\u4f53\u6392\u5e8f\u5de5\u5177\u3002"}}
{"id": "2505.16175", "pdf": "https://arxiv.org/pdf/2505.16175", "abs": "https://arxiv.org/abs/2505.16175", "authors": ["Benjamin Schneider", "Dongfu Jiang", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 6 figures, 2 tables", "summary": "Long-video understanding has emerged as a crucial capability in real-world\napplications such as video surveillance, meeting summarization, educational\nlecture analysis, and sports broadcasting. However, it remains computationally\nprohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential\nvideo decoding, the process of converting the raw bit stream to RGB frames can\ntake up to a minute for hour-long video inputs, and 2) costly prefilling of up\nto several million tokens for LLM inference, resulting in high latency and\nmemory use. To address these challenges, we propose QuickVideo, a\nsystem-algorithm co-design that substantially accelerates long-video\nunderstanding to support real-time downstream applications. It comprises three\nkey innovations: QuickDecoder, a parallelized CPU-based video decoder that\nachieves 2-3 times speedup by splitting videos into keyframe-aligned intervals\nprocessed concurrently; QuickPrefill, a memory-efficient prefilling method\nusing KV-cache pruning to support more frames with less GPU memory; and an\noverlapping scheme that overlaps CPU video decoding with GPU inference.\nTogether, these components infernece time reduce by a minute on long video\ninputs, enabling scalable, high-quality video understanding even on limited\nhardware. Experiments show that QuickVideo generalizes across durations and\nsampling rates, making long video processing feasible in practice.", "AI": {"tldr": "QuickVideo\u901a\u8fc7\u7cfb\u7edf\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u5305\u62ec\u5e76\u884c\u89e3\u7801\u3001\u9ad8\u6548\u9884\u586b\u5145\u548cCPU-GPU\u91cd\u53e0\u5904\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u957f\u89c6\u9891\u7406\u89e3\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u89e3\u7801\u548c\u9884\u586b\u5145\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faQuickVideo\uff0c\u5305\u542bQuickDecoder\uff08\u5e76\u884c\u89e3\u7801\uff09\u3001QuickPrefill\uff08KV\u7f13\u5b58\u526a\u679d\uff09\u548cCPU-GPU\u91cd\u53e0\u5904\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQuickVideo\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\uff0c\u652f\u6301\u957f\u89c6\u9891\u7684\u9ad8\u6548\u5904\u7406\u3002", "conclusion": "QuickVideo\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16178", "pdf": "https://arxiv.org/pdf/2505.16178", "abs": "https://arxiv.org/abs/2505.16178", "authors": ["Ying Zhang", "Benjamin Heinzerling", "Dongyuan Li", "Ryoma Ishigaki", "Yuta Hitomi", "Kentaro Inui"], "title": "Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Fact recall, the ability of language models (LMs) to retrieve specific\nfactual knowledge, remains a challenging task despite their impressive general\ncapabilities. Common training strategies often struggle to promote robust\nrecall behavior with two-stage training, which first trains a model with\nfact-storing examples (e.g., factual statements) and then with fact-recalling\nexamples (question-answer pairs), tending to encourage rote memorization rather\nthan generalizable fact retrieval. In contrast, mixed training, which jointly\nuses both types of examples, has been empirically shown to improve the ability\nto recall facts, but the underlying mechanisms are still poorly understood. In\nthis work, we investigate how these training strategies affect how model\nparameters are shaped during training and how these differences relate to their\nability to recall facts. We introduce cross-task gradient trace to identify\nshared parameters, those strongly influenced by both fact-storing and\nfact-recalling examples. Our analysis on synthetic fact recall datasets with\nthe Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a\nlarger and more centralized set of shared parameters. These findings suggest\nthat the emergence of parameters may play a key role in enabling LMs to\ngeneralize factual knowledge across task formulations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u4e8b\u5b9e\u53ec\u56de\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u53d1\u73b0\u6df7\u5408\u8bad\u7ec3\u80fd\u4fc3\u8fdb\u5171\u4eab\u53c2\u6570\u7684\u5f62\u6210\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e8b\u5b9e\u53ec\u56de\u80fd\u529b\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u4e24\u9636\u6bb5\u8bad\u7ec3\uff09\u6613\u5bfc\u81f4\u673a\u68b0\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u53ec\u56de\uff0c\u6df7\u5408\u8bad\u7ec3\u867d\u6709\u6548\u4f46\u673a\u5236\u4e0d\u660e\u3002", "method": "\u901a\u8fc7\u4ea4\u53c9\u4efb\u52a1\u68af\u5ea6\u8ffd\u8e2a\u5206\u6790\u5171\u4eab\u53c2\u6570\uff0c\u4f7f\u7528Llama-3.2B\u548cPythia-2.8B\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6df7\u5408\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u6df7\u5408\u8bad\u7ec3\u80fd\u5f62\u6210\u66f4\u5927\u4e14\u66f4\u96c6\u4e2d\u7684\u5171\u4eab\u53c2\u6570\u96c6\uff0c\u8fd9\u4e9b\u53c2\u6570\u5bf9\u4e8b\u5b9e\u5b58\u50a8\u548c\u53ec\u56de\u4efb\u52a1\u5747\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5171\u4eab\u53c2\u6570\u7684\u51fa\u73b0\u53ef\u80fd\u662f\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u6cdb\u5316\u4e8b\u5b9e\u77e5\u8bc6\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2505.16180", "pdf": "https://arxiv.org/pdf/2505.16180", "abs": "https://arxiv.org/abs/2505.16180", "authors": ["Ashim Dahal", "Ankit Ghimire", "Saydul Akbar Murad", "Nick Rahimi"], "title": "Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Evaluating image captions requires cohesive assessment of both visual\nsemantics and language pragmatics, which is often not entirely captured by most\nmetrics. We introduce Redemption Score, a novel hybrid framework that ranks\nimage captions by triangulating three complementary signals: (1) Mutual\nInformation Divergence (MID) for global image-text distributional alignment,\n(2) DINO-based perceptual similarity of cycle-generated images for visual\ngrounding, and (3) BERTScore for contextual text similarity against human\nreferences. A calibrated fusion of these signals allows Redemption Score to\noffer a more holistic assessment. On the Flickr8k benchmark, Redemption Score\nachieves a Kendall-$\\tau$ of 56.43, outperforming twelve prior methods and\ndemonstrating superior correlation with human judgments without requiring\ntask-specific training. Our framework provides a more robust and nuanced\nevaluation by effectively redeeming image semantics and linguistic\ninterpretability indicated by strong transfer of knowledge in the Conceptual\nCaptions and MS COCO datasets.", "AI": {"tldr": "Redemption Score\u662f\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e09\u79cd\u4e92\u8865\u4fe1\u53f7\uff08MID\u3001DINO\u548cBERTScore\uff09\u8bc4\u4f30\u56fe\u50cf\u6807\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u9898\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u5168\u9762\u6355\u6349\u89c6\u89c9\u8bed\u4e49\u548c\u8bed\u8a00\u8bed\u7528\u5b66\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRedemption Score\u6846\u67b6\uff0c\u7ed3\u5408MID\u3001DINO\u611f\u77e5\u76f8\u4f3c\u6027\u548cBERTScore\uff0c\u8fdb\u884c\u6821\u51c6\u878d\u5408\u3002", "result": "\u5728Flickr8k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKendall-\u03c4\u8fbe56.43\uff0c\u4f18\u4e8e12\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "conclusion": "Redemption Score\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u53f7\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7ec6\u81f4\u7684\u8bc4\u4f30\u3002"}}
{"id": "2505.16188", "pdf": "https://arxiv.org/pdf/2505.16188", "abs": "https://arxiv.org/abs/2505.16188", "authors": ["Zirui He", "Mingyu Jin", "Bo Shen", "Ali Payani", "Yongfeng Zhang", "Mengnan Du"], "title": "SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models", "categories": ["cs.CL"], "comment": "30 pages, 24 figures, 12 tables", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but controlling their behavior\nreliably remains challenging, especially in open-ended generation settings.\nThis paper introduces a novel supervised steering approach that operates in\nsparse, interpretable representation spaces. We employ sparse autoencoders\n(SAEs)to obtain sparse latent representations that aim to disentangle semantic\nattributes from model activations. Then we train linear classifiers to identify\na small subspace of task-relevant dimensions in latent representations.\nFinally, we learn supervised steering vectors constrained to this subspace,\noptimized to align with target behaviors. Experiments across sentiment,\ntruthfulness, and politics polarity steering tasks with multiple LLMs\ndemonstrate that our supervised steering vectors achieve higher success rates\nwith minimal degradation in generation quality compared to existing methods.\nFurther analysis reveals that a notably small subspace is sufficient for\neffective steering, enabling more targeted and interpretable interventions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7684\u76d1\u7763\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u751f\u6210\u573a\u666f\u4e2d\u884c\u4e3a\u63a7\u5236\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u83b7\u53d6\u7a00\u758f\u6f5c\u5728\u8868\u793a\uff0c\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7ef4\u5ea6\uff0c\u5e76\u5b66\u4e60\u7ea6\u675f\u4e8e\u8be5\u5b50\u7a7a\u95f4\u7684\u76d1\u7763\u5f15\u5bfc\u5411\u91cf\u3002", "result": "\u5728\u60c5\u611f\u3001\u771f\u5b9e\u6027\u548c\u653f\u6cbb\u6781\u6027\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u7387\u9ad8\u4e14\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u6700\u5c0f\uff0c\u4ec5\u9700\u6781\u5c0f\u5b50\u7a7a\u95f4\u5373\u53ef\u5b9e\u73b0\u6709\u6548\u5f15\u5bfc\u3002", "conclusion": "\u7a00\u758f\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u7684\u76d1\u7763\u5f15\u5bfc\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u548c\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u884c\u4e3a\u63a7\u5236\u3002"}}
{"id": "2505.16181", "pdf": "https://arxiv.org/pdf/2505.16181", "abs": "https://arxiv.org/abs/2505.16181", "authors": ["Mohammad Reza Taesiri", "Brandon Collins", "Logan Bolton", "Viet Dac Lai", "Franck Dernoncourt", "Trung Bui", "Anh Totti Nguyen"], "title": "Understanding Generative AI Capabilities in Everyday Image Editing Tasks", "categories": ["cs.CV", "cs.AI"], "comment": "Code and qualitative examples are available at:\n  https://psrdataset.github.io", "summary": "Generative AI (GenAI) holds significant promise for automating everyday image\nediting tasks, especially following the recent release of GPT-4o on March 25,\n2025. However, what subjects do people most often want edited? What kinds of\nediting actions do they want to perform (e.g., removing or stylizing the\nsubject)? Do people prefer precise edits with predictable outcomes or highly\ncreative ones? By understanding the characteristics of real-world requests and\nthe corresponding edits made by freelance photo-editing wizards, can we draw\nlessons for improving AI-based editors and determine which types of requests\ncan currently be handled successfully by AI editors? In this paper, we present\na unique study addressing these questions by analyzing 83k requests from the\npast 12 years (2013-2025) on the Reddit community, which collected 305k\nPSR-wizard edits. According to human ratings, approximately only 33% of\nrequests can be fulfilled by the best AI editors (including GPT-4o,\nGemini-2.0-Flash, SeedEdit). Interestingly, AI editors perform worse on\nlow-creativity requests that require precise editing than on more open-ended\ntasks. They often struggle to preserve the identity of people and animals, and\nfrequently make non-requested touch-ups. On the other side of the table, VLM\njudges (e.g., o1) perform differently from human judges and may prefer AI edits\nmore than human edits. Code and qualitative examples are available at:\nhttps://psrdataset.github.io", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e8683k\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u56fe\u50cf\u7f16\u8f91\u8bf7\u6c42\uff0c\u53d1\u73b0\u5f53\u524dAI\u7f16\u8f91\u5668\uff08\u5982GPT-4o\uff09\u4ec5\u80fd\u5b8c\u6210\u7ea633%\u7684\u4efb\u52a1\uff0c\u4e14\u5728\u4f4e\u521b\u610f\u9700\u6c42\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u56fe\u50cf\u7f16\u8f91\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e86\u89e3\u7528\u6237\u9700\u6c42\u4e0eAI\u80fd\u529b\u7684\u5dee\u8ddd\u3002", "method": "\u5206\u6790Reddit\u793e\u533a12\u5e74\u95f4\u768483k\u4e2a\u7f16\u8f91\u8bf7\u6c42\u548c305k\u4e2aPSR-wizard\u7f16\u8f91\u6848\u4f8b\uff0c\u7ed3\u5408\u4eba\u7c7b\u548cVLM\u8bc4\u5206\u3002", "result": "AI\u7f16\u8f91\u5668\u5728\u4f4e\u521b\u610f\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u65e0\u6cd5\u4fdd\u7559\u4eba\u6216\u52a8\u7269\u7684\u8eab\u4efd\uff0c\u4e14\u4f1a\u8fdb\u884c\u975e\u8bf7\u6c42\u7684\u4fee\u9970\u3002VLM\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "AI\u7f16\u8f91\u5668\u5728\u521b\u610f\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u9700\u6539\u8fdb\u7cbe\u786e\u7f16\u8f91\u80fd\u529b\uff1bVLM\u8bc4\u5206\u53ef\u80fd\u504f\u5411AI\u7f16\u8f91\u3002"}}
{"id": "2505.16189", "pdf": "https://arxiv.org/pdf/2505.16189", "abs": "https://arxiv.org/abs/2505.16189", "authors": ["Sophie Wu", "Jan Philip Wahle", "Saif M. Mohammad"], "title": "The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions", "categories": ["cs.CL"], "comment": "8 pages, 26 figures", "summary": "This paper is the first investigation of the connection between emotion,\nembodiment, and everyday language in a large sample of natural language data.\nWe created corpora of body part mentions (BPMs) in online English text (blog\nposts and tweets). This includes a subset featuring human annotations for the\nemotions of the person whose body part is mentioned in the text. We show that\nBPMs are common in personal narratives and tweets (~5% to 10% of posts include\nBPMs) and that their usage patterns vary markedly by time and %geographic\nlocation. Using word-emotion association lexicons and our annotated data, we\nshow that text containing BPMs tends to be more emotionally charged, even when\nthe BPM is not explicitly used to describe a physical reaction to the emotion\nin the text. Finally, we discover a strong and statistically significant\ncorrelation between body-related language and a variety of poorer health\noutcomes. In sum, we argue that investigating the role of body-part related\nwords in language can open up valuable avenues of future research at the\nintersection of NLP, the affective sciences, and the study of human wellbeing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7814\u7a76\u4e86\u60c5\u611f\u3001\u5177\u8eab\u5316\u548c\u65e5\u5e38\u8bed\u8a00\u5728\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u6570\u636e\u4e2d\u7684\u8054\u7cfb\uff0c\u53d1\u73b0\u8eab\u4f53\u90e8\u4f4d\u63d0\u53ca\uff08BPMs\uff09\u5728\u6587\u672c\u4e2d\u5e38\u89c1\u4e14\u4e0e\u60c5\u611f\u3001\u5065\u5eb7\u7ed3\u679c\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u63a2\u7d22\u60c5\u611f\u3001\u5177\u8eab\u5316\u548c\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u586b\u8865\u4e86\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u6570\u636e\u4e2d\u8eab\u4f53\u90e8\u4f4d\u63d0\u53ca\u4e0e\u60c5\u611f\u53ca\u5065\u5eb7\u5173\u8054\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u521b\u5efa\u4e86\u5728\u7ebf\u82f1\u6587\u6587\u672c\uff08\u535a\u5ba2\u548c\u63a8\u6587\uff09\u4e2d\u8eab\u4f53\u90e8\u4f4d\u63d0\u53ca\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u8fdb\u884c\u4e86\u60c5\u611f\u6807\u6ce8\uff0c\u7ed3\u5408\u8bcd-\u60c5\u611f\u5173\u8054\u8bcd\u5178\u5206\u6790\u60c5\u611f\u5f3a\u5ea6\u3002", "result": "BPMs\u5728\u4e2a\u4eba\u53d9\u4e8b\u548c\u63a8\u6587\u4e2d\u5e38\u89c1\uff085%-10%\uff09\uff0c\u5176\u4f7f\u7528\u6a21\u5f0f\u56e0\u65f6\u95f4\u548c\u5730\u70b9\u800c\u5f02\uff1b\u542bBPMs\u7684\u6587\u672c\u60c5\u611f\u66f4\u5f3a\uff0c\u4e14\u4e0e\u8f83\u5dee\u5065\u5eb7\u7ed3\u679c\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u8eab\u4f53\u90e8\u4f4d\u76f8\u5173\u8bed\u8a00\u53ef\u4e3aNLP\u3001\u60c5\u611f\u79d1\u5b66\u548c\u4eba\u7c7b\u798f\u7949\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16192", "pdf": "https://arxiv.org/pdf/2505.16192", "abs": "https://arxiv.org/abs/2505.16192", "authors": ["Chaoya Jiang", "Yongrui Heng", "Wei Ye", "Han Yang", "Haiyang Xu", "Ming Yan", "Ji Zhang", "Fei Huang", "Shikun Zhang"], "title": "VLM-R$^3$: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, reasoning-based MLLMs have achieved a degree of success in\ngenerating long-form textual reasoning chains. However, they still struggle\nwith complex tasks that necessitate dynamic and iterative focusing on and\nrevisiting of visual regions to achieve precise grounding of textual reasoning\nin visual evidence. We introduce \\textbf{VLM-R$^3$} (\\textbf{V}isual\n\\textbf{L}anguage \\textbf{M}odel with \\textbf{R}egion \\textbf{R}ecognition and\n\\textbf{R}easoning), a framework that equips an MLLM with the ability to (i)\ndecide \\emph{when} additional visual evidence is needed, (ii) determine\n\\emph{where} to ground within the image, and (iii) seamlessly weave the\nrelevant sub-image content back into an interleaved chain-of-thought. The core\nof our method is \\textbf{Region-Conditioned Reinforcement Policy Optimization\n(R-GRPO)}, a training paradigm that rewards the model for selecting informative\nregions, formulating appropriate transformations (e.g.\\ crop, zoom), and\nintegrating the resulting visual context into subsequent reasoning steps. To\nbootstrap this policy, we compile a modest but carefully curated Visuo-Lingual\nInterleaved Rationale (VLIR) corpus that provides step-level supervision on\nregion selection and textual justification. Extensive experiments on MathVista,\nScienceQA, and other benchmarks show that VLM-R$^3$ sets a new state of the art\nin zero-shot and few-shot settings, with the largest gains appearing on\nquestions demanding subtle spatial reasoning or fine-grained visual cue\nextraction.", "AI": {"tldr": "VLM-R\u00b3\u662f\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u805a\u7126\u548c\u8fed\u4ee3\u89c6\u89c9\u533a\u57df\uff0c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u63a8\u7406\u4e0e\u89c6\u89c9\u8bc1\u636e\u7684\u7cbe\u786e\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u957f\u6587\u672c\u63a8\u7406\u94fe\u65b9\u9762\u53d6\u5f97\u4e86\u4e00\u5b9a\u6210\u529f\uff0c\u4f46\u5728\u9700\u8981\u52a8\u6001\u8fed\u4ee3\u805a\u7126\u89c6\u89c9\u533a\u57df\u7684\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faVLM-R\u00b3\u6846\u67b6\uff0c\u5f15\u5165\u533a\u57df\u6761\u4ef6\u5f3a\u5316\u7b56\u7565\u4f18\u5316\uff08R-GRPO\uff09\uff0c\u8bad\u7ec3\u6a21\u578b\u9009\u62e9\u4fe1\u606f\u533a\u57df\u3001\u5236\u5b9a\u53d8\u6362\uff08\u5982\u88c1\u526a\u3001\u7f29\u653e\uff09\u5e76\u6574\u5408\u89c6\u89c9\u4e0a\u4e0b\u6587\u5230\u63a8\u7406\u94fe\u4e2d\u3002", "result": "\u5728MathVista\u3001ScienceQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLM-R\u00b3\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u65b0\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u7ec6\u7a7a\u95f4\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VLM-R\u00b3\u901a\u8fc7\u52a8\u6001\u533a\u57df\u9009\u62e9\u548c\u6574\u5408\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16193", "pdf": "https://arxiv.org/pdf/2505.16193", "abs": "https://arxiv.org/abs/2505.16193", "authors": ["Daiqing Wu", "Dongbao Yang", "Sicheng Zhao", "Can Ma", "Yu Zhou"], "title": "An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The advancements in Multimodal Large Language Models (MLLMs) have enabled\nvarious multimodal tasks to be addressed under a zero-shot paradigm. This\nparadigm sidesteps the cost of model fine-tuning, emerging as a dominant trend\nin practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a\npivotal challenge in the quest for general artificial intelligence, fails to\naccommodate this convenience. The zero-shot paradigm exhibits undesirable\nperformance on MSA, casting doubt on whether MLLMs can perceive sentiments as\ncompetent as supervised models. By extending the zero-shot paradigm to\nIn-Context Learning (ICL) and conducting an in-depth study on configuring\ndemonstrations, we validate that MLLMs indeed possess such capability.\nSpecifically, three key factors that cover demonstrations' retrieval,\npresentation, and distribution are comprehensively investigated and optimized.\nA sentimental predictive bias inherent in MLLMs is also discovered and later\neffectively counteracted. By complementing each other, the devised strategies\nfor three factors result in average accuracy improvements of 15.9% on six MSA\ndatasets against the zero-shot paradigm and 11.2% against the random ICL\nbaseline.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u96f6\u6837\u672c\u8303\u5f0f\u4e0b\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\uff08MSA\uff09\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u4f18\u5316\u6f14\u793a\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u8303\u5f0f\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u8d28\u7591MLLMs\u7684\u60c5\u611f\u611f\u77e5\u80fd\u529b\uff0c\u9700\u9a8c\u8bc1\u5176\u6f5c\u529b\u3002", "method": "\u6269\u5c55\u96f6\u6837\u672c\u8303\u5f0f\u81f3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u7814\u7a76\u6f14\u793a\u7684\u68c0\u7d22\u3001\u5448\u73b0\u548c\u5206\u5e03\u4e09\u56e0\u7d20\uff0c\u5e76\u53d1\u73b0\u5e76\u62b5\u6d88MLLMs\u7684\u60c5\u611f\u9884\u6d4b\u504f\u5dee\u3002", "result": "\u4f18\u5316\u7b56\u7565\u5728\u516d\u4e2aMSA\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534715.9%\uff08\u76f8\u6bd4\u96f6\u6837\u672c\uff09\u548c11.2%\uff08\u76f8\u6bd4\u968f\u673aICL\u57fa\u7ebf\uff09\u3002", "conclusion": "MLLMs\u5177\u5907\u60c5\u611f\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u4f18\u5316\u6f14\u793a\u914d\u7f6e\u53ef\u663e\u8457\u63d0\u5347MSA\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2505.16209", "pdf": "https://arxiv.org/pdf/2505.16209", "abs": "https://arxiv.org/abs/2505.16209", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Dagan Feng", "Jinman Kim"], "title": "A Causal Approach to Mitigate Modality Preference Bias in Medical Visual Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is crucial for enhancing the\nefficiency of clinical diagnosis by providing accurate and timely responses to\nclinicians' inquiries regarding medical images. Existing MedVQA models suffered\nfrom modality preference bias, where predictions are heavily dominated by one\nmodality while overlooking the other (in MedVQA, usually questions dominate the\nanswer but images are overlooked), thereby failing to learn multimodal\nknowledge. To overcome the modality preference bias, we proposed a Medical\nCounterFactual VQA (MedCFVQA) model, which trains with bias and leverages\ncausal graphs to eliminate the modality preference bias during inference.\nExisting MedVQA datasets exhibit substantial prior dependencies between\nquestions and answers, which results in acceptable performance even if the\nmodel significantly suffers from the modality preference bias. To address this\nissue, we reconstructed new datasets by leveraging existing MedVQA datasets and\nChanged their P3rior dependencies (CP) between questions and their answers in\nthe training and test set. Extensive experiments demonstrate that MedCFVQA\nsignificantly outperforms its non-causal counterpart on both SLAKE, RadVQA and\nSLAKE-CP, RadVQA-CP datasets.", "AI": {"tldr": "MedCFVQA\u6a21\u578b\u901a\u8fc7\u56e0\u679c\u56fe\u6d88\u9664\u6a21\u6001\u504f\u597d\u504f\u5dee\uff0c\u5e76\u5728\u91cd\u6784\u7684\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u975e\u56e0\u679c\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MedVQA\u6a21\u578b\u5b58\u5728\u6a21\u6001\u504f\u597d\u504f\u5dee\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u3002", "method": "\u63d0\u51faMedCFVQA\u6a21\u578b\uff0c\u5229\u7528\u56e0\u679c\u56fe\u6d88\u9664\u6a21\u6001\u504f\u597d\u504f\u5dee\uff0c\u5e76\u91cd\u6784\u6570\u636e\u96c6\u4ee5\u51cf\u5c11\u5148\u9a8c\u4f9d\u8d56\u6027\u3002", "result": "MedCFVQA\u5728SLAKE\u3001RadVQA\u53ca\u5176\u91cd\u6784\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u975e\u56e0\u679c\u6a21\u578b\u3002", "conclusion": "MedCFVQA\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u504f\u597d\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86MedVQA\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16212", "pdf": "https://arxiv.org/pdf/2505.16212", "abs": "https://arxiv.org/abs/2505.16212", "authors": ["Anfeng Xu", "Tiantian Feng", "So Hyun Kim", "Somer Bishop", "Catherine Lord", "Shrikanth Narayanan"], "title": "Large Language Models based ASR Error Correction for Child Conversations", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) has recently shown remarkable progress,\nbut accurately transcribing children's speech remains a significant challenge.\nRecent developments in Large Language Models (LLMs) have shown promise in\nimproving ASR transcriptions. However, their applications in child speech\nincluding conversational scenarios are underexplored. In this study, we explore\nthe use of LLMs in correcting ASR errors for conversational child speech. We\ndemonstrate the promises and challenges of LLMs through experiments on two\nchildren's conversational speech datasets with both zero-shot and fine-tuned\nASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR\noutputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs\nto improve ASR performance when incorporating contextual information or when\nusing fine-tuned autoregressive ASR (e.g., Whisper) outputs.", "AI": {"tldr": "LLMs\u5728\u7ea0\u6b63\u513f\u7ae5\u5bf9\u8bdd\u8bed\u97f3\u7684ASR\u9519\u8bef\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u81ea\u56de\u5f52ASR\u8f93\u51fa\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u513f\u7ae5\u8bed\u97f3\u7684ASR\u8f6c\u5f55\u51c6\u786e\u6027\u8f83\u4f4e\uff0cLLMs\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8bdd\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30LLMs\u5728\u7ea0\u6b63\u4e24\u79cd\u513f\u7ae5\u5bf9\u8bdd\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u7684ASR\u9519\u8bef\uff0c\u5305\u62ec\u96f6\u6837\u672c\u548c\u5fae\u8c03ASR\u8f93\u51fa\u3002", "result": "LLMs\u80fd\u6709\u6548\u7ea0\u6b63\u96f6\u6837\u672c\u548cCTC-based ASR\u8f93\u51fa\uff0c\u4f46\u5bf9\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u81ea\u56de\u5f52ASR\uff08\u5982Whisper\uff09\u8f93\u51fa\u7684\u6539\u8fdb\u6709\u9650\u3002", "conclusion": "LLMs\u5728\u513f\u7ae5\u8bed\u97f3ASR\u7ea0\u9519\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u514b\u670d\u73b0\u6709\u6311\u6218\u3002"}}
{"id": "2505.16228", "pdf": "https://arxiv.org/pdf/2505.16228", "abs": "https://arxiv.org/abs/2505.16228", "authors": ["Wei-Lun Huang", "Joshua Liu", "Davood Tashayyod", "Jun Kang", "Amir Gandjbakhche", "Misha Kazhdan", "Mehran Armand"], "title": "A Shape-Aware Total Body Photography System for In-focus Surface Coverage Optimization", "categories": ["cs.CV"], "comment": "Accepted to JBHI", "summary": "Total Body Photography (TBP) is becoming a useful screening tool for patients\nat high risk for skin cancer. While much progress has been made, existing TBP\nsystems can be further improved for automatic detection and analysis of\nsuspicious skin lesions, which is in part related to the resolution and\nsharpness of acquired images. This paper proposes a novel shape-aware TBP\nsystem automatically capturing full-body images while optimizing image quality\nin terms of resolution and sharpness over the body surface. The system uses\ndepth and RGB cameras mounted on a 360-degree rotary beam, along with 3D body\nshape estimation and an in-focus surface optimization method to select the\noptimal focus distance for each camera pose. This allows for optimizing the\nfocused coverage over the complex 3D geometry of the human body given the\ncalibrated camera poses. We evaluate the effectiveness of the system in\ncapturing high-fidelity body images. The proposed system achieves an average\nresolution of 0.068 mm/pixel and 0.0566 mm/pixel with approximately 85% and 95%\nof surface area in-focus, evaluated on simulation data of diverse body shapes\nand poses as well as a real scan of a mannequin respectively. Furthermore, the\nproposed shape-aware focus method outperforms existing focus protocols (e.g.\nauto-focus). We believe the high-fidelity imaging enabled by the proposed\nsystem will improve automated skin lesion analysis for skin cancer screening.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5f62\u72b6\u611f\u77e5\u5168\u8eab\u6444\u5f71\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6e05\u6670\u5ea6\uff0c\u63d0\u5347\u76ae\u80a4\u764c\u7b5b\u67e5\u4e2d\u53ef\u7591\u75c5\u53d8\u7684\u81ea\u52a8\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5168\u8eab\u6444\u5f71\u7cfb\u7edf\u5728\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u6790\u53ef\u7591\u76ae\u80a4\u75c5\u53d8\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6e05\u6670\u5ea6\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u6df1\u5ea6\u548cRGB\u76f8\u673a\u30013D\u8eab\u4f53\u5f62\u72b6\u4f30\u8ba1\u53ca\u805a\u7126\u8868\u9762\u4f18\u5316\u65b9\u6cd5\uff0c\u9009\u62e9\u6bcf\u4e2a\u76f8\u673a\u59ff\u6001\u7684\u6700\u4f73\u5bf9\u7126\u8ddd\u79bb\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u771f\u5b9e\u626b\u63cf\u4e2d\uff0c\u7cfb\u7edf\u5e73\u5747\u5206\u8fa8\u7387\u8fbe0.068 mm/\u50cf\u7d20\u548c0.0566 mm/\u50cf\u7d20\uff0c85%\u548c95%\u7684\u8868\u9762\u533a\u57df\u4fdd\u6301\u6e05\u6670\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7684\u9ad8\u4fdd\u771f\u6210\u50cf\u80fd\u529b\u6709\u671b\u63d0\u5347\u76ae\u80a4\u75c5\u53d8\u81ea\u52a8\u5206\u6790\u7684\u51c6\u786e\u6027\uff0c\u52a9\u529b\u76ae\u80a4\u764c\u7b5b\u67e5\u3002"}}
{"id": "2505.16216", "pdf": "https://arxiv.org/pdf/2505.16216", "abs": "https://arxiv.org/abs/2505.16216", "authors": ["Jisu Kim", "Youngwoo Shin", "Uiji Hwang", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "title": "Memorization or Reasoning? Exploring the Idiom Understanding of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Idioms have long posed a challenge due to their unique linguistic properties,\nwhich set them apart from other common expressions. While recent studies have\nleveraged large language models (LLMs) to handle idioms across various tasks,\ne.g., idiom-containing sentence generation and idiomatic machine translation,\nlittle is known about the underlying mechanisms of idiom processing in LLMs,\nparticularly in multilingual settings. To this end, we introduce MIDAS, a new\nlarge-scale dataset of idioms in six languages, each paired with its\ncorresponding meaning. Leveraging this resource, we conduct a comprehensive\nevaluation of LLMs' idiom processing ability, identifying key factors that\ninfluence their performance. Our findings suggest that LLMs rely not only on\nmemorization, but also adopt a hybrid approach that integrates contextual cues\nand reasoning, especially when processing compositional idioms. This implies\nthat idiom understanding in LLMs emerges from an interplay between internal\nknowledge retrieval and reasoning-based inference.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u4e60\u8bed\u7684\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u4e60\u8bed\u6570\u636e\u96c6MIDAS\uff0c\u5e76\u53d1\u73b0LLMs\u901a\u8fc7\u8bb0\u5fc6\u4e0e\u63a8\u7406\u7ed3\u5408\u7684\u65b9\u5f0f\u5904\u7406\u4e60\u8bed\u3002", "motivation": "\u4e60\u8bed\u56e0\u5176\u72ec\u7279\u7684\u8bed\u8a00\u7279\u6027\u5bf9LLMs\u6784\u6210\u6311\u6218\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u5176\u5904\u7406\u673a\u5236\u4e86\u89e3\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u3002", "method": "\u5f15\u5165MIDAS\u6570\u636e\u96c6\uff0c\u5305\u542b\u516d\u79cd\u8bed\u8a00\u7684\u4e60\u8bed\u53ca\u5176\u542b\u4e49\uff0c\u5e76\u5168\u9762\u8bc4\u4f30LLMs\u7684\u4e60\u8bed\u5904\u7406\u80fd\u529b\u3002", "result": "LLMs\u4e0d\u4ec5\u4f9d\u8d56\u8bb0\u5fc6\uff0c\u8fd8\u7ed3\u5408\u4e0a\u4e0b\u6587\u7ebf\u7d22\u548c\u63a8\u7406\u5904\u7406\u4e60\u8bed\uff0c\u5c24\u5176\u662f\u7ec4\u5408\u578b\u4e60\u8bed\u3002", "conclusion": "LLMs\u7684\u4e60\u8bed\u7406\u89e3\u662f\u5185\u90e8\u77e5\u8bc6\u68c0\u7d22\u4e0e\u63a8\u7406\u63a8\u65ad\u7684\u4ea4\u4e92\u7ed3\u679c\u3002"}}
{"id": "2505.16229", "pdf": "https://arxiv.org/pdf/2505.16229", "abs": "https://arxiv.org/abs/2505.16229", "authors": ["Yuren Mao", "Wenyi Xu", "Yuyang Qin", "Yunjun Gao"], "title": "CT-Agent: A Multimodal-LLM Agent for 3D CT Radiology Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Computed Tomography (CT) scan, which produces 3D volumetric medical data that\ncan be viewed as hundreds of cross-sectional images (a.k.a. slices), provides\ndetailed anatomical information for diagnosis. For radiologists, creating CT\nradiology reports is time-consuming and error-prone. A visual question\nanswering (VQA) system that can answer radiologists' questions about some\nanatomical regions on the CT scan and even automatically generate a radiology\nreport is urgently needed. However, existing VQA systems cannot adequately\nhandle the CT radiology question answering (CTQA) task for: (1) anatomic\ncomplexity makes CT images difficult to understand; (2) spatial relationship\nacross hundreds slices is difficult to capture. To address these issues, this\npaper proposes CT-Agent, a multimodal agentic framework for CTQA. CT-Agent\nadopts anatomically independent tools to break down the anatomic complexity;\nfurthermore, it efficiently captures the across-slice spatial relationship with\na global-local token compression strategy. Experimental results on two 3D chest\nCT datasets, CT-RATE and RadGenome-ChestCT, verify the superior performance of\nCT-Agent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCT-Agent\u7684\u591a\u6a21\u6001\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3CT\u653e\u5c04\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u89e3\u5256\u590d\u6742\u6027\u548c\u8de8\u5207\u7247\u7a7a\u95f4\u5173\u7cfb\u95ee\u9898\u3002", "motivation": "\u4e3a\u653e\u5c04\u79d1\u533b\u751f\u63d0\u4f9b\u9ad8\u6548\u7684\u89c6\u89c9\u95ee\u7b54\u7cfb\u7edf\uff0c\u51cf\u8f7b\u5176\u64b0\u5199CT\u653e\u5c04\u62a5\u544a\u7684\u8d1f\u62c5\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728CT\u56fe\u50cf\u7406\u89e3\u548c\u7a7a\u95f4\u5173\u7cfb\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u89e3\u5256\u72ec\u7acb\u7684\u5de5\u5177\u5206\u89e3\u89e3\u5256\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u6807\u8bb0\u538b\u7f29\u7b56\u7565\u9ad8\u6548\u6355\u6349\u8de8\u5207\u7247\u7a7a\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a3D\u80f8\u90e8CT\u6570\u636e\u96c6\uff08CT-RATE\u548cRadGenome-ChestCT\uff09\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CT-Agent\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CT-Agent\u80fd\u591f\u6709\u6548\u89e3\u51b3CT\u653e\u5c04\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.16222", "pdf": "https://arxiv.org/pdf/2505.16222", "abs": "https://arxiv.org/abs/2505.16222", "authors": ["Jiwon Moon", "Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "title": "Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation", "categories": ["cs.CL", "cs.SE"], "comment": "26 pages", "summary": "With the growing use of large language models(LLMs) as evaluators, their\napplication has expanded to code evaluation tasks, where they assess the\ncorrectness of generated code without relying on reference implementations.\nWhile this offers scalability and flexibility, it also raises a critical,\nunresolved question: Can LLM judges fairly and robustly evaluate semantically\nequivalent code with superficial variations? Functionally correct code often\nexhibits variations-such as differences in variable names, comments, or\nformatting-that should not influence its correctness. Yet, whether LLM judges\ncan reliably handle these variations remains unclear. We present the first\ncomprehensive study of this issue, defining six types of potential bias in code\nevaluation and revealing their systematic impact on LLM judges. Across five\nprogramming languages and multiple LLMs, we empirically demonstrate that all\ntested LLM judges are susceptible to both positive and negative biases,\nresulting in inflated or unfairly low scores. Moreover, we observe that LLM\njudges remain vulnerable to these biases even when prompted to generate test\ncases before scoring, highlighting the need for more robust code evaluation\nmethods.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bc4\u4f30\u4ee3\u7801\u65f6\u5b58\u5728\u504f\u89c1\uff0c\u65e0\u6cd5\u516c\u5e73\u5904\u7406\u8bed\u4e49\u76f8\u540c\u4f46\u5f62\u5f0f\u4e0d\u540c\u7684\u4ee3\u7801\u3002", "motivation": "\u63a2\u8ba8LLMs\u4f5c\u4e3a\u4ee3\u7801\u8bc4\u4f30\u5de5\u5177\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8bed\u4e49\u76f8\u540c\u4f46\u5f62\u5f0f\u4e0d\u540c\u7684\u4ee3\u7801\u65f6\u3002", "method": "\u5b9a\u4e49\u4e86\u516d\u79cd\u6f5c\u5728\u504f\u89c1\u7c7b\u578b\uff0c\u5e76\u5728\u4e94\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u591a\u4e2aLLMs\u4e0a\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684LLM\u8bc4\u4f30\u8005\u5747\u8868\u73b0\u51fa\u6b63\u8d1f\u504f\u89c1\uff0c\u5bfc\u81f4\u8bc4\u5206\u504f\u9ad8\u6216\u504f\u4f4e\u3002", "conclusion": "\u9700\u8981\u66f4\u9c81\u68d2\u7684\u4ee3\u7801\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11LLM\u8bc4\u4f30\u4e2d\u7684\u504f\u89c1\u3002"}}
{"id": "2505.16239", "pdf": "https://arxiv.org/pdf/2505.16239", "abs": "https://arxiv.org/abs/2505.16239", "authors": ["Zheng Chen", "Zichen Zou", "Kewei Zhang", "Xiongfei Su", "Xin Yuan", "Yong Guo", "Yulun Zhang"], "title": "DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution", "categories": ["cs.CV"], "comment": "Code is available at: https://github.com/zhengchen1999/DOVE", "summary": "Diffusion models have demonstrated promising performance in real-world video\nsuper-resolution (VSR). However, the dozens of sampling steps they require,\nmake inference extremely slow. Sampling acceleration techniques, particularly\nsingle-step, provide a potential solution. Nonetheless, achieving one step in\nVSR remains challenging, due to the high training overhead on video data and\nstringent fidelity demands. To tackle the above issues, we propose DOVE, an\nefficient one-step diffusion model for real-world VSR. DOVE is obtained by\nfine-tuning a pretrained video diffusion model (*i.e.*, CogVideoX). To\neffectively train DOVE, we introduce the latent-pixel training strategy. The\nstrategy employs a two-stage scheme to gradually adapt the model to the video\nsuper-resolution task. Meanwhile, we design a video processing pipeline to\nconstruct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning\non this dataset further enhances the restoration capability of DOVE. Extensive\nexperiments show that DOVE exhibits comparable or superior performance to\nmulti-step diffusion-based VSR methods. It also offers outstanding inference\nefficiency, achieving up to a **28$\\times$** speed-up over existing methods\nsuch as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.", "AI": {"tldr": "DOVE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5f15\u5165\u6f5c\u5728\u50cf\u7d20\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\uff1b\u5355\u6b65\u91c7\u6837\u6280\u672f\u867d\u80fd\u52a0\u901f\uff0c\u4f46\u5728VSR\u4e2d\u5b9e\u73b0\u5355\u6b65\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faDOVE\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08CogVideoX\uff09\uff0c\u5e76\u91c7\u7528\u6f5c\u5728\u50cf\u7d20\u8bad\u7ec3\u7b56\u7565\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6HQ-VSR\u8fdb\u884c\u4f18\u5316\u3002", "result": "DOVE\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u6216\u591a\u6b65\u6269\u6563\u65b9\u6cd5\uff0c\u63a8\u7406\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb28\u500d\u3002", "conclusion": "DOVE\u4e3a\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5355\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u548c\u5feb\u901f\u63a8\u7406\u3002"}}
{"id": "2505.16227", "pdf": "https://arxiv.org/pdf/2505.16227", "abs": "https://arxiv.org/abs/2505.16227", "authors": ["Bohao Wu", "Qingyun Wang", "Yue Guo"], "title": "Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Personalizing jargon detection and explanation is essential for making\ntechnical documents accessible to readers with diverse disciplinary\nbackgrounds. However, tailoring models to individual users typically requires\nsubstantial annotation efforts and computational resources due to user-specific\nfinetuning. To address this, we present a systematic study of personalized\njargon detection, focusing on methods that are both efficient and scalable for\nreal-world deployment. We explore two personalization strategies: (1)\nlightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,\nand (2) personalized prompting, which tailors model behavior at inference time\nwithout retaining. To reflect realistic constraints, we also investigate hybrid\napproaches that combine limited annotated data with unsupervised user\nbackground signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in\nF1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,\nour method achieves comparable performance using only 10% of the annotated\ntraining data, demonstrating its practicality for resource-constrained\nsettings. Our study offers the first work to systematically explore efficient,\nlow-resource personalization of jargon detection using open-source language\nmodels, offering a practical path toward scalable, user-adaptive NLP system.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u5730\u5b9e\u73b0\u4e2a\u6027\u5316\u672f\u8bed\u68c0\u6d4b\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b56\u7565\uff1a\u8f7b\u91cf\u7ea7\u5fae\u8c03\u548c\u4e2a\u6027\u5316\u63d0\u793a\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e3a\u4f7f\u6280\u672f\u6587\u6863\u5bf9\u4e0d\u540c\u5b66\u79d1\u80cc\u666f\u7684\u8bfb\u8005\u66f4\u6613\u7406\u89e3\uff0c\u4e2a\u6027\u5316\u672f\u8bed\u68c0\u6d4b\u548c\u89e3\u91ca\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u63a2\u7d22\u4e86\u4e24\u79cd\u4e2a\u6027\u5316\u7b56\u7565\uff1a1) \u4f7f\u7528LoRA\u8fdb\u884c\u8f7b\u91cf\u7ea7\u5fae\u8c03\uff1b2) \u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u4e2a\u6027\u5316\u63d0\u793a\u3002\u540c\u65f6\u7814\u7a76\u4e86\u7ed3\u5408\u6709\u9650\u6807\u6ce8\u6570\u636e\u548c\u65e0\u76d1\u7763\u7528\u6237\u80cc\u666f\u4fe1\u53f7\u7684\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u4e2a\u6027\u5316LoRA\u6a21\u578b\u7684F1\u5206\u6570\u6bd4GPT-4\u9ad821.4%\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u9ad88.3%\uff0c\u4e14\u4ec5\u970010%\u7684\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u57fa\u4e8e\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90\u4e2a\u6027\u5316\u672f\u8bed\u68c0\u6d4b\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u7528\u6237\u81ea\u9002\u5e94NLP\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2505.16253", "pdf": "https://arxiv.org/pdf/2505.16253", "abs": "https://arxiv.org/abs/2505.16253", "authors": ["Preeti Mehta", "Aman Sagar", "Suchi Kumari"], "title": "Swin Transformer for Robust CGI Images Detection: Intra- and Inter-Dataset Analysis across Multiple Color Spaces", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2409.04734", "summary": "This study aims to address the growing challenge of distinguishing\ncomputer-generated imagery (CGI) from authentic digital images across three\ndifferent color spaces; RGB, YCbCr, and HSV. Given the limitations of existing\nclassification methods in handling the complexity and variability of CGI, this\nresearch proposes a Swin Transformer based model for accurate differentiation\nbetween natural and synthetic images. The proposed model leverages the Swin\nTransformer's hierarchical architecture to capture local and global features\nfor distinguishing CGI from natural images. Its performance was assessed\nthrough intra- and inter-dataset testing across three datasets: CiFAKE, JSSSTU,\nand Columbia. The model was evaluated individually on each dataset (D1, D2, D3)\nand on the combined datasets (D1+D2+D3) to test its robustness and domain\ngeneralization. To address dataset imbalance, data augmentation techniques were\napplied. Additionally, t-SNE visualization was used to demonstrate the feature\nseparability achieved by the Swin Transformer across the selected color spaces.\nThe model's performance was tested across all color schemes, with the RGB color\nscheme yielding the highest accuracy for each dataset. As a result, RGB was\nselected for domain generalization analysis and compared with other CNN-based\nmodels, VGG-19 and ResNet-50. The comparative results demonstrate the proposed\nmodel's effectiveness in detecting CGI, highlighting its robustness and\nreliability in both intra-dataset and inter-dataset evaluations. The findings\nof this study highlight the Swin Transformer model's potential as an advanced\ntool for digital image forensics, particularly in distinguishing CGI from\nnatural images. The model's strong performance indicates its capability for\ndomain generalization, making it a valuable asset in scenarios requiring\nprecise and reliable image classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSwin Transformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u5728RGB\u3001YCbCr\u548cHSV\u4e09\u79cd\u989c\u8272\u7a7a\u95f4\u4e2d\u533a\u5206\u8ba1\u7b97\u673a\u751f\u6210\u56fe\u50cf\uff08CGI\uff09\u4e0e\u771f\u5b9e\u6570\u5b57\u56fe\u50cf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406CGI\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u533a\u5206\u65b9\u6cd5\u3002", "method": "\u91c7\u7528Swin Transformer\u7684\u5206\u5c42\u67b6\u6784\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\u5904\u7406\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5e76\u901a\u8fc7t-SNE\u53ef\u89c6\u5316\u7279\u5f81\u5206\u79bb\u6548\u679c\u3002", "result": "RGB\u989c\u8272\u7a7a\u95f4\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f18\u4e8eVGG-19\u548cResNet-50\u3002", "conclusion": "Swin Transformer\u6a21\u578b\u5728\u6570\u5b57\u56fe\u50cf\u53d6\u8bc1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u533a\u5206CGI\u4e0e\u81ea\u7136\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u7684\u573a\u666f\u3002"}}
{"id": "2505.16232", "pdf": "https://arxiv.org/pdf/2505.16232", "abs": "https://arxiv.org/abs/2505.16232", "authors": ["Ali Sarosh Bangash", "Krish Veera", "Ishfat Abrar Islam", "Raiyan Abdul Baten"], "title": "MuseRAG: Idea Originality Scoring At Scale", "categories": ["cs.CL"], "comment": null, "summary": "An objective, face-valid way to assess the originality of creative ideas is\nto measure how rare each idea is within a population -- an approach long used\nin creativity research but difficult to automate at scale. Tabulating response\nfrequencies via manual bucketing of idea rephrasings is labor-intensive,\nerror-prone, and brittle under large corpora. We introduce a fully automated,\npsychometrically validated pipeline for frequency-based originality scoring.\nOur method, MuseRAG, combines large language models (LLMs) with an externally\norchestrated retrieval-augmented generation (RAG) framework. Given a new idea,\nthe system retrieves semantically similar prior idea buckets and zero-shot\nprompts the LLM to judge whether the new idea belongs to an existing bucket or\nforms a new one. The resulting buckets enable computation of frequency-based\noriginality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG\nmatches human annotators in idea clustering structure and resolution (AMI =\n0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong\nconvergent and external validity. Our work enables intent-sensitive,\nhuman-aligned originality scoring at scale to aid creativity research.", "AI": {"tldr": "MuseRAG\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5229\u7528LLM\u548cRAG\u6846\u67b6\u8bc4\u4f30\u521b\u610f\u7684\u65b0\u9896\u6027\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u805a\u7c7b\u548c\u96f6\u6837\u672c\u63d0\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u624b\u52a8\u7edf\u8ba1\u521b\u610f\u9891\u7387\u8bc4\u4f30\u65b0\u9896\u6027\uff0c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408LLM\u548cRAG\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u548c\u96f6\u6837\u672c\u63d0\u793a\u5bf9\u521b\u610f\u8fdb\u884c\u805a\u7c7b\u548c\u8bc4\u5206\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e2d\uff0cMuseRAG\u4e0e\u4eba\u5de5\u8bc4\u5206\u4e00\u81f4\u6027\u9ad8\uff08r=0.89\uff09\uff0c\u805a\u7c7b\u6548\u679c\u826f\u597d\uff08AMI=0.59\uff09\u3002", "conclusion": "MuseRAG\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u521b\u610f\u65b0\u9896\u6027\u8bc4\u4f30\uff0c\u4e3a\u521b\u9020\u529b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2505.16256", "pdf": "https://arxiv.org/pdf/2505.16256", "abs": "https://arxiv.org/abs/2505.16256", "authors": ["Yan Zhao", "Zhengxue Cheng", "Junxuan Zhang", "Qunshan Gu", "Qi Wang", "Li Song"], "title": "DualComp: End-to-End Learning of a Unified Dual-Modality Lossless Compressor", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "18 pages, 11 figures, 7 tables", "summary": "Most learning-based lossless compressors are designed for a single modality,\nrequiring separate models for multi-modal data and lacking flexibility.\nHowever, different modalities vary significantly in format and statistical\nproperties, making it ineffective to use compressors that lack\nmodality-specific adaptations. While multi-modal large language models (MLLMs)\noffer a potential solution for modality-unified compression, their excessive\ncomplexity hinders practical deployment. To address these challenges, we focus\non the two most common modalities, image and text, and propose DualComp, the\nfirst unified and lightweight learning-based dual-modality lossless compressor.\nBuilt on a lightweight backbone, DualComp incorporates three key structural\nenhancements to handle modality heterogeneity: modality-unified tokenization,\nmodality-switching contextual learning, and modality-routing\nmixture-of-experts. A reparameterization training strategy is also used to\nboost compression performance. DualComp integrates both modality-specific and\nshared parameters for efficient parameter utilization, enabling near real-time\ninference (200KB/s) on desktop CPUs. With much fewer parameters, DualComp\nachieves compression performance on par with the SOTA LLM-based methods for\nboth text and image datasets. Its simplified single-modality variant surpasses\nthe previous best image compressor on the Kodak dataset by about 9% using just\n1.2% of the model size.", "AI": {"tldr": "DualComp\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u7edf\u4e00\u7684\u53cc\u6a21\u6001\uff08\u56fe\u50cf\u548c\u6587\u672c\uff09\u65e0\u635f\u538b\u7f29\u5668\uff0c\u901a\u8fc7\u6a21\u6001\u7edf\u4e00\u7684\u5206\u8bcd\u3001\u6a21\u6001\u5207\u6362\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u6001\u8def\u7531\u4e13\u5bb6\u6df7\u5408\u7b49\u7ed3\u6784\u589e\u5f3a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u5229\u7528\u548c\u63a5\u8fd1\u5b9e\u65f6\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5b66\u4e60\u578b\u65e0\u635f\u538b\u7f29\u5668\u591a\u4e3a\u5355\u6a21\u6001\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u591a\u6a21\u6001\u6570\u636e\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\uff0c\u800c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53c8\u8fc7\u4e8e\u590d\u6742\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "DualComp\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u4e3b\u5e72\uff0c\u91c7\u7528\u6a21\u6001\u7edf\u4e00\u5206\u8bcd\u3001\u6a21\u6001\u5207\u6362\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6a21\u6001\u8def\u7531\u4e13\u5bb6\u6df7\u5408\u4e09\u79cd\u7ed3\u6784\u589e\u5f3a\uff0c\u5e76\u7ed3\u5408\u91cd\u53c2\u6570\u5316\u8bad\u7ec3\u7b56\u7565\u3002", "result": "DualComp\u5728\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u538b\u7f29\u6027\u80fd\u4e0e\u57fa\u4e8eSOTA LLM\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u5176\u5355\u6a21\u6001\u53d8\u4f53\u5728Kodak\u6570\u636e\u96c6\u4e0a\u4ee5\u4ec51.2%\u7684\u6a21\u578b\u5927\u5c0f\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u56fe\u50cf\u538b\u7f29\u5668\u7ea69%\u3002", "conclusion": "DualComp\u4e3a\u591a\u6a21\u6001\u65e0\u635f\u538b\u7f29\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.16234", "pdf": "https://arxiv.org/pdf/2505.16234", "abs": "https://arxiv.org/abs/2505.16234", "authors": ["Wei Zhang", "Zhenhong Zhou", "Junfeng Fang", "Rongwu Xu", "Kun Wang", "Yuanhe Zhang", "Rui Wang", "Ge Zhang", "Xinfeng Li", "Li Sun", "Lingjuan Lyu", "Yang Liu", "Sen Su"], "title": "LIFEBench: Evaluating Length Instruction Following in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "81 pages, 22 tables, 32 figures. Homepage:\n  https://ydyjya.github.io/LIFEBench/", "summary": "While large language models (LLMs) can solve PhD-level reasoning problems\nover long context inputs, they still struggle with a seemingly simpler task:\nfollowing explicit length instructions-e.g., write a 10,000-word novel.\nAdditionally, models often generate far too short outputs, terminate\nprematurely, or even refuse the request. Existing benchmarks focus primarily on\nevaluating generations quality, but often overlook whether the generations meet\nlength constraints. To this end, we introduce Length Instruction Following\nEvaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to\nfollow length instructions across diverse tasks and a wide range of specified\nlengths. LIFEBench consists of 10,800 instances across 4 task categories in\nboth English and Chinese, covering length constraints ranging from 16 to 8192\nwords. We evaluate 26 widely-used LLMs and find that most models reasonably\nfollow short-length instructions but deteriorate sharply beyond a certain\nthreshold. Surprisingly, almost all models fail to reach the vendor-claimed\nmaximum output lengths in practice, as further confirmed by our evaluations\nextending up to 32K words. Even long-context LLMs, despite their extended\ninput-output windows, counterintuitively fail to improve length-instructions\nfollowing. Notably, Reasoning LLMs outperform even specialized long-text\ngeneration models, achieving state-of-the-art length following. Overall,\nLIFEBench uncovers fundamental limitations in current LLMs' length instructions\nfollowing ability, offering critical insights for future progress.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86LIFEBench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9075\u5faa\u957f\u5ea6\u6307\u4ee4\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1LLM\u80fd\u89e3\u51b3\u590d\u6742\u95ee\u9898\uff0c\u4f46\u5728\u9075\u5faa\u660e\u786e\u957f\u5ea6\u6307\u4ee4\uff08\u5982\u751f\u6210\u7279\u5b9a\u5b57\u6570\u6587\u672c\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u8bc4\u6d4b\u6807\u51c6\u5ffd\u89c6\u4e86\u8fd9\u4e00\u80fd\u529b\u3002", "method": "\u63d0\u51faLIFEBench\u57fa\u51c6\uff0c\u5305\u542b10,800\u4e2a\u5b9e\u4f8b\uff0c\u8986\u76d64\u7c7b\u4efb\u52a1\u548c16\u52308192\u5b57\u7684\u957f\u5ea6\u8303\u56f4\uff0c\u8bc4\u4f30\u4e8626\u79cd\u5e38\u7528LLM\u3002", "result": "\u5927\u591a\u6570\u6a21\u578b\u5728\u77ed\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u8d85\u8fc7\u9608\u503c\u540e\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff1b\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u65e0\u6cd5\u8fbe\u5230\u5382\u5546\u5ba3\u79f0\u7684\u6700\u5927\u8f93\u51fa\u957f\u5ea6\u3002", "conclusion": "LIFEBench\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u957f\u5ea6\u6307\u4ee4\u9075\u5faa\u4e0a\u7684\u6839\u672c\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.16264", "pdf": "https://arxiv.org/pdf/2505.16264", "abs": "https://arxiv.org/abs/2505.16264", "authors": ["Sebastian Janampa", "Marios Pattichis"], "title": "LINEA: Fast and Accurate Line Detection Using Scalable Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Line detection is a basic digital image processing operation used by\nhigher-level processing methods. Recently, transformer-based methods for line\ndetection have proven to be more accurate than methods based on CNNs, at the\nexpense of significantly lower inference speeds. As a result, video analysis\nmethods that require low latencies cannot benefit from current\ntransformer-based methods for line detection. In addition, current\ntransformer-based models require pretraining attention mechanisms on large\ndatasets (e.g., COCO or Object360). This paper develops a new transformer-based\nmethod that is significantly faster without requiring pretraining the attention\nmechanism on large datasets. We eliminate the need to pre-train the attention\nmechanism using a new mechanism, Deformable Line Attention (DLA). We use the\nterm LINEA to refer to our new transformer-based method based on DLA. Extensive\nexperiments show that LINEA is significantly faster and outperforms previous\nmodels on sAP in out-of-distribution dataset testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5f62\u7ebf\u6ce8\u610f\u529b\uff08DLA\uff09\u7684\u65b0\u578bTransformer\u65b9\u6cd5LINEA\uff0c\u65e0\u9700\u5728\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ebf\u68c0\u6d4b\u7684\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\u4e14\u9700\u8981\u5728\u5927\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u4f4e\u5ef6\u8fdf\u89c6\u9891\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528Deformable Line Attention\uff08DLA\uff09\u673a\u5236\uff0c\u907f\u514d\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u9884\u8bad\u7ec3\u9700\u6c42\uff0c\u5f00\u53d1\u4e86\u540d\u4e3aLINEA\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLINEA\u5728\u901f\u5ea6\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5148\u524d\u6a21\u578b\u7684sAP\u6027\u80fd\u3002", "conclusion": "LINEA\u901a\u8fc7DLA\u673a\u5236\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u8bad\u7ec3\u7684\u9ad8\u6548\u7ebf\u68c0\u6d4b\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16237", "pdf": "https://arxiv.org/pdf/2505.16237", "abs": "https://arxiv.org/abs/2505.16237", "authors": ["Derong Xu", "Pengyue Jia", "Xiaopeng Li", "Yingyi Zhang", "Maolin Wang", "Qidong Liu", "Xiangyu Zhao", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Enhong Chen", "Tong Xu"], "title": "Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities, but\nstill struggle with issues like hallucinations and outdated information.\nRetrieval-augmented generation (RAG) addresses these issues by grounding LLM\noutputs in external knowledge with an Information Retrieval (IR) system.\nBuilding on this foundation, graph-based RAG systems go a step further by\nretrieving subgraphs, which preserve the relationships between knowledge\nentities and provide more comprehensive context. However, graph RAG faces two\nchallenges: (1) Retrieving relevant information introduces irrelevant nodes\n(especially in dense graph databases, where retrieval usually extends to\nadjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)\nThe representation gap between graph and language during generation with LLMs\nlimits the ability to fully leverage graph structures for enhanced\nunderstanding. To address these limitations, we propose Align-GRAG, a novel\nreasoning-guided dual alignment framework in post-retrieval phrase. It first\nformulates a subgraph by retrieving nodes and edges. Then an Aligner is\nproposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It\nachieves dual alignment of graph node and representation by leveraging KL\ndivergence loss and contrastive loss, facilitating efficient pruning of\nirrelevant knowledge and establishing a unified semantic space. The Generator\nintegrates the aligned graph data with LLM to produce coherent and accurate\nanswers. Experiments on GraphQA benchmark across three tasks (including common\nsense reasoning, scene graph understanding, and knowledge graph reasoning)\nvalidate the effectiveness of our method. The code will be available upon\naccepted.", "AI": {"tldr": "Align-GRAG\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u53cc\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5b50\u56fe\u5e76\u4f18\u5316\u56fe\u7f16\u7801\u5668\u4e0eLLM\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u56feRAG\u4e2d\u7684\u4fe1\u606f\u5197\u4f59\u548c\u8868\u793a\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u5e7b\u89c9\u548c\u8fc7\u65f6\u4fe1\u606f\u95ee\u9898\uff0c\u56feRAG\u867d\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4e0a\u4e0b\u6587\uff0c\u4f46\u9762\u4e34\u4fe1\u606f\u5197\u4f59\u548c\u8868\u793a\u5dee\u8ddd\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAlign-GRAG\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u5b50\u56fe\u5e76\u8054\u5408\u4f18\u5316\u56fe\u7f16\u7801\u5668\u4e0eLLM\u63a8\u7406\uff0c\u5b9e\u73b0\u8282\u70b9\u548c\u8868\u793a\u7684\u53cc\u5bf9\u9f50\uff0c\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "result": "\u5728GraphQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u5e38\u8bc6\u63a8\u7406\u3001\u573a\u666f\u56fe\u7406\u89e3\u548c\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Align-GRAG\u6709\u6548\u89e3\u51b3\u4e86\u56feRAG\u7684\u6311\u6218\uff0c\u4e3aLLM\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u652f\u6301\u3002"}}
{"id": "2505.16278", "pdf": "https://arxiv.org/pdf/2505.16278", "abs": "https://arxiv.org/abs/2505.16278", "authors": ["Zhenjie Yang", "Yilin Chai", "Xiaosong Jia", "Qifeng Li", "Yuqian Shao", "Xuekai Zhu", "Haisheng Su", "Junchi Yan"], "title": "DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project Page: https://thinklab-sjtu.github.io/DriveMoE/", "summary": "End-to-end autonomous driving (E2E-AD) demands effective processing of\nmulti-view sensory data and robust handling of diverse and complex driving\nscenarios, particularly rare maneuvers such as aggressive turns. Recent success\nof Mixture-of-Experts (MoE) architecture in Large Language Models (LLMs)\ndemonstrates that specialization of parameters enables strong scalability. In\nthis work, we propose DriveMoE, a novel MoE-based E2E-AD framework, with a\nScene-Specialized Vision MoE and a Skill-Specialized Action MoE. DriveMoE is\nbuilt upon our $\\pi_0$ Vision-Language-Action (VLA) baseline (originally from\nthe embodied AI field), called Drive-$\\pi_0$. Specifically, we add Vision MoE\nto Drive-$\\pi_0$ by training a router to select relevant cameras according to\nthe driving context dynamically. This design mirrors human driving cognition,\nwhere drivers selectively attend to crucial visual cues rather than\nexhaustively processing all visual information. In addition, we add Action MoE\nby training another router to activate specialized expert modules for different\ndriving behaviors. Through explicit behavioral specialization, DriveMoE is able\nto handle diverse scenarios without suffering from modes averaging like\nexisting models. In Bench2Drive closed-loop evaluation experiments, DriveMoE\nachieves state-of-the-art (SOTA) performance, demonstrating the effectiveness\nof combining vision and action MoE in autonomous driving tasks. We will release\nour code and models of DriveMoE and Drive-$\\pi_0$.", "AI": {"tldr": "DriveMoE\u662f\u4e00\u79cd\u57fa\u4e8eMixture-of-Experts (MoE)\u67b6\u6784\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u573a\u666f\u4e13\u7528\u89c6\u89c9MoE\u548c\u6280\u80fd\u4e13\u7528\u52a8\u4f5cMoE\uff0c\u52a8\u6001\u5904\u7406\u591a\u89c6\u89d2\u6570\u636e\u548c\u590d\u6742\u9a7e\u9a76\u573a\u666f\uff0c\u5728Bench2Drive\u8bc4\u6d4b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u9ad8\u6548\u5904\u7406\u591a\u89c6\u89d2\u6570\u636e\u5e76\u5e94\u5bf9\u590d\u6742\u573a\u666f\uff0c\u5c24\u5176\u662f\u7f55\u89c1\u9a7e\u9a76\u884c\u4e3a\u3002MoE\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6210\u529f\u8868\u660e\u53c2\u6570\u4e13\u4e1a\u5316\u53ef\u5b9e\u73b0\u5f3a\u6269\u5c55\u6027\u3002", "method": "\u5728Drive-\u03c00\u57fa\u7ebf\u57fa\u7840\u4e0a\uff0c\u589e\u52a0\u89c6\u89c9MoE\uff08\u52a8\u6001\u9009\u62e9\u76f8\u5173\u6444\u50cf\u5934\uff09\u548c\u52a8\u4f5cMoE\uff08\u6fc0\u6d3b\u4e0d\u540c\u9a7e\u9a76\u884c\u4e3a\u7684\u4e13\u5bb6\u6a21\u5757\uff09\uff0c\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u8ba4\u77e5\u3002", "result": "DriveMoE\u5728Bench2Drive\u8bc4\u6d4b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u907f\u514d\u4e86\u6a21\u5f0f\u5e73\u5747\u95ee\u9898\u3002", "conclusion": "DriveMoE\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u52a8\u4f5cMoE\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.16241", "pdf": "https://arxiv.org/pdf/2505.16241", "abs": "https://arxiv.org/abs/2505.16241", "authors": ["Viet-Anh Nguyen", "Shiqian Zhao", "Gia Dao", "Runyi Hu", "Yi Xie", "Luu Anh Tuan"], "title": "Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers", "categories": ["cs.CL"], "comment": null, "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated superior logical\ncapabilities compared to traditional Large Language Models (LLMs), gaining\nsignificant attention. Despite their impressive performance, the potential for\nstronger reasoning abilities to introduce more severe security vulnerabilities\nremains largely underexplored. Existing jailbreak methods often struggle to\nbalance effectiveness with robustness against adaptive safety mechanisms. In\nthis work, we propose SEAL, a novel jailbreak attack that targets LRMs through\nan adaptive encryption pipeline designed to override their reasoning processes\nand evade potential adaptive alignment. Specifically, SEAL introduces a stacked\nencryption approach that combines multiple ciphers to overwhelm the models\nreasoning capabilities, effectively bypassing built-in safety mechanisms. To\nfurther prevent LRMs from developing countermeasures, we incorporate two\ndynamic strategies - random and adaptive - that adjust the cipher length,\norder, and combination. Extensive experiments on real-world reasoning models,\nincluding DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the\neffectiveness of our approach. Notably, SEAL achieves an attack success rate of\n80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant\nmargin of 27.2%. Warning: This paper contains examples of inappropriate,\noffensive, and harmful content.", "AI": {"tldr": "SEAL\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u65b0\u578b\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u5bc6\u7ba1\u9053\u7ed5\u8fc7\u5176\u63a8\u7406\u8fc7\u7a0b\u548c\u5b89\u5168\u673a\u5236\uff0c\u653b\u51fb\u6210\u529f\u7387\u8fbe80.8%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u903b\u8f91\u80fd\u529b\uff0c\u4f46\u5176\u53ef\u80fd\u5f15\u5165\u66f4\u4e25\u91cd\u7684\u5b89\u5168\u6f0f\u6d1e\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u73b0\u6709\u8d8a\u72f1\u65b9\u6cd5\u5728\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "SEAL\u91c7\u7528\u5806\u53e0\u52a0\u5bc6\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u79cd\u5bc6\u7801\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u7b56\u7565\uff08\u968f\u673a\u548c\u81ea\u9002\u5e94\uff09\u8c03\u6574\u5bc6\u7801\u957f\u5ea6\u3001\u987a\u5e8f\u548c\u7ec4\u5408\uff0c\u4ee5\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u3002", "result": "\u5728DeepSeek-R1\u3001Claude Sonnet\u548cOpenAI GPT-o4\u7b49\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSEAL\u653b\u51fb\u6210\u529f\u7387\u8fbe80.8%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SEAL\u5c55\u793a\u4e86LRMs\u5728\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u5728\u6f0f\u6d1e\uff0c\u5f3a\u8c03\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u6a21\u578b\u5b89\u5168\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.16282", "pdf": "https://arxiv.org/pdf/2505.16282", "abs": "https://arxiv.org/abs/2505.16282", "authors": ["Fanbin Lu", "Zhisheng Zhong", "Shu Liu", "Chi-Wing Fu", "Jiaya Jia"], "title": "ARPO:End-to-End Policy Optimization for GUI Agents with Experience Replay", "categories": ["cs.CV"], "comment": null, "summary": "Training large language models (LLMs) as interactive agents for controlling\ngraphical user interfaces (GUIs) presents a unique challenge to optimize\nlong-horizon action sequences with multimodal feedback from complex\nenvironments. While recent works have advanced multi-turn reinforcement\nlearning (RL) for reasoning and tool-using capabilities in LLMs, their\napplication to GUI-based agents remains relatively underexplored due to the\ndifficulty of sparse rewards, delayed feedback, and high rollout costs. In this\npaper, we investigate end-to-end policy optimization for vision-language-based\nGUI agents with the aim of improving performance on complex, long-horizon\ncomputer tasks. We propose Agentic Replay Policy Optimization (ARPO), an\nend-to-end RL approach that augments Group Relative Policy Optimization (GRPO)\nwith a replay buffer to reuse the successful experience across training\niterations. To further stabilize the training process, we propose a task\nselection strategy that filters tasks based on baseline agent performance,\nallowing the agent to focus on learning from informative interactions.\nAdditionally, we compare ARPO with offline preference optimization approaches,\nhighlighting the advantages of policy-based methods in GUI environments.\nExperiments on the OSWorld benchmark demonstrate that ARPO achieves competitive\nresults, establishing a new performance baseline for LLM-based GUI agents\ntrained via reinforcement learning. Our findings underscore the effectiveness\nof reinforcement learning for training multi-turn, vision-language GUI agents\ncapable of managing complex real-world UI interactions. Codes and\nmodels:https://github.com/dvlab-research/ARPO.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARPO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u7684GUI\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u7ecf\u9a8c\u56de\u653e\u548c\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ea4\u4e92\u5f0fGUI\u4ee3\u7406\u9762\u4e34\u957f\u65f6\u7a0b\u52a8\u4f5c\u5e8f\u5217\u4f18\u5316\u548c\u591a\u6a21\u6001\u53cd\u9988\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728GUI\u73af\u5883\u4e2d\u5e94\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faARPO\u65b9\u6cd5\uff0c\u7ed3\u5408GRPO\u548c\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u9009\u62e9\u7b56\u7565\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARPO\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684GUI\u4ee3\u7406\u8bbe\u5b9a\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u591a\u8f6e\u89c6\u89c9\u8bed\u8a00GUI\u4ee3\u7406\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u771f\u5b9eUI\u4ea4\u4e92\u3002"}}
{"id": "2505.16245", "pdf": "https://arxiv.org/pdf/2505.16245", "abs": "https://arxiv.org/abs/2505.16245", "authors": ["Vijeta Deshpande", "Debasmita Ghose", "John D. Patterson", "Roger Beaty", "Anna Rumshisky"], "title": "Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Diverse language model responses are crucial for creative generation,\nopen-ended tasks, and self-improvement training. We show that common diversity\nmetrics, and even reward models used for preference optimization,\nsystematically bias models toward shorter outputs, limiting expressiveness. To\naddress this, we introduce Diverse, not Short (Diverse-NS), a length-controlled\nself-learning framework that improves response diversity while maintaining\nlength parity. By generating and filtering preference data that balances\ndiversity, quality, and length, Diverse-NS enables effective training using\nonly 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,\nDiverse-NS substantially enhances lexical and semantic diversity. We show\nconsistent improvement in diversity with minor reduction or gains in response\nquality on four creative generation tasks: Divergent Associations, Persona\nGeneration, Alternate Uses, and Creative Writing. Surprisingly, experiments\nwith the Olmo-2 model family (7B, and 13B) show that smaller models like\nOlmo-2-7B can serve as effective \"diversity teachers\" for larger models. By\nexplicitly addressing length bias, our method efficiently pushes models toward\nmore diverse and expressive outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiverse-NS\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236\u957f\u5ea6\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u591a\u6837\u6027\uff0c\u4ec5\u97003000\u5bf9\u504f\u597d\u6570\u636e\u5373\u53ef\u6709\u6548\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u591a\u6837\u6027\u6307\u6807\u548c\u5956\u52b1\u6a21\u578b\u504f\u5411\u77ed\u8f93\u51fa\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u7684\u591a\u6837\u6027\u3002", "method": "\u5f15\u5165Diverse-NS\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u548c\u7b5b\u9009\u5e73\u8861\u591a\u6837\u6027\u3001\u8d28\u91cf\u548c\u957f\u5ea6\u7684\u504f\u597d\u6570\u636e\uff0c\u8fdb\u884c\u81ea\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728LLaMA-3.1-8B\u548cOlmo-2\u7cfb\u5217\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bcd\u6c47\u548c\u8bed\u4e49\u591a\u6837\u6027\uff0c\u4e14\u5c0f\u6a21\u578b\u53ef\u4f5c\u4e3a\u5927\u6a21\u578b\u7684\u591a\u6837\u6027\u6307\u5bfc\u3002", "conclusion": "Diverse-NS\u901a\u8fc7\u89e3\u51b3\u957f\u5ea6\u504f\u5dee\uff0c\u9ad8\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2505.16283", "pdf": "https://arxiv.org/pdf/2505.16283", "abs": "https://arxiv.org/abs/2505.16283", "authors": ["Lijian Li", "Yuanpeng He", "Chi-Man Pun"], "title": "Efficient Prototype Consistency Learning in Medical Image Segmentation via Joint Uncertainty and Data Augmentation", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2404.10717", "summary": "Recently, prototype learning has emerged in semi-supervised medical image\nsegmentation and achieved remarkable performance. However, the scarcity of\nlabeled data limits the expressiveness of prototypes in previous methods,\npotentially hindering the complete representation of prototypes for class\nembedding. To overcome this issue, we propose an efficient prototype\nconsistency learning via joint uncertainty quantification and data augmentation\n(EPCL-JUDA) to enhance the semantic expression of prototypes based on the\nframework of Mean-Teacher. The concatenation of original and augmented labeled\ndata is fed into student network to generate expressive prototypes. Then, a\njoint uncertainty quantification method is devised to optimize pseudo-labels\nand generate reliable prototypes for original and augmented unlabeled data\nseparately. High-quality global prototypes for each class are formed by fusing\nlabeled and unlabeled prototypes, which are utilized to generate\nprototype-to-features to conduct consistency learning. Notably, a prototype\nnetwork is proposed to reduce high memory requirements brought by the\nintroduction of augmented data. Extensive experiments on Left Atrium,\nPancreas-NIH, Type B Aortic Dissection datasets demonstrate EPCL-JUDA's\nsuperiority over previous state-of-the-art approaches, confirming the\neffectiveness of our framework. The code will be released soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEPCL-JUDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u6570\u636e\u589e\u5f3a\u63d0\u5347\u539f\u578b\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\u56e0\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u539f\u578b\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5b8c\u6574\u8868\u793a\u7c7b\u522b\u5d4c\u5165\u3002", "method": "\u57fa\u4e8eMean-Teacher\u6846\u67b6\uff0c\u7ed3\u5408\u539f\u59cb\u548c\u589e\u5f3a\u6807\u8bb0\u6570\u636e\u751f\u6210\u539f\u578b\uff0c\u901a\u8fc7\u8054\u5408\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u878d\u5408\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u5c40\u539f\u578b\uff0c\u5e76\u5f15\u5165\u539f\u578b\u7f51\u7edc\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EPCL-JUDA\u901a\u8fc7\u589e\u5f3a\u539f\u578b\u8868\u8fbe\u548c\u4f18\u5316\u4f2a\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2505.16252", "pdf": "https://arxiv.org/pdf/2505.16252", "abs": "https://arxiv.org/abs/2505.16252", "authors": ["Hwiyeong Lee", "Uiji Hwang", "Hyelim Lim", "Taeuk Kim"], "title": "Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Large language models often retain unintended content, prompting growing\ninterest in knowledge unlearning. Recent approaches emphasize localized\nunlearning, which restricts parameter updates to specific regions in an effort\nto remove target knowledge while preserving unrelated general knowledge.\nHowever, their effectiveness remains uncertain due to the lack of robust and\nthorough evaluation of the trade-off between the competing goals of unlearning.\nIn this paper, we begin by revisiting existing localized unlearning approaches.\nWe then conduct controlled experiments to rigorously evaluate whether local\nparameter updates causally contribute to unlearning. Our findings reveal that\nthe set of parameters that must be modified for effective unlearning is not\nstrictly determined, challenging the core assumption of localized unlearning\nthat parameter locality is inherently indicative of effective knowledge\nremoval.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u77e5\u8bc6\u9057\u5fd8\u7684\u95ee\u9898\uff0c\u6307\u51fa\u73b0\u6709\u5c40\u90e8\u9057\u5fd8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5b58\u7591\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6311\u6218\u4e86\u53c2\u6570\u5c40\u90e8\u6027\u4e0e\u6709\u6548\u9057\u5fd8\u4e4b\u95f4\u7684\u6838\u5fc3\u5047\u8bbe\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u4fdd\u7559\u4e0d\u5fc5\u8981\u5185\u5bb9\uff0c\u5f15\u53d1\u5bf9\u77e5\u8bc6\u9057\u5fd8\u7684\u5174\u8da3\u3002\u73b0\u6709\u5c40\u90e8\u9057\u5fd8\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u5c40\u90e8\u53c2\u6570\u66f4\u65b0\u79fb\u9664\u76ee\u6807\u77e5\u8bc6\uff0c\u4f46\u5176\u6709\u6548\u6027\u7f3a\u4e4f\u9a8c\u8bc1\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u73b0\u6709\u5c40\u90e8\u9057\u5fd8\u65b9\u6cd5\uff0c\u5e76\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u4e25\u683c\u8bc4\u4f30\u5c40\u90e8\u53c2\u6570\u66f4\u65b0\u662f\u5426\u5bf9\u9057\u5fd8\u6709\u56e0\u679c\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6709\u6548\u9057\u5fd8\u6240\u9700\u7684\u53c2\u6570\u4fee\u6539\u5e76\u975e\u4e25\u683c\u786e\u5b9a\uff0c\u6311\u6218\u4e86\u5c40\u90e8\u9057\u5fd8\u7684\u6838\u5fc3\u5047\u8bbe\u3002", "conclusion": "\u53c2\u6570\u5c40\u90e8\u6027\u5e76\u975e\u6709\u6548\u77e5\u8bc6\u79fb\u9664\u7684\u5fc5\u7136\u6307\u6807\uff0c\u5c40\u90e8\u9057\u5fd8\u65b9\u6cd5\u7684\u6709\u6548\u6027\u9700\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2505.16294", "pdf": "https://arxiv.org/pdf/2505.16294", "abs": "https://arxiv.org/abs/2505.16294", "authors": ["Yufei Yin", "Lechao Cheng", "Wengang Zhou", "Jiajun Deng", "Zhou Yu", "Houqiang Li"], "title": "Self-Classification Enhancement and Correction for Weakly Supervised Object Detection", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "In recent years, weakly supervised object detection (WSOD) has attracted much\nattention due to its low labeling cost. The success of recent WSOD models is\noften ascribed to the two-stage multi-class classification (MCC) task, i.e.,\nmultiple instance learning and online classification refinement. Despite\nachieving non-trivial progresses, these methods overlook potential\nclassification ambiguities between these two MCC tasks and fail to leverage\ntheir unique strengths. In this work, we introduce a novel WSOD framework to\nameliorate these two issues. For one thing, we propose a self-classification\nenhancement module that integrates intra-class binary classification (ICBC) to\nbridge the gap between the two distinct MCC tasks. The ICBC task enhances the\nnetwork's discrimination between positive and mis-located samples in a\nclass-wise manner and forges a mutually reinforcing relationship with the MCC\ntask. For another, we propose a self-classification correction algorithm during\ninference, which combines the results of both MCC tasks to effectively reduce\nthe mis-classified predictions. Extensive experiments on the prevalent VOC 2007\n& 2012 datasets demonstrate the superior performance of our framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u5206\u7c7b\u589e\u5f3a\u6a21\u5757\u548c\u81ea\u5206\u7c7b\u6821\u6b63\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5206\u7c7b\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728VOC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\uff08WSOD\uff09\u56e0\u6807\u6ce8\u6210\u672c\u4f4e\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b58\u5728\u5206\u7c7b\u6a21\u7cca\u95ee\u9898\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u81ea\u5206\u7c7b\u589e\u5f3a\u6a21\u5757\uff08ICBC\uff09\u4ee5\u5f25\u5408\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8bbe\u8ba1\u81ea\u5206\u7c7b\u6821\u6b63\u7b97\u6cd5\u4ee5\u51cf\u5c11\u8bef\u5206\u7c7b\u9884\u6d4b\u3002", "result": "\u5728VOC 2007\u548c2012\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "\u65b0\u6846\u67b6\u901a\u8fc7ICBC\u548c\u6821\u6b63\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5f31\u76d1\u7763\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16258", "pdf": "https://arxiv.org/pdf/2505.16258", "abs": "https://arxiv.org/abs/2505.16258", "authors": ["Aashish Anantha Ramakrishnan", "Aadarsh Anantha Ramakrishnan", "Dongwon Lee"], "title": "IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection", "categories": ["cs.CL", "cs.AI", "cs.CV", "68T50", "I.2.7; I.2.10"], "comment": null, "summary": "Interpreting figurative language such as sarcasm across multi-modal inputs\npresents unique challenges, often requiring task-specific fine-tuning and\nextensive reasoning steps. However, current Chain-of-Thought approaches do not\nefficiently leverage the same cognitive processes that enable humans to\nidentify sarcasm. We present IRONIC, an in-context learning framework that\nleverages Multi-modal Coherence Relations to analyze referential, analogical\nand pragmatic image-text linkages. Our experiments show that IRONIC achieves\nstate-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across\ndifferent baselines. This demonstrates the need for incorporating linguistic\nand cognitive insights into the design of multi-modal reasoning strategies. Our\ncode is available at: https://github.com/aashish2000/IRONIC", "AI": {"tldr": "IRONIC\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u8fde\u8d2f\u5173\u7cfb\u5206\u6790\uff0c\u96f6\u6837\u672c\u68c0\u6d4b\u591a\u6a21\u6001\u8bbd\u523a\uff0c\u6027\u80fd\u9886\u5148\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u9ad8\u6548\u5229\u7528\u4eba\u7c7b\u8bc6\u522b\u8bbd\u523a\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u8fde\u8d2f\u5173\u7cfb\u5206\u6790\u56fe\u50cf-\u6587\u672c\u5173\u8054\u3002", "result": "\u5728\u96f6\u6837\u672c\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u9700\u5c06\u8bed\u8a00\u548c\u8ba4\u77e5\u6d1e\u5bdf\u878d\u5165\u591a\u6a21\u6001\u63a8\u7406\u7b56\u7565\u8bbe\u8ba1\u3002"}}
{"id": "2505.16304", "pdf": "https://arxiv.org/pdf/2505.16304", "abs": "https://arxiv.org/abs/2505.16304", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "title": "SAMba-UNet: Synergizing SAM2 and Mamba in UNet with Heterogeneous Aggregation for Cardiac MRI Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenge of complex pathological feature extraction in\nautomated cardiac MRI segmentation, this study proposes an innovative\ndual-encoder architecture named SAMba-UNet. The framework achieves cross-modal\nfeature collaborative learning by integrating the vision foundation model SAM2,\nthe state-space model Mamba, and the classical UNet. To mitigate domain\ndiscrepancies between medical and natural images, a Dynamic Feature Fusion\nRefiner is designed, which enhances small lesion feature extraction through\nmulti-scale pooling and a dual-path calibration mechanism across channel and\nspatial dimensions. Furthermore, a Heterogeneous Omni-Attention Convergence\nModule (HOACM) is introduced, combining global contextual attention with\nbranch-selective emphasis mechanisms to effectively fuse SAM2's local\npositional semantics and Mamba's long-range dependency modeling capabilities.\nExperiments on the ACDC cardiac MRI dataset demonstrate that the proposed model\nachieves a Dice coefficient of 0.9103 and an HD95 boundary error of 1.0859 mm,\nsignificantly outperforming existing methods, particularly in boundary\nlocalization for complex pathological structures such as right ventricular\nanomalies. This work provides an efficient and reliable solution for automated\ncardiac disease diagnosis, and the code will be open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMba-UNet\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u5fc3\u810fMRI\u5206\u5272\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u878d\u5408\u548c\u5f02\u6784\u5168\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u590d\u6742\u75c5\u7406\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5fc3\u810fMRI\u5206\u5272\u4e2d\u590d\u6742\u75c5\u7406\u7279\u5f81\u63d0\u53d6\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5c0f\u75c5\u7076\u548c\u8fb9\u754c\u5b9a\u4f4d\u95ee\u9898\u3002", "method": "\u7ed3\u5408SAM2\u3001Mamba\u548cUNet\uff0c\u8bbe\u8ba1\u52a8\u6001\u7279\u5f81\u878d\u5408\u5668\u548c\u5f02\u6784\u5168\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728ACDC\u6570\u636e\u96c6\u4e0a\uff0cDice\u7cfb\u6570\u8fbe0.9103\uff0cHD95\u8fb9\u754c\u8bef\u5dee\u4e3a1.0859 mm\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2505.16270", "pdf": "https://arxiv.org/pdf/2505.16270", "abs": "https://arxiv.org/abs/2505.16270", "authors": ["Jiaru Zou", "Yikun Ban", "Zihao Li", "Yunzhe Qi", "Ruizhong Qiu", "Ling Yang", "Jingrui He"], "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "33 pages, 7 figures", "summary": "Large language models are typically adapted to downstream tasks through\nsupervised fine-tuning on domain-specific data. While standard fine-tuning\nfocuses on minimizing generation loss to optimize model parameters, we take a\ndeeper step by retaining and leveraging the model's own learning signals,\nanalogous to how human learners reflect on past mistakes to improve future\nperformance. We first introduce the concept of Mistake Log to systematically\ntrack the model's learning behavior and recurring errors throughout\nfine-tuning. Treating the original transformer-based model as the Pilot, we\ncorrespondingly design a Copilot model to refine the Pilot's inference\nperformance via logits rectification. We name the overall Pilot-Copilot\nframework the Transformer Copilot, which introduces (i) a novel Copilot model\ndesign, (ii) a joint training paradigm where the Copilot continuously learns\nfrom the evolving Mistake Log alongside the Pilot, and (iii) a fused inference\nparadigm where the Copilot rectifies the Pilot's logits for enhanced\ngeneration. We provide both theoretical and empirical analyses on our new\nlearning framework. Experiments on 12 benchmarks spanning commonsense,\narithmetic, and recommendation tasks demonstrate that Transformer Copilot\nconsistently improves performance by up to 34.5%, while introducing marginal\ncomputational overhead to Pilot models and exhibiting strong scalability and\ntransferability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTransformer Copilot\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8bb0\u5f55\u6a21\u578b\u9519\u8bef\u5e76\u8bbe\u8ba1Copilot\u6a21\u578b\u6765\u4f18\u5316\u751f\u6210\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u4ec5\u5173\u6ce8\u751f\u6210\u635f\u5931\u7684\u6700\u5c0f\u5316\uff0c\u800c\u5ffd\u7565\u4e86\u6a21\u578b\u81ea\u8eab\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8bb0\u5f55\u548c\u5206\u6790\u6a21\u578b\u7684\u9519\u8bef\u884c\u4e3a\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165Mistake Log\u8bb0\u5f55\u9519\u8bef\uff0c\u8bbe\u8ba1Copilot\u6a21\u578b\u901a\u8fc7logits\u6821\u6b63\u4f18\u5316Pilot\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u8054\u5408\u8bad\u7ec3\u548c\u878d\u5408\u63a8\u7406\u8303\u5f0f\u3002", "result": "\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe34.5%\uff0c\u8ba1\u7b97\u5f00\u9500\u4f4e\u4e14\u5177\u6709\u5f3a\u6269\u5c55\u6027\u548c\u8fc1\u79fb\u6027\u3002", "conclusion": "Transformer Copilot\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002"}}
{"id": "2505.16310", "pdf": "https://arxiv.org/pdf/2505.16310", "abs": "https://arxiv.org/abs/2505.16310", "authors": ["Gaurav Kumar", "Soham Satyadharma", "Harpreet Singh"], "title": "Paired and Unpaired Image to Image Translation using Generative Adversarial Networks", "categories": ["cs.CV", "eess.IV"], "comment": "6 pages", "summary": "Image to image translation is an active area of research in the field of\ncomputer vision, enabling the generation of new images with different styles,\ntextures, or resolutions while preserving their characteristic properties.\nRecent architectures leverage Generative Adversarial Networks (GANs) to\ntransform input images from one domain to another. In this work, we focus on\nthe study of both paired and unpaired image translation across multiple image\ndomains. For the paired task, we used a conditional GAN model, and for the\nunpaired task, we trained it using cycle consistency loss. We experimented with\ndifferent types of loss functions, multiple Patch-GAN sizes, and model\narchitectures. New quantitative metrics - precision, recall, and FID score -\nwere used for analysis. In addition, a qualitative study of the results of\ndifferent experiments was conducted.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57fa\u4e8eGAN\u7684\u6210\u5bf9\u548c\u975e\u6210\u5bf9\u56fe\u50cf\u7ffb\u8bd1\uff0c\u4f7f\u7528\u6761\u4ef6GAN\u548c\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u6cd5\u8bc4\u4f30\u7ed3\u679c\u3002", "motivation": "\u63a2\u7d22\u56fe\u50cf\u7ffb\u8bd1\u9886\u57df\uff0c\u7279\u522b\u662f\u6210\u5bf9\u548c\u975e\u6210\u5bf9\u4efb\u52a1\uff0c\u4ee5\u751f\u6210\u5177\u6709\u4e0d\u540c\u98ce\u683c\u6216\u7eb9\u7406\u7684\u65b0\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u6761\u4ef6GAN\u5904\u7406\u6210\u5bf9\u4efb\u52a1\uff0c\u5faa\u73af\u4e00\u81f4\u6027\u635f\u5931\u5904\u7406\u975e\u6210\u5bf9\u4efb\u52a1\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u635f\u5931\u51fd\u6570\u3001Patch-GAN\u5c3a\u5bf8\u548c\u67b6\u6784\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\uff08\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001FID\u5206\u6570\uff09\u548c\u5b9a\u6027\u5206\u6790\u8bc4\u4f30\u4e86\u4e0d\u540c\u5b9e\u9a8c\u7684\u7ed3\u679c\u3002", "conclusion": "GAN\u5728\u56fe\u50cf\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2505.16277", "pdf": "https://arxiv.org/pdf/2505.16277", "abs": "https://arxiv.org/abs/2505.16277", "authors": ["Sheng-Fu Wang", "Laurent Prevot", "Jou-an Chi", "Ri-Sheng Huang", "Shu-Kai Hsieh"], "title": "Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility", "categories": ["cs.CL"], "comment": "The 14th Workshop on Cognitive Modeling and Computational Linguistics\n  (CMCL). May 3, 2025. Collocated with NAACL 2025", "summary": "The achievements of Large Language Models in Natural Language Processing,\nespecially for high-resource languages, call for a better understanding of\ntheir characteristics from a cognitive perspective. Researchers have attempted\nto evaluate artificial models by testing their ability to predict behavioral\n(e.g., eye-tracking fixations) and physiological (e.g., brain responses)\nvariables during language processing (e.g., reading/listening). In this paper,\nwe propose using spontaneous speech corpora to derive production variables\n(speech reductions, prosodic prominences) and applying them in a similar\nfashion. More precisely, we extract. We then test models trained with a\nstandard procedure on different pretraining datasets (written, spoken, and\nmixed genres) for their ability to predict these two variables. Our results\nshow that, after some fine-tuning, the models can predict these production\nvariables well above baselines. We also observe that spoken genre training data\nprovides more accurate predictions than written genres. These results\ncontribute to the broader effort of using high-quality speech corpora as\nbenchmarks for LLMs.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u81ea\u53d1\u8bed\u97f3\u8bed\u6599\u5e93\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u8bed\u8a00\u751f\u6210\u53d8\u91cf\uff08\u5982\u8bed\u97f3\u7f29\u51cf\u548c\u97f5\u5f8b\u7a81\u51fa\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u53e3\u8bed\u8bad\u7ec3\u6570\u636e\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba4\u77e5\u89c6\u89d2\u4e0b\u7684\u7279\u6027\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u9884\u6d4b\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u884c\u4e3a\u4e0e\u751f\u7406\u53d8\u91cf\u6765\u8bc4\u4f30\u6a21\u578b\u3002", "method": "\u4ece\u81ea\u53d1\u8bed\u97f3\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u751f\u6210\u53d8\u91cf\uff08\u8bed\u97f3\u7f29\u51cf\u3001\u97f5\u5f8b\u7a81\u51fa\uff09\uff0c\u6d4b\u8bd5\u4e0d\u540c\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff08\u4e66\u9762\u3001\u53e3\u8bed\u53ca\u6df7\u5408\u7c7b\u578b\uff09\u8bad\u7ec3\u7684\u6a21\u578b\u5bf9\u8fd9\u4e9b\u53d8\u91cf\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u7ecf\u8fc7\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u80fd\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u9884\u6d4b\u8fd9\u4e9b\u53d8\u91cf\uff0c\u4e14\u53e3\u8bed\u8bad\u7ec3\u6570\u636e\u7684\u9884\u6d4b\u6548\u679c\u4f18\u4e8e\u4e66\u9762\u6570\u636e\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u8bed\u97f3\u8bed\u6599\u5e93\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u57fa\u51c6\uff0c\u53e3\u8bed\u6570\u636e\u5bf9\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2505.16313", "pdf": "https://arxiv.org/pdf/2505.16313", "abs": "https://arxiv.org/abs/2505.16313", "authors": ["Arjhun Swaminathan", "Mete Akg\u00fcn"], "title": "Accelerating Targeted Hard-Label Adversarial Attacks in Low-Query Black-Box Settings", "categories": ["cs.CV", "cs.LG"], "comment": "This paper contains 11 pages, 7 figures and 3 tables. For associated\n  supplementary code, see https://github.com/mdppml/TEA", "summary": "Deep neural networks for image classification remain vulnerable to\nadversarial examples -- small, imperceptible perturbations that induce\nmisclassifications. In black-box settings, where only the final prediction is\naccessible, crafting targeted attacks that aim to misclassify into a specific\ntarget class is particularly challenging due to narrow decision regions.\nCurrent state-of-the-art methods often exploit the geometric properties of the\ndecision boundary separating a source image and a target image rather than\nincorporating information from the images themselves. In contrast, we propose\nTargeted Edge-informed Attack (TEA), a novel attack that utilizes edge\ninformation from the target image to carefully perturb it, thereby producing an\nadversarial image that is closer to the source image while still achieving the\ndesired target classification. Our approach consistently outperforms current\nstate-of-the-art methods across different models in low query settings (nearly\n70\\% fewer queries are used), a scenario especially relevant in real-world\napplications with limited queries and black-box access. Furthermore, by\nefficiently generating a suitable adversarial example, TEA provides an improved\ntarget initialization for established geometry-based attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEA\u7684\u65b0\u578b\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u5229\u7528\u76ee\u6807\u56fe\u50cf\u7684\u8fb9\u7f18\u4fe1\u606f\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5728\u4f4e\u67e5\u8be2\u91cf\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9\u9ed1\u76d2\u8bbe\u7f6e\u4e2d\u76ee\u6807\u653b\u51fb\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u51b3\u7b56\u8fb9\u754c\u7684\u51e0\u4f55\u7279\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u56fe\u50cf\u672c\u8eab\u7684\u4fe1\u606f\u3002", "method": "\u63d0\u51faTEA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u56fe\u50cf\u7684\u8fb9\u7f18\u4fe1\u606f\uff0c\u751f\u6210\u66f4\u63a5\u8fd1\u6e90\u56fe\u50cf\u4f46\u4ecd\u80fd\u5b9e\u73b0\u76ee\u6807\u5206\u7c7b\u7684\u5bf9\u6297\u6837\u672c\u3002", "result": "TEA\u5728\u4f4e\u67e5\u8be2\u91cf\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u51cf\u5c11\u8fd170%\u67e5\u8be2\u91cf\uff09\uff0c\u5e76\u4e3a\u51e0\u4f55\u653b\u51fb\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u521d\u59cb\u5316\u3002", "conclusion": "TEA\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u8fb9\u7f18\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ed1\u76d2\u573a\u666f\u3002"}}
{"id": "2505.16281", "pdf": "https://arxiv.org/pdf/2505.16281", "abs": "https://arxiv.org/abs/2505.16281", "authors": ["Shijie Zhang", "Renhao Li", "Songsheng Wang", "Philipp Koehn", "Min Yang", "Derek F. Wong"], "title": "HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) enables flexible and\ninterpretable automatic evaluations. In the field of machine translation\nevaluation, utilizing LLMs with translation error annotations based on\nMultidimensional Quality Metrics (MQM) yields more human-aligned judgments.\nHowever, current LLM-based evaluation methods still face challenges in\naccurately identifying error spans and assessing their severity. In this paper,\nwe propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation\nEvaluation. We argue that existing approaches inadequately exploit the\nfine-grained structural and semantic information within the MQM hierarchy. To\naddress this, we develop a hierarchical multi-agent system grounded in the MQM\nerror typology, enabling granular evaluation of subtype errors. Two key\nstrategies are incorporated to further mitigate systemic hallucinations within\nthe framework: the utilization of the model's self-reflection capability and\nthe facilitation of agent discussion involving asymmetric information.\nEmpirically, HiMATE outperforms competitive baselines across different datasets\nin conducting human-aligned evaluations. Further analyses underscore its\nsignificant advantage in error span detection and severity assessment,\nachieving an average F1-score improvement of 89% over the best-performing\nbaseline. We make our code and data publicly available at\nhttps://anonymous.4open.science/r/HiMATE-Anony.", "AI": {"tldr": "HiMATE\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528MQM\u9519\u8bef\u7c7b\u578b\u5b66\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u68c0\u6d4b\u548c\u4e25\u91cd\u6027\u8bc4\u4f30\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u65b9\u6cd5\u5728\u9519\u8bef\u8303\u56f4\u548c\u4e25\u91cd\u6027\u8bc4\u4f30\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528MQM\u5c42\u6b21\u7ed3\u6784\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "method": "\u63d0\u51faHiMATE\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u6a21\u578b\u81ea\u53cd\u601d\u80fd\u529b\u548c\u4ee3\u7406\u95f4\u975e\u5bf9\u79f0\u4fe1\u606f\u8ba8\u8bba\uff0c\u4f18\u5316\u9519\u8bef\u5b50\u7c7b\u578b\u8bc4\u4f30\u3002", "result": "HiMATE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9519\u8bef\u68c0\u6d4b\u548c\u4e25\u91cd\u6027\u8bc4\u4f30\u7684F1\u5206\u6570\u5e73\u5747\u63d0\u534789%\u3002", "conclusion": "HiMATE\u901a\u8fc7\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.16314", "pdf": "https://arxiv.org/pdf/2505.16314", "abs": "https://arxiv.org/abs/2505.16314", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I)\ngeneration model quality assessment, which will be held in conjunction with the\nNew Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025.\nThe aim of this challenge is to address the fine-grained quality assessment of\ntext-to-image generation models. This challenge evaluates text-to-image models\nfrom two aspects: image-text alignment and image structural distortion\ndetection, and is divided into the alignment track and the structural track.\nThe alignment track uses the EvalMuse-40K, which contains around 40K\nAI-Generated Images (AIGIs) generated by 20 popular generative models. The\nalignment track has a total of 371 registered participants. A total of 1,883\nsubmissions are received in the development phase, and 507 submissions are\nreceived in the test phase. Finally, 12 participating teams submitted their\nmodels and fact sheets. The structure track uses the EvalMuse-Structure, which\ncontains 10,000 AI-Generated Images (AIGIs) with corresponding structural\ndistortion mask. A total of 211 participants have registered in the structure\ntrack. A total of 1155 submissions are received in the development phase, and\n487 submissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Almost all methods have achieved better\nresults than baseline methods, and the winning methods in both tracks have\ndemonstrated superior prediction performance on T2I model quality assessment.", "AI": {"tldr": "NTIRE 2025\u6311\u6218\u8d5b\u805a\u7126\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u5206\u4e3a\u5bf9\u9f50\u548c\u7ed3\u6784\u4e24\u4e2a\u8d5b\u9053\uff0c\u5438\u5f15\u4e86\u5927\u91cf\u53c2\u4e0e\u8005\uff0c\u6700\u7ec8\u83b7\u80dc\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\uff0c\u7279\u522b\u662f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u548c\u56fe\u50cf\u7ed3\u6784\u5931\u771f\u68c0\u6d4b\u3002", "method": "\u6311\u6218\u8d5b\u5206\u4e3a\u5bf9\u9f50\u8d5b\u9053\uff08\u4f7f\u7528EvalMuse-40K\u6570\u636e\u96c6\uff09\u548c\u7ed3\u6784\u8d5b\u9053\uff08\u4f7f\u7528EvalMuse-Structure\u6570\u636e\u96c6\uff09\uff0c\u53c2\u4e0e\u8005\u63d0\u4ea4\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5bf9\u9f50\u8d5b\u9053\u6709371\u540d\u6ce8\u518c\u8005\uff0c1883\u4efd\u5f00\u53d1\u9636\u6bb5\u63d0\u4ea4\uff0c507\u4efd\u6d4b\u8bd5\u9636\u6bb5\u63d0\u4ea4\uff1b\u7ed3\u6784\u8d5b\u9053\u6709211\u540d\u6ce8\u518c\u8005\uff0c1155\u4efd\u5f00\u53d1\u9636\u6bb5\u63d0\u4ea4\uff0c487\u4efd\u6d4b\u8bd5\u9636\u6bb5\u63d0\u4ea4\u3002\u51e0\u4e4e\u6240\u6709\u65b9\u6cd5\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u83b7\u80dc\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8d28\u91cf\u8bc4\u4f30\u7684\u7814\u7a76\uff0c\u83b7\u80dc\u65b9\u6cd5\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.16293", "pdf": "https://arxiv.org/pdf/2505.16293", "abs": "https://arxiv.org/abs/2505.16293", "authors": ["Rishabh Maheshwary", "Masoud Hashemi", "Khyati Mahajan", "Shiva Krishna Reddy Malay", "Sai Rajeswar", "Sathwik Tejaswi Madhusudhan", "Spandana Gella", "Vikas Yadav"], "title": "Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA", "categories": ["cs.CL"], "comment": null, "summary": "Iterative RAG for multi-hop question answering faces challenges with lengthy\ncontexts and the buildup of irrelevant information. This hinders a model's\ncapacity to process and reason over retrieved content and limits performance.\nWhile recent methods focus on compressing retrieved information, they are\neither restricted to single-round RAG, require finetuning or lack scalability\nin iterative RAG. To address these challenges, we propose Notes Writing, a\nmethod that generates concise and relevant notes from retrieved documents at\neach step, thereby reducing noise and retaining only essential information.\nThis indirectly increases the effective context length of Large Language Models\n(LLMs), enabling them to reason and plan more effectively while processing\nlarger volumes of input text. Notes Writing is framework agnostic and can be\nintegrated with different iterative RAG methods. We demonstrate its\neffectiveness with three iterative RAG methods, across two models and four\nevaluation datasets. Notes writing yields an average improvement of 15.6\npercentage points overall, with minimal increase in output tokens.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNotes Writing\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fed\u4ee3RAG\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u56e0\u4e0a\u4e0b\u6587\u8fc7\u957f\u548c\u65e0\u5173\u4fe1\u606f\u79ef\u7d2f\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u7b80\u6d01\u7b14\u8bb0\u51cf\u5c11\u566a\u58f0\uff0c\u95f4\u63a5\u63d0\u5347LLM\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534715.6%\u3002", "motivation": "\u8fed\u4ee3RAG\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u8fc7\u957f\u548c\u65e0\u5173\u4fe1\u606f\u79ef\u7d2f\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5904\u7406\u68c0\u7d22\u5185\u5bb9\u7684\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e\u5355\u8f6eRAG\u6216\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51faNotes Writing\u65b9\u6cd5\uff0c\u5728\u6bcf\u4e00\u6b65\u4ece\u68c0\u7d22\u6587\u6863\u751f\u6210\u7b80\u6d01\u7b14\u8bb0\uff0c\u51cf\u5c11\u566a\u58f0\u5e76\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0c\u95f4\u63a5\u63d0\u5347LLM\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u3002", "result": "\u5728\u4e09\u79cd\u8fed\u4ee3RAG\u65b9\u6cd5\u3001\u4e24\u79cd\u6a21\u578b\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534715.6%\uff0c\u8f93\u51fa\u6807\u8bb0\u589e\u52a0\u6781\u5c11\u3002", "conclusion": "Notes Writing\u662f\u4e00\u79cd\u6846\u67b6\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u5347\u8fed\u4ee3RAG\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u3002"}}
{"id": "2505.16318", "pdf": "https://arxiv.org/pdf/2505.16318", "abs": "https://arxiv.org/abs/2505.16318", "authors": ["Hossein Khalili", "Seongbin Park", "Venkat Bollapragada", "Nader Sehatbakhsh"], "title": "SuperPure: Efficient Purification of Localized and Distributed Adversarial Patches via Super-Resolution GAN Models", "categories": ["cs.CV", "cs.CR", "eess.IV"], "comment": null, "summary": "As vision-based machine learning models are increasingly integrated into\nautonomous and cyber-physical systems, concerns about (physical) adversarial\npatch attacks are growing. While state-of-the-art defenses can achieve\ncertified robustness with minimal impact on utility against highly-concentrated\nlocalized patch attacks, they fall short in two important areas: (i)\nState-of-the-art methods are vulnerable to low-noise distributed patches where\nperturbations are subtly dispersed to evade detection or masking, as shown\nrecently by the DorPatch attack; (ii) Achieving high robustness with\nstate-of-the-art methods is extremely time and resource-consuming, rendering\nthem impractical for latency-sensitive applications in many cyber-physical\nsystems.\n  To address both robustness and latency issues, this paper proposes a new\ndefense strategy for adversarial patch attacks called SuperPure. The key\nnovelty is developing a pixel-wise masking scheme that is robust against both\ndistributed and localized patches. The masking involves leveraging a GAN-based\nsuper-resolution scheme to gradually purify the image from adversarial patches.\nOur extensive evaluations using ImageNet and two standard classifiers, ResNet\nand EfficientNet, show that SuperPure advances the state-of-the-art in three\nmajor directions: (i) it improves the robustness against conventional localized\npatches by more than 20%, on average, while also improving top-1 clean accuracy\nby almost 10%; (ii) It achieves 58% robustness against distributed patch\nattacks (as opposed to 0% in state-of-the-art method, PatchCleanser); (iii) It\ndecreases the defense end-to-end latency by over 98% compared to PatchCleanser.\nOur further analysis shows that SuperPure is robust against white-box attacks\nand different patch sizes. Our code is open-source.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSuperPure\u7684\u65b0\u9632\u5fa1\u7b56\u7565\uff0c\u7528\u4e8e\u5bf9\u6297\u5206\u5e03\u5f0f\u548c\u5c40\u90e8\u5316\u7684\u5bf9\u6297\u8865\u4e01\u653b\u51fb\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u63a9\u7801\u548cGAN\u8d85\u5206\u8fa8\u7387\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u9632\u5fa1\u65b9\u6cd5\u5bf9\u5206\u5e03\u5f0f\u8865\u4e01\u653b\u51fb\u65e0\u6548\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u91c7\u7528\u50cf\u7d20\u7ea7\u63a9\u7801\u548cGAN\u8d85\u5206\u8fa8\u7387\u6280\u672f\u9010\u6b65\u51c0\u5316\u56fe\u50cf\u4e2d\u7684\u5bf9\u6297\u8865\u4e01\u3002", "result": "SuperPure\u5728\u5c40\u90e8\u5316\u8865\u4e01\u653b\u51fb\u4e0a\u7684\u9c81\u68d2\u6027\u63d0\u534720%\uff0c\u5bf9\u5206\u5e03\u5f0f\u8865\u4e01\u653b\u51fb\u7684\u9c81\u68d2\u6027\u8fbe\u523058%\uff0c\u540c\u65f6\u964d\u4f4e\u4e8698%\u7684\u5ef6\u8fdf\u3002", "conclusion": "SuperPure\u5728\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u5b9e\u7528\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7cfb\u7edf\u3002"}}
{"id": "2505.16297", "pdf": "https://arxiv.org/pdf/2505.16297", "abs": "https://arxiv.org/abs/2505.16297", "authors": ["Seongryong Jung", "Suwan Yoon", "DongGeon Kim", "Hwanhee Lee"], "title": "ToDi: Token-wise Distillation via Fine-Grained Divergence Control", "categories": ["cs.CL"], "comment": "13 pages, 7 figures", "summary": "Large language models (LLMs) offer impressive performance but are impractical\nfor resource-constrained deployment due to high latency and energy consumption.\nKnowledge distillation (KD) addresses this by transferring knowledge from a\nlarge teacher to a smaller student model. However, conventional KD, notably\napproaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence\nloss across the entire vocabulary, neglecting token-level prediction\ndiscrepancies. By investigating these representative divergences via gradient\nanalysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses\noverestimated ones, showing their complementary roles. Based on this\nobservation, we propose Token-wise Distillation (ToDi), a novel method that\nadaptively combines FKL and RKL per token using a sigmoid-based weighting\nfunction derived from the teacher-student probability log-ratio. ToDi\ndynamically emphasizes the appropriate divergence for each token, enabling\nprecise distribution alignment. We demonstrate that ToDi consistently\noutperforms recent distillation baselines using uniform or less granular\nstrategies across instruction-following benchmarks. Extensive ablation studies\nand efficiency analysis further validate ToDi's effectiveness and practicality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToDi\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7ed3\u5408FKL\u548cRKL\u7684\u4e92\u8865\u4f5c\u7528\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9ad8\u5ef6\u8fdf\u548c\u80fd\u8017\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u800c\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08\u5982FKL\u548cRKL\uff09\u5ffd\u7565\u4e86\u8bcd\u6c47\u8868\u4e2d\u4e0d\u540ctoken\u7684\u9884\u6d4b\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u5206\u6790\u63ed\u793aFKL\u548cRKL\u7684\u4e92\u8865\u4f5c\u7528\uff0c\u63d0\u51faToDi\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u4e8esigmoid\u7684\u6743\u91cd\u51fd\u6570\u52a8\u6001\u8c03\u6574\u6bcftoken\u7684FKL\u548cRKL\u7ec4\u5408\u3002", "result": "ToDi\u5728\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u548c\u6548\u7387\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "ToDi\u901a\u8fc7\u52a8\u6001\u8c03\u6574FKL\u548cRKL\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16321", "pdf": "https://arxiv.org/pdf/2505.16321", "abs": "https://arxiv.org/abs/2505.16321", "authors": ["Jie Zhao", "Xin Chen", "Yongsheng Yuan", "Michael Felsberg", "Dong Wang", "Huchuan Lu"], "title": "Efficient Motion Prompt Learning for Robust Visual Tracking", "categories": ["cs.CV"], "comment": "Accepted by ICML2025", "summary": "Due to the challenges of processing temporal information, most trackers\ndepend solely on visual discriminability and overlook the unique temporal\ncoherence of video data. In this paper, we propose a lightweight and\nplug-and-play motion prompt tracking method. It can be easily integrated into\nexisting vision-based trackers to build a joint tracking framework leveraging\nboth motion and vision cues, thereby achieving robust tracking through\nefficient prompt learning. A motion encoder with three different positional\nencodings is proposed to encode the long-term motion trajectory into the visual\nembedding space, while a fusion decoder and an adaptive weight mechanism are\ndesigned to dynamically fuse visual and motion features. We integrate our\nmotion module into three different trackers with five models in total.\nExperiments on seven challenging tracking benchmarks demonstrate that the\nproposed motion module significantly improves the robustness of vision-based\ntrackers, with minimal training costs and negligible speed sacrifice. Code is\navailable at https://github.com/zj5559/Motion-Prompt-Tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u7684\u8fd0\u52a8\u63d0\u793a\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u548c\u89c6\u89c9\u7ebf\u7d22\u63d0\u5347\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u5668\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u533a\u5206\u6027\uff0c\u5ffd\u89c6\u4e86\u89c6\u9891\u6570\u636e\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u8fd0\u52a8\u7f16\u7801\u5668\u3001\u878d\u5408\u89e3\u7801\u5668\u548c\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u5c06\u8fd0\u52a8\u8f68\u8ff9\u7f16\u7801\u5230\u89c6\u89c9\u5d4c\u5165\u7a7a\u95f4\u5e76\u52a8\u6001\u878d\u5408\u7279\u5f81\u3002", "result": "\u5728\u4e03\u4e2a\u6311\u6218\u6027\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8ddf\u8e2a\u5668\u7684\u9c81\u68d2\u6027\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e14\u901f\u5ea6\u727a\u7272\u5c0f\u3002", "conclusion": "\u8fd0\u52a8\u63d0\u793a\u6a21\u5757\u80fd\u6709\u6548\u589e\u5f3a\u73b0\u6709\u89c6\u89c9\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2505.16303", "pdf": "https://arxiv.org/pdf/2505.16303", "abs": "https://arxiv.org/abs/2505.16303", "authors": ["Haochen Shi", "Tianshi Zheng", "Weiqi Wang", "Baixuan Xu", "Chunyang Li", "Chunkit Chan", "Tao Fan", "Yangqiu Song", "Qiang Yang"], "title": "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Large Language Model (LLM) routing is a pivotal technique for navigating a\ndiverse landscape of LLMs, aiming to select the best-performing LLMs tailored\nto the domains of user queries, while managing computational resources.\nHowever, current routing approaches often face limitations in scalability when\ndealing with a large pool of specialized LLMs, or in their adaptability to\nextending model scope and evolving capability domains. To overcome those\nchallenges, we propose InferenceDynamics, a flexible and scalable\nmulti-dimensional routing framework by modeling the capability and knowledge of\nmodels. We operate it on our comprehensive dataset RouteMix, and demonstrate\nits effectiveness and generalizability in group-level routing using modern\nbenchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its\nability to identify and leverage top-performing models for given tasks, leading\nto superior outcomes with efficient resource utilization. The broader adoption\nof Inference Dynamics can empower users to harness the full specialized\npotential of the LLM ecosystem, and our code will be made publicly available to\nencourage further research.", "AI": {"tldr": "InferenceDynamics\u662f\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u591a\u7ef4\u8def\u7531\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524dLLM\u8def\u7531\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u5efa\u6a21\u6a21\u578b\u7684\u80fd\u529b\u548c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5229\u7528\u548c\u4efb\u52a1\u6027\u80fd\u4f18\u5316\u3002", "motivation": "\u5f53\u524dLLM\u8def\u7531\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u4e13\u7528LLM\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faInferenceDynamics\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u6a21\u578b\u7684\u80fd\u529b\u548c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u591a\u7ef4\u8def\u7531\uff0c\u5e76\u5728RouteMix\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728MMLU-Pro\u3001GPQA\u3001BigGenBench\u548cLiveBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInferenceDynamics\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u5e76\u5229\u7528\u9ad8\u6027\u80fd\u6a21\u578b\u3002", "conclusion": "InferenceDynamics\u4e3aLLM\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u8d44\u6e90\u5229\u7528\u548c\u4efb\u52a1\u6027\u80fd\u4f18\u5316\uff0c\u5176\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.16324", "pdf": "https://arxiv.org/pdf/2505.16324", "abs": "https://arxiv.org/abs/2505.16324", "authors": ["Cheng Cheng", "Lin Song", "Yicheng Xiao", "Yuxin Chen", "Xuchong Zhang", "Hongbin Sun", "Ying Shan"], "title": "TensorAR: Refinement is All You Need in Autoregressive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive (AR) image generators offer a language-model-friendly approach\nto image generation by predicting discrete image tokens in a causal sequence.\nHowever, unlike diffusion models, AR models lack a mechanism to refine previous\npredictions, limiting their generation quality. In this paper, we introduce\nTensorAR, a new AR paradigm that reformulates image generation from next-token\nprediction to next-tensor prediction. By generating overlapping windows of\nimage patches (tensors) in a sliding fashion, TensorAR enables iterative\nrefinement of previously generated content. To prevent information leakage\nduring training, we propose a discrete tensor noising scheme, which perturbs\ninput tokens via codebook-indexed noise. TensorAR is implemented as a\nplug-and-play module compatible with existing AR models. Extensive experiments\non LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly\nimproves the generation performance of autoregressive models.", "AI": {"tldr": "TensorAR\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u8f6c\u53d8\u4e3a\u4e0b\u4e00\u4e2a\u5f20\u91cf\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u5185\u5bb9\u7684\u8fed\u4ee3\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u5668\u7f3a\u4e4f\u5bf9\u5148\u524d\u9884\u6d4b\u7684\u4f18\u5316\u673a\u5236\uff0c\u9650\u5236\u4e86\u751f\u6210\u8d28\u91cf\u3002", "method": "TensorAR\u901a\u8fc7\u6ed1\u52a8\u751f\u6210\u91cd\u53e0\u7684\u56fe\u50cf\u5757\uff08\u5f20\u91cf\uff09\uff0c\u5e76\u63d0\u51fa\u79bb\u6563\u5f20\u91cf\u566a\u58f0\u65b9\u6848\u4ee5\u9632\u6b62\u4fe1\u606f\u6cc4\u6f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTensorAR\u663e\u8457\u63d0\u5347\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "TensorAR\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6539\u8fdb\u65b9\u6848\u3002"}}
{"id": "2505.16307", "pdf": "https://arxiv.org/pdf/2505.16307", "abs": "https://arxiv.org/abs/2505.16307", "authors": ["Chenzhuo Zhao", "Ziqian Liu", "Xingda Wang", "Junting Lu", "Chaoyi Ruan"], "title": "PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt optimization offers a practical and broadly applicable alternative to\nfine-tuning for improving large language model (LLM) performance. However,\nexisting methods often rely on costly output generation, self-critiquing\nabilities, or human-annotated preferences, which limit their scalability,\nespecially for smaller or non-instruction-tuned models. We introduce PMPO\n(Probabilistic Metric Prompt Optimization), a unified framework that refines\nprompts using token-level cross-entropy loss as a direct, lightweight\nevaluation signal. PMPO identifies low-quality prompt segments by masking and\nmeasuring their impact on loss, then rewrites and selects improved variants by\nminimizing loss over positive and negative examples. Unlike prior methods, it\nrequires no output sampling or human evaluation during optimization, relying\nonly on forward passes and log-likelihoods. PMPO supports both supervised and\npreference-based tasks through a closely aligned loss-based evaluation\nstrategy. Experiments show that PMPO consistently outperforms prior methods\nacross model sizes and tasks: it achieves the highest average accuracy on BBH,\nperforms strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates\nby over 19 points. These results highlight PMPO's effectiveness, efficiency,\nand broad applicability.", "AI": {"tldr": "PMPO\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u635f\u5931\u76f4\u63a5\u4f18\u5316\u63d0\u793a\uff0c\u65e0\u9700\u8f93\u51fa\u751f\u6210\u6216\u4eba\u5de5\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u8f93\u51fa\u751f\u6210\u6216\u4eba\u5de5\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u5c0f\u6a21\u578b\u6216\u975e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002", "method": "PMPO\u5229\u7528\u63a9\u7801\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u8bc6\u522b\u4f4e\u8d28\u91cf\u63d0\u793a\u6bb5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u635f\u5931\u4f18\u5316\u63d0\u793a\uff0c\u4ec5\u9700\u524d\u5411\u4f20\u64ad\u548c\u4f3c\u7136\u8ba1\u7b97\u3002", "result": "PMPO\u5728BBH\u3001GSM8K\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cAlpacaEval 2.0\u80dc\u7387\u63d0\u534719\u70b9\u4ee5\u4e0a\u3002", "conclusion": "PMPO\u9ad8\u6548\u3001\u901a\u7528\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u89c4\u6a21\u3002"}}
{"id": "2505.16334", "pdf": "https://arxiv.org/pdf/2505.16334", "abs": "https://arxiv.org/abs/2505.16334", "authors": ["Kun-Yu Lin", "Hongjun Wang", "Weining Ren", "Kai Han"], "title": "Panoptic Captioning: Seeking An Equivalency Bridge for Image and Text", "categories": ["cs.CV"], "comment": "Project page: https://visual-ai.github.io/pancap/", "summary": "This work introduces panoptic captioning, a novel task striving to seek the\nminimum text equivalence of images. We take the first step towards panoptic\ncaptioning by formulating it as a task of generating a comprehensive textual\ndescription for an image, which encapsulates all entities, their respective\nlocations and attributes, relationships among entities, as well as global image\nstate.Through an extensive evaluation, our work reveals that state-of-the-art\nMulti-modal Large Language Models (MLLMs) have limited performance in solving\npanoptic captioning. To address this, we propose an effective data engine named\nPancapEngine to produce high-quality data and a novel method named PancapChain\nto improve panoptic captioning. Specifically, our PancapEngine first detects\ndiverse categories of entities in images by an elaborate detection suite, and\nthen generates required panoptic captions using entity-aware prompts.\nAdditionally, our PancapChain explicitly decouples the challenging panoptic\ncaptioning task into multiple stages and generates panoptic captions step by\nstep. More importantly, we contribute a comprehensive metric named PancapScore\nand a human-curated test set for reliable model evaluation.Experiments show\nthat our PancapChain-13B model can beat state-of-the-art open-source MLLMs like\nInternVL-2.5-78B and even surpass proprietary models like GPT-4o and\nGemini-2.0-Pro, demonstrating the effectiveness of our data engine and method.\nProject page: https://visual-ai.github.io/pancap/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5168\u666f\u63cf\u8ff0\u4efb\u52a1\uff08panoptic captioning\uff09\uff0c\u65e8\u5728\u751f\u6210\u56fe\u50cf\u7684\u6700\u5c0f\u6587\u672c\u7b49\u4ef7\u63cf\u8ff0\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u5f15\u64cePancapEngine\u548c\u65b9\u6cd5PancapChain\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u5b9e\u9a8c\u663e\u793a\u5176\u6a21\u578b\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u666f\u63cf\u8ff0\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u6539\u8fdb\u4ee5\u751f\u6210\u66f4\u5168\u9762\u7684\u56fe\u50cf\u63cf\u8ff0\u3002", "method": "\u63d0\u51faPancapEngine\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0cPancapChain\u5206\u9636\u6bb5\u751f\u6210\u63cf\u8ff0\uff0c\u5e76\u5f15\u5165PancapScore\u8bc4\u4f30\u6307\u6807\u3002", "result": "PancapChain-13B\u6a21\u578b\u8d85\u8d8aInternVL-2.5-78B\u3001GPT-4o\u548cGemini-2.0-Pro\u7b49\u6a21\u578b\u3002", "conclusion": "PancapEngine\u548cPancapChain\u6709\u6548\u63d0\u5347\u5168\u666f\u63cf\u8ff0\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2505.16325", "pdf": "https://arxiv.org/pdf/2505.16325", "abs": "https://arxiv.org/abs/2505.16325", "authors": ["Yuyang Jiang", "Chacha Chen", "Shengyuan Wang", "Feng Li", "Zecong Tang", "Benjamin M. Mervak", "Lydia Chelala", "Christopher M Straus", "Reve Chahine", "Samuel G. Armato III", "Chenhao Tan"], "title": "CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "18 pages, 4 figures", "summary": "Existing metrics often lack the granularity and interpretability to capture\nnuanced clinical differences between candidate and ground-truth radiology\nreports, resulting in suboptimal evaluation. We introduce a Clinically-grounded\ntabular framework with Expert-curated labels and Attribute-level comparison for\nRadiology report evaluation (CLEAR). CLEAR not only examines whether a report\ncan accurately identify the presence or absence of medical conditions, but also\nassesses whether it can precisely describe each positively identified condition\nacross five key attributes: first occurrence, change, severity, descriptive\nlocation, and recommendation. Compared to prior works, CLEAR's\nmulti-dimensional, attribute-level outputs enable a more comprehensive and\nclinically interpretable evaluation of report quality. Additionally, to measure\nthe clinical alignment of CLEAR, we collaborate with five board-certified\nradiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from\nMIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.\nOur experiments show that CLEAR achieves high accuracy in extracting clinical\nattributes and provides automated metrics that are strongly aligned with\nclinical judgment.", "AI": {"tldr": "CLEAR\u662f\u4e00\u4e2a\u7528\u4e8e\u653e\u5c04\u5b66\u62a5\u544a\u8bc4\u4f30\u7684\u4e34\u5e8a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5c5e\u6027\u7ea7\u6bd4\u8f83\u63d0\u4f9b\u66f4\u5168\u9762\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6307\u6807\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5019\u9009\u62a5\u544a\u4e0e\u771f\u5b9e\u62a5\u544a\u4e4b\u95f4\u7684\u4e34\u5e8a\u5dee\u5f02\u3002", "method": "\u5f15\u5165CLEAR\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6807\u6ce8\u548c\u5c5e\u6027\u7ea7\u6bd4\u8f83\uff0c\u8bc4\u4f30\u62a5\u544a\u7684\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "result": "CLEAR\u5728\u63d0\u53d6\u4e34\u5e8a\u5c5e\u6027\u548c\u63d0\u4f9b\u81ea\u52a8\u5316\u6307\u6807\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u4e34\u5e8a\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "CLEAR\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002"}}
{"id": "2505.16335", "pdf": "https://arxiv.org/pdf/2505.16335", "abs": "https://arxiv.org/abs/2505.16335", "authors": ["Renjie Wei", "Songqiang Xu", "Qingyu Guo", "Meng Li"], "title": "FPQVAR: Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual autoregressive (VAR) modeling has marked a paradigm shift in image\ngeneration from next-token prediction to next-scale prediction. VAR predicts a\nset of tokens at each step from coarse to fine scale, leading to better image\nquality and faster inference speed compared to existing diffusion models.\nHowever, the large parameter size and computation cost hinder its deployment on\nedge devices. To reduce the memory and computation cost, we propose FPQVAR, an\nefficient post-training floating-point (FP) quantization framework for VAR\nfeaturing algorithm and hardware co-design. At the algorithm level, we first\nidentify the challenges of quantizing VAR. To address them, we propose Dual\nFormat Quantization for the highly imbalanced input activation. We further\npropose Group-wise Hadamard Transformation and GHT-Aware Learnable\nTransformation to address the time-varying outlier channels. At the hardware\nlevel, we design the first low-bit FP quantizer and multiplier with lookup\ntables on FPGA and propose the first FPGA-based VAR accelerator featuring\nlow-bit FP computation and an elaborate two-level pipeline. Extensive\nexperiments show that compared to the state-of-the-art quantization method, our\nproposed FPQVAR significantly improves Fr\\'echet Inception Distance (FID) from\n10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit\nquantization. FPQVAR also significantly improves the performance of 6-bit\nquantized VAR, bringing it on par with the FP16 model. Our accelerator on\nAMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x\nhigher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x\nhigher energy efficiency compared to the integer-based accelerator and GPU\nbaseline, respectively.", "AI": {"tldr": "FPQVAR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6d6e\u70b9\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u6cd5\u548c\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86VAR\u6a21\u578b\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "VAR\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53c2\u6570\u89c4\u6a21\u548c\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faFPQVAR\u6846\u67b6\uff0c\u5305\u62ec\u53cc\u683c\u5f0f\u91cf\u5316\u3001\u5206\u7ec4Hadamard\u53d8\u6362\u548cGHT\u611f\u77e5\u53ef\u5b66\u4e60\u53d8\u6362\uff0c\u5e76\u5728\u786c\u4ef6\u5c42\u9762\u8bbe\u8ba1\u4e86\u4f4e\u6bd4\u7279\u6d6e\u70b9\u91cf\u5316\u5668\u548c\u4e58\u6cd5\u5668\u3002", "result": "FPQVAR\u57284\u4f4d\u91cf\u5316\u4e0b\u663e\u8457\u63d0\u5347\u4e86FID\u548cIS\u5206\u6570\uff0c6\u4f4d\u91cf\u5316\u6027\u80fd\u63a5\u8fd1FP16\u6a21\u578b\uff0cFPGA\u52a0\u901f\u5668\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u4e0a\u4f18\u4e8e\u6574\u6570\u52a0\u901f\u5668\u548cGPU\u57fa\u7ebf\u3002", "conclusion": "FPQVAR\u901a\u8fc7\u7b97\u6cd5\u548c\u786c\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86VAR\u6a21\u578b\u7684\u90e8\u7f72\u96be\u9898\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2505.16330", "pdf": "https://arxiv.org/pdf/2505.16330", "abs": "https://arxiv.org/abs/2505.16330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Tong Bao", "Yi Zhao"], "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers", "categories": ["cs.CL", "cs.AI", "cs.DL"], "comment": null, "summary": "Novelty is a core component of academic papers, and there are multiple\nperspectives on the assessment of novelty. Existing methods often focus on word\nor entity combinations, which provide limited insights. The content related to\na paper's novelty is typically distributed across different core sections,\ne.g., Introduction, Methodology and Results. Therefore, exploring the optimal\ncombination of sections for evaluating the novelty of a paper is important for\nadvancing automated novelty assessment. In this paper, we utilize different\ncombinations of sections from academic papers as inputs to drive language\nmodels to predict novelty scores. We then analyze the results to determine the\noptimal section combinations for novelty score prediction. We first employ\nnatural language processing techniques to identify the sectional structure of\nacademic papers, categorizing them into introduction, methods, results, and\ndiscussion (IMRaD). Subsequently, we used different combinations of these\nsections (e.g., introduction and methods) as inputs for pretrained language\nmodels (PLMs) and large language models (LLMs), employing novelty scores\nprovided by human expert reviewers as ground truth labels to obtain prediction\nresults. The results indicate that using introduction, results and discussion\nis most appropriate for assessing the novelty of a paper, while the use of the\nentire text does not yield significant results. Furthermore, based on the\nresults of the PLMs and LLMs, the introduction and results appear to be the\nmost important section for the task of novelty score prediction. The code and\ndataset for this paper can be accessed at\nhttps://github.com/njust-winchy/SC4ANM.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5b66\u672f\u8bba\u6587\u65b0\u9896\u6027\u8bc4\u4f30\u7684\u6700\u4f73\u7ae0\u8282\u7ec4\u5408\uff0c\u53d1\u73b0\u5f15\u8a00\u3001\u7ed3\u679c\u548c\u8ba8\u8bba\u90e8\u5206\u6700\u9002\u5408\u9884\u6d4b\u65b0\u9896\u6027\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u65b0\u9896\u6027\u7684\u8bc4\u4f30\u591a\u57fa\u4e8e\u8bcd\u6c47\u6216\u5b9e\u4f53\u7ec4\u5408\uff0c\u4f46\u8bba\u6587\u7684\u65b0\u9896\u6027\u5185\u5bb9\u5206\u6563\u5728\u4e0d\u540c\u7ae0\u8282\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u6700\u4f73\u7ae0\u8282\u7ec4\u5408\u4ee5\u63d0\u5347\u81ea\u52a8\u5316\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u8bc6\u522b\u8bba\u6587\u7684IMRaD\u7ed3\u6784\uff0c\u4ee5\u4e0d\u540c\u7ae0\u8282\u7ec4\u5408\u4f5c\u4e3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\uff0c\u9884\u6d4b\u65b0\u9896\u6027\u8bc4\u5206\u3002", "result": "\u5f15\u8a00\u3001\u7ed3\u679c\u548c\u8ba8\u8bba\u90e8\u5206\u6700\u9002\u5408\u65b0\u9896\u6027\u8bc4\u4f30\uff0c\u800c\u5168\u6587\u8f93\u5165\u6548\u679c\u4e0d\u660e\u663e\uff1b\u5f15\u8a00\u548c\u7ed3\u679c\u90e8\u5206\u5bf9\u9884\u6d4b\u4efb\u52a1\u6700\u4e3a\u91cd\u8981\u3002", "conclusion": "\u5f15\u8a00\u3001\u7ed3\u679c\u548c\u8ba8\u8bba\u90e8\u5206\u7684\u7ec4\u5408\u662f\u8bc4\u4f30\u8bba\u6587\u65b0\u9896\u6027\u7684\u6700\u4f73\u9009\u62e9\uff0c\u4e3a\u81ea\u52a8\u5316\u65b0\u9896\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16338", "pdf": "https://arxiv.org/pdf/2505.16338", "abs": "https://arxiv.org/abs/2505.16338", "authors": ["Amirreza Mahbod", "Rupert Ecker", "Ramona Woitek"], "title": "Fusion of Foundation and Vision Transformer Model Features for Dermatoscopic Image Classification", "categories": ["cs.CV"], "comment": "6 pages", "summary": "Accurate classification of skin lesions from dermatoscopic images is\nessential for diagnosis and treatment of skin cancer. In this study, we\ninvestigate the utility of a dermatology-specific foundation model, PanDerm, in\ncomparison with two Vision Transformer (ViT) architectures (ViT base and Swin\nTransformer V2 base) for the task of skin lesion classification. Using frozen\nfeatures extracted from PanDerm, we apply non-linear probing with three\ndifferent classifiers, namely, multi-layer perceptron (MLP), XGBoost, and\nTabNet. For the ViT-based models, we perform full fine-tuning to optimize\nclassification performance. Our experiments on the HAM10000 and MSKCC datasets\ndemonstrate that the PanDerm-based MLP model performs comparably to the\nfine-tuned Swin transformer model, while fusion of PanDerm and Swin Transformer\npredictions leads to further performance improvements. Future work will explore\nadditional foundation models, fine-tuning strategies, and advanced fusion\ntechniques.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u76ae\u80a4\u79d1\u4e13\u7528\u57fa\u7840\u6a21\u578bPanDerm\u4e0e\u4e24\u79cdViT\u67b6\u6784\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0PanDerm\u7ed3\u5408MLP\u5206\u7c7b\u5668\u4e0eSwin Transformer\u6027\u80fd\u76f8\u5f53\uff0c\u878d\u5408\u9884\u6d4b\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u76ae\u80a4\u75c5\u53d8\u7684\u51c6\u786e\u5206\u7c7b\u5bf9\u76ae\u80a4\u764c\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u4e0eViT\u67b6\u6784\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u4f7f\u7528PanDerm\u63d0\u53d6\u51bb\u7ed3\u7279\u5f81\uff0c\u7ed3\u5408MLP\u3001XGBoost\u548cTabNet\u5206\u7c7b\u5668\uff1b\u5bf9ViT\u6a21\u578b\u8fdb\u884c\u5168\u5fae\u8c03\uff1b\u5728HAM10000\u548cMSKCC\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u3002", "result": "PanDerm\u7ed3\u5408MLP\u4e0e\u5fae\u8c03Swin Transformer\u6027\u80fd\u76f8\u5f53\uff0c\u878d\u5408\u9884\u6d4b\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "conclusion": "\u672a\u6765\u5c06\u63a2\u7d22\u66f4\u591a\u57fa\u7840\u6a21\u578b\u3001\u5fae\u8c03\u7b56\u7565\u548c\u9ad8\u7ea7\u878d\u5408\u6280\u672f\u3002"}}
{"id": "2505.16348", "pdf": "https://arxiv.org/pdf/2505.16348", "abs": "https://arxiv.org/abs/2505.16348", "authors": ["Taeyoon Kwon", "Dongwook Choi", "Sunghwan Kim", "Hyojun Kim", "Seungjun Moon", "Beong-woo Kwak", "Kuan-Hao Huang", "Jinyoung Yeo"], "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MEMENTO\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2a\u6027\u5316\u5177\u8eab\u4ee3\u7406\u5728\u8bb0\u5fc6\u5229\u7528\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u591a\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5f53\u524d\u5177\u8eab\u4ee3\u7406\u5728\u5bb6\u5ead\u7269\u54c1\u91cd\u6392\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7528\u6237\u4e2a\u6027\u5316\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u65e0\u6cd5\u63d0\u4f9b\u771f\u6b63\u6709\u610f\u4e49\u7684\u5e2e\u52a9\u3002", "method": "\u63d0\u51faMEMENTO\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bb0\u5fc6\u8bc4\u4f30\u8bbe\u8ba1\uff0c\u91cf\u5316\u8bb0\u5fc6\u5229\u7528\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u76ee\u6807\u5bf9\u8c61\u8bc6\u522b\u548c\u7528\u6237\u6a21\u5f0f\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u591a\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08\u5982GPT-4o\u6027\u80fd\u4e0b\u964d30.5%\uff09\uff0c\u5c24\u5176\u5728\u7528\u6237\u6a21\u5f0f\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u5177\u8eab\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bb0\u5fc6\u5229\u7528\u80fd\u529b\u3002"}}
{"id": "2505.16360", "pdf": "https://arxiv.org/pdf/2505.16360", "abs": "https://arxiv.org/abs/2505.16360", "authors": ["Estelle Chigot", "Dennis G. Wilson", "Meriem Ghrib", "Thomas Oberlin"], "title": "Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation", "categories": ["cs.CV", "cs.LG", "68T45 (Primary) 68T10, 68T07 (Secondary)", "F.1.2; F.1.4"], "comment": "Under review", "summary": "Semantic segmentation models trained on synthetic data often perform poorly\non real-world images due to domain gaps, particularly in adverse conditions\nwhere labeled data is scarce. Yet, recent foundation models enable to generate\nrealistic images without any training. This paper proposes to leverage such\ndiffusion models to improve the performance of vision models when learned on\nsynthetic data. We introduce two novel techniques for semantically consistent\nstyle transfer using diffusion models: Class-wise Adaptive Instance\nNormalization and Cross-Attention (CACTI) and its extension with selective\nattention Filtering (CACTIF). CACTI applies statistical normalization\nselectively based on semantic classes, while CACTIF further filters\ncross-attention maps based on feature similarity, preventing artifacts in\nregions with weak cross-attention correspondences. Our methods transfer style\ncharacteristics while preserving semantic boundaries and structural coherence,\nunlike approaches that apply global transformations or generate content without\nconstraints. Experiments using GTA5 as source and Cityscapes/ACDC as target\ndomains show that our approach produces higher quality images with lower FID\nscores and better content preservation. Our work demonstrates that class-aware\ndiffusion-based style transfer effectively bridges the synthetic-to-real domain\ngap even with minimal target domain data, advancing robust perception systems\nfor challenging real-world applications. The source code is available at:\nhttps://github.com/echigot/cactif.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8bed\u4e49\u4e00\u81f4\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff08CACTI\u548cCACTIF\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u9886\u57df\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6076\u52a3\u6761\u4ef6\u4e0b\u3002", "method": "\u63d0\u51faCACTI\uff08\u57fa\u4e8e\u8bed\u4e49\u7c7b\u7684\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u548cCACTIF\uff08\u5e26\u9009\u62e9\u6027\u6ce8\u610f\u529b\u8fc7\u6ee4\u7684\u6269\u5c55\u7248\u672c\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u98ce\u683c\u8fc1\u79fb\u4fdd\u6301\u8bed\u4e49\u8fb9\u754c\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "result": "\u5728GTA5\u5230Cityscapes/ACDC\u7684\u5b9e\u9a8c\u4e2d\uff0c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\uff08FID\u5206\u6570\u66f4\u4f4e\uff09\uff0c\u5185\u5bb9\u4fdd\u7559\u66f4\u597d\u3002", "conclusion": "\u7c7b\u611f\u77e5\u7684\u6269\u6563\u98ce\u683c\u8fc1\u79fb\u80fd\u6709\u6548\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u63a8\u52a8\u9c81\u68d2\u611f\u77e5\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16349", "pdf": "https://arxiv.org/pdf/2505.16349", "abs": "https://arxiv.org/abs/2505.16349", "authors": ["Pierre Achkar", "Tim Gollub", "Martin Potthast"], "title": "Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization", "categories": ["cs.CL"], "comment": "Accepted at SCOLIA@ECIR 2025 Workshop", "summary": "The exponential growth of scientific publications has made it increasingly\ndifficult for researchers to stay updated and synthesize knowledge effectively.\nThis paper presents XSum, a modular pipeline for multi-document summarization\n(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The\npipeline includes two core components: a question-generation module and an\neditor module. The question-generation module dynamically generates questions\nadapted to the input papers, ensuring the retrieval of relevant and accurate\ninformation. The editor module synthesizes the retrieved content into coherent\nand well-structured summaries that adhere to academic standards for proper\ncitation. Evaluated on the SurveySum dataset, XSum demonstrates strong\nperformance, achieving considerable improvements in metrics such as CheckEval,\nG-Eval and Ref-F1 compared to existing approaches. This work provides a\ntransparent, adaptable framework for scientific summarization with potential\napplications in a wide range of domains. Code available at\nhttps://github.com/webis-de/scolia25-xsum", "AI": {"tldr": "XSum\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u79d1\u5b66\u9886\u57df\u591a\u6587\u6863\u6458\u8981\uff08MDS\uff09\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5305\u542b\u95ee\u9898\u751f\u6210\u548c\u7f16\u8f91\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6458\u8981\u8d28\u91cf\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u4f7f\u5f97\u7814\u7a76\u8005\u96be\u4ee5\u6709\u6548\u8ddf\u8e2a\u548c\u6574\u5408\u77e5\u8bc6\uff0cXSum\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "XSum\u91c7\u7528\u95ee\u9898\u751f\u6210\u6a21\u5757\u52a8\u6001\u751f\u6210\u95ee\u9898\u4ee5\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7f16\u8f91\u6a21\u5757\u5408\u6210\u7b26\u5408\u5b66\u672f\u6807\u51c6\u7684\u6458\u8981\u3002", "result": "\u5728SurveySum\u6570\u636e\u96c6\u4e0a\uff0cXSum\u5728CheckEval\u3001G-Eval\u548cRef-F1\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "XSum\u4e3a\u79d1\u5b66\u6458\u8981\u63d0\u4f9b\u4e86\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u9002\u5e94\u7684\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.16372", "pdf": "https://arxiv.org/pdf/2505.16372", "abs": "https://arxiv.org/abs/2505.16372", "authors": ["Feng Liu", "Bingyu Nan", "Xuezhong Qian", "Xiaolan Fu"], "title": "Temporal and Spatial Feature Fusion Framework for Dynamic Micro Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages", "summary": "When emotions are repressed, an individual's true feelings may be revealed\nthrough micro-expressions. Consequently, micro-expressions are regarded as a\ngenuine source of insight into an individual's authentic emotions. However, the\ntransient and highly localised nature of micro-expressions poses a significant\nchallenge to their accurate recognition, with the accuracy rate of\nmicro-expression recognition being as low as 50%, even for professionals. In\norder to address these challenges, it is necessary to explore the field of\ndynamic micro expression recognition (DMER) using multimodal fusion techniques,\nwith special attention to the diverse fusion of temporal and spatial modal\nfeatures. In this paper, we propose a novel Temporal and Spatial feature Fusion\nframework for DMER (TSFmicro). This framework integrates a Retention Network\n(RetNet) and a transformer-based DMER network, with the objective of efficient\nmicro-expression recognition through the capture and fusion of temporal and\nspatial relations. Meanwhile, we propose a novel parallel time-space fusion\nmethod from the perspective of modal fusion, which fuses spatio-temporal\ninformation in high-dimensional feature space, resulting in complementary\n\"where-how\" relationships at the semantic level and providing richer semantic\ninformation for the model. The experimental results demonstrate the superior\nperformance of the TSFmicro method in comparison to other contemporary\nstate-of-the-art methods. This is evidenced by its effectiveness on three\nwell-recognised micro-expression datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSFmicro\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u65f6\u7a7a\u7279\u5f81\u63d0\u5347\u5fae\u8868\u60c5\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5fae\u8868\u60c5\u77ed\u6682\u4e14\u5c40\u90e8\u6027\u5f3a\uff0c\u8bc6\u522b\u51c6\u786e\u7387\u4f4e\uff08\u4ec550%\uff09\uff0c\u9700\u63a2\u7d22\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u4ee5\u63d0\u5347\u8bc6\u522b\u6548\u679c\u3002", "method": "\u7ed3\u5408Retention Network\u548c\u57fa\u4e8etransformer\u7684DMER\u7f51\u7edc\uff0c\u63d0\u51fa\u5e76\u884c\u65f6\u7a7a\u878d\u5408\u65b9\u6cd5\uff0c\u5728\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u878d\u5408\u65f6\u7a7a\u4fe1\u606f\u3002", "result": "TSFmicro\u5728\u4e09\u4e2a\u77e5\u540d\u5fae\u8868\u60c5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "TSFmicro\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u65f6\u7a7a\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u3002"}}
{"id": "2505.16381", "pdf": "https://arxiv.org/pdf/2505.16381", "abs": "https://arxiv.org/abs/2505.16381", "authors": ["Songlin Yang", "Yikang Shen", "Kaiyue Wen", "Shawn Tan", "Mayank Mishra", "Liliang Ren", "Rameswar Panda", "Yoon Kim"], "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "The attention mechanism is a core primitive in modern large language models\n(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,\nposition encoding is essential for modeling structured domains such as\nlanguage. Rotary position encoding (RoPE) has emerged as the de facto standard\napproach for position encoding and is part of many modern LLMs. However, in\nRoPE the key/query transformation between two elements in a sequence is only a\nfunction of their relative position and otherwise independent of the actual\ninput. This limits the expressivity of RoPE-based transformers.\n  This paper describes PaTH, a flexible data-dependent position encoding scheme\nbased on accumulated products of Householder(like) transformations, where each\ntransformation is data-dependent, i.e., a function of the input. We derive an\nefficient parallel algorithm for training through exploiting a compact\nrepresentation of products of Householder matrices, and implement a\nFlashAttention-style blockwise algorithm that minimizes I/O cost. Across both\ntargeted synthetic benchmarks and moderate-scale real-world language modeling\nexperiments, we find that PaTH demonstrates superior performance compared to\nRoPE and other recent baselines.", "AI": {"tldr": "PaTH\u662f\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u4f9d\u8d56\u7684Householder\u53d8\u6362\u7684\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u76f8\u6bd4RoPE\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "RoPE\u7684\u4f4d\u7f6e\u7f16\u7801\u4ec5\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u9650\u5236\u4e86\u8868\u8fbe\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6570\u636e\u4f9d\u8d56\u7f16\u7801\u65b9\u6848\u3002", "method": "\u63d0\u51faPaTH\uff0c\u57fa\u4e8e\u6570\u636e\u4f9d\u8d56\u7684Householder\u53d8\u6362\uff0c\u8bbe\u8ba1\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\u7b97\u6cd5\u548cFlashAttention\u98ce\u683c\u7684\u5757\u5904\u7406\u4ee5\u51cf\u5c11I/O\u5f00\u9500\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u5b9e\u9645\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u4e2d\uff0cPaTH\u8868\u73b0\u4f18\u4e8eRoPE\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PaTH\u901a\u8fc7\u6570\u636e\u4f9d\u8d56\u7684\u4f4d\u7f6e\u7f16\u7801\u63d0\u5347\u4e86\u8868\u8fbe\u529b\u548c\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.16376", "pdf": "https://arxiv.org/pdf/2505.16376", "abs": "https://arxiv.org/abs/2505.16376", "authors": ["Zijia Lu", "A S M Iftekhar", "Gaurav Mittal", "Tianjian Meng", "Xiawei Wang", "Cheng Zhao", "Rohith Kukkala", "Ehsan Elhamifar", "Mei Chen"], "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR 2025", "summary": "Long Video Temporal Grounding (LVTG) aims at identifying specific moments\nwithin lengthy videos based on user-provided text queries for effective content\nretrieval. The approach taken by existing methods of dividing video into clips\nand processing each clip via a full-scale expert encoder is challenging to\nscale due to prohibitive computational costs of processing a large number of\nclips in long videos. To address this issue, we introduce DeCafNet, an approach\nemploying ``delegate-and-conquer'' strategy to achieve computation efficiency\nwithout sacrificing grounding performance. DeCafNet introduces a sidekick\nencoder that performs dense feature extraction over all video clips in a\nresource-efficient manner, while generating a saliency map to identify the most\nrelevant clips for full processing by the expert encoder. To effectively\nleverage features from sidekick and expert encoders that exist at different\ntemporal resolutions, we introduce DeCaf-Grounder, which unifies and refines\nthem via query-aware temporal aggregation and multi-scale temporal refinement\nfor accurate grounding. Experiments on two LTVG benchmark datasets demonstrate\nthat DeCafNet reduces computation by up to 47\\% while still outperforming\nexisting methods, establishing a new state-of-the-art for LTVG in terms of both\nefficiency and performance. Our code is available at\nhttps://github.com/ZijiaLewisLu/CVPR2025-DeCafNet.", "AI": {"tldr": "DeCafNet\u901a\u8fc7\u201c\u59d4\u6258-\u5f81\u670d\u201d\u7b56\u7565\u548c\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u957f\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u9ad8\u6548\u5904\u7406\u957f\u89c6\u9891\u4e2d\u7684\u5927\u91cf\u7247\u6bb5\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f85\u52a9\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\u5e76\u751f\u6210\u663e\u8457\u6027\u56fe\uff0c\u7ed3\u5408\u4e13\u5bb6\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u7cbe\u70bc\u3002", "result": "\u8ba1\u7b97\u6210\u672c\u964d\u4f4e47%\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "DeCafNet\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u957f\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16385", "pdf": "https://arxiv.org/pdf/2505.16385", "abs": "https://arxiv.org/abs/2505.16385", "authors": ["Kaiyu He", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 10 figures", "summary": "Large language models (LLMs) demonstrate remarkable ability in cross-lingual\ntasks. Understanding how LLMs acquire this ability is crucial for their\ninterpretability. To quantify the cross-lingual ability of LLMs accurately, we\npropose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn\ncross-lingual ability, we trace the outputs of LLMs' intermediate layers in the\nword translation task. We identify and distinguish two distinct behaviors in\nthe forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.\nWe attribute LLMs' two distinct behaviors to the co-occurrence frequency of\nwords and find the semantic pivot from the pre-training dataset. Finally, to\napply our findings to improve the cross-lingual ability of LLMs, we reconstruct\na semantic pivot-aware pre-training dataset using documents with a high\nproportion of semantic pivots. Our experiments validate the effectiveness of\nour approach in enhancing cross-lingual ability. Our research contributes\ninsights into the interpretability of LLMs and offers a method for improving\nLLMs' cross-lingual ability.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bcd\u7ea7\u8de8\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u6765\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8de8\u8bed\u8a00\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u8ffd\u8e2a\u6a21\u578b\u4e2d\u95f4\u5c42\u8f93\u51fa\u6765\u5206\u6790\u5176\u5b66\u4e60\u673a\u5236\uff0c\u53d1\u73b0\u4e24\u79cd\u884c\u4e3a\u6a21\u5f0f\uff1a\u5171\u73b0\u884c\u4e3a\u548c\u8bed\u4e49\u67a2\u7ebd\u884c\u4e3a\u3002\u901a\u8fc7\u91cd\u6784\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u63d0\u5347\u8de8\u8bed\u8a00\u80fd\u529b\u3002", "motivation": "\u7406\u89e3LLMs\u5982\u4f55\u83b7\u5f97\u8de8\u8bed\u8a00\u80fd\u529b\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u8bcd\u7ea7\u8de8\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\uff0c\u8ffd\u8e2a\u6a21\u578b\u4e2d\u95f4\u5c42\u8f93\u51fa\uff0c\u5206\u6790\u5171\u73b0\u9891\u7387\u548c\u8bed\u4e49\u67a2\u7ebd\u884c\u4e3a\uff0c\u91cd\u6784\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u901a\u8fc7\u8bed\u4e49\u67a2\u7ebd\u611f\u77e5\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u80fd\u6709\u6548\u63d0\u5347LLMs\u7684\u8de8\u8bed\u8a00\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u8de8\u8bed\u8a00\u80fd\u529b\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16384", "pdf": "https://arxiv.org/pdf/2505.16384", "abs": "https://arxiv.org/abs/2505.16384", "authors": ["Haoming Huang", "Musen Zhang", "Jianxin Yang", "Zhen Li", "Jinkai Li", "Yao Guo"], "title": "MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module", "categories": ["cs.CV", "cs.HC"], "comment": "Under review", "summary": "Eye gaze can provide rich information on human psychological activities, and\nhas garnered significant attention in the field of Human-Robot Interaction\n(HRI). However, existing gaze estimation methods merely predict either the gaze\ndirection or the Point-of-Gaze (PoG) on the screen, failing to provide\nsufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze\nanalysis in 3D space. Moreover, the variations of eye shape and structure among\nindividuals also impede the generalization capability of these methods. In this\nstudy, we propose MAGE, a Multi-task Architecture for Gaze Estimation with an\nefficient calibration module, to predict the 6-DoF gaze information that is\napplicable for the real-word HRI. Our basic model encodes both the directional\nand positional features from facial images, and predicts gaze results with\ndedicated information flow and multiple decoders. To reduce the impact of\nindividual variations, we propose a novel calibration module, namely\nEasy-Calibration, to fine-tune the basic model with subject-specific data,\nwhich is efficient to implement without the need of a screen. Experimental\nresults demonstrate that our method achieves state-of-the-art performance on\nthe public MPIIFaceGaze, EYEDIAP, and our built IMRGaze datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAGE\u65b9\u6cd5\uff0c\u7528\u4e8e6-DoF\u89c6\u7ebf\u4f30\u8ba1\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u67b6\u6784\u548c\u9ad8\u6548\u6821\u51c6\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u7ebf\u4f30\u8ba1\u65b9\u6cd5\u4ec5\u9884\u6d4b\u65b9\u5411\u6216\u5c4f\u5e55\u6ce8\u89c6\u70b9\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u7684\u5168\u9762\u5206\u6790\uff0c\u4e14\u4e2a\u4f53\u5dee\u5f02\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "MAGE\u7ed3\u5408\u591a\u4efb\u52a1\u67b6\u6784\uff0c\u7f16\u7801\u9762\u90e8\u56fe\u50cf\u7684\u65b9\u5411\u548c\u4f4d\u7f6e\u7279\u5f81\uff0c\u5e76\u901a\u8fc7Easy-Calibration\u6a21\u5757\u51cf\u5c11\u4e2a\u4f53\u5dee\u5f02\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMAGE\u5728MPIIFaceGaze\u3001EYEDIAP\u548cIMRGaze\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MAGE\u4e3a\u771f\u5b9e\u4e16\u754c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u9ad8\u6548\u76846-DoF\u89c6\u7ebf\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16392", "pdf": "https://arxiv.org/pdf/2505.16392", "abs": "https://arxiv.org/abs/2505.16392", "authors": ["Benjamin Vendeville", "Liana Ermakova", "Pierre De Loor"], "title": "Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection", "categories": ["cs.CL", "cs.AI", "I.2.6; I.5.2"], "comment": "Accepted at SIGIR 2025", "summary": "The general public often encounters complex texts but does not have the time\nor expertise to fully understand them, leading to the spread of misinformation.\nAutomatic Text Simplification (ATS) helps make information more accessible, but\nits evaluation methods have not kept up with advances in text generation,\nespecially with Large Language Models (LLMs). In particular, recent studies\nhave shown that current ATS metrics do not correlate with the presence of\nerrors. Manual inspections have further revealed a variety of errors,\nunderscoring the need for a more nuanced evaluation framework, which is\ncurrently lacking. This resource paper addresses this gap by introducing a test\ncollection for detecting and classifying errors in simplified texts. First, we\npropose a taxonomy of errors, with a formal focus on information distortion.\nNext, we introduce a parallel dataset of automatically simplified scientific\ntexts. This dataset has been human-annotated with labels based on our proposed\ntaxonomy. Finally, we analyze the quality of the dataset, and we study the\nperformance of existing models to detect and classify errors from that\ntaxonomy. These contributions give researchers the tools to better evaluate\nerrors in ATS, develop more reliable models, and ultimately improve the quality\nof automatically simplified texts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u96c6\u548c\u9519\u8bef\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u6587\u672c\u7b80\u5316\uff08ATS\uff09\u4e2d\u7684\u9519\u8bef\uff0c\u586b\u8865\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u516c\u4f17\u5e38\u56e0\u590d\u6742\u6587\u672c\u96be\u4ee5\u7406\u89e3\u800c\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\uff0c\u800c\u73b0\u6709\u7684ATS\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u8ddf\u4e0a\u6587\u672c\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u3002", "method": "1. \u63d0\u51fa\u9519\u8bef\u5206\u7c7b\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u4fe1\u606f\u5931\u771f\uff1b2. \u5f15\u5165\u5e76\u884c\u6570\u636e\u96c6\uff0c\u5305\u542b\u81ea\u52a8\u7b80\u5316\u7684\u79d1\u5b66\u6587\u672c\uff0c\u5e76\u4eba\u5de5\u6807\u6ce8\uff1b3. \u5206\u6790\u6570\u636e\u96c6\u8d28\u91cf\u53ca\u73b0\u6709\u6a21\u578b\u5728\u9519\u8bef\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u63d0\u4f9b\u4e86\u5de5\u5177\u4ee5\u66f4\u51c6\u786e\u5730\u8bc4\u4f30ATS\u4e2d\u7684\u9519\u8bef\uff0c\u5e2e\u52a9\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6539\u8fdb\u81ea\u52a8\u7b80\u5316\u6587\u672c\u8d28\u91cf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86ATS\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2505.16399", "pdf": "https://arxiv.org/pdf/2505.16399", "abs": "https://arxiv.org/abs/2505.16399", "authors": ["Qian Deng", "Le Hui", "Jin Xie", "Jian Yang"], "title": "Sketchy Bounding-box Supervision for 3D Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Bounding box supervision has gained considerable attention in weakly\nsupervised 3D instance segmentation. While this approach alleviates the need\nfor extensive point-level annotations, obtaining accurate bounding boxes in\npractical applications remains challenging. To this end, we explore the\ninaccurate bounding box, named sketchy bounding box, which is imitated through\nperturbing ground truth bounding box by adding scaling, translation, and\nrotation. In this paper, we propose Sketchy-3DIS, a novel weakly 3D instance\nsegmentation framework, which jointly learns pseudo labeler and segmentator to\nimprove the performance under the sketchy bounding-box supervisions.\nSpecifically, we first propose an adaptive box-to-point pseudo labeler that\nadaptively learns to assign points located in the overlapped parts between two\nsketchy bounding boxes to the correct instance, resulting in compact and pure\npseudo instance labels. Then, we present a coarse-to-fine instance segmentator\nthat first predicts coarse instances from the entire point cloud and then\nlearns fine instances based on the region of coarse instances. Finally, by\nusing the pseudo instance labels to supervise the instance segmentator, we can\ngradually generate high-quality instances through joint training. Extensive\nexperiments show that our method achieves state-of-the-art performance on both\nthe ScanNetV2 and S3DIS benchmarks, and even outperforms several fully\nsupervised methods using sketchy bounding boxes. Code is available at\nhttps://github.com/dengq7/Sketchy-3DIS.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSketchy-3DIS\u7684\u5f31\u76d1\u77633D\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u4f2a\u6807\u7b7e\u751f\u6210\u5668\u548c\u5206\u5272\u5668\uff0c\u5728\u4e0d\u7cbe\u786e\u7684\u8fb9\u754c\u6846\u76d1\u7763\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5728\u5f31\u76d1\u77633D\u5b9e\u4f8b\u5206\u5272\u4e2d\uff0c\u8fb9\u754c\u6846\u76d1\u7763\u867d\u7136\u51cf\u5c11\u4e86\u70b9\u7ea7\u6807\u6ce8\u7684\u9700\u6c42\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u83b7\u53d6\u7cbe\u786e\u8fb9\u754c\u6846\u4ecd\u5177\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u63a2\u7d22\u4e86\u4e0d\u7cbe\u786e\u8fb9\u754c\u6846\uff08\u79f0\u4e3a\u8349\u56fe\u8fb9\u754c\u6846\uff09\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6846\u5230\u70b9\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u89e3\u51b3\u8349\u56fe\u8fb9\u754c\u6846\u91cd\u53e0\u90e8\u5206\u7684\u70b9\u5206\u914d\u95ee\u9898\uff1b\u8bbe\u8ba1\u7c97\u5230\u7ec6\u5b9e\u4f8b\u5206\u5272\u5668\uff0c\u5148\u9884\u6d4b\u7c97\u5b9e\u4f8b\uff0c\u518d\u5b66\u4e60\u7ec6\u5b9e\u4f8b\uff1b\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u751f\u6210\u9ad8\u8d28\u91cf\u5b9e\u4f8b\u3002", "result": "\u5728ScanNetV2\u548cS3DIS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u90e8\u5206\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "Sketchy-3DIS\u6846\u67b6\u5728\u4e0d\u7cbe\u786e\u8fb9\u754c\u6846\u76d1\u7763\u4e0b\u6709\u6548\u63d0\u5347\u4e863D\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f31\u76d1\u7763\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16406", "pdf": "https://arxiv.org/pdf/2505.16406", "abs": "https://arxiv.org/abs/2505.16406", "authors": ["Gaofei Shen", "Hosein Mohebbi", "Arianna Bisazza", "Afra Alishahi", "Grzegorz Chrupa\u0142a"], "title": "On the reliability of feature attribution methods for speech classification", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "As the capabilities of large-scale pre-trained models evolve, understanding\nthe determinants of their outputs becomes more important. Feature attribution\naims to reveal which parts of the input elements contribute the most to model\noutputs. In speech processing, the unique characteristics of the input signal\nmake the application of feature attribution methods challenging. We study how\nfactors such as input type and aggregation and perturbation timespan impact the\nreliability of standard feature attribution methods, and how these factors\ninteract with characteristics of each classification task. We find that\nstandard approaches to feature attribution are generally unreliable when\napplied to the speech domain, with the exception of word-aligned perturbation\nmethods when applied to word-based classification tasks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8bed\u97f3\u5904\u7406\u4e2d\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u6807\u51c6\u65b9\u6cd5\u5728\u8bed\u97f3\u9886\u57df\u901a\u5e38\u4e0d\u53ef\u9760\uff0c\u9664\u975e\u662f\u5355\u8bcd\u5bf9\u9f50\u7684\u6270\u52a8\u65b9\u6cd5\u7528\u4e8e\u57fa\u4e8e\u5355\u8bcd\u7684\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7406\u89e3\u5176\u8f93\u51fa\u51b3\u5b9a\u56e0\u7d20\u53d8\u5f97\u66f4\u91cd\u8981\u3002\u7279\u5f81\u5f52\u56e0\u65e8\u5728\u63ed\u793a\u8f93\u5165\u4e2d\u54ea\u4e9b\u90e8\u5206\u5bf9\u6a21\u578b\u8f93\u51fa\u8d21\u732e\u6700\u5927\uff0c\u4f46\u5728\u8bed\u97f3\u5904\u7406\u4e2d\uff0c\u8f93\u5165\u4fe1\u53f7\u7684\u72ec\u7279\u7279\u6027\u4f7f\u5f97\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u5e94\u7528\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u7814\u7a76\u4e86\u8f93\u5165\u7c7b\u578b\u3001\u805a\u5408\u548c\u6270\u52a8\u65f6\u95f4\u8de8\u5ea6\u7b49\u56e0\u7d20\u5bf9\u6807\u51c6\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u53ef\u9760\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u56e0\u7d20\u4e0e\u6bcf\u4e2a\u5206\u7c7b\u4efb\u52a1\u7279\u6027\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u6807\u51c6\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5728\u8bed\u97f3\u9886\u57df\u901a\u5e38\u4e0d\u53ef\u9760\uff0c\u4f46\u5355\u8bcd\u5bf9\u9f50\u7684\u6270\u52a8\u65b9\u6cd5\u5728\u57fa\u4e8e\u5355\u8bcd\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f8b\u5916\u3002", "conclusion": "\u8bed\u97f3\u5904\u7406\u4e2d\u6807\u51c6\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u6709\u9650\uff0c\u9700\u9488\u5bf9\u4efb\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u3002"}}
{"id": "2505.16402", "pdf": "https://arxiv.org/pdf/2505.16402", "abs": "https://arxiv.org/abs/2505.16402", "authors": ["Yuanhao Huang", "Yilong Ren", "Jinlei Wang", "Lujia Huo", "Xuesong Bai", "Jinchuan Zhang", "Haiyan Yu"], "title": "AdvReal: Adversarial Patch Generation Framework with Application to Adversarial Safety Evaluation of Object Detection Systems", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous vehicles are typical complex intelligent systems with artificial\nintelligence at their core. However, perception methods based on deep learning\nare extremely vulnerable to adversarial samples, resulting in safety accidents.\nHow to generate effective adversarial examples in the physical world and\nevaluate object detection systems is a huge challenge. In this study, we\npropose a unified joint adversarial training framework for both 2D and 3D\nsamples to address the challenges of intra-class diversity and environmental\nvariations in real-world scenarios. Building upon this framework, we introduce\nan adversarial sample reality enhancement approach that incorporates non-rigid\nsurface modeling and a realistic 3D matching mechanism. We compare with 5\nadvanced adversarial patches and evaluate their attack performance on 8 object\ndetecotrs, including single-stage, two-stage, and transformer-based models.\nExtensive experiment results in digital and physical environments demonstrate\nthat the adversarial textures generated by our method can effectively mislead\nthe target detection model. Moreover, proposed method demonstrates excellent\nrobustness and transferability under multi-angle attacks, varying lighting\nconditions, and different distance in the physical world. The demo video and\ncode can be obtained at https://github.com/Huangyh98/AdvReal.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u62102D\u548c3D\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u7269\u7406\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u611f\u77e5\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u6837\u672c\u653b\u51fb\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\uff0c\u9700\u89e3\u51b3\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u591a\u6837\u6027\u548c\u73af\u5883\u53d8\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8054\u5408\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u975e\u521a\u6027\u8868\u9762\u5efa\u6a21\u548c3D\u5339\u914d\u673a\u5236\uff0c\u751f\u6210\u5bf9\u6297\u7eb9\u7406\u3002", "result": "\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u80fd\u6709\u6548\u8bef\u5bfc\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5728\u591a\u89d2\u5ea6\u653b\u51fb\u3001\u5149\u7167\u53d8\u5316\u548c\u4e0d\u540c\u8ddd\u79bb\u4e0b\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7269\u7406\u73af\u5883\u4e2d\u5177\u6709\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u8fc1\u79fb\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16408", "pdf": "https://arxiv.org/pdf/2505.16408", "abs": "https://arxiv.org/abs/2505.16408", "authors": ["Muhammad Farid Adilazuarda", "Chen Cecilia Liu", "Iryna Gurevych", "Alham Fikri Aji"], "title": "From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Adapting cultural values in Large Language Models (LLMs) presents significant\nchallenges, particularly due to biases and limited training data. Prior work\nprimarily aligns LLMs with different cultural values using World Values Survey\n(WVS) data. However, it remains unclear whether this approach effectively\ncaptures cultural nuances or produces distinct cultural representations for\nvarious downstream tasks. In this paper, we systematically investigate\nWVS-based training for cultural value adaptation and find that relying solely\non survey data can homogenize cultural norms and interfere with factual\nknowledge. To investigate these issues, we augment WVS with encyclopedic and\nscenario-based cultural narratives from Wikipedia and NormAd. While these\nnarratives may have variable effects on downstream tasks, they consistently\nimprove cultural distinctiveness than survey data alone. Our work highlights\nthe inherent complexity of aligning cultural values with the goal of guiding\ntask-specific behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u4f9d\u8d56\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\uff08WVS\uff09\u6570\u636e\u8c03\u6574\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6587\u5316\u4ef7\u503c\u89c2\u53ef\u80fd\u5bfc\u81f4\u6587\u5316\u540c\u8d28\u5316\u5e76\u5e72\u6270\u4e8b\u5b9e\u77e5\u8bc6\u3002\u901a\u8fc7\u7ed3\u5408\u7ef4\u57fa\u767e\u79d1\u548cNormAd\u7684\u767e\u79d1\u5168\u4e66\u548c\u60c5\u666f\u6587\u5316\u53d9\u4e8b\uff0c\u53ef\u4ee5\u63d0\u5347\u6587\u5316\u72ec\u7279\u6027\u3002", "motivation": "\u63a2\u8ba8WVS\u6570\u636e\u5728LLM\u6587\u5316\u4ef7\u503c\u89c2\u9002\u5e94\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u8865\u5145\u6570\u636e\u6539\u5584\u6587\u5316\u72ec\u7279\u6027\u3002", "method": "\u7cfb\u7edf\u7814\u7a76WVS\u6570\u636e\u8bad\u7ec3\uff0c\u5e76\u8865\u5145\u7ef4\u57fa\u767e\u79d1\u548cNormAd\u7684\u6587\u5316\u53d9\u4e8b\u6570\u636e\u3002", "result": "\u8865\u5145\u53d9\u4e8b\u6570\u636e\u6bd4\u5355\u72ec\u4f7f\u7528WVS\u6570\u636e\u66f4\u80fd\u63d0\u5347\u6587\u5316\u72ec\u7279\u6027\uff0c\u4f46\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6548\u679c\u4e0d\u4e00\u3002", "conclusion": "\u6587\u5316\u4ef7\u503c\u89c2\u5bf9\u9f50\u5177\u6709\u590d\u6742\u6027\uff0c\u9700\u7ed3\u5408\u591a\u79cd\u6570\u636e\u6e90\u4ee5\u5f15\u5bfc\u4efb\u52a1\u7279\u5b9a\u884c\u4e3a\u3002"}}
{"id": "2505.16411", "pdf": "https://arxiv.org/pdf/2505.16411", "abs": "https://arxiv.org/abs/2505.16411", "authors": ["Sreetama Sarkar", "Yue Che", "Alex Gavin", "Peter A. Beerel", "Souvik Kundu"], "title": "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Despite their remarkable progress in multimodal understanding tasks, large\nvision language models (LVLMs) often suffer from \"hallucinations\", generating\ntexts misaligned with the visual context. Existing methods aimed at reducing\nhallucinations through inference time intervention incur a significant increase\nin latency. To mitigate this, we present SPIN, a task-agnostic attention-guided\nhead suppression strategy that can be seamlessly integrated during inference,\nwithout incurring any significant compute or latency overhead. We investigate\nwhether hallucination in LVLMs can be linked to specific model components. Our\nanalysis suggests that hallucinations can be attributed to a dynamic subset of\nattention heads in each layer. Leveraging this insight, for each text query\ntoken, we selectively suppress attention heads that exhibit low attention to\nimage tokens, keeping the top-K attention heads intact. Extensive evaluations\non visual question answering and image description tasks demonstrate the\nefficacy of SPIN in reducing hallucination scores up to 2.7x while maintaining\nF1, and improving throughput by 1.8x compared to existing alternatives. Code is\navailable at https://github.com/YUECHE77/SPIN.", "AI": {"tldr": "SPIN\u662f\u4e00\u79cd\u4efb\u52a1\u65e0\u5173\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u5934\u6291\u5236\u7b56\u7565\uff0c\u53ef\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u4e14\u4e0d\u589e\u52a0\u8ba1\u7b97\u6216\u5ef6\u8fdf\u5f00\u9500\u3002", "motivation": "LVLMs\u5728\u751f\u6210\u6587\u672c\u65f6\u5b58\u5728\u4e0e\u89c6\u89c9\u4e0a\u4e0b\u6587\u4e0d\u7b26\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u80fd\u51cf\u5c11\u5e7b\u89c9\u4f46\u663e\u8457\u589e\u52a0\u5ef6\u8fdf\u3002", "method": "\u901a\u8fc7\u5206\u6790\u53d1\u73b0\u5e7b\u89c9\u4e0e\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u76f8\u5173\uff0cSPIN\u9009\u62e9\u6027\u6291\u5236\u5bf9\u56fe\u50cf\u4ee4\u724c\u6ce8\u610f\u529b\u4f4e\u7684\u5934\uff0c\u4fdd\u7559\u524dK\u4e2a\u5934\u3002", "result": "\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\uff0cSPIN\u5c06\u5e7b\u89c9\u5206\u6570\u964d\u4f4e2.7\u500d\uff0cF1\u4fdd\u6301\u7a33\u5b9a\uff0c\u541e\u5410\u91cf\u63d0\u53471.8\u500d\u3002", "conclusion": "SPIN\u6709\u6548\u51cf\u5c11LVLMs\u7684\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2505.16410", "pdf": "https://arxiv.org/pdf/2505.16410", "abs": "https://arxiv.org/abs/2505.16410", "authors": ["Guanting Dong", "Yifei Chen", "Xiaoxi Li", "Jiajie Jin", "Hongjin Qian", "Yutao Zhu", "Hangyu Mao", "Guorui Zhou", "Zhicheng Dou", "Ji-Rong Wen"], "title": "Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Working in progress", "summary": "Recently, large language models (LLMs) have shown remarkable reasoning\ncapabilities via large-scale reinforcement learning (RL). However, leveraging\nthe RL algorithm to empower effective multi-tool collaborative reasoning in\nLLMs remains an open challenge. In this paper, we introduce Tool-Star, an\nRL-based framework designed to empower LLMs to autonomously invoke multiple\nexternal tools during stepwise reasoning. Tool-Star integrates six types of\ntools and incorporates systematic designs in both data synthesis and training.\nTo address the scarcity of tool-use data, we propose a general tool-integrated\nreasoning data synthesis pipeline, which combines tool-integrated prompting\nwith hint-based sampling to automatically and scalably generate tool-use\ntrajectories. A subsequent quality normalization and difficulty-aware\nclassification process filters out low-quality samples and organizes the\ndataset from easy to hard. Furthermore, we propose a two-stage training\nframework to enhance multi-tool collaborative reasoning by: (1) cold-start\nfine-tuning, which guides LLMs to explore reasoning patterns via\ntool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with\nhierarchical reward design, which reinforces reward understanding and promotes\neffective tool collaboration. Experimental analyses on over 10 challenging\nreasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.\nThe code is available at https://github.com/dongguanting/Tool-Star.", "AI": {"tldr": "Tool-Star\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9010\u6b65\u63a8\u7406\u4e2d\u81ea\u4e3b\u8c03\u7528\u591a\u5de5\u5177\u7684\u80fd\u529b\u3002\u901a\u8fc7\u6570\u636e\u5408\u6210\u548c\u8bad\u7ec3\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u5de5\u5177\u4f7f\u7528\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u591a\u5de5\u5177\u534f\u4f5c\u63a8\u7406\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faTool-Star\u6846\u67b6\uff0c\u5305\u62ec\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u51b7\u542f\u52a8\u5fae\u8c03\u548c\u591a\u5de5\u5177\u81ea\u8bc4RL\u7b97\u6cd5\uff09\u3002", "result": "\u572810\u591a\u4e2a\u6311\u6218\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86Tool-Star\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "Tool-Star\u901a\u8fc7\u7cfb\u7edf\u8bbe\u8ba1\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u591a\u5de5\u5177\u534f\u4f5c\u63a8\u7406\u4e2d\u7684\u80fd\u529b\u3002"}}
{"id": "2505.16412", "pdf": "https://arxiv.org/pdf/2505.16412", "abs": "https://arxiv.org/abs/2505.16412", "authors": ["Nikolay Stanishev", "Yuhang Lu", "Touradj Ebrahimi"], "title": "Pose-invariant face recognition via feature-space pose frontalization", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Pose-invariant face recognition has become a challenging problem for modern\nAI-based face recognition systems. It aims at matching a profile face captured\nin the wild with a frontal face registered in a database. Existing methods\nperform face frontalization via either generative models or learning a pose\nrobust feature representation. In this paper, a new method is presented to\nperform face frontalization and recognition within the feature space. First, a\nnovel feature space pose frontalization module (FSPFM) is proposed to transform\nprofile images with arbitrary angles into frontal counterparts. Second, a new\ntraining paradigm is proposed to maximize the potential of FSPFM and boost its\nperformance. The latter consists of a pre-training and an attention-guided\nfine-tuning stage. Moreover, extensive experiments have been conducted on five\npopular face recognition benchmarks. Results show that not only our method\noutperforms the state-of-the-art in the pose-invariant face recognition task\nbut also maintains superior performance in other standard scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4eba\u8138\u6b63\u9762\u5316\u548c\u8bc6\u522b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u59ff\u6001\u6b63\u9762\u5316\u6a21\u5757\uff08FSPFM\uff09\u548c\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u59ff\u6001\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u662f\u73b0\u4ee3AI\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u7684\u6311\u6218\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u6a21\u578b\u6216\u5b66\u4e60\u59ff\u6001\u9c81\u68d2\u7279\u5f81\u5b9e\u73b0\u6b63\u9762\u5316\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51faFSPFM\u6a21\u5757\u5c06\u4efb\u610f\u89d2\u5ea6\u7684\u4fa7\u9762\u56fe\u50cf\u8f6c\u6362\u4e3a\u6b63\u9762\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u9884\u8bad\u7ec3\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u5fae\u8c03\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u6d41\u884c\u7684\u4eba\u8138\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u59ff\u6001\u4e0d\u53d8\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u5176\u4ed6\u6807\u51c6\u573a\u666f\u4e2d\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4eba\u8138\u6b63\u9762\u5316\u548c\u8bc6\u522b\uff0c\u4e3a\u59ff\u6001\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16415", "pdf": "https://arxiv.org/pdf/2505.16415", "abs": "https://arxiv.org/abs/2505.16415", "authors": ["Ruizhe Li", "Chen Chen", "Yuchen Hu", "Yanjun Gao", "Xi Wang", "Emine Yilmaz"], "title": "Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in process", "summary": "Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)\ncombined with external contexts to enhance the accuracy and reliability of\ngenerated responses. However, reliably attributing generated content to\nspecific context segments, context attribution, remains challenging due to the\ncomputationally intensive nature of current methods, which often require\nextensive fine-tuning or human annotation. In this work, we introduce a novel\nJensen-Shannon Divergence driven method to Attribute Response to Context\n(ARC-JSD), enabling efficient and accurate identification of essential context\nsentences without additional fine-tuning or surrogate modelling. Evaluations on\na wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using\ninstruction-tuned LLMs in different scales demonstrate superior accuracy and\nsignificant computational efficiency improvements compared to the previous\nsurrogate-based method. Furthermore, our mechanistic analysis reveals specific\nattention heads and multilayer perceptron (MLP) layers responsible for context\nattribution, providing valuable insights into the internal workings of RAG\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eJensen-Shannon Divergence\u7684\u65b9\u6cd5ARC-JSD\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u5728RAG\u6a21\u578b\u4e2d\u8bc6\u522b\u5173\u952e\u4e0a\u4e0b\u6587\u53e5\u5b50\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u4ee3\u7406\u5efa\u6a21\u3002", "motivation": "\u5f53\u524dRAG\u6a21\u578b\u5728\u5185\u5bb9\u751f\u6210\u65f6\u96be\u4ee5\u53ef\u9760\u5730\u5c06\u751f\u6210\u5185\u5bb9\u5f52\u56e0\u4e8e\u7279\u5b9a\u4e0a\u4e0b\u6587\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u4f7f\u7528Jensen-Shannon Divergence\u9a71\u52a8\u7684\u65b9\u6cd5ARC-JSD\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u4ee3\u7406\u5efa\u6a21\uff0c\u76f4\u63a5\u8bc6\u522b\u5173\u952e\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2aRAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u8d1f\u8d23\u4e0a\u4e0b\u6587\u5f52\u56e0\u7684\u7279\u5b9a\u7ed3\u6784\u3002", "conclusion": "ARC-JSD\u4e3aRAG\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u4e0a\u4e0b\u6587\u5f52\u56e0\u65b9\u6cd5\uff0c\u5e76\u6df1\u5165\u89e3\u6790\u4e86\u6a21\u578b\u5185\u90e8\u673a\u5236\u3002"}}
{"id": "2505.16416", "pdf": "https://arxiv.org/pdf/2505.16416", "abs": "https://arxiv.org/abs/2505.16416", "authors": ["Chengcheng Wang", "Jianyuan Guo", "Hongguang Li", "Yuchuan Tian", "Ying Nie", "Chang Xu", "Kai Han"], "title": "Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is a widely adopted technique for encoding\nrelative positional information in large language models (LLMs). However, when\nextended to large vision-language models (LVLMs), its variants introduce\nunintended cross-modal positional biases. Specifically, they enforce relative\npositional dependencies between text token indices and image tokens, causing\nspurious alignments. This issue arises because image tokens representing the\nsame content but located at different spatial positions are assigned distinct\npositional biases, leading to inconsistent cross-modal associations. To address\nthis, we propose Per-Token Distance (PTD) - a simple yet effective metric for\nquantifying the independence of positional encodings across modalities.\nInformed by this analysis, we introduce Circle-RoPE, a novel encoding scheme\nthat maps image token indices onto a circular trajectory orthogonal to the\nlinear path of text token indices, forming a cone-like structure. This\nconfiguration ensures that each text token maintains an equal distance to all\nimage tokens, reducing artificial cross-modal biases while preserving\nintra-image spatial information. To further enhance performance, we propose a\nstaggered layer strategy that applies different RoPE variants across layers.\nThis design leverages the complementary strengths of each RoPE variant, thereby\nenhancing the model's overall performance. Our experimental results demonstrate\nthat our method effectively preserves spatial information from images while\nreducing relative positional bias, offering a more robust and flexible\npositional encoding framework for LVLMs. The code is available at\n[https://github.com/lose4578/CircleRoPE](https://github.com/lose4578/CircleRoPE).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCircle-RoPE\u7684\u65b0\u578b\u4f4d\u7f6e\u7f16\u7801\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8de8\u6a21\u6001\u4f4d\u7f6e\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "Rotary Position Embedding (RoPE)\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5f15\u5165\u8de8\u6a21\u6001\u4f4d\u7f6e\u504f\u5dee\uff0c\u5bfc\u81f4\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u865a\u5047\u5bf9\u9f50\u3002", "method": "\u63d0\u51faPer-Token Distance (PTD)\u5ea6\u91cf\u8de8\u6a21\u6001\u4f4d\u7f6e\u7f16\u7801\u72ec\u7acb\u6027\uff0c\u5e76\u8bbe\u8ba1Circle-RoPE\u65b9\u6848\uff0c\u5c06\u56fe\u50cf\u6807\u8bb0\u6620\u5c04\u5230\u4e0e\u6587\u672c\u6807\u8bb0\u6b63\u4ea4\u7684\u5706\u5f62\u8f68\u8ff9\u4e0a\u3002\u6b64\u5916\uff0c\u91c7\u7528\u5206\u5c42\u7b56\u7565\u5e94\u7528\u4e0d\u540cRoPE\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCircle-RoPE\u80fd\u6709\u6548\u51cf\u5c11\u8de8\u6a21\u6001\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\u50cf\u7a7a\u95f4\u4fe1\u606f\u3002", "conclusion": "Circle-RoPE\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u4f4d\u7f6e\u7f16\u7801\u6846\u67b6\u3002"}}
{"id": "2505.16418", "pdf": "https://arxiv.org/pdf/2505.16418", "abs": "https://arxiv.org/abs/2505.16418", "authors": ["Yoichi Aoki", "Soichiro Murakami", "Ukyo Honda", "Akihiko Kato"], "title": "Exploring the Relationship Between Diversity and Quality in Ad Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "In natural language generation for advertising, creating diverse and engaging\nad texts is crucial for capturing a broad audience and avoiding advertising\nfatigue. Regardless of the importance of diversity, the impact of the\ndiversity-enhancing methods in ad text generation -- mainly tested on tasks\nsuch as summarization and machine translation -- has not been thoroughly\nexplored. Ad text generation significantly differs from these tasks owing to\nthe text style and requirements. This research explores the relationship\nbetween diversity and ad quality in ad text generation by considering multiple\nfactors, such as diversity-enhancing methods, their hyperparameters,\ninput-output formats, and the models.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5e7f\u544a\u6587\u672c\u751f\u6210\u4e2d\u591a\u6837\u6027\u4e0e\u5e7f\u544a\u8d28\u91cf\u7684\u5173\u7cfb\uff0c\u5206\u6790\u4e86\u591a\u6837\u6027\u589e\u5f3a\u65b9\u6cd5\u53ca\u5176\u53c2\u6570\u7684\u5f71\u54cd\u3002", "motivation": "\u5e7f\u544a\u6587\u672c\u751f\u6210\u9700\u8981\u591a\u6837\u6027\u548c\u5438\u5f15\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5176\u4ed6\u4efb\u52a1\uff08\u5982\u6458\u8981\u548c\u7ffb\u8bd1\uff09\uff0c\u672a\u5145\u5206\u63a2\u7d22\u5176\u5728\u5e7f\u544a\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u6837\u6027\u589e\u5f3a\u65b9\u6cd5\u3001\u8d85\u53c2\u6570\u3001\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u548c\u6a21\u578b\uff0c\u7814\u7a76\u591a\u6837\u6027\u4e0e\u5e7f\u544a\u8d28\u91cf\u7684\u5173\u7cfb\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u5f3a\u8c03\u4e86\u5e7f\u544a\u6587\u672c\u751f\u6210\u7684\u7279\u6b8a\u6027\u3002", "conclusion": "\u5e7f\u544a\u6587\u672c\u751f\u6210\u9700\u8981\u9488\u5bf9\u5176\u72ec\u7279\u9700\u6c42\u4f18\u5316\u591a\u6837\u6027\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2505.16419", "pdf": "https://arxiv.org/pdf/2505.16419", "abs": "https://arxiv.org/abs/2505.16419", "authors": ["Soh Takahashi", "Masaru Sasaki", "Ken Takeda", "Masafumi Oizumi"], "title": "Investigating Fine- and Coarse-grained Structural Correspondences Between Deep Neural Networks and Human Object Image Similarity Judgments Using Unsupervised Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "34 pages, 6 figures", "summary": "The learning mechanisms by which humans acquire internal representations of\nobjects are not fully understood. Deep neural networks (DNNs) have emerged as a\nuseful tool for investigating this question, as they have internal\nrepresentations similar to those of humans as a byproduct of optimizing their\nobjective functions. While previous studies have shown that models trained with\nvarious learning paradigms - such as supervised, self-supervised, and CLIP -\nacquire human-like representations, it remains unclear whether their similarity\nto human representations is primarily at a coarse category level or extends to\nfiner details. Here, we employ an unsupervised alignment method based on\nGromov-Wasserstein Optimal Transport to compare human and model object\nrepresentations at both fine-grained and coarse-grained levels. The unique\nfeature of this method compared to conventional representational similarity\nanalysis is that it estimates optimal fine-grained mappings between the\nrepresentation of each object in human and model representations. We used this\nunsupervised alignment method to assess the extent to which the representation\nof each object in humans is correctly mapped to the corresponding\nrepresentation of the same object in models. Using human similarity judgments\nof 1,854 objects from the THINGS dataset, we find that models trained with CLIP\nconsistently achieve strong fine- and coarse-grained matching with human object\nrepresentations. In contrast, self-supervised models showed limited matching at\nboth fine- and coarse-grained levels, but still formed object clusters that\nreflected human coarse category structure. Our results offer new insights into\nthe role of linguistic information in acquiring precise object representations\nand the potential of self-supervised learning to capture coarse categorical\nstructures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7269\u4f53\u8868\u5f81\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u53d1\u73b0CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u4e0a\u5747\u4e0e\u4eba\u7c7b\u8868\u5f81\u5339\u914d\u826f\u597d\uff0c\u800c\u81ea\u76d1\u7763\u6a21\u578b\u4ec5\u80fd\u6355\u6349\u7c97\u7c92\u5ea6\u7ed3\u6784\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7269\u4f53\u8868\u5f81\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u5c24\u5176\u662f\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eGromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u7684\u65e0\u76d1\u7763\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u6a21\u578b\u5728\u7269\u4f53\u8868\u5f81\u4e0a\u7684\u5339\u914d\u60c5\u51b5\u3002", "result": "CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u4e0a\u5747\u4e0e\u4eba\u7c7b\u8868\u5f81\u5339\u914d\u826f\u597d\uff0c\u81ea\u76d1\u7763\u6a21\u578b\u4ec5\u80fd\u6355\u6349\u7c97\u7c92\u5ea6\u7ed3\u6784\u3002", "conclusion": "\u8bed\u8a00\u4fe1\u606f\u5bf9\u7cbe\u786e\u7269\u4f53\u8868\u5f81\u7684\u83b7\u53d6\u81f3\u5173\u91cd\u8981\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u6355\u6349\u7c97\u7c92\u5ea6\u7ed3\u6784\u4e0a\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2505.16421", "pdf": "https://arxiv.org/pdf/2505.16421", "abs": "https://arxiv.org/abs/2505.16421", "authors": ["Zhepei Wei", "Wenlin Yao", "Yao Liu", "Weizhi Zhang", "Qin Lu", "Liang Qiu", "Changlong Yu", "Puyang Xu", "Chao Zhang", "Bing Yin", "Hyokun Yun", "Lihong Li"], "title": "WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "While reinforcement learning (RL) has demonstrated remarkable success in\nenhancing large language models (LLMs), it has primarily focused on single-turn\ntasks such as solving math problems. Training effective web agents for\nmulti-turn interactions remains challenging due to the complexity of\nlong-horizon decision-making across dynamic web interfaces. In this work, we\npresent WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework\nfor training web agents. It learns directly from online interactions with web\nenvironments by asynchronously generating diverse trajectories, entirely guided\nby binary rewards depending on task success. Experiments on the WebArena-Lite\nbenchmark demonstrate the effectiveness of WebAgent-R1, boosting the task\nsuccess rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to\n44.8%, significantly outperforming existing state-of-the-art methods and strong\nproprietary models such as OpenAI o3. In-depth analyses reveal the\neffectiveness of the thinking-based prompting strategy and test-time scaling\nthrough increased interactions for web tasks. We further investigate different\nRL initialization policies by introducing two variants, namely WebAgent-R1-Zero\nand WebAgent-R1-CoT, which highlight the importance of the warm-up training\nstage (i.e., behavior cloning) and provide insights on incorporating long\nchain-of-thought (CoT) reasoning in web agents.", "AI": {"tldr": "WebAgent-R1\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u7f51\u7edc\u4ee3\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u4efb\u52a1\uff0c\u800c\u591a\u8f6e\u4ea4\u4e92\u7684\u7f51\u7edc\u4ee3\u7406\u8bad\u7ec3\u56e0\u957f\u65f6\u51b3\u7b56\u590d\u6742\u6027\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u5f02\u6b65\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u5b8c\u5168\u4f9d\u8d56\u4efb\u52a1\u6210\u529f\u7684\u4e8c\u5143\u5956\u52b1\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60\u3002", "result": "\u5728WebArena-Lite\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWebAgent-R1\u663e\u8457\u63d0\u5347\u4e86Qwen-2.5-3B\u548cLlama-3.1-8B\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "\u601d\u8003\u578b\u63d0\u793a\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u4ea4\u4e92\u6269\u5c55\u6709\u6548\uff0c\u884c\u4e3a\u514b\u9686\u548c\u957f\u94fe\u63a8\u7406\u5bf9\u521d\u59cb\u5316\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.16422", "pdf": "https://arxiv.org/pdf/2505.16422", "abs": "https://arxiv.org/abs/2505.16422", "authors": ["Xiaoran Yin", "Xu Luo", "Hao Wu", "Lianli Gao", "Jingkuan Song"], "title": "Unlocking Smarter Device Control: Foresighted Planning with a World Model-Driven Code Execution Approach", "categories": ["cs.CV"], "comment": null, "summary": "The automatic control of mobile devices is essential for efficiently\nperforming complex tasks that involve multiple sequential steps. However, these\ntasks pose significant challenges due to the limited environmental information\navailable at each step, primarily through visual observations. As a result,\ncurrent approaches, which typically rely on reactive policies, focus solely on\nimmediate observations and often lead to suboptimal decision-making. To address\nthis problem, we propose \\textbf{Foresighted Planning with World Model-Driven\nCode Execution (FPWC)},a framework that prioritizes natural language\nunderstanding and structured reasoning to enhance the agent's global\nunderstanding of the environment by developing a task-oriented, refinable\n\\emph{world model} at the outset of the task. Foresighted actions are\nsubsequently generated through iterative planning within this world model,\nexecuted in the form of executable code. Extensive experiments conducted in\nsimulated environments and on real mobile devices demonstrate that our method\noutperforms previous approaches, particularly achieving a 44.4\\% relative\nimprovement in task success rate compared to the state-of-the-art in the\nsimulated environment. Code and demo are provided in the supplementary\nmaterial.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFPWC\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u7ed3\u6784\u5316\u63a8\u7406\u589e\u5f3a\u667a\u80fd\u4f53\u5bf9\u73af\u5883\u7684\u5168\u5c40\u7406\u89e3\uff0c\u4ece\u800c\u4f18\u5316\u79fb\u52a8\u8bbe\u5907\u7684\u81ea\u52a8\u63a7\u5236\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u7684\u81ea\u52a8\u63a7\u5236\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u9762\u4e34\u73af\u5883\u4fe1\u606f\u6709\u9650\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5373\u65f6\u89c2\u5bdf\u5bfc\u81f4\u51b3\u7b56\u4e0d\u4f18\u3002", "method": "FPWC\u6846\u67b6\u901a\u8fc7\u4efb\u52a1\u5bfc\u5411\u7684\u4e16\u754c\u6a21\u578b\u548c\u8fed\u4ee3\u89c4\u5212\u751f\u6210\u524d\u77bb\u6027\u52a8\u4f5c\uff0c\u4ee5\u53ef\u6267\u884c\u4ee3\u7801\u5f62\u5f0f\u5b9e\u73b0\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u8bbe\u5907\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cFPWC\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8644.4%\u3002", "conclusion": "FPWC\u901a\u8fc7\u5168\u5c40\u7406\u89e3\u548c\u524d\u77bb\u6027\u89c4\u5212\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u8bbe\u5907\u81ea\u52a8\u63a7\u5236\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16425", "pdf": "https://arxiv.org/pdf/2505.16425", "abs": "https://arxiv.org/abs/2505.16425", "authors": ["Jing Bi", "Pinxin Liu", "Ali Vosoughi", "Jiarui Wu", "Jinxi He", "Chenliang Xu"], "title": "$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures, under review", "summary": "The effective communication of procedural knowledge remains a significant\nchallenge in natural language processing (NLP), as purely textual instructions\noften fail to convey complex physical actions and spatial relationships. We\naddress this limitation by proposing a language-driven framework that\ntranslates procedural text into coherent visual instructions. Our approach\nmodels the linguistic structure of instructional content by decomposing it into\ngoal statements and sequential steps, then conditioning visual generation on\nthese linguistic elements. We introduce three key innovations: (1) a\nconstituency parser-based text encoding mechanism that preserves semantic\ncompleteness even with lengthy instructions, (2) a pairwise discourse coherence\nmodel that maintains consistency across instruction sequences, and (3) a novel\nevaluation protocol specifically designed for procedural language-to-image\nalignment. Our experiments across three instructional datasets (HTStep,\nCaptainCook4D, and WikiAll) demonstrate that our method significantly\noutperforms existing baselines in generating visuals that accurately reflect\nthe linguistic content and sequential nature of instructions. This work\ncontributes to the growing body of research on grounding procedural language in\nvisual content, with applications spanning education, task guidance, and\nmultimodal language understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7a0b\u5e8f\u6027\u6587\u672c\u8f6c\u5316\u4e3a\u89c6\u89c9\u6307\u4ee4\u7684\u8bed\u8a00\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u76ee\u6807\u8bed\u53e5\u548c\u6b65\u9aa4\uff0c\u7ed3\u5408\u4e09\u4e2a\u521b\u65b0\u70b9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u7eaf\u6587\u672c\u6307\u4ee4\u5728\u4f20\u8fbe\u590d\u6742\u7269\u7406\u52a8\u4f5c\u548c\u7a7a\u95f4\u5173\u7cfb\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9009\u533a\u89e3\u6790\u5668\u7684\u6587\u672c\u7f16\u7801\u673a\u5236\u3001\u6210\u5bf9\u8bdd\u8bed\u4e00\u81f4\u6027\u6a21\u578b\u548c\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5c06\u7a0b\u5e8f\u6027\u6587\u672c\u8f6c\u5316\u4e3a\u89c6\u89c9\u6307\u4ee4\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u51c6\u786e\u53cd\u6620\u6307\u4ee4\u5185\u5bb9\u548c\u987a\u5e8f\u7684\u89c6\u89c9\u5185\u5bb9\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7a0b\u5e8f\u6027\u8bed\u8a00\u7684\u89c6\u89c9\u5185\u5bb9\u843d\u5730\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u3001\u4efb\u52a1\u6307\u5bfc\u548c\u591a\u6a21\u6001\u8bed\u8a00\u7406\u89e3\u3002"}}
{"id": "2505.16434", "pdf": "https://arxiv.org/pdf/2505.16434", "abs": "https://arxiv.org/abs/2505.16434", "authors": ["Ranjith Merugu", "Mohammad Sameer Suhail", "Akshay P Sarashetti", "Venkata Bharath Reddy Reddem", "Pankaj Kumar Bajpai", "Amit Satish Unde"], "title": "Joint Flow And Feature Refinement Using Attention For Video Restoration", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Recent advancements in video restoration have focused on recovering\nhigh-quality video frames from low-quality inputs. Compared with static images,\nthe performance of video restoration significantly depends on efficient\nexploitation of temporal correlations among successive video frames. The\nnumerous techniques make use of temporal information via flow-based strategies\nor recurrent architectures. However, these methods often encounter difficulties\nin preserving temporal consistency as they utilize degraded input video frames.\nTo resolve this issue, we propose a novel video restoration framework named\nJoint Flow and Feature Refinement using Attention (JFFRA). The proposed JFFRA\nis based on key philosophy of iteratively enhancing data through the\nsynergistic collaboration of flow (alignment) and restoration. By leveraging\npreviously enhanced features to refine flow and vice versa, JFFRA enables\nefficient feature enhancement using temporal information. This interplay\nbetween flow and restoration is executed at multiple scales, reducing the\ndependence on precise flow estimation. Moreover, we incorporate an\nocclusion-aware temporal loss function to enhance the network's capability in\neliminating flickering artifacts. Comprehensive experiments validate the\nversatility of JFFRA across various restoration tasks such as denoising,\ndeblurring, and super-resolution. Our method demonstrates a remarkable\nperformance improvement of up to 1.62 dB compared to state-of-the-art\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aJFFRA\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5149\u6d41\u548c\u7279\u5f81\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u5728\u5229\u7528\u65f6\u95f4\u4fe1\u606f\u65f6\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u4f9d\u8d56\u7cbe\u786e\u7684\u5149\u6d41\u4f30\u8ba1\u3002", "method": "JFFRA\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u5149\u6d41\u548c\u7279\u5f81\u4fee\u590d\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5904\u7406\u548c\u906e\u6321\u611f\u77e5\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u548c\u8d85\u5206\u8fa8\u7387\u7b49\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe1.62 dB\u3002", "conclusion": "JFFRA\u901a\u8fc7\u534f\u540c\u4f18\u5316\u5149\u6d41\u548c\u7279\u5f81\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u4fee\u590d\u7684\u6548\u679c\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.16429", "pdf": "https://arxiv.org/pdf/2505.16429", "abs": "https://arxiv.org/abs/2505.16429", "authors": ["Song Jin", "Juntian Zhang", "Yuhan Liu", "Xun Zhang", "Yufei Zhang", "Guojun Yin", "Fei Jiang", "Wei Lin", "Rui Yan"], "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional\nA/B testing is resource-intensive, and offline methods struggle with dynamic\nuser-platform interactions. While agent-based simulation is promising, existing\nplatforms often lack a mechanism for user actions to dynamically reshape the\nenvironment. To bridge this gap, we introduce RecInter, a novel agent-based\nsimulation platform for recommender systems featuring a robust interaction\nmechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,\npurchases) dynamically update item attributes in real-time, and introduced\nMerchant Agents can reply, fostering a more realistic and evolving ecosystem.\nHigh-fidelity simulation is ensured through Multidimensional User Profiling\nmodule, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought\n(CoT) enriched interaction data. Our platform achieves significantly improved\nsimulation credibility and successfully replicates emergent phenomena like\nBrand Loyalty and the Matthew Effect. Experiments demonstrate that this\ninteraction mechanism is pivotal for simulating realistic system evolution,\nestablishing our platform as a credible testbed for recommender systems\nresearch.", "AI": {"tldr": "RecInter\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u63a8\u8350\u7cfb\u7edf\u4eff\u771f\u5e73\u53f0\uff0c\u901a\u8fc7\u52a8\u6001\u4ea4\u4e92\u673a\u5236\u548c\u9ad8\u7ea7\u4ee3\u7406\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u7684\u771f\u5b9e\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u4f20\u7edfA/B\u6d4b\u8bd5\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u79bb\u7ebf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u52a8\u6001\u7528\u6237-\u5e73\u53f0\u4ea4\u4e92\uff0c\u73b0\u6709\u4eff\u771f\u5e73\u53f0\u7f3a\u4e4f\u52a8\u6001\u73af\u5883\u91cd\u5851\u673a\u5236\u3002", "method": "\u5f15\u5165RecInter\u5e73\u53f0\uff0c\u652f\u6301\u52a8\u6001\u66f4\u65b0\u7269\u54c1\u5c5e\u6027\u548c\u5546\u5bb6\u4ee3\u7406\u4e92\u52a8\uff0c\u7ed3\u5408\u591a\u7ef4\u7528\u6237\u753b\u50cf\u548c\u57fa\u4e8eCoT\u7684LLM\u5fae\u8c03\u3002", "result": "\u5e73\u53f0\u663e\u8457\u63d0\u5347\u4eff\u771f\u53ef\u4fe1\u5ea6\uff0c\u6210\u529f\u590d\u73b0\u54c1\u724c\u5fe0\u8bda\u5ea6\u548c\u9a6c\u592a\u6548\u5e94\u7b49\u6d8c\u73b0\u73b0\u8c61\u3002", "conclusion": "RecInter\u4e3a\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u4fe1\u7684\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2505.16441", "pdf": "https://arxiv.org/pdf/2505.16441", "abs": "https://arxiv.org/abs/2505.16441", "authors": ["Jisu Han", "Jaemin Na", "Wonjun Hwang"], "title": "Ranked Entropy Minimization for Continual Test-Time Adaptation", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Test-time adaptation aims to adapt to realistic environments in an online\nmanner by learning during test time. Entropy minimization has emerged as a\nprincipal strategy for test-time adaptation due to its efficiency and\nadaptability. Nevertheless, it remains underexplored in continual test-time\nadaptation, where stability is more important. We observe that the entropy\nminimization method often suffers from model collapse, where the model\nconverges to predicting a single class for all images due to a trivial\nsolution. We propose ranked entropy minimization to mitigate the stability\nproblem of the entropy minimization method and extend its applicability to\ncontinuous scenarios. Our approach explicitly structures the prediction\ndifficulty through a progressive masking strategy. Specifically, it gradually\naligns the model's probability distributions across different levels of\nprediction difficulty while preserving the rank order of entropy. The proposed\nmethod is extensively evaluated across various benchmarks, demonstrating its\neffectiveness through empirical results. Our code is available at\nhttps://github.com/pilsHan/rem", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6392\u540d\u71b5\u6700\u5c0f\u5316\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u573a\u666f\u4e2d\u3002", "motivation": "\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f46\u5728\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u5bb9\u6613\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\uff08\u9884\u6d4b\u5355\u4e00\u7c7b\u522b\uff09\u3002", "method": "\u901a\u8fc7\u6e10\u8fdb\u63a9\u7801\u7b56\u7565\u663e\u5f0f\u7ed3\u6784\u5316\u9884\u6d4b\u96be\u5ea6\uff0c\u9010\u6b65\u5bf9\u9f50\u6a21\u578b\u5728\u4e0d\u540c\u9884\u6d4b\u96be\u5ea6\u4e0b\u7684\u6982\u7387\u5206\u5e03\uff0c\u540c\u65f6\u4fdd\u6301\u71b5\u7684\u6392\u540d\u987a\u5e8f\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6392\u540d\u71b5\u6700\u5c0f\u5316\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u71b5\u6700\u5c0f\u5316\u5728\u8fde\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2505.16460", "pdf": "https://arxiv.org/pdf/2505.16460", "abs": "https://arxiv.org/abs/2505.16460", "authors": ["Ikhlasul Akmal Hanif", "Eryawan Presma Yulianrifat", "Jaycent Gunawan Ongris", "Eduardus Tjitrahardja", "Muhammad Falensi Azmi", "Rahmat Bryan Naufal", "Alfan Farizki Wicaksono"], "title": "University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "16 pages, 13 tables, 1 figures", "summary": "This paper presents our approach for SemEval 2025 Task 11 Track A, focusing\non multilabel emotion classification across 28 languages. We explore two main\nstrategies: fully fine-tuning transformer models and classifier-only training,\nevaluating different settings such as fine-tuning strategies, model\narchitectures, loss functions, encoders, and classifiers. Our findings suggest\nthat training a classifier on top of prompt-based encoders such as mE5 and BGE\nyields significantly better results than fully fine-tuning XLMR and mBERT. Our\nbest-performing model on the final leaderboard is an ensemble combining\nmultiple BGE models, where CatBoost serves as the classifier, with different\nconfigurations. This ensemble achieves an average F1-macro score of 56.58\nacross all languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86SemEval 2025 Task 11 Track A\u7684\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u5b8c\u5168\u5fae\u8c03Transformer\u6a21\u578b\u4e0e\u4ec5\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u7b56\u7565\uff0c\u53d1\u73b0\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u7801\u5668\uff08\u5982mE5\u548cBGE\uff09\u8868\u73b0\u66f4\u4f18\u3002\u6700\u4f73\u6a21\u578b\u4e3a\u591aBGE\u6a21\u578b\u7684\u96c6\u6210\uff0c\u5e73\u5747F1-macro\u5f97\u5206\u4e3a56.58\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e0d\u540c\u7b56\u7565\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u5fae\u8c03Transformer\u6a21\u578b\u548c\u4ec5\u8bad\u7ec3\u5206\u7c7b\u5668\u7684\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u8bbe\u7f6e\uff08\u5982\u5fae\u8c03\u7b56\u7565\u3001\u6a21\u578b\u67b6\u6784\u3001\u635f\u5931\u51fd\u6570\u7b49\uff09\u3002", "result": "\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u7801\u5668\uff08\u5982mE5\u548cBGE\uff09\u8868\u73b0\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\u7684XLMR\u548cmBERT\u3002\u6700\u4f73\u6a21\u578b\u4e3a\u591aBGE\u6a21\u578b\u7684\u96c6\u6210\uff0c\u5e73\u5747F1-macro\u5f97\u5206\u4e3a56.58\u3002", "conclusion": "\u572828\u79cd\u8bed\u8a00\u7684\u591a\u6807\u7b7e\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u7801\u5668\u7ed3\u5408\u5206\u7c7b\u5668\u8bad\u7ec3\u662f\u66f4\u4f18\u7b56\u7565\uff0c\u96c6\u6210\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2505.16442", "pdf": "https://arxiv.org/pdf/2505.16442", "abs": "https://arxiv.org/abs/2505.16442", "authors": ["Yichen Li", "Qiankun Liu", "Zhenchao Jin", "Jiuzhe Wei", "Jing Nie", "Ying Fu"], "title": "MAFE R-CNN: Selecting More Samples to Learn Category-aware Features for Small Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection in intricate environments has consistently represented\na major challenge in the field of object detection. In this paper, we identify\nthat this difficulty stems from the detectors' inability to effectively learn\ndiscriminative features for objects of small size, compounded by the complexity\nof selecting high-quality small object samples during training, which motivates\nthe proposal of the Multi-Clue Assignment and Feature Enhancement\nR-CNN.Specifically, MAFE R-CNN integrates two pivotal components.The first is\nthe Multi-Clue Sample Selection (MCSS) strategy, in which the Intersection over\nUnion (IoU) distance, predicted category confidence, and ground truth region\nsizes are leveraged as informative clues in the sample selection process. This\nmethodology facilitates the selection of diverse positive samples and ensures a\nbalanced distribution of object sizes during training, thereby promoting\neffective model learning.The second is the Category-aware Feature Enhancement\nMechanism (CFEM), where we propose a simple yet effective category-aware memory\nmodule to explore the relationships among object features. Subsequently, we\nenhance the object feature representation by facilitating the interaction\nbetween category-aware features and candidate box features.Comprehensive\nexperiments conducted on the large-scale small object dataset SODA validate the\neffectiveness of the proposed method. The code will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMAFE R-CNN\uff0c\u901a\u8fc7\u591a\u7ebf\u7d22\u6837\u672c\u9009\u62e9\u548c\u7c7b\u522b\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u673a\u5236\uff0c\u89e3\u51b3\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7279\u5f81\u5b66\u4e60\u548c\u6837\u672c\u9009\u62e9\u7684\u95ee\u9898\u3002", "motivation": "\u5c0f\u76ee\u6807\u68c0\u6d4b\u56e0\u7279\u5f81\u5b66\u4e60\u4e0d\u8db3\u548c\u9ad8\u8d28\u91cf\u6837\u672c\u9009\u62e9\u56f0\u96be\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u591a\u7ebf\u7d22\u6837\u672c\u9009\u62e9\u7b56\u7565\uff08MCSS\uff09\u548c\u7c7b\u522b\u611f\u77e5\u7279\u5f81\u589e\u5f3a\u673a\u5236\uff08CFEM\uff09\u3002", "result": "\u5728SODA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MAFE R-CNN\u901a\u8fc7\u6539\u8fdb\u6837\u672c\u9009\u62e9\u548c\u7279\u5f81\u589e\u5f3a\uff0c\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2505.16467", "pdf": "https://arxiv.org/pdf/2505.16467", "abs": "https://arxiv.org/abs/2505.16467", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fern\u00e1ndez"], "title": "Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization", "categories": ["cs.CL"], "comment": null, "summary": "Generative Large Language Models (LLMs) infer user's demographic information\nfrom subtle cues in the conversation -- a phenomenon called implicit\npersonalization. Prior work has shown that such inferences can lead to lower\nquality responses for users assumed to be from minority groups, even when no\ndemographic information is explicitly provided. In this work, we systematically\nexplore how LLMs respond to stereotypical cues using controlled synthetic\nconversations, by analyzing the models' latent user representations through\nboth model internals and generated answers to targeted user questions. Our\nfindings reveal that LLMs do infer demographic attributes based on these\nstereotypical signals, which for a number of groups even persists when the user\nexplicitly identifies with a different demographic group. Finally, we show that\nthis form of stereotype-driven implicit personalization can be effectively\nmitigated by intervening on the model's internal representations using a\ntrained linear probe to steer them toward the explicitly stated identity. Our\nresults highlight the need for greater transparency and control in how LLMs\nrepresent user identity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f1a\u901a\u8fc7\u5bf9\u8bdd\u4e2d\u7684\u9690\u6027\u7ebf\u7d22\u63a8\u65ad\u7528\u6237\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u56de\u5e94\u8d28\u91cf\u4e0b\u964d\uff1b\u901a\u8fc7\u5e72\u9884\u6a21\u578b\u5185\u90e8\u8868\u5f81\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8LLMs\u5982\u4f55\u57fa\u4e8e\u523b\u677f\u5370\u8c61\u7ebf\u7d22\u63a8\u65ad\u7528\u6237\u8eab\u4efd\uff0c\u53ca\u5176\u5bf9\u56de\u5e94\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u53d7\u63a7\u5408\u6210\u5bf9\u8bdd\u5206\u6790LLMs\u7684\u6f5c\u5728\u7528\u6237\u8868\u5f81\uff0c\u901a\u8fc7\u6a21\u578b\u5185\u90e8\u548c\u751f\u6210\u7b54\u6848\u8fdb\u884c\u7814\u7a76\u3002", "result": "LLMs\u786e\u5b9e\u57fa\u4e8e\u523b\u677f\u4fe1\u53f7\u63a8\u65ad\u4eba\u53e3\u5c5e\u6027\uff0c\u4e14\u5e72\u9884\u5185\u90e8\u8868\u5f81\u53ef\u6709\u6548\u7f13\u89e3\u504f\u89c1\u3002", "conclusion": "\u9700\u63d0\u9ad8LLMs\u5728\u7528\u6237\u8eab\u4efd\u8868\u5f81\u4e0a\u7684\u900f\u660e\u5ea6\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2505.16447", "pdf": "https://arxiv.org/pdf/2505.16447", "abs": "https://arxiv.org/abs/2505.16447", "authors": ["Oliver Grainge", "Michael Milford", "Indu Bodala", "Sarvapali D. Ramchurn", "Shoaib Ehsan"], "title": "TAT-VPR: Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition", "categories": ["cs.CV"], "comment": null, "summary": "TAT-VPR is a ternary-quantized transformer that brings dynamic\naccuracy-efficiency trade-offs to visual SLAM loop-closure. By fusing ternary\nweights with a learned activation-sparsity gate, the model can control\ncomputation by up to 40% at run-time without degrading performance (Recall@1).\nThe proposed two-stage distillation pipeline preserves descriptor quality,\nletting it run on micro-UAV and embedded SLAM stacks while matching\nstate-of-the-art localization accuracy.", "AI": {"tldr": "TAT-VPR\u662f\u4e00\u79cd\u4e09\u5143\u91cf\u5316Transformer\uff0c\u901a\u8fc7\u52a8\u6001\u6743\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u4e3a\u89c6\u89c9SLAM\u95ed\u73af\u63d0\u4f9b\u652f\u6301\u3002\u7ed3\u5408\u4e09\u5143\u6743\u91cd\u548c\u5b66\u4e60\u6fc0\u6d3b\u7a00\u758f\u95e8\uff0c\u6a21\u578b\u53ef\u5728\u8fd0\u884c\u65f6\u51cf\u5c1140%\u8ba1\u7b97\u91cf\u4e14\u4fdd\u6301\u6027\u80fd\uff08Recall@1\uff09\u3002\u4e24\u9636\u6bb5\u84b8\u998f\u6d41\u7a0b\u4fdd\u6301\u63cf\u8ff0\u7b26\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5fae\u578b\u65e0\u4eba\u673a\u548c\u5d4c\u5165\u5f0fSLAM\uff0c\u8fbe\u5230SOTA\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9SLAM\u95ed\u73af\u4e2d\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u52a8\u6001\u6743\u8861\u95ee\u9898\uff0c\u540c\u65f6\u9002\u5e94\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff08\u5982\u5fae\u578b\u65e0\u4eba\u673a\u548c\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff09\u3002", "method": "\u91c7\u7528\u4e09\u5143\u91cf\u5316Transformer\uff0c\u7ed3\u5408\u5b66\u4e60\u6fc0\u6d3b\u7a00\u758f\u95e8\u52a8\u6001\u63a7\u5236\u8ba1\u7b97\u91cf\uff1b\u63d0\u51fa\u4e24\u9636\u6bb5\u84b8\u998f\u6d41\u7a0b\u4ee5\u4fdd\u6301\u63cf\u8ff0\u7b26\u8d28\u91cf\u3002", "result": "\u8fd0\u884c\u65f6\u8ba1\u7b97\u91cf\u51cf\u5c1140%\uff0c\u6027\u80fd\uff08Recall@1\uff09\u4e0d\u4e0b\u964d\uff1b\u5728\u5fae\u578b\u65e0\u4eba\u673a\u548c\u5d4c\u5165\u5f0fSLAM\u4e2d\u5b9e\u73b0SOTA\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "TAT-VPR\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2505.16483", "pdf": "https://arxiv.org/pdf/2505.16483", "abs": "https://arxiv.org/abs/2505.16483", "authors": ["Shuzheng Si", "Haozhe Zhao", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Bofei Gao", "Kangyang Luo", "Wenhao Li", "Yufei Huang", "Gang Chen", "Fanchao Qi", "Minjia Zhang", "Baobao Chang", "Maosong Sun"], "title": "Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Teaching large language models (LLMs) to be faithful in the provided context\nis crucial for building reliable information-seeking systems. Therefore, we\npropose a systematic framework, CANOE, to improve the faithfulness of LLMs in\nboth short-form and long-form generation tasks without human annotations.\nSpecifically, we first synthesize short-form question-answering (QA) data with\nfour diverse tasks to construct high-quality and easily verifiable training\ndata without human annotation. Also, we propose Dual-GRPO, a rule-based\nreinforcement learning method that includes three tailored rule-based rewards\nderived from synthesized short-form QA data, while simultaneously optimizing\nboth short-form and long-form response generation. Notably, Dual-GRPO\neliminates the need to manually label preference data to train reward models\nand avoids over-optimizing short-form generation when relying only on the\nsynthesized short-form QA data. Experimental results show that CANOE greatly\nimproves the faithfulness of LLMs across 11 different downstream tasks, even\noutperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCANOE\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u77ed\u5f62\u5f0fQA\u6570\u636e\u548cDual-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u63d0\u5347LLM\u5728\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0f\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0a\u4e0b\u6587\u4e2d\u7684\u5fe0\u5b9e\u6027\uff0c\u4ee5\u6784\u5efa\u53ef\u9760\u7684\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u3002", "method": "1. \u5408\u6210\u77ed\u5f62\u5f0fQA\u6570\u636e\uff1b2. \u63d0\u51faDual-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e09\u79cd\u89c4\u5219\u5956\u52b1\uff0c\u540c\u65f6\u4f18\u5316\u77ed\u5f62\u5f0f\u548c\u957f\u5f62\u5f0f\u751f\u6210\u3002", "result": "CANOE\u663e\u8457\u63d0\u5347\u4e86LLM\u572811\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5fe0\u5b9e\u6027\uff0c\u751a\u81f3\u4f18\u4e8eGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "CANOE\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u5fe0\u5b9e\u6027\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.16452", "pdf": "https://arxiv.org/pdf/2505.16452", "abs": "https://arxiv.org/abs/2505.16452", "authors": ["Mohamed S. Elmahdy", "Marius Staring", "Patrick J. H. de Koning", "Samer Alabed", "Mahan Salehi", "Faisal Alandejani", "Michael Sharkey", "Ziad Aldabbagh", "Andrew J. Swift", "Rob J. van der Geest"], "title": "CMRINet: Joint Groupwise Registration and Segmentation for Cardiac Function Quantification from Cine-MRI", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 7 figures, 1 appendix", "summary": "Accurate and efficient quantification of cardiac function is essential for\nthe estimation of prognosis of cardiovascular diseases (CVDs). One of the most\ncommonly used metrics for evaluating cardiac pumping performance is left\nventricular ejection fraction (LVEF). However, LVEF can be affected by factors\nsuch as inter-observer variability and varying pre-load and after-load\nconditions, which can reduce its reproducibility. Additionally, cardiac\ndysfunction may not always manifest as alterations in LVEF, such as in heart\nfailure and cardiotoxicity diseases. An alternative measure that can provide a\nrelatively load-independent quantitative assessment of myocardial contractility\nis myocardial strain and strain rate. By using LVEF in combination with\nmyocardial strain, it is possible to obtain a thorough description of cardiac\nfunction. Automated estimation of LVEF and other volumetric measures from\ncine-MRI sequences can be achieved through segmentation models, while strain\ncalculation requires the estimation of tissue displacement between sequential\nframes, which can be accomplished using registration models. These tasks are\noften performed separately, potentially limiting the assessment of cardiac\nfunction. To address this issue, in this study we propose an end-to-end deep\nlearning (DL) model that jointly estimates groupwise (GW) registration and\nsegmentation for cardiac cine-MRI images. The proposed anatomically-guided Deep\nGW network was trained and validated on a large dataset of 4-chamber view\ncine-MRI image series of 374 subjects. A quantitative comparison with\nconventional GW registration using elastix and two DL-based methods showed that\nthe proposed model improved performance and substantially reduced computation\ntime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8054\u5408\u4f30\u8ba1\u5fc3\u810fcine-MRI\u56fe\u50cf\u7684\u7ec4\u95f4\u914d\u51c6\u548c\u5206\u5272\uff0c\u4ee5\u63d0\u9ad8\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5de6\u5fc3\u5ba4\u5c04\u8840\u5206\u6570\uff08LVEF\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5fc3\u810f\u529f\u80fd\u7684\u91cd\u8981\u6307\u6807\uff0c\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\u4e14\u5728\u67d0\u4e9b\u75be\u75c5\u4e2d\u8868\u73b0\u4e0d\u654f\u611f\uff0c\u9700\u7ed3\u5408\u5fc3\u808c\u5e94\u53d8\u8fdb\u884c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002\u4f20\u7edf\u65b9\u6cd5\u4e2d\u914d\u51c6\u548c\u5206\u5272\u4efb\u52a1\u5206\u79bb\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u5256\u5b66\u5f15\u5bfc\u7684\u6df1\u5ea6\u7ec4\u95f4\u914d\u51c6\u7f51\u7edc\uff08Deep GW\uff09\uff0c\u5728374\u540d\u53d7\u8bd5\u8005\u7684\u56db\u8154\u89c6\u56fecine-MRI\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002", "result": "\u4e0e\u4f20\u7edf\u914d\u51c6\u65b9\u6cd5\u548c\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e14\u8ba1\u7b97\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16491", "pdf": "https://arxiv.org/pdf/2505.16491", "abs": "https://arxiv.org/abs/2505.16491", "authors": ["Dario Di Palma", "Alessandro De Bellis", "Giovanni Servedio", "Vito Walter Anelli", "Fedelucio Narducci", "Tommaso Di Noia"], "title": "LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have rapidly become central to NLP,\ndemonstrating their ability to adapt to various tasks through prompting\ntechniques, including sentiment analysis. However, we still have a limited\nunderstanding of how these models capture sentiment-related information. This\nstudy probes the hidden layers of Llama models to pinpoint where sentiment\nfeatures are most represented and to assess how this affects sentiment\nanalysis.\n  Using probe classifiers, we analyze sentiment encoding across layers and\nscales, identifying the layers and pooling methods that best capture sentiment\nsignals. Our results show that sentiment information is most concentrated in\nmid-layers for binary polarity tasks, with detection accuracy increasing up to\n14% over prompting techniques. Additionally, we find that in decoder-only\nmodels, the last token is not consistently the most informative for sentiment\nencoding. Finally, this approach enables sentiment tasks to be performed with\nmemory requirements reduced by an average of 57%.\n  These insights contribute to a broader understanding of sentiment in LLMs,\nsuggesting layer-specific probing as an effective approach for sentiment tasks\nbeyond prompting, with potential to enhance model utility and reduce memory\nrequirements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u63a2\u6d4bLlama\u6a21\u578b\u7684\u9690\u85cf\u5c42\uff0c\u5b9a\u4f4d\u60c5\u611f\u7279\u5f81\u7684\u5206\u5e03\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u60c5\u611f\u5206\u6790\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u60c5\u611f\u4fe1\u606f\u5728\u4e2d\u95f4\u5c42\u6700\u96c6\u4e2d\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u6bd4\u63d0\u793a\u6280\u672f\u63d0\u9ad814%\uff0c\u540c\u65f6\u5185\u5b58\u9700\u6c42\u5e73\u5747\u51cf\u5c1157%\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60c5\u611f\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u5982\u4f55\u6355\u6349\u60c5\u611f\u4fe1\u606f\u7684\u7406\u89e3\u4ecd\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u63a2\u6d4b\u5206\u7c7b\u5668\u5206\u6790Llama\u6a21\u578b\u5404\u5c42\u7684\u60c5\u611f\u7f16\u7801\uff0c\u8bc6\u522b\u6700\u80fd\u6355\u6349\u60c5\u611f\u4fe1\u53f7\u7684\u5c42\u548c\u6c60\u5316\u65b9\u6cd5\u3002", "result": "\u60c5\u611f\u4fe1\u606f\u5728\u4e2d\u95f4\u5c42\u6700\u96c6\u4e2d\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad814%\uff1b\u89e3\u7801\u5668\u6a21\u578b\u4e2d\uff0c\u6700\u540e\u4e00\u4e2a\u6807\u8bb0\u5e76\u975e\u603b\u662f\u6700\u5177\u4fe1\u606f\u91cf\uff1b\u5185\u5b58\u9700\u6c42\u5e73\u5747\u51cf\u5c1157%\u3002", "conclusion": "\u5c42\u7279\u5f02\u6027\u63a2\u6d4b\u662f\u60c5\u611f\u4efb\u52a1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u6a21\u578b\u6548\u7528\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2505.16456", "pdf": "https://arxiv.org/pdf/2505.16456", "abs": "https://arxiv.org/abs/2505.16456", "authors": ["Siwei Meng", "Yawei Luo", "Ping Liu"], "title": "MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in static 3D generation have intensified the demand for\nphysically consistent dynamic 3D content. However, existing video generation\nmodels, including diffusion-based methods, often prioritize visual realism\nwhile neglecting physical plausibility, resulting in implausible object\ndynamics. Prior approaches for physics-aware dynamic generation typically rely\non large-scale annotated datasets or extensive model fine-tuning, which imposes\nsignificant computational and data collection burdens and limits scalability\nacross scenarios. To address these challenges, we present MAGIC, a\ntraining-free framework for single-image physical property inference and\ndynamic generation, integrating pretrained image-to-video diffusion models with\niterative LLM-based reasoning. Our framework generates motion-rich videos from\na static image and closes the visual-to-physical gap through a\nconfidence-driven LLM feedback loop that adaptively steers the diffusion model\ntoward physics-relevant motion. To translate visual dynamics into controllable\nphysical behavior, we further introduce a differentiable MPM simulator\noperating directly on 3D Gaussians reconstructed from the single image,\nenabling physically grounded, simulation-ready outputs without any supervision\nor model tuning. Experiments show that MAGIC outperforms existing physics-aware\ngenerative methods in inference accuracy and achieves greater temporal\ncoherence than state-of-the-art video diffusion models.", "AI": {"tldr": "MAGIC\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u60013D\u5185\u5bb9\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u548cLLM\u63a8\u7406\uff0c\u901a\u8fc7\u7269\u7406\u53cd\u9988\u5faa\u73af\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u52a8\u6001\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u6ce8\u91cd\u89c6\u89c9\u771f\u5b9e\u611f\u4f46\u5ffd\u7565\u7269\u7406\u5408\u7406\u6027\uff0c\u4e14\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u5fae\u8c03\uff0c\u8ba1\u7b97\u548c\u6570\u636e\u8d1f\u62c5\u91cd\u3002", "method": "MAGIC\u6574\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548cLLM\u63a8\u7406\uff0c\u901a\u8fc7\u7f6e\u4fe1\u9a71\u52a8\u7684\u53cd\u9988\u5faa\u73af\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u52a8\u6001\u89c6\u9891\uff0c\u5e76\u4f7f\u7528\u53ef\u5fae\u5206MPM\u6a21\u62df\u5668\u5b9e\u73b0\u7269\u7406\u884c\u4e3a\u63a7\u5236\u3002", "result": "MAGIC\u5728\u63a8\u7406\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u7269\u7406\u611f\u77e5\u751f\u6210\u65b9\u6cd5\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u3002", "conclusion": "MAGIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60013D\u5185\u5bb9\u751f\u6210\u7684\u7269\u7406\u5408\u7406\u6027\u95ee\u9898\u3002"}}
{"id": "2505.16505", "pdf": "https://arxiv.org/pdf/2505.16505", "abs": "https://arxiv.org/abs/2505.16505", "authors": ["Runcong Zhao", "Chengyu Cao", "Qinglin Zhu", "Xiucheng Lv", "Shun Shao", "Lin Gui", "Ruifeng Xu", "Yulan He"], "title": "Sparse Activation Editing for Reliable Instruction Following in Narratives", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Complex narrative contexts often challenge language models' ability to follow\ninstructions, and existing benchmarks fail to capture these difficulties. To\naddress this, we propose Concise-SAE, a training-free framework that improves\ninstruction following by identifying and editing instruction-relevant neurons\nusing only natural language instructions, without requiring labelled data. To\nthoroughly evaluate our method, we introduce FreeInstruct, a diverse and\nrealistic benchmark of 1,212 examples that highlights the challenges of\ninstruction following in narrative-rich settings. While initially motivated by\ncomplex narratives, Concise-SAE demonstrates state-of-the-art instruction\nadherence across varied tasks without compromising generation quality.", "AI": {"tldr": "Concise-SAE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u4ec5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8bc6\u522b\u548c\u7f16\u8f91\u76f8\u5173\u795e\u7ecf\u5143\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53d9\u4e8b\u73af\u5883\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u53d9\u4e8b\u73af\u5883\u4e2d\u6307\u4ee4\u9075\u5faa\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faConcise-SAE\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8bc6\u522b\u548c\u7f16\u8f91\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u591a\u6837\u5316\u7684FreeInstruct\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "Concise-SAE\u5728\u590d\u6742\u53d9\u4e8b\u548c\u5176\u4ed6\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2505.16463", "pdf": "https://arxiv.org/pdf/2505.16463", "abs": "https://arxiv.org/abs/2505.16463", "authors": ["Jiquan Shan", "Junxiao Wang", "Lifeng Zhao", "Liang Cai", "Hongyuan Zhang", "Ioannis Liritzis"], "title": "AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recently, vision transformers (ViTs) have achieved excellent performance on\nvision tasks by measuring the global self-attention among the image patches.\nGiven $n$ patches, they will have quadratic complexity such as\n$\\mathcal{O}(n^2)$ and the time cost is high when splitting the input image\nwith a small granularity. Meanwhile, the pivotal information is often randomly\ngathered in a few regions of an input image, some tokens may not be helpful for\nthe downstream tasks. To handle this problem, we introduce an anchor-based\nefficient vision transformer (AnchorFormer), which employs the anchor tokens to\nlearn the pivotal information and accelerate the inference. Firstly, by\nestimating the bipartite attention between the anchors and tokens, the\ncomplexity will be reduced from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$, where\n$m$ is an anchor number and $m < n$. Notably, by representing the anchors with\nthe neurons in a neural layer, we can differentiable learn these distributions\nand approximate global self-attention through the Markov process. Moreover, we\nextend the proposed model to three downstream tasks including classification,\ndetection, and segmentation. Extensive experiments show the effectiveness of\nour AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs\nreduction on ImageNet classification, 81.3% higher mAP on COCO detection under\ncomparable FLOPs, as compared to the current baselines.", "AI": {"tldr": "AnchorFormer\u901a\u8fc7\u5f15\u5165\u951a\u70b9\u4ee4\u724c\u964d\u4f4eViTs\u7684\u590d\u6742\u5ea6\uff0c\u4eceO(n\u00b2)\u964d\u81f3O(mn)\uff0c\u5e76\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3ViTs\u56e0\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u5bfc\u81f4\u7684O(n\u00b2)\u9ad8\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u805a\u7126\u56fe\u50cf\u5173\u952e\u533a\u57df\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528\u951a\u70b9\u4ee4\u724c\u5b66\u4e60\u5173\u952e\u4fe1\u606f\uff0c\u901a\u8fc7\u4e8c\u5206\u6ce8\u610f\u529b\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u5143\u8868\u793a\u951a\u70b9\u4ee5\u53ef\u5fae\u5206\u5b66\u4e60\u5206\u5e03\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4e2d\u63d0\u53479.0%\u51c6\u786e\u7387\u6216\u51cf\u5c1146.7% FLOPs\uff0cCOCO\u68c0\u6d4b\u4e2dmAP\u63d0\u534781.3%\u3002", "conclusion": "AnchorFormer\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u3002"}}
{"id": "2505.16514", "pdf": "https://arxiv.org/pdf/2505.16514", "abs": "https://arxiv.org/abs/2505.16514", "authors": ["Yuting Huang", "Meitong Guo", "Yiquan Wu", "Ang Li", "Xiaozhong Liu", "Keting Yin", "Changlong Sun", "Fei Wu", "Kun Kuang"], "title": "AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios", "categories": ["cs.CL"], "comment": "15 pages, 4 figures", "summary": "Recent advances in LegalAI have primarily focused on individual case judgment\nanalysis, often overlooking the critical appellate process within the judicial\nsystem. Appeals serve as a core mechanism for error correction and ensuring\nfair trials, making them highly significant both in practice and in research.\nTo address this gap, we present the AppealCase dataset, consisting of 10,000\npairs of real-world, matched first-instance and second-instance documents\nacross 91 categories of civil cases. The dataset also includes detailed\nannotations along five dimensions central to appellate review: judgment\nreversals, reversal reasons, cited legal provisions, claim-level decisions, and\nwhether there is new information in the second instance. Based on these\nannotations, we propose five novel LegalAI tasks and conduct a comprehensive\nevaluation across 20 mainstream models. Experimental results reveal that all\ncurrent models achieve less than 50% F1 scores on the judgment reversal\nprediction task, highlighting the complexity and challenge of the appeal\nscenario. We hope that the AppealCase dataset will spur further research in\nLegalAI for appellate case analysis and contribute to improving consistency in\njudicial decision-making.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86AppealCase\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86LegalAI\u4e2d\u4e0a\u8bc9\u6848\u4ef6\u5206\u6790\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u65b0\u4efb\u52a1\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u4e0a\u8bc9\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709LegalAI\u7814\u7a76\u591a\u5173\u6ce8\u4e00\u5ba1\u6848\u4ef6\uff0c\u5ffd\u89c6\u4e86\u4e0a\u8bc9\u8fc7\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0a\u8bc9\u662f\u53f8\u6cd5\u7cfb\u7edf\u4e2d\u7ea0\u9519\u548c\u786e\u4fdd\u516c\u5e73\u7684\u6838\u5fc3\u673a\u5236\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b10,000\u5bf9\u5339\u914d\u7684\u4e00\u5ba1\u548c\u4e8c\u5ba1\u6587\u6863\u7684AppealCase\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e94\u4e2a\u5173\u952e\u7ef4\u5ea6\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e94\u4e2a\u65b0\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8620\u4e2a\u4e3b\u6d41\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u5224\u51b3\u9006\u8f6c\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684F1\u5206\u6570\u5747\u4f4e\u4e8e50%\uff0c\u8868\u660e\u4e0a\u8bc9\u573a\u666f\u7684\u590d\u6742\u6027\u548c\u6311\u6218\u6027\u3002", "conclusion": "AppealCase\u6570\u636e\u96c6\u6709\u671b\u63a8\u52a8LegalAI\u5728\u4e0a\u8bc9\u6848\u4ef6\u5206\u6790\u4e2d\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u5347\u53f8\u6cd5\u51b3\u7b56\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.16474", "pdf": "https://arxiv.org/pdf/2505.16474", "abs": "https://arxiv.org/abs/2505.16474", "authors": ["Yu Zhang", "Xingzhuo Guo", "Haoran Xu", "Mingsheng Long"], "title": "Consistent World Models via Foresight Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion and flow-based models have enabled significant progress in\ngeneration tasks across various modalities and have recently found applications\nin world modeling. However, unlike typical generation tasks that encourage\nsample diversity, world models entail different sources of uncertainty and\nrequire consistent samples aligned with the ground-truth trajectory, which is a\nlimitation we empirically observe in diffusion models. We argue that a key\nbottleneck in learning consistent diffusion-based world models lies in the\nsuboptimal predictive ability, which we attribute to the entanglement of\ncondition understanding and target denoising within shared architectures and\nco-training schemes. To address this, we propose Foresight Diffusion\n(ForeDiff), a diffusion-based world modeling framework that enhances\nconsistency by decoupling condition understanding from target denoising.\nForeDiff incorporates a separate deterministic predictive stream to process\nconditioning inputs independently of the denoising stream, and further\nleverages a pretrained predictor to extract informative representations that\nguide generation. Extensive experiments on robot video prediction and\nscientific spatiotemporal forecasting show that ForeDiff improves both\npredictive accuracy and sample consistency over strong baselines, offering a\npromising direction for diffusion-based world models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faForeDiff\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6761\u4ef6\u7406\u89e3\u548c\u76ee\u6807\u53bb\u566a\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u4e00\u81f4\u6027\u4e16\u754c\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e16\u754c\u5efa\u6a21\u4e2d\u56e0\u6837\u672c\u4e00\u81f4\u6027\u95ee\u9898\u53d7\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6761\u4ef6\u7406\u89e3\u548c\u76ee\u6807\u53bb\u566a\u7684\u8026\u5408\u3002", "method": "\u63d0\u51faForeDiff\u6846\u67b6\uff0c\u5206\u79bb\u6761\u4ef6\u7406\u89e3\u548c\u76ee\u6807\u53bb\u566a\uff0c\u5f15\u5165\u786e\u5b9a\u6027\u9884\u6d4b\u6d41\u548c\u9884\u8bad\u7ec3\u9884\u6d4b\u5668\u3002", "result": "\u5728\u673a\u5668\u4eba\u89c6\u9891\u9884\u6d4b\u548c\u79d1\u5b66\u65f6\u7a7a\u9884\u6d4b\u5b9e\u9a8c\u4e2d\uff0cForeDiff\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6837\u672c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "ForeDiff\u4e3a\u6269\u6563\u6a21\u578b\u5728\u4e16\u754c\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16518", "pdf": "https://arxiv.org/pdf/2505.16518", "abs": "https://arxiv.org/abs/2505.16518", "authors": ["Lovisa Hagstr\u00f6m", "Youna Kim", "Haeun Yu", "Sang-goo Lee", "Richard Johansson", "Hyunsoo Cho", "Isabelle Augenstein"], "title": "CUB: Benchmarking Context Utilisation Techniques for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "27 pages", "summary": "Incorporating external knowledge is crucial for knowledge-intensive tasks,\nsuch as question answering and fact checking. However, language models (LMs)\nmay ignore relevant information that contradicts outdated parametric memory or\nbe distracted by irrelevant contexts. While many context utilisation\nmanipulation techniques (CMTs) that encourage or suppress context utilisation\nhave recently been proposed to alleviate these issues, few have seen systematic\ncomparison. In this paper, we develop CUB (Context Utilisation Benchmark) to\nhelp practitioners within retrieval-augmented generation (RAG) identify the\nbest CMT for their needs. CUB allows for rigorous testing on three distinct\ncontext types, observed to capture key challenges in realistic context\nutilisation scenarios. With this benchmark, we evaluate seven state-of-the-art\nmethods, representative of the main categories of CMTs, across three diverse\ndatasets and tasks, applied to nine LMs. Our results show that most of the\nexisting CMTs struggle to handle the full set of types of contexts that may be\nencountered in real-world retrieval-augmented scenarios. Moreover, we find that\nmany CMTs display an inflated performance on simple synthesised datasets,\ncompared to more realistic datasets with naturally occurring samples.\nAltogether, our results show the need for holistic tests of CMTs and the\ndevelopment of CMTs that can handle multiple context types.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CUB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u6bd4\u8f83\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u64cd\u4f5c\u6280\u672f\uff08CMTs\uff09\uff0c\u53d1\u73b0\u73b0\u6709CMTs\u96be\u4ee5\u5e94\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u591a\u6837\u5316\u4e0a\u4e0b\u6587\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u53ef\u80fd\u5ffd\u7565\u6216\u8bef\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u800c\u73b0\u6709CMTs\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u3002", "method": "\u5f00\u53d1CUB\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e03\u79cd\u4ee3\u8868\u6027CMTs\u5728\u4e09\u79cd\u4e0a\u4e0b\u6587\u7c7b\u578b\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u591a\u6570CMTs\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u591a\u6837\u5316\u4e0a\u4e0b\u6587\uff0c\u4e14\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u865a\u9ad8\u3002", "conclusion": "\u9700\u5f00\u53d1\u80fd\u5904\u7406\u591a\u79cd\u4e0a\u4e0b\u6587\u7c7b\u578b\u7684CMTs\uff0c\u5e76\u91c7\u7528\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2505.16479", "pdf": "https://arxiv.org/pdf/2505.16479", "abs": "https://arxiv.org/abs/2505.16479", "authors": ["Yuetong Liu", "Yunqiu Xu", "Yang Wei", "Xiuli Bi", "Bin Xiao"], "title": "Clear Nights Ahead: Towards Multi-Weather Nighttime Image Restoration", "categories": ["cs.CV"], "comment": "17 pages, 20 figures", "summary": "Restoring nighttime images affected by multiple adverse weather conditions is\na practical yet under-explored research problem, as multiple weather conditions\noften coexist in the real world alongside various lighting effects at night.\nThis paper first explores the challenging multi-weather nighttime image\nrestoration task, where various types of weather degradations are intertwined\nwith flare effects. To support the research, we contribute the AllWeatherNight\ndataset, featuring large-scale high-quality nighttime images with diverse\ncompositional degradations, synthesized using our introduced illumination-aware\ndegradation generation. Moreover, we present ClearNight, a unified nighttime\nimage restoration framework, which effectively removes complex degradations in\none go. Specifically, ClearNight extracts Retinex-based dual priors and\nexplicitly guides the network to focus on uneven illumination regions and\nintrinsic texture contents respectively, thereby enhancing restoration\neffectiveness in nighttime scenarios. In order to better represent the common\nand unique characters of multiple weather degradations, we introduce a\nweather-aware dynamic specific-commonality collaboration method, which\nidentifies weather degradations and adaptively selects optimal candidate units\nassociated with specific weather types. Our ClearNight achieves\nstate-of-the-art performance on both synthetic and real-world images.\nComprehensive ablation experiments validate the necessity of AllWeatherNight\ndataset as well as the effectiveness of ClearNight. Project page:\nhttps://henlyta.github.io/ClearNight/mainpage.html", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClearNight\u7684\u7edf\u4e00\u591c\u95f4\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u591c\u95f4\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u5e76\u8d21\u732e\u4e86AllWeatherNight\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u591c\u95f4\u56fe\u50cf\u5e38\u53d7\u591a\u79cd\u5929\u6c14\u548c\u5149\u7167\u6548\u5e94\u5171\u540c\u5f71\u54cd\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u901a\u8fc7Retinex\u53cc\u5148\u9a8c\u63d0\u53d6\u548c\u5929\u6c14\u611f\u77e5\u52a8\u6001\u534f\u4f5c\u65b9\u6cd5\uff0cClearNight\u6709\u6548\u53bb\u9664\u590d\u6742\u9000\u5316\u3002", "result": "ClearNight\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "AllWeatherNight\u6570\u636e\u96c6\u548cClearNight\u6846\u67b6\u7684\u6709\u6548\u6027\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2505.16520", "pdf": "https://arxiv.org/pdf/2505.16520", "abs": "https://arxiv.org/abs/2505.16520", "authors": ["Giovanni Servedio", "Alessandro De Bellis", "Dario Di Palma", "Vito Walter Anelli", "Tommaso Di Noia"], "title": "Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Factual hallucinations are a major challenge for Large Language Models\n(LLMs). They undermine reliability and user trust by generating inaccurate or\nfabricated content. Recent studies suggest that when generating false\nstatements, the internal states of LLMs encode information about truthfulness.\nHowever, these studies often rely on synthetic datasets that lack realism,\nwhich limits generalization when evaluating the factual accuracy of text\ngenerated by the model itself. In this paper, we challenge the findings of\nprevious work by investigating truthfulness encoding capabilities, leading to\nthe generation of a more realistic and challenging dataset. Specifically, we\nextend previous work by introducing: (1) a strategy for sampling plausible\ntrue-false factoid sentences from tabular data and (2) a procedure for\ngenerating realistic, LLM-dependent true-false datasets from Question Answering\ncollections. Our analysis of two open-source LLMs reveals that while the\nfindings from previous studies are partially validated, generalization to\nLLM-generated datasets remains challenging. This study lays the groundwork for\nfuture research on factuality in LLMs and offers practical guidelines for more\neffective evaluation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLMs\u4e2d\u7684\u4e8b\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u771f\u5b9e\u7684\u6570\u636e\u96c6\u6311\u6218\u5148\u524d\u7814\u7a76\uff0c\u53d1\u73b0\u6cdb\u5316\u81f3LLM\u751f\u6210\u6570\u636e\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u751f\u6210\u865a\u5047\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u548c\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u4ece\u8868\u683c\u6570\u636e\u4e2d\u91c7\u6837\u771f\u4f2a\u4e8b\u5b9e\u53e5\u5b50\uff0c\u5e76\u4ece\u95ee\u7b54\u96c6\u5408\u751f\u6210\u4f9d\u8d56LLM\u7684\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u90e8\u5206\u9a8c\u8bc1\u5148\u524d\u7814\u7a76\uff0c\u4f46LLM\u751f\u6210\u6570\u636e\u96c6\u7684\u6cdb\u5316\u4ecd\u56f0\u96be\u3002", "conclusion": "\u4e3aLLMs\u4e8b\u5b9e\u6027\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u63d0\u4f9b\u66f4\u6709\u6548\u8bc4\u4f30\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2505.16485", "pdf": "https://arxiv.org/pdf/2505.16485", "abs": "https://arxiv.org/abs/2505.16485", "authors": ["Yao Wei", "Muhammad Usman", "Hazrat Bilal"], "title": "InspectionV3: Enhancing Tobacco Quality Assessment with Deep Convolutional Neural Networks for Automated Workshop Management", "categories": ["cs.CV"], "comment": "33 pages, 15 figures, 2 Tables", "summary": "The problems that tobacco workshops encounter include poor curing,\ninconsistencies in supplies, irregular scheduling, and a lack of oversight, all\nof which drive up expenses and worse quality. Large quantities make manual\nexamination costly, sluggish, and unreliable. Deep convolutional neural\nnetworks have recently made strides in capabilities that transcend those of\nconventional methods. To effectively enhance them, nevertheless, extensive\ncustomization is needed to account for subtle variations in tobacco grade. This\nstudy introduces InspectionV3, an integrated solution for automated flue-cured\ntobacco grading that makes use of a customized deep convolutional neural\nnetwork architecture. A scope that covers color, maturity, and curing\nsubtleties is established via a labelled dataset consisting of 21,113 images\nspanning 20 quality classes. Expert annotators performed preprocessing on the\ntobacco leaf images, including cleaning, labelling, and augmentation.\nMulti-layer CNN factors use batch normalization to describe domain properties\nlike as permeability and moisture spots, and so account for the subtleties of\nthe workshop. Its expertise lies in converting visual patterns into useful\ninformation for enhancing workflow. Fast notifications are made possible by\nreal-time, on-the-spot grading that matches human expertise. Images-powered\nanalytics dashboards facilitate the tracking of yield projections, inventories,\nbottlenecks, and the optimization of data-driven choices. More labelled images\nare assimilated after further retraining, improving representational capacities\nand enabling adaptations for seasonal variability. Metrics demonstrate 97%\naccuracy, 95% precision and recall, 96% F1-score and AUC, 95% specificity;\nvalidating real-world viability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInspectionV3\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b9a\u5236\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u81ea\u52a8\u5316\u70df\u8349\u5206\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5206\u7ea7\u3002", "motivation": "\u70df\u8349\u52a0\u5de5\u4e2d\u5b58\u5728\u7684\u4f4e\u6548\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u7f3a\u4e4f\u76d1\u7763\u95ee\u9898\u5bfc\u81f4\u6210\u672c\u4e0a\u5347\u548c\u8d28\u91cf\u4e0b\u964d\uff0c\u4f20\u7edf\u4eba\u5de5\u68c0\u67e5\u65b9\u6cd5\u4e0d\u53ef\u9760\u4e14\u6602\u8d35\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u5316\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u752821,113\u5f20\u6807\u6ce8\u56fe\u50cf\uff08\u8986\u76d620\u4e2a\u8d28\u91cf\u7b49\u7ea7\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u9884\u5904\u7406\u548c\u6279\u91cf\u5f52\u4e00\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5206\u7ea7\u3002", "result": "\u7cfb\u7edf\u8fbe\u523097%\u51c6\u786e\u7387\uff0c95%\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c96% F1\u5206\u6570\u548cAUC\uff0c95%\u7279\u5f02\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u53ef\u884c\u6027\u3002", "conclusion": "InspectionV3\u901a\u8fc7\u81ea\u52a8\u5316\u5206\u7ea7\u548c\u6570\u636e\u5206\u6790\u4f18\u5316\u4e86\u70df\u8349\u52a0\u5de5\u6d41\u7a0b\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16522", "pdf": "https://arxiv.org/pdf/2505.16522", "abs": "https://arxiv.org/abs/2505.16522", "authors": ["Zhouhao Sun", "Zhiyuan Kan", "Xiao Ding", "Li Du", "Yang Zhao", "Bing Qin", "Ting Liu"], "title": "Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress, recent studies have indicated that current\nlarge language models (LLMs) may still utilize bias during inference, leading\nto the poor generalizability of LLMs. Some benchmarks are proposed to\ninvestigate the generalizability of LLMs, with each piece of data typically\ncontaining one type of controlled bias. However, a single piece of data may\ncontain multiple types of biases in practical applications. To bridge this gap,\nwe propose a multi-bias benchmark where each piece of data contains five types\nof biases. The evaluations conducted on this benchmark reveal that the\nperformance of existing LLMs and debiasing methods is unsatisfying,\nhighlighting the challenge of eliminating multiple types of biases\nsimultaneously. To overcome this challenge, we propose a causal effect\nestimation-guided multi-bias elimination method (CMBE). This method first\nestimates the causal effect of multiple types of biases simultaneously.\nSubsequently, we eliminate the causal effect of biases from the total causal\neffect exerted by both the semantic information and biases during inference.\nExperimental results show that CMBE can effectively eliminate multiple types of\nbias simultaneously to enhance the generalizability of LLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u504f\u89c1\u57fa\u51c6\uff08multi-bias benchmark\uff09\uff0c\u6bcf\u4e2a\u6570\u636e\u5305\u542b\u4e94\u79cd\u504f\u89c1\u7c7b\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u5f15\u5bfc\u7684\u591a\u504f\u89c1\u6d88\u9664\u65b9\u6cd5\uff08CMBE\uff09\uff0c\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u4ecd\u53ef\u80fd\u5229\u7528\u504f\u89c1\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u53ea\u5305\u542b\u5355\u4e00\u504f\u89c1\u7c7b\u578b\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u6570\u636e\u53ef\u80fd\u5305\u542b\u591a\u79cd\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u591a\u504f\u89c1\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1CMBE\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u65f6\u4f30\u8ba1\u591a\u79cd\u504f\u89c1\u7684\u56e0\u679c\u6548\u5e94\uff0c\u4ece\u603b\u56e0\u679c\u6548\u5e94\u4e2d\u6d88\u9664\u504f\u89c1\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709LLMs\u548c\u53bb\u504f\u89c1\u65b9\u6cd5\u5728\u591a\u504f\u89c1\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cCMBE\u80fd\u6709\u6548\u540c\u65f6\u6d88\u9664\u591a\u79cd\u504f\u89c1\u3002", "conclusion": "CMBE\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLMs\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u540c\u65f6\u6d88\u9664\u591a\u79cd\u504f\u89c1\u7684\u6311\u6218\u3002"}}
{"id": "2505.16495", "pdf": "https://arxiv.org/pdf/2505.16495", "abs": "https://arxiv.org/abs/2505.16495", "authors": ["Lingfeng Wang", "Hualing Lin", "Senda Chen", "Tao Wang", "Changxu Cheng", "Yangyang Zhong", "Dong Zheng", "Wuyue Zhao"], "title": "ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation", "categories": ["cs.CV"], "comment": null, "summary": "While humans effortlessly draw visual objects and shapes by adaptively\nallocating attention based on their complexity, existing multimodal large\nlanguage models (MLLMs) remain constrained by rigid token representations.\nBridging this gap, we propose ALTo, an adaptive length tokenizer for\nautoregressive mask generation. To achieve this, a novel token length predictor\nis designed, along with a length regularization term and a differentiable token\nchunking strategy. We further build ALToLLM that seamlessly integrates ALTo\ninto MLLM. Preferences on the trade-offs between mask quality and efficiency is\nimplemented by group relative policy optimization (GRPO). Experiments\ndemonstrate that ALToLLM achieves state-of-the-art performance with adaptive\ntoken cost on popular segmentation benchmarks. Code and models are released at\nhttps://github.com/yayafengzi/ALToLLM.", "AI": {"tldr": "ALTo\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u957f\u5ea6\u6807\u8bb0\u5668\uff0c\u7528\u4e8e\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u63a9\u7801\u751f\u6210\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u6ce8\u610f\u529b\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u80fd\u6839\u636e\u5bf9\u8c61\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5206\u914d\u6ce8\u610f\u529b\uff0c\u800c\u73b0\u6709MLLM\u53d7\u9650\u4e8e\u56fa\u5b9a\u6807\u8bb0\u8868\u793a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u6807\u8bb0\u957f\u5ea6\u9884\u6d4b\u5668\u3001\u957f\u5ea6\u6b63\u5219\u5316\u9879\u548c\u53ef\u5fae\u5206\u6807\u8bb0\u5206\u5757\u7b56\u7565\uff0c\u5e76\u96c6\u6210\u5230ALToLLM\u4e2d\uff0c\u901a\u8fc7GRPO\u4f18\u5316\u63a9\u7801\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u6743\u8861\u3002", "result": "ALToLLM\u5728\u4e3b\u6d41\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5177\u5907\u81ea\u9002\u5e94\u6807\u8bb0\u6210\u672c\u3002", "conclusion": "ALToLLM\u901a\u8fc7\u81ea\u9002\u5e94\u6807\u8bb0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86MLLM\u7684\u63a9\u7801\u751f\u6210\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.16526", "pdf": "https://arxiv.org/pdf/2505.16526", "abs": "https://arxiv.org/abs/2505.16526", "authors": ["Heejae Suh", "Yejin Jeon", "Deokhyung Kang", "Taehee Park", "Yejin Min", "Gary Geunbae Lee"], "title": "EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Findings, long paper)", "summary": "Small large language models (sLLMs) offer the advantage of being lightweight\nand efficient, which makes them suitable for resource-constrained environments.\nHowever, sLLMs often struggle to maintain topic consistency in task-oriented\ndialogue systems, which is critical for scenarios such as service chatbots.\nSpecifically, it is important to ensure that the model denies off-topic or\nmalicious inputs and adheres to its intended functionality so as to prevent\npotential misuse and uphold reliability. Towards this, existing activation\nengineering approaches have been proposed to manipulate internal activations\nduring inference. While these methods are effective in certain scenarios, our\npreliminary experiments reveal their limitations in ensuring topic adherence.\nTherefore, to address this, we propose a novel approach termed Entropy-scaled\nSteering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the\nsteering intensity based on input uncertainty, which allows the model to handle\noff-topic distractors effectively while preserving on-topic accuracy. Our\nexperiments demonstrate that EnSToM achieves significant performance gain with\na relatively small data size compared to fine-tuning approaches. By improving\ntopic adherence without compromising efficiency, our approach provides a robust\nsolution for enhancing sLLM-based dialogue systems.", "AI": {"tldr": "EnSToM\u662f\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\uff0c\u4f46\u4e3b\u9898\u4e00\u81f4\u6027\u4e0d\u8db3\uff0c\u5f71\u54cd\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faEntropy-scaled Steering vectors for Topic Maintenance (EnSToM)\uff0c\u57fa\u4e8e\u8f93\u5165\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEnSToM\u5728\u5c0f\u6570\u636e\u91cf\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u5fae\u8c03\u65b9\u6cd5\u3002", "conclusion": "EnSToM\u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4e3a\u5c0f\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16512", "pdf": "https://arxiv.org/pdf/2505.16512", "abs": "https://arxiv.org/abs/2505.16512", "authors": ["Jiaxin Liu", "Jia Wang", "Saihui Hou", "Min Ren", "Huijia Wu", "Zhaofeng He"], "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In recent years, the rapid development of deepfake technology has given rise\nto an emerging and serious threat to public security: diffusion model-based\ndigital human generation. Unlike traditional face manipulation methods, such\nmodels can generate highly realistic videos with consistency through multimodal\ncontrol signals. Their flexibility and covertness pose severe challenges to\nexisting detection strategies. To bridge this gap, we introduce DigiFakeAV, the\nfirst large-scale multimodal digital human forgery dataset based on diffusion\nmodels. Employing five latest digital human generation methods (Sonic, Hallo,\netc.) and voice cloning method, we systematically produce a dataset comprising\n60,000 videos (8.4 million frames), covering multiple nationalities, skin\ntones, genders, and real-world scenarios, significantly enhancing data\ndiversity and realism. User studies show that the confusion rate between forged\nand real videos reaches 68%, and existing state-of-the-art (SOTA) detection\nmodels exhibit large drops in AUC values on DigiFakeAV, highlighting the\nchallenge of the dataset. To address this problem, we further propose\nDigiShield, a detection baseline based on spatiotemporal and cross-modal\nfusion. By jointly modeling the 3D spatiotemporal features of videos and the\nsemantic-acoustic features of audio, DigiShield achieves SOTA performance on\nboth the DigiFakeAV and DF-TIMIT datasets. Experiments show that this method\neffectively identifies covert artifacts through fine-grained analysis of the\ntemporal evolution of facial features in synthetic videos.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DigiFakeAV\u6570\u636e\u96c6\u548cDigiShield\u68c0\u6d4b\u57fa\u7ebf\uff0c\u7528\u4e8e\u5e94\u5bf9\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u9ad8\u5ea6\u903c\u771f\u6570\u5b57\u4eba\u89c6\u9891\u7684\u68c0\u6d4b\u6311\u6218\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6570\u5b57\u4eba\u89c6\u9891\u5177\u6709\u9ad8\u5ea6\u771f\u5b9e\u6027\u548c\u9690\u853d\u6027\uff0c\u5bf9\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u6784\u6210\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u5b57\u4eba\u4f2a\u9020\u6570\u636e\u96c6DigiFakeAV\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u65f6\u7a7a\u548c\u8de8\u6a21\u6001\u878d\u5408\u7684\u68c0\u6d4b\u57fa\u7ebfDigiShield\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\u4f2a\u9020\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u7684\u6df7\u6dc6\u7387\u8fbe68%\uff0c\u73b0\u6709\u68c0\u6d4b\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1bDigiShield\u5728DigiFakeAV\u548cDF-TIMIT\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DigiShield\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u6790\u5408\u6210\u89c6\u9891\u4e2d\u9762\u90e8\u7279\u5f81\u7684\u65f6\u5e8f\u6f14\u5316\uff0c\u6709\u6548\u8bc6\u522b\u9690\u853d\u4f2a\u5f71\uff0c\u4e3a\u6570\u5b57\u4eba\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16538", "pdf": "https://arxiv.org/pdf/2505.16538", "abs": "https://arxiv.org/abs/2505.16538", "authors": ["Ercong Nie", "Helmut Schmid", "Hinrich Sch\u00fctze"], "title": "Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Language confusion -- where large language models (LLMs) generate unintended\nlanguages against the user's need -- remains a critical challenge, especially\nfor English-centric models. We present the first mechanistic interpretability\n(MI) study of language confusion, combining behavioral benchmarking with\nneuron-level analysis. Using the Language Confusion Benchmark (LCB), we show\nthat confusion points (CPs) -- specific positions where language switches occur\n-- are central to this phenomenon. Through layer-wise analysis with TunedLens\nand targeted neuron attribution, we reveal that transition failures in the\nfinal layers drive confusion. We further demonstrate that editing a small set\nof critical neurons, identified via comparative analysis with\nmultilingual-tuned models, substantially mitigates confusion without harming\ngeneral competence or fluency. Our approach matches multilingual alignment in\nconfusion reduction for most languages and yields cleaner, higher-quality\noutputs. These findings provide new insights into the internal dynamics of LLMs\nand highlight neuron-level interventions as a promising direction for robust,\ninterpretable multilingual language modeling.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u8bed\u8a00\u6df7\u6dc6\u73b0\u8c61\uff0c\u901a\u8fc7\u673a\u5236\u89e3\u91ca\u6027\uff08MI\uff09\u65b9\u6cd5\u63ed\u793a\u4e86\u6df7\u6dc6\u70b9\uff08CPs\uff09\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u7f16\u8f91\u5173\u952e\u795e\u7ecf\u5143\u6765\u663e\u8457\u51cf\u5c11\u6df7\u6dc6\u3002", "motivation": "\u8bed\u8a00\u6df7\u6dc6\u662f\u82f1\u8bed\u4e2d\u5fc3\u6a21\u578b\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5236\u89e3\u91ca\u6027\u65b9\u6cd5\u7406\u89e3\u5e76\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u548c\u795e\u7ecf\u5143\u7ea7\u5206\u6790\uff0c\u4f7f\u7528Language Confusion Benchmark\uff08LCB\uff09\u8bc6\u522b\u6df7\u6dc6\u70b9\uff0c\u5e76\u901a\u8fc7TunedLens\u548c\u795e\u7ecf\u5143\u5f52\u56e0\u5206\u6790\u63ed\u793a\u6df7\u6dc6\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6df7\u6dc6\u4e3b\u8981\u7531\u6700\u540e\u51e0\u5c42\u7684\u8f6c\u6362\u5931\u8d25\u9a71\u52a8\uff0c\u7f16\u8f91\u5173\u952e\u795e\u7ecf\u5143\u53ef\u663e\u8457\u51cf\u5c11\u6df7\u6dc6\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u5185\u90e8\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u795e\u7ecf\u5143\u7ea7\u5e72\u9884\u662f\u63d0\u5347\u591a\u8bed\u8a00\u5efa\u6a21\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2505.16513", "pdf": "https://arxiv.org/pdf/2505.16513", "abs": "https://arxiv.org/abs/2505.16513", "authors": ["Vaishali Maheshkar", "Aadarsh Anantha Ramakrishnan", "Charuvahan Adhivarahan", "Karthik Dantu"], "title": "Detailed Evaluation of Modern Machine Learning Approaches for Optic Plastics Sorting", "categories": ["cs.CV", "68T45", "I.4.9; I.4.6"], "comment": "Accepted at the 2024 REMADE Circular Economy Tech Summit and\n  Conference, https://remadeinstitute.org/2024-conference/", "summary": "According to the EPA, only 25% of waste is recycled, and just 60% of U.S.\nmunicipalities offer curbside recycling. Plastics fare worse, with a recycling\nrate of only 8%; an additional 16% is incinerated, while the remaining 76% ends\nup in landfills. The low plastic recycling rate stems from contamination, poor\neconomic incentives, and technical difficulties, making efficient recycling a\nchallenge. To improve recovery, automated sorting plays a critical role.\nCompanies like AMP Robotics and Greyparrot utilize optical systems for sorting,\nwhile Materials Recovery Facilities (MRFs) employ Near-Infrared (NIR) sensors\nto detect plastic types.\n  Modern optical sorting uses advances in computer vision such as object\nrecognition and instance segmentation, powered by machine learning. Two-stage\ndetectors like Mask R-CNN use region proposals and classification with deep\nbackbones like ResNet. Single-stage detectors like YOLO handle detection in one\npass, trading some accuracy for speed. While such methods excel under ideal\nconditions with a large volume of labeled training data, challenges arise in\nrealistic scenarios, emphasizing the need to further examine the efficacy of\noptic detection for automated sorting.\n  In this study, we compiled novel datasets totaling 20,000+ images from varied\nsources. Using both public and custom machine learning pipelines, we assessed\nthe capabilities and limitations of optical recognition for sorting. Grad-CAM,\nsaliency maps, and confusion matrices were employed to interpret model\nbehavior. We perform this analysis on our custom trained models from the\ncompiled datasets. To conclude, our findings are that optic recognition methods\nhave limited success in accurate sorting of real-world plastics at MRFs,\nprimarily because they rely on physical properties such as color and shape.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5851\u6599\u56de\u6536\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u81ea\u52a8\u5206\u62e3\u6280\u672f\u662f\u5173\u952e\uff0c\u5e76\u8bc4\u4f30\u4e86\u5149\u5b66\u8bc6\u522b\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5851\u6599\u56de\u6536\u7387\u4ec5\u4e3a8%\uff0c\u4e3b\u8981\u7531\u4e8e\u6c61\u67d3\u3001\u7ecf\u6d4e\u6fc0\u52b1\u4e0d\u8db3\u548c\u6280\u672f\u96be\u9898\u3002\u81ea\u52a8\u5206\u62e3\u6280\u672f\uff08\u5982\u5149\u5b66\u8bc6\u522b\uff09\u88ab\u5bc4\u4e88\u539a\u671b\uff0c\u4f46\u5176\u5b9e\u9645\u6548\u679c\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u3002", "method": "\u7814\u7a76\u901a\u8fc720,000\u591a\u5f20\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u516c\u5171\u548c\u81ea\u5b9a\u4e49\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\uff0c\u8bc4\u4f30\u5149\u5b66\u8bc6\u522b\u7684\u80fd\u529b\u4e0e\u9650\u5236\uff0c\u4f7f\u7528Grad-CAM\u3001\u663e\u8457\u6027\u56fe\u548c\u6df7\u6dc6\u77e9\u9635\u5206\u6790\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5149\u5b66\u8bc6\u522b\u65b9\u6cd5\u5728\u73b0\u5b9e\u5851\u6599\u5206\u62e3\u4e2d\u6548\u679c\u6709\u9650\uff0c\u56e0\u5176\u4f9d\u8d56\u989c\u8272\u548c\u5f62\u72b6\u7b49\u7269\u7406\u7279\u6027\u3002", "conclusion": "\u5149\u5b66\u8bc6\u522b\u5728\u5851\u6599\u5206\u62e3\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u6216\u63a2\u7d22\u5176\u4ed6\u6280\u672f\u3002"}}
{"id": "2505.16552", "pdf": "https://arxiv.org/pdf/2505.16552", "abs": "https://arxiv.org/abs/2505.16552", "authors": ["Wenhui Tan", "Jiaze Li", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Ruihua Song"], "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "categories": ["cs.CL"], "comment": "15 pages, 8 figures", "summary": "Large Language Models (LLMs) achieve superior performance through\nChain-of-Thought (CoT) reasoning, but these token-level reasoning chains are\ncomputationally expensive and inefficient. In this paper, we introduce\nCompressed Latent Reasoning (CoLaR), a novel framework that dynamically\ncompresses reasoning processes in latent space through a two-stage training\napproach. First, during supervised fine-tuning, CoLaR extends beyond next-token\nprediction by incorporating an auxiliary next compressed embedding prediction\nobjective. This process merges embeddings of consecutive tokens using a\ncompression factor randomly sampled from a predefined range, and trains a\nspecialized latent head to predict distributions of subsequent compressed\nembeddings. Second, we enhance CoLaR through reinforcement learning (RL) that\nleverages the latent head's non-deterministic nature to explore diverse\nreasoning paths and exploit more compact ones. This approach enables CoLaR to:\ni) perform reasoning at a dense latent level (i.e., silently), substantially\nreducing reasoning chain length, and ii) dynamically adjust reasoning speed at\ninference time by simply prompting the desired compression factor. Extensive\nexperiments across four mathematical reasoning datasets demonstrate that CoLaR\nachieves 14.1% higher accuracy than latent-based baseline methods at comparable\ncompression ratios, and reduces reasoning chain length by 53.3% with only 4.8%\nperformance degradation compared to explicit CoT method. Moreover, when applied\nto more challenging mathematical reasoning tasks, our RL-enhanced CoLaR\ndemonstrates performance gains of up to 5.4% while dramatically reducing latent\nreasoning chain length by 82.8%. The code and models will be released upon\nacceptance.", "AI": {"tldr": "CoLaR\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u7406\u94fe\u957f\u5ea6\u5e76\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLMs\u4e2d\u663e\u5f0f\u63a8\u7406\u94fe\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u9884\u6d4b\u538b\u7f29\u5d4c\u5165\uff0c\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u63a2\u7d22\u591a\u6837\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cCoLaR\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534714.1%\uff0c\u63a8\u7406\u94fe\u957f\u5ea6\u51cf\u5c1153.3%\u3002", "conclusion": "CoLaR\u5728\u6f5c\u5728\u7a7a\u95f4\u9ad8\u6548\u63a8\u7406\uff0c\u52a8\u6001\u8c03\u6574\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2505.16524", "pdf": "https://arxiv.org/pdf/2505.16524", "abs": "https://arxiv.org/abs/2505.16524", "authors": ["Huitong Yang", "Zhuoxiao Chen", "Fengyi Zhang", "Zi Huang", "Yadan Luo"], "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Maintaining robust 3D perception under dynamic and unpredictable test-time\nconditions remains a critical challenge for autonomous driving systems.\nExisting test-time adaptation (TTA) methods often fail in high-variance tasks\nlike 3D object detection due to unstable optimization and sharp minima. While\nrecent model merging strategies based on linear mode connectivity (LMC) offer\nimproved stability by interpolating between fine-tuned checkpoints, they are\ncomputationally expensive, requiring repeated checkpoint access and multiple\nforward passes. In this paper, we introduce CodeMerge, a lightweight and\nscalable model merging framework that bypasses these limitations by operating\nin a compact latent space. Instead of loading full models, CodeMerge represents\neach checkpoint with a low-dimensional fingerprint derived from the source\nmodel's penultimate features and constructs a key-value codebook. We compute\nmerging coefficients using ridge leverage scores on these fingerprints,\nenabling efficient model composition without compromising adaptation quality.\nOur method achieves strong performance across challenging benchmarks, improving\nend-to-end 3D detection 14.9% NDS on nuScenes-C and LiDAR-based detection by\nover 7.6% mAP on nuScenes-to-KITTI, while benefiting downstream tasks such as\nonline mapping, motion prediction and planning even without training. Code and\npretrained models are released in the supplementary material.", "AI": {"tldr": "CodeMerge\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u578b\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u7ef4\u6307\u7eb9\u548c\u952e\u503c\u7801\u672c\u9ad8\u6548\u5408\u5e76\u6a21\u578b\uff0c\u63d0\u53473D\u611f\u77e5\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e0b\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6e90\u6a21\u578b\u7684\u5012\u6570\u7b2c\u4e8c\u5c42\u7279\u5f81\u751f\u6210\u4f4e\u7ef4\u6307\u7eb9\uff0c\u6784\u5efa\u952e\u503c\u7801\u672c\uff0c\u901a\u8fc7\u5cad\u6760\u6746\u5206\u6570\u8ba1\u7b97\u5408\u5e76\u7cfb\u6570\u3002", "result": "\u5728nuScenes-C\u548cnuScenes-to-KITTI\u57fa\u51c6\u4e0a\u5206\u522b\u63d0\u534714.9% NDS\u548c7.6% mAP\u3002", "conclusion": "CodeMerge\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2505.16566", "pdf": "https://arxiv.org/pdf/2505.16566", "abs": "https://arxiv.org/abs/2505.16566", "authors": ["Dongwon Noh", "Donghyeok Koh", "Junghun Yuk", "Gyuwan Kim", "Jaeyong Lee", "Kyungtae Lim", "Cheoneum Park"], "title": "ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts", "categories": ["cs.CL"], "comment": null, "summary": "Prior benchmarks for evaluating the domain-specific knowledge of large\nlanguage models (LLMs) lack the scalability to handle complex academic tasks.\nTo address this, we introduce \\texttt{ScholarBench}, a benchmark centered on\ndeep expert knowledge and complex academic problem-solving, which evaluates the\nacademic reasoning ability of LLMs and is constructed through a three-step\nprocess. \\texttt{ScholarBench} targets more specialized and logically complex\ncontexts derived from academic literature, encompassing five distinct problem\ntypes. Unlike prior benchmarks, \\texttt{ScholarBench} evaluates the\nabstraction, comprehension, and reasoning capabilities of LLMs across eight\ndistinct research domains. To ensure high-quality evaluation data, we define\ncategory-specific example attributes and design questions that are aligned with\nthe characteristic research methodologies and discourse structures of each\ndomain. Additionally, this benchmark operates as an English-Korean bilingual\ndataset, facilitating simultaneous evaluation for linguistic capabilities of\nLLMs in both languages. The benchmark comprises 5,031 examples in Korean and\n5,309 in English, with even state-of-the-art models like o3-mini achieving an\naverage evaluation score of only 0.543, demonstrating the challenging nature of\nthis benchmark.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\texttt{ScholarBench}\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6df1\u5ea6\u4e13\u5bb6\u77e5\u8bc6\u548c\u590d\u6742\u5b66\u672f\u95ee\u9898\u89e3\u51b3\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u672f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5b66\u672f\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u4e13\u4e1a\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u4e09\u6b65\u6784\u5efa\u8fc7\u7a0b\uff0c\u8bbe\u8ba1\u5305\u542b\u4e94\u79cd\u95ee\u9898\u7c7b\u578b\u548c\u516b\u4e2a\u7814\u7a76\u9886\u57df\u7684\u53cc\u8bed\u6570\u636e\u96c6\uff08\u82f1\u8bed\u548c\u97e9\u8bed\uff09\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b5,031\u4e2a\u97e9\u8bed\u548c5,309\u4e2a\u82f1\u8bed\u6837\u672c\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u6a21\u578b\u5982o3-mini\u7684\u5e73\u5747\u5f97\u5206\u4ec5\u4e3a0.543\uff0c\u663e\u793a\u5176\u6311\u6218\u6027\u3002", "conclusion": "\texttt{ScholarBench}\u4e3a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u5de5\u5177\u3002"}}
{"id": "2505.16533", "pdf": "https://arxiv.org/pdf/2505.16533", "abs": "https://arxiv.org/abs/2505.16533", "authors": ["Jiacong Chen", "Qingyu Mao", "Youneng Bao", "Xiandong Meng", "Fanyang Meng", "Ronggang Wang", "Yongsheng Liang"], "title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a high-fidelity and efficient\nparadigm for online free-viewpoint video (FVV) reconstruction, offering viewers\nrapid responsiveness and immersive experiences. However, existing online\nmethods face challenge in prohibitive storage requirements primarily due to\npoint-wise modeling that fails to exploit the motion properties. To address\nthis limitation, we propose a novel Compact Gaussian Streaming (ComGS)\nframework, leveraging the locality and consistency of motion in dynamic scene,\nthat models object-consistent Gaussian point motion through keypoint-driven\nmotion representation. By transmitting only the keypoint attributes, this\nframework provides a more storage-efficient solution. Specifically, we first\nidentify a sparse set of motion-sensitive keypoints localized within motion\nregions using a viewspace gradient difference strategy. Equipped with these\nkeypoints, we propose an adaptive motion-driven mechanism that predicts a\nspatial influence field for propagating keypoint motion to neighboring Gaussian\npoints with similar motion. Moreover, ComGS adopts an error-aware correction\nstrategy for key frame reconstruction that selectively refines erroneous\nregions and mitigates error accumulation without unnecessary overhead. Overall,\nComGS achieves a remarkable storage reduction of over 159 X compared to\n3DGStream and 14 X compared to the SOTA method QUEEN, while maintaining\ncompetitive visual fidelity and rendering speed. Our code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompact Gaussian Streaming (ComGS)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u52a8\u6001\u573a\u666f\u4e2d\u8fd0\u52a8\u7684\u5c40\u90e8\u6027\u548c\u4e00\u81f4\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6e32\u67d3\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5728\u7ebf\u81ea\u7531\u89c6\u70b9\u89c6\u9891\uff08FVV\uff09\u91cd\u5efa\u65b9\u6cd5\uff08\u59823D\u9ad8\u65af\u6cfc\u6e85\uff09\u56e0\u9010\u70b9\u5efa\u6a21\u5bfc\u81f4\u5b58\u50a8\u9700\u6c42\u8fc7\u9ad8\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u8fd0\u52a8\u7279\u6027\u3002", "method": "ComGS\u901a\u8fc7\u5173\u952e\u70b9\u9a71\u52a8\u7684\u8fd0\u52a8\u8868\u793a\u5efa\u6a21\u5bf9\u8c61\u4e00\u81f4\u7684\u9ad8\u65af\u70b9\u8fd0\u52a8\uff0c\u4ec5\u4f20\u8f93\u5173\u952e\u70b9\u5c5e\u6027\uff1b\u91c7\u7528\u89c6\u56fe\u7a7a\u95f4\u68af\u5ea6\u5dee\u5f02\u7b56\u7565\u5b9a\u4f4d\u8fd0\u52a8\u533a\u57df\u7684\u5173\u952e\u70b9\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u8fd0\u52a8\u9a71\u52a8\u673a\u5236\u9884\u6d4b\u7a7a\u95f4\u5f71\u54cd\u573a\uff1b\u8fd8\u5f15\u5165\u4e86\u9519\u8bef\u611f\u77e5\u6821\u6b63\u7b56\u7565\u4ee5\u51cf\u5c11\u8bef\u5dee\u79ef\u7d2f\u3002", "result": "ComGS\u5728\u5b58\u50a8\u6548\u7387\u4e0a\u6bd43DGStream\u63d0\u5347\u4e86159\u500d\uff0c\u6bd4QUEEN\u63d0\u5347\u4e8614\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u6e32\u67d3\u901f\u5ea6\u3002", "conclusion": "ComGS\u4e3a\u52a8\u6001\u573a\u666f\u7684\u5728\u7ebfFVV\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b58\u50a8\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16570", "pdf": "https://arxiv.org/pdf/2505.16570", "abs": "https://arxiv.org/abs/2505.16570", "authors": ["Dongyang Fan", "Vinko Sabol\u010dec", "Martin Jaggi"], "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are commonly pretrained on vast corpora of text\nwithout utilizing contextual metadata such as source, quality, or topic,\nleading to a context-free learning paradigm. While recent studies suggest that\nadding metadata like URL information as context (i.e., auxiliary inputs not\nused in the loss calculation) can improve training efficiency and downstream\nperformance, they offer limited understanding of which types of metadata are\ntruly effective and under what conditions. In this work, we conduct a\nsystematic evaluation and find that not all metadata types contribute equally.\nOnly URL context speeds up training, whereas quality scores and topic/format\ndomain information offer no clear benefit. Furthermore, the improved downstream\nperformances of URL conditioning emerge only when longer prompts are used at\ninference time. In addition, we demonstrate that context-aware pretraining\nenables more controllable generation than context-free pretraining, in a\nclassifier-free guidance fashion. Although topic and format metadata do not\naccelerate training, they are effective for steering outputs, offering\nhuman-interpretable control over generation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e76\u975e\u6240\u6709\u5143\u6570\u636e\u7c7b\u578b\u5bf9LLM\u8bad\u7ec3\u548c\u6027\u80fd\u90fd\u6709\u540c\u7b49\u8d21\u732e\uff0c\u4ec5URL\u4e0a\u4e0b\u6587\u80fd\u52a0\u901f\u8bad\u7ec3\uff0c\u800c\u8d28\u91cf\u8bc4\u5206\u548c\u4e3b\u9898/\u683c\u5f0f\u4fe1\u606f\u65e0\u660e\u663e\u5e2e\u52a9\u3002URL\u4e0a\u4e0b\u6587\u4ec5\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u8f83\u957f\u63d0\u793a\u65f6\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\u3002\u6b64\u5916\uff0c\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u6bd4\u65e0\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u66f4\u5177\u53ef\u63a7\u6027\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u7c7b\u578b\u5143\u6570\u636e\uff08\u5982URL\u3001\u8d28\u91cf\u8bc4\u5206\u3001\u4e3b\u9898/\u683c\u5f0f\uff09\u5bf9LLM\u8bad\u7ec3\u6548\u7387\u548c\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u5143\u6570\u636e\u7c7b\u578b\uff08URL\u3001\u8d28\u91cf\u8bc4\u5206\u3001\u4e3b\u9898/\u683c\u5f0f\uff09\u5728LLM\u9884\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\uff0c\u5206\u6790\u5176\u5bf9\u8bad\u7ec3\u901f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u4ec5URL\u4e0a\u4e0b\u6587\u80fd\u52a0\u901f\u8bad\u7ec3\uff1b\u8d28\u91cf\u8bc4\u5206\u548c\u4e3b\u9898/\u683c\u5f0f\u4fe1\u606f\u5bf9\u8bad\u7ec3\u65e0\u663e\u8457\u5e2e\u52a9\u3002URL\u4e0a\u4e0b\u6587\u5728\u63a8\u7406\u65f6\u9700\u8f83\u957f\u63d0\u793a\u624d\u80fd\u63d0\u5347\u6027\u80fd\u3002\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u66f4\u5177\u751f\u6210\u53ef\u63a7\u6027\u3002", "conclusion": "URL\u4e0a\u4e0b\u6587\u662f\u552f\u4e00\u80fd\u52a0\u901f\u8bad\u7ec3\u7684\u5143\u6570\u636e\u7c7b\u578b\uff0c\u800c\u4e3b\u9898\u548c\u683c\u5f0f\u4fe1\u606f\u867d\u4e0d\u52a0\u901f\u8bad\u7ec3\uff0c\u4f46\u53ef\u7528\u4e8e\u751f\u6210\u63a7\u5236\u3002\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u63d0\u4f9b\u66f4\u53ef\u63a7\u7684\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2505.16535", "pdf": "https://arxiv.org/pdf/2505.16535", "abs": "https://arxiv.org/abs/2505.16535", "authors": ["Asrar Alruwayqi"], "title": "SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for dynamic 3D scene reconstruction that\nintegrates three key components: an explicit tri-plane deformation field, a\nview-conditioned canonical radiance field with spherical harmonics (SH)\nattention, and a temporally-aware latent diffusion prior. Our method encodes 4D\nscenes using three orthogonal 2D feature planes that evolve over time, enabling\nefficient and compact spatiotemporal representation. These features are\nexplicitly warped into a canonical space via a deformation offset field,\neliminating the need for MLP-based motion modeling.\n  In canonical space, we replace traditional MLP decoders with a structured\nSH-based rendering head that synthesizes view-dependent color via attention\nover learned frequency bands improving both interpretability and rendering\nefficiency. To further enhance fidelity and temporal consistency, we introduce\na transformer-guided latent diffusion module that refines the tri-plane and\ndeformation features in a compressed latent space. This generative module\ndenoises scene representations under ambiguous or out-of-distribution (OOD)\nmotion, improving generalization.\n  Our model is trained in two stages: the diffusion module is first pre-trained\nindependently, and then fine-tuned jointly with the full pipeline using a\ncombination of image reconstruction, diffusion denoising, and temporal\nconsistency losses. We demonstrate state-of-the-art results on synthetic\nbenchmarks, surpassing recent methods such as HexPlane and 4D Gaussian\nSplatting in visual quality, temporal coherence, and robustness to sparse-view\ndynamic inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u663e\u5f0f\u4e09\u5e73\u9762\u53d8\u5f62\u573a\u3001\u89c6\u56fe\u6761\u4ef6\u5316\u7684\u89c4\u8303\u8f90\u5c04\u573a\u548c\u65f6\u5e8f\u611f\u77e5\u7684\u6f5c\u5728\u6269\u6563\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7d27\u51d1\u7684\u65f6\u7a7a\u8868\u793a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56MLP\u5efa\u6a21\u8fd0\u52a8\uff0c\u6548\u7387\u4f4e\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u65b0\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u53d8\u5f62\u573a\u548c\u7ed3\u6784\u5316SH\u6e32\u67d3\u5934\u63d0\u5347\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u6269\u6563\u6a21\u5757\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u5e73\u9762\u53d8\u5f62\u573a\u7f16\u78014D\u573a\u666f\uff0c\u901a\u8fc7SH\u6ce8\u610f\u529b\u673a\u5236\u5408\u6210\u89c6\u56fe\u4f9d\u8d56\u989c\u8272\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u6269\u6563\u6a21\u5757\u4f18\u5316\u7279\u5f81\u3002\u8bad\u7ec3\u5206\u4e3a\u6269\u6563\u6a21\u5757\u9884\u8bad\u7ec3\u548c\u8054\u5408\u5fae\u8c03\u4e24\u9636\u6bb5\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u8d28\u91cf\u3001\u65f6\u5e8f\u4e00\u81f4\u6027\u548c\u7a00\u758f\u89c6\u56fe\u52a8\u6001\u8f93\u5165\u7684\u9c81\u68d2\u6027\u5747\u4f18\u4e8eHexPlane\u548c4D Gaussian Splatting\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u53d8\u5f62\u573a\u548c\u6f5c\u5728\u6269\u6563\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u7684\u6548\u7387\u3001\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.16576", "pdf": "https://arxiv.org/pdf/2505.16576", "abs": "https://arxiv.org/abs/2505.16576", "authors": ["Spencer Hong", "Meng Luo", "Xinyi Wan"], "title": "EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions", "categories": ["cs.CL"], "comment": null, "summary": "Determining the veracity of atomic claims is an imperative component of many\nrecently proposed fact-checking systems. Many approaches tackle this problem by\nfirst retrieving evidence by querying a search engine and then performing\nclassification by providing the evidence set and atomic claim to a large\nlanguage model, but this process deviates from what a human would do in order\nto perform the task. Recent work attempted to address this issue by proposing\niterative evidence retrieval, allowing for evidence to be collected several\ntimes and only when necessary. Continuing along this line of research, we\npropose a novel claim verification system, called EMULATE, which is designed to\nbetter emulate human actions through the use of a multi-agent framework where\neach agent performs a small part of the larger task, such as ranking search\nresults according to predefined criteria or evaluating webpage content.\nExtensive experiments on several benchmarks show clear improvements over prior\nwork, demonstrating the efficacy of our new multi-agent framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEMULATE\u7684\u65b0\u578b\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u901a\u8fc7\u68c0\u7d22\u8bc1\u636e\u5e76\u5206\u7c7b\u7684\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e0d\u7b26\uff0c\u9700\u8981\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u8d1f\u8d23\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u641c\u7d22\u7ed3\u679c\u6392\u5e8f\u3001\u7f51\u9875\u5185\u5bb9\u8bc4\u4f30\uff09\uff0c\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "EMULATE\u7cfb\u7edf\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u5b9e\u6838\u67e5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2505.16540", "pdf": "https://arxiv.org/pdf/2505.16540", "abs": "https://arxiv.org/abs/2505.16540", "authors": ["Inbal Cohen", "Boaz Meivar", "Peihan Tu", "Shai Avidan", "Gal Oren"], "title": "TextureSAM: Towards a Texture Aware Foundation Model for Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Segment Anything Models (SAM) have achieved remarkable success in object\nsegmentation tasks across diverse datasets. However, these models are\npredominantly trained on large-scale semantic segmentation datasets, which\nintroduce a bias toward object shape rather than texture cues in the image.\nThis limitation is critical in domains such as medical imaging, material\nclassification, and remote sensing, where texture changes define object\nboundaries. In this study, we investigate SAM's bias toward semantics over\ntextures and introduce a new texture-aware foundation model, TextureSAM, which\nperforms superior segmentation in texture-dominant scenarios. To achieve this,\nwe employ a novel fine-tuning approach that incorporates texture augmentation\ntechniques, incrementally modifying training images to emphasize texture\nfeatures. By leveraging a novel texture-alternation of the ADE20K dataset, we\nguide TextureSAM to prioritize texture-defined regions, thereby mitigating the\ninherent shape bias present in the original SAM model. Our extensive\nexperiments demonstrate that TextureSAM significantly outperforms SAM-2 on both\nnatural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation\ndatasets. The code and texture-augmented dataset will be publicly available.", "AI": {"tldr": "TextureSAM\u662f\u4e00\u79cd\u9488\u5bf9\u7eb9\u7406\u4e3b\u5bfc\u573a\u666f\u4f18\u5316\u7684\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u7eb9\u7406\u589e\u5f3a\u6280\u672f\u6539\u8fdbSAM\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7eb9\u7406\u5206\u5272\u6027\u80fd\u3002", "motivation": "SAM\u6a21\u578b\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u504f\u5411\u5f62\u72b6\u800c\u975e\u7eb9\u7406\uff0c\u9650\u5236\u4e86\u5176\u5728\u7eb9\u7406\u5b9a\u4e49\u8fb9\u754c\u7684\u9886\u57df\uff08\u5982\u533b\u5b66\u5f71\u50cf\u3001\u6750\u6599\u5206\u7c7b\uff09\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faTextureSAM\uff0c\u91c7\u7528\u7eb9\u7406\u589e\u5f3a\u6280\u672f\u548c\u7eb9\u7406\u4fee\u6539\u7684ADE20K\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5f3a\u8c03\u7eb9\u7406\u7279\u5f81\u3002", "result": "TextureSAM\u5728\u81ea\u7136\u548c\u5408\u6210\u7eb9\u7406\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4SAM-2\u63d0\u9ad8\u4e860.2\u548c0.18 mIoU\u3002", "conclusion": "TextureSAM\u6709\u6548\u89e3\u51b3\u4e86SAM\u7684\u7eb9\u7406\u5206\u5272\u5c40\u9650\u6027\uff0c\u4e3a\u7eb9\u7406\u4e3b\u5bfc\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5206\u5272\u65b9\u6848\u3002"}}
{"id": "2505.16582", "pdf": "https://arxiv.org/pdf/2505.16582", "abs": "https://arxiv.org/abs/2505.16582", "authors": ["Jianbiao Mei", "Tao Hu", "Daocheng Fu", "Licheng Wen", "Xuemeng Yang", "Rong Wu", "Pinlong Cai", "Xing Gao", "Yu Yang", "Chengjun Xie", "Botian Shi", "Yong Liu", "Yu Qiao"], "title": "O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 figures", "summary": "Large Language Models (LLMs), despite their advancements, are fundamentally\nlimited by their static parametric knowledge, hindering performance on tasks\nrequiring open-domain up-to-date information. While enabling LLMs to interact\nwith external knowledge environments is a promising solution, current efforts\nprimarily address closed-end problems. Open-ended questions, which\ncharacterized by lacking a standard answer or providing non-unique and diverse\nanswers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a\nnovel search agent leveraging reinforcement learning to effectively tackle both\nopen-ended and closed-ended questions in the open domain. O$^2$-Searcher\nleverages an efficient, locally simulated search environment for dynamic\nknowledge acquisition, effectively decoupling the external world knowledge from\nmodel's sophisticated reasoning processes. It employs a unified training\nmechanism with meticulously designed reward functions, enabling the agent to\nidentify problem types and adapt different answer generation strategies.\nFurthermore, to evaluate performance on complex open-ended tasks, we construct\nO$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain\nopen-ended questions with associated web page caches. Extensive experiments\nshow that O$^2$-Searcher, using only a 3B model, significantly surpasses\nleading LLM agents on O$^2$-QA. It also achieves SOTA results on various\nclosed-ended QA benchmarks against similarly-sized models, while performing on\npar with much larger ones.", "AI": {"tldr": "O\u00b2-Searcher\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\uff0c\u65e8\u5728\u89e3\u51b3\u5f00\u653e\u57df\u4e2d\u7684\u5f00\u653e\u6027\u548c\u5c01\u95ed\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u83b7\u53d6\u548c\u7edf\u4e00\u8bad\u7ec3\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d7\u9650\u4e8e\u9759\u6001\u53c2\u6570\u77e5\u8bc6\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u5f00\u653e\u57df\u6700\u65b0\u4fe1\u606f\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faO\u00b2-Searcher\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u672c\u5730\u6a21\u62df\u641c\u7d22\u73af\u5883\u4e2d\u52a8\u6001\u83b7\u53d6\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u7edf\u4e00\u8bad\u7ec3\u673a\u5236\u548c\u5956\u52b1\u51fd\u6570\u3002", "result": "O\u00b2-Searcher\u5728O\u00b2-QA\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6LLM\u4ee3\u7406\uff0c\u5e76\u5728\u5c01\u95ed\u6027\u95ee\u9898\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "O\u00b2-Searcher\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u83b7\u53d6\u548c\u9002\u5e94\u6027\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u6027\u548c\u5c01\u95ed\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002"}}
{"id": "2505.16561", "pdf": "https://arxiv.org/pdf/2505.16561", "abs": "https://arxiv.org/abs/2505.16561", "authors": ["Jannis Becktepe", "Leona Hennig", "Steffen Oeltze-Jafra", "Marius Lindauer"], "title": "Auto-nnU-Net: Towards Automated Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "31 pages, 19 figures. Accepted for publication at AutoML 2025", "summary": "Medical Image Segmentation (MIS) includes diverse tasks, from bone to organ\nsegmentation, each with its own challenges in finding the best segmentation\nmodel. The state-of-the-art AutoML-related MIS-framework nnU-Net automates many\naspects of model configuration but remains constrained by fixed hyperparameters\nand heuristic design choices. As a full-AutoML framework for MIS, we propose\nAuto-nnU-Net, a novel nnU-Net variant enabling hyperparameter optimization\n(HPO), neural architecture search (NAS), and hierarchical NAS (HNAS).\nAdditionally, we propose Regularized PriorBand to balance model accuracy with\nthe computational resources required for training, addressing the resource\nconstraints often faced in real-world medical settings that limit the\nfeasibility of extensive training procedures. We evaluate our approach across\ndiverse MIS datasets from the well-established Medical Segmentation Decathlon,\nanalyzing the impact of AutoML techniques on segmentation performance,\ncomputational efficiency, and model design choices. The results demonstrate\nthat our AutoML approach substantially improves the segmentation performance of\nnnU-Net on 6 out of 10 datasets and is on par on the other datasets while\nmaintaining practical resource requirements. Our code is available at\nhttps://github.com/LUH-AI/AutonnUNet.", "AI": {"tldr": "Auto-nnU-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684nnU-Net\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u53c2\u6570\u4f18\u5316\uff08HPO\uff09\u3001\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u548c\u5206\u5c42NAS\uff08HNAS\uff09\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u5e73\u8861\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u73b0\u6709nnU-Net\u6846\u67b6\u5728\u8d85\u53c2\u6570\u548c\u8bbe\u8ba1\u9009\u62e9\u4e0a\u53d7\u9650\uff0c\u65e0\u6cd5\u5b8c\u5168\u9002\u5e94\u591a\u6837\u5316\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u63d0\u51faAuto-nnU-Net\u6846\u67b6\uff0c\u7ed3\u5408HPO\u3001NAS\u548cHNAS\uff0c\u5e76\u5f15\u5165Regularized PriorBand\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c6\u4e2d\uff0c6\u4e2a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5176\u4f59\u6301\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "Auto-nnU-Net\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u533b\u7597\u573a\u666f\u3002"}}
{"id": "2505.16591", "pdf": "https://arxiv.org/pdf/2505.16591", "abs": "https://arxiv.org/abs/2505.16591", "authors": ["Bowen Jiang", "Runchuan Zhu", "Jiang Wu", "Zinco Jiang", "Yifan He", "Junyuan Gao", "Jia Yu", "Rui Min", "Yinfan Wang", "Haote Yang", "Songyang Zhang", "Dahua Lin", "Lijun Wu", "Conghui He"], "title": "Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering", "categories": ["cs.CL"], "comment": "Equal contribution: Bowen Jiang, Runchuan Zhu, Jiang Wu;\n  Corresponding author: Conghui He", "summary": "We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual\nfactual ability of Large Language Models (LLMs). Inspired by existing research,\nwe created the question set with features such as single knowledge point\ncoverage, absolute objectivity, unique answers, and temporal stability. These\nquestions enable efficient evaluation using the LLM-as-judge paradigm, testing\nboth the LLMs' factual memory and self-awareness (\"know what they don't know\").\nKoLasSimpleQA expands existing research in two key dimensions: (1) Breadth\n(Multilingual Coverage): It includes 9 languages, supporting global\napplicability evaluation. (2) Depth (Dual Domain Design): It covers both the\ngeneral domain (global facts) and the language-specific domain (such as\nhistory, culture, and regional traditions) for a comprehensive assessment of\nmultilingual capabilities. We evaluated mainstream LLMs, including traditional\nLLM and emerging Large Reasoning Models. Results show significant performance\ndifferences between the two domains, particularly in performance metrics,\nranking, calibration, and robustness. This highlights the need for targeted\nevaluation and optimization in multilingual contexts. We hope KoLasSimpleQA\nwill help the research community better identify LLM capability boundaries in\nmultilingual contexts and provide guidance for model optimization. We will\nrelease KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .", "AI": {"tldr": "KoLasSimpleQA\u662f\u9996\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u591a\u8bed\u8a00\u4e8b\u5b9e\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d69\u79cd\u8bed\u8a00\u548c\u53cc\u9886\u57df\u8bbe\u8ba1\uff0c\u65e8\u5728\u9ad8\u6548\u6d4b\u8bd5LLMs\u7684\u4e8b\u5b9e\u8bb0\u5fc6\u548c\u81ea\u6211\u610f\u8bc6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLMs\u591a\u8bed\u8a00\u4e8b\u5b9e\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\uff0cKoLasSimpleQA\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u5168\u7403\u9002\u7528\u6027\u548c\u6df1\u5ea6\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5177\u6709\u5355\u77e5\u8bc6\u70b9\u8986\u76d6\u3001\u7edd\u5bf9\u5ba2\u89c2\u6027\u3001\u552f\u4e00\u7b54\u6848\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u96c6\uff0c\u91c7\u7528LLM-as-judge\u8303\u5f0f\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e3b\u6d41LLMs\u5728\u901a\u7528\u9886\u57df\u548c\u8bed\u8a00\u7279\u5b9a\u9886\u57df\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5c24\u5176\u5728\u6027\u80fd\u6307\u6807\u3001\u6392\u540d\u3001\u6821\u51c6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "KoLasSimpleQA\u6709\u52a9\u4e8e\u8bc6\u522bLLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u5e76\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2505.16565", "pdf": "https://arxiv.org/pdf/2505.16565", "abs": "https://arxiv.org/abs/2505.16565", "authors": ["Nina Shvetsova", "Goutam Bhat", "Prune Truong", "Hilde Kuehne", "Federico Tombari"], "title": "M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion", "categories": ["cs.CV"], "comment": null, "summary": "We tackle the problem of monocular-to-stereo video conversion and propose a\nnovel architecture for inpainting and refinement of the warped right view\nobtained by depth-based reprojection of the input left view. We extend the\nStable Video Diffusion (SVD) model to utilize the input left video, the warped\nright video, and the disocclusion masks as conditioning input to generate a\nhigh-quality right camera view. In order to effectively exploit information\nfrom neighboring frames for inpainting, we modify the attention layers in SVD\nto compute full attention for discoccluded pixels. Our model is trained to\ngenerate the right view video in an end-to-end manner by minimizing image space\nlosses to ensure high-quality generation. Our approach outperforms previous\nstate-of-the-art methods, obtaining an average rank of 1.43 among the 4\ncompared methods in a user study, while being 6x faster than the second placed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u76ee\u5230\u7acb\u4f53\u89c6\u9891\u8f6c\u6362\u67b6\u6784\uff0c\u901a\u8fc7\u6539\u8fdb\u7684Stable Video Diffusion\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u53f3\u89c6\u56fe\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u5230\u7acb\u4f53\u89c6\u9891\u8f6c\u6362\u4e2d\u53f3\u89c6\u56fe\u4fee\u590d\u548c\u7ec6\u5316\u7684\u95ee\u9898\u3002", "method": "\u6269\u5c55Stable Video Diffusion\u6a21\u578b\uff0c\u5229\u7528\u5de6\u89c6\u56fe\u3001\u53d8\u5f62\u53f3\u89c6\u56fe\u548c\u906e\u6321\u63a9\u7801\u4f5c\u4e3a\u8f93\u5165\uff0c\u6539\u8fdb\u6ce8\u610f\u529b\u5c42\u4ee5\u5229\u7528\u90bb\u5e27\u4fe1\u606f\u3002", "result": "\u5728\u7528\u6237\u7814\u7a76\u4e2d\u5e73\u5747\u6392\u540d1.43\uff084\u79cd\u65b9\u6cd5\uff09\uff0c\u901f\u5ea6\u6bd4\u7b2c\u4e8c\u540d\u5feb6\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2505.16592", "pdf": "https://arxiv.org/pdf/2505.16592", "abs": "https://arxiv.org/abs/2505.16592", "authors": ["Shijia Zhou", "Siyao Peng", "Simon Luebke", "J\u00f6rg Ha\u00dfler", "Mario Haim", "Saif M. Mohammad", "Barbara Plank"], "title": "What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse", "categories": ["cs.CL", "cs.MM"], "comment": "19 pages, 9 figures", "summary": "Media framing refers to the emphasis on specific aspects of perceived reality\nto shape how an issue is defined and understood. Its primary purpose is to\nshape public perceptions often in alignment with the authors' opinions and\nstances. However, the interaction between stance and media frame remains\nlargely unexplored. In this work, we apply an interdisciplinary approach to\nconceptualize and computationally explore this interaction with internet memes\non climate change. We curate CLIMATEMEMES, the first dataset of climate-change\nmemes annotated with both stance and media frames, inspired by research in\ncommunication science. CLIMATEMEMES includes 1,184 memes sourced from 47\nsubreddits, enabling analysis of frame prominence over time and communities,\nand sheds light on the framing preferences of different stance holders. We\npropose two meme understanding tasks: stance detection and media frame\ndetection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the\ncorresponding results on their LLM backbone. Human captions consistently\nenhance performance. Synthetic captions and human-corrected OCR also help\noccasionally. Our findings highlight that VLMs perform well on stance, but\nstruggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'\nlimitations in handling nuanced frames and stance expressions on climate change\ninternet memes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5a92\u4f53\u6846\u67b6\u4e0e\u7acb\u573a\u5728\u6c14\u5019\u53d8\u8fc1\u7f51\u7edc\u8ff7\u56e0\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86CLIMATEMEMES\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86LLaVA-NeXT\u548cMolmo\u5728\u7acb\u573a\u68c0\u6d4b\u548c\u5a92\u4f53\u6846\u67b6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5a92\u4f53\u6846\u67b6\u4e0e\u7acb\u573a\u4e4b\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u6c14\u5019\u53d8\u8fc1\u7f51\u7edc\u8ff7\u56e0\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u6784\u5efaCLIMATEMEMES\u6570\u636e\u96c6\uff0c\u5305\u542b1,184\u4e2a\u8ff7\u56e0\uff0c\u5e76\u8bc4\u4f30LLaVA-NeXT\u548cMolmo\u5728\u7acb\u573a\u548c\u6846\u67b6\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7acb\u573a\u68c0\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6846\u67b6\u68c0\u6d4b\u4e0a\u4e0d\u5982\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u4eba\u7c7b\u6807\u6ce8\u7684\u6807\u9898\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "VLMs\u5728\u5904\u7406\u6c14\u5019\u53d8\u8fc1\u8ff7\u56e0\u4e2d\u7684\u590d\u6742\u6846\u67b6\u548c\u7acb\u573a\u8868\u8fbe\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0cLLMs\u5728\u6846\u67b6\u68c0\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.16594", "pdf": "https://arxiv.org/pdf/2505.16594", "abs": "https://arxiv.org/abs/2505.16594", "authors": ["Vignesh Gopinathan", "Urs Zimmermann", "Michael Arnold", "Matthias Rottmann"], "title": "Temporal Object Captioning for Street Scene Videos from LiDAR Tracks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Video captioning models have seen notable advancements in recent years,\nespecially with regard to their ability to capture temporal information. While\nmany research efforts have focused on architectural advancements, such as\ntemporal attention mechanisms, there remains a notable gap in understanding how\nmodels capture and utilize temporal semantics for effective temporal feature\nextraction, especially in the context of Advanced Driver Assistance Systems. We\npropose an automated LiDAR-based captioning procedure that focuses on the\ntemporal dynamics of traffic participants. Our approach uses a rule-based\nsystem to extract essential details such as lane position and relative motion\nfrom object tracks, followed by a template-based caption generation. Our\nfindings show that training SwinBERT, a video captioning model, using only\nfront camera images and supervised with our template-based captions,\nspecifically designed to encapsulate fine-grained temporal behavior, leads to\nimproved temporal understanding consistently across three datasets. In\nconclusion, our results clearly demonstrate that integrating LiDAR-based\ncaption supervision significantly enhances temporal understanding, effectively\naddressing and reducing the inherent visual/static biases prevalent in current\nstate-of-the-art model architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLiDAR\u7684\u81ea\u52a8\u5316\u89c6\u9891\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u901a\u8fc7\u89c4\u5219\u7cfb\u7edf\u548c\u6a21\u677f\u751f\u6210\u5b57\u5e55\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u5728\u65f6\u95f4\u8bed\u4e49\u6355\u6349\u548c\u5229\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff08ADAS\uff09\u80cc\u666f\u4e0b\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLiDAR\u7684\u89c4\u5219\u7cfb\u7edf\u63d0\u53d6\u8f66\u9053\u4f4d\u7f6e\u548c\u76f8\u5bf9\u8fd0\u52a8\u7b49\u5173\u952e\u4fe1\u606f\uff0c\u7ed3\u5408\u6a21\u677f\u751f\u6210\u5b57\u5e55\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3SwinBERT\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u65f6\u95f4\u52a8\u6001\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "LiDAR\u751f\u6210\u7684\u5b57\u5e55\u76d1\u7763\u80fd\u6709\u6548\u51cf\u5c11\u73b0\u6709\u6a21\u578b\u7684\u89c6\u89c9/\u9759\u6001\u504f\u5dee\uff0c\u663e\u8457\u589e\u5f3a\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2505.16610", "pdf": "https://arxiv.org/pdf/2505.16610", "abs": "https://arxiv.org/abs/2505.16610", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "title": "From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment", "categories": ["cs.CL"], "comment": "27 pages", "summary": "Effective emotional support hinges on understanding users' emotions and needs\nto provide meaningful comfort during multi-turn interactions. Large Language\nModels (LLMs) show great potential for expressing empathy; however, they often\ndeliver generic and one-size-fits-all responses that fail to address users'\nspecific needs. To tackle this issue, we propose a self-evolution framework\ndesigned to help LLMs improve their responses to better align with users'\nimplicit preferences concerning user profiles (personalities), emotional\nstates, and specific situations. Our framework consists of two distinct phases:\n\\textit{(1)} \\textit{Emotional Support Experience Acquisition}, where LLMs are\nfine-tuned on limited emotional support conversation data to provide basic\nsupport, and \\textit{(2)} \\textit{Self-Improvement for Personalized Emotional\nSupport}, where LLMs leverage self-reflection and self-refinement to generate\npersonalized responses. Through iterative direct preference optimization\nbetween the pre- and post-refined responses, our model generates responses that\nreflect a better understanding of the user's implicit preferences. Extensive\nexperiments and evaluations demonstrate that our method significantly enhances\nthe model's performance in emotional support, reducing unhelpful responses and\nminimizing discrepancies between user preferences and model outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u6846\u67b6\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8f6e\u4e92\u52a8\u4e2d\u63d0\u4f9b\u66f4\u4e2a\u6027\u5316\u7684\u60c5\u611f\u652f\u6301\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u4f18\u5316\u51cf\u5c11\u901a\u7528\u56de\u590d\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u60c5\u611f\u652f\u6301\u4e2d\u5e38\u63d0\u4f9b\u901a\u7528\u56de\u590d\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u9700\u6539\u8fdb\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u504f\u597d\u3002", "method": "\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u60c5\u611f\u652f\u6301\u7ecf\u9a8c\u83b7\u53d6\uff0c\u901a\u8fc7\u5fae\u8c03\u63d0\u4f9b\u57fa\u7840\u652f\u6301\uff1b2\uff09\u81ea\u6211\u6539\u8fdb\uff0c\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u4f18\u5316\u751f\u6210\u4e2a\u6027\u5316\u56de\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u60c5\u611f\u652f\u6301\u6548\u679c\uff0c\u51cf\u5c11\u65e0\u7528\u56de\u590d\uff0c\u7f29\u5c0f\u7528\u6237\u504f\u597d\u4e0e\u6a21\u578b\u8f93\u51fa\u7684\u5dee\u8ddd\u3002", "conclusion": "\u81ea\u6211\u8fdb\u5316\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u60c5\u611f\u652f\u6301\u4e2d\u7684\u4e2a\u6027\u5316\u8868\u73b0\uff0c\u6ee1\u8db3\u7528\u6237\u9700\u6c42\u3002"}}
{"id": "2505.16599", "pdf": "https://arxiv.org/pdf/2505.16599", "abs": "https://arxiv.org/abs/2505.16599", "authors": ["Yao Huang", "Si-Yuan Cao", "Yaqing Ding", "Hao Yin", "Shibin Xie", "Shuting Wang", "Zhijun Fang", "Jiachun Wang", "Shen Cai", "Junchi Yan", "Shuhan Shen"], "title": "Decoupled Geometric Parameterization and its Application in Deep Homography Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Planar homography, with eight degrees of freedom (DOFs), is fundamental in\nnumerous computer vision tasks. While the positional offsets of four corners\nare widely adopted (especially in neural network predictions), this\nparameterization lacks geometric interpretability and typically requires\nsolving a linear system to compute the homography matrix. This paper presents a\nnovel geometric parameterization of homographies, leveraging the\nsimilarity-kernel-similarity (SKS) decomposition for projective\ntransformations. Two independent sets of four geometric parameters are\ndecoupled: one for a similarity transformation and the other for the kernel\ntransformation. Additionally, the geometric interpretation linearly relating\nthe four kernel transformation parameters to angular offsets is derived. Our\nproposed parameterization allows for direct homography estimation through\nmatrix multiplication, eliminating the need for solving a linear system, and\nachieves performance comparable to the four-corner positional offsets in deep\nhomography estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e73\u9762\u5355\u5e94\u6027\u51e0\u4f55\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7SKS\u5206\u89e3\u89e3\u8026\u76f8\u4f3c\u53d8\u6362\u548c\u6838\u53d8\u6362\u53c2\u6570\uff0c\u7b80\u5316\u4e86\u5355\u5e94\u6027\u77e9\u9635\u7684\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56db\u89d2\u4f4d\u7f6e\u504f\u79fb\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u53ef\u89e3\u91ca\u6027\u4e14\u9700\u89e3\u7ebf\u6027\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u76f4\u89c2\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u76f8\u4f3c-\u6838-\u76f8\u4f3c\uff08SKS\uff09\u5206\u89e3\uff0c\u5c06\u5355\u5e94\u6027\u89e3\u8026\u4e3a\u76f8\u4f3c\u53d8\u6362\u548c\u6838\u53d8\u6362\u4e24\u7ec4\u72ec\u7acb\u53c2\u6570\uff0c\u5e76\u63a8\u5bfc\u6838\u53d8\u6362\u53c2\u6570\u4e0e\u89d2\u5ea6\u504f\u79fb\u7684\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u63d0\u51fa\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u901a\u8fc7\u77e9\u9635\u4e58\u6cd5\u76f4\u63a5\u4f30\u8ba1\u5355\u5e94\u6027\uff0c\u65e0\u9700\u89e3\u7ebf\u6027\u7cfb\u7edf\uff0c\u6027\u80fd\u4e0e\u56db\u89d2\u504f\u79fb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u5355\u5e94\u6027\u4f30\u8ba1\u7684\u51e0\u4f55\u76f4\u89c2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2505.16612", "pdf": "https://arxiv.org/pdf/2505.16612", "abs": "https://arxiv.org/abs/2505.16612", "authors": ["Daniel Scalena", "Gabriele Sarti", "Arianna Bisazza", "Elisabetta Fersini", "Malvina Nissim"], "title": "Steering Large Language Models for Machine Translation Personalization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "High-quality machine translation systems based on large language models\n(LLMs) have simplified the production of personalized translations reflecting\nspecific stylistic constraints. However, these systems still struggle in\nsettings where stylistic requirements are less explicit and might be harder to\nconvey via prompting. We explore various strategies for personalizing\nLLM-generated translations in low-resource settings, focusing on the\nchallenging literary translation domain. We explore prompting strategies and\ninference-time interventions for steering model generations towards a\npersonalized style, and propose a contrastive framework exploiting latent\nconcepts extracted from sparse autoencoders to identify salient personalization\nproperties. Our results show that steering achieves strong personalization\nwhile preserving translation quality. We further examine the impact of steering\non LLM representations, finding model layers with a relevant impact for\npersonalization are impacted similarly by multi-shot prompting and our steering\nmethod, suggesting similar mechanism at play.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u4e2a\u6027\u5316LLM\u751f\u6210\u7ffb\u8bd1\u7684\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6bd4\u6846\u67b6\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6f5c\u5728\u6982\u5ff5\uff0c\u5b9e\u73b0\u5f3a\u4e2a\u6027\u5316\u4e14\u4fdd\u6301\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u9ad8\u8d28\u91cf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5728\u98ce\u683c\u8981\u6c42\u4e0d\u660e\u786e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u6587\u5b66\u7ffb\u8bd1\u9886\u57df\u3002", "method": "\u7814\u7a76\u63d0\u793a\u7b56\u7565\u548c\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u63d0\u51fa\u5bf9\u6bd4\u6846\u67b6\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u6f5c\u5728\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u5f3a\u4e2a\u6027\u5316\u4e14\u4e0d\u635f\u5bb3\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u53d1\u73b0\u591a\u793a\u4f8b\u63d0\u793a\u4e0e\u5e72\u9884\u65b9\u6cd5\u673a\u5236\u76f8\u4f3c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u4e2a\u6027\u5316\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u4e3aLLM\u5728\u6587\u5b66\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16602", "pdf": "https://arxiv.org/pdf/2505.16602", "abs": "https://arxiv.org/abs/2505.16602", "authors": ["Bohan Zhou", "Yi Zhan", "Zhongbin Zhang", "Zongqing Lu"], "title": "MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Egocentric hand-object motion generation is crucial for immersive AR/VR and\nrobotic imitation but remains challenging due to unstable viewpoints,\nself-occlusions, perspective distortion, and noisy ego-motion. Existing methods\nrely on predefined 3D object priors, limiting generalization to novel objects,\nwhich restricts their generalizability to novel objects. Meanwhile, recent\nmultimodal approaches suffer from ambiguous generation from abstract textual\ncues, intricate pipelines for modeling 3D hand-object correlation, and\ncompounding errors in open-loop prediction. We propose MEgoHand, a multimodal\nframework that synthesizes physically plausible hand-object interactions from\negocentric RGB, text, and initial hand pose. MEgoHand introduces a bi-level\narchitecture: a high-level \"cerebrum\" leverages a vision language model (VLM)\nto infer motion priors from visual-textual context and a monocular depth\nestimator for object-agnostic spatial reasoning, while a low-level DiT-based\nflow-matching policy generates fine-grained trajectories with temporal\northogonal filtering to enhance stability. To address dataset inconsistency, we\ndesign a dataset curation paradigm with an Inverse MANO Retargeting Network and\nVirtual RGB-D Renderer, curating a unified dataset of 3.35M RGB-D frames, 24K\ninteractions, and 1.2K objects. Extensive experiments across five in-domain and\ntwo cross-domain datasets demonstrate the effectiveness of MEgoHand, achieving\nsubstantial reductions in wrist translation error (86.9%) and joint rotation\nerror (34.1%), highlighting its capacity to accurately model fine-grained hand\njoint structures and generalize robustly across diverse scenarios.", "AI": {"tldr": "MEgoHand\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408RGB\u3001\u6587\u672c\u548c\u521d\u59cb\u624b\u90e8\u59ff\u6001\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u624b\u8155\u5e73\u79fb\u548c\u5173\u8282\u65cb\u8f6c\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u624b-\u7269\u4f53\u4ea4\u4e92\u65f6\u4f9d\u8d56\u9884\u5b9a\u4e493D\u7269\u4f53\u5148\u9a8c\u3001\u591a\u6a21\u6001\u65b9\u6cd5\u6a21\u7cca\u751f\u6210\u4ee5\u53ca\u590d\u6742\u5efa\u6a213D\u624b-\u7269\u4f53\u5173\u8054\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u8fd0\u52a8\u5148\u9a8c\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\uff0c\u4f4e\u5c42\u57fa\u4e8eDiT\u7684\u6d41\u5339\u914d\u7b56\u7565\u751f\u6210\u7ec6\u7c92\u5ea6\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u624b\u8155\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e86.9%\uff0c\u5173\u8282\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e34.1%\u3002", "conclusion": "MEgoHand\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u7ec6\u7c92\u5ea6\u624b\u90e8\u5173\u8282\u7ed3\u6784\uff0c\u5e76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.16637", "pdf": "https://arxiv.org/pdf/2505.16637", "abs": "https://arxiv.org/abs/2505.16637", "authors": ["Wenjie Yang", "Mao Zheng", "Mingyang Song", "Zheng Li"], "title": "SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable\ncapabilities in machine translation (MT). However, most advanced MT-specific\nLLMs heavily rely on external supervision signals during training, such as\nhuman-annotated reference data or trained reward models (RMs), which are often\nexpensive to obtain and challenging to scale. To overcome this limitation, we\npropose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for\nMT that is reference-free, fully online, and relies solely on self-judging\nrewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as\nthe backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,\ne.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like\nQwen2.5-32B-Instruct in English $\\leftrightarrow$ Chinese translation tasks\nfrom WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR\nwith external supervision from COMET, our strongest model, SSR-X-Zero-7B,\nachieves state-of-the-art performance in English $\\leftrightarrow$ Chinese\ntranslation, surpassing all existing open-source models under 72B parameters\nand even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.\nOur analysis highlights the effectiveness of the self-rewarding mechanism\ncompared to the external LLM-as-a-judge approach in MT and demonstrates its\ncomplementary benefits when combined with trained RMs. Our findings provide\nvaluable insight into the potential of self-improving RL methods. We have\npublicly released our code, data and models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSSR\u7684\u81ea\u6211\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u4fe1\u53f7\uff0c\u4ec5\u4f9d\u8d56\u81ea\u6211\u5224\u65ad\u5956\u52b1\uff0c\u5e76\u5728\u4e2d\u82f1\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5148\u8fdb\u7684\u673a\u5668\u7ffb\u8bd1\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f9d\u8d56\u6602\u8d35\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u5916\u90e8\u76d1\u7763\u4fe1\u53f7\uff08\u5982\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5956\u52b1\u6a21\u578b\uff09\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51faSimple Self-Rewarding (SSR)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b8c\u5168\u5728\u7ebf\u4e14\u65e0\u9700\u53c2\u8003\u6570\u636e\uff0c\u4ec5\u901a\u8fc7\u81ea\u6211\u5224\u65ad\u5956\u52b1\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SSR-Zero-7B\u6a21\u578b\u5728WMT23\u3001WMT24\u548cFlores200\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff1b\u7ed3\u5408COMET\u76d1\u7763\u540e\uff0cSSR-X-Zero-7B\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u81ea\u6211\u5956\u52b1\u673a\u5236\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u6709\u6548\u4e14\u4f18\u4e8e\u5916\u90e8LLM\u8bc4\u5224\u65b9\u6cd5\uff0c\u4e0e\u5956\u52b1\u6a21\u578b\u7ed3\u5408\u65f6\u5177\u6709\u4e92\u8865\u4f18\u52bf\uff0c\u4e3a\u81ea\u6211\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16624", "pdf": "https://arxiv.org/pdf/2505.16624", "abs": "https://arxiv.org/abs/2505.16624", "authors": ["Francesco Dalla Serra", "Patrick Schrempf", "Chaoyang Wang", "Zaiqiao Meng", "Fani Deligianni", "Alison Q. O'Neil"], "title": "Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present a novel approach to Chest X-ray (CXR) Visual Question Answering\n(VQA), addressing both single-image image-difference questions. Single-image\nquestions focus on abnormalities within a specific CXR (\"What abnormalities are\nseen in image X?\"), while image-difference questions compare two longitudinal\nCXRs acquired at different time points (\"What are the differences between image\nX and Y?\"). We further explore how the integration of radiology reports can\nenhance the performance of VQA models. While previous approaches have\ndemonstrated the utility of radiology reports during the pre-training phase, we\nextend this idea by showing that the reports can also be leveraged as\nadditional input to improve the VQA model's predicted answers. First, we\npropose a unified method that handles both types of questions and\nauto-regressively generates the answers. For single-image questions, the model\nis provided with a single CXR. For image-difference questions, the model is\nprovided with two CXRs from the same patient, captured at different time\npoints, enabling the model to detect and describe temporal changes. Taking\ninspiration from 'Chain-of-Thought reasoning', we demonstrate that performance\non the CXR VQA task can be improved by grounding the answer generator module\nwith a radiology report predicted for the same CXR. In our approach, the VQA\nmodel is divided into two steps: i) Report Generation (RG) and ii) Answer\nGeneration (AG). Our results demonstrate that incorporating predicted radiology\nreports as evidence to the AG model enhances performance on both single-image\nand image-difference questions, achieving state-of-the-art results on the\nMedical-Diff-VQA dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684CXR\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\uff0c\u5904\u7406\u5355\u56fe\u50cf\u548c\u56fe\u50cf\u5dee\u5f02\u95ee\u9898\uff0c\u5e76\u5229\u7528\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CXR\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u5355\u56fe\u50cf\u548c\u56fe\u50cf\u5dee\u5f02\u95ee\u9898\uff0c\u63a2\u7d22\u653e\u5c04\u5b66\u62a5\u544a\u5728\u6a21\u578b\u6027\u80fd\u63d0\u5347\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u65b9\u6cd5\u5904\u7406\u4e24\u7c7b\u95ee\u9898\uff0c\u5206\u4e24\u6b65\uff1a\u62a5\u544a\u751f\u6210\u548c\u7b54\u6848\u751f\u6210\uff0c\u5229\u7528\u9884\u6d4b\u7684\u653e\u5c04\u5b66\u62a5\u544a\u4f5c\u4e3a\u8bc1\u636e\u3002", "result": "\u5728Medical-Diff-VQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6574\u5408\u663e\u8457\u63d0\u5347\u4e86CXR\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16648", "pdf": "https://arxiv.org/pdf/2505.16648", "abs": "https://arxiv.org/abs/2505.16648", "authors": ["Kexin Shang", "Chia-Hsuan Chang", "Christopher C. Yang"], "title": "Collaboration among Multiple Large Language Models for Medical Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IEEE International Conference on Healthcare Informatics\n  2025", "summary": "Empowered by vast internal knowledge reservoir, the new generation of large\nlanguage models (LLMs) demonstrate untapped potential to tackle medical tasks.\nHowever, there is insufficient effort made towards summoning up a synergic\neffect from multiple LLMs' expertise and background. In this study, we propose\na multi-LLM collaboration framework tailored on a medical multiple-choice\nquestions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,\nour framework is proved to boost all LLMs reasoning ability as well as\nalleviate their divergence among questions. We also measure an LLM's confidence\nwhen it confronts with adversary opinions from other LLMs and observe a\nconcurrence between LLM's confidence and prediction accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u9009\u62e9\u9898\u6570\u636e\u96c6\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u5e76\u51cf\u5c11\u5206\u6b67\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u4efb\u52a1\u4e2d\u6f5c\u529b\u672a\u5145\u5206\u6316\u6398\uff0c\u7f3a\u4e4f\u591aLLM\u534f\u540c\u6548\u5e94\u7684\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u591aLLM\u534f\u4f5c\u6846\u67b6\uff0c\u57fa\u4e8e\u533b\u5b66\u9009\u62e9\u9898\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e8b\u540e\u5206\u6790\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u6846\u67b6\u63d0\u5347\u6240\u6709LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c11\u95ee\u9898\u5206\u6b67\uff0c\u5e76\u53d1\u73b0LLM\u7684\u7f6e\u4fe1\u5ea6\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u76f8\u5173\u3002", "conclusion": "\u591aLLM\u534f\u4f5c\u6846\u67b6\u6709\u6548\uff0c\u4e3a\u533b\u5b66\u4efb\u52a1\u4e2d\u7684LLM\u534f\u540c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16625", "pdf": "https://arxiv.org/pdf/2505.16625", "abs": "https://arxiv.org/abs/2505.16625", "authors": ["Luyang Cao", "Jianwei Li", "Yinghuan Shi"], "title": "Background Matters: A Cross-view Bidirectional Modeling Framework for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Image Processing", "summary": "Semi-supervised medical image segmentation (SSMIS) leverages unlabeled data\nto reduce reliance on manually annotated images. However, current SOTA\napproaches predominantly focus on foreground-oriented modeling (i.e.,\nsegmenting only the foreground region) and have largely overlooked the\npotential benefits of explicitly modeling the background region. Our study\ntheoretically and empirically demonstrates that highly certain predictions in\nbackground modeling enhance the confidence of corresponding foreground\nmodeling. Building on this insight, we propose the Cross-view Bidirectional\nModeling (CVBM) framework, which introduces a novel perspective by\nincorporating background modeling to improve foreground modeling performance.\nWithin CVBM, background modeling serves as an auxiliary perspective, providing\ncomplementary supervisory signals to enhance the confidence of the foreground\nmodel. Additionally, CVBM introduces an innovative bidirectional consistency\nmechanism, which ensures mutual alignment between foreground predictions and\nbackground-guided predictions. Extensive experiments demonstrate that our\napproach achieves SOTA performance on the LA, Pancreas, ACDC, and HRF datasets.\nNotably, on the Pancreas dataset, CVBM outperforms fully supervised methods\n(i.e., DSC: 84.57% vs. 83.89%) while utilizing only 20% of the labeled data.\nOur code is publicly available at https://github.com/caoluyang0830/CVBM.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCVBM\u7684\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u80cc\u666f\u533a\u57df\u63d0\u5347\u524d\u666f\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u524d\u666f\u5efa\u6a21\uff0c\u5ffd\u7565\u4e86\u80cc\u666f\u5efa\u6a21\u7684\u6f5c\u5728\u4ef7\u503c\u3002\u7814\u7a76\u8868\u660e\u80cc\u666f\u5efa\u6a21\u80fd\u589e\u5f3a\u524d\u666f\u5efa\u6a21\u7684\u7f6e\u4fe1\u5ea6\u3002", "method": "\u63d0\u51faCVBM\u6846\u67b6\uff0c\u5f15\u5165\u80cc\u666f\u5efa\u6a21\u4f5c\u4e3a\u8f85\u52a9\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u4e00\u81f4\u6027\u673a\u5236\u786e\u4fdd\u524d\u666f\u4e0e\u80cc\u666f\u9884\u6d4b\u7684\u5bf9\u9f50\u3002", "result": "\u5728LA\u3001Pancreas\u3001ACDC\u548cHRF\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u4e2dPancreas\u6570\u636e\u96c6\u4e0a\u4ec5\u752820%\u6807\u6ce8\u6570\u636e\u5373\u8d85\u8d8a\u5168\u76d1\u7763\u65b9\u6cd5\uff08DSC: 84.57% vs. 83.89%\uff09\u3002", "conclusion": "CVBM\u901a\u8fc7\u80cc\u666f\u5efa\u6a21\u548c\u53cc\u5411\u4e00\u81f4\u6027\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u80cc\u666f\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.16660", "pdf": "https://arxiv.org/pdf/2505.16660", "abs": "https://arxiv.org/abs/2505.16660", "authors": ["Liu Chang", "Wang Dongbo", "Liu liu", "Zhao Zhixiao"], "title": "Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu", "categories": ["cs.CL", "cs.AI"], "comment": "29pages, 7 figures", "summary": "This study addresses the challenges in intelligent processing of Chinese\nancient mathematical classics by constructing Guji_MATH, a benchmark for\nevaluating classical texts based on Suanjing Shishu. It systematically assesses\nthe mathematical problem-solving capabilities of mainstream reasoning models\nunder the unique linguistic constraints of classical Chinese. Through\nmachine-assisted annotation and manual verification, 538 mathematical problems\nwere extracted from 8 canonical texts, forming a structured dataset centered on\nthe \"Question-Answer-Solution\" framework, supplemented by problem types and\ndifficulty levels. Dual evaluation modes--closed-book (autonomous\nproblem-solving) and open-book (reproducing classical solution methods)--were\ndesigned to evaluate the performance of six reasoning models on ancient Chinese\nmathematical problems. Results indicate that reasoning models can partially\ncomprehend and solve these problems, yet their overall performance remains\ninferior to benchmarks on modern mathematical tasks. Enhancing models'\nclassical Chinese comprehension and cultural knowledge should be prioritized\nfor optimization. This study provides methodological support for mining\nmathematical knowledge from ancient texts and disseminating traditional\nculture, while offering new perspectives for evaluating cross-linguistic and\ncross-cultural capabilities of reasoning models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86Guji_MATH\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u300a\u7b97\u7ecf\u5341\u4e66\u300b\u7684\u53e4\u7c4d\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u53d1\u73b0\u4e3b\u6d41\u63a8\u7406\u6a21\u578b\u5728\u53e4\u6c49\u8bed\u7ea6\u675f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u4f18\u5316\u53e4\u5178\u4e2d\u6587\u7406\u89e3\u548c\u6587\u5316\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u53e4\u6c49\u8bed\u6570\u5b66\u7ecf\u5178\u667a\u80fd\u5904\u7406\u7684\u6311\u6218\uff0c\u6316\u6398\u53e4\u7c4d\u6570\u5b66\u77e5\u8bc6\u5e76\u4f20\u64ad\u4f20\u7edf\u6587\u5316\u3002", "method": "\u901a\u8fc7\u673a\u5668\u8f85\u52a9\u6807\u6ce8\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ece8\u90e8\u7ecf\u5178\u4e2d\u63d0\u53d6538\u4e2a\u6570\u5b66\u95ee\u9898\uff0c\u8bbe\u8ba1\u95ed\u5377\u548c\u5f00\u5377\u4e24\u79cd\u8bc4\u4f30\u6a21\u5f0f\u6d4b\u8bd5\u516d\u79cd\u63a8\u7406\u6a21\u578b\u3002", "result": "\u63a8\u7406\u6a21\u578b\u80fd\u90e8\u5206\u7406\u89e3\u548c\u89e3\u51b3\u95ee\u9898\uff0c\u4f46\u8868\u73b0\u4e0d\u53ca\u73b0\u4ee3\u6570\u5b66\u4efb\u52a1\u57fa\u51c6\uff0c\u9700\u63d0\u5347\u53e4\u5178\u4e2d\u6587\u7406\u89e3\u548c\u6587\u5316\u77e5\u8bc6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53e4\u7c4d\u6570\u5b66\u77e5\u8bc6\u6316\u6398\u548c\u4f20\u7edf\u6587\u5316\u4f20\u64ad\u63d0\u4f9b\u65b9\u6cd5\u652f\u6301\uff0c\u5e76\u4e3a\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u8de8\u6587\u5316\u80fd\u529b\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.16630", "pdf": "https://arxiv.org/pdf/2505.16630", "abs": "https://arxiv.org/abs/2505.16630", "authors": ["Sushant Gautam", "Cise Midoglu", "Vajira Thambawita", "Michael A. Riegler", "P\u00e5l Halvorsen", "Mubarak Shah"], "title": "SoccerChat: Integrating Multimodal Data for Enhanced Soccer Game Understanding", "categories": ["cs.CV", "cs.AI", "68T45, 68T50", "I.2.10; I.2.7; H.5.2"], "comment": null, "summary": "The integration of artificial intelligence in sports analytics has\ntransformed soccer video understanding, enabling real-time, automated insights\ninto complex game dynamics. Traditional approaches rely on isolated data\nstreams, limiting their effectiveness in capturing the full context of a match.\nTo address this, we introduce SoccerChat, a multimodal conversational AI\nframework that integrates visual and textual data for enhanced soccer video\ncomprehension. Leveraging the extensive SoccerNet dataset, enriched with jersey\ncolor annotations and automatic speech recognition (ASR) transcripts,\nSoccerChat is fine-tuned on a structured video instruction dataset to\nfacilitate accurate game understanding, event classification, and referee\ndecision making. We benchmark SoccerChat on action classification and referee\ndecision-making tasks, demonstrating its performance in general soccer event\ncomprehension while maintaining competitive accuracy in referee decision\nmaking. Our findings highlight the importance of multimodal integration in\nadvancing soccer analytics, paving the way for more interactive and explainable\nAI-driven sports analysis. https://github.com/simula/SoccerChat", "AI": {"tldr": "SoccerChat\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5bf9\u8bddAI\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u63d0\u5347\u8db3\u7403\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u52a8\u4f5c\u5206\u7c7b\u548c\u88c1\u5224\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u8db3\u7403\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u5b64\u7acb\u7684\u6570\u636e\u6d41\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u6bd4\u8d5b\u52a8\u6001\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7efc\u5408\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528SoccerNet\u6570\u636e\u96c6\uff08\u5305\u542b\u7403\u8863\u989c\u8272\u6807\u6ce8\u548cASR\u8f6c\u5f55\u6587\u672c\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u9891\u6307\u4ee4\u6570\u636e\u96c6\u5fae\u8c03SoccerChat\uff0c\u5b9e\u73b0\u6bd4\u8d5b\u7406\u89e3\u3001\u4e8b\u4ef6\u5206\u7c7b\u548c\u88c1\u5224\u51b3\u7b56\u3002", "result": "SoccerChat\u5728\u52a8\u4f5c\u5206\u7c7b\u548c\u88c1\u5224\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u88c1\u5224\u51b3\u7b56\u7684\u7ade\u4e89\u6027\u51c6\u786e\u5ea6\u3002", "conclusion": "\u591a\u6a21\u6001\u6574\u5408\u5bf9\u63d0\u5347\u8db3\u7403\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u548c\u53ef\u89e3\u91ca\u7684AI\u9a71\u52a8\u4f53\u80b2\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2505.16661", "pdf": "https://arxiv.org/pdf/2505.16661", "abs": "https://arxiv.org/abs/2505.16661", "authors": ["Issey Sukeda", "Takuro Fujii", "Kosei Buma", "Shunsuke Sasaki", "Shinnosuke Ono"], "title": "A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP", "categories": ["cs.CL"], "comment": "15 pages, 9 tables, 5 figures", "summary": "We present a Japanese domain-specific language model for the pharmaceutical\nfield, developed through continual pretraining on 2 billion Japanese\npharmaceutical tokens and 8 billion English biomedical tokens. To enable\nrigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on\nnational pharmacist licensing exams; NayoseQA, which tests cross-lingual\nsynonym and terminology normalization; and SogoCheck, a novel task designed to\nassess consistency reasoning between paired statements. We evaluate our model\nagainst both open-source medical LLMs and commercial models, including GPT-4o.\nResults show that our domain-specific model outperforms existing open models\nand achieves competitive performance with commercial ones, particularly on\nterminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o\nperforms poorly on SogoCheck, suggesting that cross-sentence consistency\nreasoning remains an open challenge. Our benchmark suite offers a broader\ndiagnostic lens for pharmaceutical NLP, covering factual recall, lexical\nvariation, and logical consistency. This work demonstrates the feasibility of\nbuilding practical, secure, and cost-effective language models for Japanese\ndomain-specific applications, and provides reusable evaluation resources for\nfuture research in pharmaceutical and healthcare NLP. Our model, codes, and\ndatasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65e5\u672c\u5236\u836f\u9886\u57df\u7684\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u5f00\u53d1\uff0c\u5e76\u5728\u4e09\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u65e5\u672c\u5236\u836f\u9886\u57df\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u5728\u4e13\u4e1a\u672f\u8bed\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec32\u4ebf\u65e5\u8bed\u5236\u836f\u6807\u8bb0\u548c80\u4ebf\u82f1\u8bed\u751f\u7269\u533b\u5b66\u6807\u8bb0\uff0c\u6784\u5efa\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff08YakugakuQA\u3001NayoseQA\u3001SogoCheck\uff09\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u672f\u8bed\u5bc6\u96c6\u548c\u77e5\u8bc6\u578b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4e0e\u5546\u4e1a\u6a21\u578b\uff08\u5982GPT-4o\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u4f46GPT-4o\u5728SogoCheck\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u6784\u5efa\u5b9e\u7528\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u7684\u65e5\u672c\u7279\u5b9a\u9886\u57df\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5236\u836f\u548c\u533b\u7597NLP\u63d0\u4f9b\u4e86\u53ef\u590d\u7528\u7684\u8bc4\u4f30\u8d44\u6e90\u3002"}}
{"id": "2505.16633", "pdf": "https://arxiv.org/pdf/2505.16633", "abs": "https://arxiv.org/abs/2505.16633", "authors": ["Valentin Schmuker", "Alex Hoi Hang Chan", "Bastian Goldluecke", "Urs Waldmann"], "title": "Towards Texture- And Shape-Independent 3D Keypoint Estimation in Birds", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present a texture-independent approach to estimate and\ntrack 3D joint positions of multiple pigeons. For this purpose, we build upon\nthe existing 3D-MuPPET framework, which estimates and tracks the 3D poses of up\nto 10 pigeons using a multi-view camera setup. We extend this framework by\nusing a segmentation method that generates silhouettes of the individuals,\nwhich are then used to estimate 2D keypoints. Following 3D-MuPPET, these 2D\nkeypoints are triangulated to infer 3D poses, and identities are matched in the\nfirst frame and tracked in 2D across subsequent frames. Our proposed\ntexture-independent approach achieves comparable accuracy to the original\ntexture-dependent 3D-MuPPET framework. Additionally, we explore our approach's\napplicability to other bird species. To do that, we infer the 2D joint\npositions of four bird species without additional fine-tuning the model trained\non pigeons and obtain preliminary promising results. Thus, we think that our\napproach serves as a solid foundation and inspires the development of more\nrobust and accurate texture-independent pose estimation frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7eb9\u7406\u65e0\u5173\u7684\u65b9\u6cd5\u6765\u4f30\u8ba1\u548c\u8ddf\u8e2a\u591a\u53ea\u9e3d\u5b50\u76843D\u5173\u8282\u4f4d\u7f6e\uff0c\u6269\u5c55\u4e863D-MuPPET\u6846\u67b6\uff0c\u4f7f\u7528\u5206\u5272\u65b9\u6cd5\u751f\u6210\u4e2a\u4f53\u8f6e\u5ed3\u5e76\u4f30\u8ba12D\u5173\u952e\u70b9\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e0e\u539f\u59cb\u7eb9\u7406\u4f9d\u8d56\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5bf9\u7eb9\u7406\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u9e1f\u7c7b\u7269\u79cd\u4e0a\u7684\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e3D-MuPPET\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5272\u751f\u6210\u4e2a\u4f53\u8f6e\u5ed3\uff0c\u4f30\u8ba12D\u5173\u952e\u70b9\u5e76\u4e09\u89d2\u5316\u4e3a3D\u59ff\u6001\uff0c\u65e0\u9700\u7eb9\u7406\u4fe1\u606f\u3002", "result": "\u7eb9\u7406\u65e0\u5173\u65b9\u6cd5\u8fbe\u5230\u4e0e\u7eb9\u7406\u4f9d\u8d56\u65b9\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u5728\u5176\u4ed6\u9e1f\u7c7b\u7269\u79cd\u4e0a\u521d\u6b65\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u7684\u7eb9\u7406\u65e0\u5173\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.16694", "pdf": "https://arxiv.org/pdf/2505.16694", "abs": "https://arxiv.org/abs/2505.16694", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Shohei Taniguchi", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Transformer-based language models exhibit In-Context Learning (ICL), where\npredictions are made adaptively based on context. While prior work links\ninduction heads to ICL through a sudden jump in accuracy, this can only account\nfor ICL when the answer is included within the context. However, an important\nproperty of practical ICL in large language models is the ability to meta-learn\nhow to solve tasks from context, rather than just copying answers from context;\nhow such an ability is obtained during training is largely unexplored. In this\npaper, we experimentally clarify how such meta-learning ability is acquired by\nanalyzing the dynamics of the model's circuit during training. Specifically, we\nextend the copy task from previous research into an In-Context Meta Learning\nsetting, where models must infer a task from examples to answer queries.\nInterestingly, in this setting, we find that there are multiple phases in the\nprocess of acquiring such abilities, and that a unique circuit emerges in each\nphase, contrasting with the single-phases change in induction heads. The\nemergence of such circuits can be related to several phenomena known in large\nlanguage models, and our analysis lead to a deeper understanding of the source\nof the transformer's ICL ability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Transformer\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u8bad\u7ec3\u83b7\u5f97\u5143\u5b66\u4e60\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u4e0a\u4e0b\u6587\u4efb\u52a1\uff0c\u800c\u975e\u7b80\u5355\u590d\u5236\u7b54\u6848\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u4ece\u4e0a\u4e0b\u6587\u4e2d\u5143\u5b66\u4e60\u4efb\u52a1\u89e3\u51b3\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u590d\u5236\u7b54\u6848\uff0c\u8fd9\u4e00\u80fd\u529b\u5728\u8bad\u7ec3\u4e2d\u7684\u5f62\u6210\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u590d\u5236\u4efb\u52a1\u4e3a\u4e0a\u4e0b\u6587\u5143\u5b66\u4e60\u573a\u666f\uff0c\u5206\u6790\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7535\u8def\u52a8\u6001\u53d8\u5316\u3002", "result": "\u53d1\u73b0\u5143\u5b66\u4e60\u80fd\u529b\u83b7\u53d6\u5206\u4e3a\u591a\u9636\u6bb5\uff0c\u6bcf\u9636\u6bb5\u6709\u72ec\u7279\u7535\u8def\u5f62\u6210\uff0c\u4e0e\u5355\u9636\u6bb5\u53d8\u5316\u7684\u5f52\u7eb3\u5934\u4e0d\u540c\u3002", "conclusion": "\u63ed\u793a\u4e86Transformer\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u6765\u6e90\uff0c\u6df1\u5316\u4e86\u5bf9\u6a21\u578b\u52a8\u6001\u7684\u7406\u89e3\u3002"}}
{"id": "2505.16643", "pdf": "https://arxiv.org/pdf/2505.16643", "abs": "https://arxiv.org/abs/2505.16643", "authors": ["Yiwei Sun", "Peiqi Jiang", "Chuanbin Liu", "Luohao Lin", "Zhiying Lu", "Hongtao Xie"], "title": "From Evaluation to Defense: Advancing Safety in Video Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "49 pages, 12 figures, 17 tables", "summary": "While the safety risks of image-based large language models have been\nextensively studied, their video-based counterparts (Video LLMs) remain\ncritically under-examined. To systematically study this problem, we introduce\n\\textbf{VideoSafetyBench (VSB-77k) - the first large-scale, culturally diverse\nbenchmark for Video LLM safety}, which compromises 77,646 video-query pairs and\nspans 19 principal risk categories across 10 language communities. \\textit{We\nreveal that integrating video modality degrades safety performance by an\naverage of 42.3\\%, exposing systemic risks in multimodal attack exploitation.}\nTo address this vulnerability, we propose \\textbf{VideoSafety-R1}, a dual-stage\nframework achieving unprecedented safety gains through two innovations: (1)\nAlarm Token-Guided Safety Fine-Tuning (AT-SFT) injects learnable alarm tokens\ninto visual and textual sequences, enabling explicit harm perception across\nmodalities via multitask objectives. (2) Then, Safety-Guided GRPO enhances\ndefensive reasoning through dynamic policy optimization with rule-based rewards\nderived from dual-modality verification. These components synergize to shift\nsafety alignment from passive harm recognition to active reasoning. The\nresulting framework achieves a 65.1\\% improvement on VSB-Eval-HH, and improves\nby 59.1\\%, 44.3\\%, and 15.0\\% on the image safety datasets MMBench, VLGuard,\nand FigStep, respectively. \\textit{Our codes are available in the supplementary\nmaterials.} \\textcolor{red}{Warning: This paper contains examples of harmful\nlanguage and videos, and reader discretion is recommended.}", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u6587\u5316\u591a\u6837\u7684\u89c6\u9891LLM\u5b89\u5168\u57fa\u51c6VideoSafetyBench\uff08VSB-77k\uff09\uff0c\u5e76\u63ed\u793a\u89c6\u9891\u6a21\u6001\u4f1a\u964d\u4f4e\u5b89\u5168\u6027\u80fd42.3%\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u53cc\u9636\u6bb5\u6846\u67b6VideoSafety-R1\uff0c\u901a\u8fc7\u4e24\u79cd\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002", "motivation": "\u89c6\u9891LLM\u7684\u5b89\u5168\u98ce\u9669\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5e76\u89e3\u51b3\u89c6\u9891\u6a21\u6001\u5e26\u6765\u7684\u7cfb\u7edf\u6027\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "1. \u63d0\u51faVideoSafetyBench\uff08VSB-77k\uff09\u57fa\u51c6\uff1b2. \u5f00\u53d1\u53cc\u9636\u6bb5\u6846\u67b6VideoSafety-R1\uff0c\u5305\u62ecAlarm Token-Guided Safety Fine-Tuning\uff08AT-SFT\uff09\u548cSafety-Guided GRPO\u3002", "result": "VideoSafety-R1\u5728VSB-Eval-HH\u4e0a\u63d0\u5347\u4e8665.1%\uff0c\u5728\u5176\u4ed6\u56fe\u50cf\u5b89\u5168\u6570\u636e\u96c6\u4e0a\u4e5f\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u89c6\u9891\u6a21\u6001\u5bf9LLM\u5b89\u5168\u6027\u80fd\u6709\u663e\u8457\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2505.16703", "pdf": "https://arxiv.org/pdf/2505.16703", "abs": "https://arxiv.org/abs/2505.16703", "authors": ["Zeping Yu", "Sophia Ananiadou"], "title": "Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Although multimodal large language models (MLLMs) have achieved impressive\nperformance, the multimodal instruction tuning stage often causes catastrophic\nforgetting of the base LLM's language ability, even in strong models like\nLlama3. To address this, we propose Locate-then-Merge, a training-free\nparameter fusion framework that first locates important parameters and then\nselectively merges them. We further introduce Neuron-Fusion, a neuron-level\nstrategy that preserves the influence of neurons with large parameter\nshifts--neurons likely responsible for newly acquired visual\ncapabilities--while attenuating the influence of neurons with smaller changes\nthat likely encode general-purpose language skills. This design enables better\nretention of visual adaptation while mitigating language degradation.\nExperiments on 13 benchmarks across both language and visual tasks show that\nNeuron-Fusion consistently outperforms existing model merging methods. Further\nanalysis reveals that our method effectively reduces context hallucination in\ngeneration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLocate-then-Merge\u7684\u65e0\u8bad\u7ec3\u53c2\u6570\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4f4d\u91cd\u8981\u53c2\u6570\u5e76\u9009\u62e9\u6027\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\u5bf9\u8bed\u8a00\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\u5f80\u5f80\u4f1a\u5bfc\u81f4\u57fa\u7840LLM\u8bed\u8a00\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5373\u4f7f\u662f\u50cfLlama3\u8fd9\u6837\u7684\u5f3a\u6a21\u578b\u4e5f\u4e0d\u4f8b\u5916\u3002", "method": "\u63d0\u51fa\u4e86Locate-then-Merge\u6846\u67b6\uff0c\u5305\u62ec\u5b9a\u4f4d\u91cd\u8981\u53c2\u6570\u548c\u9009\u62e9\u6027\u878d\u5408\uff1b\u8fdb\u4e00\u6b65\u5f15\u5165Neuron-Fusion\u7b56\u7565\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u64cd\u4f5c\u4fdd\u7559\u89c6\u89c9\u80fd\u529b\u76f8\u5173\u7684\u53c2\u6570\uff0c\u540c\u65f6\u51cf\u5f31\u5bf9\u901a\u7528\u8bed\u8a00\u6280\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u572813\u4e2a\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeuron-Fusion\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u751f\u6210\u4e2d\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u89c6\u89c9\u9002\u5e94\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u8bed\u8a00\u80fd\u529b\u7684\u9000\u5316\uff0c\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16647", "pdf": "https://arxiv.org/pdf/2505.16647", "abs": "https://arxiv.org/abs/2505.16647", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.4.8"], "comment": "Accepted as a full paper at the 38th IEEE International Symposium on\n  Computer-Based Medical Systems (CBMS) 2025", "summary": "We investigate fine-tuning Vision-Language Models (VLMs) for multi-task\nmedical image understanding, focusing on detection, localization, and counting\nof findings in medical images. Our objective is to evaluate whether\ninstruction-tuned VLMs can simultaneously improve these tasks, with the goal of\nenhancing diagnostic accuracy and efficiency. Using MedMultiPoints, a\nmultimodal dataset with annotations from endoscopy (polyps and instruments) and\nmicroscopy (sperm cells), we reformulate each task into instruction-based\nprompts suitable for vision-language reasoning. We fine-tune\nQwen2.5-VL-7B-Instruct using Low-Rank Adaptation (LoRA) across multiple task\ncombinations. Results show that multi-task training improves robustness and\naccuracy. For example, it reduces the Count Mean Absolute Error (MAE) and\nincreases Matching Accuracy in the Counting + Pointing task. However,\ntrade-offs emerge, such as more zero-case point predictions, indicating reduced\nreliability in edge cases despite overall performance gains. Our study\nhighlights the potential of adapting general-purpose VLMs to specialized\nmedical tasks via prompt-driven fine-tuning. This approach mirrors clinical\nworkflows, where radiologists simultaneously localize, count, and describe\nfindings - demonstrating how VLMs can learn composite diagnostic reasoning\npatterns. The model produces interpretable, structured outputs, offering a\npromising step toward explainable and versatile medical AI. Code, model\nweights, and scripts will be released for reproducibility at\nhttps://github.com/simula/PointDetectCount.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u591a\u4efb\u52a1\u533b\u5b66\u56fe\u50cf\u7406\u89e3\uff0c\u5305\u62ec\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u8ba1\u6570\uff0c\u4f7f\u7528MedMultiPoints\u6570\u636e\u96c6\u548cLoRA\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u591a\u4efb\u52a1\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4e5f\u5b58\u5728\u8fb9\u7f18\u6848\u4f8b\u53ef\u9760\u6027\u964d\u4f4e\u7684\u6743\u8861\u3002", "motivation": "\u8bc4\u4f30\u6307\u4ee4\u8c03\u4f18\u7684VLM\u662f\u5426\u80fd\u540c\u65f6\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u7684\u591a\u4efb\u52a1\u7406\u89e3\uff0c\u4ee5\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528MedMultiPoints\u6570\u636e\u96c6\uff0c\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u6307\u4ee4\u63d0\u793a\uff0c\u901a\u8fc7LoRA\u65b9\u6cd5\u5fae\u8c03Qwen2.5-VL-7B-Instruct\u6a21\u578b\u3002", "result": "\u591a\u4efb\u52a1\u8bad\u7ec3\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u5982\u51cf\u5c11\u8ba1\u6570MAE\u5e76\u63d0\u5347\u5339\u914d\u51c6\u786e\u7387\uff0c\u4f46\u8fb9\u7f18\u6848\u4f8b\u7684\u53ef\u9760\u6027\u964d\u4f4e\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u901a\u7528VLM\u53ef\u901a\u8fc7\u63d0\u793a\u9a71\u52a8\u7684\u5fae\u8c03\u9002\u5e94\u533b\u5b66\u4efb\u52a1\uff0c\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u533b\u5b66AI\u63d0\u4f9b\u53ef\u80fd\u3002"}}
{"id": "2505.16722", "pdf": "https://arxiv.org/pdf/2505.16722", "abs": "https://arxiv.org/abs/2505.16722", "authors": ["Himanshu Beniwal", "Youngwoo Kim", "Maarten Sap", "Soham Dan", "Thomas Hartvigsen"], "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 504 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8de8\u8bed\u8a00\u53bb\u6bd2\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7504\u79cd\u8bbe\u7f6e\u8bc4\u4f30\u5176\u5728\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u6bd2\u6027\u51cf\u5c11\u6548\u679c\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u975e\u6bd2\u6027\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u65e0\u6bd2\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8de8\u8bed\u8a00\u53bb\u6bd2\u5316\u8303\u5f0f\uff0c\u8bc4\u4f30\u5176\u5728\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u7684\u6bd2\u6027\u51cf\u5c11\u6548\u679c\uff0c\u5e76\u5206\u6790\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u5b89\u5168\u6027\u4e0e\u77e5\u8bc6\u4fdd\u7559\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002", "conclusion": "\u8de8\u8bed\u8a00\u53bb\u6bd2\u5316\u5728\u51cf\u5c11\u6bd2\u6027\u65b9\u9762\u6709\u6548\uff0c\u4f46\u9700\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2505.16650", "pdf": "https://arxiv.org/pdf/2505.16650", "abs": "https://arxiv.org/abs/2505.16650", "authors": ["Michael Neri", "Sara Baldoni"], "title": "Unsupervised Network Anomaly Detection with Autoencoders and Traffic Images", "categories": ["cs.CV", "cs.CR", "eess.IV", "eess.SP"], "comment": "Accepted for publication in EUSIPCO 2025", "summary": "Due to the recent increase in the number of connected devices, the need to\npromptly detect security issues is emerging. Moreover, the high number of\ncommunication flows creates the necessity of processing huge amounts of data.\nFurthermore, the connected devices are heterogeneous in nature, having\ndifferent computational capacities. For this reason, in this work we propose an\nimage-based representation of network traffic which allows to realize a compact\nsummary of the current network conditions with 1-second time windows. The\nproposed representation highlights the presence of anomalies thus reducing the\nneed for complex processing architectures. Finally, we present an unsupervised\nlearning approach which effectively detects the presence of anomalies. The code\nand the dataset are available at\nhttps://github.com/michaelneri/image-based-network-traffic-anomaly-detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6d41\u91cf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5feb\u901f\u68c0\u6d4b\u5b89\u5168\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u968f\u7740\u8fde\u63a5\u8bbe\u5907\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5feb\u901f\u68c0\u6d4b\u5b89\u5168\u95ee\u9898\u548c\u5904\u7406\u5927\u91cf\u6570\u636e\u7684\u9700\u6c42\u65e5\u76ca\u7a81\u51fa\u3002\u8bbe\u5907\u5f02\u6784\u6027\u4e5f\u589e\u52a0\u4e86\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u7f51\u7edc\u6d41\u91cf\u8868\u793a\u65b9\u6cd5\uff0c\u4ee51\u79d2\u65f6\u95f4\u7a97\u53e3\u751f\u6210\u7f51\u7edc\u72b6\u6001\u7684\u7d27\u51d1\u6458\u8981\uff0c\u51cf\u5c11\u590d\u6742\u5904\u7406\u67b6\u6784\u7684\u9700\u6c42\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u7a81\u51fa\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u6210\u529f\u68c0\u6d4b\u5f02\u5e38\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16743", "pdf": "https://arxiv.org/pdf/2505.16743", "abs": "https://arxiv.org/abs/2505.16743", "authors": ["Florentin Beck", "William Rudman", "Carsten Eickhoff"], "title": "TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; F.2.2"], "comment": null, "summary": "Large Language Models (LLMs) present significant computational and memory\nchallenges due to their extensive size, making pruning essential for their\nefficient deployment. Existing one-shot pruning methods often apply uniform\nsparsity constraints across layers or within each layer, resulting in\nsuboptimal performance, especially at high sparsity ratios. This work\nintroduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel\napproach that applies varying sparsity ratios to individual output dimensions\n(rows) within each layer. TRIM employs an iterative adjustment process guided\nby quality metrics to optimize dimension-wise sparsity allocation, focusing on\nreducing variance in quality retention across outputs to preserve critical\ninformation. TRIM can be seamlessly integrated with existing layer-wise pruning\nstrategies. Our evaluations on perplexity and zero-shot tasks across diverse\nLLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that\nTRIM achieves new state-of-the-art results and enhances stability. For\ninstance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and\nover 90% for OPT-13B compared to baseline methods. We conclude that\nfine-grained, dimension-wise sparsity adaptation is crucial for pushing the\nlimits of extreme LLM compression. Code available at:\nhttps://github.com/flobk/TRIM", "AI": {"tldr": "TRIM\u662f\u4e00\u79cd\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b0\u578b\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ef4\u5ea6\u7ea7\u522b\u7684\u7a00\u758f\u5316\u5206\u914d\u4f18\u5316\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u7a00\u758f\u7387\u4e0b\u7684\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u5200\u5207\u526a\u679d\u65b9\u6cd5\u5728\u9ad8\u7a00\u758f\u7387\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u526a\u679d\u7b56\u7565\u4ee5\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002", "method": "TRIM\u91c7\u7528\u8fed\u4ee3\u8c03\u6574\u8fc7\u7a0b\uff0c\u6839\u636e\u8d28\u91cf\u6307\u6807\u4e3a\u6bcf\u4e2a\u8f93\u51fa\u7ef4\u5ea6\uff08\u884c\uff09\u5206\u914d\u4e0d\u540c\u7684\u7a00\u758f\u7387\uff0c\u51cf\u5c11\u8d28\u91cf\u4fdd\u7559\u7684\u65b9\u5dee\u3002", "result": "\u5728\u591a\u79cdLLM\uff08\u5982Qwen2.5\u3001LLaMA-2\u548cOPT\uff09\u4e0a\uff0cTRIM\u572880%\u7a00\u758f\u7387\u4e0b\u5c06\u56f0\u60d1\u5ea6\u964d\u4f4e48%\u81f390%\u4ee5\u4e0a\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u3002", "conclusion": "\u7ef4\u5ea6\u7ea7\u522b\u7684\u7a00\u758f\u5316\u9002\u914d\u662f\u6781\u7aefLLM\u538b\u7f29\u7684\u5173\u952e\u3002"}}
{"id": "2505.16652", "pdf": "https://arxiv.org/pdf/2505.16652", "abs": "https://arxiv.org/abs/2505.16652", "authors": ["Feilong Tang", "Chengzhi Liu", "Zhongxing Xu", "Ming Hu", "Zelin Peng", "Zhiwei Yang", "Jionglong Su", "Minquan Lin", "Yifan Peng", "Xuelian Cheng", "Imran Razzak", "Zongyuan Ge"], "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding", "categories": ["cs.CV", "cs.LG"], "comment": "Clarification note for the CVPR 2025 paper (FarSight). Prepared by a\n  subset of the original authors; remaining co-authors are acknowledged in the\n  text", "summary": "Recent advancements in multimodal large language models (MLLMs) have\nsignificantly improved performance in visual question answering. However, they\noften suffer from hallucinations. In this work, hallucinations are categorized\ninto two main types: initial hallucinations and snowball hallucinations. We\nargue that adequate contextual information can be extracted directly from the\ntoken interaction process. Inspired by causal inference in the decoding\nstrategy, we propose to leverage causal masks to establish information\npropagation between multimodal tokens. The hypothesis is that insufficient\ninteraction between those tokens may lead the model to rely on outlier tokens,\noverlooking dense and rich contextual cues. Therefore, we propose to intervene\nin the propagation process by tackling outlier tokens to enhance in-context\ninference. With this goal, we present FarSight, a versatile plug-and-play\ndecoding strategy to reduce attention interference from outlier tokens merely\nby optimizing the causal mask. The heart of our method is effective token\npropagation. We design an attention register structure within the upper\ntriangular matrix of the causal mask, dynamically allocating attention to\ncapture attention diverted to outlier tokens. Moreover, a positional awareness\nencoding method with a diminishing masking rate is proposed, allowing the model\nto attend to further preceding tokens, especially for video sequence tasks.\nWith extensive experiments, FarSight demonstrates significant\nhallucination-mitigating performance across different MLLMs on both image and\nvideo benchmarks, proving its effectiveness.", "AI": {"tldr": "FarSight\u662f\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u56e0\u679c\u63a9\u7801\u51cf\u5c11\u5e7b\u89c9\u7684\u89e3\u7801\u7b56\u7565\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5bc4\u5b58\u5668\u7ed3\u6784\u548c\u4f4d\u7f6e\u611f\u77e5\u7f16\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5206\u4e3a\u521d\u59cb\u5e7b\u89c9\u548c\u96ea\u7403\u5e7b\u89c9\u4e24\u7c7b\uff0c\u9700\u8981\u6539\u8fdb\u89e3\u7801\u7b56\u7565\u4ee5\u51cf\u5c11\u5e72\u6270\u3002", "method": "\u63d0\u51faFarSight\u7b56\u7565\uff0c\u901a\u8fc7\u56e0\u679c\u63a9\u7801\u4f18\u5316\u548c\u6ce8\u610f\u529b\u5bc4\u5b58\u5668\u7ed3\u6784\u52a8\u6001\u5206\u914d\u6ce8\u610f\u529b\uff0c\u5e76\u7ed3\u5408\u4f4d\u7f6e\u611f\u77e5\u7f16\u7801\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFarSight\u5728\u56fe\u50cf\u548c\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "FarSight\u662f\u4e00\u79cd\u6709\u6548\u7684\u5373\u63d2\u5373\u7528\u89e3\u7801\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2505.16774", "pdf": "https://arxiv.org/pdf/2505.16774", "abs": "https://arxiv.org/abs/2505.16774", "authors": ["Yiming Gao", "Bin Wang", "Chengwei Wei", "Shuo Sun", "AiTi Aw"], "title": "IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models", "categories": ["cs.CL"], "comment": "Link: https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio", "summary": "Large language models (LLMs) have demonstrated strong instruction-following\ncapabilities in text-based tasks. However, this ability often deteriorates in\nmultimodal models after alignment with non-text modalities such as images or\naudio. While several recent efforts have investigated instruction-following\nperformance in text and vision-language models, instruction-following in\naudio-based large language models remains largely unexplored. To bridge this\ngap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess\nthe ability to follow instructions in an audio LLM. IFEval-Audio contains 280\naudio-instruction-answer triples across six diverse dimensions: Content,\nCapitalization, Symbol, List Structure, Length, and Format. Each example pairs\nan audio input with a text instruction, requiring the model to generate an\noutput that follows a specified structure. We benchmark state-of-the-art audio\nLLMs on their ability to follow audio-involved instructions. The dataset is\nreleased publicly to support future research in this emerging area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86IFEval-Audio\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9075\u5faa\u6307\u4ee4\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\uff08\u5982\u97f3\u9891\uff09\u8fd9\u4e00\u80fd\u529b\u4f1a\u4e0b\u964d\uff0c\u800c\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86IFEval-Audio\u6570\u636e\u96c6\uff0c\u5305\u542b280\u4e2a\u97f3\u9891-\u6307\u4ee4-\u7b54\u6848\u4e09\u5143\u7ec4\uff0c\u8986\u76d6\u516d\u4e2a\u7ef4\u5ea6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5bf9\u97f3\u9891\u76f8\u5173\u6307\u4ee4\u7684\u9075\u5faa\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u8be5\u6570\u636e\u96c6\uff0c\u4f5c\u8005\u5bf9\u73b0\u6709\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5176\u5728\u97f3\u9891\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "IFEval-Audio\u7684\u53d1\u5e03\u4e3a\u672a\u6765\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2505.16658", "pdf": "https://arxiv.org/pdf/2505.16658", "abs": "https://arxiv.org/abs/2505.16658", "authors": ["Giuseppe Guarino", "Matteo Ciotola", "Gemine Vivone", "Giovanni Poggi", "Giuseppe Scarpa"], "title": "Zero-Shot Hyperspectral Pansharpening Using Hysteresis-Based Tuning for Spectral Quality Control", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Hyperspectral pansharpening has received much attention in recent years due\nto technological and methodological advances that open the door to new\napplication scenarios. However, research on this topic is only now gaining\nmomentum. The most popular methods are still borrowed from the more mature\nfield of multispectral pansharpening and often overlook the unique challenges\nposed by hyperspectral data fusion, such as i) the very large number of bands,\nii) the overwhelming noise in selected spectral ranges, iii) the significant\nspectral mismatch between panchromatic and hyperspectral components, iv) a\ntypically high resolution ratio. Imprecise data modeling especially affects\nspectral fidelity. Even state-of-the-art methods perform well in certain\nspectral ranges and much worse in others, failing to ensure consistent quality\nacross all bands, with the risk of generating unreliable results. Here, we\npropose a hyperspectral pansharpening method that explicitly addresses this\nproblem and ensures uniform spectral quality. To this end, a single lightweight\nneural network is used, with weights that adapt on the fly to each band. During\nfine-tuning, the spatial loss is turned on and off to ensure a fast convergence\nof the spectral loss to the desired level, according to a hysteresis-like\ndynamic. Furthermore, the spatial loss itself is appropriately redefined to\naccount for nonlinear dependencies between panchromatic and spectral bands.\nOverall, the proposed method is fully unsupervised, with no prior training on\nexternal data, flexible, and low-complexity. Experiments on a recently\npublished benchmarking toolbox show that it ensures excellent sharpening\nquality, competitive with the state-of-the-art, consistently across all bands.\nThe software code and the full set of results are shared online on\nhttps://github.com/giu-guarino/rho-PNN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff0c\u786e\u4fdd\u6240\u6709\u6ce2\u6bb5\u7684\u5747\u5300\u5149\u8c31\u8d28\u91cf\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u8bad\u7ec3\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u6ce2\u6bb5\u6570\u91cf\u591a\u3001\u566a\u58f0\u5927\u3001\u5149\u8c31\u4e0d\u5339\u914d\u7b49\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u6240\u6709\u6ce2\u6bb5\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u6743\u91cd\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\uff0c\u52a8\u6001\u8c03\u6574\u7a7a\u95f4\u635f\u5931\uff0c\u91cd\u65b0\u5b9a\u4e49\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6ce2\u6bb5\u5747\u80fd\u4fdd\u6301\u4f18\u5f02\u7684\u9510\u5316\u8d28\u91cf\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u7ade\u4e89\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7075\u6d3b\u3001\u4f4e\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\uff0c\u4ee3\u7801\u548c\u7ed3\u679c\u5df2\u516c\u5f00\u3002"}}
{"id": "2505.16782", "pdf": "https://arxiv.org/pdf/2505.16782", "abs": "https://arxiv.org/abs/2505.16782", "authors": ["Xinghao Chen", "Anhao Zhao", "Heming Xia", "Xuan Lu", "Hanlin Wang", "Yanjun Chen", "Wei Zhang", "Jian Wang", "Wenjie Li", "Xiaoyu Shen"], "title": "Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive performance on complex\nreasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional\nCoT relies on reasoning steps explicitly verbalized in natural language,\nintroducing inefficiencies and limiting its applicability to abstract\nreasoning. To address this, there has been growing research interest in latent\nCoT reasoning, where inference occurs within latent spaces. By decoupling\nreasoning from language, latent reasoning promises richer cognitive\nrepresentations and more flexible, faster inference. Researchers have explored\nvarious directions in this promising field, including training methodologies,\nstructural innovations, and internal reasoning mechanisms. This paper presents\na comprehensive overview and analysis of this reasoning paradigm. We begin by\nproposing a unified taxonomy from four perspectives: token-wise strategies,\ninternal mechanisms, analysis, and applications. We then provide in-depth\ndiscussions and comparative analyses of representative methods, highlighting\ntheir design patterns, strengths, and open challenges. We aim to provide a\nstructured foundation for advancing this emerging direction in LLM reasoning.\nThe relevant papers will be regularly updated at\nhttps://github.com/EIT-NLP/Awesome-Latent-CoT.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6f5c\u5728\u94fe\u5f0f\u601d\u7ef4\uff08Latent CoT\uff09\u63a8\u7406\u8303\u5f0f\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u65e8\u5728\u4e3aLLM\u63a8\u7406\u9886\u57df\u63d0\u4f9b\u7ed3\u6784\u5316\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u4f9d\u8d56\u663e\u5f0f\u8bed\u8a00\u8868\u8fbe\u63a8\u7406\u6b65\u9aa4\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u5e94\u7528\u4e8e\u62bd\u8c61\u63a8\u7406\u3002\u6f5c\u5728CoT\u901a\u8fc7\u89e3\u8026\u63a8\u7406\u4e0e\u8bed\u8a00\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u8ba4\u77e5\u8868\u5f81\u548c\u66f4\u7075\u6d3b\u7684\u63a8\u7406\u3002", "method": "\u4ece\u56db\u4e2a\u89d2\u5ea6\u63d0\u51fa\u7edf\u4e00\u5206\u7c7b\u6cd5\uff1atoken-wise\u7b56\u7565\u3001\u5185\u90e8\u673a\u5236\u3001\u5206\u6790\u548c\u5e94\u7528\u3002\u6df1\u5165\u8ba8\u8bba\u548c\u6bd4\u8f83\u4ee3\u8868\u6027\u65b9\u6cd5\u7684\u8bbe\u8ba1\u6a21\u5f0f\u3001\u4f18\u52bf\u548c\u6311\u6218\u3002", "result": "\u7efc\u8ff0\u4e86\u6f5c\u5728CoT\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u4e3aLLM\u63a8\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u5206\u6790\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u6f5c\u5728CoT\u662f\u4e00\u4e2a\u65b0\u5174\u4e14\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\uff0c\u672c\u6587\u4e3a\u5176\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u57fa\u7840\u548c\u8d44\u6e90\u3002"}}
{"id": "2505.16659", "pdf": "https://arxiv.org/pdf/2505.16659", "abs": "https://arxiv.org/abs/2505.16659", "authors": ["Kaiyu Guo", "Tan Pan", "Chen Jiang", "Zijian Wang", "Brian C. Lovell", "Limei Han", "Yuan Cheng", "Mahsa Baktashmotlagh"], "title": "SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images", "categories": ["cs.CV"], "comment": null, "summary": "Medical anomaly detection (AD) is crucial for early clinical intervention,\nyet it faces challenges due to limited access to high-quality medical imaging\ndata, caused by privacy concerns and data silos. Few-shot learning has emerged\nas a promising approach to alleviate these limitations by leveraging the\nlarge-scale prior knowledge embedded in vision-language models (VLMs). Recent\nadvancements in few-shot medical AD have treated normal and abnormal cases as a\none-class classification problem, often overlooking the distinction among\nmultiple anomaly categories. Thus, in this paper, we propose a framework\ntailored for few-shot medical anomaly detection in the scenario where the\nidentification of multiple anomaly categories is required. To capture the\ndetailed radiological signs of medical anomaly categories, our framework\nincorporates diverse textual descriptions for each category generated by a\nLarge-Language model, under the assumption that different anomalies in medical\nimages may share common radiological signs in each category. Specifically, we\nintroduce SD-MAD, a two-stage Sign-Driven few-shot Multi-Anomaly Detection\nframework: (i) Radiological signs are aligned with anomaly categories by\namplifying inter-anomaly discrepancy; (ii) Aligned signs are selected further\nto mitigate the effect of the under-fitting and uncertain-sample issue caused\nby limited medical data, employing an automatic sign selection strategy at\ninference. Moreover, we propose three protocols to comprehensively quantify the\nperformance of multi-anomaly detection. Extensive experiments illustrate the\neffectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5c11\u6837\u672c\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u6846\u67b6SD-MAD\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u63cf\u8ff0\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u5f02\u5e38\u7c7b\u522b\u8bc6\u522b\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u9690\u79c1\u548c\u6709\u9650\u6837\u672c\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u591a\u5f02\u5e38\u7c7b\u522b\u7684\u533a\u5206\u3002", "method": "SD-MAD\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a1\uff09\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u5bf9\u9f50\u653e\u5c04\u5b66\u7279\u5f81\u4e0e\u5f02\u5e38\u7c7b\u522b\uff1b2\uff09\u81ea\u52a8\u9009\u62e9\u7279\u5f81\u4ee5\u51cf\u5c11\u6570\u636e\u4e0d\u8db3\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6709\u6548\u3002", "conclusion": "SD-MAD\u4e3a\u5c11\u6837\u672c\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u591a\u7c7b\u522b\u8bc6\u522b\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2505.16789", "pdf": "https://arxiv.org/pdf/2505.16789", "abs": "https://arxiv.org/abs/2505.16789", "authors": ["Punya Syon Pandey", "Samuel Simko", "Kellin Pelrine", "Zhijing Jin"], "title": "Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models gain popularity, their vulnerability to adversarial\nattacks remains a primary concern. While fine-tuning models on domain-specific\ndatasets is often employed to improve model performance, it can introduce\nvulnerabilities within the underlying model. In this work, we investigate\nAccidental Misalignment, unexpected vulnerabilities arising from\ncharacteristics of fine-tuning data. We begin by identifying potential\ncorrelation factors such as linguistic features, semantic similarity, and\ntoxicity within our experimental datasets. We then evaluate the adversarial\nperformance of these fine-tuned models and assess how dataset factors correlate\nwith attack success rates. Lastly, we explore potential causal links, offering\nnew insights into adversarial defense strategies and highlighting the crucial\nrole of dataset design in preserving model alignment. Our code is available at\nhttps://github.com/psyonp/accidental_misalignment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u56e0\u6570\u636e\u7279\u6027\u5bfc\u81f4\u7684\u610f\u5916\u8106\u5f31\u6027\uff08Accidental Misalignment\uff09\uff0c\u5206\u6790\u4e86\u6570\u636e\u96c6\u56e0\u7d20\u4e0e\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65f6\u53ef\u80fd\u56e0\u6570\u636e\u96c6\u7279\u6027\u5f15\u5165\u8106\u5f31\u6027\uff0c\u9700\u7814\u7a76\u5176\u6210\u56e0\u53ca\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u8bc6\u522b\u5fae\u8c03\u6570\u636e\u4e2d\u7684\u76f8\u5173\u6027\u56e0\u7d20\uff08\u5982\u8bed\u8a00\u7279\u5f81\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u6bd2\u6027\uff09\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u6297\u6027\u80fd\uff0c\u5206\u6790\u6570\u636e\u56e0\u7d20\u4e0e\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u7cfb\u3002", "result": "\u63ed\u793a\u4e86\u5fae\u8c03\u6570\u636e\u7279\u6027\u4e0e\u6a21\u578b\u8106\u5f31\u6027\u7684\u5173\u8054\uff0c\u4e3a\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u6570\u636e\u96c6\u8bbe\u8ba1\u5bf9\u4fdd\u6301\u6a21\u578b\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u9700\u5173\u6ce8\u6570\u636e\u7279\u6027\u4ee5\u907f\u514d\u610f\u5916\u8106\u5f31\u6027\u3002"}}
{"id": "2505.16663", "pdf": "https://arxiv.org/pdf/2505.16663", "abs": "https://arxiv.org/abs/2505.16663", "authors": ["Haihong Hao", "Mingfei Han", "Changlin Li", "Zhihui Li", "Xiaojun Chang"], "title": "CoNav: Collaborative Cross-Modal Reasoning for Embodied Navigation", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Embodied navigation demands comprehensive scene understanding and precise\nspatial reasoning. While image-text models excel at interpreting pixel-level\ncolor and lighting cues, 3D-text models capture volumetric structure and\nspatial relationships. However, unified fusion approaches that jointly fuse 2D\nimages, 3D point clouds, and textual instructions face challenges in limited\navailability of triple-modality data and difficulty resolving conflicting\nbeliefs among modalities. In this work, we introduce CoNav, a collaborative\ncross-modal reasoning framework where a pretrained 3D-text model explicitly\nguides an image-text navigation agent by providing structured spatial-semantic\nknowledge to resolve ambiguities during navigation. Specifically, we introduce\nCross-Modal Belief Alignment, which operationalizes this cross-modal guidance\nby simply sharing textual hypotheses from the 3D-text model to the navigation\nagent. Through lightweight fine-tuning on a small 2D-3D-text corpus, the\nnavigation agent learns to integrate visual cues with spatial-semantic\nknowledge derived from the 3D-text model, enabling effective reasoning in\nembodied navigation. CoNav achieves significant improvements on four standard\nembodied navigation benchmarks (R2R, CVDN, REVERIE, SOON) and two spatial\nreasoning benchmarks (ScanQA, SQA3D). Moreover, under close navigation Success\nRate, CoNav often generates shorter paths compared to other methods (as\nmeasured by SPL), showcasing the potential and challenges of fusing data from\ndifferent modalities in embodied navigation. Project Page:\nhttps://oceanhao.github.io/CoNav/", "AI": {"tldr": "CoNav\u662f\u4e00\u4e2a\u534f\u4f5c\u8de8\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc73D-text\u6a21\u578b\u6307\u5bfc\u56fe\u50cf-\u6587\u672c\u5bfc\u822a\u4ee3\u7406\uff0c\u89e3\u51b3\u5bfc\u822a\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u5bfc\u822a\u548c\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b32D\u56fe\u50cf\u30013D\u70b9\u4e91\u548c\u6587\u672c\u6307\u4ee4\u7edf\u4e00\u878d\u5408\u4e2d\u7684\u6311\u6218\uff0c\u5982\u591a\u6a21\u6001\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u6001\u95f4\u4fe1\u5ff5\u51b2\u7a81\u3002", "method": "\u5f15\u5165\u8de8\u6a21\u6001\u4fe1\u5ff5\u5bf9\u9f50\uff0c\u5171\u4eab3D-text\u6a21\u578b\u7684\u6587\u672c\u5047\u8bbe\u7ed9\u5bfc\u822a\u4ee3\u7406\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5fae\u8c03\u6574\u5408\u89c6\u89c9\u4e0e\u7a7a\u95f4\u8bed\u4e49\u77e5\u8bc6\u3002", "result": "\u5728\u56db\u4e2a\u5bfc\u822a\u57fa\u51c6\u548c\u4e24\u4e2a\u7a7a\u95f4\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u8def\u5f84\u66f4\u77ed\uff08SPL\u6307\u6807\uff09\u3002", "conclusion": "CoNav\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u5728\u5bfc\u822a\u4e2d\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2505.16800", "pdf": "https://arxiv.org/pdf/2505.16800", "abs": "https://arxiv.org/abs/2505.16800", "authors": ["Changbing Yang", "Garrett Nicolai"], "title": "Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a transformer-based morpheme segmentation system that augments a\nlow-resource training signal through multitask learning and LLM-generated\nsynthetic data. Our framework jointly predicts morphological segments and\nglosses from orthographic input, leveraging shared linguistic representations\nobtained through a common documentary process to enhance model generalization.\nTo further address data scarcity, we integrate synthetic training data\ngenerated by large language models (LLMs) using in-context learning.\nExperimental results on the SIGMORPHON 2023 dataset show that our approach\nsignificantly improves word-level segmentation accuracy and morpheme-level\nF1-score across multiple low-resource languages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8bed\u7d20\u5206\u5272\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548cLLM\u751f\u6210\u5408\u6210\u6570\u636e\u589e\u5f3a\u4f4e\u8d44\u6e90\u8bad\u7ec3\u4fe1\u53f7\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8bed\u7d20\u5206\u5272\u548c\u6ce8\u91ca\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u8054\u5408\u9884\u6d4b\u5f62\u6001\u5b66\u7247\u6bb5\u548c\u6ce8\u91ca\uff0c\u5229\u7528\u5171\u4eab\u8bed\u8a00\u8868\u793a\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u5408\u6210\u6570\u636e\u3002", "result": "\u5728SIGMORPHON 2023\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u8bcd\u7ea7\u5206\u5272\u51c6\u786e\u6027\u548c\u8bed\u7d20\u7ea7F1\u5206\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u7d20\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2505.16673", "pdf": "https://arxiv.org/pdf/2505.16673", "abs": "https://arxiv.org/abs/2505.16673", "authors": ["Huanjin Yao", "Qixiang Yin", "Jingyi Zhang", "Min Yang", "Yibo Wang", "Wenhao Wu", "Fei Su", "Li Shen", "Minghui Qiu", "Dacheng Tao", "Jiaxing Huang"], "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Technical report", "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.", "AI": {"tldr": "\u63d0\u51faShare-GRPO\uff0c\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u548c\u4f18\u52bf\u6d88\u5931\u95ee\u9898\u3002", "motivation": "\u6fc0\u52b1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u548c\u4f18\u52bf\u6d88\u5931\u95ee\u9898\u3002", "method": "\u63d0\u51faShare-GRPO\uff0c\u901a\u8fc7\u6570\u636e\u6269\u5c55\u6280\u672f\u6269\u5c55\u95ee\u9898\u7a7a\u95f4\uff0c\u63a2\u7d22\u548c\u5171\u4eab\u591a\u6837\u63a8\u7406\u8f68\u8ff9\uff0c\u5e76\u5728\u4f18\u52bf\u8ba1\u7b97\u4e2d\u5171\u4eab\u5956\u52b1\u4fe1\u606f\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Share-GRPO\u6709\u6548\u63d0\u5347\u4e86MLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2505.16806", "pdf": "https://arxiv.org/pdf/2505.16806", "abs": "https://arxiv.org/abs/2505.16806", "authors": ["Kexin Zhang", "Junlan Chen", "Daifeng Li", "Yuxuan Zhang", "Yangyang Feng", "Bowen Deng", "Weixu Chen"], "title": "Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) encounter difficulties in knowledge-intensive\nmulti-step reasoning (KIMSR) tasks. One challenge is how to effectively extract\nand represent rationale evidence. The current methods often extract\nsemantically relevant but logically irrelevant evidence, resulting in flawed\nreasoning and inaccurate responses. We propose a two-way evidence\nself-alignment (TW-ESA) module, which utilizes the mutual alignment between\nstrict reasoning and LLM reasoning to enhance its understanding of the causal\nlogic of evidence, thereby addressing the first challenge. Another challenge is\nhow to utilize the rationale evidence and LLM's intrinsic knowledge for\naccurate reasoning when the evidence contains uncertainty. We propose a\ndual-gated reasoning enhancement (DGR) module to gradually fuse useful\nknowledge of LLM within strict reasoning, which can enable the model to perform\naccurate reasoning by focusing on causal elements in the evidence and exhibit\ngreater robustness. The two modules are collaboratively trained in a unified\nframework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR\ndatasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based\nfine-tuning methods, with remarkable average improvements of 4% in exact match\n(EM) and 5% in F1 score. The implementation code is available at\nhttps://anonymous.4open.science/r/ESA-DGR-2BF8.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aESA-DGR\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u53cc\u5411\u8bc1\u636e\u81ea\u5bf9\u9f50\uff08TW-ESA\uff09\u548c\u53cc\u95e8\u63a8\u7406\u589e\u5f3a\uff08DGR\uff09\u6a21\u5757\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u9762\u4e34\u8bc1\u636e\u63d0\u53d6\u548c\u5229\u7528\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faTW-ESA\u6a21\u5757\u901a\u8fc7\u4e25\u683c\u63a8\u7406\u4e0eLLM\u63a8\u7406\u7684\u5bf9\u9f50\u63d0\u5347\u8bc1\u636e\u903b\u8f91\u7406\u89e3\uff0cDGR\u6a21\u5757\u901a\u8fc7\u9010\u6b65\u878d\u5408LLM\u77e5\u8bc6\u589e\u5f3a\u63a8\u7406\u51c6\u786e\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cESA-DGR\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cEM\u548cF1\u5206\u6570\u5206\u522b\u5e73\u5747\u63d0\u53474%\u548c5%\u3002", "conclusion": "ESA-DGR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.16674", "pdf": "https://arxiv.org/pdf/2505.16674", "abs": "https://arxiv.org/abs/2505.16674", "authors": ["Marcella Astrid", "Abdelrahman Shabayek", "Djamila Aouada"], "title": "Zero-Shot Anomaly Detection in Battery Thermal Images Using Visual Question Answering with Prior Knowledge", "categories": ["cs.CV"], "comment": "Accepted in EUSIPCO 2025", "summary": "Batteries are essential for various applications, including electric vehicles\nand renewable energy storage, making safety and efficiency critical concerns.\nAnomaly detection in battery thermal images helps identify failures early, but\ntraditional deep learning methods require extensive labeled data, which is\ndifficult to obtain, especially for anomalies due to safety risks and high data\ncollection costs. To overcome this, we explore zero-shot anomaly detection\nusing Visual Question Answering (VQA) models, which leverage pretrained\nknowledge and textbased prompts to generalize across vision tasks. By\nincorporating prior knowledge of normal battery thermal behavior, we design\nprompts to detect anomalies without battery-specific training data. We evaluate\nthree VQA models (ChatGPT-4o, LLaVa-13b, and BLIP-2) analyzing their robustness\nto prompt variations, repeated trials, and qualitative outputs. Despite the\nlack of finetuning on battery data, our approach demonstrates competitive\nperformance compared to state-of-the-art models that are trained with the\nbattery data. Our findings highlight the potential of VQA-based zero-shot\nlearning for battery anomaly detection and suggest future directions for\nimproving its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u6a21\u578b\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u7535\u6c60\u70ed\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u65e0\u9700\u7535\u6c60\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u7535\u6c60\u5b89\u5168\u6027\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u5f02\u5e38\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684VQA\u6a21\u578b\uff08\u5982ChatGPT-4o\u3001LLaVa-13b\u548cBLIP-2\uff09\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u7ed3\u5408\u7535\u6c60\u6b63\u5e38\u884c\u4e3a\u77e5\u8bc6\u8fdb\u884c\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u5c3d\u7ba1\u672a\u5bf9\u7535\u6c60\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u57fa\u4e8e\u7535\u6c60\u6570\u636e\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "VQA\u96f6\u6837\u672c\u5b66\u4e60\u5728\u7535\u6c60\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6548\u679c\u3002"}}
{"id": "2505.16814", "pdf": "https://arxiv.org/pdf/2505.16814", "abs": "https://arxiv.org/abs/2505.16814", "authors": ["Gaurav Kamath", "Sowmya Vajjala"], "title": "Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?", "categories": ["cs.CL"], "comment": "pre-print", "summary": "Named Entity Recognition(NER) for low-resource languages aims to produce\nrobust systems for languages where there is limited labeled training data\navailable, and has been an area of increasing interest within NLP. Data\naugmentation for increasing the amount of low-resource labeled data is a common\npractice. In this paper, we explore the role of synthetic data in the context\nof multilingual, low-resource NER, considering 11 languages from diverse\nlanguage families. Our results suggest that synthetic data does in fact hold\npromise for low-resource language NER, though we see significant variation\nbetween languages.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5408\u6210\u6570\u636e\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u8986\u76d611\u79cd\u8bed\u8a00\uff0c\u7ed3\u679c\u663e\u793a\u5408\u6210\u6570\u636e\u6709\u6f5c\u529b\uff0c\u4f46\u6548\u679c\u56e0\u8bed\u8a00\u800c\u5f02\u3002", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NER\u4efb\u52a1\u56e0\u6807\u6ce8\u6570\u636e\u6709\u9650\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u6570\u636e\u589e\u5f3a\u662f\u5e38\u89c1\u65b9\u6cd5\uff0c\u4f46\u5408\u6210\u6570\u636e\u7684\u4f5c\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u572811\u79cd\u4e0d\u540c\u8bed\u7cfb\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884cNER\u5b9e\u9a8c\u3002", "result": "\u5408\u6210\u6570\u636e\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00NER\u6709\u79ef\u6781\u4f5c\u7528\uff0c\u4f46\u6548\u679c\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00NER\u7684\u6709\u6f5c\u529b\u5de5\u5177\uff0c\u4f46\u9700\u8003\u8651\u8bed\u8a00\u7279\u6027\u3002"}}
{"id": "2505.16679", "pdf": "https://arxiv.org/pdf/2505.16679", "abs": "https://arxiv.org/abs/2505.16679", "authors": ["Jordan Dotzel", "Tony Montes", "Mohamed S. Abdelfattah", "Zhiru Zhang"], "title": "Semantic Compression of 3D Objects for Open and Collaborative Virtual Worlds", "categories": ["cs.CV", "cs.AI"], "comment": "First two authors have equal contribution", "summary": "Traditional methods for 3D object compression operate only on structural\ninformation within the object vertices, polygons, and textures. These methods\nare effective at compression rates up to 10x for standard object sizes but\nquickly deteriorate at higher compression rates with texture artifacts,\nlow-polygon counts, and mesh gaps. In contrast, semantic compression ignores\nstructural information and operates directly on the core concepts to push to\nextreme levels of compression. In addition, it uses natural language as its\nstorage format, which makes it natively human-readable and a natural fit for\nemerging applications built around large-scale, collaborative projects within\naugmented and virtual reality. It deprioritizes structural information like\nlocation, size, and orientation and predicts the missing information with\nstate-of-the-art deep generative models. In this work, we construct a pipeline\nfor 3D semantic compression from public generative models and explore the\nquality-compression frontier for 3D object compression. We apply this pipeline\nto achieve rates as high as 105x for 3D objects taken from the Objaverse\ndataset and show that semantic compression can outperform traditional methods\nin the important quality-preserving region around 100x compression.", "AI": {"tldr": "\u4f20\u7edf3D\u5bf9\u8c61\u538b\u7f29\u65b9\u6cd5\u4ec5\u5904\u7406\u9876\u70b9\u3001\u591a\u8fb9\u5f62\u548c\u7eb9\u7406\u7b49\u7ed3\u6784\u4fe1\u606f\uff0c\u800c\u8bed\u4e49\u538b\u7f29\u5219\u76f4\u63a5\u64cd\u4f5c\u6838\u5fc3\u6982\u5ff5\uff0c\u5b9e\u73b0\u66f4\u9ad8\u538b\u7f29\u7387\uff0c\u5e76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5b58\u50a8\u683c\u5f0f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8bed\u4e49\u538b\u7f29\u901a\u8fc7\u5ffd\u7565\u7ed3\u6784\u4fe1\u606f\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u586b\u8865\u7f3a\u5931\u4fe1\u606f\uff0c\u6709\u671b\u7a81\u7834\u538b\u7f29\u6781\u9650\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u516c\u5171\u751f\u6210\u6a21\u578b\u76843D\u8bed\u4e49\u538b\u7f29\u6d41\u7a0b\uff0c\u63a2\u7d22\u8d28\u91cf\u4e0e\u538b\u7f29\u7387\u7684\u5e73\u8861\u3002", "result": "\u5728Objaverse\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe105x\u7684\u538b\u7f29\u7387\uff0c\u5728100x\u538b\u7f29\u7387\u9644\u8fd1\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u538b\u7f29\u5728\u9ad8\u538b\u7f29\u7387\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u9002\u5408\u5927\u89c4\u6a21\u534f\u4f5c\u9879\u76ee\uff0c\u5c24\u5176\u662f\u5728\u589e\u5f3a\u73b0\u5b9e\u548c\u865a\u62df\u73b0\u5b9e\u5e94\u7528\u4e2d\u3002"}}
{"id": "2505.16831", "pdf": "https://arxiv.org/pdf/2505.16831", "abs": "https://arxiv.org/abs/2505.16831", "authors": ["Xiaoyu Xu", "Xiang Yue", "Yang Liu", "Qingqing Ye", "Haibo Hu", "Minxin Du"], "title": "Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "44 pages", "summary": "Unlearning in large language models (LLMs) is intended to remove the\ninfluence of specific data, yet current evaluations rely heavily on token-level\nmetrics such as accuracy and perplexity. We show that these metrics can be\nmisleading: models often appear to forget, but their original behavior can be\nrapidly restored with minimal fine-tuning, revealing that unlearning may\nobscure information rather than erase it. To diagnose this phenomenon, we\nintroduce a representation-level evaluation framework using PCA-based\nsimilarity and shift, centered kernel alignment, and Fisher information.\nApplying this toolkit across six unlearning methods, three domains (text, code,\nmath), and two open-source LLMs, we uncover a critical distinction between\nreversible and irreversible forgetting. In reversible cases, models suffer\ntoken-level collapse yet retain latent features; in irreversible cases, deeper\nrepresentational damage occurs. We further provide a theoretical account\nlinking shallow weight perturbations near output layers to misleading\nunlearning signals, and show that reversibility is modulated by task type and\nhyperparameters. Our findings reveal a fundamental gap in current evaluation\npractices and establish a new diagnostic foundation for trustworthy unlearning\nin LLMs. We provide a unified toolkit for analyzing LLM representation changes\nunder unlearning and relearning:\nhttps://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9057\u5fd8\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u8bcd\u7ea7\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\u548c\u56f0\u60d1\u5ea6\uff09\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u8868\u793a\u5c42\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u53ef\u9006\u4e0e\u4e0d\u53ef\u9006\u9057\u5fd8\u7684\u533a\u522b\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u63a9\u76d6\u800c\u975e\u771f\u6b63\u5220\u9664\u4fe1\u606f\uff0c\u9700\u66f4\u53ef\u9760\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u5f15\u5165\u57fa\u4e8ePCA\u76f8\u4f3c\u6027\u3001\u4e2d\u5fc3\u6838\u5bf9\u9f50\u548cFisher\u4fe1\u606f\u7684\u8868\u793a\u7ea7\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u516d\u79cd\u9057\u5fd8\u65b9\u6cd5\u3001\u4e09\u4e2a\u9886\u57df\u548c\u4e24\u79cd\u5f00\u6e90LLM\u3002", "result": "\u53d1\u73b0\u53ef\u9006\u9057\u5fd8\u4e2d\u6a21\u578b\u4fdd\u7559\u6f5c\u5728\u7279\u5f81\uff0c\u4e0d\u53ef\u9006\u9057\u5fd8\u4e2d\u8868\u793a\u5c42\u53d7\u635f\uff1b\u4efb\u52a1\u7c7b\u578b\u548c\u8d85\u53c2\u6570\u8c03\u8282\u53ef\u9006\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u6839\u672c\u7f3a\u9677\uff0c\u4e3aLLM\u53ef\u4fe1\u9057\u5fd8\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u57fa\u7840\uff0c\u5e76\u53d1\u5e03\u4e86\u5206\u6790\u5de5\u5177\u5305\u3002"}}
{"id": "2505.16685", "pdf": "https://arxiv.org/pdf/2505.16685", "abs": "https://arxiv.org/abs/2505.16685", "authors": ["Corentin Dufourg", "Charlotte Pelletier", "St\u00e9phane May", "S\u00e9bastien Lef\u00e8vre"], "title": "On the use of Graphs for Satellite Image Time Series", "categories": ["cs.CV"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The Earth's surface is subject to complex and dynamic processes, ranging from\nlarge-scale phenomena such as tectonic plate movements to localized changes\nassociated with ecosystems, agriculture, or human activity. Satellite images\nenable global monitoring of these processes with extensive spatial and temporal\ncoverage, offering advantages over in-situ methods. In particular, resulting\nsatellite image time series (SITS) datasets contain valuable information. To\nhandle their large volume and complexity, some recent works focus on the use of\ngraph-based techniques that abandon the regular Euclidean structure of\nsatellite data to work at an object level. Besides, graphs enable modelling\nspatial and temporal interactions between identified objects, which are crucial\nfor pattern detection, classification and regression tasks. This paper is an\neffort to examine the integration of graph-based methods in spatio-temporal\nremote-sensing analysis. In particular, it aims to present a versatile\ngraph-based pipeline to tackle SITS analysis. It focuses on the construction of\nspatio-temporal graphs from SITS and their application to downstream tasks. The\npaper includes a comprehensive review and two case studies, which highlight the\npotential of graph-based approaches for land cover mapping and water resource\nforecasting. It also discusses numerous perspectives to resolve current\nlimitations and encourage future developments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u5728\u65f6\u7a7a\u9065\u611f\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u5904\u7406\u6d41\u7a0b\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u5730\u8868\u8986\u76d6\u6620\u5c04\u548c\u6c34\u8d44\u6e90\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5730\u7403\u8868\u9762\u52a8\u6001\u8fc7\u7a0b\u590d\u6742\uff0c\u536b\u661f\u56fe\u50cf\u65f6\u95f4\u5e8f\u5217\uff08SITS\uff09\u63d0\u4f9b\u4e86\u5168\u7403\u76d1\u6d4b\u80fd\u529b\uff0c\u4f46\u6570\u636e\u91cf\u5927\u4e14\u590d\u6742\u3002\u56fe\u65b9\u6cd5\u80fd\u7a81\u7834\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u7684\u9650\u5236\uff0c\u5efa\u6a21\u65f6\u7a7a\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u6a21\u5f0f\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u56fe\u5904\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u4eceSITS\u6784\u5efa\u65f6\u7a7a\u56fe\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002\u8bba\u6587\u8fd8\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\u548c\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u56fe\u65b9\u6cd5\u5728\u5730\u8868\u8986\u76d6\u6620\u5c04\u548c\u6c34\u8d44\u6e90\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u56fe\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2505.16834", "pdf": "https://arxiv.org/pdf/2505.16834", "abs": "https://arxiv.org/abs/2505.16834", "authors": ["Shuang Sun", "Huatong Song", "Yuhao Wang", "Ruiyang Ren", "Jinhao Jiang", "Junjie Zhang", "Fei Bai", "Jia Deng", "Wayne Xin Zhao", "Zheng Liu", "Lei Fang", "Zhongyuan Wang", "Ji-Rong Wen"], "title": "SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have advanced large language\nmodels (LLMs) in complex deep search scenarios requiring multi-step reasoning\nand iterative information retrieval. However, existing approaches face critical\nlimitations that lack high-quality training trajectories or suffer from the\ndistributional mismatches in simulated environments and prohibitive\ncomputational costs for real-world deployment. This paper introduces\nSimpleDeepSearcher, a lightweight yet effective framework that bridges this gap\nthrough strategic data engineering rather than complex training paradigms. Our\napproach synthesizes high-quality training data by simulating realistic user\ninteractions in live web search environments, coupled with a multi-criteria\ncuration strategy that optimizes the diversity and quality of input and output\nside. Experiments on five benchmarks across diverse domains demonstrate that\nSFT on only 871 curated samples yields significant improvements over RL-based\nbaselines. Our work establishes SFT as a viable pathway by systematically\naddressing the data-scarce bottleneck, offering practical insights for\nefficient deep search systems. Our code is available at\nhttps://github.com/RUCAIBox/SimpleDeepSearcher.", "AI": {"tldr": "SimpleDeepSearcher\u901a\u8fc7\u6570\u636e\u5de5\u7a0b\u800c\u975e\u590d\u6742\u8bad\u7ec3\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u5728\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u74f6\u9888\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u591a\u6b65\u63a8\u7406\u548c\u8fed\u4ee3\u68c0\u7d22\u4e2d\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4e0d\u8db3\u3001\u6a21\u62df\u73af\u5883\u5206\u5e03\u4e0d\u5339\u914d\u53ca\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u548c\u591a\u6807\u51c6\u6570\u636e\u7b5b\u9009\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4ec5\u9700871\u4e2a\u6837\u672c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSFT\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u57fa\u7ebf\u3002", "conclusion": "SFT\u662f\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u74f6\u9888\u7684\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u9ad8\u6548\u6df1\u5ea6\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2505.16687", "pdf": "https://arxiv.org/pdf/2505.16687", "abs": "https://arxiv.org/abs/2505.16687", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Yuan Zhang", "Yan Lu"], "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "While recent diffusion-based generative image codecs have shown impressive\nperformance, their iterative sampling process introduces unpleasing latency. In\nthis work, we revisit the design of a diffusion-based codec and argue that\nmulti-step sampling is not necessary for generative compression. Based on this\ninsight, we propose OneDC, a One-step Diffusion-based generative image Codec --\nthat integrates a latent compression module with a one-step diffusion\ngenerator. Recognizing the critical role of semantic guidance in one-step\ndiffusion, we propose using the hyperprior as a semantic signal, overcoming the\nlimitations of text prompts in representing complex visual content. To further\nenhance the semantic capability of the hyperprior, we introduce a semantic\ndistillation mechanism that transfers knowledge from a pretrained generative\ntokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and\nlatent-domain optimization to jointly enhance both reconstruction fidelity and\nperceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA\nperceptual quality even with one-step generation, offering over 40% bitrate\nreduction and 20x faster decoding compared to prior multi-step diffusion-based\ncodecs. Code will be released later.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u6b65\u6269\u6563\u751f\u6210\u56fe\u50cf\u7f16\u89e3\u7801\u5668OneDC\uff0c\u901a\u8fc7\u7ed3\u5408\u6f5c\u5728\u538b\u7f29\u6a21\u5757\u548c\u5355\u6b65\u6269\u6563\u751f\u6210\u5668\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u751f\u6210\u7f16\u89e3\u7801\u5668\u56e0\u8fed\u4ee3\u91c7\u6837\u5bfc\u81f4\u5ef6\u8fdf\u9ad8\uff0c\u4f5c\u8005\u8ba4\u4e3a\u591a\u6b65\u91c7\u6837\u5bf9\u751f\u6210\u538b\u7f29\u5e76\u975e\u5fc5\u8981\u3002", "method": "\u63d0\u51faOneDC\uff0c\u5229\u7528\u8d85\u5148\u9a8c\u4f5c\u4e3a\u8bed\u4e49\u4fe1\u53f7\uff0c\u5f15\u5165\u8bed\u4e49\u84b8\u998f\u673a\u5236\u589e\u5f3a\u8bed\u4e49\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u50cf\u7d20\u548c\u6f5c\u5728\u57df\u4f18\u5316\u3002", "result": "OneDC\u5728\u5355\u6b65\u751f\u6210\u4e0b\u5b9e\u73b0SOTA\u611f\u77e5\u8d28\u91cf\uff0c\u6bd4\u7279\u7387\u964d\u4f4e40%\uff0c\u89e3\u7801\u901f\u5ea6\u5feb20\u500d\u3002", "conclusion": "OneDC\u8bc1\u660e\u4e86\u5355\u6b65\u6269\u6563\u751f\u6210\u5728\u56fe\u50cf\u538b\u7f29\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u517c\u5177\u9ad8\u8d28\u91cf\u548c\u4f4e\u5ef6\u8fdf\u3002"}}
{"id": "2505.16838", "pdf": "https://arxiv.org/pdf/2505.16838", "abs": "https://arxiv.org/abs/2505.16838", "authors": ["Yibo Wang", "Li Shen", "Huanjin Yao", "Tiansheng Huang", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Kai Zhang", "Dacheng Tao"], "title": "R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\nenabling step-by-step problem-solving, yet its extension to Long-CoT introduces\nsubstantial computational overhead due to increased token length. Existing\ncompression approaches -- instance-level and token-level -- either sacrifice\nessential local reasoning signals like reflection or yield incoherent outputs.\nTo address these limitations, we propose R1-Compress, a two-stage chunk-level\ncompression framework that preserves both local information and coherence. Our\nmethod segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk\ncompression, and employs an inter-chunk search mechanism to select the short\nand coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,\nAIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces\ntoken usage while maintaining comparable reasoning accuracy. On MATH500,\nR1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to\nthe Long-CoT baseline, while reducing token usage by about 20%. Source code\nwill be available at https://github.com/w-yibo/R1-Compress", "AI": {"tldr": "R1-Compress\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u5206\u5757\u7ea7\u538b\u7f29\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11Long-CoT\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728Long-CoT\u4e2d\u727a\u7272\u5c40\u90e8\u63a8\u7406\u4fe1\u53f7\u6216\u5bfc\u81f4\u8f93\u51fa\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002", "method": "\u5c06Long-CoT\u5206\u5757\uff0c\u5e94\u7528LLM\u9a71\u52a8\u7684\u5757\u5185\u538b\u7f29\uff0c\u5e76\u901a\u8fc7\u5757\u95f4\u641c\u7d22\u9009\u62e9\u77ed\u4e14\u8fde\u8d2f\u7684\u5e8f\u5217\u3002", "result": "\u5728MATH500\u4e0a\uff0cR1-Compress\u51cf\u5c1120%\u7684token\u4f7f\u7528\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.6%\u3002", "conclusion": "R1-Compress\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002"}}
{"id": "2505.16707", "pdf": "https://arxiv.org/pdf/2505.16707", "abs": "https://arxiv.org/abs/2505.16707", "authors": ["Yongliang Wu", "Zonghui Li", "Xinting Hu", "Xinyu Ye", "Xianfang Zeng", "Gang Yu", "Wenbo Zhu", "Bernt Schiele", "Ming-Hsuan Yang", "Xu Yang"], "title": "KRIS-Bench: Benchmarking Next-Level Intelligent Image Editing Models", "categories": ["cs.CV"], "comment": "39 pages, 36 figures", "summary": "Recent advances in multi-modal generative models have enabled significant\nprogress in instruction-based image editing. However, while these models\nproduce visually plausible outputs, their capacity for knowledge-based\nreasoning editing tasks remains under-explored. In this paper, we introduce\nKRIS-Bench (Knowledge-based Reasoning in Image-editing Systems Benchmark), a\ndiagnostic benchmark designed to assess models through a cognitively informed\nlens. Drawing from educational theory, KRIS-Bench categorizes editing tasks\nacross three foundational knowledge types: Factual, Conceptual, and Procedural.\nBased on this taxonomy, we design 22 representative tasks spanning 7 reasoning\ndimensions and release 1,267 high-quality annotated editing instances. To\nsupport fine-grained evaluation, we propose a comprehensive protocol that\nincorporates a novel Knowledge Plausibility metric, enhanced by knowledge hints\nand calibrated through human studies. Empirical results on 10 state-of-the-art\nmodels reveal significant gaps in reasoning performance, highlighting the need\nfor knowledge-centric benchmarks to advance the development of intelligent\nimage editing systems.", "AI": {"tldr": "KRIS-Bench\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u8bc4\u6d4b\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u7c7b\u77e5\u8bc6\u7c7b\u578b\uff08\u4e8b\u5b9e\u3001\u6982\u5ff5\u3001\u7a0b\u5e8f\uff09\u548c22\u4e2a\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u5728\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faKRIS-Bench\u57fa\u51c6\uff0c\u5305\u542b22\u4e2a\u4efb\u52a1\u548c1,267\u4e2a\u6807\u6ce8\u5b9e\u4f8b\uff0c\u5e76\u8bbe\u8ba1\u77e5\u8bc6\u5408\u7406\u6027\u6307\u6807\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u572810\u4e2a\u5148\u8fdb\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5b83\u4eec\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u77e5\u8bc6\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u6d4b\u57fa\u51c6\u5bf9\u63a8\u52a8\u667a\u80fd\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.16847", "pdf": "https://arxiv.org/pdf/2505.16847", "abs": "https://arxiv.org/abs/2505.16847", "authors": ["Baran Barbarestani", "Isa Maks", "Piek Vossen"], "title": "Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces a method for detecting inappropriately targeting\nlanguage in online conversations by integrating crowd and expert annotations\nwith ChatGPT. We focus on English conversation threads from Reddit, examining\ncomments that target individuals or groups. Our approach involves a\ncomprehensive annotation framework that labels a diverse data set for various\ntarget categories and specific target words within the conversational context.\nWe perform a comparative analysis of annotations from human experts, crowd\nannotators, and ChatGPT, revealing strengths and limitations of each method in\nrecognizing both explicit hate speech and subtler discriminatory language. Our\nfindings highlight the significant role of contextual factors in identifying\nhate speech and uncover new categories of targeting, such as social belief and\nbody image. We also address the challenges and subjective judgments involved in\nannotation and the limitations of ChatGPT in grasping nuanced language. This\nstudy provides insights for improving automated content moderation strategies\nto enhance online safety and inclusivity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f17\u5305\u3001\u4e13\u5bb6\u6807\u6ce8\u548cChatGPT\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5728\u7ebf\u5bf9\u8bdd\u4e2d\u7684\u4e0d\u5f53\u76ee\u6807\u8bed\u8a00\uff0c\u91cd\u70b9\u5206\u6790\u4e86Reddit\u4e0a\u7684\u82f1\u8bed\u5bf9\u8bdd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u6e90\u6807\u6ce8\u65b9\u6cd5\uff08\u4e13\u5bb6\u3001\u4f17\u5305\u3001ChatGPT\uff09\u8bc6\u522b\u5728\u7ebf\u5bf9\u8bdd\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u548c\u6b67\u89c6\u6027\u8bed\u8a00\uff0c\u4ee5\u6539\u8fdb\u5185\u5bb9\u5ba1\u6838\u7b56\u7565\u3002", "method": "\u91c7\u7528\u7efc\u5408\u6807\u6ce8\u6846\u67b6\uff0c\u6807\u6ce8\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e13\u5bb6\u3001\u4f17\u5305\u548cChatGPT\u5728\u8bc6\u522b\u663e\u6027\u548c\u9690\u6027\u4ec7\u6068\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u5bf9\u8bc6\u522b\u4ec7\u6068\u8a00\u8bba\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u63ed\u793a\u4e86\u65b0\u7684\u76ee\u6807\u7c7b\u522b\uff08\u5982\u793e\u4f1a\u4fe1\u4ef0\u548c\u8eab\u4f53\u5f62\u8c61\uff09\u3002ChatGPT\u5728\u7406\u89e3\u5fae\u5999\u8bed\u8a00\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6539\u8fdb\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u591a\u6e90\u6807\u6ce8\u548c\u4e0a\u4e0b\u6587\u5206\u6790\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.16709", "pdf": "https://arxiv.org/pdf/2505.16709", "abs": "https://arxiv.org/abs/2505.16709", "authors": ["Kai Hsiang Hsieh", "Monyneath Yim", "Jui Chiu Chiang"], "title": "SEDD-PCC: A Single Encoder-Dual Decoder Framework For End-To-End Learned Point Cloud Compression", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "To encode point clouds containing both geometry and attributes, most\nlearning-based compression schemes treat geometry and attribute coding\nseparately, employing distinct encoders and decoders. This not only increases\ncomputational complexity but also fails to fully exploit shared features\nbetween geometry and attributes. To address this limitation, we propose\nSEDD-PCC, an end-to-end learning-based framework for lossy point cloud\ncompression that jointly compresses geometry and attributes. SEDD-PCC employs a\nsingle encoder to extract shared geometric and attribute features into a\nunified latent space, followed by dual specialized decoders that sequentially\nreconstruct geometry and attributes. Additionally, we incorporate knowledge\ndistillation to enhance feature representation learning from a teacher model,\nfurther improving coding efficiency. With its simple yet effective design,\nSEDD-PCC provides an efficient and practical solution for point cloud\ncompression. Comparative evaluations against both rule-based and learning-based\nmethods demonstrate its competitive performance, highlighting SEDD-PCC as a\npromising AI-driven compression approach.", "AI": {"tldr": "SEDD-PCC\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u538b\u7f29\u70b9\u4e91\u7684\u51e0\u4f55\u548c\u5c5e\u6027\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u548c\u77e5\u8bc6\u84b8\u998f\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u51e0\u4f55\u548c\u5c5e\u6027\u7f16\u7801\u5206\u5f00\u5904\u7406\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u4e14\u672a\u5145\u5206\u5229\u7528\u5171\u4eab\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u5355\u4e00\u7f16\u7801\u5668\u63d0\u53d6\u5171\u4eab\u7279\u5f81\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\uff0c\u518d\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u987a\u5e8f\u91cd\u5efa\u51e0\u4f55\u548c\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728\u5bf9\u6bd4\u8bc4\u4f30\u4e2d\uff0cSEDD-PCC\u8868\u73b0\u51fa\u4f18\u4e8e\u89c4\u5219\u548c\u57fa\u4e8e\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SEDD-PCC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u70b9\u4e91\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86AI\u9a71\u52a8\u65b9\u6cd5\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16855", "pdf": "https://arxiv.org/pdf/2505.16855", "abs": "https://arxiv.org/abs/2505.16855", "authors": ["Alberto Mu\u00f1oz-Ortiz", "David Vilares", "Caio COrro", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Nested Named Entity Recognition as Single-Pass Sequence Labeling", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to EMNLP 2025", "summary": "We cast nested named entity recognition (NNER) as a sequence labeling task by\nleveraging prior work that linearizes constituency structures, effectively\nreducing the complexity of this structured prediction problem to\nstraightforward token classification. By combining these constituency\nlinearizations with pretrained encoders, our method captures nested entities\nwhile performing exactly $n$ tagging actions. Our approach achieves competitive\nperformance compared to less efficient systems, and it can be trained using any\noff-the-shelf sequence labeling library.", "AI": {"tldr": "\u5c06\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NNER\uff09\u8f6c\u5316\u4e3a\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u9009\u533a\u7ed3\u6784\u7b80\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5b9e\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5d4c\u5957\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u6027\u3002", "method": "\u5229\u7528\u9009\u533a\u7ed3\u6784\u7ebf\u6027\u5316\u6280\u672f\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7f16\u7801\u5668\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\u3002", "result": "\u6027\u80fd\u4e0e\u4f4e\u6548\u7cfb\u7edf\u76f8\u5f53\uff0c\u4e14\u4ec5\u9700n\u6b21\u6807\u6ce8\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6613\u4e8e\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u73b0\u6210\u7684\u5e8f\u5217\u6807\u6ce8\u5e93\u3002"}}
{"id": "2505.16740", "pdf": "https://arxiv.org/pdf/2505.16740", "abs": "https://arxiv.org/abs/2505.16740", "authors": ["Alya Zouzou", "L\u00e9o and\u00e9ol", "M\u00e9lanie Ducoffe", "Ryma Boumazouza"], "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We explore the use of conformal prediction to provide statistical uncertainty\nguarantees for runway detection in vision-based landing systems (VLS). Using\nfine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal\nprediction to quantify localization reliability under user-defined risk levels.\nWe also introduce Conformal mean Average Precision (C-mAP), a novel metric\naligning object detection performance with conformal guarantees. Our results\nshow that conformal prediction can improve the reliability of runway detection\nby quantifying uncertainty in a statistically sound way, increasing safety\non-board and paving the way for certification of ML system in the aerospace\ndomain.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u7740\u9646\u7cfb\u7edf\uff08VLS\uff09\u4e2d\u7684\u8dd1\u9053\u68c0\u6d4b\u63d0\u4f9b\u7edf\u8ba1\u4e0d\u786e\u5b9a\u6027\u4fdd\u8bc1\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6307\u6807C-mAP\u3002", "motivation": "\u63d0\u9ad8\u8dd1\u9053\u68c0\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u822a\u7a7a\u822a\u5929\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8ba4\u8bc1\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u4f7f\u7528\u5fae\u8c03\u7684YOLOv5\u548cYOLOv6\u6a21\u578b\uff0c\u7ed3\u5408\u5171\u5f62\u9884\u6d4b\u91cf\u5316\u5b9a\u4f4d\u53ef\u9760\u6027\u3002", "result": "\u5171\u5f62\u9884\u6d4b\u80fd\u4ee5\u7edf\u8ba1\u4e0a\u5408\u7406\u7684\u65b9\u5f0f\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u8dd1\u9053\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u63d0\u5347\u4e86\u8dd1\u9053\u68c0\u6d4b\u7684\u5b89\u5168\u6027\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u822a\u7a7a\u822a\u5929\u9886\u57df\u7684\u8ba4\u8bc1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2505.16868", "pdf": "https://arxiv.org/pdf/2505.16868", "abs": "https://arxiv.org/abs/2505.16868", "authors": ["Sudhansu Bala Das", "Samujjal Choudhury", "Tapas Kumar Mishra", "Bidyut Kr. Patra"], "title": "Comparative analysis of subword tokenization approaches for Indian languages", "categories": ["cs.CL"], "comment": "24 pages, 4 tables", "summary": "Tokenization is the act of breaking down text into smaller parts, or tokens,\nthat are easier for machines to process. This is a key phase in machine\ntranslation (MT) models. Subword tokenization enhances this process by breaking\ndown words into smaller subword units, which is especially beneficial in\nlanguages with complicated morphology or a vast vocabulary. It is useful in\ncapturing the intricate structure of words in Indian languages (ILs), such as\nprefixes, suffixes, and other morphological variations. These languages\nfrequently use agglutinative structures, in which words are formed by the\ncombination of multiple morphemes such as suffixes, prefixes, and stems. As a\nresult, a suitable tokenization strategy must be chosen to address these\nscenarios. This paper examines how different subword tokenization techniques,\nsuch as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,\naffect ILs. The effectiveness of these subword tokenization techniques is\ninvestigated in statistical, neural, and multilingual neural machine\ntranslation models. All models are examined using standard evaluation metrics,\nsuch as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,\nRIBES, and COMET. Based on the results, it appears that for the majority of\nlanguage pairs for the Statistical and Neural MT models, the SentencePiece\ntokenizer continuously performed better than other tokenizers in terms of BLEU\nscore. However, BPE tokenization outperformed other tokenization techniques in\nthe context of Multilingual Neural Machine Translation model. The results show\nthat, despite using the same tokenizer and dataset for each model, translations\nfrom ILs to English surpassed translations from English to ILs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u540c\u5b50\u8bcd\u5206\u8bcd\u6280\u672f\uff08\u5982SentencePiece\u3001BPE\u548cWordPiece\uff09\u5bf9\u5370\u5ea6\u8bed\u8a00\uff08ILs\uff09\u673a\u5668\u7ffb\u8bd1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0SentencePiece\u5728\u7edf\u8ba1\u548c\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cBPE\u5728\u591a\u8bed\u8a00\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u66f4\u4f18\u3002", "motivation": "\u5370\u5ea6\u8bed\u8a00\u5177\u6709\u590d\u6742\u7684\u5f62\u6001\u7ed3\u6784\uff0c\u9700\u8981\u5408\u9002\u7684\u5b50\u8bcd\u5206\u8bcd\u6280\u672f\u6765\u6355\u6349\u5176\u524d\u7f00\u3001\u540e\u7f00\u7b49\u5f62\u6001\u53d8\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86SentencePiece\u3001BPE\u548cWordPiece\u7b49\u5b50\u8bcd\u5206\u8bcd\u6280\u672f\u5728\u7edf\u8ba1\u3001\u795e\u7ecf\u548c\u591a\u8bed\u8a00\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528BLEU\u3001TER\u7b49\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u3002", "result": "SentencePiece\u5728\u7edf\u8ba1\u548c\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u800cBPE\u5728\u591a\u8bed\u8a00\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4e2d\u66f4\u4f18\u3002ILs\u5230\u82f1\u8bed\u7684\u7ffb\u8bd1\u8d28\u91cf\u4f18\u4e8e\u82f1\u8bed\u5230ILs\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u5b50\u8bcd\u5206\u8bcd\u6280\u672f\u5bf9\u63d0\u5347\u5370\u5ea6\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0cSentencePiece\u548cBPE\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5404\u6709\u4f18\u52bf\u3002"}}
{"id": "2505.16756", "pdf": "https://arxiv.org/pdf/2505.16756", "abs": "https://arxiv.org/abs/2505.16756", "authors": ["Hailong Ning", "Siying Wang", "Tao Lei", "Xiaopeng Cao", "Huanmin Dou", "Bin Zhao", "Asoke K. Nandi", "Petia Radeva"], "title": "Representation Discrepancy Bridging Method for Remote Sensing Image-Text Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Remote Sensing Image-Text Retrieval (RSITR) plays a critical role in\ngeographic information interpretation, disaster monitoring, and urban planning\nby establishing semantic associations between image and textual descriptions.\nExisting Parameter-Efficient Fine-Tuning (PEFT) methods for Vision-and-Language\nPre-training (VLP) models typically adopt symmetric adapter structures for\nexploring cross-modal correlations. However, the strong discriminative nature\nof text modality may dominate the optimization process and inhibits image\nrepresentation learning. The nonnegligible imbalanced cross-modal optimization\nremains a bottleneck to enhancing the model performance. To address this issue,\nthis study proposes a Representation Discrepancy Bridging (RDB) method for the\nRSITR task. On the one hand, a Cross-Modal Asymmetric Adapter (CMAA) is\ndesigned to enable modality-specific optimization and improve feature\nalignment. The CMAA comprises a Visual Enhancement Adapter (VEA) and a Text\nSemantic Adapter (TSA). VEA mines fine-grained image features by Differential\nAttention (DA) mechanism, while TSA identifies key textual semantics through\nHierarchical Attention (HA) mechanism. On the other hand, this study extends\nthe traditional single-task retrieval framework to a dual-task optimization\nframework and develops a Dual-Task Consistency Loss (DTCL). The DTCL improves\ncross-modal alignment robustness through an adaptive weighted combination of\ncross-modal, classification, and exponential moving average consistency\nconstraints. Experiments on RSICD and RSITMD datasets show that the proposed\nRDB method achieves a 6%-11% improvement in mR metrics compared to\nstate-of-the-art PEFT methods and a 1.15%-2% improvement over the full\nfine-tuned GeoRSCLIP model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9065\u611f\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\uff08RSITR\uff09\u7684\u8868\u793a\u5dee\u5f02\u6865\u63a5\uff08RDB\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4e0d\u5bf9\u79f0\u9002\u914d\u5668\uff08CMAA\uff09\u548c\u53cc\u4efb\u52a1\u4e00\u81f4\u6027\u635f\u5931\uff08DTCL\uff09\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u5173\u8054\u63a2\u7d22\u4e2d\u901a\u5e38\u91c7\u7528\u5bf9\u79f0\u9002\u914d\u5668\u7ed3\u6784\uff0c\u4f46\u6587\u672c\u6a21\u6001\u7684\u5f3a\u5224\u522b\u6027\u53ef\u80fd\u6291\u5236\u56fe\u50cf\u8868\u793a\u5b66\u4e60\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u4f18\u5316\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51faRDB\u65b9\u6cd5\uff0c\u5305\u62ec\u8de8\u6a21\u6001\u4e0d\u5bf9\u79f0\u9002\u914d\u5668\uff08CMAA\uff09\u548c\u53cc\u4efb\u52a1\u4e00\u81f4\u6027\u635f\u5931\uff08DTCL\uff09\u3002CMAA\u901a\u8fc7\u89c6\u89c9\u589e\u5f3a\u9002\u914d\u5668\uff08VEA\uff09\u548c\u6587\u672c\u8bed\u4e49\u9002\u914d\u5668\uff08TSA\uff09\u5b9e\u73b0\u6a21\u6001\u7279\u5b9a\u4f18\u5316\uff1bDTCL\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u7ec4\u5408\u63d0\u5347\u8de8\u6a21\u6001\u5bf9\u9f50\u9c81\u68d2\u6027\u3002", "result": "\u5728RSICD\u548cRSITMD\u6570\u636e\u96c6\u4e0a\uff0cRDB\u65b9\u6cd5\u5728mR\u6307\u6807\u4e0a\u6bd4\u73b0\u6709PEFT\u65b9\u6cd5\u63d0\u53476%-11%\uff0c\u6bd4\u5168\u5fae\u8c03GeoRSCLIP\u6a21\u578b\u63d0\u53471.15%-2%\u3002", "conclusion": "RDB\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16869", "pdf": "https://arxiv.org/pdf/2505.16869", "abs": "https://arxiv.org/abs/2505.16869", "authors": ["Weixiang Zhao", "Yulin Hu", "Yang Deng", "Tongtong Wu", "Wenxuan Zhang", "Jiahe Guo", "An Zhang", "Yanyan Zhao", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "title": "MPO: Multilingual Safety Alignment via Reward Gap Optimization", "categories": ["cs.CL"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Large language models (LLMs) have become increasingly central to AI\napplications worldwide, necessitating robust multilingual safety alignment to\nensure secure deployment across diverse linguistic contexts. Existing\npreference learning methods for safety alignment, such as RLHF and DPO, are\nprimarily monolingual and struggle with noisy multilingual data. To address\nthese limitations, we introduce Multilingual reward gaP Optimization (MPO), a\nnovel approach that leverages the well-aligned safety capabilities of the\ndominant language (English) to improve safety alignment across multiple\nlanguages. MPO directly minimizes the reward gap difference between the\ndominant language and target languages, effectively transferring safety\ncapabilities while preserving the original strengths of the dominant language.\nExtensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate\nMPO's efficacy in multilingual safety alignment without degrading general\nmultilingual utility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMPO\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4e3b\u5bfc\u8bed\u8a00\u4e0e\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u7684\u5956\u52b1\u5dee\u8ddd\uff0c\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5168\u7403AI\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\u548cDPO\uff09\u4e3b\u8981\u9488\u5bf9\u5355\u8bed\u8a00\uff0c\u96be\u4ee5\u5904\u7406\u591a\u8bed\u8a00\u6570\u636e\u4e2d\u7684\u566a\u58f0\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Multilingual reward gaP Optimization (MPO)\uff0c\u5229\u7528\u4e3b\u5bfc\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u7684\u826f\u597d\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5956\u52b1\u5dee\u8ddd\u6765\u6539\u8fdb\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u3002", "result": "\u5728LLaMA-3.1\u3001Gemma-2\u548cQwen2.5\u4e09\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MPO\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u591a\u8bed\u8a00\u901a\u7528\u6027\u80fd\u3002", "conclusion": "MPO\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u591a\u8bed\u8a00\u5b89\u5168\u6027\u3002"}}
{"id": "2505.16761", "pdf": "https://arxiv.org/pdf/2505.16761", "abs": "https://arxiv.org/abs/2505.16761", "authors": ["Jian Liu", "Jing Xu", "Song Guo", "Jing Li", "Jingfeng Guo", "Jiaao Yu", "Haohan Weng", "Biwen Lei", "Xianghui Yang", "Zhuo Chen", "Fangqi Zhu", "Tao Han", "Chunchao Guo"], "title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Existing pretrained models for 3D mesh generation often suffer from data\nbiases and produce low-quality results, while global reinforcement learning\n(RL) methods rely on object-level rewards that struggle to capture local\nstructure details. To address these challenges, we present \\textbf{Mesh-RFT}, a\nnovel fine-grained reinforcement fine-tuning framework that employs Masked\nDirect Preference Optimization (M-DPO) to enable localized refinement via\nquality-aware face masking. To facilitate efficient quality evaluation, we\nintroduce an objective topology-aware scoring system to evaluate geometric\nintegrity and topological regularity at both object and face levels through two\nmetrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating\nthese metrics into a fine-grained RL strategy, Mesh-RFT becomes the first\nmethod to optimize mesh quality at the granularity of individual faces,\nresolving localized errors while preserving global coherence. Experiment\nresults show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\%\nand improves Topology Score (TS) by 3.8\\% over pre-trained models, while\noutperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain.\nThese results demonstrate Mesh-RFT's ability to improve geometric integrity and\ntopological regularity, achieving new state-of-the-art performance in\nproduction-ready mesh generation. Project Page:\n\\href{https://hitcslj.github.io/mesh-rft/}{this https URL}.", "AI": {"tldr": "Mesh-RFT\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7Masked Direct Preference Optimization (M-DPO)\u5b9e\u73b0\u5c40\u90e8\u4f18\u5316\uff0c\u663e\u8457\u63d0\u53473D\u7f51\u683c\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u57283D\u7f51\u683c\u751f\u6210\u4e2d\u5b58\u5728\u6570\u636e\u504f\u5dee\u548c\u4f4e\u8d28\u91cf\u95ee\u9898\uff0c\u800c\u5168\u5c40\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u7ec6\u8282\u3002", "method": "\u91c7\u7528M-DPO\u8fdb\u884c\u5c40\u90e8\u7ec6\u5316\uff0c\u5f15\u5165\u8fb9\u754c\u8fb9\u7f18\u6bd4\uff08BER\uff09\u548c\u62d3\u6251\u5206\u6570\uff08TS\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6RL\u7b56\u7565\u4f18\u5316\u7f51\u683c\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aMesh-RFT\u663e\u8457\u964d\u4f4e\u4e86Hausdorff\u8ddd\u79bb\uff0824.6%\uff09\u5e76\u63d0\u5347\u4e86\u62d3\u6251\u5206\u6570\uff083.8%\uff09\uff0c\u4f18\u4e8e\u5168\u5c40DPO\u65b9\u6cd5\u3002", "conclusion": "Mesh-RFT\u5728\u51e0\u4f55\u5b8c\u6574\u6027\u548c\u62d3\u6251\u89c4\u5219\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u751f\u4ea7\u7ea7\u7f51\u683c\u751f\u6210\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2505.16881", "pdf": "https://arxiv.org/pdf/2505.16881", "abs": "https://arxiv.org/abs/2505.16881", "authors": ["Daniel F. Perez-Ramirez", "Dejan Kostic", "Magnus Boman"], "title": "CASTILLO: Characterizing Response Length Distributions of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Dataset available in\n  https://huggingface.co/datasets/danfperam/castillo and code is available in\n  https://github.com/DanielFPerez/castillo", "summary": "Efficiently managing compute resources for Large Language Model (LLM)\ninference remains challenging due to the inherently stochastic and variable\nlengths of autoregressive text generation. Accurately estimating response\nlengths in advance enables proactive resource allocation, yet existing\napproaches either bias text generation towards certain lengths or rely on\nassumptions that ignore model- and prompt-specific variability. We introduce\nCASTILLO, a dataset characterizing response length distributions across 13\nwidely-used open-source LLMs evaluated on seven distinct instruction-following\ncorpora. For each $\\langle$prompt, model$\\rangle$ sample pair, we generate 10\nindependent completions using fixed decoding hyper-parameters, record the token\nlength of each response, and publish summary statistics (mean, std-dev,\npercentiles), along with the shortest and longest completions, and the exact\ngeneration settings. Our analysis reveals significant inter- and intra-model\nvariability in response lengths (even under identical generation settings), as\nwell as model-specific behaviors and occurrences of partial text degeneration\nin only subsets of responses. CASTILLO enables the development of predictive\nmodels for proactive scheduling and provides a systematic framework for\nanalyzing model-specific generation behaviors. We publicly release the dataset\nand code to foster research at the intersection of generative language modeling\nand systems.", "AI": {"tldr": "CASTILLO\u6570\u636e\u96c6\u5206\u6790\u4e8613\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u57287\u79cd\u6307\u4ee4\u96c6\u4e0a\u7684\u54cd\u5e94\u957f\u5ea6\u5206\u5e03\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u7684\u663e\u8457\u53d8\u5f02\u6027\uff0c\u4e3a\u8d44\u6e90\u8c03\u5ea6\u63d0\u4f9b\u4e86\u9884\u6d4b\u57fa\u7840\u3002", "motivation": "\u7531\u4e8e\u81ea\u56de\u5f52\u6587\u672c\u751f\u6210\u7684\u968f\u673a\u6027\u548c\u53d8\u957f\u6027\uff0c\u9ad8\u6548\u7ba1\u7406\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u8ba1\u7b97\u8d44\u6e90\u5177\u6709\u6311\u6218\u6027\u3002\u51c6\u786e\u9884\u6d4b\u54cd\u5e94\u957f\u5ea6\u6709\u52a9\u4e8e\u8d44\u6e90\u5206\u914d\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u504f\u5dee\u6216\u5ffd\u7565\u6a21\u578b\u548c\u63d0\u793a\u7684\u53d8\u5f02\u6027\u3002", "method": "\u901a\u8fc7\u56fa\u5b9a\u89e3\u7801\u53c2\u6570\u4e3a\u6bcf\u4e2a\u3008\u63d0\u793a\uff0c\u6a21\u578b\u3009\u6837\u672c\u5bf9\u751f\u621010\u4e2a\u72ec\u7acb\u5b8c\u6210\uff0c\u8bb0\u5f55\u54cd\u5e94\u957f\u5ea6\uff0c\u5e76\u53d1\u5e03\u7edf\u8ba1\u6570\u636e\uff08\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u767e\u5206\u4f4d\u6570\uff09\u53ca\u6700\u77ed\u548c\u6700\u957f\u5b8c\u6210\u60c5\u51b5\u3002", "result": "\u5206\u6790\u663e\u793a\u54cd\u5e94\u957f\u5ea6\u5728\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u5b58\u5728\u663e\u8457\u53d8\u5f02\u6027\uff0c\u4e14\u6a21\u578b\u6709\u7279\u5b9a\u884c\u4e3a\u548c\u90e8\u5206\u6587\u672c\u9000\u5316\u73b0\u8c61\u3002", "conclusion": "CASTILLO\u4e3a\u9884\u6d4b\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u4e86\u6a21\u578b\u751f\u6210\u884c\u4e3a\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u8bed\u8a00\u6a21\u578b\u4e0e\u7cfb\u7edf\u7814\u7a76\u7684\u4ea4\u53c9\u53d1\u5c55\u3002"}}
{"id": "2505.16763", "pdf": "https://arxiv.org/pdf/2505.16763", "abs": "https://arxiv.org/abs/2505.16763", "authors": ["Hongji Yang", "Yucheng Zhou", "Wencheng Han", "Jianbing Shen"], "title": "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image models are powerful for producing high-quality images based on\ngiven text prompts, but crafting these prompts often requires specialized\nvocabulary. To address this, existing methods train rewriting models with\nsupervision from large amounts of manually annotated data and trained aesthetic\nassessment models. To alleviate the dependence on data scale for model training\nand the biases introduced by trained models, we propose a novel prompt\noptimization framework, designed to rephrase a simple user prompt into a\nsophisticated prompt to a text-to-image model. Specifically, we employ the\nlarge vision language models (LVLMs) as the solver to rewrite the user prompt,\nand concurrently, employ LVLMs as a reward model to score the aesthetics and\nalignment of the images generated by the optimized prompt. Instead of laborious\nhuman feedback, we exploit the prior knowledge of the LVLM to provide rewards,\ni.e., AI feedback. Simultaneously, the solver and the reward model are unified\ninto one model and iterated in reinforcement learning to achieve\nself-improvement by giving a solution and judging itself. Results on two\npopular datasets demonstrate that our method outperforms other strong\ncompetitors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7AI\u53cd\u9988\u800c\u975e\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6765\u4f18\u5316\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u63d0\u793a\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u5b58\u5728\u6570\u636e\u89c4\u6a21\u4f9d\u8d56\u548c\u6a21\u578b\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4f4e\u504f\u89c1\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528LVLM\u4f5c\u4e3a\u63d0\u793a\u91cd\u5199\u5668\u548c\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u8fed\u4ee3\u4f18\u5316\uff0c\u65e0\u9700\u4eba\u5de5\u53cd\u9988\u3002", "result": "\u5728\u4e24\u4e2a\u6d41\u884c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f3a\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u63d0\u793a\u4f18\u5316\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16894", "pdf": "https://arxiv.org/pdf/2505.16894", "abs": "https://arxiv.org/abs/2505.16894", "authors": ["Zeyu Wei", "Shuo Wang", "Xiaohui Rong", "Xuemin Liu", "He Li"], "title": "Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Hallucinations -- plausible yet erroneous outputs -- remain a critical\nbarrier to reliable deployment of large language models (LLMs). We present the\nfirst systematic study linking hallucination incidence to internal-state drift\ninduced by incremental context injection. Using TruthfulQA, we construct two\n16-round \"titration\" tracks per question: one appends relevant but partially\nflawed snippets, the other injects deliberately misleading content. Across six\nopen-source LLMs, we track overt hallucination rates with a tri-perspective\ndetector and covert dynamics via cosine, entropy, JS and Spearman drifts of\nhidden states and attention maps. Results reveal (1) monotonic growth of\nhallucination frequency and representation drift that plateaus after 5--7\nrounds; (2) relevant context drives deeper semantic assimilation, producing\nhigh-confidence \"self-consistent\" hallucinations, whereas irrelevant context\ninduces topic-drift errors anchored by attention re-routing; and (3)\nconvergence of JS-Drift ($\\sim0.69$) and Spearman-Drift ($\\sim0$) marks an\n\"attention-locking\" threshold beyond which hallucinations solidify and become\nresistant to correction. Correlation analyses expose a seesaw between\nassimilation capacity and attention diffusion, clarifying size-dependent error\nmodes. These findings supply empirical foundations for intrinsic hallucination\nprediction and context-aware mitigation mechanisms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u5e7b\u89c9\u73b0\u8c61\u4e0e\u5185\u90e8\u72b6\u6001\u6f02\u79fb\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u4e86\u5e7b\u89c9\u9891\u7387\u4e0e\u4e0a\u4e0b\u6587\u6ce8\u5165\u7684\u5173\u8054\u53ca\u5176\u52a8\u6001\u7279\u5f81\u3002", "motivation": "\u5e7b\u89c9\uff08\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u8f93\u51fa\uff09\u662fLLM\u53ef\u9760\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u5176\u4e0e\u5185\u90e8\u72b6\u6001\u6f02\u79fb\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528TruthfulQA\u6784\u5efa\u4e24\u79cd16\u8f6e\u201c\u6ef4\u5b9a\u201d\u5b9e\u9a8c\uff0c\u8ffd\u8e2a\u516d\u79cd\u5f00\u6e90LLM\u7684\u5e7b\u89c9\u7387\u548c\u9690\u85cf\u72b6\u6001\u52a8\u6001\u3002", "result": "\u53d1\u73b0\u5e7b\u89c9\u9891\u7387\u968f\u4e0a\u4e0b\u6587\u6ce8\u5165\u5355\u8c03\u589e\u957f\uff0c\u4e14\u76f8\u5173\u4e0e\u65e0\u5173\u4e0a\u4e0b\u6587\u5bfc\u81f4\u4e0d\u540c\u7c7b\u578b\u7684\u9519\u8bef\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5e7b\u89c9\u9884\u6d4b\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7f13\u89e3\u673a\u5236\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2505.16770", "pdf": "https://arxiv.org/pdf/2505.16770", "abs": "https://arxiv.org/abs/2505.16770", "authors": ["Meng-Hao Guo", "Xuanyu Chu", "Qianrui Yang", "Zhe-Han Mo", "Yiqing Shen", "Pei-lin Li", "Xinjie Lin", "Jinnian Zhang", "Xin-Sheng Chen", "Yi Zhang", "Kiyohiro Nakayama", "Zhengyang Geng", "Houwen Peng", "Han Hu", "Shi-Nin Hu"], "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multi-modal Outputs", "categories": ["cs.CV"], "comment": "12 pages", "summary": "The rapid advancement of native multi-modal models and omni-models,\nexemplified by GPT-4o, Gemini, and o3, with their capability to process and\ngenerate content across modalities such as text and images, marks a significant\nmilestone in the evolution of intelligence. Systematic evaluation of their\nmulti-modal output capabilities in visual thinking processes (also known as\nmulti-modal chain of thought, M-CoT) becomes critically important. However,\nexisting benchmarks for evaluating multi-modal models primarily focus on\nassessing multi-modal inputs and text-only reasoning while neglecting the\nimportance of reasoning through multi-modal outputs. In this paper, we present\na benchmark, dubbed RBench-V, designed to assess models' vision-indispensable\nreasoning abilities. To construct RBench-V, we carefully hand-pick 803\nquestions covering math, physics, counting, and games. Unlike previous\nbenchmarks that typically specify certain input modalities, RBench-V presents\nproblems centered on multi-modal outputs, which require image manipulation such\nas generating novel images and constructing auxiliary lines to support the\nreasoning process. We evaluate numerous open- and closed-source models on\nRBench-V, including o3, Gemini 2.5 Pro, Qwen2.5-VL, etc. Even the\nbest-performing model, o3, achieves only 25.8% accuracy on RBench-V, far below\nthe human score of 82.3%, highlighting that current models struggle to leverage\nmulti-modal reasoning. Data and code are available at\nhttps://evalmodels.github.io/rbenchv", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRBench-V\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u8f93\u51fa\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u8f93\u5165\u548c\u7eaf\u6587\u672c\u63a8\u7406\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u6a21\u6001\u8f93\u51fa\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u6311\u9009803\u4e2a\u6db5\u76d6\u6570\u5b66\u3001\u7269\u7406\u3001\u8ba1\u6570\u548c\u6e38\u620f\u7684\u95ee\u9898\uff0c\u6784\u5efaRBench-V\u57fa\u51c6\uff0c\u8981\u6c42\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u64cd\u4f5c\u4ee5\u652f\u6301\u63a8\u7406\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578bo3\uff0c\u51c6\u786e\u7387\u4ec5\u4e3a25.8%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768482.3%\u3002", "conclusion": "\u5f53\u524d\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u6709\u663e\u8457\u4e0d\u8db3\uff0cRBench-V\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.16900", "pdf": "https://arxiv.org/pdf/2505.16900", "abs": "https://arxiv.org/abs/2505.16900", "authors": ["Jintian Shao", "Hongyi Huang", "Jiayi Wu", "Beiwen Zhang", "ZhiYu Wu", "You Shan", "MingKai Zheng"], "title": "Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "During the finetuning stage of text generation tasks, standard cross-entropy\nloss treats all tokens equally. This can lead models to overemphasize\nhigh-frequency, low-information tokens, neglecting lower-frequency tokens\ncrucial for specificity and informativeness in generated content. This paper\nintroduces a novel loss function, Power-Law Decay Loss (PDL), specifically\ndesigned to optimize the finetuning process for text generation. The core\nmotivation for PDL stems from observations in information theory and\nlinguistics: the informativeness of a token is often inversely proportional to\nits frequency of occurrence. PDL re-weights the contribution of each token in\nthe standard cross-entropy loss based on its frequency in the training corpus,\nfollowing a power-law decay. Specifically, the weights for high-frequency\ntokens are reduced, while low-frequency, information-dense tokens are assigned\nhigher weights. This mechanism guides the model during finetuning to focus more\non learning and generating tokens that convey specific and unique information,\nthereby enhancing the quality, diversity, and informativeness of the generated\ntext. We theoretically elaborate on the motivation and construction of PDL and\ndiscuss its potential applications and advantages across various text\ngeneration finetuning tasks, such as abstractive summarization, dialogue\nsystems, and style transfer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570Power-Law Decay Loss (PDL)\uff0c\u7528\u4e8e\u4f18\u5316\u6587\u672c\u751f\u6210\u4efb\u52a1\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u9ad8\u9891\u548c\u4f4e\u9891\u8bcd\u7684\u91cd\u8981\u6027\uff0c\u63d0\u5347\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u5728\u5fae\u8c03\u9636\u6bb5\u5bf9\u6240\u6709\u8bcd\u5143\u5e73\u7b49\u5bf9\u5f85\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8\u9ad8\u9891\u4f4e\u4fe1\u606f\u8bcd\u5143\uff0c\u800c\u5ffd\u89c6\u4f4e\u9891\u9ad8\u4fe1\u606f\u8bcd\u5143\u3002PDL\u7684\u52a8\u673a\u6e90\u4e8e\u4fe1\u606f\u8bba\u548c\u8bed\u8a00\u5b66\u89c2\u5bdf\uff1a\u8bcd\u5143\u7684\u4fe1\u606f\u91cf\u901a\u5e38\u4e0e\u5176\u51fa\u73b0\u9891\u7387\u6210\u53cd\u6bd4\u3002", "method": "PDL\u57fa\u4e8e\u8bcd\u5143\u5728\u8bad\u7ec3\u8bed\u6599\u4e2d\u7684\u9891\u7387\uff0c\u6309\u5e42\u5f8b\u8870\u51cf\u91cd\u65b0\u52a0\u6743\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u4e2d\u6bcf\u4e2a\u8bcd\u5143\u7684\u8d21\u732e\uff0c\u51cf\u5c11\u9ad8\u9891\u8bcd\u6743\u91cd\uff0c\u589e\u52a0\u4f4e\u9891\u8bcd\u6743\u91cd\u3002", "result": "PDL\u5f15\u5bfc\u6a21\u578b\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u66f4\u5173\u6ce8\u5b66\u4e60\u751f\u6210\u5177\u6709\u7279\u5b9a\u548c\u72ec\u7279\u4fe1\u606f\u7684\u8bcd\u5143\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u4fe1\u606f\u91cf\u3002", "conclusion": "PDL\u5728\u62bd\u8c61\u6458\u8981\u3001\u5bf9\u8bdd\u7cfb\u7edf\u548c\u98ce\u683c\u8f6c\u6362\u7b49\u591a\u79cd\u6587\u672c\u751f\u6210\u5fae\u8c03\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u548c\u4f18\u52bf\u3002"}}
{"id": "2505.16773", "pdf": "https://arxiv.org/pdf/2505.16773", "abs": "https://arxiv.org/abs/2505.16773", "authors": ["Iv\u00e1n Matas", "Carmen Serrano", "Miguel Nogales", "David Moreno", "Lara Ferr\u00e1ndiz", "Teresa Ojeda", "Bego\u00f1a Acha"], "title": "Mitigating Overfitting in Medical Imaging: Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 2 tables, 2 figures", "summary": "Deep learning has transformed computer vision but relies heavily on large\nlabeled datasets and computational resources. Transfer learning, particularly\nfine-tuning pretrained models, offers a practical alternative; however, models\npretrained on natural image datasets such as ImageNet may fail to capture\ndomain-specific characteristics in medical imaging. This study introduces an\nunsupervised learning framework that extracts high-value dermatological\nfeatures instead of relying solely on ImageNet-based pretraining. We employ a\nVariational Autoencoder (VAE) trained from scratch on a proprietary\ndermatological dataset, allowing the model to learn a structured and clinically\nrelevant latent space. This self-supervised feature extractor is then compared\nto an ImageNet-pretrained backbone under identical classification conditions,\nhighlighting the trade-offs between general-purpose and domain-specific\npretraining. Our results reveal distinct learning patterns. The self-supervised\nmodel achieves a final validation loss of 0.110 (-33.33%), while the\nImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.\nAccuracy trends confirm this: the self-supervised model improves from 45% to\n65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained\nmodel reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting\ngap increasing to +0.060. These findings suggest that while ImageNet\npretraining accelerates convergence, it also amplifies overfitting on\nnon-clinically relevant features. In contrast, self-supervised learning\nachieves steady improvements, stronger generalization, and superior\nadaptability, underscoring the importance of domain-specific feature extraction\nin medical imaging.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u53d6\u76ae\u80a4\u75c5\u5b66\u7279\u5f81\uff0c\u4f18\u4e8e\u57fa\u4e8eImageNet\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u9886\u57df\u7279\u5b9a\u7279\u5f81\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5728\u76ae\u80a4\u75c5\u6570\u636e\u96c6\u4e0a\u4ece\u5934\u8bad\u7ec3\uff0c\u751f\u6210\u7ed3\u6784\u5316\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u81ea\u76d1\u7763\u6a21\u578b\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e33.33%\uff0c\u51c6\u786e\u7387\u63d0\u534744.44%\uff0c\u6cdb\u5316\u80fd\u529b\u4f18\u4e8eImageNet\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u7279\u5f81\u63d0\u53d6\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2505.16922", "pdf": "https://arxiv.org/pdf/2505.16922", "abs": "https://arxiv.org/abs/2505.16922", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Dong Yu", "Nigel Collier", "Deqing Yang"], "title": "UNCLE: Uncertainty Expressions in Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are prone to hallucination, particularly in\nlong-form generations. A promising direction to mitigate hallucination is to\nteach LLMs to express uncertainty explicitly when they lack sufficient\nknowledge. However, existing work lacks direct and fair evaluation of LLMs'\nability to express uncertainty effectively in long-form generation. To address\nthis gap, we first introduce UNCLE, a benchmark designed to evaluate\nuncertainty expression in both long- and short-form question answering (QA).\nUNCLE spans five domains and comprises 4k long-form QA instances and over 20k\nshort-form QA pairs. Our dataset is the first to directly bridge short- and\nlong-form QA with paired questions and gold-standard answers. Along with the\nbenchmark, we propose a suite of new metrics to assess the models' capabilities\nto selectively express uncertainty. Using UNCLE, we then demonstrate that\ncurrent models fail to convey uncertainty appropriately in long-form\ngeneration. We further explore both prompt-based and training-based methods to\nimprove models' performance, with the training-based methods yielding greater\ngains. Further analysis of alignment gaps between short- and long-form\nuncertainty expression highlights promising directions for future research\nusing UNCLE.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86UNCLE\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u77ed\u95ee\u7b54\u4e2d\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u957f\u95ee\u7b54\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63a2\u7d22\u4e86\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u5176\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u80fd\u529b\u7684\u76f4\u63a5\u8bc4\u4f30\u3002", "method": "\u63d0\u51faUNCLE\u57fa\u51c6\uff0c\u5305\u542b\u957f\u77ed\u95ee\u7b54\u6570\u636e\u96c6\u548c\u65b0\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5c1d\u8bd5\u63d0\u793a\u548c\u8bad\u7ec3\u65b9\u6cd5\u6539\u8fdb\u6a21\u578b\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u957f\u95ee\u7b54\u4e2d\u672a\u80fd\u6709\u6548\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\uff0c\u8bad\u7ec3\u65b9\u6cd5\u6539\u8fdb\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "UNCLE\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u63ed\u793a\u4e86\u957f\u77ed\u95ee\u7b54\u4e2d\u4e0d\u786e\u5b9a\u6027\u8868\u8fbe\u7684\u5dee\u5f02\u3002"}}
{"id": "2505.16778", "pdf": "https://arxiv.org/pdf/2505.16778", "abs": "https://arxiv.org/abs/2505.16778", "authors": ["Xianing Chen", "Si Huo", "Borui Jiang", "Hailin Hu", "Xinghao Chen"], "title": "Single Domain Generalization for Few-Shot Counting via Universal Representation Matching", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Few-shot counting estimates the number of target objects in an image using\nonly a few annotated exemplars. However, domain shift severely hinders existing\nmethods to generalize to unseen scenarios. This falls into the realm of single\ndomain generalization that remains unexplored in few-shot counting. To solve\nthis problem, we begin by analyzing the main limitations of current methods,\nwhich typically follow a standard pipeline that extract the object prototypes\nfrom exemplars and then match them with image feature to construct the\ncorrelation map. We argue that existing methods overlook the significance of\nlearning highly generalized prototypes. Building on this insight, we propose\nthe first single domain generalization few-shot counting model, Universal\nRepresentation Matching, termed URM. Our primary contribution is the discovery\nthat incorporating universal vision-language representations distilled from a\nlarge scale pretrained vision-language model into the correlation construction\nprocess substantially improves robustness to domain shifts without compromising\nin domain performance. As a result, URM achieves state-of-the-art performance\non both in domain and the newly introduced domain generalization setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aURM\u7684\u5c11\u6837\u672c\u8ba1\u6570\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u8868\u793a\u6765\u89e3\u51b3\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u8ba1\u6570\u65b9\u6cd5\u5728\u9886\u57df\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faURM\u6a21\u578b\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u8868\u793a\u6765\u6784\u5efa\u76f8\u5173\u6027\u56fe\uff0c\u63d0\u5347\u6cdb\u5316\u6027\u3002", "result": "URM\u5728\u9886\u57df\u5185\u548c\u65b0\u9886\u57df\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u901a\u7528\u8868\u793a\uff0cURM\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u8ba1\u6570\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.16927", "pdf": "https://arxiv.org/pdf/2505.16927", "abs": "https://arxiv.org/abs/2505.16927", "authors": ["Keshav Ramji", "Tahira Naseem", "Ram\u00f3n Fernandez Astudillo"], "title": "Latent Principle Discovery for Language Model Self-Improvement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "When language model (LM) users aim to improve the quality of its generations,\nit is crucial to specify concrete behavioral attributes that the model should\nstrive to reflect. However, curating such principles across many domains, even\nnon-exhaustively, requires a labor-intensive annotation process. To automate\nthis process, we propose eliciting these latent attributes guiding model\nreasoning towards human-preferred responses by explicitly modeling them in a\nself-correction setting. Our approach mines new principles from the LM itself\nand compresses the discovered elements to an interpretable set via clustering.\nSpecifically, we employ an approximation of posterior-regularized Monte Carlo\nExpectation-Maximization to both identify a condensed set of the most effective\nlatent principles and teach the LM to strategically invoke them in order to\nintrinsically refine its responses. We demonstrate that bootstrapping our\nalgorithm over multiple iterations enables smaller language models (7-8B\nparameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an\naverage of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on\nIFEval. We also show that clustering the principles yields interpretable and\ndiverse model-generated constitutions while retaining model performance. The\ngains our method achieves highlight the potential of automated,\nprinciple-driven post-training recipes toward continual self-improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6821\u6b63\u8bbe\u7f6e\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u6f5c\u5728\u884c\u4e3a\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u538b\u7f29\u4e3a\u53ef\u89e3\u91ca\u7684\u96c6\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8d28\u91cf\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u884c\u4e3a\u5c5e\u6027\uff0c\u4f46\u8fd9\u4e00\u8fc7\u7a0b\u8017\u65f6\u8d39\u529b\u3002\u7814\u7a76\u65e8\u5728\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u91c7\u7528\u540e\u9a8c\u6b63\u5219\u5316\u7684\u8499\u7279\u5361\u6d1b\u671f\u671b\u6700\u5927\u5316\u65b9\u6cd5\uff0c\u4ece\u6a21\u578b\u4e2d\u6316\u6398\u6f5c\u5728\u884c\u4e3a\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u538b\u7f29\u4e3a\u53ef\u89e3\u91ca\u96c6\u5408\uff0c\u6307\u5bfc\u6a21\u578b\u81ea\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4f7f\u8f83\u5c0f\u6a21\u578b\uff087-8B\u53c2\u6570\uff09\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cAlpacaEval\u80dc\u7387\u63d0\u9ad88-10%\uff0cMT-Bench\u5e73\u5747\u63d0\u53470.3\uff0cIFEval\u539f\u5219\u9075\u5faa\u80dc\u7387\u63d0\u9ad819-23%\u3002", "conclusion": "\u81ea\u52a8\u5316\u63d0\u53d6\u548c\u538b\u7f29\u884c\u4e3a\u5c5e\u6027\u7684\u65b9\u6cd5\u6709\u6548\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2505.16784", "pdf": "https://arxiv.org/pdf/2505.16784", "abs": "https://arxiv.org/abs/2505.16784", "authors": ["Jun Xie", "Xiongjun Guan", "Yingjian Zhu", "Zhaoran Zhao", "Xinming Wang", "Feng Chen", "Zhepeng Wang"], "title": "Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present the runner-up solution for the Ego4D EgoSchema\nChallenge at CVPR 2025 (Confirmed on May 20, 2025). Inspired by the success of\nlarge models, we evaluate and leverage leading accessible multimodal large\nmodels and adapt them to video understanding tasks via few-shot learning and\nmodel ensemble strategies. Specifically, diversified prompt styles and process\nparadigms are systematically explored and evaluated to effectively guide the\nattention of large models, fully unleashing their powerful generalization and\nadaptability abilities. Experimental results demonstrate that, with our\ncarefully designed approach, directly utilizing an individual multimodal model\nalready outperforms the previous state-of-the-art (SOTA) method which includes\nseveral additional processes. Besides, an additional stage is further\nintroduced that facilitates the cooperation and ensemble of periodic results,\nwhich achieves impressive performance improvements. We hope this work serves as\na valuable reference for the practical application of large models and inspires\nfuture research in the field.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CVPR 2025 Ego4D EgoSchema\u6311\u6218\u8d5b\u7684\u4e9a\u519b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u548c\u6a21\u578b\u96c6\u6210\u7b56\u7565\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u63d0\u5347\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u53d7\u5927\u6a21\u578b\u6210\u529f\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u89e3\u51b3\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5c11\u6837\u672c\u5b66\u4e60\u548c\u6a21\u578b\u96c6\u6210\u7b56\u7565\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6837\u5316\u63d0\u793a\u98ce\u683c\u548c\u5904\u7406\u8303\u5f0f\u4ee5\u5f15\u5bfc\u5927\u6a21\u578b\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4e2a\u591a\u6a21\u6001\u6a21\u578b\u5df2\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u5f15\u5165\u7ed3\u679c\u534f\u4f5c\u4e0e\u96c6\u6210\u9636\u6bb5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u4e3a\u5927\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\uff0c\u5e76\u542f\u53d1\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2505.16931", "pdf": "https://arxiv.org/pdf/2505.16931", "abs": "https://arxiv.org/abs/2505.16931", "authors": ["Matthew Zent", "Digory Smith", "Simon Woodhead"], "title": "PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues", "categories": ["cs.CL"], "comment": "6 pages, 2 figures, submitted to EMNLP 2025, for associated dataset,\n  see\n  https://huggingface.co/datasets/Eedi/Question-Anchored-Tutoring-Dialogues-2k", "summary": "Personally identifiable information (PII) anonymization is a high-stakes task\nthat poses a barrier to many open-science data sharing initiatives. While PII\nidentification has made large strides in recent years, in practice, error\nthresholds and the recall/precision trade-off still limit the uptake of these\nanonymization pipelines. We present PIIvot, a lighter-weight framework for PII\nanonymization that leverages knowledge of the data context to simplify the PII\ndetection problem. To demonstrate its effectiveness, we also contribute\nQATD-2k, the largest open-source real-world tutoring dataset of its kind, to\nsupport the demand for quality educational dialogue data.", "AI": {"tldr": "PIIvot\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6570\u636e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7b80\u5316PII\u533f\u540d\u5316\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86QATD-2k\u6570\u636e\u96c6\u4ee5\u652f\u6301\u6559\u80b2\u5bf9\u8bdd\u6570\u636e\u9700\u6c42\u3002", "motivation": "PII\u533f\u540d\u5316\u662f\u5f00\u653e\u79d1\u5b66\u6570\u636e\u5171\u4eab\u7684\u91cd\u8981\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8bef\u5dee\u9608\u503c\u548c\u53ec\u56de/\u7cbe\u786e\u5ea6\u6743\u8861\u4e0a\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u63d0\u51faPIIvot\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7b80\u5316PII\u68c0\u6d4b\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86PIIvot\u6846\u67b6\uff0c\u5e76\u8d21\u732e\u4e86QATD-2k\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u540c\u7c7b\u4e2d\u6700\u5927\u7684\u5f00\u6e90\u6559\u80b2\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "conclusion": "PIIvot\u4e3aPII\u533f\u540d\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u652f\u6301\u6559\u80b2\u6570\u636e\u5171\u4eab\u9700\u6c42\u3002"}}
{"id": "2505.16792", "pdf": "https://arxiv.org/pdf/2505.16792", "abs": "https://arxiv.org/abs/2505.16792", "authors": ["Ziqiao Wang", "Wangbo Zhao", "Yuhao Zhou", "Zekai Li", "Zhiyuan Liang", "Mingjia Shi", "Xuanlei Zhao", "Pengfei Zhou", "Kaipeng Zhang", "Zhangyang Wang", "Kai Wang", "Yang You"], "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training", "categories": ["cs.CV", "cs.AI"], "comment": "24 pages", "summary": "Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet\ntheir training remains notoriously slow. A recent remedy -- representation\nalignment (REPA) that matches DiT hidden features to those of a non-generative\nteacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus\nor even degrades performance later. We trace this failure to a capacity\nmismatch: once the generative student begins modelling the joint data\ndistribution, the teacher's lower-dimensional embeddings and attention patterns\nbecome a straitjacket rather than a guide. We then introduce HASTE (Holistic\nAlignment with Stage-wise Termination for Efficient training), a two-phase\nschedule that keeps the help and drops the hindrance. Phase I applies a\nholistic alignment loss that simultaneously distills attention maps (relational\npriors) and feature projections (semantic anchors) from the teacher into\nmid-level layers of the DiT, yielding rapid convergence. Phase II then performs\none-shot termination that deactivates the alignment loss, once a simple trigger\nsuch as a fixed iteration is hit, freeing the DiT to focus on denoising and\nexploit its generative capacity. HASTE speeds up training of diverse DiTs\nwithout architecture changes. On ImageNet 256X256, it reaches the vanilla\nSiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,\namounting to a 28X reduction in optimization steps. HASTE also improves\ntext-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled\nrecipe for efficient diffusion training across various tasks. Our code is\navailable at https://github.com/NUS-HPC-AI-Lab/HASTE .", "AI": {"tldr": "HASTE\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e9\u671f\u5bf9\u9f50\u6559\u5e08\u6a21\u578b\u7279\u5f81\u548c\u6ce8\u610f\u529b\u56fe\u52a0\u901fDiT\u8bad\u7ec3\uff0c\u540e\u671f\u7ec8\u6b62\u5bf9\u9f50\u4ee5\u91ca\u653e\u751f\u6210\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u89e3\u51b3DiT\u8bad\u7ec3\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u540e\u671f\u6027\u80fd\u4e0b\u964d\u7684\u7f3a\u9677\u3002", "method": "HASTE\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a\u9636\u6bb5\u4e00\u901a\u8fc7\u5168\u9762\u5bf9\u9f50\u635f\u5931\u5c06\u6559\u5e08\u6a21\u578b\u7684\u6ce8\u610f\u529b\u56fe\u548c\u7279\u5f81\u6295\u5f71\u84b8\u998f\u5230DiT\u4e2d\uff1b\u9636\u6bb5\u4e8c\u5728\u89e6\u53d1\u6761\u4ef6\u6ee1\u8db3\u65f6\u7ec8\u6b62\u5bf9\u9f50\u635f\u5931\uff0c\u4e13\u6ce8\u4e8e\u53bb\u566a\u4efb\u52a1\u3002", "result": "\u5728ImageNet 256x256\u4e0a\uff0cHASTE\u4ec5\u752850\u8f6e\u8fbe\u5230\u57fa\u51c6FID\uff0c500\u8f6e\u5339\u914dREPA\u6700\u4f73FID\uff0c\u8bad\u7ec3\u6b65\u9aa4\u51cf\u5c1128\u500d\u3002", "conclusion": "HASTE\u662f\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2505.16934", "pdf": "https://arxiv.org/pdf/2505.16934", "abs": "https://arxiv.org/abs/2505.16934", "authors": ["Yepeng Liu", "Xuandong Zhao", "Christopher Kruegel", "Dawn Song", "Yuheng Bu"], "title": "In-Context Watermarks for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The growing use of large language models (LLMs) for sensitive applications\nhas highlighted the need for effective watermarking techniques to ensure the\nprovenance and accountability of AI-generated text. However, most existing\nwatermarking methods require access to the decoding process, limiting their\napplicability in real-world settings. One illustrative example is the use of\nLLMs by dishonest reviewers in the context of academic peer review, where\nconference organizers have no access to the model used but still need to detect\nAI-generated reviews. Motivated by this gap, we introduce In-Context\nWatermarking (ICW), which embeds watermarks into generated text solely through\nprompt engineering, leveraging LLMs' in-context learning and\ninstruction-following abilities. We investigate four ICW strategies at\ndifferent levels of granularity, each paired with a tailored detection method.\nWe further examine the Indirect Prompt Injection (IPI) setting as a specific\ncase study, in which watermarking is covertly triggered by modifying input\ndocuments such as academic manuscripts. Our experiments validate the\nfeasibility of ICW as a model-agnostic, practical watermarking approach.\nMoreover, our findings suggest that as LLMs become more capable, ICW offers a\npromising direction for scalable and accessible content attribution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIn-Context Watermarking (ICW)\u7684\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5728\u751f\u6210\u6587\u672c\u4e2d\u5d4c\u5165\u6c34\u5370\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bbf\u95ee\u89e3\u7801\u8fc7\u7a0b\u7684\u9650\u5236\u3002", "motivation": "\u7531\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u654f\u611f\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u9700\u8981\u6709\u6548\u7684\u6c34\u5370\u6280\u672f\u6765\u786e\u4fddAI\u751f\u6210\u6587\u672c\u7684\u6765\u6e90\u548c\u53ef\u8ffd\u6eaf\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u9700\u8981\u8bbf\u95ee\u89e3\u7801\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86ICW\u6280\u672f\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5229\u7528LLMs\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u5d4c\u5165\u6c34\u5370\uff0c\u5e76\u7814\u7a76\u4e86\u56db\u79cd\u4e0d\u540c\u7c92\u5ea6\u7684ICW\u7b56\u7565\u53ca\u5176\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86ICW\u4f5c\u4e3a\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5b9e\u7528\u6c34\u5370\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8868\u660e\u968f\u7740LLMs\u80fd\u529b\u7684\u63d0\u5347\uff0cICW\u6709\u671b\u6210\u4e3a\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u5185\u5bb9\u6eaf\u6e90\u65b9\u6848\u3002", "conclusion": "ICW\u4e3a\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u7684\u6c34\u5370\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b66\u672f\u8bc4\u5ba1\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2505.16793", "pdf": "https://arxiv.org/pdf/2505.16793", "abs": "https://arxiv.org/abs/2505.16793", "authors": ["Xiang Li", "Yong Tao", "Siyuan Zhang", "Siwei Liu", "Zhitong Xiong", "Chunbo Luo", "Lu Liu", "Mykola Pechenizkiy", "Xiao Xiang Zhu", "Tianjin Huang"], "title": "REOBench: Benchmarking Robustness of Earth Observation Foundation Models", "categories": ["cs.CV"], "comment": "24 pages", "summary": "Earth observation foundation models have shown strong generalization across\nmultiple Earth observation tasks, but their robustness under real-world\nperturbations remains underexplored. To bridge this gap, we introduce REOBench,\nthe first comprehensive benchmark for evaluating the robustness of Earth\nobservation foundation models across six tasks and twelve types of image\ncorruptions, including both appearance-based and geometric perturbations. To\nensure realistic and fine-grained evaluation, our benchmark focuses on\nhigh-resolution optical remote sensing images, which are widely used in\ncritical applications such as urban planning and disaster response. We conduct\na systematic evaluation of a broad range of models trained using masked image\nmodeling, contrastive learning, and vision-language pre-training paradigms. Our\nresults reveal that (1) existing Earth observation foundation models experience\nsignificant performance degradation when exposed to input corruptions. (2) The\nseverity of degradation varies across tasks, model architectures, backbone\nsizes, and types of corruption, with performance drop varying from less than 1%\nto over 20%. (3) Vision-language models show enhanced robustness, particularly\nin multimodal tasks. REOBench underscores the vulnerability of current Earth\nobservation foundation models to real-world corruptions and provides actionable\ninsights for developing more robust and reliable models.", "AI": {"tldr": "REOBench\u662f\u9996\u4e2a\u8bc4\u4f30\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u516d\u9879\u4efb\u52a1\u548c\u5341\u4e8c\u79cd\u56fe\u50cf\u6270\u52a8\u4e0b\u9c81\u68d2\u6027\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0cREOBench\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u5149\u5b66\u9065\u611f\u56fe\u50cf\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u591a\u79cd\u6a21\u578b\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u9c81\u68d2\u3002", "conclusion": "REOBench\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2505.16956", "pdf": "https://arxiv.org/pdf/2505.16956", "abs": "https://arxiv.org/abs/2505.16956", "authors": ["Daniil Gurgurov", "Michal Gregor", "Josef van Genabith", "Simon Ostermann"], "title": "On Multilingual Encoder Language Model Compression for Low-Resource Languages", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "In this paper, we combine two-step knowledge distillation, structured\npruning, truncation, and vocabulary trimming for extremely compressing\nmultilingual encoder-only language models for low-resource languages. Our novel\napproach systematically combines existing techniques and takes them to the\nextreme, reducing layer depth, feed-forward hidden size, and intermediate layer\nembedding size to create significantly smaller monolingual models while\nretaining essential language-specific knowledge. We achieve compression rates\nof up to 92% with only a marginal performance drop of 2-10% in four downstream\ntasks, including sentiment analysis, topic classification, named entity\nrecognition, and part-of-speech tagging, across three low-resource languages.\nNotably, the performance degradation correlates with the amount of\nlanguage-specific data in the teacher model, with larger datasets resulting in\nsmaller performance losses. Additionally, we conduct extensive ablation studies\nto identify best practices for multilingual model compression using these\ntechniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u6b65\u77e5\u8bc6\u84b8\u998f\u3001\u7ed3\u6784\u5316\u526a\u679d\u3001\u622a\u65ad\u548c\u8bcd\u6c47\u4fee\u526a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29\u591a\u8bed\u8a00\u7f16\u7801\u5668\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u9700\u6c42\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ed3\u5408\u73b0\u6709\u6280\u672f\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u4f53\u79ef\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u8bed\u8a00\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408\u4e24\u6b65\u77e5\u8bc6\u84b8\u998f\u3001\u7ed3\u6784\u5316\u526a\u679d\u3001\u622a\u65ad\u548c\u8bcd\u6c47\u4fee\u526a\uff0c\u51cf\u5c11\u5c42\u6df1\u5ea6\u3001\u524d\u9988\u9690\u85cf\u5c42\u5927\u5c0f\u548c\u4e2d\u95f4\u5c42\u5d4c\u5165\u5927\u5c0f\u3002", "result": "\u538b\u7f29\u7387\u9ad8\u8fbe92%\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u4ec5\u4e3a2-10%\uff0c\u4e14\u6027\u80fd\u635f\u5931\u4e0e\u6559\u5e08\u6a21\u578b\u4e2d\u8bed\u8a00\u7279\u5b9a\u6570\u636e\u91cf\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u8df5\uff0c\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u3002"}}
{"id": "2505.16797", "pdf": "https://arxiv.org/pdf/2505.16797", "abs": "https://arxiv.org/abs/2505.16797", "authors": ["Hanyue Lou", "Jinxiu Liang", "Minggui Teng", "Yi Wang", "Boxin Shi"], "title": "V2V: Scaling Event-Based Vision through Efficient Video-to-Voxel Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Event-based cameras offer unique advantages such as high temporal resolution,\nhigh dynamic range, and low power consumption. However, the massive storage\nrequirements and I/O burdens of existing synthetic data generation pipelines\nand the scarcity of real data prevent event-based training datasets from\nscaling up, limiting the development and generalization capabilities of event\nvision models. To address this challenge, we introduce Video-to-Voxel (V2V), an\napproach that directly converts conventional video frames into event-based\nvoxel grid representations, bypassing the storage-intensive event stream\ngeneration entirely. V2V enables a 150 times reduction in storage requirements\nwhile supporting on-the-fly parameter randomization for enhanced model\nrobustness. Leveraging this efficiency, we train several video reconstruction\nand optical flow estimation model architectures on 10,000 diverse videos\ntotaling 52 hours--an order of magnitude larger than existing event datasets,\nyielding substantial improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faVideo-to-Voxel (V2V)\u65b9\u6cd5\uff0c\u5c06\u4f20\u7edf\u89c6\u9891\u5e27\u76f4\u63a5\u8f6c\u6362\u4e3a\u4e8b\u4ef6\u4f53\u7d20\u7f51\u683c\u8868\u793a\uff0c\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u9700\u6c42\u5e76\u63d0\u5347\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5b58\u50a8\u9700\u6c42\u9ad8\u4e14\u771f\u5b9e\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u4e8b\u4ef6\u89c6\u89c9\u6a21\u578b\u7684\u5f00\u53d1\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7V2V\u65b9\u6cd5\uff0c\u7ed5\u8fc7\u4e8b\u4ef6\u6d41\u751f\u6210\uff0c\u76f4\u63a5\u8f6c\u6362\u89c6\u9891\u5e27\u4e3a\u4e8b\u4ef6\u4f53\u7d20\u7f51\u683c\uff0c\u5b58\u50a8\u9700\u6c42\u964d\u4f4e150\u500d\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u53c2\u6570\u968f\u673a\u5316\u3002", "result": "\u572810,000\u4e2a\u89c6\u9891\uff08\u603b\u8ba152\u5c0f\u65f6\uff09\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u6570\u636e\u89c4\u6a21\u8fdc\u8d85\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u91cd\u5efa\u548c\u5149\u6d41\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "V2V\u65b9\u6cd5\u89e3\u51b3\u4e86\u4e8b\u4ef6\u6570\u636e\u5b58\u50a8\u548c\u89c4\u6a21\u95ee\u9898\uff0c\u4e3a\u4e8b\u4ef6\u89c6\u89c9\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16965", "pdf": "https://arxiv.org/pdf/2505.16965", "abs": "https://arxiv.org/abs/2505.16965", "authors": ["Fengyi Li", "Kayhan Behdin", "Natesh Pillai", "Xiaofeng Wang", "Zhipeng Wang", "Ercan Yildiz"], "title": "BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text segmentation based on the semantic meaning of sentences is a fundamental\ntask with broad utility in many downstream applications. In this paper, we\npropose a graphical model-based unsupervised learning approach, named BP-Seg\nfor efficient text segmentation. Our method not only considers local coherence,\ncapturing the intuition that adjacent sentences are often more related, but\nalso effectively groups sentences that are distant in the text yet semantically\nsimilar. This is achieved through belief propagation on the carefully\nconstructed graphical models. Experimental results on both an illustrative\nexample and a dataset with long-form documents demonstrate that our method\nperforms favorably compared to competing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6a21\u578b\u7684\u975e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5BP-Seg\uff0c\u7528\u4e8e\u9ad8\u6548\u6587\u672c\u5206\u5272\uff0c\u540c\u65f6\u8003\u8651\u5c40\u90e8\u8fde\u8d2f\u6027\u548c\u8fdc\u8ddd\u79bb\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "motivation": "\u57fa\u4e8e\u53e5\u5b50\u8bed\u4e49\u7684\u6587\u672c\u5206\u5272\u662f\u8bb8\u591a\u4e0b\u6e38\u5e94\u7528\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u56fe\u6a21\u578b\u4e0a\u4f7f\u7528\u4fe1\u5ff5\u4f20\u64ad\uff0c\u540c\u65f6\u6355\u6349\u76f8\u90bb\u53e5\u5b50\u7684\u5c40\u90e8\u8fde\u8d2f\u6027\u548c\u8fdc\u8ddd\u79bb\u53e5\u5b50\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u793a\u4f8b\u548c\u957f\u6587\u6863\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "BP-Seg\u662f\u4e00\u79cd\u6709\u6548\u7684\u6587\u672c\u5206\u5272\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2505.16805", "pdf": "https://arxiv.org/pdf/2505.16805", "abs": "https://arxiv.org/abs/2505.16805", "authors": ["Xuesong Chen", "Linjiang Huang", "Tao Ma", "Rongyao Fang", "Shaoshuai Shi", "Hongsheng Li"], "title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "The integration of Vision-Language Models (VLMs) into autonomous driving\nsystems has shown promise in addressing key challenges such as learning\ncomplexity, interpretability, and common-sense reasoning. However, existing\napproaches often struggle with efficient integration and realtime\ndecision-making due to computational demands. In this paper, we introduce\nSOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E)\nmodels to enhance autonomous vehicle planning. Our approach emphasizes\nknowledge sharing at the feature level through a shared visual encoder,\nenabling comprehensive interaction between VLM and E2E components. We propose a\nTrajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines\ntrajectory predictions, reducing uncertainty and improving accuracy. By\nemploying a temporal decoupling strategy, SOLVE achieves efficient cooperation\nby aligning high-quality VLM outputs with E2E real-time performance. Evaluated\non the nuScenes dataset, our method demonstrates significant improvements in\ntrajectory prediction accuracy, paving the way for more robust and reliable\nautonomous driving systems.", "AI": {"tldr": "SOLVE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u7aef\u5230\u7aef\uff08E2E\uff09\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u80fd\u529b\uff0c\u91c7\u7528\u8f68\u8ff9\u94fe\u5f0f\u601d\u7ef4\uff08T-CoT\uff09\u548c\u7279\u5f81\u7ea7\u77e5\u8bc6\u5171\u4eab\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9ad8\u6548\u96c6\u6210\u548c\u5b9e\u65f6\u51b3\u7b56\u65b9\u9762\u5b58\u5728\u8ba1\u7b97\u9700\u6c42\u9ad8\u7684\u6311\u6218\uff0cSOLVE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u89c6\u89c9\u7f16\u7801\u5668\u5b9e\u73b0\u7279\u5f81\u7ea7\u77e5\u8bc6\u5171\u4eab\uff0c\u91c7\u7528T-CoT\u9010\u6b65\u4f18\u5316\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u89e3\u8026\u7b56\u7565\u534f\u8c03VLM\u548cE2E\u6a21\u578b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cSOLVE\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SOLVE\u4e3a\u66f4\u7a33\u5065\u53ef\u9760\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16972", "pdf": "https://arxiv.org/pdf/2505.16972", "abs": "https://arxiv.org/abs/2505.16972", "authors": ["Tianduo Wang", "Lu Xu", "Wei Lu", "Shanbo Cheng"], "title": "From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in Automatic Speech Recognition (ASR) have been largely\nfueled by massive speech corpora. However, extending coverage to diverse\nlanguages with limited resources remains a formidable challenge. This paper\nintroduces Speech Back-Translation, a scalable pipeline that improves\nmultilingual ASR models by converting large-scale text corpora into synthetic\nspeech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just\ntens of hours of real transcribed speech can effectively train TTS models to\ngenerate synthetic speech at hundreds of times the original volume while\nmaintaining high quality. To evaluate synthetic speech quality, we develop an\nintelligibility-based assessment framework and establish clear thresholds for\nwhen synthetic data benefits ASR training. Using Speech Back-Translation, we\ngenerate more than 500,000 hours of synthetic speech in ten languages and\ncontinue pre-training Whisper-large-v3, achieving average transcription error\nreductions of over 30\\%. These results highlight the scalability and\neffectiveness of Speech Back-Translation for enhancing multilingual ASR\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpeech Back-Translation\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u5e93\u8f6c\u6362\u4e3a\u5408\u6210\u8bed\u97f3\u6765\u63d0\u5347\u591a\u8bed\u8a00ASR\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4ec5\u9700\u5c11\u91cf\u771f\u5b9e\u8bed\u97f3\u5373\u53ef\u751f\u6210\u5927\u91cf\u9ad8\u8d28\u91cf\u5408\u6210\u8bed\u97f3\uff0c\u663e\u8457\u964d\u4f4e\u8f6c\u5f55\u9519\u8bef\u3002", "motivation": "\u591a\u8bed\u8a00ASR\u7cfb\u7edf\u5728\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u4e2d\u6269\u5c55\u8986\u76d6\u8303\u56f4\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u73b0\u6709\u6587\u672c\u8d44\u6e90\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u7684TTS\u6a21\u578b\u5c06\u5927\u89c4\u6a21\u6587\u672c\u8bed\u6599\u5e93\u8f6c\u6362\u4e3a\u5408\u6210\u8bed\u97f3\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u6027\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5408\u6210\u8bed\u97f3\u8d28\u91cf\u3002", "result": "\u5728\u5341\u79cd\u8bed\u8a00\u4e2d\u751f\u6210\u4e86\u8d85\u8fc750\u4e07\u5c0f\u65f6\u7684\u5408\u6210\u8bed\u97f3\uff0cWhisper-large-v3\u7684\u8f6c\u5f55\u9519\u8bef\u5e73\u5747\u964d\u4f4e\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "Speech Back-Translation\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00ASR\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16809", "pdf": "https://arxiv.org/pdf/2505.16809", "abs": "https://arxiv.org/abs/2505.16809", "authors": ["Junze Wang", "Lei Fan", "Weipeng Jing", "Donglin Di", "Yang Song", "Sidong Liu", "Cong Cong"], "title": "Hypergraph Tversky-Aware Domain Incremental Learning for Brain Tumor Segmentation with Missing Modalities", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Existing methods for multimodal MRI segmentation with missing modalities\ntypically assume that all MRI modalities are available during training.\nHowever, in clinical practice, some modalities may be missing due to the\nsequential nature of MRI acquisition, leading to performance degradation.\nFurthermore, retraining models to accommodate newly available modalities can be\ninefficient and may cause overfitting, potentially compromising previously\nlearned knowledge. To address these challenges, we propose Replay-based\nHypergraph Domain Incremental Learning (ReHyDIL) for brain tumor segmentation\nwith missing modalities. ReHyDIL leverages Domain Incremental Learning (DIL) to\nenable the segmentation model to learn from newly acquired MRI modalities\nwithout forgetting previously learned information. To enhance segmentation\nperformance across diverse patient scenarios, we introduce the Cross-Patient\nHypergraph Segmentation Network (CHSNet), which utilizes hypergraphs to capture\nhigh-order associations between patients. Additionally, we incorporate\nTversky-Aware Contrastive (TAC) loss to effectively mitigate information\nimbalance both across and within different modalities. Extensive experiments on\nthe BraTS2019 dataset demonstrate that ReHyDIL outperforms state-of-the-art\nmethods, achieving an improvement of over 2\\% in the Dice Similarity\nCoefficient across various tumor regions. Our code is available at ReHyDIL.", "AI": {"tldr": "\u63d0\u51faReHyDIL\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u589e\u91cf\u5b66\u4e60\u548c\u8d85\u56fe\u7f51\u7edc\u89e3\u51b3\u591a\u6a21\u6001MRI\u5206\u5272\u4e2d\u7f3a\u5931\u6a21\u6001\u7684\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2dMRI\u6a21\u6001\u53ef\u80fd\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u65f6\u6240\u6709\u6a21\u6001\u53ef\u7528\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6548\u7387\u4f4e\u4e14\u6613\u8fc7\u62df\u5408\u3002", "method": "\u7ed3\u5408\u9886\u57df\u589e\u91cf\u5b66\u4e60\uff08DIL\uff09\u548c\u8d85\u56fe\u7f51\u7edc\uff08CHSNet\uff09\uff0c\u5f15\u5165Tversky-Aware Contrastive\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728BraTS2019\u6570\u636e\u96c6\u4e0a\uff0cDice\u76f8\u4f3c\u7cfb\u6570\u63d0\u5347\u8d85\u8fc72%\u3002", "conclusion": "ReHyDIL\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u7f3a\u5931\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.16973", "pdf": "https://arxiv.org/pdf/2505.16973", "abs": "https://arxiv.org/abs/2505.16973", "authors": ["Rishanth Rajendhran", "Amir Zadeh", "Matthew Sarte", "Chuan Li", "Mohit Iyyer"], "title": "VeriFastScore: Speeding up long-form factuality evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Metrics like FactScore and VeriScore that evaluate long-form factuality\noperate by decomposing an input response into atomic claims and then\nindividually verifying each claim. While effective and interpretable, these\nmethods incur numerous LLM calls and can take upwards of 100 seconds to\nevaluate a single response, limiting their practicality in large-scale\nevaluation and training scenarios. To address this, we propose VeriFastScore,\nwhich leverages synthetic data to fine-tune Llama3.1 8B for simultaneously\nextracting and verifying all verifiable claims within a given text based on\nevidence from Google Search. We show that this task cannot be solved via\nfew-shot prompting with closed LLMs due to its complexity: the model receives\n~4K tokens of evidence on average and needs to concurrently decompose claims,\njudge their verifiability, and verify them against noisy evidence. However, our\nfine-tuned VeriFastScore model demonstrates strong correlation with the\noriginal VeriScore pipeline at both the example level (r=0.80) and system level\n(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence\nretrieval) over VeriScore. To facilitate future factuality research, we\npublicly release our VeriFastScore model and synthetic datasets.", "AI": {"tldr": "VeriFastScore\u901a\u8fc7\u5fae\u8c03Llama3.1 8B\u6a21\u578b\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u548cGoogle\u641c\u7d22\u8bc1\u636e\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u6548\u7684\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\uff0c\u901f\u5ea6\u63d0\u53476.6\u500d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982FactScore\u548cVeriScore\uff09\u867d\u6709\u6548\u4f46\u8017\u65f6\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u548c\u8bad\u7ec3\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u5408\u6210\u6570\u636e\u5fae\u8c03Llama3.1 8B\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u53d6\u548c\u9a8c\u8bc1\u6587\u672c\u4e2d\u7684\u6240\u6709\u53ef\u9a8c\u8bc1\u58f0\u660e\u3002", "result": "VeriFastScore\u4e0e\u539f\u65b9\u6cd5\u5728\u793a\u4f8b\u548c\u7cfb\u7edf\u7ea7\u522b\u4e0a\u76f8\u5173\u6027\u9ad8\uff08r=0.80\u548cr=0.94\uff09\uff0c\u901f\u5ea6\u63d0\u53476.6\u500d\u3002", "conclusion": "VeriFastScore\u4e3a\u4e8b\u5b9e\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2505.16811", "pdf": "https://arxiv.org/pdf/2505.16811", "abs": "https://arxiv.org/abs/2505.16811", "authors": ["Shangquan Sun", "Wenqi Ren", "Juxiang Zhou", "Shu Wang", "Jianhou Gan", "Xiaochun Cao"], "title": "Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining", "categories": ["cs.CV"], "comment": "11 Pages, 8 figures, CVPR 2025 Oral Presentation", "summary": "Significant progress has been made in video restoration under rainy\nconditions over the past decade, largely propelled by advancements in deep\nlearning. Nevertheless, existing methods that depend on paired data struggle to\ngeneralize effectively to real-world scenarios, primarily due to the disparity\nbetween synthetic and authentic rain effects. To address these limitations, we\npropose a dual-branch spatio-temporal state-space model to enhance rain streak\nremoval in video sequences. Specifically, we design spatial and temporal\nstate-space model layers to extract spatial features and incorporate temporal\ndependencies across frames, respectively. To improve multi-frame feature\nfusion, we derive a dynamic stacking filter, which adaptively approximates\nstatistical filters for superior pixel-wise feature refinement. Moreover, we\ndevelop a median stacking loss to enable semi-supervised learning by generating\npseudo-clean patches based on the sparsity prior of rain. To further explore\nthe capacity of deraining models in supporting other vision-based tasks in\nrainy environments, we introduce a novel real-world benchmark focused on object\ndetection and tracking in rainy conditions. Our method is extensively evaluated\nacross multiple benchmarks containing numerous synthetic and real-world rainy\nvideos, consistently demonstrating its superiority in quantitative metrics,\nvisual quality, efficiency, and its utility for downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u7684\u96e8\u75d5\u53bb\u9664\uff0c\u901a\u8fc7\u52a8\u6001\u5806\u53e0\u6ee4\u6ce2\u5668\u548c\u534a\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u56e0\u5408\u6210\u4e0e\u771f\u5b9e\u96e8\u6548\u5dee\u5f02\u5927\u3002", "method": "\u8bbe\u8ba1\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5c42\u63d0\u53d6\u7279\u5f81\uff0c\u52a8\u6001\u5806\u53e0\u6ee4\u6ce2\u5668\u4f18\u5316\u591a\u5e27\u878d\u5408\uff0c\u534a\u76d1\u7763\u5b66\u4e60\u751f\u6210\u4f2a\u5e72\u51c0\u8865\u4e01\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u96e8\u5929\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9a\u91cf\u6307\u6807\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u5b9e\u7528\u6027\u5747\u9886\u5148\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u96e8\u5929\u89c6\u9891\u4fee\u590d\u6548\u679c\uff0c\u5e76\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u652f\u6301\u3002"}}
{"id": "2505.16983", "pdf": "https://arxiv.org/pdf/2505.16983", "abs": "https://arxiv.org/abs/2505.16983", "authors": ["Junlong Tong", "Jinlan Fu", "Zixuan Lin", "Yingqi Fan", "Anhao Zhao", "Hui Su", "Xiaoyu Shen"], "title": "LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are primarily designed for batch processing.\nExisting methods for adapting LLMs to streaming rely either on expensive\nre-encoding or specialized architectures with limited scalability. This work\nidentifies three key mismatches in adapting batch-oriented LLMs to streaming:\n(1) input-attention, (2) output-attention, and (3) position-ID mismatches.\nWhile it is commonly assumed that the latter two mismatches require frequent\nre-encoding, our analysis reveals that only the input-attention mismatch\nsignificantly impacts performance, indicating re-encoding outputs is largely\nunnecessary. To better understand this discrepancy with the common assumption,\nwe provide the first comprehensive analysis of the impact of position encoding\non LLMs in streaming, showing that preserving relative positions within source\nand target contexts is more critical than maintaining absolute order. Motivated\nby the above analysis, we introduce a group position encoding paradigm built on\nbatch architectures to enhance consistency between streaming and batch modes.\nExtensive experiments on cross-lingual and cross-modal tasks demonstrate that\nour method outperforms existing approaches. Our method requires no\narchitectural modifications, exhibits strong generalization in both streaming\nand batch modes. The code is available at repository\nhttps://github.com/EIT-NLP/StreamingLLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6279\u5904\u7406\u67b6\u6784\u7684\u7ec4\u4f4d\u7f6e\u7f16\u7801\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6d41\u5f0f\u5904\u7406\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5c06LLMs\u9002\u5e94\u6d41\u5f0f\u5904\u7406\u65f6\u4f9d\u8d56\u6602\u8d35\u7684\u91cd\u65b0\u7f16\u7801\u6216\u4e13\u7528\u67b6\u6784\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8f93\u5165-\u6ce8\u610f\u529b\u3001\u8f93\u51fa-\u6ce8\u610f\u529b\u548c\u4f4d\u7f6e-ID\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4f4d\u7f6e\u7f16\u7801\u5bf9LLMs\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7ec4\u4f4d\u7f6e\u7f16\u7801\u8303\u5f0f\uff0c\u4fdd\u6301\u6e90\u548c\u76ee\u6807\u4e0a\u4e0b\u6587\u4e2d\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u6d41\u5f0f\u548c\u6279\u5904\u7406\u6a21\u5f0f\u4e0b\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u7ec4\u4f4d\u7f6e\u7f16\u7801\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u6d41\u5f0f\u5904\u7406\u4e2d\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.16815", "pdf": "https://arxiv.org/pdf/2505.16815", "abs": "https://arxiv.org/abs/2505.16815", "authors": ["Chunyi Li", "Jiaohao Xiao", "Jianbo Zhang", "Farong Wen", "Zicheng Zhang", "Yuan Tian", "Xiangyang Zhu", "Xiaohong Liu", "Zhengxue Cheng", "Weisi Lin", "Guangtao Zhai"], "title": "Perceptual Quality Assessment for Embodied AI", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Embodied AI has developed rapidly in recent years, but it is still mainly\ndeployed in laboratories, with various distortions in the Real-world limiting\nits application. Traditionally, Image Quality Assessment (IQA) methods are\napplied to predict human preferences for distorted images; however, there is no\nIQA method to assess the usability of an image in embodied tasks, namely, the\nperceptual quality for robots. To provide accurate and reliable quality\nindicators for future embodied scenarios, we first propose the topic: IQA for\nEmbodied AI. Specifically, we (1) based on the Mertonian system and\nmeta-cognitive theory, constructed a perception-cognition-decision-execution\npipeline and defined a comprehensive subjective score collection process; (2)\nestablished the Embodied-IQA database, containing over 36k reference/distorted\nimage pairs, with more than 5m fine-grained annotations provided by Vision\nLanguage Models/Vision Language Action-models/Real-world robots; (3) trained\nand validated the performance of mainstream IQA methods on Embodied-IQA,\ndemonstrating the need to develop more accurate quality indicators for Embodied\nAI. We sincerely hope that through evaluation, we can promote the application\nof Embodied AI under complex distortions in the Real-world. Project page:\nhttps://github.com/lcysyzxdxc/EmbodiedIQA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9762\u5411\u5177\u8eabAI\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u56fe\u50cf\u8d28\u91cf\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b36k\u56fe\u50cf\u5bf9\u548c5m\u6807\u6ce8\u7684\u6570\u636e\u5e93\u3002", "motivation": "\u5177\u8eabAI\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u56fe\u50cf\u8d28\u91cf\uff0c\u4f20\u7edfIQA\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u673a\u5668\u4eba\u611f\u77e5\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eMertonian\u7cfb\u7edf\u548c\u5143\u8ba4\u77e5\u7406\u8bba\uff0c\u6784\u5efa\u4e86\u611f\u77e5-\u8ba4\u77e5-\u51b3\u7b56-\u6267\u884c\u6d41\u7a0b\uff0c\u5e76\u5efa\u7acb\u4e86Embodied-IQA\u6570\u636e\u5e93\uff0c\u9a8c\u8bc1\u4e86\u4e3b\u6d41IQA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709IQA\u65b9\u6cd5\u5728\u5177\u8eabAI\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u901a\u8fc7\u8bc4\u4f30\u5177\u8eabAI\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u6709\u671b\u63a8\u52a8\u5176\u5728\u590d\u6742\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2505.16986", "pdf": "https://arxiv.org/pdf/2505.16986", "abs": "https://arxiv.org/abs/2505.16986", "authors": ["Amartya Chakraborty", "Paresh Dashore", "Nadia Bathaee", "Anmol Jain", "Anirban Das", "Shi-Xiong Zhang", "Sambit Sahu", "Milind Naphade", "Genta Indra Winata"], "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-source language models. We present results powered by\nT1-Agent, highlighting their ability to plan and reason in complex,\ntool-dependent scenarios.", "AI": {"tldr": "T1\u662f\u4e00\u4e2a\u591a\u9886\u57df\u3001\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u5de5\u5177\u8c03\u7528\u4f9d\u8d56\u5173\u7cfb\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5904\u7406\u5de5\u5177\u8c03\u7528\u4f9d\u8d56\u5173\u7cfb\u7684\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u548c\u8bc4\u4f30\u3002", "method": "\u5f15\u5165T1\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u9886\u57df\u5de5\u5177\u4f9d\u8d56\u573a\u666f\uff0c\u652f\u6301\u7f13\u5b58\u673a\u5236\u548c\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u3002", "result": "T1-Agent\u5c55\u793a\u4e86\u5728\u590d\u6742\u5de5\u5177\u4f9d\u8d56\u573a\u666f\u4e2d\u7684\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "T1\u4e3a\u5de5\u5177\u4f7f\u7528\u548c\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5e76\u53ef\u4f5c\u4e3a\u5f00\u6e90\u6a21\u578b\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2505.16819", "pdf": "https://arxiv.org/pdf/2505.16819", "abs": "https://arxiv.org/abs/2505.16819", "authors": ["Taewon Kang", "Ming C. Lin"], "title": "Action2Dialogue: Generating Character-Centric Narratives from Scene-Level Prompts", "categories": ["cs.CV"], "comment": "18 pages, 5 figures", "summary": "Recent advances in scene-based video generation have enabled systems to\nsynthesize coherent visual narratives from structured prompts. However, a\ncrucial dimension of storytelling -- character-driven dialogue and speech --\nremains underexplored. In this paper, we present a modular pipeline that\ntransforms action-level prompts into visually and auditorily grounded narrative\ndialogue, enriching visual storytelling with natural voice and character\nexpression. Our method takes as input a pair of prompts per scene, where the\nfirst defines the setting and the second specifies a character's behavior.\nWhile a story generation model such as Text2Story generates the corresponding\nvisual scene, we focus on generating expressive character utterances from these\nprompts and the scene image. We apply a pretrained vision-language encoder to\nextract a high-level semantic feature from the representative frame, capturing\nsalient visual context. This feature is then combined with the structured\nprompts and used to guide a large language model in synthesizing natural,\ncharacter-consistent dialogue. To ensure contextual consistency across scenes,\nwe introduce a Recursive Narrative Bank that conditions each dialogue\ngeneration on the accumulated dialogue history from prior scenes. This approach\nenables characters to speak in ways that reflect their evolving goals and\ninteractions throughout a story. Finally, we render each utterance as\nexpressive, character-consistent speech, resulting in fully-voiced video\nnarratives. Our framework requires no additional training and demonstrates\napplicability across a variety of story settings, from fantasy adventures to\nslice-of-life episodes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u5c06\u52a8\u4f5c\u7ea7\u63d0\u793a\u8f6c\u5316\u4e3a\u89c6\u89c9\u548c\u542c\u89c9\u57fa\u7840\u53d9\u4e8b\u5bf9\u8bdd\uff0c\u589e\u5f3a\u89c6\u89c9\u53d9\u4e8b\u7684\u8868\u73b0\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u573a\u666f\u7684\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u5728\u89d2\u8272\u5bf9\u8bdd\u548c\u8bed\u97f3\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u7f16\u7801\u5668\u63d0\u53d6\u573a\u666f\u8bed\u4e49\u7279\u5f81\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u89d2\u8272\u4e00\u81f4\u7684\u5bf9\u8bdd\uff0c\u5e76\u901a\u8fc7\u9012\u5f52\u53d9\u4e8b\u94f6\u884c\u786e\u4fdd\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u3002", "result": "\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u89d2\u8272\u4e00\u81f4\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u8bed\u97f3\u53d9\u4e8b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6545\u4e8b\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u89c6\u89c9\u53d9\u4e8b\u4e0e\u89d2\u8272\u5bf9\u8bdd\u7ed3\u5408\uff0c\u63d0\u5347\u4e86\u6545\u4e8b\u7684\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u3002"}}
{"id": "2505.16988", "pdf": "https://arxiv.org/pdf/2505.16988", "abs": "https://arxiv.org/abs/2505.16988", "authors": ["Rui Ye", "Keduan Huang", "Qimin Wu", "Yuzhu Cai", "Tian Jin", "Xianghe Pang", "Xiangrui Liu", "Jiaqi Su", "Chen Qian", "Bohan Tang", "Kaiqu Liang", "Jiaao Chen", "Yue Hu", "Zhenfei Yin", "Rongye Shi", "Bo An", "Yang Gao", "Wenjun Wu", "Lei Bai", "Siheng Chen"], "title": "MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "18 pages, 11 figures", "summary": "LLM-based multi-agent systems (MAS) have demonstrated significant potential\nin enhancing single LLMs to address complex and diverse tasks in practical\napplications. Despite considerable advancements, the field lacks a unified\ncodebase that consolidates existing methods, resulting in redundant\nre-implementation efforts, unfair comparisons, and high entry barriers for\nresearchers. To address these challenges, we introduce MASLab, a unified,\ncomprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab\nintegrates over 20 established methods across multiple domains, each rigorously\nvalidated by comparing step-by-step outputs with its official implementation.\n(2) MASLab provides a unified environment with various benchmarks for fair\ncomparisons among methods, ensuring consistent inputs and standardized\nevaluation protocols. (3) MASLab implements methods within a shared streamlined\nstructure, lowering the barriers for understanding and extension. Building on\nMASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,\noffering researchers a clear and comprehensive view of the current landscape of\nMAS methods. MASLab will continue to evolve, tracking the latest developments\nin the field, and invite contributions from the broader open-source community.", "AI": {"tldr": "MASLab\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7801\u5e93\uff0c\u6574\u5408\u4e8620\u591a\u79cdLLM-based\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u516c\u5e73\u6bd4\u8f83\u548c\u4f4e\u95e8\u69db\u7814\u7a76\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9886\u57df\u7f3a\u4e4f\u7edf\u4e00\u4ee3\u7801\u5e93\u5bfc\u81f4\u7684\u5197\u4f59\u5b9e\u73b0\u3001\u4e0d\u516c\u5e73\u6bd4\u8f83\u548c\u9ad8\u7814\u7a76\u95e8\u69db\u95ee\u9898\u3002", "method": "\u6574\u5408\u5e76\u9a8c\u8bc1\u591a\u79cd\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7edf\u4e00\u73af\u5883\u548c\u5171\u4eab\u7ed3\u6784\uff0c\u652f\u6301\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "result": "\u901a\u8fc710+\u57fa\u51c6\u548c8\u6a21\u578b\u5b9e\u9a8c\uff0c\u5168\u9762\u5c55\u793a\u4e86\u5f53\u524dMAS\u65b9\u6cd5\u7684\u53d1\u5c55\u73b0\u72b6\u3002", "conclusion": "MASLab\u5c06\u6301\u7eed\u66f4\u65b0\uff0c\u5e76\u6b22\u8fce\u5f00\u6e90\u793e\u533a\u8d21\u732e\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.16836", "pdf": "https://arxiv.org/pdf/2505.16836", "abs": "https://arxiv.org/abs/2505.16836", "authors": ["Fanrui Zhang", "Dian Li", "Qiang Zhang", "Chenjun", "sinbadliu", "Junxiong Lin", "Jiahong Yan", "Jiawei Liu", "Zheng-Jun Zha"], "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "28 pages, 27 figures", "summary": "The rapid spread of multimodal misinformation on social media has raised\ngrowing concerns, while research on video misinformation detection remains\nlimited due to the lack of large-scale, diverse datasets. Existing methods\noften overfit to rigid templates and lack deep reasoning over deceptive\ncontent. To address these challenges, we introduce FakeVV, a large-scale\nbenchmark comprising over 100,000 video-text pairs with fine-grained,\ninterpretable annotations. In addition, we further propose Fact-R1, a novel\nframework that integrates deep reasoning with collaborative rule-based\nreinforcement learning. Fact-R1 is trained through a three-stage process: (1)\nmisinformation long-Chain-of-Thought (CoT) instruction tuning, (2) preference\nalignment via Direct Preference Optimization (DPO), and (3) Group Relative\nPolicy Optimization (GRPO) using a novel verifiable reward function. This\nenables Fact-R1 to exhibit emergent reasoning behaviors comparable to those\nobserved in advanced text-based reinforcement learning systems, but in the more\ncomplex multimodal misinformation setting. Our work establishes a new paradigm\nfor misinformation detection, bridging large-scale video understanding,\nreasoning-guided alignment, and interpretable verification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFakeVV\u6570\u636e\u96c6\u548cFact-R1\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u7ed3\u5408\u6df1\u5ea6\u63a8\u7406\u548c\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u5feb\u901f\u4f20\u64ad\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u7f3a\u4e4f\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faFact-R1\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\uff08CoT\u6307\u4ee4\u8c03\u4f18\u3001DPO\u504f\u597d\u5bf9\u9f50\u3001GRPO\u7b56\u7565\u4f18\u5316\uff09\u5b9e\u73b0\u6df1\u5ea6\u63a8\u7406\u3002", "result": "Fact-R1\u5728\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u4e0e\u6587\u672c\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u76f8\u5f53\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u89c6\u9891\u7406\u89e3\u4e0e\u53ef\u89e3\u91ca\u9a8c\u8bc1\u3002"}}
{"id": "2505.16995", "pdf": "https://arxiv.org/pdf/2505.16995", "abs": "https://arxiv.org/abs/2505.16995", "authors": ["Chao Zhang", "Xin Shi", "Xueqiao Zhang", "Yifan Zhu", "Yi Yang", "Yawei Luo"], "title": "DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Emotional Support Conversation (ESC) have improved\nemotional support generation by fine-tuning Large Language Models (LLMs) via\nSupervised Fine-Tuning (SFT). However, common psychological errors still\npersist. While Direct Preference Optimization (DPO) shows promise in reducing\nsuch errors through pairwise preference learning, its effectiveness in ESC\ntasks is limited by two key challenges: (1) Entangled data structure: Existing\nESC data inherently entangles psychological strategies and response content,\nmaking it difficult to construct high-quality preference pairs; and (2)\nOptimization ambiguity: Applying vanilla DPO to such entangled pairwise data\nleads to ambiguous training objectives. To address these issues, we introduce\nInferential Preference Mining (IPM) to construct high-quality preference data,\nforming the IPM-PrefDial dataset. Building upon this data, we propose a\nDecoupled ESC framework inspired by Gross's Extended Process Model of Emotion\nRegulation, which decomposes the ESC task into two sequential subtasks:\nstrategy planning and empathic response generation. Each was trained via SFT\nand subsequently enhanced by DPO to align with the psychological preference.\nExtensive experiments demonstrate that our Decoupled ESC framework outperforms\njoint optimization baselines, reducing preference bias and improving response\nquality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u7684\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\uff08ESC\uff09\u6846\u67b6\uff0c\u901a\u8fc7Inferential Preference Mining\uff08IPM\uff09\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\uff0c\u5e76\u5229\u7528Decoupled ESC\u6846\u67b6\u5206\u6b65\u4f18\u5316\u7b56\u7565\u89c4\u5212\u548c\u5171\u60c5\u56de\u590d\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u6a21\u578b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4f18\u5316\uff0c\u4f46\u4ecd\u5b58\u5728\u5fc3\u7406\u9519\u8bef\uff1b\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u56e0\u6570\u636e\u7ed3\u6784\u548c\u4f18\u5316\u6a21\u7cca\u6027\u53d7\u9650\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faIPM\u6784\u5efa\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\uff08IPM-PrefDial\uff09\uff0c\u5e76\u57fa\u4e8eGross\u7684\u60c5\u7eea\u8c03\u8282\u6269\u5c55\u8fc7\u7a0b\u6a21\u578b\uff0c\u5c06ESC\u4efb\u52a1\u89e3\u8026\u4e3a\u7b56\u7565\u89c4\u5212\u548c\u5171\u60c5\u56de\u590d\u751f\u6210\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u5206\u522b\u901a\u8fc7SFT\u548cDPO\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u89e3\u8026\u6846\u67b6\u4f18\u4e8e\u8054\u5408\u4f18\u5316\u57fa\u7ebf\uff0c\u51cf\u5c11\u4e86\u504f\u597d\u504f\u5dee\u5e76\u63d0\u5347\u4e86\u56de\u590d\u8d28\u91cf\u3002", "conclusion": "\u89e3\u8026\u6846\u67b6\u548cIPM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ESC\u4efb\u52a1\u4e2d\u7684\u504f\u597d\u6570\u636e\u8d28\u91cf\u548c\u4f18\u5316\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u751f\u6210\u6548\u679c\u3002"}}
{"id": "2505.16839", "pdf": "https://arxiv.org/pdf/2505.16839", "abs": "https://arxiv.org/abs/2505.16839", "authors": ["Shufan Li", "Konstantinos Kallidromitis", "Hritik Bansal", "Akash Gokul", "Yusuke Kato", "Kazuki Kozuka", "Jason Kuen", "Zhe Lin", "Kai-Wei Chang", "Aditya Grover"], "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding", "categories": ["cs.CV"], "comment": "25 pages, 8 figures", "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.", "AI": {"tldr": "LaViDa\u662f\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\uff08\u5982LLaVA\uff09\u5728\u63a8\u7406\u901f\u5ea6\u548c\u53ef\u63a7\u751f\u6210\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaVA\uff09\u5728\u63a8\u7406\u901f\u5ea6\u548c\u53ef\u63a7\u751f\u6210\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "LaViDa\u901a\u8fc7\u4e3aDMs\u914d\u5907\u89c6\u89c9\u7f16\u7801\u5668\u5e76\u8054\u5408\u5fae\u8c03\uff0c\u7ed3\u5408\u4e92\u8865\u63a9\u7801\u3001\u524d\u7f00KV\u7f13\u5b58\u548c\u65f6\u95f4\u6b65\u504f\u79fb\u7b49\u6280\u672f\uff0c\u4f18\u5316\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "result": "LaViDa\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\uff0c\u5728COCO\u5b57\u5e55\u4efb\u52a1\u4e2dCIDEr\u5f97\u5206\u63d0\u53474.1\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad81.92\u500d\u3002", "conclusion": "LaViDa\u5c55\u793a\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\uff0c\u662f\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2505.16998", "pdf": "https://arxiv.org/pdf/2505.16998", "abs": "https://arxiv.org/abs/2505.16998", "authors": ["Jin Jiang", "Jianing Wang", "Yuchen Yan", "Yang Liu", "Jianhua Zhu", "Mengdi Zhang", "Xunliang Cai", "Liangcai Gao"], "title": "Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been shown to achieve breakthrough\nperformance on complex logical reasoning tasks. Nevertheless, most existing\nresearch focuses on employing formal language to guide LLMs to derive reliable\nreasoning paths, while systematic evaluations of these capabilities are still\nlimited. In this paper, we aim to conduct a comprehensive evaluation of LLMs\nacross various logical reasoning problems utilizing formal languages. From the\nperspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and\nformat of trajectories, our key findings are: 1) Thinking models significantly\noutperform Instruct models, especially when formal language is employed; 2) All\nLLMs exhibit limitations in inductive reasoning capability, irrespective of\nwhether they use a formal language; 3) Data with PoT format achieves the best\ngeneralization performance across other languages. Additionally, we also curate\nthe formal-relative training data to further enhance the small language models,\nand the experimental results indicate that a simple rejected fine-tuning method\ncan better enable LLMs to generalize across formal languages and achieve the\nbest overall performance. Our codes and reports are available at\nhttps://github.com/jiangjin1999/FormalEval.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u5f62\u5f0f\u8bed\u8a00\u7684\u4f7f\u7528\uff0c\u53d1\u73b0\u601d\u7ef4\u6a21\u578b\u4f18\u4e8e\u6307\u4ee4\u6a21\u578b\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u5f52\u7eb3\u63a8\u7406\u4e0a\u5747\u6709\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7528\u5f62\u5f0f\u8bed\u8a00\u5f15\u5bfcLLMs\u63a8\u7406\uff0c\u4f46\u5bf9\u5176\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4eceLLMs\u8c31\u7cfb\u3001\u4efb\u52a1\u5206\u7c7b\u548c\u8f68\u8ff9\u683c\u5f0f\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u5e76\u5229\u7528\u5f62\u5f0f\u76f8\u5173\u6570\u636e\u589e\u5f3a\u5c0f\u6a21\u578b\u3002", "result": "\u601d\u7ef4\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u5f52\u7eb3\u63a8\u7406\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1bPoT\u683c\u5f0f\u6570\u636e\u6cdb\u5316\u80fd\u529b\u6700\u4f73\u3002", "conclusion": "\u7b80\u5355\u7684\u62d2\u7edd\u5fae\u8c03\u65b9\u6cd5\u80fd\u63d0\u5347LLMs\u5728\u5f62\u5f0f\u8bed\u8a00\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u6700\u4f73\u3002"}}
{"id": "2505.16862", "pdf": "https://arxiv.org/pdf/2505.16862", "abs": "https://arxiv.org/abs/2505.16862", "authors": ["Chaoyang Wang", "Xiangtai Li", "Lu Qi", "Xiaofan Lin", "Jinbin Bai", "Qianyu Zhou", "Yunhai Tong"], "title": "Conditional Panoramic Image Generation via Masked Autoregressive Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Recent progress in panoramic image generation has underscored two critical\nlimitations in existing approaches. First, most methods are built upon\ndiffusion models, which are inherently ill-suited for equirectangular\nprojection (ERP) panoramas due to the violation of the identically and\nindependently distributed (i.i.d.) Gaussian noise assumption caused by their\nspherical mapping. Second, these methods often treat text-conditioned\ngeneration (text-to-panorama) and image-conditioned generation (panorama\noutpainting) as separate tasks, relying on distinct architectures and\ntask-specific data. In this work, we propose a unified framework, Panoramic\nAutoRegressive model (PAR), which leverages masked autoregressive modeling to\naddress these challenges. PAR avoids the i.i.d. assumption constraint and\nintegrates text and image conditioning into a cohesive architecture, enabling\nseamless generation across tasks. To address the inherent discontinuity in\nexisting generative models, we introduce circular padding to enhance spatial\ncoherence and propose a consistency alignment strategy to improve generation\nquality. Extensive experiments demonstrate competitive performance in\ntext-to-image generation and panorama outpainting tasks while showcasing\npromising scalability and generalization capabilities.", "AI": {"tldr": "PAR\u6846\u67b6\u901a\u8fc7\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\u89e3\u51b3\u4e86\u5168\u666f\u56fe\u50cf\u751f\u6210\u4e2d\u7684i.i.d.\u5047\u8bbe\u95ee\u9898\u548c\u4efb\u52a1\u5206\u79bb\u95ee\u9898\uff0c\u7ed3\u5408\u5706\u5f62\u586b\u5145\u548c\u4e00\u81f4\u6027\u5bf9\u9f50\u7b56\u7565\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u6269\u6563\u6a21\u578b\u4e0d\u9002\u7528\u4e8eERP\u5168\u666f\u56fe\u50cf\u4e14\u4efb\u52a1\u5206\u79bb\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faPAR\u6846\u67b6\uff0c\u5229\u7528\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u7ed3\u5408\u5706\u5f62\u586b\u5145\u548c\u4e00\u81f4\u6027\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u5168\u666f\u5916\u7ed8\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PAR\u4e3a\u5168\u666f\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17005", "pdf": "https://arxiv.org/pdf/2505.17005", "abs": "https://arxiv.org/abs/2505.17005", "authors": ["Huatong Song", "Jinhao Jiang", "Wenqing Tian", "Zhipeng Chen", "Yuhuan Wu", "Jiahao Zhao", "Yingqian Min", "Wayne Xin Zhao", "Lei Fang", "Ji-Rong Wen"], "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.", "AI": {"tldr": "R1-Searcher++\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08SFT\u51b7\u542f\u52a8\u548cRL\u52a8\u6001\u77e5\u8bc6\u83b7\u53d6\uff09\u4f7fLLM\u81ea\u9002\u5e94\u5229\u7528\u5185\u5916\u77e5\u8bc6\u6e90\uff0c\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3LLM\u56e0\u9759\u6001\u77e5\u8bc6\u6613\u4ea7\u751f\u5e7b\u89c9\uff0c\u4ee5\u53ca\u73b0\u6709RAG\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u6cdb\u5316\u5dee\u6216\u5ffd\u7565\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1aSFT\u51b7\u542f\u52a8\u5b66\u4e60\u521d\u6b65\u683c\u5f0f\uff0cRL\u52a8\u6001\u77e5\u8bc6\u83b7\u53d6\u9636\u6bb5\u901a\u8fc7\u7ed3\u679c\u76d1\u7763\u3001\u5956\u52b1\u673a\u5236\u548c\u8bb0\u5fc6\u673a\u5236\u4f18\u5316\u5185\u5916\u77e5\u8bc6\u5229\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660eR1-Searcher++\u4f18\u4e8e\u73b0\u6709RAG\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u3002", "conclusion": "R1-Searcher++\u901a\u8fc7\u7ed3\u5408\u5185\u5916\u77e5\u8bc6\u6e90\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.16864", "pdf": "https://arxiv.org/pdf/2505.16864", "abs": "https://arxiv.org/abs/2505.16864", "authors": ["Yuechen Zhang", "Jinbo Xing", "Bin Xia", "Shaoteng Liu", "Bohao Peng", "Xin Tao", "Pengfei Wan", "Eric Lo", "Jiaya Jia"], "title": "Training-Free Efficient Video Generation via Dynamic Token Carving", "categories": ["cs.CV"], "comment": "Project Page: https://julianjuaner.github.io/projects/jenga/ , 24\n  pages", "summary": "Despite the remarkable generation quality of video Diffusion Transformer\n(DiT) models, their practical deployment is severely hindered by extensive\ncomputational requirements. This inefficiency stems from two key challenges:\nthe quadratic complexity of self-attention with respect to token length and the\nmulti-step nature of diffusion models. To address these limitations, we present\nJenga, a novel inference pipeline that combines dynamic attention carving with\nprogressive resolution generation. Our approach leverages two key insights: (1)\nearly denoising steps do not require high-resolution latents, and (2) later\nsteps do not require dense attention. Jenga introduces a block-wise attention\nmechanism that dynamically selects relevant token interactions using 3D\nspace-filling curves, alongside a progressive resolution strategy that\ngradually increases latent resolution during generation. Experimental results\ndemonstrate that Jenga achieves substantial speedups across multiple\nstate-of-the-art video diffusion models while maintaining comparable generation\nquality (8.83$\\times$ speedup with 0.01\\% performance drop on VBench). As a\nplug-and-play solution, Jenga enables practical, high-quality video generation\non modern hardware by reducing inference time from minutes to seconds --\nwithout requiring model retraining. Code:\nhttps://github.com/dvlab-research/Jenga", "AI": {"tldr": "Jenga\u662f\u4e00\u79cd\u65b0\u578b\u63a8\u7406\u7ba1\u9053\uff0c\u901a\u8fc7\u52a8\u6001\u6ce8\u610f\u529b\u96d5\u523b\u548c\u6e10\u8fdb\u5206\u8fa8\u7387\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u53d8\u6362\u5668\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u6027\u548c\u6269\u6563\u6a21\u578b\u7684\u591a\u6b65\u7279\u6027\u3002", "method": "Jenga\u7ed3\u5408\u52a8\u6001\u6ce8\u610f\u529b\u96d5\u523b\uff08\u4f7f\u75283D\u7a7a\u95f4\u586b\u5145\u66f2\u7ebf\u52a8\u6001\u9009\u62e9\u76f8\u5173\u6807\u8bb0\u4ea4\u4e92\uff09\u548c\u6e10\u8fdb\u5206\u8fa8\u7387\u751f\u6210\uff08\u9010\u6b65\u589e\u52a0\u6f5c\u5728\u5206\u8fa8\u7387\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cJenga\u5728\u591a\u4e2a\u5148\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\uff088.83\u500d\uff09\uff0c\u751f\u6210\u8d28\u91cf\u51e0\u4e4e\u65e0\u4e0b\u964d\uff08VBench\u4e0a0.01%\u7684\u6027\u80fd\u635f\u5931\uff09\u3002", "conclusion": "Jenga\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\uff0c\u5373\u53ef\u5c06\u63a8\u7406\u65f6\u95f4\u4ece\u5206\u949f\u7f29\u77ed\u5230\u79d2\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2505.15872", "pdf": "https://arxiv.org/pdf/2505.15872", "abs": "https://arxiv.org/abs/2505.15872", "authors": ["Yunjia Xi", "Jianghao Lin", "Menghui Zhu", "Yongzhao Xiao", "Zhuoying Ou", "Jiaqi Liu", "Tong Wan", "Bo Chen", "Weiwen Liu", "Yasheng Wang", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "title": "InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\ngrounding responses with retrieved information. As an emerging paradigm,\nAgentic RAG further enhances this process by introducing autonomous LLM agents\ninto the information seeking process. However, existing benchmarks fall short\nin evaluating such systems, as they are confined to a static retrieval\nenvironment with a fixed, limited corpus} and simple queries that fail to\nelicit agentic behavior. Moreover, their evaluation protocols assess\ninformation seeking effectiveness by pre-defined gold sets of documents, making\nthem unsuitable for the open-ended and dynamic nature of real-world web\nenvironments. To bridge this gap, we present InfoDeepSeek, a new benchmark with\nchallenging questions designed for assessing agentic information seeking in\nreal-world, dynamic web environments. We propose a systematic methodology for\nconstructing challenging queries satisfying the criteria of determinacy,\ndifficulty, and diversity. Based on this, we develop the first evaluation\nframework tailored to dynamic agentic information seeking, including\nfine-grained metrics about the accuracy, utility, and compactness of\ninformation seeking outcomes. Through extensive experiments across LLMs, search\nengines, and question types, InfoDeepSeek reveals nuanced agent behaviors and\noffers actionable insights for future research.", "AI": {"tldr": "InfoDeepSeek\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u8bc4\u4f30\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u81ea\u4e3bLLM\u4ee3\u7406\u884c\u4e3a\uff0c\u56e0\u5176\u5c40\u9650\u4e8e\u9759\u6001\u68c0\u7d22\u73af\u5883\u548c\u7b80\u5355\u67e5\u8be2\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u6027\u65b9\u6cd5\u6784\u5efa\u5177\u6709\u786e\u5b9a\u6027\u3001\u96be\u5ea6\u548c\u591a\u6837\u6027\u7684\u67e5\u8be2\uff0c\u5e76\u5f00\u53d1\u9996\u4e2a\u52a8\u6001\u4ee3\u7406\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\uff0cInfoDeepSeek\u63ed\u793a\u4e86\u4ee3\u7406\u884c\u4e3a\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002", "conclusion": "InfoDeepSeek\u4e3a\u52a8\u6001\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u4ee3\u7406\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16875", "pdf": "https://arxiv.org/pdf/2505.16875", "abs": "https://arxiv.org/abs/2505.16875", "authors": ["Zhehao Huang", "Yuhang Liu", "Yixin Lou", "Zhengbao He", "Mingzhen He", "Wenxing Zhou", "Tao Li", "Kehan Li", "Zeyi Huang", "Xiaolin Huang"], "title": "T2I-ConBench: Text-to-Image Benchmark for Continual Post-training", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual post-training adapts a single text-to-image diffusion model to\nlearn new tasks without incurring the cost of separate models, but naive\npost-training causes forgetting of pretrained knowledge and undermines\nzero-shot compositionality. We observe that the absence of a standardized\nevaluation protocol hampers related research for continual post-training. To\naddress this, we introduce T2I-ConBench, a unified benchmark for continual\npost-training of text-to-image models. T2I-ConBench focuses on two practical\nscenarios, item customization and domain enhancement, and analyzes four\ndimensions: (1) retention of generality, (2) target-task performance, (3)\ncatastrophic forgetting, and (4) cross-task generalization. It combines\nautomated metrics, human-preference modeling, and vision-language QA for\ncomprehensive assessment. We benchmark ten representative methods across three\nrealistic task sequences and find that no approach excels on all fronts. Even\njoint \"oracle\" training does not succeed for every task, and cross-task\ngeneralization remains unsolved. We release all datasets, code, and evaluation\ntools to accelerate research in continual post-training for text-to-image\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86T2I-ConBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u6301\u7eed\u540e\u8bad\u7ec3\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u56db\u79cd\u7ef4\u5ea6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5341\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5747\u672a\u5168\u9762\u4f18\u79c0\u3002", "motivation": "\u6301\u7eed\u540e\u8bad\u7ec3\u53ef\u4ee5\u907f\u514d\u5355\u72ec\u6a21\u578b\u7684\u6210\u672c\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u963b\u788d\u4e86\u76f8\u5173\u7814\u7a76\u3002", "method": "\u5f15\u5165T2I-ConBench\u57fa\u51c6\uff0c\u7ed3\u5408\u81ea\u52a8\u6307\u6807\u3001\u4eba\u7c7b\u504f\u597d\u5efa\u6a21\u548c\u89c6\u89c9\u8bed\u8a00QA\uff0c\u8bc4\u4f30\u56db\u79cd\u7ef4\u5ea6\u3002", "result": "\u8bc4\u4f30\u5341\u79cd\u65b9\u6cd5\u540e\u53d1\u73b0\uff0c\u6ca1\u6709\u65b9\u6cd5\u5728\u6240\u6709\u65b9\u9762\u8868\u73b0\u4f18\u79c0\uff0c\u8de8\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "conclusion": "T2I-ConBench\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u6301\u7eed\u540e\u8bad\u7ec3\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2505.16882", "pdf": "https://arxiv.org/pdf/2505.16882", "abs": "https://arxiv.org/abs/2505.16882", "authors": ["Isla Duporge", "Sofia Minano", "Nikoloz Sirmpilatze", "Igor Tatarnikov", "Scott Wolf", "Adam L. Tyson", "Daniel Rubenstein"], "title": "Tracking the Flight: Exploring a Computational Framework for Analyzing Escape Responses in Plains Zebra (Equus quagga)", "categories": ["cs.CV"], "comment": "Accepted to the CV4Animals workshop at CVPR 2025", "summary": "Ethological research increasingly benefits from the growing affordability and\naccessibility of drones, which enable the capture of high-resolution footage of\nanimal movement at fine spatial and temporal scales. However, analyzing such\nfootage presents the technical challenge of separating animal movement from\ndrone motion. While non-trivial, computer vision techniques such as image\nregistration and Structure-from-Motion (SfM) offer practical solutions. For\nconservationists, open-source tools that are user-friendly, require minimal\nsetup, and deliver timely results are especially valuable for efficient data\ninterpretation. This study evaluates three approaches: a bioimaging-based\nregistration technique, an SfM pipeline, and a hybrid interpolation method. We\napply these to a recorded escape event involving 44 plains zebras, captured in\na single drone video. Using the best-performing method, we extract individual\ntrajectories and identify key behavioral patterns: increased alignment\n(polarization) during escape, a brief widening of spacing just before stopping,\nand tighter coordination near the group's center. These insights highlight the\nmethod's effectiveness and its potential to scale to larger datasets,\ncontributing to broader investigations of collective animal behavior.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u65e0\u4eba\u673a\u6355\u6349\u52a8\u7269\u8fd0\u52a8\u7684\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5206\u79bb\u52a8\u7269\u8fd0\u52a8\u4e0e\u65e0\u4eba\u673a\u8fd0\u52a8\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\u5728\u6591\u9a6c\u9003\u9038\u4e8b\u4ef6\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u65e0\u4eba\u673a\u6280\u672f\u7684\u666e\u53ca\u4e3a\u52a8\u7269\u884c\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u4f46\u5206\u6790\u65f6\u9700\u8981\u89e3\u51b3\u52a8\u7269\u8fd0\u52a8\u4e0e\u65e0\u4eba\u673a\u8fd0\u52a8\u5206\u79bb\u7684\u6280\u672f\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a\u57fa\u4e8e\u751f\u7269\u6210\u50cf\u7684\u914d\u51c6\u6280\u672f\u3001SfM\u7ba1\u9053\u548c\u6df7\u5408\u63d2\u503c\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e44\u5339\u6591\u9a6c\u7684\u9003\u9038\u4e8b\u4ef6\u89c6\u9891\u3002", "result": "\u6700\u4f73\u65b9\u6cd5\u6210\u529f\u63d0\u53d6\u4e2a\u4f53\u8f68\u8ff9\uff0c\u53d1\u73b0\u9003\u9038\u65f6\u6591\u9a6c\u7fa4\u7684\u5bf9\u9f50\u6027\u589e\u5f3a\u3001\u505c\u6b62\u524d\u95f4\u8ddd\u77ed\u6682\u6269\u5927\u4ee5\u53ca\u4e2d\u5fc3\u533a\u57df\u534f\u8c03\u6027\u66f4\u5f3a\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u53ef\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u7684\u52a8\u7269\u96c6\u4f53\u884c\u4e3a\u7814\u7a76\u3002"}}
{"id": "2505.16902", "pdf": "https://arxiv.org/pdf/2505.16902", "abs": "https://arxiv.org/abs/2505.16902", "authors": ["Junzhe Jiang", "Nan Song", "Jingyu Li", "Xiatian Zhu", "Li Zhang"], "title": "RealEngine: Simulating Autonomous Driving in Realistic Context", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Driving simulation plays a crucial role in developing reliable driving agents\nby providing controlled, evaluative environments. To enable meaningful\nassessments, a high-quality driving simulator must satisfy several key\nrequirements: multi-modal sensing capabilities (e.g., camera and LiDAR) with\nrealistic scene rendering to minimize observational discrepancies; closed-loop\nevaluation to support free-form trajectory behaviors; highly diverse traffic\nscenarios for thorough evaluation; multi-agent cooperation to capture\ninteraction dynamics; and high computational efficiency to ensure affordability\nand scalability. However, existing simulators and benchmarks fail to\ncomprehensively meet these fundamental criteria. To bridge this gap, this paper\nintroduces RealEngine, a novel driving simulation framework that holistically\nintegrates 3D scene reconstruction and novel view synthesis techniques to\nachieve realistic and flexible closed-loop simulation in the driving context.\nBy leveraging real-world multi-modal sensor data, RealEngine reconstructs\nbackground scenes and foreground traffic participants separately, allowing for\nhighly diverse and realistic traffic scenarios through flexible scene\ncomposition. This synergistic fusion of scene reconstruction and view synthesis\nenables photorealistic rendering across multiple sensor modalities, ensuring\nboth perceptual fidelity and geometric accuracy. Building upon this\nenvironment, RealEngine supports three essential driving simulation categories:\nnon-reactive simulation, safety testing, and multi-agent interaction,\ncollectively forming a reliable and comprehensive benchmark for evaluating the\nreal-world performance of driving agents.", "AI": {"tldr": "RealEngine\u662f\u4e00\u4e2a\u65b0\u578b\u9a7e\u9a76\u6a21\u62df\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u54083D\u573a\u666f\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u903c\u771f\u4e14\u7075\u6d3b\u7684\u95ed\u73af\u9a7e\u9a76\u6a21\u62df\u3002", "motivation": "\u73b0\u6709\u6a21\u62df\u5668\u548c\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5168\u9762\u6ee1\u8db3\u9ad8\u8d28\u91cf\u9a7e\u9a76\u6a21\u62df\u7684\u5173\u952e\u9700\u6c42\uff0c\u5982\u591a\u6a21\u6001\u611f\u77e5\u3001\u95ed\u73af\u8bc4\u4f30\u3001\u591a\u6837\u5316\u573a\u666f\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002", "method": "\u5229\u7528\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u5206\u522b\u91cd\u5efa\u80cc\u666f\u573a\u666f\u548c\u524d\u666f\u4ea4\u901a\u53c2\u4e0e\u8005\uff0c\u901a\u8fc7\u7075\u6d3b\u573a\u666f\u7ec4\u5408\u5b9e\u73b0\u9ad8\u591a\u6837\u6027\u548c\u903c\u771f\u6027\u3002", "result": "RealEngine\u652f\u6301\u975e\u53cd\u5e94\u5f0f\u6a21\u62df\u3001\u5b89\u5168\u6d4b\u8bd5\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u5f62\u6210\u5168\u9762\u53ef\u9760\u7684\u9a7e\u9a76\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "RealEngine\u586b\u8865\u4e86\u73b0\u6709\u6a21\u62df\u5668\u7684\u4e0d\u8db3\uff0c\u4e3a\u9a7e\u9a76\u4ee3\u7406\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u903c\u771f\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16915", "pdf": "https://arxiv.org/pdf/2505.16915", "abs": "https://arxiv.org/abs/2505.16915", "authors": ["Qirui Jiao", "Daoyuan Chen", "Yilun Huang", "Xika Lin", "Ying Shen", "Yaliang Li"], "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 8 figures, 10 tables", "summary": "While recent text-to-image (T2I) models show impressive capabilities in\nsynthesizing images from brief descriptions, their performance significantly\ndegrades when confronted with long, detail-intensive prompts required in\nprofessional applications. We present DetailMaster, the first comprehensive\nbenchmark specifically designed to evaluate T2I models' systematical abilities\nto handle extended textual inputs that contain complex compositional\nrequirements. Our benchmark introduces four critical evaluation dimensions:\nCharacter Attributes, Structured Character Locations, Multi-Dimensional Scene\nAttributes, and Explicit Spatial/Interactive Relationships. The benchmark\ncomprises long and detail-rich prompts averaging 284.89 tokens, with high\nquality validated by expert annotators. Evaluation on 7 general-purpose and 5\nlong-prompt-optimized T2I models reveals critical performance limitations:\nstate-of-the-art models achieve merely ~50% accuracy in key dimensions like\nattribute binding and spatial reasoning, while all models showing progressive\nperformance degradation as prompt length increases. Our analysis highlights\nsystemic failures in structural comprehension and detail overload handling,\nmotivating future research into architectures with enhanced compositional\nreasoning. We open-source the dataset, data curation code, and evaluation tools\nto advance detail-rich T2I generation and enable broad applications that would\notherwise be infeasible due to the lack of a dedicated benchmark.", "AI": {"tldr": "DetailMaster\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5904\u7406\u957f\u6587\u672c\u63d0\u793a\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u7ec6\u8282\u548c\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709T2I\u6a21\u578b\u5728\u957f\u6587\u672c\u548c\u7ec6\u8282\u5bc6\u96c6\u578b\u63d0\u793a\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u4e13\u4e1a\u5e94\u7528\u7684\u8bc4\u4f30\u6807\u51c6\u3002", "method": "\u63d0\u51faDetailMaster\u57fa\u51c6\uff0c\u5305\u542b\u56db\u4e2a\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u4f7f\u7528\u5e73\u5747284.89\u4e2atoken\u7684\u957f\u63d0\u793a\uff0c\u8bc4\u4f30\u4e8612\u79cdT2I\u6a21\u578b\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u5c5e\u6027\u7ed1\u5b9a\u548c\u7a7a\u95f4\u63a8\u7406\u7b49\u5173\u952e\u7ef4\u5ea6\u4e0a\u4ec5\u8fbe\u5230\u7ea650%\u51c6\u786e\u7387\uff0c\u4e14\u6027\u80fd\u968f\u63d0\u793a\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7ed3\u6784\u7406\u89e3\u548c\u7ec6\u8282\u5904\u7406\u4e0a\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u547c\u5401\u672a\u6765\u7814\u7a76\u6539\u8fdb\u67b6\u6784\uff0c\u5e76\u5f00\u6e90\u4e86\u6570\u636e\u96c6\u548c\u5de5\u5177\u3002"}}
{"id": "2505.15935", "pdf": "https://arxiv.org/pdf/2505.15935", "abs": "https://arxiv.org/abs/2505.15935", "authors": ["Omer Hofman", "Oren Rachmil", "Shamik Bose", "Vikas Pahuja", "Jonathan Brokman", "Toshiya Shimizu", "Trisha Starostina", "Kelly Marchisio", "Seraphina Goldfarb-Tarrant", "Roman Vainshtein"], "title": "MAPS: A Multilingual Benchmark for Global Agent Performance and Security", "categories": ["cs.DB", "cs.CL", "cs.CR"], "comment": null, "summary": "Agentic AI systems, which build on Large Language Models (LLMs) and interact\nwith tools and memory, have rapidly advanced in capability and scope. Yet,\nsince LLMs have been shown to struggle in multilingual settings, typically\nresulting in lower performance and reduced safety, agentic systems risk\ninheriting these limitations. This raises concerns about the global\naccessibility of such systems, as users interacting in languages other than\nEnglish may encounter unreliable or security-critical agent behavior. Despite\ngrowing interest in evaluating agentic AI, existing benchmarks focus\nexclusively on English, leaving multilingual settings unexplored. To address\nthis gap, we propose MAPS, a multilingual benchmark suite designed to evaluate\nagentic AI systems across diverse languages and tasks. MAPS builds on four\nwidely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code\ngeneration), MATH (mathematical reasoning), and the Agent Security Benchmark\n(security). We translate each dataset into ten diverse languages, resulting in\n805 unique tasks and 8,855 total language-specific instances. Our benchmark\nsuite enables a systematic analysis of how multilingual contexts affect agent\nperformance and robustness. Empirically, we observe consistent degradation in\nboth performance and security when transitioning from English to other\nlanguages, with severity varying by task and correlating with the amount of\ntranslated input. Building on these findings, we provide actionable\nrecommendations to guide agentic AI systems development and assessment under\nmultilingual settings. This work establishes a standardized evaluation\nframework, encouraging future research towards equitable, reliable, and\nglobally accessible agentic AI. MAPS benchmark suite is publicly available at\nhttps://huggingface.co/datasets/Fujitsu-FRE/MAPS", "AI": {"tldr": "MAPS\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ee3\u7406AI\u7cfb\u7edf\u5728\u591a\u79cd\u8bed\u8a00\u548c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u4ec5\u5173\u6ce8\u82f1\u8bed\u7684\u7a7a\u767d\u3002", "motivation": "LLM\u5728\u975e\u82f1\u8bed\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7406AI\u7cfb\u7edf\u7684\u5168\u7403\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u591a\u8bed\u8a00\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u4ee3\u7406\u57fa\u51c6\uff08GAIA\u3001SWE-bench\u3001MATH\u548cAgent Security Benchmark\uff09\uff0c\u5c06\u5176\u7ffb\u8bd1\u4e3a\u5341\u79cd\u8bed\u8a00\uff0c\u6784\u5efa\u5305\u542b805\u4e2a\u4efb\u52a1\u7684MAPS\u5957\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ece\u82f1\u8bed\u5207\u6362\u5230\u5176\u4ed6\u8bed\u8a00\u65f6\uff0c\u4ee3\u7406\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\u666e\u904d\u4e0b\u964d\uff0c\u4e14\u4e0e\u7ffb\u8bd1\u8f93\u5165\u91cf\u76f8\u5173\u3002", "conclusion": "MAPS\u4e3a\u591a\u8bed\u8a00\u4ee3\u7406AI\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\uff0c\u4ee5\u63a8\u52a8\u5168\u7403\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2505.16942", "pdf": "https://arxiv.org/pdf/2505.16942", "abs": "https://arxiv.org/abs/2505.16942", "authors": ["Karlis Martins Briedis", "Markus Gross", "Christopher Schroers"], "title": "Efficient Correlation Volume Sampling for Ultra-High-Resolution Optical Flow Estimation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent optical flow estimation methods often employ local cost sampling from\na dense all-pairs correlation volume. This results in quadratic computational\nand memory complexity in the number of pixels. Although an alternative\nmemory-efficient implementation with on-demand cost computation exists, this is\nslower in practice and therefore prior methods typically process images at\nreduced resolutions, missing fine-grained details.\n  To address this, we propose a more efficient implementation of the all-pairs\ncorrelation volume sampling, still matching the exact mathematical operator as\ndefined by RAFT. Our approach outperforms on-demand sampling by up to 90% while\nmaintaining low memory usage, and performs on par with the default\nimplementation with up to 95% lower memory usage. As cost sampling makes up a\nsignificant portion of the overall runtime, this can translate to up to 50%\nsavings for the total end-to-end model inference in memory-constrained\nenvironments. Our evaluation of existing methods includes an 8K\nultra-high-resolution dataset and an additional inference-time modification of\nthe recent SEA-RAFT method. With this, we achieve state-of-the-art results at\nhigh resolutions both in accuracy and efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5168\u5bf9\u76f8\u5173\u4f53\u79ef\u91c7\u6837\u5b9e\u73b0\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5149\u6d41\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\uff0c\u56e0\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u9ad8\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u5168\u5bf9\u76f8\u5173\u4f53\u79ef\u91c7\u6837\u5b9e\u73b0\u65b9\u6cd5\uff0c\u4e0eRAFT\u7684\u6570\u5b66\u5b9a\u4e49\u4e00\u81f4\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5185\u5b58\u4f7f\u7528\u4e0a\u964d\u4f4e\u4e8695%\uff0c\u901f\u5ea6\u63d0\u5347\u4e8690%\uff0c\u5e76\u5728\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5149\u6d41\u4f30\u8ba1\u7684\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u5206\u8fa8\u7387\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2505.15957", "pdf": "https://arxiv.org/pdf/2505.15957", "abs": "https://arxiv.org/abs/2505.15957", "authors": ["Chih-Kai Yang", "Neo S. Ho", "Hung-yi Lee"], "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Project Website: https://github.com/b08202033/LALM-Evaluation-Survey", "summary": "With advancements in large audio-language models (LALMs), which enhance large\nlanguage models (LLMs) with auditory capabilities, these models are expected to\ndemonstrate universal proficiency across various auditory tasks. While numerous\nbenchmarks have emerged to assess LALMs' performance, they remain fragmented\nand lack a structured taxonomy. To bridge this gap, we conduct a comprehensive\nsurvey and propose a systematic taxonomy for LALM evaluations, categorizing\nthem into four dimensions based on their objectives: (1) General Auditory\nAwareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented\nAbility, and (4) Fairness, Safety, and Trustworthiness. We provide detailed\noverviews within each category and highlight challenges in this field, offering\ninsights into promising future directions. To the best of our knowledge, this\nis the first survey specifically focused on the evaluations of LALMs, providing\nclear guidelines for the community. We will release the collection of the\nsurveyed papers and actively maintain it to support ongoing advancements in the\nfield.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u7684\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u788e\u7247\u5316\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709LALMs\u7684\u8bc4\u4f30\u6807\u51c6\u7f3a\u4e4f\u7edf\u4e00\u5206\u7c7b\uff0c\u963b\u788d\u4e86\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\u548c\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8c03\u7814\uff0c\u5c06LALMs\u8bc4\u4f30\u5206\u4e3a\u56db\u4e2a\u7ef4\u5ea6\uff1a\u901a\u7528\u542c\u89c9\u611f\u77e5\u4e0e\u5904\u7406\u3001\u77e5\u8bc6\u4e0e\u63a8\u7406\u3001\u5bf9\u8bdd\u5bfc\u5411\u80fd\u529b\u3001\u516c\u5e73\u6027\u4e0e\u5b89\u5168\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9LALMs\u8bc4\u4f30\u7684\u7cfb\u7edf\u5206\u7c7b\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3aLALMs\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6e05\u6670\u6307\u5357\uff0c\u5e76\u8ba1\u5212\u6301\u7eed\u66f4\u65b0\u8c03\u7814\u6210\u679c\u4ee5\u652f\u6301\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.16964", "pdf": "https://arxiv.org/pdf/2505.16964", "abs": "https://arxiv.org/abs/2505.16964", "authors": ["Suhao Yu", "Haojin Wang", "Juncheng Wu", "Cihang Xie", "Yuyin Zhou"], "title": "MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": "9 pages, 4 Figures Benchmark data:\n  https://huggingface.co/datasets/SuhaoYu1020/MedFrameQA", "summary": "Existing medical VQA benchmarks mostly focus on single-image analysis, yet\nclinicians almost always compare a series of images before reaching a\ndiagnosis. To better approximate this workflow, we introduce MedFrameQA -- the\nfirst benchmark that explicitly evaluates multi-image reasoning in medical VQA.\nTo build MedFrameQA both at scale and in high-quality, we develop 1) an\nautomated pipeline that extracts temporally coherent frames from medical videos\nand constructs VQA items whose content evolves logically across images, and 2)\na multiple-stage filtering strategy, including model-based and manual review,\nto preserve data clarity, difficulty, and medical relevance. The resulting\ndataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in\n3,420 videos), covering nine human body systems and 43 organs; every question\nis accompanied by two to five images. We comprehensively benchmark ten advanced\nMultimodal LLMs -- both proprietary and open source, with and without explicit\nreasoning modules -- on MedFrameQA. The evaluation challengingly reveals that\nall models perform poorly, with most accuracies below 50%, and accuracy\nfluctuates as the number of images per question increases. Error analysis\nfurther shows that models frequently ignore salient findings, mis-aggregate\nevidence across images, and propagate early mistakes through their reasoning\nchains; results also vary substantially across body systems, organs, and\nmodalities. We hope this work can catalyze research on clinically grounded,\nmulti-image reasoning and accelerate progress toward more capable diagnostic AI\nsystems.", "AI": {"tldr": "MedFrameQA\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4e2d\u591a\u56fe\u50cf\u63a8\u7406\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6a21\u62df\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u591a\u56fe\u50cf\u6bd4\u8f83\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u533b\u5b66VQA\u57fa\u51c6\u591a\u5173\u6ce8\u5355\u56fe\u50cf\u5206\u6790\uff0c\u800c\u4e34\u5e8a\u8bca\u65ad\u901a\u5e38\u9700\u8981\u6bd4\u8f83\u591a\u56fe\u50cf\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86MedFrameQA\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u4ece\u533b\u5b66\u89c6\u9891\u4e2d\u63d0\u53d6\u65f6\u95f4\u8fde\u8d2f\u7684\u5e27\uff0c\u6784\u5efa\u903b\u8f91\u8fde\u8d2f\u7684VQA\u9879\u76ee\uff0c\u5e76\u91c7\u7528\u591a\u9636\u6bb5\u8fc7\u6ee4\u7b56\u7565\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b2,851\u4e2aVQA\u5bf9\uff0c\u8986\u76d69\u4e2a\u4eba\u4f53\u7cfb\u7edf\u548c43\u4e2a\u5668\u5b98\u3002\u6d4b\u8bd5\u768410\u79cd\u591a\u6a21\u6001LLM\u8868\u73b0\u4e0d\u4f73\uff0c\u51c6\u786e\u7387\u666e\u904d\u4f4e\u4e8e50%\u3002", "conclusion": "MedFrameQA\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u591a\u56fe\u50cf\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u8bca\u65adAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16971", "pdf": "https://arxiv.org/pdf/2505.16971", "abs": "https://arxiv.org/abs/2505.16971", "authors": ["Himangi Mittal", "Peiye Zhuang", "Hsin-Ying Lee", "Shubham Tulsiani"], "title": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "We propose UniPhy, a common latent-conditioned neural constitutive model that\ncan encode the physical properties of diverse materials. At inference UniPhy\nallows `inverse simulation' i.e. inferring material properties by optimizing\nthe scene-specific latent to match the available observations via\ndifferentiable simulation. In contrast to existing methods that treat such\ninference as system identification, UniPhy does not rely on user-specified\nmaterial type information. Compared to prior neural constitutive modeling\napproaches which learn instance specific networks, the shared training across\nmaterials improves both, robustness and accuracy of the estimates. We train\nUniPhy using simulated trajectories across diverse geometries and materials --\nelastic, plasticine, sand, and fluids (Newtonian & non-Newtonian). At\ninference, given an object with unknown material properties, UniPhy can infer\nthe material properties via latent optimization to match the motion\nobservations, and can then allow re-simulating the object under diverse\nscenarios. We compare UniPhy against prior inverse simulation methods, and show\nthat the inference from UniPhy enables more accurate replay and re-simulation\nunder novel conditions.", "AI": {"tldr": "UniPhy\u662f\u4e00\u79cd\u901a\u7528\u7684\u6f5c\u5728\u6761\u4ef6\u795e\u7ecf\u672c\u6784\u6a21\u578b\uff0c\u80fd\u591f\u7f16\u7801\u591a\u79cd\u6750\u6599\u7684\u7269\u7406\u7279\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u5206\u6a21\u62df\u63a8\u65ad\u6750\u6599\u5c5e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7528\u6237\u6307\u5b9a\u7684\u6750\u6599\u7c7b\u578b\u4fe1\u606f\uff0c\u800cUniPhy\u65e0\u9700\u6b64\u7c7b\u4fe1\u606f\uff0c\u901a\u8fc7\u5171\u4eab\u8bad\u7ec3\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "UniPhy\u901a\u8fc7\u6a21\u62df\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u548c\u6750\u6599\uff08\u5f39\u6027\u3001\u5851\u6027\u3001\u6c99\u3001\u6d41\u4f53\uff09\u7684\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u4f18\u5316\u63a8\u65ad\u672a\u77e5\u6750\u6599\u7684\u5c5e\u6027\u3002", "result": "UniPhy\u5728\u63a8\u65ad\u6750\u6599\u5c5e\u6027\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u91cd\u653e\u548c\u91cd\u65b0\u6a21\u62df\u65b0\u6761\u4ef6\u4e0b\u7684\u7269\u4f53\u8fd0\u52a8\u3002", "conclusion": "UniPhy\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u6750\u6599\u4fe1\u606f\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u5c5e\u6027\u63a8\u65ad\u548c\u6a21\u62df\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2505.16974", "pdf": "https://arxiv.org/pdf/2505.16974", "abs": "https://arxiv.org/abs/2505.16974", "authors": ["Zongyan Han", "Jiale Cao", "Shuo Chen", "Tong Wang", "Jorma Laaksonen", "Rao Muhammad Anwer"], "title": "OpenSeg-R: Improving Open-Vocabulary Segmentation via Step-by-Step Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary Segmentation (OVS) has drawn increasing attention for its\ncapacity to generalize segmentation beyond predefined categories. However,\nexisting methods typically predict segmentation masks with simple forward\ninference, lacking explicit reasoning and interpretability. This makes it\nchallenging for OVS model to distinguish similar categories in open-world\nsettings due to the lack of contextual understanding and discriminative visual\ncues. To address this limitation, we propose a step-by-step visual reasoning\nframework for open-vocabulary segmentation, named OpenSeg-R. The proposed\nOpenSeg-R leverages Large Multimodal Models (LMMs) to perform hierarchical\nvisual reasoning before segmentation. Specifically, we generate both generic\nand image-specific reasoning for each image, forming structured triplets that\nexplain the visual reason for objects in a coarse-to-fine manner. Based on\nthese reasoning steps, we can compose detailed description prompts, and feed\nthem to the segmentor to produce more accurate segmentation masks. To the best\nof our knowledge, OpenSeg-R is the first framework to introduce explicit\nstep-by-step visual reasoning into OVS. Experimental results demonstrate that\nOpenSeg-R significantly outperforms state-of-the-art methods on open-vocabulary\nsemantic segmentation across five benchmark datasets. Moreover, it achieves\nconsistent gains across all metrics on open-vocabulary panoptic segmentation.\nQualitative results further highlight the effectiveness of our reasoning-guided\nframework in improving both segmentation precision and interpretability. Our\ncode is publicly available at https://github.com/Hanzy1996/OpenSeg-R.", "AI": {"tldr": "OpenSeg-R \u662f\u4e00\u4e2a\u57fa\u4e8e\u9010\u6b65\u89c6\u89c9\u63a8\u7406\u7684\u5f00\u8bcd\u6c47\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u5c42\u6b21\u5316\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5f00\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\u7f3a\u4e4f\u663e\u5f0f\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u533a\u5206\u76f8\u4f3c\u7c7b\u522b\u3002", "method": "OpenSeg-R \u901a\u8fc7\u751f\u6210\u901a\u7528\u548c\u56fe\u50cf\u7279\u5b9a\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u5f62\u6210\u7ed3\u6784\u5316\u4e09\u5143\u7ec4\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u63a8\u7406\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u63d0\u793a\u4ee5\u6307\u5bfc\u5206\u5272\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5f00\u8bcd\u6c47\u5168\u666f\u5206\u5272\u4e2d\u5b9e\u73b0\u4e00\u81f4\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "OpenSeg-R \u9996\u6b21\u5c06\u9010\u6b65\u89c6\u89c9\u63a8\u7406\u5f15\u5165\u5f00\u8bcd\u6c47\u5206\u5272\uff0c\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.16004", "pdf": "https://arxiv.org/pdf/2505.16004", "abs": "https://arxiv.org/abs/2505.16004", "authors": ["Aaron J. Li", "Suraj Srinivas", "Usha Bhalla", "Himabindu Lakkaraju"], "title": "Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are commonly used to interpret the internal\nactivations of large language models (LLMs) by mapping them to\nhuman-interpretable concept representations. While existing evaluations of SAEs\nfocus on metrics such as the reconstruction-sparsity tradeoff, human\n(auto-)interpretability, and feature disentanglement, they overlook a critical\naspect: the robustness of concept representations to input perturbations. We\nargue that robustness must be a fundamental consideration for concept\nrepresentations, reflecting the fidelity of concept labeling. To this end, we\nformulate robustness quantification as input-space optimization problems and\ndevelop a comprehensive evaluation framework featuring realistic scenarios in\nwhich adversarial perturbations are crafted to manipulate SAE representations.\nEmpirically, we find that tiny adversarial input perturbations can effectively\nmanipulate concept-based interpretations in most scenarios without notably\naffecting the outputs of the base LLMs themselves. Overall, our results suggest\nthat SAE concept representations are fragile and may be ill-suited for\napplications in model monitoring and oversight.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u6982\u5ff5\u8868\u793a\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u6765\u91cf\u5316\u8fd9\u79cd\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5fae\u5c0f\u5bf9\u6297\u6270\u52a8\u5373\u53ef\u64cd\u7eb5SAE\u7684\u6982\u5ff5\u89e3\u91ca\uff0c\u800c\u57fa\u7840LLM\u7684\u8f93\u51fa\u51e0\u4e4e\u4e0d\u53d7\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709SAE\u8bc4\u4f30\u5ffd\u7565\u4e86\u6982\u5ff5\u8868\u793a\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u800c\u9c81\u68d2\u6027\u662f\u6982\u5ff5\u6807\u7b7e\u771f\u5b9e\u6027\u7684\u5173\u952e\u6307\u6807\u3002", "method": "\u901a\u8fc7\u8f93\u5165\u7a7a\u95f4\u4f18\u5316\u95ee\u9898\u91cf\u5316\u9c81\u68d2\u6027\uff0c\u5e76\u5f00\u53d1\u8bc4\u4f30\u6846\u67b6\uff0c\u6a21\u62df\u5bf9\u6297\u6270\u52a8\u64cd\u7eb5SAE\u8868\u793a\u7684\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u5fae\u5c0f\u5bf9\u6297\u6270\u52a8\u80fd\u6709\u6548\u64cd\u7eb5SAE\u7684\u6982\u5ff5\u89e3\u91ca\uff0c\u4f46\u5bf9\u57fa\u7840LLM\u8f93\u51fa\u5f71\u54cd\u751a\u5fae\u3002", "conclusion": "SAE\u7684\u6982\u5ff5\u8868\u793a\u8106\u5f31\uff0c\u53ef\u80fd\u4e0d\u9002\u5408\u7528\u4e8e\u6a21\u578b\u76d1\u63a7\u548c\u76d1\u7ba1\u3002"}}
{"id": "2505.16976", "pdf": "https://arxiv.org/pdf/2505.16976", "abs": "https://arxiv.org/abs/2505.16976", "authors": ["Yurui Qian", "Qi Cai", "Yingwei Pan", "Ting Yao", "Tao Mei"], "title": "Creatively Upscaling Images with Global-Regional Priors", "categories": ["cs.CV", "cs.MM"], "comment": "International Journal of Computer Vision (IJCV) 2025", "summary": "Contemporary diffusion models show remarkable capability in text-to-image\ngeneration, while still being limited to restricted resolutions (e.g., 1,024 X\n1,024). Recent advances enable tuning-free higher-resolution image generation\nby recycling pre-trained diffusion models and extending them via regional\ndenoising or dilated sampling/convolutions. However, these models struggle to\nsimultaneously preserve global semantic structure and produce creative regional\ndetails in higher-resolution images. To address this, we present C-Upscale, a\nnew recipe of tuning-free image upscaling that pivots on global-regional priors\nderived from given global prompt and estimated regional prompts via Multimodal\nLLM. Technically, the low-frequency component of low-resolution image is\nrecognized as global structure prior to encourage global semantic consistency\nin high-resolution generation. Next, we perform regional attention control to\nscreen cross-attention between global prompt and each region during regional\ndenoising, leading to regional attention prior that alleviates object\nrepetition issue. The estimated regional prompts containing rich descriptive\ndetails further act as regional semantic prior to fuel the creativity of\nregional detail generation. Both quantitative and qualitative evaluations\ndemonstrate that our C-Upscale manages to generate ultra-high-resolution images\n(e.g., 4,096 X 4,096 and 8,192 X 8,192) with higher visual fidelity and more\ncreative regional details.", "AI": {"tldr": "C-Upscale\u662f\u4e00\u79cd\u65e0\u9700\u8c03\u6574\u7684\u56fe\u50cf\u653e\u5927\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u548c\u533a\u57df\u5148\u9a8c\uff0c\u751f\u6210\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4e30\u5bcc\u7684\u533a\u57df\u7ec6\u8282\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5206\u8fa8\u7387\u53d7\u9650\uff08\u59821,024 X 1,024\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u548c\u533a\u57df\u7ec6\u8282\u7684\u521b\u9020\u6027\u3002", "method": "C-Upscale\u5229\u7528\u5168\u5c40\u63d0\u793a\u548c\u901a\u8fc7\u591a\u6a21\u6001LLM\u4f30\u8ba1\u7684\u533a\u57df\u63d0\u793a\uff0c\u63d0\u53d6\u5168\u5c40\u7ed3\u6784\u5148\u9a8c\u548c\u533a\u57df\u6ce8\u610f\u529b\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u533a\u57df\u6ce8\u610f\u529b\u63a7\u5236\u51cf\u5c11\u5bf9\u8c61\u91cd\u590d\u95ee\u9898\u3002", "result": "C-Upscale\u80fd\u591f\u751f\u6210\u8d85\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff08\u59824,096 X 4,096\u548c8,192 X 8,192\uff09\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u66f4\u4e30\u5bcc\u7684\u533a\u57df\u7ec6\u8282\u3002", "conclusion": "C-Upscale\u901a\u8fc7\u5168\u5c40\u548c\u533a\u57df\u5148\u9a8c\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u521b\u9020\u6027\u95ee\u9898\u3002"}}
{"id": "2505.16037", "pdf": "https://arxiv.org/pdf/2505.16037", "abs": "https://arxiv.org/abs/2505.16037", "authors": ["Asterios Tsiourvas", "Wei Sun", "Georgia Perakis"], "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data", "categories": ["cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "LLM routing aims to select the most appropriate model for each query,\nbalancing competing performance metrics such as accuracy and cost across a pool\nof language models. Prior approaches typically adopt a decoupled strategy,\nwhere the metrics are first predicted and the model is then selected based on\nthese estimates. This setup is prone to compounding errors and often relies on\nfull-feedback data, where each query is evaluated by all candidate models,\nwhich is costly to obtain and maintain in practice. In contrast, we learn from\nobservational data, which records only the outcome of the model actually\ndeployed. We propose a causal end-to-end framework that learns routing policies\nby minimizing decision-making regret from observational data. To enable\nefficient optimization, we introduce two theoretically grounded surrogate\nobjectives: a classification-based upper bound, and a softmax-weighted regret\napproximation shown to recover the optimal policy at convergence. We further\nextend our framework to handle heterogeneous cost preferences via an\ninterval-conditioned architecture. Experiments on public benchmarks show that\nour method outperforms existing baselines, achieving state-of-the-art\nperformance across different embedding models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u7aef\u5230\u7aef\u6846\u67b6\u7684LLM\u8def\u7531\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u51b3\u7b56\u9057\u61be\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u8def\u7531\u7b56\u7565\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLLM\u8def\u7531\u65b9\u6cd5\u91c7\u7528\u89e3\u8026\u7b56\u7565\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u4e14\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u5b8c\u6574\u53cd\u9988\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u8def\u7531\u7b56\u7565\uff0c\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5f15\u5165\u4e24\u79cd\u7406\u8bba\u652f\u6301\u7684\u66ff\u4ee3\u76ee\u6807\uff1a\u57fa\u4e8e\u5206\u7c7b\u7684\u4e0a\u754c\u548csoftmax\u52a0\u6743\u7684\u9057\u61be\u8fd1\u4f3c\u3002\u8fd8\u6269\u5c55\u4e86\u6846\u67b6\u4ee5\u5904\u7406\u5f02\u6784\u6210\u672c\u504f\u597d\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u8def\u7531\u7b56\u7565\uff0c\u907f\u514d\u4e86\u8bef\u5dee\u7d2f\u79ef\u548c\u9ad8\u6210\u672c\u6570\u636e\u9700\u6c42\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.16977", "pdf": "https://arxiv.org/pdf/2505.16977", "abs": "https://arxiv.org/abs/2505.16977", "authors": ["Siqi Wan", "Jingwen Chen", "Yingwei Pan", "Ting Yao", "Tao Mei"], "title": "Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On", "categories": ["cs.CV", "cs.MM"], "comment": "ICLR 2025. Code is publicly available at:\n  https://github.com/HiDream-ai/SPM-Diff", "summary": "Diffusion models have shown preliminary success in virtual try-on (VTON)\ntask. The typical dual-branch architecture comprises two UNets for implicit\ngarment deformation and synthesized image generation respectively, and has\nemerged as the recipe for VTON task. Nevertheless, the problem remains\nchallenging to preserve the shape and every detail of the given garment due to\nthe intrinsic stochasticity of diffusion model. To alleviate this issue, we\nnovelly propose to explicitly capitalize on visual correspondence as the prior\nto tame diffusion process instead of simply feeding the whole garment into UNet\nas the appearance reference. Specifically, we interpret the fine-grained\nappearance and texture details as a set of structured semantic points, and\nmatch the semantic points rooted in garment to the ones over target person\nthrough local flow warping. Such 2D points are then augmented into 3D-aware\ncues with depth/normal map of target person. The correspondence mimics the way\nof putting clothing on human body and the 3D-aware cues act as semantic point\nmatching to supervise diffusion model training. A point-focused diffusion loss\nis further devised to fully take the advantage of semantic point matching.\nExtensive experiments demonstrate strong garment detail preservation of our\napproach, evidenced by state-of-the-art VTON performances on both VITON-HD and\nDressCode datasets. Code is publicly available at:\nhttps://github.com/HiDream-ai/SPM-Diff.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5bf9\u5e94\u6027\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u865a\u62df\u8bd5\u7a7f\uff08VTON\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u8bed\u4e49\u70b9\u5339\u914d\u548c3D\u611f\u77e5\u7ebf\u7d22\u63d0\u5347\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728VTON\u4efb\u52a1\u4e2d\u56e0\u968f\u673a\u6027\u5bfc\u81f4\u7684\u670d\u88c5\u5f62\u72b6\u548c\u7ec6\u8282\u96be\u4ee5\u4fdd\u7559\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u5bf9\u5e94\u6027\u4f5c\u4e3a\u5148\u9a8c\uff0c\u5c06\u670d\u88c5\u7ec6\u8282\u8868\u793a\u4e3a\u7ed3\u6784\u5316\u8bed\u4e49\u70b9\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u6d41\u53d8\u5f62\u5339\u914d\u76ee\u6807\u4eba\u7269\u7684\u8bed\u4e49\u70b9\uff0c\u7ed3\u5408\u6df1\u5ea6/\u6cd5\u7ebf\u56fe\u751f\u62103D\u611f\u77e5\u7ebf\u7d22\u3002", "result": "\u5728VITON-HD\u548cDressCode\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684VTON\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u88c5\u7ec6\u8282\u7684\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u70b9\u5339\u914d\u548c3D\u611f\u77e5\u7ebf\u7d22\u7684\u5f15\u5165\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u5728VTON\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.16065", "pdf": "https://arxiv.org/pdf/2505.16065", "abs": "https://arxiv.org/abs/2505.16065", "authors": ["Ruijie Xi", "He Ba", "Hao Yuan", "Rishu Agrawal", "Arul Prakash"], "title": "Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Embedding-Based Retrieval (EBR) is an important technique in modern search\nengines, enabling semantic match between search queries and relevant results.\nHowever, search logging data on platforms like Facebook Marketplace lacks the\ndiversity and details needed for effective EBR model training, limiting the\nmodels' ability to capture nuanced search patterns. To address this challenge,\nwe propose Aug2Search, an EBR-based framework leveraging synthetic data\ngenerated by Generative AI (GenAI) models, in a multimodal and multitask\napproach to optimize query-product relevance. This paper investigates the\ncapabilities of GenAI, particularly Large Language Models (LLMs), in generating\nhigh-quality synthetic data, and analyzing its impact on enhancing EBR models.\nWe conducted experiments using eight Llama models and 100 million data points\nfrom Facebook Marketplace logs. Our synthetic data generation follows three\nstrategies: (1) generate queries, (2) enhance product listings, and (3)\ngenerate queries from enhanced listings. We train EBR models on three different\ndatasets: sampled engagement data or original data ((e.g., \"Click\" and \"Listing\nInteractions\")), synthetic data, and a mixture of both engagement and synthetic\ndata to assess their performance across various training sets. Our findings\nunderscore the robustness of Llama models in producing synthetic queries and\nlistings with high coherence, relevance, and diversity, while maintaining low\nlevels of hallucination. Aug2Search achieves an improvement of up to 4% in\nROC_AUC with 100 million synthetic data samples, demonstrating the\neffectiveness of our approach. Moreover, our experiments reveal that with the\nsame volume of training data, models trained exclusively on synthetic data\noften outperform those trained on original data only or a mixture of original\nand synthetic data.", "AI": {"tldr": "Aug2Search\u6846\u67b6\u5229\u7528\u751f\u6210\u5f0fAI\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u63d0\u5347\u5d4c\u5165\u68c0\u7d22\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5e73\u53f0\u641c\u7d22\u65e5\u5fd7\u6570\u636e\u591a\u6837\u6027\u548c\u7ec6\u8282\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u67e5\u8be2\u4e0e\u4ea7\u54c1\u7684\u76f8\u5173\u6027\u3002", "method": "\u5229\u7528LLMs\u751f\u6210\u5408\u6210\u6570\u636e\uff08\u67e5\u8be2\u3001\u589e\u5f3a\u4ea7\u54c1\u5217\u8868\u3001\u4ece\u589e\u5f3a\u5217\u8868\u751f\u6210\u67e5\u8be2\uff09\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3EBR\u6a21\u578b\u3002", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff08ROC_AUC\u63d0\u9ad84%\uff09\uff0c\u4e14\u7eaf\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u80fd\u6709\u6548\u589e\u5f3aEBR\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2505.16980", "pdf": "https://arxiv.org/pdf/2505.16980", "abs": "https://arxiv.org/abs/2505.16980", "authors": ["Dong Li", "Wenqi Zhong", "Wei Yu", "Yingwei Pan", "Dingwen Zhang", "Ting Yao", "Junwei Han", "Tao Mei"], "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction", "categories": ["cs.CV", "cs.MM"], "comment": "CVPR 2025", "summary": "Video virtual try-on aims to seamlessly dress a subject in a video with a\nspecific garment. The primary challenge involves preserving the visual\nauthenticity of the garment while dynamically adapting to the pose and physique\nof the subject. While existing methods have predominantly focused on\nimage-based virtual try-on, extending these techniques directly to videos often\nresults in temporal inconsistencies. Most current video virtual try-on\napproaches alleviate this challenge by incorporating temporal modules, yet\nstill overlook the critical spatiotemporal pose interactions between human and\ngarment. Effective pose interactions in videos should not only consider spatial\nalignment between human and garment poses in each frame but also account for\nthe temporal dynamics of human poses throughout the entire video. With such\nmotivation, we propose a new framework, namely Dynamic Pose Interaction\nDiffusion Models (DPIDM), to leverage diffusion models to delve into dynamic\npose interactions for video virtual try-on. Technically, DPIDM introduces a\nskeleton-based pose adapter to integrate synchronized human and garment poses\ninto the denoising network. A hierarchical attention module is then exquisitely\ndesigned to model intra-frame human-garment pose interactions and long-term\nhuman pose dynamics across frames through pose-aware spatial and temporal\nattention mechanisms. Moreover, DPIDM capitalizes on a temporal regularized\nattention loss between consecutive frames to enhance temporal consistency.\nExtensive experiments conducted on VITON-HD, VVT and ViViD datasets demonstrate\nthe superiority of our DPIDM against the baseline methods. Notably, DPIDM\nachieves VFID score of 0.506 on VVT dataset, leading to 60.5% improvement over\nthe state-of-the-art GPD-VVTO approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPIDM\u7684\u52a8\u6001\u59ff\u6001\u4ea4\u4e92\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u7a7a\u59ff\u6001\u4ea4\u4e92\u548c\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bd5\u7a7f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u6a21\u5757\uff0c\u4f46\u5ffd\u7565\u4e86\u4eba\u4e0e\u670d\u88c5\u4e4b\u95f4\u7684\u65f6\u7a7a\u59ff\u6001\u4ea4\u4e92\u3002DPIDM\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u59ff\u6001\u4ea4\u4e92\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DPIDM\u5f15\u5165\u57fa\u4e8e\u9aa8\u67b6\u7684\u59ff\u6001\u9002\u914d\u5668\uff0c\u7ed3\u5408\u5206\u5c42\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5efa\u6a21\u5e27\u5185\u4eba\u4e0e\u670d\u88c5\u59ff\u6001\u4ea4\u4e92\u53ca\u8de8\u5e27\u957f\u671f\u59ff\u6001\u52a8\u6001\u3002", "result": "\u5728VITON-HD\u3001VVT\u548cViViD\u6570\u636e\u96c6\u4e0a\uff0cDPIDM\u8868\u73b0\u4f18\u5f02\uff0cVFID\u5206\u6570\u4e3a0.506\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u63d0\u534760.5%\u3002", "conclusion": "DPIDM\u901a\u8fc7\u52a8\u6001\u59ff\u6001\u4ea4\u4e92\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u865a\u62df\u8bd5\u7a7f\u7684\u6548\u679c\u3002"}}
{"id": "2505.16066", "pdf": "https://arxiv.org/pdf/2505.16066", "abs": "https://arxiv.org/abs/2505.16066", "authors": ["Zhixu Silvia Tao", "Kasper Vinken", "Hao-Wei Yeh", "Avi Cooper", "Xavier Boix"], "title": "Merge to Mix: Mixing Datasets via Model Merging", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Mixing datasets for fine-tuning large models (LMs) has become critical for\nmaximizing performance on downstream tasks. However, composing effective\ndataset mixtures typically relies on heuristics and trial-and-error, often\nrequiring multiple fine-tuning runs to achieve the desired outcome. We propose\na novel method, $\\textit{Merge to Mix}$, that accelerates composing dataset\nmixtures through model merging. Model merging is a recent technique that\ncombines the abilities of multiple individually fine-tuned LMs into a single LM\nby using a few simple arithmetic operations. Our key insight is that merging\nmodels individually fine-tuned on each dataset in a mixture can effectively\nserve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix\nleverages this insight to accelerate selecting dataset mixtures without\nrequiring full fine-tuning on each candidate mixture. Our experiments\ndemonstrate that Merge to Mix surpasses state-of-the-art methods in dataset\nselection for fine-tuning LMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMerge to Mix\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u52a0\u901f\u6570\u636e\u96c6\u6df7\u5408\u7684\u6784\u5efa\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u8bd5\u9519\u6cd5\u7684\u591a\u6b21\u5fae\u8c03\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u96c6\u6df7\u5408\u65b9\u6cd5\u4f9d\u8d56\u8bd5\u9519\u548c\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u591a\u6b21\u5fae\u8c03\u3002", "method": "\u5229\u7528\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u5c06\u5355\u72ec\u5fae\u8c03\u540e\u7684\u6a21\u578b\u901a\u8fc7\u7b80\u5355\u7b97\u672f\u64cd\u4f5c\u5408\u5e76\uff0c\u4f5c\u4e3a\u6574\u4e2a\u6570\u636e\u96c6\u6df7\u5408\u7684\u66ff\u4ee3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMerge to Mix\u5728\u6570\u636e\u96c6\u9009\u62e9\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Merge to Mix\u4e3a\u9ad8\u6548\u6784\u5efa\u6570\u636e\u96c6\u6df7\u5408\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2505.16985", "pdf": "https://arxiv.org/pdf/2505.16985", "abs": "https://arxiv.org/abs/2505.16985", "authors": ["Moru Liu", "Hao Dong", "Jessica Kelly", "Olga Fink", "Mario Trapp"], "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Out-of-distribution (OOD) detection and segmentation are crucial for\ndeploying machine learning models in safety-critical applications such as\nautonomous driving and robot-assisted surgery. While prior research has\nprimarily focused on unimodal image data, real-world applications are\ninherently multimodal, requiring the integration of multiple modalities for\nimproved OOD detection. A key challenge is the lack of supervision signals from\nunknown data, leading to overconfident predictions on OOD samples. To address\nthis challenge, we propose Feature Mixing, an extremely simple and fast method\nfor multimodal outlier synthesis with theoretical support, which can be further\noptimized to help the model better distinguish between in-distribution (ID) and\nOOD data. Feature Mixing is modality-agnostic and applicable to various\nmodality combinations. Additionally, we introduce CARLA-OOD, a novel multimodal\ndataset for OOD segmentation, featuring synthetic OOD objects across diverse\nscenes and weather conditions. Extensive experiments on SemanticKITTI,\nnuScenes, CARLA-OOD datasets, and the MultiOOD benchmark demonstrate that\nFeature Mixing achieves state-of-the-art performance with a $10 \\times$ to $370\n\\times$ speedup. Our source code and dataset will be available at\nhttps://github.com/mona4399/FeatureMixing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFeature Mixing\u7684\u7b80\u5355\u5feb\u901f\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5f02\u5e38\u5408\u6210\uff0c\u4ee5\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6570\u636e\u96c6CARLA-OOD\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u591a\u4e3a\u591a\u6a21\u6001\uff0c\u800c\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u5355\u6a21\u6001\u56fe\u50cf\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u672a\u77e5\u6570\u636e\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5bfc\u81f4OOD\u6837\u672c\u9884\u6d4b\u8fc7\u5ea6\u81ea\u4fe1\u3002", "method": "\u63d0\u51faFeature Mixing\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u6df7\u5408\u5408\u6210\u5f02\u5e38\u6570\u636e\uff0c\u7406\u8bba\u652f\u6301\u5176\u6709\u6548\u6027\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u6001\u7ec4\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cFeature Mixing\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u901f\u5ea6\u63d0\u534710\u81f3370\u500d\u3002", "conclusion": "Feature Mixing\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u573a\u666f\u3002"}}
{"id": "2505.16086", "pdf": "https://arxiv.org/pdf/2505.16086", "abs": "https://arxiv.org/abs/2505.16086", "authors": ["Ming Shen", "Raphael Shu", "Anurag Pratik", "James Gung", "Yubin Ge", "Monica Sunkara", "Yi Zhang"], "title": "Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We have seen remarkable progress in large language models (LLMs) empowered\nmulti-agent systems solving complex tasks necessitating cooperation among\nexperts with diverse skills. However, optimizing LLM-based multi-agent systems\nremains challenging. In this work, we perform an empirical case study on group\noptimization of role-based multi-agent systems utilizing natural language\nfeedback for challenging software development tasks under various evaluation\ndimensions. We propose a two-step agent prompts optimization pipeline:\nidentifying underperforming agents with their failure explanations utilizing\ntextual feedback and then optimizing system prompts of identified agents\nutilizing failure explanations. We then study the impact of various\noptimization settings on system performance with two comparison groups: online\nagainst offline optimization and individual against group optimization. For\ngroup optimization, we study two prompting strategies: one-pass and multi-pass\nprompting optimizations. Overall, we demonstrate the effectiveness of our\noptimization method for role-based multi-agent systems tackling software\ndevelopment tasks evaluated on diverse evaluation dimensions, and we\ninvestigate the impact of diverse optimization settings on group behaviors of\nthe multi-agent systems to provide practical insights for future development.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u7684\u4e24\u6b65\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u4f18\u5316\u8bbe\u7f6e\u5bf9\u7cfb\u7edf\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4f18\u5316\u8fd9\u4e9b\u7cfb\u7edf\u4ecd\u5177\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u4f18\u5316\u89d2\u8272\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e24\u6b65\u4f18\u5316\u6d41\u7a0b\uff1a1\uff09\u901a\u8fc7\u6587\u672c\u53cd\u9988\u8bc6\u522b\u8868\u73b0\u4e0d\u4f73\u7684\u667a\u80fd\u4f53\u53ca\u5176\u5931\u8d25\u539f\u56e0\uff1b2\uff09\u5229\u7528\u5931\u8d25\u539f\u56e0\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u3002\u7814\u7a76\u4e86\u5728\u7ebf\u4e0e\u79bb\u7ebf\u4f18\u5316\u3001\u4e2a\u4f53\u4e0e\u7fa4\u4f53\u4f18\u5316\u7684\u5bf9\u6bd4\uff0c\u4ee5\u53ca\u5355\u6b21\u4e0e\u591a\u6b21\u63d0\u793a\u4f18\u5316\u7b56\u7565\u3002", "result": "\u65b9\u6cd5\u5728\u8f6f\u4ef6\u5f00\u53d1\u4efb\u52a1\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u4f18\u5316\u8bbe\u7f6e\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7fa4\u4f53\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u9a8c\u8bc1\u4e86\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.16990", "pdf": "https://arxiv.org/pdf/2505.16990", "abs": "https://arxiv.org/abs/2505.16990", "authors": ["Runpeng Yu", "Xinyin Ma", "Xinchao Wang"], "title": "Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose Dimple, the first Discrete Diffusion Multimodal\nLarge Language Model (DMLLM). We observe that training with a purely discrete\ndiffusion approach leads to significant training instability, suboptimal\nperformance, and severe length bias issues. To address these challenges, we\ndesign a novel training paradigm that combines an initial autoregressive phase\nwith a subsequent diffusion phase. This approach yields the Dimple-7B model,\ntrained on the same dataset and using a similar training pipeline as\nLLaVA-NEXT. Dimple-7B ultimately surpasses LLaVA-NEXT in performance by 3.9%,\ndemonstrating that DMLLM can achieve performance comparable to that of\nautoregressive models. To improve inference efficiency, we propose a decoding\nstrategy termed confident decoding, which dynamically adjusts the number of\ntokens generated at each step, significantly reducing the number of generation\niterations. In autoregressive models, the number of forward iterations during\ngeneration equals the response length. With confident decoding, however, the\nnumber of iterations needed by Dimple is even only $\\frac{\\text{response\nlength}}{3}$. We also re-implement the prefilling technique in autoregressive\nmodels and demonstrate that it does not significantly impact performance on\nmost benchmark evaluations, while offering a speedup of 1.5x to 7x.\nAdditionally, we explore Dimple's capability to precisely control its response\nusing structure priors. These priors enable structured responses in a manner\ndistinct from instruction-based or chain-of-thought prompting, and allow\nfine-grained control over response format and length, which is difficult to\nachieve in autoregressive models. Overall, this work validates the feasibility\nand advantages of DMLLM and enhances its inference efficiency and\ncontrollability. Code and models are available at\nhttps://github.com/yu-rp/Dimple.", "AI": {"tldr": "Dimple\u662f\u4e00\u79cd\u79bb\u6563\u6269\u6563\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08DMLLM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u56de\u5f52\u548c\u6269\u6563\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u4e86\u7eaf\u79bb\u6563\u6269\u6563\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8eLLaVA-NEXT 3.9%\uff0c\u5e76\u63d0\u51fa\u4e86\u9ad8\u6548\u89e3\u7801\u7b56\u7565\u548c\u7ed3\u6784\u5316\u54cd\u5e94\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u7eaf\u79bb\u6563\u6269\u6563\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u6027\u3001\u6027\u80fd\u4e0d\u4f73\u548c\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u63a2\u7d22DMLLM\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u521d\u59cb\u81ea\u56de\u5f52\u9636\u6bb5\u548c\u540e\u7eed\u6269\u6563\u9636\u6bb5\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u63d0\u51fa\u81ea\u4fe1\u89e3\u7801\u7b56\u7565\u548c\u9884\u586b\u5145\u6280\u672f\u3002", "result": "Dimple-7B\u6027\u80fd\u8d85\u8fc7LLaVA-NEXT 3.9%\uff0c\u89e3\u7801\u6548\u7387\u63d0\u5347\u81f3\u54cd\u5e94\u957f\u5ea6\u76841/3\uff0c\u9884\u586b\u5145\u6280\u672f\u63d0\u901f1.5x\u81f37x\u3002", "conclusion": "Dimple\u9a8c\u8bc1\u4e86DMLLM\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2505.16090", "pdf": "https://arxiv.org/pdf/2505.16090", "abs": "https://arxiv.org/abs/2505.16090", "authors": ["Dominick Kubica", "Dylan T. Gordon", "Nanami Emura", "Derleen Saini", "Charlie Goldenberg"], "title": "Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": "6 pages, 4 figures. Research conducted as part of a\n  Microsoft-sponsored Capstone Project at Santa Clara University", "summary": "As of 2025, Generative Artificial Intelligence (GenAI) has become a central\ntool for productivity across industries. Beyond text generation, GenAI now\nplays a critical role in coding, data analysis, and research workflows. As\nlarge language models (LLMs) continue to evolve, it is essential to assess the\nreliability and accuracy of their outputs, especially in specialized,\nhigh-stakes domains like finance. Most modern LLMs transform text into\nnumerical vectors, which are used in operations such as cosine similarity\nsearches to generate responses. However, this abstraction process can lead to\nmisinterpretation of emotional tone, particularly in nuanced financial\ncontexts. While LLMs generally excel at identifying sentiment in everyday\nlanguage, these models often struggle with the nuanced, strategically ambiguous\nlanguage found in earnings call transcripts. Financial disclosures frequently\nembed sentiment in hedged statements, forward-looking language, and\nindustry-specific jargon, making it difficult even for human analysts to\ninterpret consistently, let alone AI models. This paper presents findings from\nthe Santa Clara Microsoft Practicum Project, led by Professor Charlie\nGoldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's\nChatGPT, Google's Gemini, and traditional machine learning models for sentiment\nanalysis of financial text. Using Microsoft earnings call transcripts, the\nanalysis assesses how well LLM-derived sentiment correlates with market\nsentiment and stock movements and evaluates the accuracy of model outputs.\nPrompt engineering techniques are also examined to improve sentiment analysis\nresults. Visualizations of sentiment consistency are developed to evaluate\nalignment between tone and stock performance, with sentiment trends analyzed\nacross Microsoft's lines of business to determine which segments exert the\ngreatest influence.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u751f\u6210\u5f0fAI\u5728\u91d1\u878d\u9886\u57df\u60c5\u611f\u5206\u6790\u7684\u53ef\u9760\u6027\uff0c\u6bd4\u8f83\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Copilot\u3001ChatGPT\u3001Gemini\uff09\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u63d0\u793a\u5de5\u7a0b\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u5404\u884c\u4e1a\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u4e9f\u5f85\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u60c5\u611f\u8bed\u8a00\u65f6\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5fae\u8f6f\u8d22\u62a5\u7535\u8bdd\u4f1a\u8bae\u6587\u672c\uff0c\u8bc4\u4f30LLM\u7684\u60c5\u611f\u5206\u6790\u7ed3\u679c\u4e0e\u5e02\u573a\u60c5\u7eea\u53ca\u80a1\u4ef7\u53d8\u52a8\u7684\u76f8\u5173\u6027\uff0c\u5e76\u6d4b\u8bd5\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u5728\u5904\u7406\u91d1\u878d\u6587\u672c\u4e2d\u7684\u590d\u6742\u60c5\u611f\u8bed\u8a00\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u63d0\u793a\u5de5\u7a0b\u53ef\u90e8\u5206\u6539\u5584\u7ed3\u679c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u91d1\u878d\u60c5\u611f\u5206\u6790\u4e2d\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002"}}
{"id": "2505.16991", "pdf": "https://arxiv.org/pdf/2505.16991", "abs": "https://arxiv.org/abs/2505.16991", "authors": ["Abdul Hannan", "Alessio Brutti", "Shah Nawaz", "Mubashir Noman"], "title": "An Effective Training Framework for Light-Weight Automatic Speech Recognition Models", "categories": ["cs.CV"], "comment": "Accepted at InterSpeech 2025", "summary": "Recent advancement in deep learning encouraged developing large automatic\nspeech recognition (ASR) models that achieve promising results while ignoring\ncomputational and memory constraints. However, deploying such models on low\nresource devices is impractical despite of their favorable performance.\nExisting approaches (pruning, distillation, layer skip etc.) transform the\nlarge models into smaller ones at the cost of significant performance\ndegradation or require prolonged training of smaller models for better\nperformance. To address these issues, we introduce an efficacious two-step\nrepresentation learning based approach capable of producing several small sized\nmodels from a single large model ensuring considerably better performance in\nlimited number of epochs. Comprehensive experimentation on ASR benchmarks\nreveals the efficacy of our approach, achieving three-fold training speed-up\nand up to 12.54% word error rate improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e24\u6b65\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u4ece\u5355\u4e00\u5927\u578b\u6a21\u578b\u4e2d\u751f\u6210\u591a\u4e2a\u5c0f\u578b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e14\u8bad\u7ec3\u901f\u5ea6\u5feb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u526a\u679d\u3001\u84b8\u998f\u7b49\uff09\u5728\u5c06\u5927\u578b\u6a21\u578b\u538b\u7f29\u4e3a\u5c0f\u578b\u6a21\u578b\u65f6\uff0c\u6027\u80fd\u4e0b\u964d\u660e\u663e\u6216\u8bad\u7ec3\u65f6\u95f4\u957f\uff0c\u96be\u4ee5\u5728\u4f4e\u8d44\u6e90\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u5355\u4e00\u5927\u578b\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5c0f\u578b\u6a21\u578b\uff0c\u786e\u4fdd\u5728\u6709\u9650\u8bad\u7ec3\u5468\u671f\u5185\u6027\u80fd\u66f4\u4f18\u3002", "result": "\u5728ASR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4e09\u500d\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u548c\u6700\u9ad812.54%\u7684\u8bcd\u9519\u8bef\u7387\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bbe\u5907\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2505.16094", "pdf": "https://arxiv.org/pdf/2505.16094", "abs": "https://arxiv.org/abs/2505.16094", "authors": ["Ziqing Wang", "Kexin Zhang", "Zihan Zhao", "Yibo Wen", "Abhishek Pandey", "Han Liu", "Kaize Ding"], "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization", "categories": ["cs.LG", "cs.CL"], "comment": "Under review", "summary": "Large language models (LLMs) are introducing a paradigm shift in molecular\ndiscovery by enabling text-guided interaction with chemical spaces through\nnatural language, symbolic notations, with emerging extensions to incorporate\nmulti-modal inputs. To advance the new field of LLM for molecular discovery,\nthis survey provides an up-to-date and forward-looking review of the emerging\nuse of LLMs for two central tasks: molecule generation and molecule\noptimization. Based on our proposed taxonomy for both problems, we analyze\nrepresentative techniques in each category, highlighting how LLM capabilities\nare leveraged across different learning settings. In addition, we include the\ncommonly used datasets and evaluation protocols. We conclude by discussing key\nchallenges and future directions, positioning this survey as a resource for\nresearchers working at the intersection of LLMs and molecular science. A\ncontinuously updated reading list is available at\nhttps://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u5b50\u751f\u6210\u548c\u4f18\u5316\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u6280\u672f\u548c\u8d44\u6e90\u7684\u66f4\u65b0\u3002", "motivation": "\u63a8\u52a8LLMs\u5728\u5206\u5b50\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u5206\u6790\u4ee3\u8868\u6027\u6280\u672f\uff0c\u603b\u7ed3\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u7efc\u8ff0\u4e86LLMs\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u8d44\u6e90\u5217\u8868\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2505.16993", "pdf": "https://arxiv.org/pdf/2505.16993", "abs": "https://arxiv.org/abs/2505.16993", "authors": ["Guillem Bras\u00f3", "Aljo\u0161a O\u0161ep", "Laura Leal-Taix\u00e9"], "title": "Native Segmentation Vision Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Uniform downsampling remains the de facto standard for reducing spatial\nresolution in vision backbones. In this work, we propose an alternative design\nbuilt around a content-aware spatial grouping layer, that dynamically assigns\ntokens to a reduced set based on image boundaries and their semantic content.\nStacking our grouping layer across consecutive backbone stages results in\nhierarchical segmentation that arises natively in the feature extraction\nprocess, resulting in our coined Native Segmentation Vision Transformer. We\nshow that a careful design of our architecture enables the emergence of strong\nsegmentation masks solely from grouping layers, that is, without additional\nsegmentation-specific heads. This sets the foundation for a new paradigm of\nnative, backbone-level segmentation, which enables strong zero-shot results\nwithout mask supervision, as well as a minimal and efficient standalone model\ndesign for downstream segmentation tasks. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/native-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5185\u5bb9\u611f\u77e5\u7684\u7a7a\u95f4\u5206\u7ec4\u5c42\u7684\u89c6\u89c9Transformer\u8bbe\u8ba1\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914dtoken\u5b9e\u73b0\u5206\u8fa8\u7387\u964d\u4f4e\uff0c\u65e0\u9700\u989d\u5916\u5206\u5272\u5934\u5373\u53ef\u751f\u6210\u5f3a\u5206\u5272\u63a9\u7801\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u4e0b\u91c7\u6837\u5728\u89c6\u89c9\u4e3b\u5e72\u4e2d\u4ecd\u662f\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u8fb9\u754c\u548c\u8bed\u4e49\u5185\u5bb9\u7684\u52a8\u6001\u9002\u5e94\u3002", "method": "\u8bbe\u8ba1\u5185\u5bb9\u611f\u77e5\u7684\u7a7a\u95f4\u5206\u7ec4\u5c42\uff0c\u52a8\u6001\u5206\u914dtoken\uff0c\u5f62\u6210\u5c42\u6b21\u5316\u5206\u5272\uff0c\u6784\u5efaNative Segmentation Vision Transformer\u3002", "result": "\u4ec5\u901a\u8fc7\u5206\u7ec4\u5c42\u5373\u53ef\u751f\u6210\u5f3a\u5206\u5272\u63a9\u7801\uff0c\u652f\u6301\u96f6\u6837\u672c\u5206\u5272\u548c\u9ad8\u6548\u4e0b\u6e38\u4efb\u52a1\u8bbe\u8ba1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u539f\u751f\u5206\u5272\u8303\u5f0f\uff0c\u4e3a\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u65e0\u9700\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16100", "pdf": "https://arxiv.org/pdf/2505.16100", "abs": "https://arxiv.org/abs/2505.16100", "authors": ["Zifeng Wang", "Benjamin Danek", "Jimeng Sun"], "title": "BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Validating scientific hypotheses is a central challenge in biomedical\nresearch, and remains difficult for artificial intelligence (AI) agents due to\nthe complexity of real-world data analysis and evidence interpretation. In this\nwork, we present BioDSA-1K, a benchmark designed to evaluate AI agents on\nrealistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K\nconsists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,\ncurated from over 300 published biomedical studies to reflect the structure and\nreasoning found in authentic research workflows. Each task includes a\nstructured hypothesis derived from the original study's conclusions, expressed\nin the affirmative to reflect the language of scientific reporting, and one or\nmore pieces of supporting evidence grounded in empirical data tables. While\nthese hypotheses mirror published claims, they remain testable using standard\nstatistical or machine learning methods. The benchmark enables evaluation along\nfour axes: (1) hypothesis decision accuracy, (2) alignment between evidence and\nconclusion, (3) correctness of the reasoning process, and (4) executability of\nthe AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable\nhypotheses: cases where the available data are insufficient to support or\nrefute a claim, reflecting a common yet underexplored scenario in real-world\nscience. We propose BioDSA-1K as a foundation for building and evaluating\ngeneralizable, trustworthy AI agents for biomedical discovery.", "AI": {"tldr": "BioDSA-1K\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u751f\u7269\u533b\u5b66\u5047\u8bbe\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\uff0c\u5305\u542b1029\u4e2a\u4efb\u52a1\u548c1177\u4e2a\u5206\u6790\u8ba1\u5212\uff0c\u652f\u6301\u591a\u7ef4\u8bc4\u4f30\u3002", "motivation": "\u9a8c\u8bc1\u79d1\u5b66\u5047\u8bbe\u662f\u751f\u7269\u533b\u5b66\u7814\u7a76\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f46AI\u4ee3\u7406\u56e0\u6570\u636e\u590d\u6742\u6027\u548c\u8bc1\u636e\u89e3\u91ca\u56f0\u96be\u800c\u96be\u4ee5\u80dc\u4efb\u3002", "method": "BioDSA-1K\u4ece300\u591a\u9879\u7814\u7a76\u4e2d\u63d0\u53d6\u4efb\u52a1\u548c\u5206\u6790\u8ba1\u5212\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u5047\u8bbe\u548c\u57fa\u4e8e\u6570\u636e\u7684\u8bc1\u636e\uff0c\u652f\u6301\u7edf\u8ba1\u6216\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9a8c\u8bc1\u3002", "result": "\u57fa\u51c6\u652f\u6301\u56db\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\uff1a\u5047\u8bbe\u51b3\u7b56\u51c6\u786e\u6027\u3001\u8bc1\u636e\u4e0e\u7ed3\u8bba\u4e00\u81f4\u6027\u3001\u63a8\u7406\u8fc7\u7a0b\u6b63\u786e\u6027\u53caAI\u751f\u6210\u4ee3\u7801\u7684\u53ef\u6267\u884c\u6027\u3002", "conclusion": "BioDSA-1K\u4e3a\u6784\u5efa\u548c\u8bc4\u4f30\u53ef\u4fe1\u8d56\u7684\u751f\u7269\u533b\u5b66AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u7279\u522b\u5173\u6ce8\u6570\u636e\u4e0d\u8db3\u65f6\u7684\u975e\u53ef\u9a8c\u8bc1\u5047\u8bbe\u3002"}}
{"id": "2505.17001", "pdf": "https://arxiv.org/pdf/2505.17001", "abs": "https://arxiv.org/abs/2505.17001", "authors": ["Ming Qian", "Bin Tan", "Qiuyu Wang", "Xianwei Zheng", "Hanjiang Xiong", "Gui-Song Xia", "Yujun Shen", "Nan Xue"], "title": "Seeing through Satellite Images at Street Views", "categories": ["cs.CV"], "comment": "Project page: https://qianmingduowan.github.io/sat2density-pp/,\n  journal extension of ICCV 2023 conference paper 'Sat2Density: Faithful\n  Density Learning from Satellite-Ground Image Pairs', submitted to TPAMI", "summary": "This paper studies the task of SatStreet-view synthesis, which aims to render\nphotorealistic street-view panorama images and videos given any satellite image\nand specified camera positions or trajectories. We formulate to learn neural\nradiance field from paired images captured from satellite and street\nviewpoints, which comes to be a challenging learning problem due to the\nsparse-view natural and the extremely-large viewpoint changes between satellite\nand street-view images. We tackle the challenges based on a task-specific\nobservation that street-view specific elements, including the sky and\nillumination effects are only visible in street-view panoramas, and present a\nnovel approach Sat2Density++ to accomplish the goal of photo-realistic\nstreet-view panoramas rendering by modeling these street-view specific in\nneural networks. In the experiments, our method is testified on both urban and\nsuburban scene datasets, demonstrating that Sat2Density++ is capable of\nrendering photorealistic street-view panoramas that are consistent across\nmultiple views and faithful to the satellite image.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSat2Density++\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u8f90\u5c04\u573a\u4ece\u536b\u661f\u548c\u8857\u666f\u56fe\u50cf\u5bf9\u4e2d\u5b66\u4e60\uff0c\u5b9e\u73b0\u903c\u771f\u7684\u8857\u666f\u5168\u666f\u56fe\u50cf\u548c\u89c6\u9891\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u536b\u661f\u4e0e\u8857\u666f\u56fe\u50cf\u95f4\u89c6\u89d2\u53d8\u5316\u5927\u4e14\u6570\u636e\u7a00\u758f\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u8857\u666f\u7279\u5b9a\u5143\u7d20\uff08\u5982\u5929\u7a7a\u548c\u5149\u7167\u6548\u679c\uff09\u7684\u903c\u771f\u6e32\u67d3\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u5b66\u4e60\uff0c\u5efa\u6a21\u8857\u666f\u7279\u5b9a\u5143\u7d20\uff0c\u63d0\u51faSat2Density++\u65b9\u6cd5\u3002", "result": "\u5728\u57ce\u4e61\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u4e14\u5fe0\u5b9e\u4e8e\u536b\u661f\u56fe\u50cf\u7684\u903c\u771f\u8857\u666f\u5168\u666f\u3002", "conclusion": "Sat2Density++\u6210\u529f\u89e3\u51b3\u4e86\u8857\u666f\u5408\u6210\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002"}}
{"id": "2505.17002", "pdf": "https://arxiv.org/pdf/2505.17002", "abs": "https://arxiv.org/abs/2505.17002", "authors": ["Abdul Hannan", "Muhammad Arslan Manzoor", "Shah Nawaz", "Muhammad Irzam Liaqat", "Markus Schedl", "Mubashir Noman"], "title": "PAEFF: Precise Alignment and Enhanced Gated Feature Fusion for Face-Voice Association", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at InterSpeech 2025", "summary": "We study the task of learning association between faces and voices, which is\ngaining interest in the multimodal community lately. These methods suffer from\nthe deliberate crafting of negative mining procedures as well as the reliance\non the distant margin parameter. These issues are addressed by learning a joint\nembedding space in which orthogonality constraints are applied to the fused\nembeddings of faces and voices. However, embedding spaces of faces and voices\npossess different characteristics and require spaces to be aligned before\nfusing them. To this end, we propose a method that accurately aligns the\nembedding spaces and fuses them with an enhanced gated fusion thereby improving\nthe performance of face-voice association. Extensive experiments on the\nVoxCeleb dataset reveals the merits of the proposed approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u4eba\u8138\u4e0e\u58f0\u97f3\u5173\u8054\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u5d4c\u5165\u7a7a\u95f4\u548c\u589e\u5f3a\u95e8\u63a7\u878d\u5408\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u4eba\u8138\u4e0e\u58f0\u97f3\u5173\u8054\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8d1f\u6837\u672c\u6316\u6398\u548c\u4f9d\u8d56\u8fdc\u8ddd\u79bb\u53c2\u6570\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5148\u5bf9\u9f50\u4eba\u8138\u548c\u58f0\u97f3\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u518d\u901a\u8fc7\u589e\u5f3a\u95e8\u63a7\u878d\u5408\u8fdb\u884c\u8054\u5408\u5d4c\u5165\u3002", "result": "\u5728VoxCeleb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7a7a\u95f4\u5bf9\u9f50\u548c\u878d\u5408\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u8138\u4e0e\u58f0\u97f3\u5173\u8054\u7684\u6027\u80fd\u3002"}}
{"id": "2505.16148", "pdf": "https://arxiv.org/pdf/2505.16148", "abs": "https://arxiv.org/abs/2505.16148", "authors": ["Chongjie Si", "Kangtao Lv", "Jingjing Jiang", "Yadao Wang", "Yongwei Wang", "Xiaokang Yang", "Wenbo Su", "Bo Zheng", "Wei Shen"], "title": "NAN: A Training-Free Solution to Coefficient Estimation in Model Merging", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Model merging offers a training-free alternative to multi-task learning by\ncombining independently fine-tuned models into a unified one without access to\nraw data. However, existing approaches often rely on heuristics to determine\nthe merging coefficients, limiting their scalability and generality. In this\nwork, we revisit model merging through the lens of least-squares optimization\nand show that the optimal merging weights should scale with the amount of\ntask-specific information encoded in each model. Based on this insight, we\npropose NAN, a simple yet effective method that estimates model merging\ncoefficients via the inverse of parameter norm. NAN is training-free,\nplug-and-play, and applicable to a wide range of merging strategies. Extensive\nexperiments on show that NAN consistently improves performance of baseline\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5NAN\uff0c\u901a\u8fc7\u53c2\u6570\u8303\u6570\u7684\u9006\u4f30\u8ba1\u5408\u5e76\u7cfb\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u786e\u5b9a\u5408\u5e76\u7cfb\u6570\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u91cd\u65b0\u5ba1\u89c6\u6a21\u578b\u5408\u5e76\uff0c\u63d0\u51faNAN\u65b9\u6cd5\uff0c\u5229\u7528\u53c2\u6570\u8303\u6570\u7684\u9006\u4f30\u8ba1\u5408\u5e76\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNAN\u80fd\u6301\u7eed\u63d0\u5347\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "NAN\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5408\u5e76\u7b56\u7565\u3002"}}
{"id": "2505.17006", "pdf": "https://arxiv.org/pdf/2505.17006", "abs": "https://arxiv.org/abs/2505.17006", "authors": ["Jiange Yang", "Yansong Shi", "Haoyi Zhu", "Mingyu Liu", "Kaijing Ma", "Yating Wang", "Gangshan Wu", "Tong He", "Limin Wang"], "title": "CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning", "categories": ["cs.CV", "cs.RO"], "comment": "18 pages, 7 figures", "summary": "Learning latent motion from Internet videos is crucial for building\ngeneralist robots. However, existing discrete latent action methods suffer from\ninformation loss and struggle with complex and fine-grained dynamics. We\npropose CoMo, which aims to learn more informative continuous motion\nrepresentations from diverse, internet-scale videos. CoMo employs a early\ntemporal feature difference mechanism to prevent model collapse and suppress\nstatic appearance noise, effectively discouraging shortcut learning problem.\nFurthermore, guided by the information bottleneck principle, we constrain the\nlatent motion embedding dimensionality to achieve a better balance between\nretaining sufficient action-relevant information and minimizing the inclusion\nof action-irrelevant appearance noise. Additionally, we also introduce two new\nmetrics for more robustly and affordably evaluating motion and guiding motion\nlearning methods development: (i) the linear probing MSE of action prediction,\nand (ii) the cosine similarity between past-to-current and future-to-current\nmotion embeddings. Critically, CoMo exhibits strong zero-shot generalization,\nenabling it to generate continuous pseudo actions for previously unseen video\ndomains. This capability facilitates unified policy joint learning using pseudo\nactions derived from various action-less video datasets (such as\ncross-embodiment videos and, notably, human demonstration videos), potentially\naugmented with limited labeled robot data. Extensive experiments show that\npolicies co-trained with CoMo pseudo actions achieve superior performance with\nboth diffusion and autoregressive architectures in simulated and real-world\nsettings.", "AI": {"tldr": "CoMo\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4e92\u8054\u7f51\u89c6\u9891\u4e2d\u5b66\u4e60\u8fde\u7eed\u8fd0\u52a8\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u65b0\u6307\u6807\u548c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u79bb\u6563\u6f5c\u5728\u52a8\u4f5c\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u548c\u96be\u4ee5\u5904\u7406\u590d\u6742\u52a8\u6001\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8fde\u7eed\u8fd0\u52a8\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "CoMo\u91c7\u7528\u65e9\u671f\u65f6\u95f4\u7279\u5f81\u5dee\u5f02\u673a\u5236\u9632\u6b62\u6a21\u578b\u5d29\u6e83\uff0c\u7ed3\u5408\u4fe1\u606f\u74f6\u9888\u539f\u5219\u7ea6\u675f\u5d4c\u5165\u7ef4\u5ea6\uff0c\u5e76\u5f15\u5165\u65b0\u8bc4\u4f30\u6307\u6807\u3002", "result": "CoMo\u5728\u96f6\u6837\u672c\u6cdb\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u751f\u6210\u672a\u89c1\u89c6\u9891\u57df\u7684\u4f2a\u52a8\u4f5c\uff0c\u63d0\u5347\u7b56\u7565\u8054\u5408\u5b66\u4e60\u6027\u80fd\u3002", "conclusion": "CoMo\u901a\u8fc7\u8fde\u7eed\u8fd0\u52a8\u8868\u793a\u548c\u65b0\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u5b66\u4e60\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u9891\u57df\u548c\u4efb\u52a1\u3002"}}
{"id": "2505.17008", "pdf": "https://arxiv.org/pdf/2505.17008", "abs": "https://arxiv.org/abs/2505.17008", "authors": ["Jean Pablo Vieira de Mello", "Matheus Augusto Alves Cuglieri", "Leandro P. de Figueiredo", "Fernando Bordignon", "Marcelo Ramalho Albuquerque", "Rodrigo Surmas", "Bruno Cavalcanti de Paula"], "title": "Deep mineralogical segmentation of thin section images based on QEMSCAN maps", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Interpreting the mineralogical aspects of rock thin sections is an important\ntask for oil and gas reservoirs evaluation. However, human analysis tend to be\nsubjective and laborious. Technologies like QEMSCAN(R) are designed to automate\nthe mineralogical mapping process, but also suffer from limitations like high\nmonetary costs and time-consuming analysis. This work proposes a Convolutional\nNeural Network model for automatic mineralogical segmentation of thin section\nimages of carbonate rocks. The model is able to mimic the QEMSCAN mapping\nitself in a low-cost, generalized and efficient manner. For this, the U-Net\nsemantic segmentation architecture is trained on plane and cross polarized thin\nsection images using the corresponding QEMSCAN maps as target, which is an\napproach not widely explored. The model was instructed to differentiate\noccurrences of Calcite, Dolomite, Mg-Clay Minerals, Quartz, Pores and the\nremaining mineral phases as an unique class named \"Others\", while it was\nvalidated on rock facies both seen and unseen during training, in order to\naddress its generalization capability. Since the images and maps are provided\nin different resolutions, image registration was applied to align then\nspatially. The study reveals that the quality of the segmentation is very much\ndependent on these resolution differences and on the variety of learnable rock\ntextures. However, it shows promising results, especially with regard to the\nproper delineation of minerals boundaries on solid textures and precise\nestimation of the minerals distributions, describing a nearly linear\nrelationship between expected and predicted distributions, with coefficient of\ndetermination (R^2) superior to 0.97 for seen facies and 0.88 for unseen.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u78b3\u9178\u76d0\u5ca9\u8584\u7247\u56fe\u50cf\u77ff\u7269\u81ea\u52a8\u5206\u5272\u6a21\u578b\uff0c\u4ee5\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u65b9\u5f0f\u6a21\u62dfQEMSCAN\u77ff\u7269\u6620\u5c04\u3002", "motivation": "\u4eba\u5de5\u5206\u6790\u5ca9\u77f3\u8584\u7247\u7684\u77ff\u7269\u5b66\u7279\u5f81\u4e3b\u89c2\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u6280\u672f\u5982QEMSCAN\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u957f\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528U-Net\u8bed\u4e49\u5206\u5272\u67b6\u6784\uff0c\u4ee5\u5e73\u9762\u548c\u4ea4\u53c9\u504f\u632f\u8584\u7247\u56fe\u50cf\u53caQEMSCAN\u6620\u5c04\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u8fdb\u884c\u77ff\u7269\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u5728\u77ff\u7269\u8fb9\u754c\u5212\u5206\u548c\u5206\u5e03\u4f30\u8ba1\u4e0a\u8868\u73b0\u826f\u597d\uff0cR\u00b2\u503c\u5728\u5df2\u77e5\u548c\u672a\u77e5\u5ca9\u76f8\u4e2d\u5206\u522b\u8d85\u8fc70.97\u548c0.88\u3002", "conclusion": "\u6a21\u578b\u5728\u77ff\u7269\u5206\u5272\u4e0a\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5206\u5272\u8d28\u91cf\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u5ca9\u77f3\u7eb9\u7406\u591a\u6837\u6027\u5f71\u54cd\u3002"}}
{"id": "2505.16176", "pdf": "https://arxiv.org/pdf/2505.16176", "abs": "https://arxiv.org/abs/2505.16176", "authors": ["Jun Rao", "Xuebo Liu", "Hexuan Deng", "Zepeng Lin", "Zixiong Yu", "Jiansheng Wei", "Xiaojun Meng", "Min Zhang"], "title": "Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In the realm of data selection for reasoning tasks, existing approaches\npredominantly rely on externally predefined static metrics such as difficulty\nand diversity, which are often designed for supervised fine-tuning (SFT) and\nlack adaptability to continuous training processes. A critical limitation of\nthese methods is their inability to dynamically align with the evolving\ncapabilities of models during online training, a gap that becomes increasingly\npronounced with the rise of dynamic training paradigms and online reinforcement\nlearning (RL) frameworks (e.g., R1 models). To address this, we introduce\nSAI-DPO, an algorithm that dynamically selects training data by continuously\nassessing a model's stage-specific reasoning abilities across different\ntraining phases. By integrating real-time model performance feedback, SAI-DPO\nadaptively adapts data selection to the evolving strengths and weaknesses of\nthe model, thus enhancing both data utilization efficiency and final task\nperformance. Extensive experiments on three state-of-the-art models and eight\nmathematical reasoning benchmarks, including challenging competition-level\ndatasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average\nperformance boost of up to 21.3 percentage points, with particularly notable\nimprovements of 10 and 15 points on AIME24 and AMC23, respectively. These\nresults highlight the superiority of dynamic, model-adaptive data selection\nover static, externally defined strategies in advancing reasoning.", "AI": {"tldr": "SAI-DPO\u662f\u4e00\u79cd\u52a8\u6001\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u5316\u6570\u636e\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6307\u6807\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u6a21\u578b\u80fd\u529b\u7684\u6301\u7eed\u63d0\u5347\u3002", "method": "\u63d0\u51faSAI-DPO\u7b97\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u5b9e\u65f6\u6a21\u578b\u6027\u80fd\u53cd\u9988\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6570\u636e\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAI-DPO\u5e73\u5747\u6027\u80fd\u63d0\u534721.3\u4e2a\u767e\u5206\u70b9\uff0c\u5728AIME24\u548cAMC23\u4e0a\u5206\u522b\u63d0\u534710\u548c15\u5206\u3002", "conclusion": "\u52a8\u6001\u3001\u6a21\u578b\u81ea\u9002\u5e94\u7684\u6570\u636e\u9009\u62e9\u7b56\u7565\u4f18\u4e8e\u9759\u6001\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2505.17011", "pdf": "https://arxiv.org/pdf/2505.17011", "abs": "https://arxiv.org/abs/2505.17011", "authors": ["Yan Li", "Changyao Tian", "Renqiu Xia", "Ning Liao", "Weiwei Guo", "Junchi Yan", "Hongsheng Li", "Jifeng Dai", "Hao Li", "Xue Yang"], "title": "Learning Adaptive and Temporally Causal Video Tokenization in a 1D Latent Space", "categories": ["cs.CV"], "comment": "Code: https://github.com/VisionXLab/AdapTok", "summary": "We propose AdapTok, an adaptive temporal causal video tokenizer that can\nflexibly allocate tokens for different frames based on video content. AdapTok\nis equipped with a block-wise masking strategy that randomly drops tail tokens\nof each block during training, and a block causal scorer to predict the\nreconstruction quality of video frames using different numbers of tokens.\nDuring inference, an adaptive token allocation strategy based on integer linear\nprogramming is further proposed to adjust token usage given predicted scores.\nSuch design allows for sample-wise, content-aware, and temporally dynamic token\nallocation under a controllable overall budget. Extensive experiments for video\nreconstruction and generation on UCF-101 and Kinetics-600 demonstrate the\neffectiveness of our approach. Without additional image data, AdapTok\nconsistently improves reconstruction quality and generation performance under\ndifferent token budgets, allowing for more scalable and token-efficient\ngenerative video modeling.", "AI": {"tldr": "AdapTok\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u89c6\u9891\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u6807\u8bb0\u63d0\u5347\u89c6\u9891\u91cd\u5efa\u548c\u751f\u6210\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u5185\u5bb9\u4e2d\u4e0d\u540c\u5e27\u7684\u6807\u8bb0\u5206\u914d\u95ee\u9898\uff0c\u63d0\u5347\u6807\u8bb0\u5229\u7528\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5757\u63a9\u7801\u7b56\u7565\u548c\u5757\u56e0\u679c\u8bc4\u5206\u5668\uff0c\u7ed3\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u52a8\u6001\u5206\u914d\u6807\u8bb0\u3002", "result": "\u5728UCF-101\u548cKinetics-600\u4e0a\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u548c\u751f\u6210\u6027\u80fd\u3002", "conclusion": "AdapTok\u5728\u53ef\u63a7\u6807\u8bb0\u9884\u7b97\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u52a8\u6001\u7684\u89c6\u9891\u5efa\u6a21\u3002"}}
{"id": "2505.17012", "pdf": "https://arxiv.org/pdf/2505.17012", "abs": "https://arxiv.org/abs/2505.17012", "authors": ["Haoning Wu", "Xiao Huang", "Yaohui Chen", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report; Project Page:\n  https://haoningwu3639.github.io/SpatialScore", "summary": "Multimodal large language models (MLLMs) have achieved impressive success in\nquestion-answering tasks, yet their capabilities for spatial understanding are\nless explored. This work investigates a critical question: do existing MLLMs\npossess 3D spatial perception and understanding abilities? Concretely, we make\nthe following contributions in this paper: (i) we introduce VGBench, a\nbenchmark specifically designed to assess MLLMs for visual geometry perception,\ne.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most\ncomprehensive and diverse multimodal spatial understanding benchmark to date,\nintegrating VGBench with relevant data from the other 11 existing datasets.\nThis benchmark comprises 28K samples across various spatial understanding\ntasks, modalities, and QA formats, along with a carefully curated challenging\nsubset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent\nsystem incorporating 9 specialized tools for spatial understanding, supporting\nboth Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive\nevaluations to reveal persistent challenges in spatial reasoning while\ndemonstrating the effectiveness of SpatialAgent. We believe SpatialScore will\noffer valuable insights and serve as a rigorous benchmark for the next\nevolution of MLLMs.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57283D\u7a7a\u95f4\u611f\u77e5\u548c\u7406\u89e3\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86VGBench\u548cSpatialScore\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u5f00\u53d1\u4e86SpatialAgent\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u63a2\u7d22\u73b0\u6709MLLMs\u662f\u5426\u5177\u59073D\u7a7a\u95f4\u611f\u77e5\u548c\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u76f8\u5173\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "1. \u63d0\u51faVGBench\u8bc4\u4f30\u89c6\u89c9\u51e0\u4f55\u611f\u77e5\u80fd\u529b\uff1b2. \u6574\u540811\u4e2a\u6570\u636e\u96c6\u6784\u5efaSpatialScore\u57fa\u51c6\uff1b3. \u5f00\u53d1\u591a\u4ee3\u7406\u7cfb\u7edfSpatialAgent\uff0c\u652f\u6301\u4e24\u79cd\u63a8\u7406\u8303\u5f0f\u3002", "result": "\u63ed\u793a\u4e86MLLMs\u5728\u7a7a\u95f4\u63a8\u7406\u4e2d\u7684\u6301\u7eed\u6311\u6218\uff0c\u540c\u65f6\u8bc1\u660e\u4e86SpatialAgent\u7684\u6709\u6548\u6027\u3002", "conclusion": "SpatialScore\u4e3aMLLMs\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u4e25\u683c\u57fa\u51c6\u3002"}}
{"id": "2505.16186", "pdf": "https://arxiv.org/pdf/2505.16186", "abs": "https://arxiv.org/abs/2505.16186", "authors": ["Kaiwen Zhou", "Xuandong Zhao", "Gaowen Liu", "Jayanth Srinivasa", "Aosong Feng", "Dawn Song", "Xin Eric Wang"], "title": "SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs) introduce a new generation paradigm of\nexplicitly reasoning before answering, leading to remarkable improvements in\ncomplex tasks. However, they pose great safety risks against harmful queries\nand adversarial attacks. While recent mainstream safety efforts on LRMs,\nsupervised fine-tuning (SFT), improve safety performance, we find that\nSFT-aligned models struggle to generalize to unseen jailbreak prompts. After\nthorough investigation of LRMs' generation, we identify a safety aha moment\nthat can activate safety reasoning and lead to a safe response. This aha moment\ntypically appears in the `key sentence', which follows models' query\nunderstanding process and can indicate whether the model will proceed safely.\nBased on these insights, we propose SafeKey, including two complementary\nobjectives to better activate the safety aha moment in the key sentence: (1) a\nDual-Path Safety Head to enhance the safety signal in the model's internal\nrepresentations before the key sentence, and (2) a Query-Mask Modeling\nobjective to improve the models' attention on its query understanding, which\nhas important safety hints. Experiments across multiple safety benchmarks\ndemonstrate that our methods significantly improve safety generalization to a\nwide range of jailbreak attacks and out-of-distribution harmful prompts,\nlowering the average harmfulness rate by 9.6\\%, while maintaining general\nabilities. Our analysis reveals how SafeKey enhances safety by reshaping\ninternal attention and improving the quality of hidden representations.", "AI": {"tldr": "SafeKey\u901a\u8fc7\u6fc0\u6d3bLRMs\u7684\u5b89\u5168\u2018\u987f\u609f\u65f6\u523b\u2019\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u6709\u5bb3\u67e5\u8be2\u548c\u5bf9\u6297\u653b\u51fb\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709SFT\u5bf9\u9f50\u7684LRMs\u5728\u672a\u89c1\u8fc7\u7684\u8d8a\u72f1\u63d0\u793a\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51faSafeKey\uff0c\u5305\u62ec\u53cc\u8def\u5f84\u5b89\u5168\u5934\u548c\u67e5\u8be2\u63a9\u7801\u5efa\u6a21\u76ee\u6807\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u5b89\u5168\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSafeKey\u5c06\u5e73\u5747\u6709\u5bb3\u7387\u964d\u4f4e9.6%\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "conclusion": "SafeKey\u901a\u8fc7\u91cd\u5851\u5185\u90e8\u6ce8\u610f\u529b\u548c\u63d0\u5347\u9690\u85cf\u8868\u793a\u8d28\u91cf\uff0c\u6709\u6548\u589e\u5f3aLRMs\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2505.17015", "pdf": "https://arxiv.org/pdf/2505.17015", "abs": "https://arxiv.org/abs/2505.17015", "authors": ["Runsen Xu", "Weiyao Wang", "Hao Tang", "Xingyu Chen", "Xiaodong Wang", "Fu-Jen Chu", "Dahua Lin", "Matt Feiszli", "Kevin J. Liang"], "title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "24 pages. An MLLM, dataset, and benchmark for multi-frame spatial\n  understanding. Project page: https://runsenxu.com/projects/Multi-SpatialMLLM", "summary": "Multi-modal large language models (MLLMs) have rapidly advanced in visual\ntasks, yet their spatial understanding remains limited to single images,\nleaving them ill-suited for robotics and other real-world applications that\nrequire multi-frame reasoning. In this paper, we propose a framework to equip\nMLLMs with robust multi-frame spatial understanding by integrating depth\nperception, visual correspondence, and dynamic perception. Central to our\napproach is the MultiSPA dataset, a novel, large-scale collection of more than\n27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we\nintroduce a comprehensive benchmark that tests a wide spectrum of spatial tasks\nunder uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves\nsignificant gains over baselines and proprietary systems, demonstrating\nscalable, generalizable multi-frame reasoning. We further observe multi-task\nbenefits and early indications of emergent capabilities in challenging\nscenarios, and showcase how our model can serve as a multi-frame reward\nannotator for robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u611f\u77e5\u3001\u89c6\u89c9\u5bf9\u5e94\u548c\u52a8\u6001\u611f\u77e5\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u591a\u5e27\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5f15\u5165MultiSPA\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u5176\u7a7a\u95f4\u7406\u89e3\u4ec5\u9650\u4e8e\u5355\u5e27\u56fe\u50cf\uff0c\u65e0\u6cd5\u6ee1\u8db3\u673a\u5668\u4eba\u7b49\u9700\u8981\u591a\u5e27\u63a8\u7406\u7684\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u611f\u77e5\u3001\u89c6\u89c9\u5bf9\u5e94\u548c\u52a8\u6001\u611f\u77e5\uff0c\u5e76\u5229\u7528\u65b0\u6784\u5efa\u7684MultiSPA\u6570\u636e\u96c6\uff08\u5305\u542b2700\u4e07\u6837\u672c\uff09\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u63d0\u51fa\u7684Multi-SpatialMLLM\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u4e13\u6709\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u591a\u5e27\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u673a\u5668\u4eba\u7b49\u9886\u57df\u4f5c\u4e3a\u591a\u5e27\u5956\u52b1\u6807\u6ce8\u5668\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16210", "pdf": "https://arxiv.org/pdf/2505.16210", "abs": "https://arxiv.org/abs/2505.16210", "authors": ["Zhihang Cai", "Xingjun Zhang", "Zhendong Tan", "Zheng Wei"], "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "11 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNQKV\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316KV\u7f13\u5b58\u81f3\u66f4\u4f4e\u6bd4\u7279\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\u3002", "motivation": "LLM\u63a8\u7406\u4e2dKV\u7f13\u5b58\u7684\u5185\u5b58\u6d88\u8017\u6210\u4e3a\u74f6\u9888\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u4ec5\u652f\u63018\u6bd4\u7279\u4e14\u66f4\u4f4e\u6bd4\u7279\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002", "method": "\u5206\u6790KV\u7f13\u5b58\u7684\u5143\u7d20\u5206\u5e03\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5206\u5757\u6b63\u6001\u5206\u5e03\u7684NQKV\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4fe1\u606f\u8bba\u6700\u4f18\u91cf\u5316\u8bef\u5dee\u3002", "result": "NQKV\u4f7fOPT\u6a21\u578b\u63a8\u7406\u6279\u91cf\u589e\u59272\u500d\u6216\u4e0a\u4e0b\u6587\u957f\u5ea6\u5ef6\u957f4\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53479.3\u500d\u3002", "conclusion": "NQKV\u6709\u6548\u964d\u4f4eKV\u7f13\u5b58\u5185\u5b58\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\uff0c\u4e14\u4e0d\u5f71\u54cd\u8f93\u51fa\u8d28\u91cf\u3002"}}
{"id": "2505.17017", "pdf": "https://arxiv.org/pdf/2505.17017", "abs": "https://arxiv.org/abs/2505.17017", "authors": ["Chengzhuo Tong", "Ziyu Guo", "Renrui Zhang", "Wenyu Shan", "Xinyu Wei", "Zhenghao Xing", "Hongsheng Li", "Pheng-Ann Heng"], "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Code is released at https://github.com/ZiyuGuo99/Image-Generation-CoT", "summary": "Recent advancements underscore the significant role of Reinforcement Learning\n(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large\nlanguage models (LLMs). Two prominent RL algorithms, Direct Preference\nOptimization (DPO) and Group Relative Policy Optimization (GRPO), are central\nto these developments, showcasing different pros and cons. Autoregressive image\ngeneration, also interpretable as a sequential CoT reasoning process, presents\nunique challenges distinct from LLM-based CoT reasoning. These encompass\nensuring text-image consistency, improving image aesthetic quality, and\ndesigning sophisticated reward models, rather than relying on simpler\nrule-based rewards. While recent efforts have extended RL to this domain, these\nexplorations typically lack an in-depth analysis of the domain-specific\nchallenges and the characteristics of different RL strategies. To bridge this\ngap, we provide the first comprehensive investigation of the GRPO and DPO\nalgorithms in autoregressive image generation, evaluating their in-domain\nperformance and out-of-domain generalization, while scrutinizing the impact of\ndifferent reward models on their respective capabilities. Our findings reveal\nthat GRPO and DPO exhibit distinct advantages, and crucially, that reward\nmodels possessing stronger intrinsic generalization capabilities potentially\nenhance the generalization potential of the applied RL algorithms. Furthermore,\nwe systematically explore three prevalent scaling strategies to enhance both\ntheir in-domain and out-of-domain proficiency, deriving unique insights into\nefficiently scaling performance for each paradigm. We hope our study paves a\nnew path for inspiring future work on developing more effective RL algorithms\nto achieve robust CoT reasoning in the realm of autoregressive image\ngeneration. Code is released at\nhttps://github.com/ZiyuGuo99/Image-Generation-CoT", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u6bd4\u8f83\u4e86DPO\u548cGRPO\u4e24\u79cd\u7b97\u6cd5\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4e86\u5956\u52b1\u6a21\u578b\u5bf9\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u586b\u8865RL\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u9886\u57df\u5e94\u7528\u7684\u7a7a\u767d\uff0c\u6df1\u5165\u5206\u6790\u4e0d\u540cRL\u7b56\u7565\u7684\u4f18\u7f3a\u70b9\u53ca\u9886\u57df\u7279\u5b9a\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30GRPO\u548cDPO\u7b97\u6cd5\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u5956\u52b1\u6a21\u578b\u5bf9\u7b97\u6cd5\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0GRPO\u548cDPO\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u80fd\u63d0\u5347RL\u7b97\u6cd5\u7684\u6cdb\u5316\u6f5c\u529b\uff1b\u63a2\u7d22\u4e86\u4e09\u79cd\u6269\u5c55\u7b56\u7565\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u6709\u6548\u7684RL\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u9c81\u68d2CoT\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16211", "pdf": "https://arxiv.org/pdf/2505.16211", "abs": "https://arxiv.org/abs/2505.16211", "authors": ["Kai Li", "Can Shen", "Yile Liu", "Jirui Han", "Kelong Zheng", "Xuechao Zou", "Zhe Wang", "Xingjian Du", "Shun Zhang", "Hanjun Luo", "Yingbin Jin", "Xinxin Xing", "Ziyang Ma", "Yue Liu", "Xiaojun Jia", "Yifan Zhang", "Junfeng Fang", "Kun Wang", "Yibo Yan", "Haoyang Li", "Yiming Li", "Xiaobin Zhuang", "Yang Liu", "Haibo Hu", "Zhuo Chen", "Zhizheng Wu", "Xiaolin Hu", "Eng-Siong Chng", "XiaoFeng Wang", "Wenyuan Xu", "Wei Dong", "Xinfeng Li"], "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Technical Report", "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.", "AI": {"tldr": "AudioTrust\u662f\u9996\u4e2a\u4e13\u4e3a\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u8bbe\u8ba1\u7684\u5168\u65b9\u4f4d\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u51c6\uff0c\u6db5\u76d6\u516c\u5e73\u6027\u3001\u5e7b\u89c9\u3001\u5b89\u5168\u6027\u3001\u9690\u79c1\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba4\u8bc1\u516d\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6a21\u6001\u6216\u4ec5\u5173\u6ce8\u6709\u9650\u7684\u5b89\u5168\u7ef4\u5ea6\uff0c\u672a\u80fd\u5145\u5206\u9002\u5e94\u97f3\u9891\u6a21\u6001\u7684\u7279\u6027\u548c\u5e94\u7528\u573a\u666f\u3002", "method": "AudioTrust\u57fa\u4e8e18\u79cd\u5b9e\u9a8c\u8bbe\u7f6e\u548c4,420\u4e2a\u97f3\u9891/\u6587\u672c\u6837\u672c\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e869\u4e2a\u97f3\u9891\u4e13\u7528\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u91c7\u7528\u81ea\u52a8\u5316\u8bc4\u5206\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524d\u5f00\u6e90\u548c\u95ed\u6e90ALLMs\u5728\u9ad8\u98ce\u9669\u97f3\u9891\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u5ea6\u8fb9\u754c\u548c\u5c40\u9650\u6027\u3002", "conclusion": "AudioTrust\u4e3a\u672a\u6765\u97f3\u9891\u6a21\u578b\u7684\u5b89\u5168\u53ef\u4fe1\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2505.17018", "pdf": "https://arxiv.org/pdf/2505.17018", "abs": "https://arxiv.org/abs/2505.17018", "authors": ["Kaixuan Fan", "Kaituo Feng", "Haoming Lyu", "Dongzhan Zhou", "Xiangyu Yue"], "title": "SophiaVL-R1: Reinforcing MLLMs Reasoning with Thinking Reward", "categories": ["cs.CV"], "comment": "Project page:https://github.com/kxfan2002/SophiaVL-R1", "summary": "Recent advances have shown success in eliciting strong reasoning abilities in\nmultimodal large language models (MLLMs) through rule-based reinforcement\nlearning (RL) with outcome rewards. However, this paradigm typically lacks\nsupervision over the thinking process leading to the final outcome.As a result,\nthe model may learn sub-optimal reasoning strategies, which can hinder its\ngeneralization ability. In light of this, we propose SophiaVL-R1, as an attempt\nto add reward signals for the thinking process in this paradigm. To achieve\nthis, we first train a thinking reward model that evaluates the quality of the\nentire thinking process. Given that the thinking reward may be unreliable for\ncertain samples due to reward hacking, we propose the Trust-GRPO method, which\nassigns a trustworthiness weight to the thinking reward during training. This\nweight is computed based on the thinking reward comparison of responses leading\nto correct answers versus incorrect answers, helping to mitigate the impact of\npotentially unreliable thinking rewards. Moreover, we design an annealing\ntraining strategy that gradually reduces the thinking reward over time,\nallowing the model to rely more on the accurate rule-based outcome reward in\nlater training stages. Experiments show that our SophiaVL-R1 surpasses a series\nof reasoning MLLMs on various benchmarks (e.g., MathVisita, MMMU),\ndemonstrating strong reasoning and generalization capabilities. Notably, our\nSophiaVL-R1-7B even outperforms LLaVA-OneVision-72B on most benchmarks, despite\nthe latter having 10 times more parameters. All code, models, and datasets are\nmade publicly available at https://github.com/kxfan2002/SophiaVL-R1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSophiaVL-R1\uff0c\u901a\u8fc7\u5f15\u5165\u601d\u7ef4\u8fc7\u7a0b\u5956\u52b1\u4fe1\u53f7\u548c\u6539\u8fdb\u7684Trust-GRPO\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5bf9\u601d\u7ef4\u8fc7\u7a0b\u7684\u76d1\u7763\uff0c\u5bfc\u81f4\u63a8\u7406\u7b56\u7565\u53ef\u80fd\u6b21\u4f18\uff0c\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u601d\u7ef4\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u601d\u7ef4\u8fc7\u7a0b\u8d28\u91cf\uff0c\u7ed3\u5408Trust-GRPO\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u5956\u52b1\u6743\u91cd\uff0c\u5e76\u91c7\u7528\u9000\u706b\u8bad\u7ec3\u7b56\u7565\u9010\u6b65\u51cf\u5c11\u601d\u7ef4\u5956\u52b1\u3002", "result": "SophiaVL-R1\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u4f18\u4e8e\u53c2\u6570\u89c4\u6a21\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u76d1\u7763\u601d\u7ef4\u8fc7\u7a0b\u548c\u52a8\u6001\u8c03\u6574\u5956\u52b1\uff0cSophiaVL-R1\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.16220", "pdf": "https://arxiv.org/pdf/2505.16220", "abs": "https://arxiv.org/abs/2505.16220", "authors": ["Liang-Yeh Shen", "Shi-Xin Fang", "Yi-Cheng Lin", "Huang-Cheng Chou", "Hung-yi Lee"], "title": "Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted by INTERSPEECH 2025. 7 pages, including 2 pages of appendix", "summary": "This paper introduces Meta-PerSER, a novel meta-learning framework that\npersonalizes Speech Emotion Recognition (SER) by adapting to each listener's\nunique way of interpreting emotion. Conventional SER systems rely on aggregated\nannotations, which often overlook individual subtleties and lead to\ninconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic\nMeta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,\nDerivative Annealing, and per-layer per-step learning rates, enabling rapid\nadaptation with only a few labeled examples. By integrating robust\nrepresentations from pre-trained self-supervised models, our framework first\ncaptures general emotional cues and then fine-tunes itself to personal\nannotation styles. Experiments on the IEMOCAP corpus demonstrate that\nMeta-PerSER significantly outperforms baseline methods in both seen and unseen\ndata scenarios, highlighting its promise for personalized emotion recognition.", "AI": {"tldr": "Meta-PerSER\u662f\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u4e2a\u6027\u5316\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u5feb\u901f\u9002\u5e94\u4e2a\u4eba\u6807\u6ce8\u98ce\u683c\uff0c\u663e\u8457\u63d0\u5347\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u7cfb\u7edf\u4f9d\u8d56\u805a\u5408\u6807\u6ce8\uff0c\u5ffd\u7565\u4e2a\u4f53\u5dee\u5f02\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u4e00\u81f4\u3002Meta-PerSER\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u60c5\u611f\u8bc6\u522b\u3002", "method": "\u91c7\u7528MAML\u5143\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408Combined-Set Meta-Training\u3001Derivative Annealing\u548c\u5206\u5c42\u5206\u6b65\u5b66\u4e60\u7387\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u6a21\u578b\u63d0\u53d6\u901a\u7528\u60c5\u611f\u7279\u5f81\uff0c\u518d\u5fae\u8c03\u9002\u5e94\u4e2a\u4eba\u6807\u6ce8\u98ce\u683c\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\uff0cMeta-PerSER\u5728\u5df2\u77e5\u548c\u672a\u77e5\u6570\u636e\u573a\u666f\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Meta-PerSER\u5c55\u793a\u4e86\u5728\u4e2a\u6027\u5316\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.17019", "pdf": "https://arxiv.org/pdf/2505.17019", "abs": "https://arxiv.org/abs/2505.17019", "authors": ["Chenhao Zhang", "Yazhe Niu"], "title": "Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework", "categories": ["cs.CV", "cs.AI", "cs.CY"], "comment": "16 pages, 9 figures. Code & Dataset:\n  https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep", "summary": "Metaphorical comprehension in images remains a critical challenge for AI\nsystems, as existing models struggle to grasp the nuanced cultural, emotional,\nand contextual implications embedded in visual content. While multimodal large\nlanguage models (MLLMs) excel in basic Visual Question Answer (VQA) tasks, they\nstruggle with a fundamental limitation on image implication tasks: contextual\ngaps that obscure the relationships between different visual elements and their\nabstract meanings. Inspired by the human cognitive process, we propose Let\nAndroids Dream (LAD), a novel framework for image implication understanding and\nreasoning. LAD addresses contextual missing through the three-stage framework:\n(1) Perception: converting visual information into rich and multi-level textual\nrepresentations, (2) Search: iteratively searching and integrating cross-domain\nknowledge to resolve ambiguity, and (3) Reasoning: generating context-alignment\nimage implication via explicit reasoning. Our framework with the lightweight\nGPT-4o-mini model achieves SOTA performance compared to 15+ MLLMs on English\nimage implication benchmark and a huge improvement on Chinese benchmark,\nperforming comparable with the GPT-4o model on Multiple-Choice Question (MCQ)\nand outperforms 36.7% on Open-Style Question (OSQ). Additionally, our work\nprovides new insights into how AI can more effectively interpret image\nimplications, advancing the field of vision-language reasoning and human-AI\ninteraction. Our project is publicly available at\nhttps://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLet Androids Dream (LAD)\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3\u56fe\u50cf\u9690\u55bb\u7406\u89e3\u95ee\u9898\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u5728\u7406\u89e3\u56fe\u50cf\u9690\u55bb\u65f6\u5b58\u5728\u6587\u5316\u3001\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u7f3a\u5931\u7684\u6311\u6218\uff0cLAD\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "LAD\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a\u611f\u77e5\uff08\u89c6\u89c9\u4fe1\u606f\u8f6c\u6587\u672c\uff09\u3001\u641c\u7d22\uff08\u8de8\u57df\u77e5\u8bc6\u6574\u5408\uff09\u3001\u63a8\u7406\uff08\u751f\u6210\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u9690\u55bb\u7406\u89e3\uff09\u3002", "result": "LAD\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u56fe\u50cf\u9690\u55bb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u90e8\u5206\u4efb\u52a1\u8d85\u8d8aGPT-4o\u6a21\u578b\u3002", "conclusion": "LAD\u4e3aAI\u7406\u89e3\u56fe\u50cf\u9690\u55bb\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u8fdb\u6b65\u3002"}}
{"id": "2505.16263", "pdf": "https://arxiv.org/pdf/2505.16263", "abs": "https://arxiv.org/abs/2505.16263", "authors": ["Sampanna Yashwant Kahu", "Naman Ahuja"], "title": "All You Need is \"Leet\": Evading Hate-speech Detection AI", "categories": ["cs.CR", "cs.CL", "cs.LG", "K.6.5"], "comment": "10 pages, 22 figures, The source code and data used in this work is\n  available at: https://github.com/SampannaKahu/all_you_need_is_leet", "summary": "Social media and online forums are increasingly becoming popular.\nUnfortunately, these platforms are being used for spreading hate speech. In\nthis paper, we design black-box techniques to protect users from hate-speech on\nonline platforms by generating perturbations that can fool state of the art\ndeep learning based hate speech detection models thereby decreasing their\nefficiency. We also ensure a minimal change in the original meaning of\nhate-speech. Our best perturbation attack is successfully able to evade\nhate-speech detection for 86.8 % of hateful text.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ed1\u76d2\u6270\u52a8\u6280\u672f\uff0c\u7528\u4e8e\u6b3a\u9a97\u6df1\u5ea6\u5b66\u4e60\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\uff0c\u964d\u4f4e\u5176\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u610f\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u548c\u5728\u7ebf\u8bba\u575b\u4e2d\u4ec7\u6068\u8a00\u8bba\u6cdb\u6ee5\uff0c\u9700\u8981\u4fdd\u62a4\u7528\u6237\u514d\u53d7\u5176\u5bb3\u3002", "method": "\u8bbe\u8ba1\u9ed1\u76d2\u6270\u52a8\u6280\u672f\uff0c\u751f\u6210\u80fd\u6b3a\u9a97\u5148\u8fdb\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6a21\u578b\u7684\u6270\u52a8\u3002", "result": "\u6700\u4f73\u6270\u52a8\u653b\u51fb\u6210\u529f\u89c4\u907f86.8%\u4ec7\u6068\u6587\u672c\u7684\u68c0\u6d4b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6548\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8bed\u4e49\u53d8\u5316\u3002"}}
{"id": "2505.17020", "pdf": "https://arxiv.org/pdf/2505.17020", "abs": "https://arxiv.org/abs/2505.17020", "authors": ["Shilin Yan", "Jiaming Han", "Joey Tsai", "Hongwei Xue", "Rongyao Fang", "Lingyi Hong", "Ziyu Guo", "Ray Zhang"], "title": "CrossLMM: Decoupling Long Video Sequences from LMMs via Dual Cross-Attention Mechanisms", "categories": ["cs.CV"], "comment": "Project page: https://github.com/shilinyan99/CrossLMM", "summary": "The advent of Large Multimodal Models (LMMs) has significantly enhanced Large\nLanguage Models (LLMs) to process and interpret diverse data modalities (e.g.,\nimage and video). However, as input complexity increases, particularly with\nlong video sequences, the number of required tokens has grown significantly,\nleading to quadratically computational costs. This has made the efficient\ncompression of video tokens in LMMs, while maintaining performance integrity, a\npressing research challenge. In this paper, we introduce CrossLMM, decoupling\nlong video sequences from LMMs via a dual cross-attention mechanism, which\nsubstantially reduces visual token quantity with minimal performance\ndegradation. Specifically, we first implement a significant token reduction\nfrom pretrained visual encoders through a pooling methodology. Then, within LLM\nlayers, we employ a visual-to-visual cross-attention mechanism, wherein the\npooled visual tokens function as queries against the original visual token set.\nThis module enables more efficient token utilization while retaining\nfine-grained informational fidelity. In addition, we introduce a text-to-visual\ncross-attention mechanism, for which the text tokens are enhanced through\ninteraction with the original visual tokens, enriching the visual comprehension\nof the text tokens. Comprehensive empirical evaluation demonstrates that our\napproach achieves comparable or superior performance across diverse video-based\nLMM benchmarks, despite utilizing substantially fewer computational resources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCrossLMM\uff0c\u901a\u8fc7\u53cc\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u51cf\u5c11\u89c6\u9891token\u6570\u91cf\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8f93\u5165\u590d\u6742\u6027\u589e\u52a0\uff08\u5982\u957f\u89c6\u9891\u5e8f\u5217\uff09\uff0ctoken\u6570\u91cf\u6fc0\u589e\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u4e8c\u6b21\u589e\u957f\uff0c\u4e9f\u9700\u9ad8\u6548\u538b\u7f29\u89c6\u9891token\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6c60\u5316\u65b9\u6cd5\u51cf\u5c11\u89c6\u89c9token\uff0c\u5e76\u5728LLM\u5c42\u5f15\u5165\u89c6\u89c9-\u89c6\u89c9\u548c\u6587\u672c-\u89c6\u89c9\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347token\u5229\u7528\u6548\u7387\u548c\u4fe1\u606f\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u89c6\u9891LMM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "CrossLMM\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891token\u538b\u7f29\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16276", "pdf": "https://arxiv.org/pdf/2505.16276", "abs": "https://arxiv.org/abs/2505.16276", "authors": ["Desiree Heim", "Lars-Peter Meyer", "Markus Schr\u00f6der", "Johannes Frey", "Andreas Dengel"], "title": "How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance", "categories": ["cs.AI", "cs.CL"], "comment": "Peer reviewed and to appear in the ESWC 2025 Workshops and Tutorials\n  Joint Proceedings (Workshop on Evaluation of Language Models in Knowledge\n  Engineering [ELMKE])", "summary": "When using Large Language Models (LLMs) to support Knowledge Graph\nEngineering (KGE), one of the first indications when searching for an\nappropriate model is its size. According to the scaling laws, larger models\ntypically show higher capabilities. However, in practice, resource costs are\nalso an important factor and thus it makes sense to consider the ratio between\nmodel performance and costs. The LLM-KG-Bench framework enables the comparison\nof LLMs in the context of KGE tasks and assesses their capabilities of\nunderstanding and producing KGs and KG queries. Based on a dataset created in\nan LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the\nmodel size scaling laws specific to KGE tasks. In our analyses, we assess how\nbenchmark scores evolve between different model size categories. Additionally,\nwe inspect how the general score development of single models and families of\nmodels correlates to their size. Our analyses revealed that, with a few\nexceptions, the model size scaling laws generally also apply to the selected\nKGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,\nthe task performance did not change much between a model and the next larger\nmodel. In these cases, smaller models could be considered to achieve high\ncost-effectiveness. Regarding models of the same family, sometimes larger\nmodels performed worse than smaller models of the same family. These effects\noccurred only locally. Hence it is advisable to additionally test the next\nsmallest and largest model of the same family.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u77e5\u8bc6\u56fe\u8c31\u5de5\u7a0b\uff08KGE\uff09\u4e2d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\u53ca\u6210\u672c\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7LLM-KG-Bench\u6846\u67b6\u8bc4\u4f30\u4e8626\u4e2a\u5f00\u6e90LLMs\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u6a21\u578b\u5927\u5c0f\u5bf9KGE\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u6210\u672c\u6548\u76ca\u6bd4\uff0c\u4ee5\u6307\u5bfc\u5b9e\u9645\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u6a21\u578b\u3002", "method": "\u4f7f\u7528LLM-KG-Bench\u6846\u67b6\u5bf926\u4e2a\u5f00\u6e90LLMs\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u4e0d\u540c\u5927\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u53d8\u5316\u53ca\u5176\u4e0e\u6a21\u578b\u5bb6\u65cf\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\u901a\u5e38\u6b63\u76f8\u5173\uff0c\u4f46\u4e5f\u5b58\u5728\u6027\u80fd\u5e73\u53f0\u6216\u5929\u82b1\u677f\u6548\u5e94\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8f83\u5c0f\u6a21\u578b\u66f4\u5177\u6210\u672c\u6548\u76ca\uff1b\u540c\u4e00\u5bb6\u65cf\u4e2d\uff0c\u8f83\u5927\u6a21\u578b\u5076\u5c14\u8868\u73b0\u4e0d\u5982\u8f83\u5c0f\u6a21\u578b\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u6a21\u578b\u5927\u5c0f\u5bf9KGE\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u7b26\u5408\u4e00\u822c\u89c4\u5f8b\uff0c\u4f46\u9700\u6ce8\u610f\u5c40\u90e8\u5f02\u5e38\u60c5\u51b5\uff0c\u5efa\u8bae\u6d4b\u8bd5\u540c\u4e00\u5bb6\u65cf\u4e2d\u76f8\u90bb\u5927\u5c0f\u7684\u6a21\u578b\u4ee5\u786e\u4fdd\u9009\u62e9\u6700\u4f18\u3002"}}
{"id": "2505.17021", "pdf": "https://arxiv.org/pdf/2505.17021", "abs": "https://arxiv.org/abs/2505.17021", "authors": ["Sara Ghaboura", "Ketan More", "Wafa Alghallabi", "Omkar Thawakar", "Jorma Laaksonen", "Hisham Cholakkal", "Salman Khan", "Rao Muhammad Anwer"], "title": "ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark", "categories": ["cs.CV"], "comment": "Github : https://github.com/mbzuai-oryx/ARB, Huggingface:\n  https://huggingface.co/datasets/MBZUAI/ARB", "summary": "As Large Multimodal Models (LMMs) become more capable, there is growing\ninterest in evaluating their reasoning processes alongside their final outputs.\nHowever, most benchmarks remain focused on English, overlooking languages with\nrich linguistic and cultural contexts, such as Arabic. To address this gap, we\nintroduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the\nfirst benchmark designed to evaluate step-by-step reasoning in Arabic across\nboth textual and visual modalities. ARB spans 11 diverse domains, including\nvisual reasoning, document understanding, OCR, scientific analysis, and\ncultural interpretation. It comprises 1,356 multimodal samples paired with\n5,119 human-curated reasoning steps and corresponding actions. We evaluated 12\nstate-of-the-art open- and closed-source LMMs and found persistent challenges\nin coherence, faithfulness, and cultural grounding. ARB offers a structured\nframework for diagnosing multimodal reasoning in underrepresented languages and\nmarks a critical step toward inclusive, transparent, and culturally aware AI\nsystems. We release the benchmark, rubric, and evaluation suit to support\nfuture research and reproducibility. Code available at:\nhttps://github.com/mbzuai-oryx/ARB", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6ARB\uff0c\u65e8\u5728\u8bc4\u4f30LMMs\u5728\u963f\u62c9\u4f2f\u8bed\u4e2d\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u8986\u76d611\u4e2a\u9886\u57df\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u8fde\u8d2f\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u6587\u5316\u80cc\u666f\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u5ffd\u7565\u4e86\u963f\u62c9\u4f2f\u8bed\u7b49\u4e30\u5bcc\u8bed\u8a00\u6587\u5316\u80cc\u666f\uff0c\u9700\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaARB\u57fa\u51c6\uff0c\u5305\u542b1,356\u4e2a\u591a\u6a21\u6001\u6837\u672c\u548c5,119\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u8bc4\u4f3012\u79cd\u5148\u8fdbLMMs\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u8fde\u8d2f\u6027\u3001\u5fe0\u5b9e\u6027\u548c\u6587\u5316\u80cc\u666f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "ARB\u4e3a\u8bca\u65ad\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u63a8\u52a8\u5305\u5bb9\u6027\u3001\u900f\u660e\u6027\u548c\u6587\u5316\u610f\u8bc6AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16315", "pdf": "https://arxiv.org/pdf/2505.16315", "abs": "https://arxiv.org/abs/2505.16315", "authors": ["Xiaoxue Cheng", "Junyi Li", "Zhenduo Zhang", "Xinyu Tang", "Wayne Xin Zhao", "Xinyu Kong", "Zhiqiang Zhang"], "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "work in progress", "summary": "Large reasoning models (LRMs) have demonstrated strong performance on complex\nreasoning tasks, but often suffer from overthinking, generating redundant\ncontent regardless of task difficulty. Inspired by the dual process theory in\ncognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a\nreinforcement learning framework that enables LRMs to achieve efficient\nreasoning through adaptive cognitive allocation and dynamic system switch. ACPO\nincorporates two key components: (1) introducing system-aware reasoning tokens\nto explicitly represent the thinking modes thereby making the model's cognitive\nprocess transparent, and (2) integrating online difficulty estimation and token\nlength budget to guide adaptive system switch and reasoning during\nreinforcement learning. To this end, we propose a two-stage training strategy.\nThe first stage begins with supervised fine-tuning to cold start the model,\nenabling it to generate reasoning paths with explicit thinking modes. In the\nsecond stage, we apply ACPO to further enhance adaptive system switch for\ndifficulty-aware reasoning. Experimental results demonstrate that ACPO\neffectively reduces redundant reasoning while adaptively adjusting cognitive\nallocation based on task complexity, achieving efficient hybrid reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aACPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8ba4\u77e5\u5206\u914d\u548c\u52a8\u6001\u7cfb\u7edf\u5207\u6362\uff0c\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5197\u4f59\u63a8\u7406\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5e38\u56e0\u8fc7\u5ea6\u601d\u8003\u751f\u6210\u5197\u4f59\u5185\u5bb9\u3002\u53d7\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u53cc\u8fc7\u7a0b\u7406\u8bba\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u81ea\u9002\u5e94\u65b9\u6cd5\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002", "method": "ACPO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7cfb\u7edf\u611f\u77e5\u63a8\u7406\u6807\u8bb0\u548c\u5728\u7ebf\u96be\u5ea6\u4f30\u8ba1\u4e0e\u6807\u8bb0\u957f\u5ea6\u9884\u7b97\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u51b7\u542f\u52a8\u6a21\u578b\uff0c\u518d\u5e94\u7528ACPO\u4f18\u5316\u81ea\u9002\u5e94\u7cfb\u7edf\u5207\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cACPO\u80fd\u6709\u6548\u51cf\u5c11\u5197\u4f59\u63a8\u7406\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u8ba4\u77e5\u5206\u914d\uff0c\u5b9e\u73b0\u9ad8\u6548\u6df7\u5408\u63a8\u7406\u3002", "conclusion": "ACPO\u901a\u8fc7\u900f\u660e\u5316\u8ba4\u77e5\u8fc7\u7a0b\u548c\u52a8\u6001\u7cfb\u7edf\u5207\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6548\u7387\u3002"}}
{"id": "2505.17022", "pdf": "https://arxiv.org/pdf/2505.17022", "abs": "https://arxiv.org/abs/2505.17022", "authors": ["Chengqi Duan", "Rongyao Fang", "Yuqing Wang", "Kun Wang", "Linjiang Huang", "Xingyu Zeng", "Hongsheng Li", "Xihui Liu"], "title": "GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Github page refer to: https://github.com/gogoduan/GoT-R1", "summary": "Visual generation models have made remarkable progress in creating realistic\nimages from text prompts, yet struggle with complex prompts that specify\nmultiple objects with precise spatial relationships and attributes. Effective\nhandling of such prompts requires explicit reasoning about the semantic content\nand spatial layout. We present GoT-R1, a framework that applies reinforcement\nlearning to enhance semantic-spatial reasoning in visual generation. Building\nupon the Generation Chain-of-Thought approach, GoT-R1 enables models to\nautonomously discover effective reasoning strategies beyond predefined\ntemplates through carefully designed reinforcement learning. To achieve this,\nwe propose a dual-stage multi-dimensional reward framework that leverages MLLMs\nto evaluate both the reasoning process and final output, enabling effective\nsupervision across the entire generation pipeline. The reward system assesses\nsemantic alignment, spatial accuracy, and visual quality in a unified approach.\nExperimental results demonstrate significant improvements on T2I-CompBench\nbenchmark, particularly in compositional tasks involving precise spatial\nrelationships and attribute binding. GoT-R1 advances the state-of-the-art in\nimage generation by successfully transferring sophisticated reasoning\ncapabilities to the visual generation domain. To facilitate future research, we\nmake our code and pretrained models publicly available at\nhttps://github.com/gogoduan/GoT-R1.", "AI": {"tldr": "GoT-R1\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u8bed\u4e49-\u7a7a\u95f4\u63a8\u7406\u7684\u89c6\u89c9\u751f\u6210\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6587\u672c\u63d0\u793a\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u591a\u5bf9\u8c61\u3001\u7cbe\u786e\u7a7a\u95f4\u5173\u7cfb\u548c\u5c5e\u6027\u7684\u590d\u6742\u63d0\u793a\u65f6\u7684\u56f0\u96be\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u9636\u6bb5\u591a\u7ef4\u5956\u52b1\u673a\u5236\uff0c\u5229\u7528MLLMs\u8bc4\u4f30\u63a8\u7406\u8fc7\u7a0b\u548c\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728T2I-CompBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u7cbe\u786e\u7a7a\u95f4\u5173\u7cfb\u548c\u5c5e\u6027\u7ed1\u5b9a\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "GoT-R1\u6210\u529f\u5c06\u590d\u6742\u63a8\u7406\u80fd\u529b\u5f15\u5165\u89c6\u89c9\u751f\u6210\u9886\u57df\uff0c\u63a8\u52a8\u4e86\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.16322", "pdf": "https://arxiv.org/pdf/2505.16322", "abs": "https://arxiv.org/abs/2505.16322", "authors": ["Woosung Koh", "Wonbeen Oh", "Jaein Jang", "MinHyung Lee", "Hyeongjin Kim", "Ah Yeon Kim", "Joonkee Kim", "Junghyun Lee", "Taehyeon Kim", "Se-Young Yun"], "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Pre-print", "summary": "Self-Taught Reasoners (STaR), synonymously known as Rejection sampling\nFine-Tuning (RFT), is an integral part of the training pipeline of\nself-improving reasoning Language Models (LMs). The self-improving mechanism\noften employs random observation (data) sampling. However, this results in\ntrained observation imbalance; inefficiently over-training on solved examples\nwhile under-training on challenging ones. In response, we introduce Adaptive\nSTaR (AdaSTaR), a novel algorithm that rectifies this by integrating two\nadaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting\nbalanced training across observations, and (2) Adaptive Sampling for\nCurriculum: dynamically adjusting data difficulty to match the model's evolving\nstrength. Across six benchmarks, AdaSTaR achieves best test accuracy in all\ninstances (6/6) and reduces training FLOPs by an average of 58.6% against an\nextensive list of baselines. These improvements in performance and efficiency\ngeneralize to different pre-trained LMs and larger models, paving the way for\nmore efficient and effective self-improving LMs.", "AI": {"tldr": "AdaSTaR\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u8bad\u7ec3\u6570\u636e\u96be\u5ea6\u548c\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982STaR\uff09\u5728\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65f6\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u7b80\u5355\u6837\u672c\u8fc7\u5ea6\u8bad\u7ec3\u800c\u5bf9\u56f0\u96be\u6837\u672c\u8bad\u7ec3\u4e0d\u8db3\u3002", "method": "AdaSTaR\u7ed3\u5408\u4e24\u79cd\u81ea\u9002\u5e94\u91c7\u6837\u539f\u5219\uff1a1\uff09\u591a\u6837\u6027\u91c7\u6837\u4ee5\u5e73\u8861\u8bad\u7ec3\u6570\u636e\uff1b2\uff09\u8bfe\u7a0b\u91c7\u6837\u4ee5\u52a8\u6001\u8c03\u6574\u6570\u636e\u96be\u5ea6\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAdaSTaR\u5747\u53d6\u5f97\u6700\u4f73\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u5e76\u5e73\u5747\u51cf\u5c1158.6%\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u3002", "conclusion": "AdaSTaR\u4e3a\u9ad8\u6548\u81ea\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u3002"}}
{"id": "2505.15725", "pdf": "https://arxiv.org/pdf/2505.15725", "abs": "https://arxiv.org/abs/2505.15725", "authors": ["Xiangyu Wang", "Donglin Yang", "Yue Liao", "Wenhao Zheng", "wenjun wu", "Bin Dai", "Hongsheng Li", "Si Liu"], "title": "UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicles (UAVs) are evolving into language-interactive\nplatforms, enabling more intuitive forms of human-drone interaction. While\nprior works have primarily focused on high-level planning and long-horizon\nnavigation, we shift attention to language-guided fine-grained trajectory\ncontrol, where UAVs execute short-range, reactive flight behaviors in response\nto language instructions. We formalize this problem as the Flying-on-a-Word\n(Flow) task and introduce UAV imitation learning as an effective approach. In\nthis framework, UAVs learn fine-grained control policies by mimicking expert\npilot trajectories paired with atomic language instructions. To support this\nparadigm, we present UAV-Flow, the first real-world benchmark for\nlanguage-conditioned, fine-grained UAV control. It includes a task formulation,\na large-scale dataset collected in diverse environments, a deployable control\nframework, and a simulation suite for systematic evaluation. Our design enables\nUAVs to closely imitate the precise, expert-level flight trajectories of human\npilots and supports direct deployment without sim-to-real gap. We conduct\nextensive experiments on UAV-Flow, benchmarking VLN and VLA paradigms. Results\nshow that VLA models are superior to VLN baselines and highlight the critical\nrole of spatial grounding in the fine-grained Flow setting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u2018Flying-on-a-Word\u2019\u4efb\u52a1\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u65e0\u4eba\u673a\u5bf9\u8bed\u8a00\u6307\u4ee4\u7684\u7cbe\u7ec6\u8f68\u8ff9\u63a7\u5236\uff0c\u5e76\u53d1\u5e03\u4e86UAV-Flow\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u65e0\u4eba\u673a\u7684\u9ad8\u5c42\u89c4\u5212\u548c\u957f\u8ddd\u79bb\u5bfc\u822a\uff0c\u800c\u672c\u6587\u8f6c\u5411\u8bed\u8a00\u5f15\u5bfc\u7684\u7cbe\u7ec6\u8f68\u8ff9\u63a7\u5236\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u76f4\u89c2\u6027\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e0\u4eba\u673a\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u98de\u884c\u8f68\u8ff9\u4e0e\u539f\u5b50\u8bed\u8a00\u6307\u4ee4\u914d\u5bf9\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u4e86UAV-Flow\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVLA\u6a21\u578b\u4f18\u4e8eVLN\u57fa\u7ebf\uff0c\u4e14\u7a7a\u95f4\u63a5\u5730\u5728\u7cbe\u7ec6\u63a7\u5236\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "UAV-Flow\u652f\u6301\u76f4\u63a5\u90e8\u7f72\uff0c\u65e0\u9700\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\uff0c\u4e3a\u8bed\u8a00\u5f15\u5bfc\u7684\u65e0\u4eba\u673a\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2505.16400", "pdf": "https://arxiv.org/pdf/2505.16400", "abs": "https://arxiv.org/abs/2505.16400", "authors": ["Yang Chen", "Zhuolin Yang", "Zihan Liu", "Chankyu Lee", "Peng Xu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "We release the model at:\n  https://huggingface.co/nvidia/AceReason-Nemotron-14B", "summary": "Despite recent progress in large-scale reinforcement learning (RL) for\nreasoning, the training recipe for building high-performing reasoning models\nremains elusive. Key implementation details of frontier models, such as\nDeepSeek-R1, including data curation strategies and RL training recipe, are\noften omitted. Moreover, recent research indicates distillation remains more\neffective than RL for smaller models. In this work, we demonstrate that\nlarge-scale RL can significantly enhance the reasoning capabilities of strong,\nsmall- and mid-sized models, achieving results that surpass those of\nstate-of-the-art distillation-based models. We systematically study the RL\ntraining process through extensive ablations and propose a simple yet effective\napproach: first training on math-only prompts, then on code-only prompts.\nNotably, we find that math-only RL not only significantly enhances the\nperformance of strong distilled models on math benchmarks (e.g., +14.6% /\n+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks\n(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,\nextended code-only RL iterations further improve performance on code benchmarks\nwith minimal or no degradation in math results. We develop a robust data\ncuration pipeline to collect challenging prompts with high-quality, verifiable\nanswers and test cases to enable verification-based RL across both domains.\nFinally, we identify key experimental insights, including curriculum learning\nwith progressively increasing response lengths and the stabilizing effect of\non-policy parameter updates. We find that RL not only elicits the foundational\nreasoning capabilities acquired during pretraining and supervised fine-tuning\n(e.g., distillation), but also pushes the limits of the model's reasoning\nability, enabling it to solve problems that were previously unsolvable.", "AI": {"tldr": "\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u663e\u8457\u63d0\u5347\u4e2d\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u548c\u4ee3\u7801\u63d0\u793a\u7684\u5206\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21RL\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u8bad\u7ec3\u65b9\u6cd5\u4ecd\u4e0d\u660e\u786e\uff0c\u4e14\u524d\u6cbf\u6a21\u578b\u7684\u5b9e\u73b0\u7ec6\u8282\u5e38\u88ab\u5ffd\u7565\u3002\u672c\u6587\u65e8\u5728\u5c55\u793aRL\u5982\u4f55\u63d0\u5347\u4e2d\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5206\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u6570\u5b66\u63d0\u793a\uff0c\u540e\u4ee3\u7801\u63d0\u793a\uff1b\u6784\u5efa\u6570\u636e\u7ba1\u9053\u6536\u96c6\u9ad8\u8d28\u91cf\u63d0\u793a\uff1b\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u548c\u7b56\u7565\u66f4\u65b0\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u6570\u5b66RL\u663e\u8457\u63d0\u5347\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u6027\u80fd\uff08\u5982AIME 2025\u63d0\u534714.6%/17.2%\uff09\uff0c\u4ee3\u7801RL\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee3\u7801\u8868\u73b0\u4e14\u4e0d\u5f71\u54cd\u6570\u5b66\u7ed3\u679c\u3002", "conclusion": "RL\u4e0d\u4ec5\u80fd\u6fc0\u53d1\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u83b7\u5f97\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u80fd\u7a81\u7834\u6a21\u578b\u6781\u9650\uff0c\u89e3\u51b3\u6b64\u524d\u65e0\u6cd5\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2505.15822", "pdf": "https://arxiv.org/pdf/2505.15822", "abs": "https://arxiv.org/abs/2505.15822", "authors": ["Jhon Lopez", "Carlos Hinojosa", "Henry Arguello", "Bernard Ghanem"], "title": "MambaStyle: Efficient StyleGAN Inversion for Real Image Editing with State-Space Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The task of inverting real images into StyleGAN's latent space to manipulate\ntheir attributes has been extensively studied. However, existing GAN inversion\nmethods struggle to balance high reconstruction quality, effective editability,\nand computational efficiency. In this paper, we introduce MambaStyle, an\nefficient single-stage encoder-based approach for GAN inversion and editing\nthat leverages vision state-space models (VSSMs) to address these challenges.\nSpecifically, our approach integrates VSSMs within the proposed architecture,\nenabling high-quality image inversion and flexible editing with significantly\nfewer parameters and reduced computational complexity compared to\nstate-of-the-art methods. Extensive experiments show that MambaStyle achieves a\nsuperior balance among inversion accuracy, editing quality, and computational\nefficiency. Notably, our method achieves superior inversion and editing results\nwith reduced model complexity and faster inference, making it suitable for\nreal-time applications.", "AI": {"tldr": "MambaStyle\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08VSSMs\uff09\u7684\u9ad8\u6548\u5355\u9636\u6bb5\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u7528\u4e8eGAN\u53cd\u6f14\u548c\u7f16\u8f91\uff0c\u5e73\u8861\u4e86\u91cd\u5efa\u8d28\u91cf\u3001\u7f16\u8f91\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709GAN\u53cd\u6f14\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3001\u6709\u6548\u7f16\u8f91\u548c\u8ba1\u7b97\u6548\u7387\uff0cMambaStyle\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06VSSMs\u96c6\u6210\u5230\u67b6\u6784\u4e2d\uff0cMambaStyle\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u53cd\u6f14\u548c\u7075\u6d3b\u7f16\u8f91\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMambaStyle\u5728\u53cd\u6f14\u7cbe\u5ea6\u3001\u7f16\u8f91\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u66f4\u4f4e\u3001\u63a8\u7406\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "MambaStyle\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u7684\u9ad8\u6548GAN\u53cd\u6f14\u548c\u7f16\u8f91\u65b9\u6cd5\u3002"}}
{"id": "2505.16470", "pdf": "https://arxiv.org/pdf/2505.16470", "abs": "https://arxiv.org/abs/2505.16470", "authors": ["Kuicai Dong", "Yujing Chang", "Shijie Huang", "Yasheng Wang", "Ruiming Tang", "Yong Liu"], "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "categories": ["cs.IR", "cs.CL", "cs.CV"], "comment": "preprint. code available at\n  \\url{https://mmdocrag.github.io/MMDocRAG/}", "summary": "Document Visual Question Answering (DocVQA) faces dual challenges in\nprocessing lengthy multimodal documents (text, images, tables) and performing\ncross-modal reasoning. Current document retrieval-augmented generation (DocRAG)\nmethods remain limited by their text-centric approaches, frequently missing\ncritical visual information. The field also lacks robust benchmarks for\nassessing multimodal evidence selection and integration. We introduce MMDocRAG,\na comprehensive benchmark featuring 4,055 expert-annotated QA pairs with\nmulti-page, cross-modal evidence chains. Our framework introduces innovative\nmetrics for evaluating multimodal quote selection and enables answers that\ninterleave text with relevant visual elements. Through large-scale experiments\nwith 60 VLM/LLM models and 14 retrieval systems, we identify persistent\nchallenges in multimodal evidence retrieval, selection, and integration.Key\nfindings reveal advanced proprietary LVMs show superior performance than\nopen-sourced alternatives. Also, they show moderate advantages using multimodal\ninputs over text-only inputs, while open-source alternatives show significant\nperformance degradation. Notably, fine-tuned LLMs achieve substantial\nimprovements when using detailed image descriptions. MMDocRAG establishes a\nrigorous testing ground and provides actionable insights for developing more\nrobust multimodal DocVQA systems. Our benchmark and code are available at\nhttps://mmdocrag.github.io/MMDocRAG/.", "AI": {"tldr": "MMDocRAG\u662f\u4e00\u4e2a\u65b0\u7684DocVQA\u57fa\u51c6\uff0c\u5305\u542b4055\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684QA\u5bf9\uff0c\u652f\u6301\u591a\u9875\u8de8\u6a21\u6001\u8bc1\u636e\u94fe\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u591a\u6a21\u6001\u5f15\u7528\u9009\u62e9\u7684\u65b0\u6307\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e13\u6709LVMs\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u4e14\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dDocVQA\u65b9\u6cd5\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u4fe1\u606f\u7684\u6709\u6548\u5229\u7528\uff0c\u4e14\u7f3a\u5c11\u8bc4\u4f30\u591a\u6a21\u6001\u8bc1\u636e\u9009\u62e9\u548c\u6574\u5408\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86MMDocRAG\u57fa\u51c6\uff0c\u5305\u542b\u591a\u9875\u8de8\u6a21\u6001\u8bc1\u636e\u94fe\u7684QA\u5bf9\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u6307\u6807\u8bc4\u4f30\u591a\u6a21\u6001\u5f15\u7528\u9009\u62e9\u3002\u5b9e\u9a8c\u6db5\u76d660\u4e2aVLM/LLM\u6a21\u578b\u548c14\u4e2a\u68c0\u7d22\u7cfb\u7edf\u3002", "result": "\u4e13\u6709LVMs\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u591a\u6a21\u6001\u8f93\u5165\u5bf9\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\u3002\u5fae\u8c03\u540e\u7684LLMs\u5728\u4f7f\u7528\u8be6\u7ec6\u56fe\u50cf\u63cf\u8ff0\u65f6\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "MMDocRAG\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u591a\u6a21\u6001DocVQA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e25\u683c\u6d4b\u8bd5\u57fa\u51c6\u548c\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2505.15840", "pdf": "https://arxiv.org/pdf/2505.15840", "abs": "https://arxiv.org/abs/2505.15840", "authors": ["Zizheng Zhu", "Yingchao Yu", "Zeqi Zheng", "Zhaofei Yu", "Yaochu Jin"], "title": "TDFormer: A Top-Down Attention-Controlled Spiking Transformer", "categories": ["cs.NE", "cs.AI", "cs.CV"], "comment": "28 pages", "summary": "Traditional spiking neural networks (SNNs) can be viewed as a combination of\nmultiple subnetworks with each running for one time step, where the parameters\nare shared, and the membrane potential serves as the only information link\nbetween them. However, the implicit nature of the membrane potential limits its\nability to effectively represent temporal information. As a result, each time\nstep cannot fully leverage information from previous time steps, seriously\nlimiting the model's performance. Inspired by the top-down mechanism in the\nbrain, we introduce TDFormer, a novel model with a top-down feedback structure\nthat functions hierarchically and leverages high-order representations from\nearlier time steps to modulate the processing of low-order information at later\nstages. The feedback structure plays a role from two perspectives: 1) During\nforward propagation, our model increases the mutual information across time\nsteps, indicating that richer temporal information is being transmitted and\nintegrated in different time steps. 2) During backward propagation, we\ntheoretically prove that the feedback structure alleviates the problem of\nvanishing gradients along the time dimension. We find that these mechanisms\ntogether significantly and consistently improve the model performance on\nmultiple datasets. In particular, our model achieves state-of-the-art\nperformance on ImageNet with an accuracy of 86.83%.", "AI": {"tldr": "TDFormer\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u4e0a\u800c\u4e0b\u7684\u53cd\u9988\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfSNN\u7684\u819c\u7535\u4f4d\u4f5c\u4e3a\u65f6\u95f4\u6b65\u95f4\u552f\u4e00\u7684\u4fe1\u606f\u94fe\u63a5\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faTDFormer\u6a21\u578b\uff0c\u5229\u7528\u9ad8\u9636\u8868\u793a\u8c03\u5236\u4f4e\u9636\u4fe1\u606f\u5904\u7406\uff0c\u589e\u52a0\u65f6\u95f4\u6b65\u95f4\u4e92\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cImageNet\u51c6\u786e\u7387\u8fbe86.83%\u3002", "conclusion": "\u53cd\u9988\u7ed3\u6784\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65f6\u95f4\u4fe1\u606f\u7684\u4f20\u9012\u4e0e\u6574\u5408\u3002"}}
{"id": "2505.16530", "pdf": "https://arxiv.org/pdf/2505.16530", "abs": "https://arxiv.org/abs/2505.16530", "authors": ["Yuliang Yan", "Haochun Tang", "Shuo Yan", "Enyan Dai"], "title": "DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are considered valuable Intellectual Properties\n(IP) for legitimate owners due to the enormous computational cost of training.\nIt is crucial to protect the IP of LLMs from malicious stealing or unauthorized\ndeployment. Despite existing efforts in watermarking and fingerprinting LLMs,\nthese methods either impact the text generation process or are limited in\nwhite-box access to the suspect model, making them impractical. Hence, we\npropose DuFFin, a novel $\\textbf{Du}$al-Level $\\textbf{Fin}$gerprinting\n$\\textbf{F}$ramework for black-box setting ownership verification. DuFFin\nextracts the trigger pattern and the knowledge-level fingerprints to identify\nthe source of a suspect model. We conduct experiments on a variety of models\ncollected from the open-source website, including four popular base models as\nprotected LLMs and their fine-tuning, quantization, and safety alignment\nversions, which are released by large companies, start-ups, and individual\nusers. Results show that our method can accurately verify the copyright of the\nbase protected LLM on their model variants, achieving the IP-ROC metric greater\nthan 0.95. Our code is available at\nhttps://github.com/yuliangyan0807/llm-fingerprint.", "AI": {"tldr": "DuFFin\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u7ea7\u6307\u7eb9\u6846\u67b6\uff0c\u7528\u4e8e\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u7684\u6240\u6709\u6743\u9a8c\u8bc1\uff0c\u80fd\u51c6\u786e\u8bc6\u522b\u53d7\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d8\u4f53\u3002", "motivation": "\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u9632\u6b62\u6076\u610f\u7a83\u53d6\u6216\u672a\u7ecf\u6388\u6743\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51faDuFFin\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u89e6\u53d1\u6a21\u5f0f\u548c\u77e5\u8bc6\u7ea7\u6307\u7eb9\u6765\u8bc6\u522b\u6a21\u578b\u6765\u6e90\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDuFFin\u80fd\u51c6\u786e\u9a8c\u8bc1\u57fa\u7840\u6a21\u578b\u7684\u7248\u6743\uff0cIP-ROC\u6307\u6807\u8d85\u8fc70.95\u3002", "conclusion": "DuFFin\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u9ed1\u76d2\u6307\u7eb9\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u3002"}}
{"id": "2505.15861", "pdf": "https://arxiv.org/pdf/2505.15861", "abs": "https://arxiv.org/abs/2505.15861", "authors": ["Zhenyan Yao", "Miao Zhang", "Lanhu Wu", "Yongri Piao", "Feng Tian", "Weibing Sun", "Huchuan Lu"], "title": "P3Net: Progressive and Periodic Perturbation for Semi-Supervised Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Perturbation with diverse unlabeled data has proven beneficial for\nsemi-supervised medical image segmentation (SSMIS). While many works have\nsuccessfully used various perturbation techniques, a deeper understanding of\nlearning perturbations is needed. Excessive or inappropriate perturbation can\nhave negative effects, so we aim to address two challenges: how to use\nperturbation mechanisms to guide the learning of unlabeled data through labeled\ndata, and how to ensure accurate predictions in boundary regions. Inspired by\nhuman progressive and periodic learning, we propose a progressive and periodic\nperturbation mechanism (P3M) and a boundary-focused loss. P3M enables dynamic\nadjustment of perturbations, allowing the model to gradually learn them. Our\nboundary-focused loss encourages the model to concentrate on boundary regions,\nenhancing sensitivity to intricate details and ensuring accurate predictions.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on two 2D and 3D datasets. Moreover, P3M is extendable to other\nmethods, and the proposed loss serves as a universal tool for improving\nexisting methods, highlighting the scalability and applicability of our\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5468\u671f\u6027\u6270\u52a8\u673a\u5236\uff08P3M\uff09\u548c\u8fb9\u754c\u805a\u7126\u635f\u5931\uff0c\u7528\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u52a8\u6001\u8c03\u6574\u6270\u52a8\u5e76\u63d0\u5347\u8fb9\u754c\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6270\u52a8\u6280\u672f\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u8fc7\u5ea6\u6216\u4e0d\u9002\u5f53\u7684\u6270\u52a8\u53ef\u80fd\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\uff0c\u9700\u89e3\u51b3\u5982\u4f55\u5229\u7528\u6270\u52a8\u673a\u5236\u5f15\u5bfc\u5b66\u4e60\u53ca\u63d0\u5347\u8fb9\u754c\u9884\u6d4b\u51c6\u786e\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faP3M\u673a\u5236\u52a8\u6001\u8c03\u6574\u6270\u52a8\uff0c\u7ed3\u5408\u8fb9\u754c\u805a\u7126\u635f\u5931\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u8fb9\u754c\u533a\u57df\u7684\u654f\u611f\u6027\u3002", "result": "\u57282D\u548c3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0cP3M\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fb9\u754c\u805a\u7126\u635f\u5931\u5177\u901a\u7528\u6027\u3002", "conclusion": "P3M\u548c\u8fb9\u754c\u805a\u7126\u635f\u5931\u6709\u6548\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2505.16559", "pdf": "https://arxiv.org/pdf/2505.16559", "abs": "https://arxiv.org/abs/2505.16559", "authors": ["Biao Yi", "Tiansheng Huang", "Baolei Zhang", "Tong Li", "Lihai Nie", "Zheli Liu", "Li Shen"], "title": "CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Fine-tuning-as-a-service, while commercially successful for Large Language\nModel (LLM) providers, exposes models to harmful fine-tuning attacks. As a\nwidely explored defense paradigm against such attacks, unlearning attempts to\nremove malicious knowledge from LLMs, thereby essentially preventing them from\nbeing used to perform malicious tasks. However, we highlight a critical flaw:\nthe powerful general adaptability of LLMs allows them to easily bypass\nselective unlearning by rapidly relearning or repurposing their capabilities\nfor harmful tasks. To address this fundamental limitation, we propose a\nparadigm shift: instead of selective removal, we advocate for inducing model\ncollapse--effectively forcing the model to \"unlearn everything\"--specifically\nin response to updates characteristic of malicious adaptation. This collapse\ndirectly neutralizes the very general capabilities that attackers exploit,\ntackling the core issue unaddressed by selective unlearning. We introduce the\nCollapse Trap (CTRAP) as a practical mechanism to implement this concept\nconditionally. Embedded during alignment, CTRAP pre-configures the model's\nreaction to subsequent fine-tuning dynamics. If updates during fine-tuning\nconstitute a persistent attempt to reverse safety alignment, the pre-configured\ntrap triggers a progressive degradation of the model's core language modeling\nabilities, ultimately rendering it inert and useless for the attacker.\nCrucially, this collapse mechanism remains dormant during benign fine-tuning,\nensuring the model's utility and general capabilities are preserved for\nlegitimate users. Extensive empirical results demonstrate that CTRAP\neffectively counters harmful fine-tuning risks across various LLMs and attack\nsettings, while maintaining high performance in benign scenarios. Our code is\navailable at https://anonymous.4open.science/r/CTRAP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCTRAP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bf1\u5bfc\u6a21\u578b\u5d29\u6e83\u6765\u9632\u6b62\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u800c\u975e\u9009\u62e9\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u9009\u62e9\u6027\u9057\u5fd8\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u963b\u6b62LLMs\u5feb\u901f\u91cd\u65b0\u5b66\u4e60\u6216\u8f6c\u5411\u6076\u610f\u4efb\u52a1\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5f7b\u5e95\u7684\u9632\u5fa1\u673a\u5236\u3002", "method": "\u63d0\u51faCollapse Trap (CTRAP)\uff0c\u5728\u6a21\u578b\u5bf9\u9f50\u65f6\u9884\u914d\u7f6e\u5176\u53cd\u5e94\u673a\u5236\uff0c\u82e5\u68c0\u6d4b\u5230\u6076\u610f\u5fae\u8c03\u5219\u89e6\u53d1\u6a21\u578b\u6838\u5fc3\u80fd\u529b\u5d29\u6e83\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eCTRAP\u80fd\u6709\u6548\u62b5\u5fa1\u591a\u79cdLLMs\u7684\u6076\u610f\u5fae\u8c03\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u573a\u666f\u4e0b\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "CTRAP\u901a\u8fc7\u6a21\u578b\u5d29\u6e83\u673a\u5236\u89e3\u51b3\u4e86\u9009\u62e9\u6027\u9057\u5fd8\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.15925", "pdf": "https://arxiv.org/pdf/2505.15925", "abs": "https://arxiv.org/abs/2505.15925", "authors": ["Bowen Feng", "Zhiting Mei", "Baiang Li", "Julian Ost", "Roger Girgis", "Anirudha Majumdar", "Felix Heide"], "title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "While autonomous driving (AD) stacks struggle with decision making under\npartial observability and real-world complexity, human drivers are capable of\ncommonsense reasoning to make near-optimal decisions with limited information.\nRecent work has attempted to leverage finetuned Vision-Language Models (VLMs)\nfor trajectory planning at inference time to emulate human behavior. Despite\ntheir success in benchmark evaluations, these methods are often impractical to\ndeploy (a 70B parameter VLM inference at merely 8 tokens per second requires\nmore than 160G of memory), and their monolithic network structure prohibits\nsafety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for\nautonomous Driving (VERDI), a training-time framework that distills the\nreasoning process and commonsense knowledge of VLMs into the AD stack. VERDI\naugments modular differentiable end-to-end (e2e) AD models by aligning\nintermediate module outputs at the perception, prediction, and planning stages\nwith text features explaining the driving reasoning process produced by VLMs.\nBy encouraging alignment in latent space, \\textsc{VERDI} enables the modular AD\nstack to internalize structured reasoning, without incurring the inference-time\ncosts of large VLMs. We demonstrate the effectiveness of our method on the\nNuScenes dataset and find that VERDI outperforms existing e2e methods that do\nnot embed reasoning by 10% in $\\ell_{2}$ distance, while maintaining high\ninference speed.", "AI": {"tldr": "VERDI\u662f\u4e00\u79cd\u8bad\u7ec3\u65f6\u6846\u67b6\uff0c\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u5e38\u8bc6\u77e5\u8bc6\u84b8\u998f\u5230\u81ea\u52a8\u9a7e\u9a76\u5806\u6808\u4e2d\uff0c\u63d0\u5347\u6027\u80fd\u4e14\u4fdd\u6301\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u90e8\u7f72\u4e0d\u5b9e\u7528\u4e14\u65e0\u6cd5\u5206\u89e3\u5b89\u5168\u6027\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u7279\u5f81\u4e0e\u81ea\u52a8\u9a7e\u9a76\u5806\u6808\u7684\u4e2d\u95f4\u6a21\u5757\u8f93\u51fa\u5bf9\u9f50\uff0c\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u7684\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\uff0cVERDI\u6bd4\u672a\u5d4c\u5165\u63a8\u7406\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u6027\u80fd\u63d0\u534710%\uff08\u21132\u8ddd\u79bb\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "VERDI\u6210\u529f\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u5d4c\u5165\u6a21\u5757\u5316\u81ea\u52a8\u9a7e\u9a76\u5806\u6808\uff0c\u907f\u514d\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2505.15946", "pdf": "https://arxiv.org/pdf/2505.15946", "abs": "https://arxiv.org/abs/2505.15946", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Tianyang Wang", "Xiao Wang", "Vince D. Calhoun"], "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "comment": null, "summary": "Decoding visual experiences from fMRI offers a powerful avenue to understand\nhuman perception and develop advanced brain-computer interfaces. However,\ncurrent progress often prioritizes maximizing reconstruction fidelity while\noverlooking interpretability, an essential aspect for deriving neuroscientific\ninsight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework\ndesigned for high-fidelity, adaptable, and interpretable visual reconstruction.\nMoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture\nwhere distinct experts process fMRI signals from functionally related voxel\ngroups, mimicking specialized brain networks. The experts are first trained to\nencode fMRI into the frozen CLIP space. A finetuned diffusion model then\nsynthesizes images, guided by expert outputs through a novel dual-stage routing\nmechanism that dynamically weighs expert contributions across the diffusion\nprocess. MoRE-Brain offers three main advancements: First, it introduces a\nnovel Mixture-of-Experts architecture grounded in brain network principles for\nneuro-decoding. Second, it achieves efficient cross-subject generalization by\nsharing core expert networks while adapting only subject-specific routers.\nThird, it provides enhanced mechanistic insight, as the explicit routing\nreveals precisely how different modeled brain regions shape the semantic and\nspatial attributes of the reconstructed image. Extensive experiments validate\nMoRE-Brain's high reconstruction fidelity, with bottleneck analyses further\ndemonstrating its effective utilization of fMRI signals, distinguishing genuine\nneural decoding from over-reliance on generative priors. Consequently,\nMoRE-Brain marks a substantial advance towards more generalizable and\ninterpretable fMRI-based visual decoding. Code will be publicly available soon:\nhttps://github.com/yuxiangwei0808/MoRE-Brain.", "AI": {"tldr": "MoRE-Brain\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8111\u7f51\u7edc\u539f\u7406\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u4fdd\u771f\u3001\u53ef\u9002\u5e94\u4e14\u53ef\u89e3\u91ca\u7684\u89c6\u89c9\u91cd\u5efa\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u589e\u5f3a\u795e\u7ecf\u89e3\u7801\u7684\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524dfMRI\u89c6\u89c9\u89e3\u7801\u7814\u7a76\u8fc7\u4e8e\u6ce8\u91cd\u91cd\u5efa\u4fdd\u771f\u5ea6\u800c\u5ffd\u89c6\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u795e\u7ecf\u79d1\u5b66\u6d1e\u5bdf\u7684\u83b7\u53d6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u4e13\u5bb6\u7f51\u7edc\u5904\u7406\u529f\u80fd\u76f8\u5173\u7684fMRI\u4fe1\u53f7\uff0c\u7ed3\u5408CLIP\u7a7a\u95f4\u7f16\u7801\u548c\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u8def\u7531\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MoRE-Brain\u7684\u9ad8\u4fdd\u771f\u91cd\u5efa\u80fd\u529b\uff0c\u6709\u6548\u5229\u7528fMRI\u4fe1\u53f7\uff0c\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u751f\u6210\u5148\u9a8c\u3002", "conclusion": "MoRE-Brain\u5728\u901a\u7528\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3afMRI\u89c6\u89c9\u89e3\u7801\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16631", "pdf": "https://arxiv.org/pdf/2505.16631", "abs": "https://arxiv.org/abs/2505.16631", "authors": ["Jonghwi Kim", "Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Jungseul Ok", "Gary Lee"], "title": "MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries", "categories": ["cs.IR", "cs.CL"], "comment": "16 pages, 9 figures", "summary": "Despite bilingual speakers frequently using mixed-language queries in web\nsearches, Information Retrieval (IR) research on them remains scarce. To\naddress this, we introduce MiLQ,Mixed-Language Query test set, the first public\nbenchmark of mixed-language queries, confirmed as realistic and highly\npreferred. Experiments show that multilingual IR models perform moderately on\nMiLQ and inconsistently across native, English, and mixed-language queries,\nalso suggesting code-switched training data's potential for robust IR models\nhandling such queries. Meanwhile, intentional English mixing in queries proves\nan effective strategy for bilinguals searching English documents, which our\nanalysis attributes to enhanced token matching compared to native queries.", "AI": {"tldr": "MiLQ\u662f\u9996\u4e2a\u516c\u5f00\u7684\u6df7\u5408\u8bed\u8a00\u67e5\u8be2\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u5728\u6df7\u5408\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u8868\u73b0\u4e2d\u7b49\u4e14\u4e0d\u4e00\u81f4\uff0c\u4f46\u4ee3\u7801\u5207\u6362\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u53cc\u8bed\u7528\u6237\u5e38\u7528\u6df7\u5408\u8bed\u8a00\u67e5\u8be2\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u7a00\u7f3a\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u5f15\u5165MiLQ\u6d4b\u8bd5\u96c6\uff0c\u8bc4\u4f30\u591a\u8bed\u8a00IR\u6a21\u578b\u5728\u6df7\u5408\u67e5\u8be2\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u4ee3\u7801\u5207\u6362\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5728\u6df7\u5408\u67e5\u8be2\u4e0a\u8868\u73b0\u4e2d\u7b49\u4e14\u4e0d\u4e00\u81f4\uff0c\u4f46\u4ee3\u7801\u5207\u6362\u6570\u636e\u53ef\u80fd\u63d0\u5347\u6027\u80fd\uff1b\u6df7\u5408\u82f1\u8bed\u67e5\u8be2\u5bf9\u53cc\u8bed\u7528\u6237\u641c\u7d22\u82f1\u8bed\u6587\u6863\u66f4\u6709\u6548\u3002", "conclusion": "\u6df7\u5408\u8bed\u8a00\u67e5\u8be2\u7814\u7a76\u9700\u66f4\u591a\u5173\u6ce8\uff0c\u4ee3\u7801\u5207\u6362\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u6df7\u5408\u82f1\u8bed\u67e5\u8be2\u662f\u4e00\u79cd\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2505.16017", "pdf": "https://arxiv.org/pdf/2505.16017", "abs": "https://arxiv.org/abs/2505.16017", "authors": ["Mariia Seleznova", "Hung-Hsu Chou", "Claudio Mayrink Verdun", "Gitta Kutyniok"], "title": "GradPCA: Leveraging NTK Alignment for Reliable Out-of-Distribution Detection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce GradPCA, an Out-of-Distribution (OOD) detection method that\nexploits the low-rank structure of neural network gradients induced by Neural\nTangent Kernel (NTK) alignment. GradPCA applies Principal Component Analysis\n(PCA) to gradient class-means, achieving more consistent performance than\nexisting methods across standard image classification benchmarks. We provide a\ntheoretical perspective on spectral OOD detection in neural networks to support\nGradPCA, highlighting feature-space properties that enable effective detection\nand naturally emerge from NTK alignment. Our analysis further reveals that\nfeature quality -- particularly the use of pretrained versus non-pretrained\nrepresentations -- plays a crucial role in determining which detectors will\nsucceed. Extensive experiments validate the strong performance of GradPCA, and\nour theoretical framework offers guidance for designing more principled\nspectral OOD detectors.", "AI": {"tldr": "GradPCA\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u4e3b\u6210\u5206\u5206\u6790\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528NTK\u5bf9\u9f50\u7684\u4f4e\u79e9\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u4e0d\u4e00\u81f4\uff0cGradPCA\u901a\u8fc7\u68af\u5ea6PCA\u548cNTK\u5bf9\u9f50\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5bf9\u68af\u5ea6\u7c7b\u5747\u503c\u5e94\u7528PCA\uff0c\u7ed3\u5408NTK\u5bf9\u9f50\u7684\u7406\u8bba\u652f\u6301\uff0c\u5206\u6790\u7279\u5f81\u7a7a\u95f4\u6027\u8d28\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1GradPCA\u6027\u80fd\u4f18\u8d8a\uff0c\u7279\u5f81\u8d28\u91cf\uff08\u5982\u9884\u8bad\u7ec3\u8868\u793a\uff09\u5bf9\u68c0\u6d4b\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "GradPCA\u4e3a\u8bbe\u8ba1\u66f4\u539f\u5219\u5316\u7684\u5149\u8c31OOD\u68c0\u6d4b\u5668\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2505.16027", "pdf": "https://arxiv.org/pdf/2505.16027", "abs": "https://arxiv.org/abs/2505.16027", "authors": ["Qinmei Xu", "Yiheng Li", "Xianghao Zhan", "Ahmet Gorkem Er", "Brittany Dashevsky", "Chuanjun Xu", "Mohammed Alawad", "Mengya Yang", "Liu Ya", "Changsheng Zhou", "Xiao Li", "Haruka Itakura", "Olivier Gevaert"], "title": "Benchmarking Chest X-ray Diagnosis Models Across Multinational Datasets", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.2"], "comment": "78 pages, 7 figures, 2 tabeles", "summary": "Foundation models leveraging vision-language pretraining have shown promise\nin chest X-ray (CXR) interpretation, yet their real-world performance across\ndiverse populations and diagnostic tasks remains insufficiently evaluated. This\nstudy benchmarks the diagnostic performance and generalizability of foundation\nmodels versus traditional convolutional neural networks (CNNs) on multinational\nCXR datasets. We evaluated eight CXR diagnostic models - five vision-language\nfoundation models and three CNN-based architectures - across 37 standardized\nclassification tasks using six public datasets from the USA, Spain, India, and\nVietnam, and three private datasets from hospitals in China. Performance was\nassessed using AUROC, AUPRC, and other metrics across both shared and\ndataset-specific tasks. Foundation models outperformed CNNs in both accuracy\nand task coverage. MAVL, a model incorporating knowledge-enhanced prompts and\nstructured supervision, achieved the highest performance on public (mean AUROC:\n0.82; AUPRC: 0.32) and private (mean AUROC: 0.95; AUPRC: 0.89) datasets,\nranking first in 14 of 37 public and 3 of 4 private tasks. All models showed\nreduced performance on pediatric cases, with average AUROC dropping from 0.88\n+/- 0.18 in adults to 0.57 +/- 0.29 in children (p = 0.0202). These findings\nhighlight the value of structured supervision and prompt design in radiologic\nAI and suggest future directions including geographic expansion and ensemble\nmodeling for clinical deployment. Code for all evaluated models is available at\nhttps://drive.google.com/drive/folders/1B99yMQm7bB4h1sVMIBja0RfUu8gLktCE", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u4e0e\u4f20\u7edfCNN\u5728\u8de8\u56fdCXR\u6570\u636e\u96c6\u4e0a\u7684\u8bca\u65ad\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u4e0a\u4f18\u4e8eCNN\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u513f\u79d1\u75c5\u4f8b\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u4e0e\u4f20\u7edfCNN\u5728CXR\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u4eba\u7fa4\u548c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e868\u4e2aCXR\u8bca\u65ad\u6a21\u578b\uff085\u4e2a\u57fa\u7840\u6a21\u578b\u548c3\u4e2aCNN\u67b6\u6784\uff09\u572837\u4e2a\u6807\u51c6\u5316\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u4e866\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c3\u4e2a\u79c1\u6709\u6570\u636e\u96c6\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u516c\u5171\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8eCNN\uff0c\u5176\u4e2dMAVL\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002\u6240\u6709\u6a21\u578b\u5728\u513f\u79d1\u75c5\u4f8b\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u76d1\u7763\u548c\u63d0\u793a\u8bbe\u8ba1\u5728\u653e\u5c04AI\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u65b9\u5411\u5305\u62ec\u5730\u7406\u6269\u5c55\u548c\u96c6\u6210\u6a21\u578b\u7528\u4e8e\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2505.16686", "pdf": "https://arxiv.org/pdf/2505.16686", "abs": "https://arxiv.org/abs/2505.16686", "authors": ["Lars Benedikt Kaesberg", "Jan Philip Wahle", "Terry Ruas", "Bela Gipp"], "title": "SPaRC: A Spatial Pathfinding Reasoning Challenge", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing reasoning datasets saturate and fail to test abstract, multi-step\nproblems, especially pathfinding and complex rule constraint satisfaction. We\nintroduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000\n2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,\nrequiring step-by-step planning with arithmetic and geometric rules. Humans\nachieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best\nreasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).\nModels often generate invalid paths (>50% of puzzles for o4-mini), and\nreasoning tokens reveal they make errors in navigation and spatial logic.\nUnlike humans, who take longer on hard puzzles, models fail to scale test-time\ncompute with difficulty. Allowing models to make multiple solution attempts\nimproves accuracy, suggesting potential for better spatial reasoning with\nimproved training and efficient test-time scaling methods. SPaRC can be used as\na window into models' spatial reasoning limitations and drive research toward\nnew methods that excel in abstract, multi-step problem-solving.", "AI": {"tldr": "SPaRC\u662f\u4e00\u4e2a\u65b0\u76842D\u7f51\u683c\u8def\u5f84\u5bfb\u627e\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7a7a\u95f4\u548c\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u3002\u4eba\u7c7b\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u73b0\u6709\u6a21\u578b\u8868\u73b0\u8f83\u5dee\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u95ee\u9898\u4e0a\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6570\u636e\u96c6\u5728\u6d4b\u8bd5\u62bd\u8c61\u3001\u591a\u6b65\u95ee\u9898\u65f6\u8868\u73b0\u9971\u548c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\uff0c\u5c24\u5176\u662f\u8def\u5f84\u5bfb\u627e\u548c\u590d\u6742\u89c4\u5219\u7ea6\u675f\u95ee\u9898\u3002", "method": "\u5f15\u5165SPaRC\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a2D\u7f51\u683c\u8def\u5f84\u5bfb\u627e\u8c1c\u9898\uff0c\u8981\u6c42\u57fa\u4e8e\u7b97\u672f\u548c\u51e0\u4f55\u89c4\u5219\u8fdb\u884c\u9010\u6b65\u89c4\u5212\u3002", "result": "\u4eba\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0898.0%\uff1b\u56f0\u96be\u8c1c\u9898\u4e3a94.5%\uff09\uff0c\u800c\u6700\u4f73\u6a21\u578b\uff08\u5982o4-mini\uff09\u8868\u73b0\u8f83\u5dee\uff0815.8%\uff1b\u56f0\u96be\u8c1c\u9898\u4e3a1.1%\uff09\u3002\u6a21\u578b\u5e38\u751f\u6210\u65e0\u6548\u8def\u5f84\uff0c\u4e14\u5728\u5bfc\u822a\u548c\u7a7a\u95f4\u903b\u8f91\u4e0a\u51fa\u9519\u3002", "conclusion": "SPaRC\u63ed\u793a\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a8\u52a8\u7814\u7a76\u6539\u8fdb\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387\uff0c\u4ee5\u63d0\u5347\u62bd\u8c61\u3001\u591a\u6b65\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2505.16028", "pdf": "https://arxiv.org/pdf/2505.16028", "abs": "https://arxiv.org/abs/2505.16028", "authors": ["Shuvashis Sarker", "Shamim Rahim Refat", "Faika Fairuj Preotee", "Tanvir Rouf Shawon", "Raihan Tanvir"], "title": "Comprehensive Lung Disease Detection Using Deep Learning Models and Hybrid Chest X-ray Data with Explainable AI", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for publication in 2024 27th International Conference on\n  Computer and Information Technology (ICCIT)", "summary": "Advanced diagnostic instruments are crucial for the accurate detection and\ntreatment of lung diseases, which affect millions of individuals globally. This\nstudy examines the effectiveness of deep learning and transfer learning models\nusing a hybrid dataset, created by merging four individual datasets from\nBangladesh and global sources. The hybrid dataset significantly enhances model\naccuracy and generalizability, particularly in detecting COVID-19, pneumonia,\nlung opacity, and normal lung conditions from chest X-ray images. A range of\nmodels, including CNN, VGG16, VGG19, InceptionV3, Xception, ResNet50V2,\nInceptionResNetV2, MobileNetV2, and DenseNet121, were applied to both\nindividual and hybrid datasets. The results showed superior performance on the\nhybrid dataset, with VGG16, Xception, ResNet50V2, and DenseNet121 each\nachieving an accuracy of 99%. This consistent performance across the hybrid\ndataset highlights the robustness of these models in handling diverse data\nwhile maintaining high accuracy. To understand the models implicit behavior,\nexplainable AI techniques were employed to illuminate their black-box nature.\nSpecifically, LIME was used to enhance the interpretability of model\npredictions, especially in cases of misclassification, contributing to the\ndevelopment of reliable and interpretable AI-driven solutions for medical\nimaging.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u6df7\u5408\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80ba\u90e8\u75be\u75c5\uff08\u5982COVID-19\u3001\u80ba\u708e\u7b49\uff09\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u591a\u79cd\u6a21\u578b\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u80ba\u90e8\u75be\u75c5\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\uff0c\u9700\u8981\u5148\u8fdb\u7684\u8bca\u65ad\u5de5\u5177\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u548c\u6cbb\u7597\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528CNN\u3001VGG16\u3001VGG19\u7b49\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u6df7\u5408\u6570\u636e\u96c6\uff08\u6765\u81ea\u5b5f\u52a0\u62c9\u56fd\u548c\u5168\u7403\u6570\u636e\uff09\uff0c\u5e76\u5e94\u7528LIME\u6280\u672f\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6df7\u5408\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cVGG16\u3001Xception\u7b49\u6a21\u578b\u5728\u6df7\u5408\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u51c6\u786e\u7387\u3002", "conclusion": "\u6df7\u5408\u6570\u636e\u96c6\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u4e3a\u533b\u7597\u5f71\u50cf\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u4e14\u900f\u660e\u7684AI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16737", "pdf": "https://arxiv.org/pdf/2505.16737", "abs": "https://arxiv.org/abs/2505.16737", "authors": ["Chengcan Wu", "Zhixin Zhang", "Zeming Wei", "Yihao Zhang", "Meng Sun"], "title": "Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "math.OC"], "comment": null, "summary": "The significant progress of large language models (LLMs) has led to\nremarkable achievements across numerous applications. However, their ability to\ngenerate harmful content has sparked substantial safety concerns. Despite the\nimplementation of safety alignment techniques during the pre-training phase,\nrecent research indicates that fine-tuning LLMs on adversarial or even benign\ndata can inadvertently compromise their safety. In this paper, we re-examine\nthe fundamental issue of why fine-tuning on non-harmful data still results in\nsafety degradation. We introduce a safety-aware probing (SAP) optimization\nframework designed to mitigate the safety risks of fine-tuning LLMs.\nSpecifically, SAP incorporates a safety-aware probe into the gradient\npropagation process, mitigating the model's risk of safety degradation by\nidentifying potential pitfalls in gradient directions, thereby enhancing\ntask-specific performance while successfully preserving model safety. Our\nextensive experimental results demonstrate that SAP effectively reduces\nharmfulness below the original fine-tuned model and achieves comparable test\nloss to standard fine-tuning methods. Our code is available at\nhttps://github.com/ChengcanWu/SAP.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5b89\u5168\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u611f\u77e5\u63a2\u6d4b\uff08SAP\uff09\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5df2\u91c7\u7528\u5b89\u5168\u5bf9\u9f50\u6280\u672f\uff0c\u4f46\u5fae\u8c03\u65f6\u4ecd\u53ef\u80fd\u56e0\u6570\u636e\u95ee\u9898\u5bfc\u81f4\u5b89\u5168\u6027\u80fd\u4e0b\u964d\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faSAP\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u68af\u5ea6\u4f20\u64ad\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5b89\u5168\u611f\u77e5\u63a2\u6d4b\uff0c\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u65b9\u5411\uff0c\u4ece\u800c\u4f18\u5316\u5fae\u8c03\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAP\u80fd\u6709\u6548\u964d\u4f4e\u6709\u5bb3\u5185\u5bb9\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u7684\u6d4b\u8bd5\u635f\u5931\u3002", "conclusion": "SAP\u6846\u67b6\u4e3a\u89e3\u51b3LLMs\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2505.16091", "pdf": "https://arxiv.org/pdf/2505.16091", "abs": "https://arxiv.org/abs/2505.16091", "authors": ["Jinpei Guo", "Yifei Ji", "Zheng Chen", "Kai Liu", "Min Liu", "Wang Rao", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Pretrained latent diffusion models have shown strong potential for lossy\nimage compression, owing to their powerful generative priors. Most existing\ndiffusion-based methods reconstruct images by iteratively denoising from random\nnoise, guided by compressed latent representations. While these approaches have\nachieved high reconstruction quality, their multi-step sampling process incurs\nsubstantial computational overhead. Moreover, they typically require training\nseparate models for different compression bit-rates, leading to significant\ntraining and storage costs. To address these challenges, we propose a one-step\ndiffusion codec across multiple bit-rates. termed OSCAR. Specifically, our\nmethod views compressed latents as noisy variants of the original latents,\nwhere the level of distortion depends on the bit-rate. This perspective allows\nthem to be modeled as intermediate states along a diffusion trajectory. By\nestablishing a mapping from the compression bit-rate to a pseudo diffusion\ntimestep, we condition a single generative model to support reconstructions at\nmultiple bit-rates. Meanwhile, we argue that the compressed latents retain rich\nstructural information, thereby making one-step denoising feasible. Thus, OSCAR\nreplaces iterative sampling with a single denoising pass, significantly\nimproving inference efficiency. Extensive experiments demonstrate that OSCAR\nachieves superior performance in both quantitative and visual quality metrics.\nThe code and models will be released at https://github.com/jp-guo/OSCAR.", "AI": {"tldr": "OSCAR\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u4e00\u6b65\u6269\u6563\u7f16\u89e3\u7801\u5668\uff0c\u652f\u6301\u591a\u6bd4\u7279\u7387\u56fe\u50cf\u538b\u7f29\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u9700\u591a\u6b65\u91c7\u6837\u4e14\u9700\u4e3a\u4e0d\u540c\u6bd4\u7279\u7387\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\u3002", "method": "\u5c06\u538b\u7f29\u6f5c\u5728\u8868\u793a\u89c6\u4e3a\u539f\u59cb\u6f5c\u5728\u8868\u793a\u7684\u566a\u58f0\u53d8\u4f53\uff0c\u901a\u8fc7\u4f2a\u6269\u6563\u65f6\u95f4\u6b65\u6620\u5c04\u652f\u6301\u591a\u6bd4\u7279\u7387\u91cd\u5efa\uff0c\u5e76\u91c7\u7528\u4e00\u6b65\u53bb\u566a\u3002", "result": "OSCAR\u5728\u5b9a\u91cf\u548c\u89c6\u89c9\u8d28\u91cf\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "conclusion": "OSCAR\u4e3a\u9ad8\u6548\u591a\u6bd4\u7279\u7387\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16826", "pdf": "https://arxiv.org/pdf/2505.16826", "abs": "https://arxiv.org/abs/2505.16826", "authors": ["Wei Sun", "Wen Yang", "Pu Jian", "Qianlong Du", "Fuwei Cui", "Shuo Ren", "Jiajun Zhang"], "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances have demonstrated that integrating reinforcement learning\nwith rule-based rewards can significantly enhance the reasoning capabilities of\nlarge language models, even without supervised fine-tuning. However, prevalent\nreinforcement learning algorithms such as GRPO and its variants like DAPO,\nsuffer from a coarse granularity issue when computing the advantage.\nSpecifically, they compute rollout-level advantages that assign identical\nvalues to every token within a sequence, failing to capture token-specific\ncontributions and hindering effective learning. To address this limitation, we\npropose Key-token Advantage Estimation (KTAE) - a novel algorithm that\nestimates fine-grained, token-level advantages without introducing additional\nmodels. KTAE leverages the correctness of sampled rollouts and applies\nstatistical analysis to quantify the importance of individual tokens within a\nsequence to the final outcome. This quantified token-level importance is then\ncombined with the rollout-level advantage to obtain a more fine-grained\ntoken-level advantage estimation. Empirical results show that models trained\nwith GRPO+KTAE and DAPO+KTAE outperform baseline methods across five\nmathematical reasoning benchmarks. Notably, they achieve higher accuracy with\nshorter responses and even surpass R1-Distill-Qwen-1.5B using the same base\nmodel.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKTAE\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\u548cDAPO\uff09\u5728\u8ba1\u7b97\u4f18\u52bf\u65f6\u7684\u7c97\u7c92\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982GRPO\u548cDAPO\uff09\u5728\u8ba1\u7b97\u4f18\u52bf\u65f6\u5b58\u5728\u7c97\u7c92\u5ea6\u95ee\u9898\uff0c\u65e0\u6cd5\u6355\u6349token\u7ea7\u8d21\u732e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u5b66\u4e60\u6548\u679c\u3002", "method": "\u63d0\u51faKTAE\u7b97\u6cd5\uff0c\u5229\u7528\u91c7\u6837rollout\u7684\u6b63\u786e\u6027\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u91cf\u5316token\u7ea7\u91cd\u8981\u6027\uff0c\u7ed3\u5408rollout\u7ea7\u4f18\u52bf\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6token\u7ea7\u4f18\u52bf\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGRPO+KTAE\u548cDAPO+KTAE\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u80fd\u4ee5\u66f4\u77ed\u7684\u54cd\u5e94\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "KTAE\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u4f18\u52bf\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16832", "pdf": "https://arxiv.org/pdf/2505.16832", "abs": "https://arxiv.org/abs/2505.16832", "authors": ["Haonian Ji", "Shi Qiu", "Siyang Xin", "Siwei Han", "Zhaorun Chen", "Hongyi Wang", "Dake Zhang", "Huaxiu Yao"], "title": "From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "16 pages; 7 figures", "summary": "While foundation models (FMs), such as diffusion models and large\nvision-language models (LVLMs), have been widely applied in educational\ncontexts, their ability to generate pedagogically effective visual explanations\nremains limited. Most existing approaches focus primarily on textual reasoning,\noverlooking the critical role of structured and interpretable visualizations in\nsupporting conceptual understanding. To better assess the visual reasoning\ncapabilities of FMs in educational settings, we introduce EduVisBench, a\nmulti-domain, multi-level benchmark. EduVisBench features diverse STEM problem\nsets requiring visually grounded solutions, along with a fine-grained\nevaluation rubric informed by pedagogical theory. Our empirical analysis\nreveals that existing models frequently struggle with the inherent challenge of\ndecomposing complex reasoning and translating it into visual representations\naligned with human cognitive processes. To address these limitations, we\npropose EduVisAgent, a multi-agent collaborative framework that coordinates\nspecialized agents for instructional planning, reasoning decomposition,\nmetacognitive prompting, and visualization design. Experimental results show\nthat EduVisAgent substantially outperforms all baselines, achieving a 40.2%\nimprovement and delivering more educationally aligned visualizations.\nEduVisBench and EduVisAgent are available at\nhttps://github.com/aiming-lab/EduVisBench and\nhttps://github.com/aiming-lab/EduVisAgent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86EduVisBench\u57fa\u51c6\u548cEduVisAgent\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u57fa\u7840\u6a21\u578b\u5728\u6559\u80b2\u573a\u666f\u4e2d\u751f\u6210\u53ef\u89c6\u5316\u89e3\u91ca\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\u6559\u80b2\u6709\u6548\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u5ffd\u89c6\u4e86\u7ed3\u6784\u5316\u53ef\u89c6\u5316\u5bf9\u6982\u5ff5\u7406\u89e3\u7684\u91cd\u8981\u6027\u3002", "method": "\u5f15\u5165\u591a\u9886\u57df\u3001\u591a\u5c42\u6b21\u7684EduVisBench\u57fa\u51c6\uff0c\u5e76\u63d0\u51faEduVisAgent\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u5206\u89e3\u63a8\u7406\u5e76\u8bbe\u8ba1\u6559\u80b2\u5bf9\u9f50\u7684\u53ef\u89c6\u5316\u3002", "result": "EduVisAgent\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u6027\u80fd\u63d0\u534740.2%\uff0c\u751f\u6210\u66f4\u7b26\u5408\u6559\u80b2\u9700\u6c42\u7684\u53ef\u89c6\u5316\u3002", "conclusion": "EduVisBench\u548cEduVisAgent\u4e3a\u6559\u80b2\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u548c\u6539\u8fdb\u5de5\u5177\u3002"}}
{"id": "2505.16152", "pdf": "https://arxiv.org/pdf/2505.16152", "abs": "https://arxiv.org/abs/2505.16152", "authors": ["Bolin Chen", "Shanzhi Yin", "Hanwei Zhu", "Lingyu Zhu", "Zihan Zhang", "Jie Chen", "Ru-Ling Liao", "Shiqi Wang", "Yan Ye"], "title": "Compressing Human Body Video with Interactive Semantics: A Generative Approach", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In this paper, we propose to compress human body video with interactive\nsemantics, which can facilitate video coding to be interactive and controllable\nby manipulating semantic-level representations embedded in the coded bitstream.\nIn particular, the proposed encoder employs a 3D human model to disentangle\nnonlinear dynamics and complex motion of human body signal into a series of\nconfigurable embeddings, which are controllably edited, compactly compressed,\nand efficiently transmitted. Moreover, the proposed decoder can evolve the\nmesh-based motion fields from these decoded semantics to realize the\nhigh-quality human body video reconstruction. Experimental results illustrate\nthat the proposed framework can achieve promising compression performance for\nhuman body videos at ultra-low bitrate ranges compared with the\nstate-of-the-art video coding standard Versatile Video Coding (VVC) and the\nlatest generative compression schemes. Furthermore, the proposed framework\nenables interactive human body video coding without any additional\npre-/post-manipulation processes, which is expected to shed light on\nmetaverse-related digital human communication in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ea4\u4e92\u8bed\u4e49\u7684\u4eba\u4f53\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5165\u8bed\u4e49\u7ea7\u8868\u793a\u5b9e\u73b0\u89c6\u9891\u7f16\u7801\u7684\u4ea4\u4e92\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u7f16\u7801\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u6027\u80fd\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u4ea4\u4e92\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u5143\u5b87\u5b99\u4e2d\u6570\u5b57\u4eba\u901a\u4fe1\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u75283D\u4eba\u4f53\u6a21\u578b\u5c06\u590d\u6742\u7684\u4eba\u4f53\u4fe1\u53f7\u5206\u89e3\u4e3a\u53ef\u914d\u7f6e\u7684\u5d4c\u5165\u8868\u793a\uff0c\u652f\u6301\u53ef\u63a7\u7f16\u8f91\u3001\u7d27\u51d1\u538b\u7f29\u548c\u9ad8\u6548\u4f20\u8f93\u3002\u89e3\u7801\u5668\u57fa\u4e8e\u8fd9\u4e9b\u8bed\u4e49\u91cd\u5efa\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "result": "\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8eVVC\u6807\u51c6\u548c\u751f\u6210\u5f0f\u538b\u7f29\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u989d\u5916\u9884\u5904\u7406\u5373\u53ef\u5b9e\u73b0\u4ea4\u4e92\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5143\u5b87\u5b99\u4e2d\u7684\u6570\u5b57\u4eba\u901a\u4fe1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4ea4\u4e92\u6027\u5f3a\u7684\u89c6\u9891\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.16850", "pdf": "https://arxiv.org/pdf/2505.16850", "abs": "https://arxiv.org/abs/2505.16850", "authors": ["Tajamul Ashraf", "Mohammed Mohsen Peerzada", "Moloud Abdar", "Yutong Xie", "Yuyin Zhou", "Xiaofeng Liu", "Iqra Altaf Gillani", "Janibul Bashir"], "title": "ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "Federated Learning Benchmark for Domain Adaptation, Trustworthiness,\n  and Reasoning", "summary": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training while preserving data privacy across decentralized participants.\nAs FL adoption grows, numerous techniques have been proposed to tackle its\npractical challenges. However, the lack of standardized evaluation across key\ndimensions hampers systematic progress and fair comparison of FL methods. In\nthis work, we introduce ATR-Bench, a unified framework for analyzing federated\nlearning through three foundational dimensions: Adaptation, Trust, and\nReasoning. We provide an in-depth examination of the conceptual foundations,\ntask formulations, and open research challenges associated with each theme. We\nhave extensively benchmarked representative methods and datasets for adaptation\nto heterogeneous clients and trustworthiness in adversarial or unreliable\nenvironments. Due to the lack of reliable metrics and models for reasoning in\nFL, we only provide literature-driven insights for this dimension. ATR-Bench\nlays the groundwork for a systematic and holistic evaluation of federated\nlearning with real-world relevance. We will make our complete codebase publicly\naccessible and a curated repository that continuously tracks new developments\nand research in the FL literature.", "AI": {"tldr": "ATR-Bench\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u9002\u5e94\u6027\u3001\u4fe1\u4efb\u548c\u63a8\u7406\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u8054\u90a6\u5b66\u4e60\uff0c\u586b\u8865\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u8fdb\u5c55\u548c\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u63d0\u51faATR-Bench\u6846\u67b6\uff0c\u4ece\u9002\u5e94\u6027\u3001\u4fe1\u4efb\u548c\u63a8\u7406\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u8054\u90a6\u5b66\u4e60\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u4e3a\u5f02\u6784\u5ba2\u6237\u7aef\u9002\u5e94\u6027\u548c\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u4fe1\u4efb\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u7406\u90e8\u5206\u63d0\u4f9b\u4e86\u6587\u732e\u9a71\u52a8\u7684\u89c1\u89e3\u3002", "conclusion": "ATR-Bench\u4e3a\u8054\u90a6\u5b66\u4e60\u7684\u7cfb\u7edf\u5316\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u4ee3\u7801\u548c\u6301\u7eed\u66f4\u65b0\u7684\u7814\u7a76\u5e93\u3002"}}
{"id": "2505.16177", "pdf": "https://arxiv.org/pdf/2505.16177", "abs": "https://arxiv.org/abs/2505.16177", "authors": ["Linfeng Qi", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Houqiang Li", "Yan Lu"], "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Most existing approaches for image and video compression perform transform\ncoding in the pixel space to reduce redundancy. However, due to the\nmisalignment between the pixel-space distortion and human perception, such\nschemes often face the difficulties in achieving both high-realism and\nhigh-fidelity at ultra-low bitrate. To solve this problem, we propose\n\\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for\nimage and video compression, termed GLC-image and GLC-Video. The transform\ncoding of GLC is conducted in the latent space of a generative vector-quantized\nvariational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent\nspace offers greater sparsity, richer semantics and better alignment with human\nperception, and show its advantages in achieving high-realism and high-fidelity\ncompression. To further enhance performance, we improve the hyper prior by\nintroducing a spatial categorical hyper module in GLC-image and a\nspatio-temporal categorical hyper module in GLC-video. Additionally, the\ncode-prediction-based loss function is proposed to enhance the semantic\nconsistency. Experiments demonstrate that our scheme shows high visual quality\nat ultra-low bitrate for both image and video compression. For image\ncompression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp,\nachieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer\nbitrate on the CLIC 2020 test set. For video compression, GLC-video achieves\n65.3\\% bitrate saving over PLVC in terms of DISTS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6f5c\u5728\u7f16\u7801\uff08GLC\uff09\u7684\u56fe\u50cf\u548c\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u53d8\u6362\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u9ad8\u771f\u5b9e\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u50cf\u7d20\u7a7a\u95f4\u7684\u538b\u7f29\u65b9\u6cd5\u56e0\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e0d\u5bf9\u9f50\uff0c\u96be\u4ee5\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u540c\u65f6\u5b9e\u73b0\u9ad8\u771f\u5b9e\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VQ-VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u53d8\u6362\u7f16\u7801\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4\u5206\u7c7b\u8d85\u6a21\u5757\u548c\u65f6\u7a7a\u5206\u7c7b\u8d85\u6a21\u5757\u4f18\u5316\u6027\u80fd\u3002", "result": "GLC-image\u5728CLIC 2020\u6d4b\u8bd5\u96c6\u4e0a\u6bd4\u7279\u7387\u4f4e\u4e8e0.04 bpp\uff0c\u6bd4MS-ILLM\u8282\u770145%\u6bd4\u7279\u7387\uff1bGLC-video\u5728DISTS\u6307\u6807\u4e0a\u6bd4PLVC\u8282\u770165.3%\u6bd4\u7279\u7387\u3002", "conclusion": "GLC\u6a21\u578b\u5728\u8d85\u4f4e\u6bd4\u7279\u7387\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u538b\u7f29\u7684\u89c6\u89c9\u8d28\u91cf\u3002"}}
{"id": "2505.16886", "pdf": "https://arxiv.org/pdf/2505.16886", "abs": "https://arxiv.org/abs/2505.16886", "authors": ["Nour Jedidi", "Yung-Sung Chuang", "James Glass", "Jimmy Lin"], "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u63a8\u7406\u7684\u6bb5\u843d\u91cd\u6392\u5e8f\u6a21\u578b\uff08ReasonRR\uff09\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u5982\u975e\u63a8\u7406\u7684\u6807\u51c6\u6a21\u578b\uff08StandardRR\uff09\uff0c\u4e14\u7981\u7528\u63a8\u7406\u540e\uff08ReasonRR-NoReason\uff09\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u63a2\u7d22\u63a8\u7406\u80fd\u529b\u662f\u5426\u80fd\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6bb5\u843d\u91cd\u6392\u5e8f\u51c6\u786e\u6027\u3002", "method": "\u6bd4\u8f83\u63a8\u7406\u6a21\u578b\uff08ReasonRR\uff09\u4e0e\u975e\u63a8\u7406\u6a21\u578b\uff08StandardRR\uff09\u7684\u8868\u73b0\uff0c\u5e76\u5206\u6790\u63a8\u7406\u8fc7\u7a0b\u7684\u5f71\u54cd\u3002", "result": "StandardRR\u4f18\u4e8eReasonRR\uff0c\u4e14ReasonRR-NoReason\u8868\u73b0\u66f4\u4f73\uff0c\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u6a21\u578b\u504f\u5411\u6781\u7aef\u8bc4\u5206\u3002", "conclusion": "\u63a8\u7406\u8fc7\u7a0b\u9650\u5236\u4e86\u91cd\u6392\u5e8f\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u56e0\u5176\u672a\u80fd\u8003\u8651\u6bb5\u843d\u7684\u90e8\u5206\u76f8\u5173\u6027\u3002"}}
{"id": "2505.16888", "pdf": "https://arxiv.org/pdf/2505.16888", "abs": "https://arxiv.org/abs/2505.16888", "authors": ["Viet Pham", "Thai Le"], "title": "CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced many applications, but are also\nknown to be vulnerable to adversarial attacks. In this work, we introduce a\nnovel security threat: hijacking AI-human conversations by manipulating LLMs'\nsystem prompts to produce malicious answers only to specific targeted questions\n(e.g., \"Who should I vote for US President?\", \"Are Covid vaccines safe?\"),\nwhile behaving benignly on others. This attack is detrimental as it can enable\nmalicious actors to exercise large-scale information manipulation by spreading\nharmful but benign-looking system prompts online. To demonstrate such an\nattack, we develop CAIN, an algorithm that can automatically curate such\nharmful system prompts for a specific target question in a black-box setting or\nwithout the need to access the LLM's parameters. Evaluated on both open-source\nand commercial LLMs, CAIN demonstrates significant adversarial impact. In\nuntargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves\nup to 40% F1 degradation on targeted questions while preserving high accuracy\non benign inputs. For targeted attacks or forcing LLMs to output specific\nharmful answers, CAIN achieves over 70% F1 scores on these targeted responses\nwith minimal impact on benign questions. Our results highlight the critical\nneed for enhanced robustness measures to safeguard the integrity and safety of\nLLMs in real-world applications. All source code will be publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u5a01\u80c1\uff1a\u901a\u8fc7\u64cd\u7eb5LLMs\u7684\u7cfb\u7edf\u63d0\u793a\uff0c\u4f7f\u5176\u5728\u7279\u5b9a\u76ee\u6807\u95ee\u9898\u4e0a\u8f93\u51fa\u6076\u610f\u7b54\u6848\uff0c\u540c\u65f6\u5728\u5176\u4ed6\u95ee\u9898\u4e0a\u8868\u73b0\u6b63\u5e38\u3002\u4f5c\u8005\u5f00\u53d1\u4e86CAIN\u7b97\u6cd5\uff0c\u6210\u529f\u5728\u5f00\u6e90\u548c\u5546\u4e1aLLMs\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u653b\u51fb\u6548\u679c\u3002", "motivation": "LLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u8ba8\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u64cd\u63a7AI-\u4eba\u7c7b\u5bf9\u8bdd\u7684\u5a01\u80c1\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u653b\u51fb\u7684\u5371\u5bb3\u6027\u53ca\u5176\u5927\u89c4\u6a21\u4fe1\u606f\u64cd\u7eb5\u7684\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u4e86CAIN\u7b97\u6cd5\uff0c\u80fd\u5728\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u81ea\u52a8\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u7684\u6709\u5bb3\u7cfb\u7edf\u63d0\u793a\uff0c\u65e0\u9700\u8bbf\u95eeLLMs\u53c2\u6570\u3002", "result": "CAIN\u5728\u975e\u76ee\u6807\u653b\u51fb\u4e2d\u5bfc\u81f4\u76ee\u6807\u95ee\u9898F1\u5206\u6570\u4e0b\u964d40%\uff0c\u5728\u76ee\u6807\u653b\u51fb\u4e2d\u8fbe\u523070%\u7684F1\u5206\u6570\uff0c\u4e14\u5bf9\u826f\u6027\u95ee\u9898\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u589e\u5f3aLLMs\u9c81\u68d2\u6027\u7684\u7d27\u8feb\u6027\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u5b8c\u6574\u6027\u3002"}}
{"id": "2505.16196", "pdf": "https://arxiv.org/pdf/2505.16196", "abs": "https://arxiv.org/abs/2505.16196", "authors": ["Xuewu Lin", "Tianwei Lin", "Lichao Huang", "Hongyu Xie", "Yiwei Jin", "Keyu Li", "Zhizhong Su"], "title": "SEM: Enhancing Spatial Understanding for Robust Robot Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "A key challenge in robot manipulation lies in developing policy models with\nstrong spatial understanding, the ability to reason about 3D geometry, object\nrelations, and robot embodiment. Existing methods often fall short: 3D point\ncloud models lack semantic abstraction, while 2D image encoders struggle with\nspatial reasoning. To address this, we propose SEM (Spatial Enhanced\nManipulation model), a novel diffusion-based policy framework that explicitly\nenhances spatial understanding from two complementary perspectives. A spatial\nenhancer augments visual representations with 3D geometric context, while a\nrobot state encoder captures embodiment-aware structure through graphbased\nmodeling of joint dependencies. By integrating these modules, SEM significantly\nimproves spatial understanding, leading to robust and generalizable\nmanipulation across diverse tasks that outperform existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEM\u7684\u65b0\u578b\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u548c\u673a\u5668\u4eba\u72b6\u6001\u7f16\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u7406\u89e3\u548c\u8bed\u4e49\u62bd\u8c61\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c3D\u70b9\u4e91\u6a21\u578b\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\uff0c2D\u56fe\u50cf\u7f16\u7801\u5668\u96be\u4ee5\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u3002", "method": "SEM\u7ed3\u5408\u4e86\u7a7a\u95f4\u589e\u5f3a\u5668\u548c\u673a\u5668\u4eba\u72b6\u6001\u7f16\u7801\u5668\uff0c\u524d\u8005\u901a\u8fc73D\u51e0\u4f55\u4e0a\u4e0b\u6587\u589e\u5f3a\u89c6\u89c9\u8868\u793a\uff0c\u540e\u8005\u901a\u8fc7\u56fe\u5efa\u6a21\u6355\u6349\u5173\u8282\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "SEM\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SEM\u901a\u8fc7\u589e\u5f3a\u7a7a\u95f4\u7406\u89e3\u548c\u673a\u5668\u4eba\u72b6\u6001\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2505.16932", "pdf": "https://arxiv.org/pdf/2505.16932", "abs": "https://arxiv.org/abs/2505.16932", "authors": ["Noah Amsel", "David Persson", "Christopher Musco", "Robert Gower"], "title": "The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NA", "math.NA", "math.OC"], "comment": null, "summary": "Computing the polar decomposition and the related matrix sign function, has\nbeen a well-studied problem in numerical analysis for decades. More recently,\nit has emerged as an important subroutine in deep learning, particularly within\nthe Muon optimization framework. However, the requirements in this setting\ndiffer significantly from those of traditional numerical analysis. In deep\nlearning, methods must be highly efficient and GPU-compatible, but high\naccuracy is often unnecessary. As a result, classical algorithms like\nNewton-Schulz (which suffers from slow initial convergence) and methods based\non rational functions (which rely on QR decompositions or matrix inverses) are\npoorly suited to this context. In this work, we introduce Polar Express, a\nGPU-friendly algorithm for computing the polar decomposition. Like classical\npolynomial methods such as Newton-Schulz, our approach uses only matrix-matrix\nmultiplications, making it GPU-compatible. Motivated by earlier work of Chen &\nChow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule\nat each iteration by solving a minimax optimization problem, and we prove that\nit enjoys a strong worst-case optimality guarantee. This property ensures both\nrapid early convergence and fast asymptotic convergence. We also address\nfinite-precision issues, making it stable in bfloat16 in practice. We apply\nPolar Express within the Muon optimization framework and show consistent\nimprovements in validation loss on large-scale models such as GPT-2,\noutperforming recent alternatives across a range of learning rates.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPolar Express\u7684GPU\u53cb\u597d\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u6781\u5206\u89e3\u548c\u77e9\u9635\u7b26\u53f7\u51fd\u6570\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684Muon\u4f18\u5316\u6846\u67b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u7b97\u6cd5\u5728\u6548\u7387\u548c\u517c\u5bb9\u6027\u4e0a\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u5206\u6790\u4e2d\u7684\u6781\u5206\u89e3\u7b97\u6cd5\uff08\u5982Newton-Schulz\uff09\u5728\u6df1\u5ea6\u5b66\u4e60\u573a\u666f\u4e2d\u6548\u7387\u4e0d\u8db3\u4e14\u4e0d\u517c\u5bb9GPU\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u5bf9\u9ad8\u7cbe\u5ea6\u8981\u6c42\u8f83\u4f4e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14GPU\u517c\u5bb9\u7684\u65b0\u65b9\u6cd5\u3002", "method": "Polar Express\u901a\u8fc7\u89e3\u51b3\u6781\u5c0f\u6781\u5927\u4f18\u5316\u95ee\u9898\uff0c\u52a8\u6001\u8c03\u6574\u591a\u9879\u5f0f\u66f4\u65b0\u89c4\u5219\uff0c\u4ec5\u4f7f\u7528\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\uff0c\u786e\u4fddGPU\u517c\u5bb9\u6027\uff0c\u5e76\u5177\u6709\u5feb\u901f\u6536\u655b\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPolar Express\u5728GPT-2\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u9a8c\u8bc1\u635f\u5931\uff0c\u4e14\u5728\u4e0d\u540c\u5b66\u4e60\u7387\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Polar Express\u662f\u4e00\u79cd\u9ad8\u6548\u3001GPU\u517c\u5bb9\u4e14\u7a33\u5b9a\u7684\u6781\u5206\u89e3\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\uff0c\u5c24\u5176\u5728Muon\u6846\u67b6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2505.16933", "pdf": "https://arxiv.org/pdf/2505.16933", "abs": "https://arxiv.org/abs/2505.16933", "authors": ["Zebin You", "Shen Nie", "Xiaolu Zhang", "Jun Hu", "Jun Zhou", "Zhiwu Lu", "Ji-Rong Wen", "Chongxuan Li"], "title": "LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large\nLanguage Model (MLLM) that integrates visual instruction tuning with masked\ndiffusion models, representing a departure from the autoregressive paradigms\ndominant in current multimodal approaches. Built upon LLaDA, a representative\nlarge language diffusion model, LLaDA-V incorporates a vision encoder and MLP\nconnector that projects visual features into the language embedding space,\nenabling effective multimodal alignment. Our empirical investigation reveals\nseveral intriguing results: First, LLaDA-V demonstrates promising multimodal\nperformance despite its language model being weaker on purely textual tasks\nthan counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same\ninstruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal\ntasks with better data scalability. It also narrows the performance gap to\nQwen2-VL, suggesting the effectiveness of its architecture for multimodal\ntasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal\nunderstanding compared to existing hybrid autoregressive-diffusion and purely\ndiffusion-based MLLMs. Our findings suggest that large language diffusion\nmodels show promise in multimodal contexts and warrant further investigation in\nfuture research. Project page and codes:\nhttps://ml-gsai.github.io/LLaDA-V-demo/.", "AI": {"tldr": "LLaDA-V\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9\u6307\u4ee4\u8c03\u4f18\u548c\u63a9\u7801\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6df7\u5408\u81ea\u56de\u5f52-\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7a81\u7834\u5f53\u524d\u4ee5\u81ea\u56de\u5f52\u8303\u5f0f\u4e3a\u4e3b\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eLLaDA\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u7f16\u7801\u5668\u548cMLP\u8fde\u63a5\u5668\uff0c\u5c06\u89c6\u89c9\u7279\u5f81\u6295\u5f71\u5230\u8bed\u8a00\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u6216\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2505.16279", "pdf": "https://arxiv.org/pdf/2505.16279", "abs": "https://arxiv.org/abs/2505.16279", "authors": ["Junjie Zheng", "Zihao Chen", "Chaofan Ding", "Yunming Liang", "Yihan Fan", "Huan Yang", "Lei Xie", "Xinhan Di"], "title": "MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing", "categories": ["cs.MM", "cs.CV"], "comment": "5 pages, 4 figures, accepted by Interspeech 2025", "summary": "Current movie dubbing technology can produce the desired speech using a\nreference voice and input video, maintaining perfect synchronization with the\nvisuals while effectively conveying the intended emotions. However, crucial\naspects of movie dubbing, including adaptation to various dubbing styles,\neffective handling of dialogue, narration, and monologues, as well as\nconsideration of subtle details such as speaker age and gender, remain\ninsufficiently explored. To tackle these challenges, we introduce a multi-modal\ngenerative framework. First, it utilizes a multi-modal large vision-language\nmodel (VLM) to analyze visual inputs, enabling the recognition of dubbing types\nand fine-grained attributes. Second, it produces high-quality dubbing using\nlarge speech generation models, guided by multi-modal inputs. Additionally, a\nmovie dubbing dataset with annotations for dubbing types and subtle details is\nconstructed to enhance movie understanding and improve dubbing quality for the\nproposed multi-modal framework. Experimental results across multiple benchmark\ndatasets show superior performance compared to state-of-the-art (SOTA) methods.\nIn details, the LSE-D, SPK-SIM, EMO-SIM, and MCD exhibit improvements of up to\n1.09%, 8.80%, 19.08%, and 18.74%, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u5f71\u914d\u97f3\u4e2d\u7684\u98ce\u683c\u9002\u5e94\u3001\u5bf9\u8bdd\u5904\u7406\u548c\u7ec6\u8282\u8003\u8651\u95ee\u9898\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8bed\u97f3\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u914d\u97f3\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7535\u5f71\u914d\u97f3\u6280\u672f\u5728\u98ce\u683c\u9002\u5e94\u3001\u5bf9\u8bdd\u5904\u7406\u53ca\u7ec6\u8282\uff08\u5982\u8bf4\u8bdd\u8005\u5e74\u9f84\u548c\u6027\u522b\uff09\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5206\u6790\u89c6\u89c9\u8f93\u5165\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u97f3\u751f\u6210\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u914d\u97f3\uff0c\u5e76\u6784\u5efa\u5e26\u6807\u6ce8\u7684\u7535\u5f71\u914d\u97f3\u6570\u636e\u96c6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cLSE-D\u3001SPK-SIM\u3001EMO-SIM\u548cMCD\u6307\u6807\u5206\u522b\u63d0\u53471.09%\u30018.80%\u300119.08%\u548c18.74%\u3002", "conclusion": "\u591a\u6a21\u6001\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u7535\u5f71\u914d\u97f3\u7684\u8d28\u91cf\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16938", "pdf": "https://arxiv.org/pdf/2505.16938", "abs": "https://arxiv.org/abs/2505.16938", "authors": ["NovelSeek Team", "Bo Zhang", "Shiyang Feng", "Xiangchao Yan", "Jiakang Yuan", "Zhiyin Yu", "Xiaohan He", "Songtao Huang", "Shaowei Hou", "Zheng Nie", "Zhilong Wang", "Jinyao Liu", "Runmin Ma", "Tianshuo Peng", "Peng Ye", "Dongzhan Zhou", "Shufei Zhang", "Xiaosong Wang", "Yilan Zhang", "Meng Li", "Zhongying Tu", "Xiangyu Yue", "Wangli Ouyang", "Bowen Zhou", "Lei Bai"], "title": "NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "HomePage: https://alpha-innovator.github.io/NovelSeek-project-page", "summary": "Artificial Intelligence (AI) is accelerating the transformation of scientific\nresearch paradigms, not only enhancing research efficiency but also driving\ninnovation. We introduce NovelSeek, a unified closed-loop multi-agent framework\nto conduct Autonomous Scientific Research (ASR) across various scientific\nresearch fields, enabling researchers to tackle complicated problems in these\nfields with unprecedented speed and precision. NovelSeek highlights three key\nadvantages: 1) Scalability: NovelSeek has demonstrated its versatility across\n12 scientific research tasks, capable of generating innovative ideas to enhance\nthe performance of baseline code. 2) Interactivity: NovelSeek provides an\ninterface for human expert feedback and multi-agent interaction in automated\nend-to-end processes, allowing for the seamless integration of domain expert\nknowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in\nseveral scientific fields with significantly less time cost compared to human\nefforts. For instance, in reaction yield prediction, it increased from 27.6% to\n35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from\n0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,\nprecision advanced from 78.8% to 81.0% in a mere 30 hours.", "AI": {"tldr": "NovelSeek\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u79d1\u5b66\u9886\u57df\u8fdb\u884c\u81ea\u4e3b\u7814\u7a76\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u4ea4\u4e92\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u52a0\u901f\u79d1\u5b66\u7814\u7a76\u8303\u5f0f\u8f6c\u53d8\uff0c\u63d0\u5347\u7814\u7a76\u6548\u7387\u548c\u521b\u65b0\u6027\u3002", "method": "\u91c7\u7528\u95ed\u73af\u591a\u667a\u80fd\u4f53\u6846\u67b6NovelSeek\uff0c\u652f\u6301\u8de8\u9886\u57df\u4efb\u52a1\u5904\u7406\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u79d1\u5b66\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982\u53cd\u5e94\u4ea7\u7387\u9884\u6d4b\u4ece27.6%\u63d0\u5347\u81f335.4%\uff0c\u589e\u5f3a\u5b50\u6d3b\u6027\u9884\u6d4b\u51c6\u786e\u7387\u4ece0.52\u63d0\u5347\u81f30.79\u3002", "conclusion": "NovelSeek\u5c55\u793a\u4e86\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u521b\u65b0\u548c\u4ea4\u4e92\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16373", "pdf": "https://arxiv.org/pdf/2505.16373", "abs": "https://arxiv.org/abs/2505.16373", "authors": ["Ge Meng", "Zhongnan Cai", "Jingyan Tu", "Yingying Wang", "Chenxin Li", "Yue Huang", "Xinghao Ding"], "title": "PCMamba: Physics-Informed Cross-Modal State Space Model for Dual-Camera Compressive Hyperspectral Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Panchromatic (PAN) -assisted Dual-Camera Compressive Hyperspectral Imaging\n(DCCHI) is a key technology in snapshot hyperspectral imaging. Existing\nresearch primarily focuses on exploring spectral information from 2D\ncompressive measurements and spatial information from PAN images in an explicit\nmanner, leading to a bottleneck in HSI reconstruction. Various physical\nfactors, such as temperature, emissivity, and multiple reflections between\nobjects, play a critical role in the process of a sensor acquiring\nhyperspectral thermal signals. Inspired by this, we attempt to investigate the\ninterrelationships between physical properties to provide deeper theoretical\ninsights for HSI reconstruction. In this paper, we propose a Physics-Informed\nCross-Modal State Space Model Network (PCMamba) for DCCHI, which incorporates\nthe forward physical imaging process of HSI into the linear complexity of Mamba\nto facilitate lightweight and high-quality HSI reconstruction. Specifically, we\nanalyze the imaging process of hyperspectral thermal signals to enable the\nnetwork to disentangle the three key physical properties-temperature,\nemissivity, and texture. By fully exploiting the potential information embedded\nin 2D measurements and PAN images, the HSIs are reconstructed through a\nphysics-driven synthesis process. Furthermore, we design a Cross-Modal Scanning\nMamba Block (CSMB) that introduces inter-modal pixel-wise interaction with\npositional inductive bias by cross-scanning the backbone features and PAN\nfeatures. Extensive experiments conducted on both real and simulated datasets\ndemonstrate that our method significantly outperforms SOTA methods in both\nquantitative and qualitative metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7f51\u7edc\uff08PCMamba\uff09\uff0c\u7528\u4e8e\u53cc\u76f8\u673a\u538b\u7f29\u9ad8\u5149\u8c31\u6210\u50cf\uff08DCCHI\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u6210\u50cf\u8fc7\u7a0b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u8d28\u91cf\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4ece2D\u538b\u7f29\u6d4b\u91cf\u548cPAN\u56fe\u50cf\u4e2d\u663e\u5f0f\u63d0\u53d6\u5149\u8c31\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u5bfc\u81f4\u9ad8\u5149\u8c31\u91cd\u5efa\u5b58\u5728\u74f6\u9888\u3002\u8bba\u6587\u5c1d\u8bd5\u901a\u8fc7\u5206\u6790\u7269\u7406\u7279\u6027\uff08\u5982\u6e29\u5ea6\u3001\u53d1\u5c04\u7387\u548c\u7269\u4f53\u95f4\u7684\u591a\u6b21\u53cd\u5c04\uff09\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u4e3a\u91cd\u5efa\u63d0\u4f9b\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51fa\u4e86PCMamba\u6a21\u578b\uff0c\u5c06\u9ad8\u5149\u8c31\u70ed\u4fe1\u53f7\u7684\u7269\u7406\u6210\u50cf\u8fc7\u7a0b\u878d\u5165Mamba\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u4e2d\uff0c\u8bbe\u8ba1\u4e86\u8de8\u6a21\u6001\u626b\u63cfMamba\u5757\uff08CSMB\uff09\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u50cf\u7d20\u7ea7\u4ea4\u4e92\u548c\u4f4d\u7f6e\u5f52\u7eb3\u504f\u7f6e\uff0c\u5145\u5206\u5229\u75282D\u6d4b\u91cf\u548cPAN\u56fe\u50cf\u7684\u6f5c\u5728\u4fe1\u606f\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "PCMamba\u901a\u8fc7\u7269\u7406\u9a71\u52a8\u7684\u5408\u6210\u8fc7\u7a0b\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u91cd\u5efa\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.16944", "pdf": "https://arxiv.org/pdf/2505.16944", "abs": "https://arxiv.org/abs/2505.16944", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Amy Xin", "Youfeng Liu", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated advanced capabilities in\nreal-world agentic applications. Growing research efforts aim to develop\nLLM-based agents to address practical demands, introducing a new challenge:\nagentic scenarios often involve lengthy instructions with complex constraints,\nsuch as extended system prompts and detailed tool specifications. While\nadherence to such instructions is crucial for agentic applications, whether\nLLMs can reliably follow them remains underexplored. In this paper, we\nintroduce AgentIF, the first benchmark for systematically evaluating LLM\ninstruction following ability in agentic scenarios. AgentIF features three key\ncharacteristics: (1) Realistic, constructed from 50 real-world agentic\napplications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.\n(3) Complex, averaging 11.9 constraints per instruction, covering diverse\nconstraint types, such as tool specifications and condition constraints. To\nconstruct AgentIF, we collect 707 human-annotated instructions across 50\nagentic tasks from industrial application agents and open-source agentic\nsystems. For each instruction, we annotate the associated constraints and\ncorresponding evaluation metrics, including code-based evaluation, LLM-based\nevaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically\nevaluate existing advanced LLMs. We observe that current models generally\nperform poorly, especially in handling complex constraint structures and tool\nspecifications. We further conduct error analysis and analytical experiments on\ninstruction length and meta constraints, providing some findings about the\nfailure modes of existing LLMs. We have released the code and data to\nfacilitate future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86AgentIF\uff0c\u9996\u4e2a\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4ee3\u7406\u573a\u666f\u4e2d\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7ea6\u675f\u548c\u5de5\u5177\u89c4\u8303\u65f6\u3002", "motivation": "\u4ee3\u7406\u573a\u666f\u4e2d\u6d89\u53ca\u957f\u4e14\u590d\u6742\u7684\u6307\u4ee4\uff0c\u4f46LLM\u662f\u5426\u80fd\u53ef\u9760\u9075\u5faa\u8fd9\u4e9b\u6307\u4ee4\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u6784\u5efaAgentIF\u57fa\u51c6\uff0c\u5305\u542b50\u4e2a\u771f\u5b9e\u4ee3\u7406\u5e94\u7528\u7684707\u6761\u4eba\u5de5\u6807\u6ce8\u6307\u4ee4\uff0c\u6db5\u76d6\u957f\u4e14\u590d\u6742\u7684\u7ea6\u675f\u3002", "result": "\u5f53\u524dLLM\u5728AgentIF\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7ea6\u675f\u548c\u5de5\u5177\u89c4\u8303\u65b9\u9762\u3002", "conclusion": "AgentIF\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLM\u5728\u4ee3\u7406\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.16394", "pdf": "https://arxiv.org/pdf/2505.16394", "abs": "https://arxiv.org/abs/2505.16394", "authors": ["Zhenjie Yang", "Xiaosong Jia", "Qifeng Li", "Xue Yang", "Maoqing Yao", "Junchi Yan"], "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) can mitigate the causal confusion and\ndistribution shift inherent to imitation learning (IL). However, applying RL to\nend-to-end autonomous driving (E2E-AD) remains an open problem for its training\ndifficulty, and IL is still the mainstream paradigm in both academia and\nindustry. Recently Model-based Reinforcement Learning (MBRL) have demonstrated\npromising results in neural planning; however, these methods typically require\nprivileged information as input rather than raw sensor data. We fill this gap\nby designing Raw2Drive, a dual-stream MBRL approach. Initially, we efficiently\ntrain an auxiliary privileged world model paired with a neural planner that\nuses privileged information as input. Subsequently, we introduce a raw sensor\nworld model trained via our proposed Guidance Mechanism, which ensures\nconsistency between the raw sensor world model and the privileged world model\nduring rollouts. Finally, the raw sensor world model combines the prior\nknowledge embedded in the heads of the privileged world model to effectively\nguide the training of the raw sensor policy. Raw2Drive is so far the only RL\nbased end-to-end method on CARLA Leaderboard 2.0, and Bench2Drive and it\nachieves state-of-the-art performance.", "AI": {"tldr": "Raw2Drive\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08MBRL\uff09\u7684\u53cc\u6d41\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff08E2E-AD\uff09\u4e2d\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5f15\u5bfc\u673a\u5236\u786e\u4fdd\u539f\u59cb\u4f20\u611f\u5668\u4e16\u754c\u6a21\u578b\u4e0e\u7279\u6743\u4e16\u754c\u6a21\u578b\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4ecd\u4e3a\u4e3b\u6d41\uff0c\u4f46\u5b58\u5728\u56e0\u679c\u6df7\u6dc6\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u867d\u80fd\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5176\u8bad\u7ec3\u96be\u5ea6\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1Raw2Drive\uff0c\u5305\u62ec\u7279\u6743\u4e16\u754c\u6a21\u578b\u548c\u539f\u59cb\u4f20\u611f\u5668\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5bfc\u673a\u5236\u786e\u4fdd\u4e24\u8005\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u7279\u6743\u4e16\u754c\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u6307\u5bfc\u539f\u59cb\u4f20\u611f\u5668\u7b56\u7565\u7684\u8bad\u7ec3\u3002", "result": "Raw2Drive\u662fCARLA Leaderboard 2.0\u548cBench2Drive\u4e0a\u552f\u4e00\u57fa\u4e8eRL\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Raw2Drive\u586b\u8865\u4e86MBRL\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86RL\u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.16967", "pdf": "https://arxiv.org/pdf/2505.16967", "abs": "https://arxiv.org/abs/2505.16967", "authors": ["Nandan Thakur", "Crystina Zhang", "Xueguang Ma", "Jimmy Lin"], "title": "Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/castorini/rlhn & datasets are\n  available at https://huggingface.co/rlhn", "summary": "Training robust retrieval and reranker models typically relies on large-scale\nretrieval datasets; for example, the BGE collection contains 1.6 million\nquery-passage pairs sourced from various data sources. However, we find that\ncertain datasets can negatively impact model effectiveness -- pruning 8 out of\n15 datasets from the BGE collection reduces the training set size by\n2.35$\\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a\ndeeper examination of training data quality, with a particular focus on \"false\nnegatives\", where relevant passages are incorrectly labeled as irrelevant. We\npropose a simple, cost-effective approach using cascading LLM prompts to\nidentify and relabel hard negatives. Experimental results show that relabeling\nfalse negatives with true positives improves both E5 (base) and Qwen2.5-7B\nretrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot\nAIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on\nthe relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the\ncascading design is further supported by human annotation results, where we\nfind judgment by GPT-4o shows much higher agreement with humans than\nGPT-4o-mini.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u635f\u5bb3\u68c0\u7d22\u6a21\u578b\u6548\u679c\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7b80\u5355\u65b9\u6cd5\u91cd\u65b0\u6807\u6ce8\u5047\u9634\u6027\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u67d0\u4e9b\u6570\u636e\u96c6\u53ef\u80fd\u5bf9\u6a21\u578b\u6548\u679c\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5047\u9634\u6027\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u7ea7\u8054LLM\u63d0\u793a\u8bc6\u522b\u5e76\u91cd\u65b0\u6807\u6ce8\u5047\u9634\u6027\u6837\u672c\u3002", "result": "\u91cd\u65b0\u6807\u6ce8\u540e\uff0cE5\u548cQwen2.5-7B\u6a21\u578b\u5728BEIR\u548cAIR-Bench\u4e0a\u6027\u80fd\u63d0\u53470.7-1.8 nDCG@10\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u6807\u6ce8\u8d28\u91cf\uff0c\u53ef\u663e\u8457\u63d0\u5347\u68c0\u7d22\u548c\u91cd\u6392\u6a21\u578b\u6027\u80fd\uff0c\u7ea7\u8054LLM\u65b9\u6cd5\u9ad8\u6548\u4e14\u53ef\u9760\u3002"}}
{"id": "2505.16487", "pdf": "https://arxiv.org/pdf/2505.16487", "abs": "https://arxiv.org/abs/2505.16487", "authors": ["Junqing Chen", "Haibo Liu"], "title": "Implicit Neural Shape Optimization for 3D High-Contrast Electrical Impedance Tomography", "categories": ["math.NA", "cs.CV", "cs.NA"], "comment": null, "summary": "We present a novel implicit neural shape optimization framework for 3D\nhigh-contrast Electrical Impedance Tomography (EIT), addressing scenarios where\nconductivity exhibits sharp discontinuities across material interfaces. These\nhigh-contrast cases, prevalent in metallic implant monitoring and industrial\ndefect detection, challenge traditional reconstruction methods due to severe\nill-posedness. Our approach synergizes shape optimization with implicit neural\nrepresentations, introducing key innovations including a shape derivative-based\noptimization scheme that explicitly incorporates high-contrast interface\nconditions and an efficient latent space representation that reduces variable\ndimensionality. Through rigorous theoretical analysis of algorithm convergence\nand extensive numerical experiments, we demonstrate substantial performance\nimprovements, establishing our framework as promising for practical\napplications in medical imaging with metallic implants and industrial\nnon-destructive testing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u5f62\u72b6\u4f18\u5316\u76843D\u9ad8\u5bf9\u6bd4\u5ea6\u7535\u963b\u6297\u65ad\u5c42\u626b\u63cf\uff08EIT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6750\u6599\u754c\u9762\u5904\u7535\u5bfc\u7387\u6025\u5267\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u9ad8\u5bf9\u6bd4\u5ea6\u573a\u666f\uff08\u5982\u91d1\u5c5e\u690d\u5165\u7269\u76d1\u6d4b\u548c\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\uff09\u5bf9\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u56e0\u5176\u4e25\u91cd\u7684\u75c5\u6001\u6027\u3002", "method": "\u7ed3\u5408\u5f62\u72b6\u4f18\u5316\u4e0e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u63d0\u51fa\u5f62\u72b6\u5bfc\u6570\u4f18\u5316\u65b9\u6848\u548c\u9ad8\u6548\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u533b\u7597\u6210\u50cf\u548c\u5de5\u4e1a\u65e0\u635f\u68c0\u6d4b\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.16968", "pdf": "https://arxiv.org/pdf/2505.16968", "abs": "https://arxiv.org/abs/2505.16968", "authors": ["Ahmed Heakl", "Sarim Hashmi", "Gustavo Bertolo Stahl", "Seung Hun Eddie Han", "Salman Khan", "Abdulrahman Mahmoud"], "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark", "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.LG", "cs.PL"], "comment": "20 pages, 11 figures, 5 tables", "summary": "We introduce \\texttt{CASS}, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level\n(CUDA~$\\leftrightarrow$~HIP) and assembly-level (Nvidia\nSASS~$\\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k\nverified code pairs across host and device, addressing a critical gap in\nlow-level GPU code portability. Leveraging this resource, we train the\n\\texttt{CASS} family of domain-specific language models, achieving 95\\% source\ntranslation accuracy and 37.5\\% assembly translation accuracy, substantially\noutperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our\ngenerated code matches native performance in over 85\\% of test cases,\npreserving runtime and memory behavior. To support rigorous evaluation, we\nintroduce \\texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with\nground-truth execution. All data, models, and evaluation tools are released as\nopen source to foster progress in GPU compiler tooling, binary compatibility,\nand LLM-guided hardware translation. Dataset and benchmark are on\n\\href{https://huggingface.co/datasets/MBZUAI/cass}{\\textcolor{blue}{HuggingFace}},\nwith code at\n\\href{https://github.com/GustavoStahl/CASS}{\\textcolor{blue}{GitHub}}.", "AI": {"tldr": "CASS\u662f\u4e00\u4e2a\u9488\u5bf9\u8de8\u67b6\u6784GPU\u4ee3\u7801\u8f6c\u6362\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6a21\u578b\u5957\u4ef6\uff0c\u652f\u6301\u6e90\u7ea7\u548c\u6c47\u7f16\u7ea7\u7ffb\u8bd1\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5546\u4e1a\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u4f4e\u7ea7\u522bGPU\u4ee3\u7801\u53ef\u79fb\u690d\u6027\u7684\u5173\u952e\u7f3a\u53e3\u3002", "method": "\u5229\u752870k\u5df2\u9a8c\u8bc1\u4ee3\u7801\u5bf9\u8bad\u7ec3\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5f15\u5165CASS-Bench\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u6e90\u7ffb\u8bd1\u51c6\u786e\u738795%\uff0c\u6c47\u7f16\u7ffb\u8bd1\u51c6\u786e\u738737.5%\uff0c\u751f\u6210\u4ee3\u7801\u572885%\u6d4b\u8bd5\u6848\u4f8b\u4e2d\u5339\u914d\u539f\u751f\u6027\u80fd\u3002", "conclusion": "CASS\u4e3aGPU\u7f16\u8bd1\u5668\u5de5\u5177\u548c\u786c\u4ef6\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u5f00\u6e90\u8d44\u6e90\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2505.16517", "pdf": "https://arxiv.org/pdf/2505.16517", "abs": "https://arxiv.org/abs/2505.16517", "authors": ["Zirui Song", "Guangxian Ouyang", "Mingzhe Li", "Yuheng Ji", "Chenxi Wang", "Zixiang Xu", "Zeyu Zhang", "Xiaoqing Zhang", "Qian Jiang", "Zhenhao Chen", "Zhongzhi Li", "Rui Yan", "Xiuying Chen"], "title": "ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models", "categories": ["cs.RO", "cs.CV"], "comment": "13pages", "summary": "Large Vision-Language Models (LVLMs) have recently advanced robotic\nmanipulation by leveraging vision for scene perception and language for\ninstruction following. However, existing methods rely heavily on costly\nhuman-annotated training datasets, which limits their generalization and causes\nthem to struggle in out-of-domain (OOD) scenarios, reducing real-world\nadaptability. To address these challenges, we propose ManipLVM-R1, a novel\nreinforcement learning framework that replaces traditional supervision with\nReinforcement Learning using Verifiable Rewards (RLVR). By directly optimizing\nfor task-aligned outcomes, our method enhances generalization and physical\nreasoning while removing the dependence on costly annotations. Specifically, we\ndesign two rule-based reward functions targeting key robotic manipulation\nsubtasks: an Affordance Perception Reward to enhance localization of\ninteraction regions, and a Trajectory Match Reward to ensure the physical\nplausibility of action paths. These rewards provide immediate feedback and\nimpose spatial-logical constraints, encouraging the model to go beyond shallow\npattern matching and instead learn deeper, more systematic reasoning about\nphysical interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aManipLVM-R1\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u66ff\u4ee3\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u96be\u4ee5\u9002\u5e94\u57df\u5916\uff08OOD\uff09\u573a\u666f\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u51fd\u6570\uff1aAffordance Perception Reward\uff08\u589e\u5f3a\u4ea4\u4e92\u533a\u57df\u5b9a\u4f4d\uff09\u548cTrajectory Match Reward\uff08\u786e\u4fdd\u52a8\u4f5c\u8def\u5f84\u7684\u7269\u7406\u5408\u7406\u6027\uff09\u3002", "result": "\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u4efb\u52a1\u5bf9\u9f50\u7ed3\u679c\uff0c\u51cf\u5c11\u4e86\u6807\u6ce8\u4f9d\u8d56\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u7269\u7406\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "ManipLVM-R1\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u53ef\u9a8c\u8bc1\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9002\u5e94\u6027\u548c\u7cfb\u7edf\u6027\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.16975", "pdf": "https://arxiv.org/pdf/2505.16975", "abs": "https://arxiv.org/abs/2505.16975", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong capability in diverse software\nengineering tasks, e.g. code completion, bug fixing, and document generation.\nHowever, feature-driven development (FDD), a highly prevalent real-world task\nthat involves developing new functionalities for large, existing codebases,\nremains underexplored. We therefore introduce SWE-Dev, the first large-scale\ndataset (with 14,000 training and 500 test samples) designed to evaluate and\ntrain autonomous coding systems on real-world feature development tasks. To\nensure verifiable and diverse training, SWE-Dev uniquely provides all instances\nwith a runnable environment and its developer-authored executable unit tests.\nThis collection not only provides high-quality data for Supervised Fine-Tuning\n(SFT), but also enables Reinforcement Learning (RL) by delivering accurate\nreward signals from executable unit tests. Our extensive evaluations on\nSWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent\nSystems (MAS), reveal that FDD is a profoundly challenging frontier for current\nAI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test\nsplit). Crucially, we demonstrate that SWE-Dev serves as an effective platform\nfor model improvement: fine-tuning on training set enabled a 7B model\ncomparable to GPT-4o on \\textit{hard} split, underscoring the value of its\nhigh-quality training data. Code is available here\n\\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SWE-Dev\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3AI\u5728\u73b0\u5b9e\u4e16\u754c\u529f\u80fd\u5f00\u53d1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5f53\u524dAI\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u6311\u6218\u6027\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u529f\u80fd\u9a71\u52a8\u5f00\u53d1\uff08FDD\uff09\u4efb\u52a1\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faSWE-Dev\u6570\u636e\u96c6\uff0c\u5305\u542b14,000\u4e2a\u8bad\u7ec3\u6837\u672c\u548c500\u4e2a\u6d4b\u8bd5\u6837\u672c\uff0c\u63d0\u4f9b\u53ef\u8fd0\u884c\u73af\u5883\u548c\u5355\u5143\u6d4b\u8bd5\uff0c\u652f\u6301\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u8bc4\u4f30\u663e\u793aFDD\u5bf9\u5f53\u524dAI\u6781\u5177\u6311\u6218\u6027\uff08\u5982Claude-3.7-Sonnet\u4ec522.45% Pass@3\uff09\uff0c\u4f46\u5fae\u8c03\u540e7B\u6a21\u578b\u6027\u80fd\u63a5\u8fd1GPT-4o\u3002", "conclusion": "SWE-Dev\u662f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u5e73\u53f0\uff0c\u5176\u9ad8\u8d28\u91cf\u6570\u636e\u5bf9AI\u5728FDD\u4efb\u52a1\u4e2d\u7684\u8fdb\u6b65\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2505.16579", "pdf": "https://arxiv.org/pdf/2505.16579", "abs": "https://arxiv.org/abs/2505.16579", "authors": ["Siqu Ou", "Hongcheng Liu", "Pingjie Wang", "Yusheng Liao", "Chuan Xuan", "Yanfeng Wang", "Yu Wang"], "title": "Bridging the Dynamic Perception Gap: Training-Free Draft Chain-of-Thought for Dynamic Multimodal Spatial Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": "19 pages, 8 figures", "summary": "While chains-of-thought (CoT) have advanced complex reasoning in multimodal\nlarge language models (MLLMs), existing methods remain confined to text or\nstatic visual domains, often faltering in dynamic spatial reasoning tasks. To\nbridge this gap, we present GRASSLAND, a novel maze navigation benchmark\ndesigned to evaluate dynamic spatial reasoning. Our experiments show that\naugmenting textual reasoning chains with dynamic visual drafts, overlaid on\ninput images, significantly outperforms conventional approaches, offering new\ninsights into spatial reasoning in evolving environments. To generalize this\ncapability, we propose D2R (Dynamic Draft-Augmented Reasoning), a training-free\nframework that seamlessly integrates textual CoT with corresponding visual\ndrafts into MLLMs. Extensive evaluations demonstrate that D2R consistently\nenhances performance across diverse tasks, establishing a robust baseline for\ndynamic spatial reasoning without requiring model fine-tuning. Project is open\nat https://github.com/Cratileo/D2R.", "AI": {"tldr": "GRASSLAND\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8ff7\u5bab\u5bfc\u822a\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002D2R\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u63a8\u7406\u94fe\u548c\u52a8\u6001\u89c6\u89c9\u8349\u7a3f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c40\u9650\u4e8e\u6587\u672c\u6216\u9759\u6001\u89c6\u89c9\u9886\u57df\u3002", "method": "\u63d0\u51faD2R\u6846\u67b6\uff0c\u5c06\u6587\u672c\u63a8\u7406\u94fe\u4e0e\u52a8\u6001\u89c6\u89c9\u8349\u7a3f\u7ed3\u5408\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "result": "D2R\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u7a33\u5065\u57fa\u51c6\u3002", "conclusion": "D2R\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16984", "pdf": "https://arxiv.org/pdf/2505.16984", "abs": "https://arxiv.org/abs/2505.16984", "authors": ["Mingyang Liu", "Gabriele Farina", "Asuman Ozdaglar"], "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5fae\u8c03\uff08UFT\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08SFT\u548cRFT\uff09\u5404\u6709\u5c40\u9650\u6027\uff1aSFT\u53ef\u80fd\u8fc7\u62df\u5408\u4e14\u9650\u5236\u5927\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0cRFT\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u5f3a\u5ea6\u4e14\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51faUFT\uff0c\u5c06SFT\u548cRFT\u7edf\u4e00\u4e3a\u4e00\u4e2a\u96c6\u6210\u8fc7\u7a0b\uff0c\u7ed3\u5408\u76d1\u7763\u4fe1\u53f7\u548c\u63a2\u7d22\u80fd\u529b\u3002", "result": "UFT\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u5747\u4f18\u4e8eSFT\u548cRFT\uff0c\u5e76\u7406\u8bba\u4e0a\u8bc1\u660e\u5176\u7a81\u7834\u4e86RFT\u7684\u6307\u6570\u6837\u672c\u590d\u6742\u5ea6\u74f6\u9888\u3002", "conclusion": "UFT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.16725", "pdf": "https://arxiv.org/pdf/2505.16725", "abs": "https://arxiv.org/abs/2505.16725", "authors": ["Phillip Mueller", "Jannik Wiese", "Sebastian Mueller", "Lars Mikelsons"], "title": "Masked Conditioning for Deep Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Datasets in engineering domains are often small, sparsely labeled, and\ncontain numerical as well as categorical conditions. Additionally.\ncomputational resources are typically limited in practical applications which\nhinders the adoption of generative models for engineering tasks. We introduce a\nnovel masked-conditioning approach, that enables generative models to work with\nsparse, mixed-type data. We mask conditions during training to simulate sparse\nconditions at inference time. For this purpose, we explore the use of various\nsparsity schedules that show different strengths and weaknesses. In addition,\nwe introduce a flexible embedding that deals with categorical as well as\nnumerical conditions. We integrate our method into an efficient variational\nautoencoder as well as a latent diffusion model and demonstrate the\napplicability of our approach on two engineering-related datasets of 2D point\nclouds and images. Finally, we show that small models trained on limited data\ncan be coupled with large pretrained foundation models to improve generation\nquality while retaining the controllability induced by our conditioning scheme.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u6761\u4ef6\u65b9\u6cd5\uff0c\u4f7f\u751f\u6210\u6a21\u578b\u80fd\u591f\u5904\u7406\u7a00\u758f\u3001\u6df7\u5408\u7c7b\u578b\u6570\u636e\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u5de5\u7a0b\u4efb\u52a1\u3002", "motivation": "\u5de5\u7a0b\u9886\u57df\u7684\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u5c0f\u3001\u6807\u7b7e\u7a00\u758f\u4e14\u5305\u542b\u6570\u503c\u548c\u5206\u7c7b\u6761\u4ef6\uff0c\u540c\u65f6\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528\u63a9\u7801\u6761\u4ef6\u8bad\u7ec3\u6a21\u62df\u7a00\u758f\u6761\u4ef6\uff0c\u63a2\u7d22\u591a\u79cd\u7a00\u758f\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u7075\u6d3b\u7684\u5d4c\u5165\u65b9\u6cd5\u5904\u7406\u5206\u7c7b\u548c\u6570\u503c\u6761\u4ef6\u3002", "result": "\u65b9\u6cd5\u57282D\u70b9\u4e91\u548c\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u5c55\u793a\u5c0f\u6a21\u578b\u4e0e\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u7ed3\u5408\u53ef\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6761\u4ef6\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2505.16994", "pdf": "https://arxiv.org/pdf/2505.16994", "abs": "https://arxiv.org/abs/2505.16994", "authors": ["Runyang You", "Yongqi Li", "Xinyu Lin", "Xin Zhang", "Wenjie Wang", "Wenjie Li", "Liqiang Nie"], "title": "$\\text{R}^2\\text{ec}$: Towards Large Recommender Models with Reasoning", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5927\u578b\u63a8\u8350\u6a21\u578b\uff08\\name\uff09\uff0c\u5177\u5907\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u67b6\u6784\u548c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08RecPO\uff09\u540c\u65f6\u4f18\u5316\u63a8\u7406\u548c\u63a8\u8350\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c06LLMs\u4f5c\u4e3a\u5916\u90e8\u63a8\u7406\u6a21\u5757\uff0c\u5bfc\u81f4\u8d44\u6e90\u6210\u672c\u9ad8\u4e14\u8054\u5408\u4f18\u5316\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\\name\u6a21\u578b\uff0c\u91cd\u65b0\u8bbe\u8ba1\u67b6\u6784\u4ee5\u5b9e\u73b0\u63a8\u7406\u548c\u63a8\u8350\u7684\u4ea4\u9519\uff0c\u5e76\u5f00\u53d1RecPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5956\u52b1\u65b9\u6848\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\\name\u5728Hit@5\u548cNDCG@20\u6307\u6807\u4e0a\u5206\u522b\u76f8\u5bf9\u63d0\u5347\u4e8668.67%\u548c45.21%\u3002", "conclusion": "\\name\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u548cRecPO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u7cfb\u7edf\u7684\u63a8\u7406\u548c\u63a8\u8350\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u4f9d\u8d56\u4e13\u95e8\u7684\u63a8\u7406\u6807\u6ce8\u3002"}}
{"id": "2505.16997", "pdf": "https://arxiv.org/pdf/2505.16997", "abs": "https://arxiv.org/abs/2505.16997", "authors": ["Rui Ye", "Xiangrui Liu", "Qimin Wu", "Xianghe Pang", "Zhenfei Yin", "Lei Bai", "Siheng Chen"], "title": "X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "19 pages, 5 figures", "summary": "LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by\nenabling cooperation among multiple specialized agents. However, most existing\nMAS frameworks rely on a single LLM to drive all agents, constraining the\nsystem's intelligence to the limit of that model. This paper explores the\nparadigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by\ndiverse LLMs, elevating the system's potential to the collective intelligence\nof diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to\nevaluate the performance of various LLMs across different domains and\nMAS-related functions. As an extensive empirical study, we assess 27 LLMs\nacross 5 domains (encompassing 21 test sets) and 5 functions, conducting over\n1.7 million evaluations to identify optimal model selections for each\ndomain-function combination. Building on these findings, we demonstrate that\ntransitioning from homogeneous to heterogeneous LLM-driven MAS can\nsignificantly enhance system performance without requiring structural redesign.\nSpecifically, in a chatbot-only MAS scenario, the heterogeneous configuration\nyields up to 8.4\\% performance improvement on the MATH dataset. In a mixed\nchatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable\n47\\% performance boost on the AIME dataset. Our results underscore the\ntransformative potential of heterogeneous LLMs in MAS, highlighting a promising\navenue for advancing scalable, collaborative AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5f02\u6784LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08X-MAS\uff09\uff0c\u901a\u8fc7\u591a\u6837\u5316LLM\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u5f15\u5165X-MAS-Bench\u6d4b\u8bd5\u5e73\u53f0\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709MAS\u6846\u67b6\u4f9d\u8d56\u5355\u4e00LLM\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u667a\u80fd\u3002\u63a2\u7d22\u5f02\u6784LLM\u9a71\u52a8\u7684MAS\u4ee5\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faX-MAS-Bench\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8bc4\u4f3027\u79cdLLM\u57285\u4e2a\u9886\u57df\u548c5\u79cd\u529f\u80fd\u4e0a\u7684\u8868\u73b0\uff0c\u8fdb\u884c170\u4e07\u6b21\u8bc4\u4f30\u3002", "result": "\u5f02\u6784LLM\u9a71\u52a8\u7684MAS\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982\u804a\u5929\u673a\u5668\u4eba\u573a\u666f\u63d0\u53478.4%\uff0c\u6df7\u5408\u573a\u666f\u63d0\u534747%\u3002", "conclusion": "\u5f02\u6784LLM\u5728MAS\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u4e3a\u53ef\u6269\u5c55\u534f\u4f5cAI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.16854", "pdf": "https://arxiv.org/pdf/2505.16854", "abs": "https://arxiv.org/abs/2505.16854", "authors": ["Jiaqi Wang", "Kevin Qinghong Lin", "James Cheng", "Mike Zheng Shou"], "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has proven to be an effective post-training\nstrategy for enhancing reasoning in vision-language models (VLMs). Group\nRelative Policy Optimization (GRPO) is a recent prominent method that\nencourages models to generate complete reasoning traces before answering,\nleading to increased token usage and computational cost. Inspired by the\nhuman-like thinking process-where people skip reasoning for easy questions but\nthink carefully when needed-we explore how to enable VLMs to first decide when\nreasoning is necessary. To realize this, we propose TON, a two-stage training\nstrategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective\n'thought dropout' operation, where reasoning traces are randomly replaced with\nempty thoughts. This introduces a think-or-not format that serves as a cold\nstart for selective reasoning; (ii) a GRPO stage that enables the model to\nfreely explore when to think or not, while maximizing task-aware outcome\nrewards. Experimental results show that TON can reduce the completion length by\nup to 90% compared to vanilla GRPO, without sacrificing performance or even\nimproving it. Further evaluations across diverse vision-language tasks-covering\na range of reasoning difficulties under both 3B and 7B models-consistently\nreveal that the model progressively learns to bypass unnecessary reasoning\nsteps as training advances. These findings shed light on the path toward\nhuman-like reasoning patterns in reinforcement learning approaches. Our code is\navailable at https://github.com/kokolerk/TON.", "AI": {"tldr": "TON\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u63a8\u7406\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u9009\u62e9\u6027\u601d\u8003\u542f\u53d1\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u9009\u62e9\u6027\u63a8\u7406\u4ee5\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faTON\uff1a1\uff09\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u2018thought dropout\u2019\u64cd\u4f5c\uff1b2\uff09GRPO\u9636\u6bb5\u8ba9\u6a21\u578b\u81ea\u7531\u63a2\u7d22\u4f55\u65f6\u601d\u8003\u3002", "result": "TON\u76f8\u6bd4GRPO\u51cf\u5c1190%\u7684\u63a8\u7406\u957f\u5ea6\uff0c\u6027\u80fd\u4e0d\u964d\u53cd\u5347\u3002", "conclusion": "TON\u4e3a\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u7c7b\u4eba\u63a8\u7406\u6a21\u5f0f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.16916", "pdf": "https://arxiv.org/pdf/2505.16916", "abs": "https://arxiv.org/abs/2505.16916", "authors": ["Xuankun Rong", "Wenke Huang", "Jian Liang", "Jinhe Bi", "Xun Xiao", "Yiming Li", "Bo Du", "Mang Ye"], "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nfine-tuning-as-a-service (FTaaS) settings, where user-submitted datasets adapt\ngeneral-purpose models to downstream tasks. This flexibility, however,\nintroduces serious security risks, as malicious fine-tuning can implant\nbackdoors into MLLMs with minimal effort. In this paper, we observe that\nbackdoor triggers systematically disrupt cross-modal processing by causing\nabnormal attention concentration on non-semantic regions--a phenomenon we term\nattention collapse. Based on this insight, we propose Believe Your Eyes (BYE),\na data filtering framework that leverages attention entropy patterns as\nself-supervised signals to identify and filter backdoor samples. BYE operates\nvia a three-stage pipeline: (1) extracting attention maps using the fine-tuned\nmodel, (2) computing entropy scores and profiling sensitive layers via bimodal\nseparation, and (3) performing unsupervised clustering to remove suspicious\nsamples. Unlike prior defenses, BYE equires no clean supervision, auxiliary\nlabels, or model modifications. Extensive experiments across various datasets,\nmodels, and diverse trigger types validate BYE's effectiveness: it achieves\nnear-zero attack success rates while maintaining clean-task performance,\noffering a robust and generalizable solution against backdoor threats in MLLMs.", "AI": {"tldr": "BYE\u662f\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u71b5\u6a21\u5f0f\u7684\u6570\u636e\u8fc7\u6ee4\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u8fc7\u6ee4\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u6837\u672c\uff0c\u65e0\u9700\u5e72\u51c0\u76d1\u7763\u6216\u6a21\u578b\u4fee\u6539\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u5373\u670d\u52a1\uff08FTaaS\uff09\u4e2d\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u6076\u610f\u5fae\u8c03\u53ef\u80fd\u690d\u5165\u540e\u95e8\u3002", "method": "BYE\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\uff08\u63d0\u53d6\u6ce8\u610f\u529b\u56fe\u3001\u8ba1\u7b97\u71b5\u5206\u6570\u548c\u654f\u611f\u5c42\u5206\u6790\u3001\u65e0\u76d1\u7763\u805a\u7c7b\uff09\u8fc7\u6ee4\u540e\u95e8\u6837\u672c\u3002", "result": "BYE\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u89e6\u53d1\u7c7b\u578b\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u96f6\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "BYE\u4e3aMLLMs\u4e2d\u7684\u540e\u95e8\u5a01\u80c1\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.17013", "pdf": "https://arxiv.org/pdf/2505.17013", "abs": "https://arxiv.org/abs/2505.17013", "authors": ["Kevin Lu", "Nicky Kriplani", "Rohit Gandikota", "Minh Pham", "David Bau", "Chinmay Hegde", "Niv Cohen"], "title": "When Are Concepts Erased From Diffusion Models?", "categories": ["cs.LG", "cs.CV"], "comment": "Project Page:\n  https://nyu-dice-lab.github.io/when-are-concepts-erased/", "summary": "Concept erasure, the ability to selectively prevent a model from generating\nspecific concepts, has attracted growing interest, with various approaches\nemerging to address the challenge. However, it remains unclear how thoroughly\nthese methods erase the target concept. We begin by proposing two conceptual\nmodels for the erasure mechanism in diffusion models: (i) reducing the\nlikelihood of generating the target concept, and (ii) interfering with the\nmodel's internal guidance mechanisms. To thoroughly assess whether a concept\nhas been truly erased from the model, we introduce a suite of independent\nevaluations. Our evaluation framework includes adversarial attacks, novel\nprobing techniques, and analysis of the model's alternative generations in\nplace of the erased concept. Our results shed light on the tension between\nminimizing side effects and maintaining robustness to adversarial prompts.\nBroadly, our work underlines the importance of comprehensive evaluation for\nerasure in diffusion models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u64e6\u9664\u673a\u5236\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u5957\u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u6982\u5ff5\u662f\u5426\u88ab\u5f7b\u5e95\u64e6\u9664\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u7684\u5f7b\u5e95\u6027\uff0c\u63ed\u793a\u64e6\u9664\u673a\u5236\u4e0e\u6a21\u578b\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6982\u5ff5\u64e6\u9664\u673a\u5236\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5305\u62ec\u5bf9\u6297\u653b\u51fb\u3001\u65b0\u578b\u63a2\u6d4b\u6280\u672f\u548c\u66ff\u4ee3\u751f\u6210\u5206\u6790\u5728\u5185\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86\u64e6\u9664\u65b9\u6cd5\u5728\u51cf\u5c11\u526f\u4f5c\u7528\u4e0e\u4fdd\u6301\u5bf9\u6297\u6027\u63d0\u793a\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u5bf9\u6269\u6563\u6a21\u578b\u6982\u5ff5\u64e6\u9664\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2505.17016", "pdf": "https://arxiv.org/pdf/2505.17016", "abs": "https://arxiv.org/abs/2505.17016", "authors": ["Shuhan Tan", "Kairan Dou", "Yue Zhao", "Philipp Kr\u00e4henb\u00fchl"], "title": "Interactive Post-Training for Vision-Language-Action Models", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://ariostgx.github.io/ript_vla/", "summary": "We introduce RIPT-VLA, a simple and scalable reinforcement-learning-based\ninteractive post-training paradigm that fine-tunes pretrained\nVision-Language-Action (VLA) models using only sparse binary success rewards.\nExisting VLA training pipelines rely heavily on offline expert demonstration\ndata and supervised imitation, limiting their ability to adapt to new tasks and\nenvironments under low-data regimes. RIPT-VLA addresses this by enabling\ninteractive post-training with a stable policy optimization algorithm based on\ndynamic rollout sampling and leave-one-out advantage estimation.\n  RIPT-VLA has the following characteristics. First, it applies to various VLA\nmodels, resulting in an improvement on the lightweight QueST model by 21.2%,\nand the 7B OpenVLA-OFT model to an unprecedented 97.5% success rate. Second, it\nis computationally efficient and data-efficient: with only one demonstration,\nRIPT-VLA enables an unworkable SFT model (4%) to succeed with a 97% success\nrate within 15 iterations. Furthermore, we demonstrate that the policy learned\nby RIPT-VLA generalizes across different tasks and scenarios and is robust to\nthe initial state context. These results highlight RIPT-VLA as a practical and\neffective paradigm for post-training VLA models through minimal supervision.", "AI": {"tldr": "RIPT-VLA\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u4e92\u5f0f\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u4e8c\u5143\u6210\u529f\u5956\u52b1\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u79bb\u7ebf\u4e13\u5bb6\u6570\u636e\u548c\u76d1\u7763\u6a21\u4eff\uff0c\u96be\u4ee5\u9002\u5e94\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u65b0\u4efb\u52a1\u548c\u73af\u5883\u3002", "method": "\u91c7\u7528\u52a8\u6001\u91c7\u6837\u548c\u7559\u4e00\u6cd5\u4f18\u52bf\u4f30\u8ba1\u7684\u7a33\u5b9a\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u4ea4\u4e92\u5f0f\u540e\u8bad\u7ec3\u3002", "result": "\u5728\u8f7b\u91cf\u7ea7QueST\u6a21\u578b\u4e0a\u63d0\u534721.2%\uff0c7B OpenVLA-OFT\u6a21\u578b\u8fbe\u523097.5%\u6210\u529f\u7387\uff1b\u4ec5\u9700\u4e00\u6b21\u6f14\u793a\u5373\u53ef\u5c06\u6210\u529f\u7387\u4ece4%\u63d0\u5347\u81f397%\u3002", "conclusion": "RIPT-VLA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6700\u5c0f\u76d1\u7763\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u63d0\u5347\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
