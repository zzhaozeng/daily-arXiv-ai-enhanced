<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: 提出一种新的输入感知的块级剪枝方法IG-Pruning，通过动态选择层掩码来优化大型语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型计算需求增长，高效推理对实际部署至关重要。传统深度剪枝方法使用固定块掩码，在不同任务和输入下表现不佳。

Method: 提出两阶段方法：(1)通过语义聚类和L0优化发现多样掩码候选，(2)实现无需大量训练的高效动态剪枝。

Result: 实验证明该方法在性能上始终优于最先进的静态深度剪枝方法。

Conclusion: IG-Pruning特别适合资源受限的部署场景，为LLMs的高效推理提供了新解决方案。

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [2] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 该研究开发了探测和评估医疗聊天机器人性能的自动化基础设施，并发现LLM注释者之间一致性较低，推荐在LLM评估中使用多个LLM作为评估者。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人需要在涉及非医疗因素的情况下提供一致的建议，因此研究旨在理解医疗聊天机器人失效的条件。

Method: 开发了一个基础设施，自动生成探测LLM的查询，并使用多个LLM-as-a-judge设置和提示评估这些查询的答案。提示创建流程对患者人口统计、病史、疾病和写作风格进行采样，生成现实问题。评估流程使用LLM-as-a-judge和智能工作流进行幻觉和遗漏检测，以及LLM-as-a-judge治疗类别检测。

Result: LLM注释者表现出较低的一致性评分（平均Cohen's Kappa $κ=0.118$），只有特定的（回答，评估）LLM对在不同写作风格、性别和种族间产生统计显著差异。

Conclusion: 推荐在LLM评估中使用多个LLM作为评估者，以避免得出统计显著但不可推广的结果，尤其是在缺乏真实数据的情况下，并建议发布LLM间一致性指标以提高透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [3] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: 提出LTD-Bench，通过可视化输出评估LLMs的空间推理能力，揭示传统数值指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs评估依赖不透明的数值指标，无法直观反映模型在空间推理等实际应用中的能力缺陷。

Method: 设计LTD-Bench，包含生成（点阵绘图/代码）和识别任务，分三个难度层级评估语言-空间双向映射。

Result: 实验显示，即使表现优异的LLMs在空间概念双向映射上存在严重缺陷，可视化输出可有效诊断模型相似性。

Conclusion: LTD-Bench将抽象评估转化为直观可视化，弥补了统计性能与实际能力间的鸿沟，暴露了LLMs作为世界模型的核心缺陷。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [4] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: 提出M-Solomon，一个通用多模态嵌入器，通过自适应决定何时增强查询，提高查询相关文档的检索效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的嵌入器在增强查询时，存在延迟高和某些查询性能降低的问题，并且在多模态环境下研究不足。

Method: 将训练数据集分为需要和不需要增强的查询，通过MLLM生成适当增强，并引入自适应查询增强，学习生成'/augment'前缀或简单字符串'/embed'。

Result: M-Solomon在不增强的基线上有显著提升，并且比总是增强的基线表现更好，嵌入延迟更低。

Conclusion: M-Solomon通过自适应查询增强，在多模态环境中实现了更高效的查询处理。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [5] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文介绍了一种名为AyurParam-2.9B的领域专用双语语言模型，该模型通过大量专家筛选的阿育吠陀数据集进行微调，在特定医学领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在需要深度文化、语言和主题专业知识的领域表现不佳，尤其是在阿育吠陀等需要复杂文本和临床知识的传统医学体系中。

Method: AyurParam-2.9B是基于Param-1-2.9B模型，通过涵盖经典文本和临床指导的双语（英语和印地语）阿育吠陀数据集进行微调。数据集包含上下文感知、推理和客观问答，并具有严格的事实精确性和教学清晰性的标注协议。

Result: 在BhashaBench-Ayur基准测试中，AyurParam-2.9B不仅超越了所有同规模（1.5-3B参数）开源指令调整模型，还表现出与更大模型竞争或更优的性能。

Conclusion: AyurParam-2.9B的结果突出了在提供可靠的、文化契合的专业医学知识AI时，真实领域适应和高质量监督的必要性。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [6] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: 提出AutoAdv框架，通过多轮自适应对话实现高效大模型越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估多关注单轮交互，而现实攻击常通过多轮自适应对话展开，存在安全漏洞。

Method: AutoAdv框架结合三种自适应机制：模式管理器、温度管理器和两阶段重写策略，无需训练即可自动化多轮越狱攻击。

Result: 在Llama-3.1-8B上六轮内攻击成功率高达95%，比单轮基线提升24%；在GPT-4o-mini、Qwen3-235B和Mistral-7B等模型上多轮攻击效果均优于单轮。

Conclusion: 当前对齐策略对单轮交互优化有效，但在多轮对话中防护能力不足，亟需开发多轮感知的防御机制。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [7] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型在专业领域（如金融）中的局限性，并提出了通过合并领域特定的持续预训练（CPT）“专家”模型来提升性能的方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域表现不佳，合并领域特定的CPT专家模型提供了一种实用且成本较低的替代方案，避免了昂贵且不稳定的多技能训练。

Method: 研究采用三种合并方法（Task Arithmetic, TIES, DARE-TIES），通过三阶段评估（知识恢复、互补性和涌现性）在包含18个任务和8个数据集的金融基准上进行实验。

Result: 合并专家与其基础模型可恢复CPT过程中丢失的一般知识，合并不同专家模型能提升性能并可能产生跨领域的涌现技能；Task Arithmetic性能强但对超参数敏感，TIES更稳健。

Conclusion: 该研究首次对CPT模型合并进行了基础分析，建立了原则性框架，并为从现有资产构建多技能大型语言模型提供了明确指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [8] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 评估基于角色的提示是否改善大型语言模型（LLM）在宏观经济预测任务中的表现，发现GPT-4o和人类预测者准确性相似，但角色描述并未提供显著的预测优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨通过引入经济学相关的角色提示，是否能够提高LLM在宏观经济预测中的准确性，并与人类专家的预测进行比较。

Method: 利用来自PersonaHub语料库的2,368个与经济学相关的角色，GPT-4o被提示复制欧洲央行专业预测者调查的50个季度（2013-2025）。将基于角色的预测与没有角色描述的基线预测进行比较。

Result: GPT-4o和人类预测者实现了非常相似的准确性，差异在统计上显著但实际较小。角色描述未显示可测量的预测优势。

Conclusion: GPT-4o即使在没有角色描述的情况下，只要提供相关上下文数据，也能实现与专家竞争性的预测准确性，提示中的多样角色描述对预测结果的影响较小。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [9] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 该研究分析了Google Translate在英译罗过程中出现的词汇错误，旨在提高机器翻译的质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过减少错误和改进词汇选择，提升Google Translate的翻译质量，尤其是在官方和医疗信息翻译方面。

Method: 通过对230个由Google Translate从英语翻译为罗马尼亚语的文本进行全面的错误分析，重点分析其中的词汇错误。

Result: 尚未提供具体结果。

Conclusion: 该研究希望通过分析词汇错误，为改进Google Translate的性能提供建议，从而提升机器翻译的整体质量。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [10] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: 提出NTKT，将知识追踪重构为利用预训练大语言模型的下一个标记预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有KT模型忽略了问题文本，限制预测性能，而问题文本包含重要的教学信息。

Method: NTKT将学生历史记录和问题内容表示为文本序列，利用预训练大语言模型进行下一个标记预测。

Result: NTKT在性能上显著优于现有模型，并且在冷启动问题和用户上具有更好的泛化能力。

Conclusion: 问题内容在知识追踪中至关重要，利用大语言模型的预训练表示可以更有效建模学生学习。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [11] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: 本文介绍了一种新的贝叶斯框架Confidence-Guided Early Stopping (CGES)，通过置信度信号动态停止采样，以减少大语言模型在测试时的调用次数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在测试时通常需要多次调用，通过多数投票聚合预测结果，但这种方法需要固定的调用次数，并且当正确答案较为罕见时会失效。

Method: CGES利用从标记概率或奖励模型导出的标量置信度信号，形成候选答案的后验分布，并在后验质量超过阈值时自适应停止采样。

Result: 在五个推理基准测试中，CGES将平均模型调用次数减少了约69%，同时准确率与自一致性方法相差不超过0.06个百分点。

Conclusion: CGES提供了一种理论保障的方法，可以显著减少模型调用次数，同时保持与自一致性方法相当的准确率，即使在使用有噪声的置信度信号时也是如此。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [12] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: 提出了一种新的框架TRACE，通过程序化策略应用解决大型语言模型的对齐问题，实现精确、高效且不影响模型性能的策略更新。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的对齐方法存在静态、脆弱和维护成本高等问题，难以适应不断变化的规范和策略，导致对齐-现实差距。

Method: TRACE框架通过程序化分类现有偏好数据，利用对齐影响评分识别高影响冲突，并应用混合优化策略来精确更新模型偏好。

Result: 在多个模型家族（Qwen2.5-7B, Gemma-2-9B, Llama-3.1-8B）及PKU-SafeRLHF数据集上，TRACE成功执行新策略而不降低通用能力。

Conclusion: TRACE为维持大型语言模型对齐提供了可扩展、动态且成本效益高的方法，为可持续和负责任的AI部署奠定了基础。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [13] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 通过设计控制数据集Biography-Reasoning，分析不同类型和任务下LLMs在微调过程中产生的事实幻觉，并提出KnownPatch方法缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨大语言模型在微调过程中引入新知识时产生的事实幻觉的具体表现和潜在机制，因此需要进一步分析。

Method: 设计控制数据集Biography-Reasoning，对多种知识类型和任务类型进行细粒度分析，并引入KnownPatch方法，在训练后期加入少量已知知识样本。

Result: 在微调数据集上，当特定知识类型完全为新知识时，LLMs表现出明显增加的幻觉倾向。KnownPatch方法有效缓解了新知识引起的幻觉，并提高了性能。

Conclusion: 新知识的高度陌生性，而不是新知识的总体比例，是导致幻觉更强的驱动因素。KnownPatch方法通过缓解新知识学习对模型关注关键实体的干扰，减少了幻觉。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [14] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 本文提出了一种名为'最优奇异损伤'的方法，通过选择性稀疏化低秩近似更新，实现大型语言模型微调后参数更新的高效存储。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各类应用中越来越普遍，但其庞大的规模限制了存储和处理能力。大多数应用依赖于预训练模型并进行微调，但存储这些微调模型是一个巨大挑战。研究表明，微调主要影响少量参数，因此需要更高效的存储方案。

Method: 利用微调更新是低秩且稀疏的特性，提出了一种名为'最优奇异损伤'的方法，通过利用奇异向量的交叉重要性，选择性地稀疏化低秩近似更新，确保保留最具影响力的组件。

Result: 实验表明，与单独使用低秩近似或稀疏化相比，该方法在相同内存预算下实现了显著更高的存储效率和更优的准确性。

Conclusion: 选择性稀疏化低秩近似更新是一种有效的参数存储方法，能够在保留模型性能的同时显著减少存储需求。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [15] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: PragExTra 是首个用于语用显化的多语言数据集和检测框架，涵盖八种语言对。


<details>
  <summary>Details</summary>
Motivation: 翻译中常常需要增加背景信息，使隐含的文化意义对新读者更加明确，即语用显化，但这一现象很少通过计算建模进行讨论。

Method: 利用 TED-Multi 和 Europarl 构建多语言语料库，通过空对齐和主动学习结合人工标注识别语用显化候选案例。

Result: 实体和系统级显化最为常见，主动学习使分类器准确率提高了7-8个百分点，最高达到0.88的准确率和0.82的F1值。

Conclusion: PragExTra 将语用显化确立为一种可测量、跨语言的现象，并朝着构建具有文化意识的机器翻译迈出了重要一步。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [16] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 本文研究了低资源语言国家在人工智能（AI）应用方面的不均衡现象，发现AI在低资源语言上的表现较差，导致这些国家的AI使用率较低。


<details>
  <summary>Details</summary>
Motivation: AI在全球范围内快速传播，但采用率不均衡，尤其在低资源语言国家中表现不佳，这可能会影响AI的普及。

Method: 使用加权回归模型，从社会经济和 demographic 因素中分离出语言因素的影响。

Result: 低资源语言国家的AI用户比例比基准水平低约20%。

Conclusion: 语言可及性是影响AI公平传播的重要独立障碍。

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [17] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出了一种集中式多LLM框架CoRL，通过控制器LLM选择性地协调专家模型，以在预算不同的情况下实现性能和成本的最优权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同领域表现出互补性，并且推理成本不同，因此需要设计高效的多智能体LLM系统。现有分散式框架推理成本高，需要一个更经济的集中式框架。

Method: 引入CoRL，一种强化学习框架，通过双目标强化学习最大化任务性能并最小化总体推理成本，在不同预算条件下自适应调整行为。

Result: 在四个不同基准测试中，CoRL在高预算设置下超越最佳专家LLM，并在低预算模式下保持强劲性能。

Conclusion: 集中式协调在实现可扩展和经济高效的多智能体LLM系统中非常有效。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [18] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的自动回归多嵌入检索器（AMER），通过生成多个查询向量来解决传统单向量检索器在多模态分布下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统文本检索器生成一个查询向量，难以处理相关文档的多模态分布问题，即一个查询可能有多种不同的解释。

Method: 开发了一种名为AMER的新检索器架构，该模型自回归地生成多个查询向量，并用这些向量从文档集中检索文档。

Result: 在合成向量化数据上，该方法能完美捕捉多个目标分布，性能比单嵌入模型高出4倍；在两个真实数据集上，相对于单嵌入基线分别提高了4%和21%。

Conclusion: AMER展示了使用多查询向量检索器的潜力，为未来研究开辟了新方向，特别是在目标文档嵌入之间差异较大的情况下表现更优。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [19] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: 随着模型上下文长度增加，现有评估多依赖检索，无法全面测试长文本推理能力。Oolong 是一个新的长文本推理基准，要求模型在原子级别分析文本块并聚合分析以回答分布问题。


<details>
  <summary>Details</summary>
Motivation: 当前长文本评估方法主要集中于检索任务，忽略了模型在长上下文中进行复杂推理的需求。因此，需要一种更全面的评估方法，以测试模型在真实场景下对长文本的处理能力。

Method: Oolong 基准分为 Oolong-synth 和 Oolong-real 两个任务集，前者为可轻松消融的自然合成任务，后者为需要处理真实对话数据的下游设置。任务要求模型进行上下文分类、计数以及时间和用户关系推理。

Result: 即使是前沿模型，如 GPT-5、Claude-Sonnet-4 和 Gemini-2.5-Pro，在 Oolong 的两个任务集上都难以达到 50% 的准确率，表明当前模型在长文本推理方面仍有较大提升空间。

Conclusion: Oolong 基准揭示了当前模型在长文本推理任务上的不足，并提供了数据和评估工具，以促进能够处理大量文本的模型的开发。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [20] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: 引入深度价值基准（DVB），用于评估大型语言模型是否学习到基本的人类价值观或仅是表面偏好。


<details>
  <summary>Details</summary>
Motivation: AI对齐要求系统能捕捉深层价值，而不是表面偏好，以确保行为一致性。

Method: DVB采用新颖实验设计，在训练阶段故意让深层价值和浅层特征相关联，在测试阶段打破这些关联，从而测量模型的深度价值泛化率（DVGR）。

Result: 在9个模型中，平均DVGR仅为0.30，所有模型的泛化能力低于随机水平，较大模型的DVGR反而更低。

Conclusion: DVB提供了对齐核心特征的可解释度量，当前模型在深度价值泛化方面表现不佳。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [21] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: 提出了一种自适应奖励预测方法Re-FORC，通过轻量级适配器训练推理模型，优化推理链的停止、模型和思考步长的选择以及测试时的扩展。


<details>
  <summary>Details</summary>
Motivation: 为了在保持准确率的同时减少计算成本，需要一种方法来预测预期奖励并动态控制推理过程。

Method: Re-FORC训练轻量级适配器，根据上下文预测预期未来奖励，并控制未来思考token的数量。

Result: Re-FORC实现了26%的计算减少，4%的准确率提升，55%的计算成本降低，并在高/低计算环境下分别提升11%和7%的准确率。

Conclusion: Re-FORC提供了一种有效的方式来进行动态推理，通过成本每token阈值控制长度，同时提前估算计算时间。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [22] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 该论文提出了一个名为ATHENA的框架，通过结合符号效用函数和个性化语义适应，以更好地模拟人类在高风险决策中的行为。


<details>
  <summary>Details</summary>
Motivation: 个体决策，尤其是在疫苗接受等高风险情境中，通常与群体最优预测存在差距。这种差距源于个体决策过程的独特性，包括数值属性和语言影响。

Method: ATHENA框架包含两个阶段：首先，通过LLM增强的符号发现技术，发现稳健的群体级符号效用函数；其次，通过个体级语义适应，创建个性化语义模板，以模拟个性化选择。

Result: 在现实世界的交通方式和疫苗选择任务中，ATHENA始终优于基于效用、机器学习和其他基于LLM的模型，F1分数至少提高6.5%。

Conclusion: 通过有机整合符号效用建模和语义适应，ATHENA为建模以人为中心的决策提供了一个新方案。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [23] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: 本文提出了一种新的多智能体系统协作框架STRMAC，通过状态感知路由和自进化数据生成，提高了协作效率和系统性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然能整合不同的专业知识并灵活协作，但受限于僵化的智能体调度和低效的协调策略。因此，需要一种能够动态适应任务需求的高效协作方法。

Method: 提出了STRMAC，一个状态感知路由框架。该方法分别编码交互历史和智能体知识，以驱动路由器在每个步骤中自适应地选择最合适的单个智能体。同时，引入自进化数据生成方法，加速高质量执行路径的收集，以进行系统训练。

Result: 在协作推理基准测试中，该方法实现了最先进的性能，相比基线方法提高了23.8%，并将数据收集开销减少了90.1%。

Conclusion: STRMAC框架通过自适应路由和高效的数据生成策略，显著提升了多智能体系统的协作效率和性能，为复杂任务的解决提供了新的可能。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [24] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 提出了一种新方法\method，通过分解复杂问题、清理噪声表格和基于思维程序（PoT）的推理器，提高大型语言模型在表格数据上的复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂表格数据的推理中表现不佳，主要由于复杂查询、噪声数据和有限的数值能力。

Method: \method框架包含三个部分：(1) 查询分解器分解复杂问题，(2) 表格净化器清理和过滤噪声表格，(3) 基于PoT的推理器生成可执行代码从净化表格中得出最终答案。

Result: 在TAT-QA、TableBench和CalTab151数据集上，\method分别实现了8.79%、6.08%和19.87%的准确率提升，优于现有方法。

Conclusion: \method有效提升了大型语言模型在复杂表格数值推理任务中的表现，并提供了一个稳健的解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [25] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出了一个新框架，将多模态大语言模型处理冲突信息的行为分解为相对推理不确定性和内在模态偏好两个因素，并构建了一个可控数据集验证该框架。


<details>
  <summary>Details</summary>
Motivation: 以前的研究仅使用粗粒度的数据集级统计来测量多模态模型处理冲突的行为，忽略了模型在单模态推理中的信心影响，因此需要更细致的分析框架。

Method: 引入了一个新框架，将多模态冲突处理行为分解为相对推理不确定性和内在模态偏好，并构建了一个可控数据集，系统地变化视觉和文本输入的推理难度。

Result: 使用熵作为细粒度不确定性度量，发现遵循某种模态的概率随着其相对不确定性的增加而单调减少，并提出了平衡点的概念作为模型内在偏好的实用指标。

Conclusion: 相对不确定性和内在偏好是多模态冲突处理行为的两个基本原则，该框架提供了定量分析和机制性见解，帮助理解多模态大模型如何处理冲突信息。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [26] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的框架ProQ-BERT，用于利用多模态电子健康记录（EHR）预测慢性肾病（CKD）的进展。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病（CKD）影响全球近10%的人口，准确预测其进展对及时干预和资源优化至关重要。

Method: ProQ-BERT整合了人口统计学、临床和实验室数据，采用基于量化的连续实验值标记化和注意力机制以提高可解释性。模型采用掩码语言建模进行预训练，并针对从3a期到5期的进展进行二分类任务微调。

Result: 在对91,816名患者的队列进行评估时，ProQ-BERT的表现始终优于CEHR-BERT，短期预测的ROC-AUC高达0.995，PR-AUC高达0.989。

Conclusion: 该研究强调了Transformer架构和时间设计选择在临床预后建模中的有效性，为个性化CKD护理提供了有希望的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [27] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 提出了一种基于模糊软集合理论的专业系统，用于通过测量临床和生理参数评估乳腺癌风险。


<details>
  <summary>Details</summary>
Motivation: 由于疾病的复杂性和患者风险因素的可变性，及时检测乳腺癌仍然是一个挑战。早期诊断对于有效治疗和提高生存率至关重要。

Method: 该系统整合了身体质量指数、胰岛素水平、瘦素水平、脂联素水平和年龄作为输入变量，通过一组模糊推理规则和软集合计算来评估乳腺癌风险。

Result: 这些参数可从常规血液分析中获得，为非侵入性和便捷的初步评估提供了方法。模型的开发和验证数据来自UCI机器学习库。

Conclusion: 该专家系统旨在帮助医疗专业人员识别高风险患者，并决定是否需要进一步诊断程序，如活检。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [28] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出了一种基于二分类视角的生成模型PR曲线估计新框架，解决了传统标量评价指标信息不足和PR曲线估计难的问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在图像和文本领域的成功，其评价方法受到广泛关注；现有标量指标存在局限性，而PR曲线虽能提供更丰富分析，却面临估计挑战。

Method: 从二分类视角出发设计全新PR曲线估计框架，进行严格的统计分析，推导出PR估计风险的极小极大上界，并扩展多种经典PR指标。

Result: 所提框架不仅理论上获得风险上界，还统一了现有仅关注曲线极值的PR指标，实验验证了不同场景下曲线行为的差异性。

Conclusion: 该框架为生成模型的PR曲线估计提供了理论保障和实践工具，弥补了标量指标与曲线极端值指标间的空白，推动了生成模型评估的发展。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [29] [ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning](https://arxiv.org/abs/2511.02424)
*Jae-Woo Choi,Hyungmin Kim,Hyobin Ong,Minsu Jang,Dohyung Kim,Jaehong Kim,Youngwoo Yoon*

Main category: cs.AI

TL;DR: 提出了一种新的分层任务规划方法ReAcTree，通过将复杂目标分解为子目标，并利用动态构建的智能体树和双记忆系统，提高了大型语言模型在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在复杂、长期任务中表现不佳，因为它们依赖于将所有过去决策和观察混合在一起的单一轨迹，试图通过一个统一的过程解决整个任务。

Method: 提出ReAcTree，该方法将复杂目标分解为更易于管理的子目标，并通过动态构建的智能体树来处理每个子目标。每个智能体节点能够推理、行动并进一步扩展树，而控制流节点协调智能体节点的执行策略。此外，引入了两种互补的记忆系统：智能体节点从事件记忆中检索特定目标的子目标级示例，并通过工作记忆共享特定环境的观察结果。

Result: 在WAH-NL和ALFRED数据集上的实验表明，ReAcTree在多种大型语言模型上持续优于如ReAct等强任务规划基线。特别是在WAH-NL上，ReAcTree与Qwen 2.5 72B结合使用时，达到了61%的目标成功率，几乎是ReAct的31%的两倍。

Conclusion: ReAcTree通过分层规划和双记忆系统显著提升了解决复杂任务的能力，为大型语言模型在自主智能体任务规划中的应用提供了新的方向。

Abstract: Recent advancements in large language models (LLMs) have enabled significant
progress in decision-making and task planning for embodied autonomous agents.
However, most existing methods still struggle with complex, long-horizon tasks
because they rely on a monolithic trajectory that entangles all past decisions
and observations, attempting to solve the entire task in a single unified
process. To address this limitation, we propose ReAcTree, a hierarchical
task-planning method that decomposes a complex goal into more manageable
subgoals within a dynamically constructed agent tree. Each subgoal is handled
by an LLM agent node capable of reasoning, acting, and further expanding the
tree, while control flow nodes coordinate the execution strategies of agent
nodes. In addition, we integrate two complementary memory systems: each agent
node retrieves goal-specific, subgoal-level examples from episodic memory and
shares environment-specific observations through working memory. Experiments on
the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently
outperforms strong task-planning baselines such as ReAct across diverse LLMs.
Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5
72B, nearly doubling ReAct's 31%.

</details>


### [30] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 本文介绍了一种将可验证奖励强化学习（RLVR）应用于开放式任务的新方法，通过可验证多选重构（VMR）提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR在数学和编程等领域取得了显著进展，但在开放式任务中缺乏标准答案的情况下，尚未有效利用推理能力。本文旨在探索强化推理是否能提升开放式任务的表现。

Method: 引入可验证多选重构（VMR），将开放式数据重构为可验证的多选格式，使RLVR能够应用于没有明确标准答案的领域。

Result: 在多个开放式任务基准测试中，VMR方法显著提高了大语言模型的表现，在八个基准测试中，平均得分提高了5.99分。

Conclusion: VMR是一种有效的策略，可以通过重构开放式任务为多选格式，使RLVR能够应用于这些任务并显著提升模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [31] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 本文提出KLPEG框架，利用知识图谱和大型语言模型提高视频游戏增量更新的自动化测试效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏快速迭代和频繁更新给测试效率和特异性带来了挑战，现有基于LLM的自动化测试方法缺乏结构化知识积累机制。

Method: KLPEG框架构建和维护知识图谱（KG）以系统建模游戏元素、任务依赖和因果关系，并利用LLM解析自然语言更新日志，通过KG上的多跳推理识别影响范围，生成针对更新的测试用例。

Result: 在Overcooked和Minecraft两个游戏环境中的实验表明，KLPEG能更准确地定位受更新影响的功能，并在更少步骤内完成测试。

Conclusion: KLPEG框架显著提高了自动化测试的有效性和效率，通过知识图谱和LLM的结合，解决了增量游戏更新的测试挑战。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [32] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA是一个新的基准测试，用于评估大型语言模型在多领域、现实生活中的定量推理能力。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在真实世界多领域中的定量推理能力，并识别其错误类型和互补性。

Method: 使用Omni的计算器引擎验证输出，在500个跨领域的自然语言任务上测试了五个最先进的系统。

Result: 五个系统的准确率为45%到63%，主要错误为舍入误差（35%）和计算错误（33%）。模型在数学和工程领域表现较强，在物理和自然科学领域较弱。相关分析显示模型之间具有部分互补性。

Conclusion: ORCA基准测试揭示了当前大型语言模型在定量推理方面的局限性和互补性，强调了需要改进计算精度和跨领域泛化能力。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [33] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出了一种基于GR(1)规范的自适应屏蔽框架，用于强化学习中的安全保证，该方法能在线检测环境假设违规并自动修复规范。


<details>
  <summary>Details</summary>
Motivation: 传统屏蔽方法多为静态，无法在环境假设被违反时进行调整，导致安全性无法保证，因此需要一种能够动态适应环境变化的屏蔽方法。

Method: 该方法利用GR(1)规范和归纳逻辑编程（ILP）在线检测和修复环境假设违规，确保持续的安全性和可实现性。

Result: 通过Minepump和Atari Seaquest两个案例研究，表明该自适应屏蔽方法在保持近乎最优奖励和完美逻辑合规性方面优于静态屏蔽。

Conclusion: 自适应屏蔽方法比静态符号控制器在优化辅助奖励时更有效，并能在必要时优雅地调整目标，从而提升整体性能。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [34] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: DecompSR是一个用于分析组合空间推理能力的大型基准数据集和生成框架。


<details>
  <summary>Details</summary>
Motivation: 为了分析大型语言模型在组合空间推理任务中的表现，特别是对组合性的不同方面进行独立变化和评估。

Method: 构建一个名为DecompSR的基准数据集，该数据集允许独立变化组合性的几个方面（如推理深度、实体和语言变异性、输入顺序、干扰项和系统性），并通过符号求解器保证数据集的正确性。

Result: 在多种大型语言模型上进行了全面基准测试，发现这些模型在空间推理任务中对于生产性和系统性的泛化能力较弱，但在语言变异性方面表现较为稳健。

Conclusion: DecompSR提供了一个可证明正确且严格的基准测试数据集，可以独立变化组合性的关键方面，从而对大型语言模型的组合推理能力进行稳健和细粒度的探测。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [35] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 该论文提出了一个用于评估异构智能体协作能力的迷宫解决基准测试，并发现智能体在协作时表现往往不如单独行动。


<details>
  <summary>Details</summary>
Motivation: AI发展越来越依赖于异构智能体系统的协作，但在部分可观察条件下，协作的有效性尚未得到充分研究。

Method: 提出了一种协作迷宫解决基准，评估了32种开源和闭源模型在单独、同构和异构配对中的表现，并引入“接力推理”方法改善协作效果。

Result: 发现模型在协作时存在“协作差距”，即模型在单独行动时表现良好，但在协作时表现下降，尤其是某些小型模型在配对中几乎完全失败。

Conclusion: 强调了协作意识评估、训练策略和交互设计的重要性，以提高智能体协作能力，这同样适用于AI-AI和人-AI协作。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [36] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文引入了span query以推广推理服务器的接口，展示其在多种用例下的应用，并提出了优化KV缓存和注意力局部性的方法。


<details>
  <summary>Details</summary>
Motivation: 随着客户端需求超越聊天完成，现有推理服务器仍主要优化于聊天完成，因此需要一种通用接口来适应不同的用例，尤其是非聊天场景。

Method: 引入span queries作为推理调用的表达式树，并通过结合交换性约束进行优化，提高KV缓存和注意力的局部性。

Result: span queries在两个非聊天用例中实现了10-20倍的TTFT降低，并且在小模型上通过优化注意力性能超过了大模型的推理服务器。

Conclusion: span queries为推理服务器提供了一种通用且高效的接口，通过优化缓存和注意力机制，可以显著提升不同用例下的性能。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [37] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出了一种利用大型语言模型（LLM）辅助的半自动化方法，通过PyIRK框架将自然语言描述和数学定义转化为形式化知识图谱，以增强控制工程领域的知识结构化与可访问性。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域的研究产出迅速增长，需要新的方法来构建和形式化领域知识，以实现知识的易访问性、协作性和可验证性。

Method: 基于PyIRK框架，使用LLM辅助将自然语言描述和LaTeX源码中的数学定义转化为形式化知识图谱，并生成增强源文档的“交互式语义层”。

Result: 成功实现了将自然语言描述和数学定义转化为形式化知识图谱的方法，并展示了其在控制工程领域的应用。

Conclusion: 该方法有助于实现控制工程领域知识库的结构化、易访问性和可验证性，为领域知识的传播和共享提供了新途径。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [38] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文介绍了一种名为'modality sabotage'的诊断故障模式，并提出了一个轻量级、模型不可知的评估层来分析和解决多模态大型语言模型中不透明的推理过程。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型的推理过程不透明，难以确定哪个模态驱动预测、如何解决冲突或何时一个流会占主导地位。

Method: 提出了一个将每个模态视为一个智能体的评估层，生成候选标签和简短的自评估以进行审计，并采用简单的融合机制聚合这些输出。

Result: 在情感识别基准测试中应用该诊断层揭示了系统性的可靠性模式，提供了关于失败是否来自数据集伪影或模型局限性的洞察。

Conclusion: 该框架为多模态推理提供了诊断支架，支持对融合动态进行原则性审计，并为可能的干预措施提供信息。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [39] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文介绍了一种通过分解攻击能力并优化各组成部分来增强AI控制环境中攻击策略的方法，解决了在复杂AI部署中因计算限制导致数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署变得更加复杂和高风险，估算其风险变得尤为重要。良好的控制评估需要强有力的攻击策略，但在复杂环境中由于计算限制导致数据稀缺，因此提出此方法。

Method: 将攻击能力分解为五个组成部分：怀疑建模、攻击选择、计划合成、执行和巧妙性，并分别进行优化。开发了一种攻击动态的概率模型，利用该模拟优化攻击超参数，并验证其在SHADE-Arena环境中的迁移效果。

Result: 该方法显著增强了攻击强度，使用提出的方法将安全得分从基线0.87降低到0.41。

Conclusion: 通过分解和优化攻击能力的各个组成部分，并借助概率模型进行超参数优化，可以在数据稀缺的情况下有效提升攻击策略的强度，从而改进AI控制评估。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>
