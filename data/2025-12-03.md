<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 33]
- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review](https://arxiv.org/abs/2512.02024)
*Yan Yang,Mouxiao Bian,Peiling Li,Bingjian Wen,Ruiyao Chen,Kangkun Mao,Xiaojun Ye,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Kaifeng Qiu,Junyan Wu*

Main category: cs.CL

TL;DR: 开发RxBench，一个用于处方审核的综合性基准，评估18种LLM，揭示性能差异，并实现针对性微调以提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在临床决策支持中的迅速发展，需要一个系统和细粒度的评估框架来衡量其在处方审核中的能力。

Method: 构建了RxBench，涵盖常见的处方审核类别和14种处方错误类型，包括1,150个单选、230个多选和879个简答题，由经验丰富的临床药师审核。并对18种先进LLM进行了基准测试，并对中等层次模型进行了针对性微调。

Result: Gemini-2.5-pro-preview-05-06、Grok-4-0709和DeepSeek-R1-0528在性能和稳健性上表现最佳。领先的LLM在某些任务上可以匹配或超过人类药师的表现。微调后的中等层次模型在简答题任务上性能接近领先的通用LLM。

Conclusion: RxBench建立了一个标准化、面向错误类型的框架，揭示了前沿LLM在处方审核中的能力与局限，为构建更可靠和专业的临床工具奠定了基础。

Abstract: The rapid advancement of large language models (LLMs) has accelerated their integration into clinical decision support, particularly in prescription review. To enable systematic and fine-grained evaluation, we developed RxBench, a comprehensive benchmark that covers common prescription review categories and consolidates 14 frequent types of prescription errors drawn from authoritative pharmacy references. RxBench consists of 1,150 single-choice, 230 multiple-choice, and 879 short-answer items, all reviewed by experienced clinical pharmacists. We benchmarked 18 state-of-the-art LLMs and identified clear stratification of performance across tasks. Notably, Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528 consistently formed the first tier, outperforming other models in both accuracy and robustness. Comparisons with licensed pharmacists indicated that leading LLMs can match or exceed human performance in certain tasks. Furthermore, building on insights from our benchmark evaluation, we performed targeted fine-tuning on a mid-tier model, resulting in a specialized model that rivals leading general-purpose LLMs in performance on short-answer question tasks. The main contribution of RxBench lies in establishing a standardized, error-type-oriented framework that not only reveals the capabilities and limitations of frontier LLMs in prescription review but also provides a foundational resource for building more reliable and specialized clinical tools.

</details>


### [2] [Beyond Confidence: Adaptive and Coherent Decoding for Diffusion Language Models](https://arxiv.org/abs/2512.02044)
*Kecheng Chen,Ziru Liu,Xijia Tao,Hui Liu,Xinyu Fu,Suiyun Zhang,Dandan Tu,Lingpeng Kong,Rui Liu,Haoliang Li*

Main category: cs.CL

TL;DR: 提出了一种新颖的推理框架CCD，通过轨迹校正和自适应采样策略，提高扩散语言模型的生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理方法依赖局部和即时步骤的指标，缺乏更可靠的视角，导致采样轨迹不一致和生成质量次优。

Method: CCD框架包括轨迹校正机制和自适应采样策略，利用历史上下文增强序列一致性，并根据一致性度量动态调整解掩码预算。

Result: 在Dream和LLaDA等基准测试上，该方法在推理速度和性能上均有显著提升，最高可实现3.48倍加速和3.91%的性能提升。

Conclusion: CCD通过利用历史上下文和动态调整策略，有效地提高了生成轨迹的质量，同时加速了采样过程。

Abstract: Diffusion Language Models (DLMs) have recently achieved significant success due to their any-order generation capabilities. However, existing inference methods typically rely on local, immediate-step metrics such as confidence or entropy which inherently lack a more reliable perspective. This limitation frequently leads to inconsistent sampling trajectories and suboptimal generation quality. To address this, we propose Coherent Contextual Decoding (CCD), a novel inference framework built upon two core innovations. First, CCD employs a trajectory rectification mechanism that leverages historical context to enhance sequence coherence, enabling the early rejection of suboptimal paths. We demonstrate that this mechanism is theoretically equivalent to modeling the consistency of historical steps via the conditional mutual information between context and token predictions. Building on this theoretical insight, we further address the inefficiency of conventional uniform decoding budgets. Instead of rigid allocations based on diffusion steps, we introduce an adaptive sampling strategy that dynamically adjusts the unmasking budget for each step according to our consistency metric. Consequently, our method significantly improves the quality of generation trajectories while accelerating the sampling process. Empirically, our method achieves a simultaneous enhancement in both inference speed and performance across diverse benchmarks on Dream and LLaDA, delivering up to 3.48x speedup alongside 3.91% performance improvement.

</details>


### [3] [Reversing Large Language Models for Efficient Training and Fine-Tuning](https://arxiv.org/abs/2512.02056)
*Eshed Gal,Moshe Eliasof,Javier Turek,Uri Ascher,Eran Treister,Eldad Haber*

Main category: cs.CL

TL;DR: 本文提出了内存高效、可逆的LLM架构，以减少训练时内存消耗。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高且耗时，通常需要微调以完成特定任务，因此减少内存消耗是一个重要问题。

Method: 受对称和辛微分方程的启发，引入了可逆架构，利用时间可逆动态在反向传播中检索隐藏状态，无需存储激活。还提出了通过微调将现有不可逆LLM转换为可逆架构的方法。

Result: 在多个数据集和基准测试中，性能相当或有所提升，显著减少了内存消耗，支持更大批量处理。

Conclusion: 该可逆架构提供了一种可扩展且高效的方法，降低了训练和微调LLM的内存和计算成本。

Abstract: Large Language Models (LLMs) are known for their expensive and time-consuming training. Thus, oftentimes, LLMs are fine-tuned to address a specific task, given the pretrained weights of a pre-trained LLM considered a foundation model. In this work, we introduce memory-efficient, reversible architectures for LLMs, inspired by symmetric and symplectic differential equations, and investigate their theoretical properties. Different from standard, baseline architectures that store all intermediate activations, the proposed models use time-reversible dynamics to retrieve hidden states during backpropagation, relieving the need to store activations. This property allows for a drastic reduction in memory consumption, allowing for the processing of larger batch sizes for the same available memory, thereby offering improved throughput. In addition, we propose an efficient method for converting existing, non-reversible LLMs into reversible architectures through fine-tuning, rendering our approach practical for exploiting existing pre-trained models. Our results show comparable or improved performance on several datasets and benchmarks, on several LLMs, building a scalable and efficient path towards reducing the memory and computational costs associated with both training from scratch and fine-tuning of LLMs.

</details>


### [4] [Dialect Identification Using Resource-Efficient Fine-Tuning Approaches](https://arxiv.org/abs/2512.02074)
*Zirui Lin,Haris Gulzar,Monnika Roslianna Busto,Akiko Masaki,Takeharu Eda,Kazuhiro Nakadai*

Main category: cs.CL

TL;DR: 本文探索了内存高效微调（MEFT）方法在语音模型中的应用，以降低计算和内存成本，同时保持与完全微调和参数高效微调相当的性能。


<details>
  <summary>Details</summary>
Motivation: 方言识别（DI）任务可以通过识别同一语言中的不同方言来改善下游语音相关任务。然而，针对DI等任务对语音模型进行微调在计算成本和内存需求方面代价高昂。现有的参数高效微调（PEFT）方法虽然参数效率高，但在内存效率和训练速度方面改进有限。

Method: 本研究探索了内存高效微调（MEFT）方法，并将其应用于通用预训练语音模型。通过多种MEFT方法，全面分析GPU内存使用和微调速度。以Whisper模型为例，对KeSpeech数据集中的六种普通话次方言进行微调。

Result: 在KeSpeech数据集上的实验表明，该方法最多可以减少73.25%的GPU内存使用，并将训练速度加快2.1倍，同时保持与完全微调和PEFT方法相当的准确率。

Conclusion: MEFT方法在显著减少内存使用和加快训练速度的同时，能够保持与完全微调和PEFT方法相当的性能，为高效微调语音模型提供了可行的解决方案。

Abstract: Dialect Identification (DI) is a task to recognize different dialects within the same language from a speech signal. DI can help to improve the downstream speech related tasks even when speakers have a strong dialect. However, fine-tuning a speech model for tasks like DI is expensive in terms of computation cost and memory requirement. Recent studies have explored fine-tuning pre-trained speech models for tasks like DI using Parameter-Efficient Fine-Tuning (PEFT) methods, which offer parameter efficiency but limited improvement in memory efficiency and training speed. To address these challenges, we explore Memory-Efficient Fine-Tuning (MEFT) methods, originally proposed for language processing, and apply them to the general-purpose pre-trained speech model. We then comprehensively analyze the GPU memory usage and fine-tuning speed based on various MEFT methods. As a case study, we fine-tune the Whisper model to identify six Mandarin subdialects from the KeSpeech dataset, reducing GPU memory usage by up to 73.25% and accelerating training speed by a factor of 2.1, while maintaining accuracy comparable to vanilla fine-tuning and PEFT methods.

</details>


### [5] [Feature Selection Empowered BERT for Detection of Hate Speech with Vocabulary Augmentation](https://arxiv.org/abs/2512.02141)
*Pritish N. Desai,Tanay Kewalramani,Srimanta Mandal*

Main category: cs.CL

TL;DR: 本文提出了一种通过减少训练集大小并增强分词器以适应新出现的仇恨言论术语，来微调BERT进行仇恨言论分类的数据高效策略。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上滥用言论的不断出现和演变，特别是新出现的俚语和模糊术语，对检测系统构成了持续挑战。

Method: 采用TF-IDF样本选择机制，保留最具信息量的75%的样本，并通过增加特定领域俚语和词汇变体来增强分词器。

Result: 在广泛使用的仇恨言论数据集上的实验表明，该方法在提高计算效率的同时，实现了具有竞争力的性能。

Conclusion: 该方法在可扩展和适应性滥用内容审核方面具有潜力。

Abstract: Abusive speech on social media poses a persistent and evolving challenge, driven by the continuous emergence of novel slang and obfuscated terms designed to circumvent detection systems. In this work, we present a data efficient strategy for fine tuning BERT on hate speech classification by significantly reducing training set size without compromising performance. Our approach employs a TF IDF-based sample selection mechanism to retain only the most informative 75 percent of examples, thereby minimizing training overhead. To address the limitations of BERT's native vocabulary in capturing evolving hate speech terminology, we augment the tokenizer with domain-specific slang and lexical variants commonly found in abusive contexts. Experimental results on a widely used hate speech dataset demonstrate that our method achieves competitive performance while improving computational efficiency, highlighting its potential for scalable and adaptive abusive content moderation.

</details>


### [6] [A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation](https://arxiv.org/abs/2512.02195)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 本文介绍MODOMA系统，一个用于无监督语言习得实验的多智能体计算实验室环境。


<details>
  <summary>Details</summary>
Motivation: 进行无监督语言习得研究，通过两个语言模型（成人和儿童智能体）的交互来获取语言知识。

Method: 采用统计和基于规则的方法，通过成人智能体和儿童智能体之间的互动进行语言习得实验。

Result: 实验表明，儿童智能体能够习得并表征功能和内容类别，并且结果与人工数据生成的模式相似。

Conclusion: MODOMA方法在模拟语言习得方面是有效的，为计算语言习得实验提供了新的可能性。

Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.

</details>


### [7] [Swivuriso: The South African Next Voices Multilingual Speech Dataset](https://arxiv.org/abs/2512.02201)
*Vukosi Marivatee,Kayode Olaleye,Sitwala Mundia,Andinda Bakainga,Unarine Netshifhefhe,Mahmooda Milanzie,Tsholofelo Hope Mogale,Thapelo Sindane,Zainab Abdulrasaq,Kesego Mokgosi,Chijioke Okorie,Nia Zion Van Wyk,Graham Morrissey,Dale Dunbar,Francois Smit,Tsosheletso Chidi,Rooweither Mabuya,Andiswa Bukula,Respect Mlambo,Tebogo Macucwa,Idris Abdulmumin,and Seani Rananga*

Main category: cs.CL

TL;DR: 本文介绍了Swivuriso，一个3000小时的多语言语音数据集，旨在支持和评估七种南非语言的自动语音识别（ASR）技术。


<details>
  <summary>Details</summary>
Motivation: Swivuriso旨在解决现有ASR数据集在南非语言中的显著空白，支持农业、医疗和一般领域的多语言语音识别技术发展。

Method: 描述了数据集的设计原则、伦理考虑和数据收集过程，并介绍了使用该数据集进行ASR模型训练和微调的基线结果。

Result: 展示了使用Swivuriso数据集进行ASR模型训练和微调的基线结果，并与其他相关语言的ASR数据集进行了比较。

Conclusion: Swivuriso为南非语言的ASR技术发展提供了一个重要资源，有助于填补现有数据集的空白，并提升相关模型的性能。

Abstract: This paper introduces Swivuriso, a 3000-hour multilingual speech dataset developed as part of the African Next Voices project, to support the development and benchmarking of automatic speech recognition (ASR) technologies in seven South African languages. Covering agriculture, healthcare, and general domain topics, Swivuriso addresses significant gaps in existing ASR datasets. We describe the design principles, ethical considerations, and data collection procedures that guided the dataset creation. We present baseline results of training/finetuning ASR models with this data and compare to other ASR datasets for the langauges concerned.

</details>


### [8] [Lightweight Latent Reasoning for Narrative Tasks](https://arxiv.org/abs/2512.02240)
*Alexander Gurung,Nikolay Malkin,Mirella Lapata*

Main category: cs.CL

TL;DR: LiteReason是一种新的潜在推理方法，可以与标准标记采样和强化学习技术结合，通过轻量级Reasoning Projector模块，帮助模型跳过一些推理步骤，从而减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通过生成长的思维链或“推理轨迹”来处理复杂任务，但这一过程计算成本很高，尤其是在涉及大量标记的叙事相关任务中。因此，作者提出了LiteReason方法以提高计算效率。

Method: LiteReason使用一个轻量级的Reasoning Projector模块，该模块被训练为生成连续潜在标记，以帮助模型“跳过”推理步骤。在强化学习过程中，策略模型决定何时激活投影仪，根据需要切换潜在和离散推理。

Result: 在情节漏洞检测和书籍章节生成任务上的实验结果表明，LiteReason优于潜在推理基线，并且接近非潜在强化学习训练的效果，同时将最终推理长度减少了77-92%。

Conclusion: LiteReason引导强化学习训练到性能和计算折衷曲线的更高效部分，从而显著降低计算成本，同时保持高性能。

Abstract: Large language models (LLMs) tackle complex tasks by generating long chains of thought or "reasoning traces" that act as latent variables in the generation of an output given a query. A model's ability to generate such traces can be optimized with reinforcement learning (RL) to improve their utility in predicting an answer. This optimization comes at a high computational cost, especially for narrative-related tasks that involve retrieving and processing many tokens. To this end, we propose LiteReason, a latent reasoning method that can be interleaved with standard token sampling and easily combined with RL techniques. LiteReason employs a lightweight Reasoning Projector module, trained to produce continuous latent tokens that help the model 'skip' reasoning steps. During RL, the policy model decides when to activate the projector, switching between latent and discrete reasoning as needed. Experimental results on plot hole detection and book chapter generation show that our method outperforms latent reasoning baselines and comes close to matching non-latent RL training, while reducing final reasoning length by 77-92%. Overall, LiteReason guides RL training to a more efficient part of the performance-computation tradeoff curve.

</details>


### [9] [DETAIL Matters: Measuring the Impact of Prompt Specificity on Reasoning in Large Language Models](https://arxiv.org/abs/2512.02246)
*Olivia Kim*

Main category: cs.CL

TL;DR: 本文提出DETAIL框架，评估提示特异性对大模型推理性能的影响，发现更具体的提示能提升准确性，尤其是对小模型和程序性任务。


<details>
  <summary>Details</summary>
Motivation: 提示设计对大模型推理性能至关重要，但提示特异性（详细或模糊程度）的影响尚未被充分研究。

Method: 使用GPT-4生成多级提示，通过困惑度量化特异性，采用基于GPT的语义等价性评估正确性，在GPT-4和O3-mini的30个新推理任务上实验。

Result: 提示特异性提升准确性，尤其对较小模型和程序性任务；揭示了适应性提示策略的必要性。

Conclusion: 提示特异性对性能有显著影响，应开发自适应提示策略，并提供了支持后续研究的工具和数据。

Abstract: Prompt design plays a critical role in the reasoning performance of large language models (LLMs), yet the impact of prompt specificity - how detailed or vague a prompt is - remains understudied. This paper introduces DETAIL, a framework for evaluating LLM performance across varying levels of prompt specificity. We generate multi-level prompts using GPT-4, quantify specificity via perplexity, and assess correctness using GPT-based semantic equivalence. Experiments on 30 novel reasoning tasks across GPT-4 and O3-mini reveal that specificity improves accuracy, especially for smaller models and procedural tasks. Our results highlight the need for adaptive prompting strategies and provide tools and data to support further research.

</details>


### [10] [HealthContradict: Evaluating Biomedical Knowledge Conflicts in Language Models](https://arxiv.org/abs/2512.02299)
*Boya Zhang,Alban Bornet,Rui Yang,Nan Liu,Douglas Teodoro*

Main category: cs.CL

TL;DR: 本文通过HealthContradict数据集评估语言模型在长且矛盾的医学语境下的推理能力，并考察了不同提示设置对模型输出的影响。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型如何利用上下文信息回答健康问题，以及它们的回答如何受到矛盾上下文的影响。

Method: 使用HealthContradict数据集，包括920个独特实例，每个实例包含一个与健康相关的问题、基于科学证据的事实性答案和两个持矛盾立场的文档。通过不同的提示设置，包括正确、错误或矛盾语境，并测量它们对模型输出的影响。

Result: 实验表明，调优后的生物医学语言模型的优势不仅在于预训练的参数知识，还体现在利用正确语境并抵制错误语境的能力。

Conclusion: HealthContradict为评估语言模型的上下文推理能力提供了比现有医学问答评测基准更精细的区分。

Abstract: How do language models use contextual information to answer health questions? How are their responses impacted by conflicting contexts? We assess the ability of language models to reason over long, conflicting biomedical contexts using HealthContradict, an expert-verified dataset comprising 920 unique instances, each consisting of a health-related question, a factual answer supported by scientific evidence, and two documents presenting contradictory stances. We consider several prompt settings, including correct, incorrect or contradictory context, and measure their impact on model outputs. Compared to existing medical question-answering evaluation benchmarks, HealthContradict provides greater distinctions of language models' contextual reasoning capabilities. Our experiments show that the strength of fine-tuned biomedical language models lies not only in their parametric knowledge from pretraining, but also in their ability to exploit correct context while resisting incorrect context.

</details>


### [11] [When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers](https://arxiv.org/abs/2512.02304)
*Jack Lu,Ryan Teehan,Jinran Jin,Mengye Ren*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可以同时充当问题解决者和解决方案验证者，验证者通过从候选答案池中选择高质量答案来提高解决者的性能。本文系统性地研究了37个模型，涵盖了多个家族、规模和基础与后训练变体，评估了9个逻辑推理、结构化谜题、符号计算、数学、常识、事实回忆和领域知识的基准。研究比较了同一家族内的自我验证和跨家族的验证，并引入了验证者增益（verifier gain）这一指标，预测基于验证器的测试时间拒绝采样带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注自我验证，很少探讨验证者如何判断同一家族或不同家族模型的输出，且后训练对验证的影响尚不清楚。本研究旨在填补这一空白，系统分析不同模型家族、规模和训练方法对验证性能的影响。

Method: 研究涵盖了37个模型，包括不同家族、规模和基础与后训练变体，评估了9个不同领域的基准。比较了自我验证和跨家族验证，并引入了验证者增益（verifier gain）指标，分析了验证者增益和假阳性率如何随模型大小和后训练变化，并研究了数据集的可验证性差异。

Result: 研究发现跨家族验证特别有效；后训练减少自我改进但增强跨家族改进；数学和逻辑任务表现出最高的内在可验证性。验证者增益指标能够有效预测验证器带来的性能提升。

Conclusion: 跨家族验证在提高性能方面特别有效，后训练虽然减少自我改进，但增强了跨家族改进。数学和逻辑任务具有较高的可验证性，验证者增益是预测性能提升的有效指标。

Abstract: Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.

</details>


### [12] [A Concise Review of Hallucinations in LLMs and their Mitigation](https://arxiv.org/abs/2512.02527)
*Parth Pulkundwar,Vivek Dhanawade,Rohit Yadav,Minal Sonkar,Medha Asurlekar,Sarita Rathod*

Main category: cs.CL

TL;DR: 本文总结了语言模型中幻觉的类型、起源及缓解方法，为理解与减少幻觉提供了全面资源。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的幻觉对自然语言处理领域构成挑战，因此理解和减少幻觉至关重要。

Method: 通过文献综述，提供关于幻觉类型、起源及缓解方法的简明总结。

Result: 本文成为理解幻觉现象和如何缓解的综合性一站式资源。

Conclusion: 总结和分析了幻觉问题，并提出了缓解策略，为自然语言处理领域的研究提供参考。

Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.

</details>


### [13] [What Signals Really Matter for Misinformation Tasks? Evaluating Fake-News Detection and Virality Prediction under Real-World Constraints](https://arxiv.org/abs/2512.02552)
*Francesco Paolo Savatteri,Chahan Vidal-Gorène,Florian Cafiero*

Main category: cs.CL

TL;DR: 本文评估了在线错误信息的两项任务：假新闻检测和病毒式传播预测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在操作性环境中快速反应，对假新闻和病毒式传播进行预测。

Method: 使用EVONS和FakeNewsNet数据集，比较了文本嵌入（RoBERTa和Mistral）、轻量级数字特征（时间、粉丝数、验证、点赞）和序列模型（GRU、门控架构、Transformer编码器）。

Result: 文本内容对假新闻检测是强判别器，而数字特征在语言模型不可用或计算受限的情况下仍可行；病毒式传播预测更难，对标签构建高度敏感。

Conclusion: 假新闻检测中，文本特征表现良好；而病毒式传播预测需要非线性结构特征，并讨论了评估设计和可重复性限制的影响。

Abstract: We present an evaluation-driven study of two practical tasks regarding online misinformation: (i) fake-news detection and (ii) virality prediction in the context of operational settings, with the necessity for rapid reaction. Using the EVONS and FakeNewsNet datasets, we compare textual embeddings (RoBERTa; with a control using Mistral) against lightweight numeric features (timing, follower counts, verification, likes) and sequence models (GRU, gating architectures, Transformer encoders). We show that textual content alone is a strong discriminator for fake-news detection, while numeric-only pipelines remain viable when language models are unavailable or compute is constrained. Virality prediction is markedly harder than fake-news detection and is highly sensitive to label construction; in our setup, a median-based ''viral'' split (<50 likes) is pragmatic but underestimates real-world virality, and time-censoring for engagement features is desirable yet difficult under current API limits. Dimensionality-reduction analyses suggest non-linear structure is more informative for virality than for fake-news detection (t-SNE > PCA on numeric features). Swapping RoBERTa for Mistral embeddings yields only modest deltas, leaving conclusions unchanged. We discuss implications for evaluation design and report reproducibility constraints that realistically affect the field. We release splits and code where possible and provide guidance for metric selection.

</details>


### [14] [ADORE: Autonomous Domain-Oriented Relevance Engine for E-commerce](https://arxiv.org/abs/2512.02555)
*Zheng Fang,Donghao Xie,Ming Pang,Chunyuan Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo*

Main category: cs.CL

TL;DR: 提出ADORE框架，通过三个创新模块解决电商搜索中的相关性建模问题。


<details>
  <summary>Details</summary>
Motivation: 电商搜索中的相关性建模面临术语匹配的语义差距和神经模型对特定领域硬样本稀缺的挑战。

Method: ADORE框架包含：(1) 规则感知相关性判别模块；(2) 错误类型感知数据合成模块；(3) 关键属性增强知识蒸馏模块。

Result: 通过大规模实验和在线A/B测试验证了ADORE的有效性。

Conclusion: ADORE框架为工业应用中的资源高效、认知一致的相关性建模建立了新范式。

Abstract: Relevance modeling in e-commerce search remains challenged by semantic gaps in term-matching methods (e.g., BM25) and neural models' reliance on the scarcity of domain-specific hard samples. We propose ADORE, a self-sustaining framework that synergizes three innovations: (1) A Rule-aware Relevance Discrimination module, where a Chain-of-Thought LLM generates intent-aligned training data, refined via Kahneman-Tversky Optimization (KTO) to align with user behavior; (2) An Error-type-aware Data Synthesis module that auto-generates adversarial examples to harden robustness; and (3) A Key-attribute-enhanced Knowledge Distillation module that injects domain-specific attribute hierarchies into a deployable student model. ADORE automates annotation, adversarial generation, and distillation, overcoming data scarcity while enhancing reasoning. Large-scale experiments and online A/B testing verify the effectiveness of ADORE. The framework establishes a new paradigm for resource-efficient, cognitively aligned relevance modeling in industrial applications.

</details>


### [15] [From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks](https://arxiv.org/abs/2512.02580)
*Changpeng Yang,Jinyang Wu,Yuchen Liu,Shuai Zhang,Yang Li,Qiliang Liang,Hongzhen Wang,Shuai Nie,Jiaming Xu,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于优势信号的自适应课程机制CAPO，通过先正后负的训练信号提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练中无差别混合正负信号，导致指导模糊和收益有限，需要一种更清晰的训练信号调度策略。

Method: CAPO机制首先使用仅有正向优势的信号进行模仿学习以建立稳健基础，然后逐步引入负向信号以培养判别能力。

Result: 在数学推理任务中实现稳定且显著的改进，并在多模态GUI推理场景中有效泛化。

Conclusion: CAPO是一种兼容多种优化方法（如GRPO、PPO等）的通用且稳健的优化框架。

Abstract: Reinforcement learning has emerged as a paradigm for post-training large language models, boosting their reasoning capabilities. Such approaches compute an advantage value for each sample, reflecting better or worse performance than expected, thereby yielding both positive and negative signals for training. However, the indiscriminate mixing of the two signals in existing methods, especially from the early stages, may lead to ambiguous guidance and limited gains. To address this issue, we propose **CAPO** (**C**urriculum **A**dvantage **P**olicy **O**ptimization), an adaptive curriculum mechanism based on advantage signals. The proposed mechanism bootstraps imitation learning with positive-only advantage samples to establish robust foundations, and subsequently introduces negative signals to cultivate discriminative capabilities, thereby improving generalization across complex scenarios. Compatible with diverse optimization methods including GRPO, PPO, RLOO, and Reinforce++, our method consistently achieves stable and significant improvements in mathematical reasoning tasks, and further generalizes effectively to multimodal Graphical User Interface (GUI) reasoning scenarios, establishing itself as a versatile and robust optimization framework.

</details>


### [16] [An Empirical Survey of Model Merging Algorithms for Social Bias Mitigation](https://arxiv.org/abs/2512.02689)
*Daiki Shirafuji,Tatsuhiko Saito,Yasutomo Kimura*

Main category: cs.CL

TL;DR: 本文调查了七种模型合并算法在减轻大型语言模型社会偏见方面的效果，并评估了其对下游任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLMs) 会继承并放大其预训练语料库中的社会偏见，这威胁到公平性和社会信任。尽管已有研究探索通过模型合并来 “编辑” LLM 参数以减轻社会偏见，但缺乏经验性比较。

Method: 本研究实证调查了 Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, 和 Nearswap 七种算法，在 GPT, LLaMA, 和 Qwen 家族的 13 个开放权重模型上应用。使用 BBQ, BOLD, 和 HONEST 三个偏见数据集进行综合评估，并测量了这些技术对 SuperGLUE 基准下游任务性能的影响。

Result: 研究发现偏见减少和下游性能之间存在权衡：实现更大偏见缓解的方法会降低准确性，尤其是在需要阅读理解、常识和因果推理的任务上。在合并算法中，Linear, SLERP, 和 Nearswap 在不降低整体性能的情况下持续减少偏见，其中 SLERP 在中等插值权重时成为最平衡的选择。

Conclusion: 模型合并算法在减轻偏见方面具有潜力，但过度去偏或不当的合并方法可能导致重要的语言能力下降。

Abstract: Large language models (LLMs) are known to inherit and even amplify societal biases present in their pre-training corpora, threatening fairness and social trust. To address this issue, recent work has explored ``editing'' LLM parameters to mitigate social bias with model merging approaches; however, there is no empirical comparison. In this work, we empirically survey seven algorithms: Linear, Karcher Mean, SLERP, NuSLERP, TIES, DELLA, and Nearswap, applying 13 open weight models in the GPT, LLaMA, and Qwen families. We perform a comprehensive evaluation using three bias datasets (BBQ, BOLD, and HONEST) and measure the impact of these techniques on LLM performance in downstream tasks of the SuperGLUE benchmark. We find a trade-off between bias reduction and downstream performance: methods achieving greater bias mitigation degrade accuracy, particularly on tasks requiring reading comprehension and commonsense and causal reasoning. Among the merging algorithms, Linear, SLERP, and Nearswap consistently reduce bias while maintaining overall performance, with SLERP at moderate interpolation weights emerging as the most balanced choice. These results highlight the potential of model merging algorithms for bias mitigation, while indicating that excessive debiasing or inappropriate merging methods may lead to the degradation of important linguistic abilities.

</details>


### [17] [CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer](https://arxiv.org/abs/2512.02711)
*Lavish Bansal,Naman Mishra*

Main category: cs.CL

TL;DR: 本文介绍了一种名为CREST的参数高效多语言安全分类模型，支持100种语言，参数仅为0.5B。


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏主要针对高资源语言，低资源语言用户被忽视，需要一种能够覆盖多语言的安全解决方案。

Method: 通过在13种高资源语言上进行训练，利用基于聚类的跨语言迁移，实现向100种语言的有效泛化。

Result: 在六个安全基准上，CREST优于同规模的现有最佳护栏，并与参数显著更多的模型取得了竞争性的结果。

Conclusion: 语言特定的护栏存在局限性，开发通用的、与语言无关的安全系统至关重要，以实现全球范围的有效扩展。

Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.

</details>


### [18] [Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs](https://arxiv.org/abs/2512.02719)
*Julian Ma,Jun Wang,Zafeirios Fountas*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）在显式推理之外是否具备类似人类的贝叶斯最优多模态信号整合能力，通过构建心理学启发的行为基准BayesBench，揭示了模型能力与策略间的关键差异。


<details>
  <summary>Details</summary>
Motivation: 人类在感知任务中能直觉性地运用近似最优的贝叶斯策略整合多模态噪声信号，而LLMs的隐式计算策略尚未被系统研究，现有性能导向的评估可能忽略了模型处理不确定性的稳健性。

Method: 采用心理学范式，设计包含文本和图像的长度、位置、距离、持续时间四类数量估计任务的BayesBench基准，通过控制噪声、上下文和指令提示的消融实验，评估9种LLMs与人类行为的一致性，并引入贝叶斯一致性分数（BCS）量化模型行为的贝叶斯特性。

Result: GPT-5 Mini等模型虽在文本任务中达到完美准确率，但无法有效整合视觉线索；模型行为存在能力与策略的分离现象，即高精度不保证对不确定性的稳健处理，部分模型展现出与人类相似的贝叶斯一致性行为。

Conclusion: LLMs可自发产生基于原则的不确定性处理机制，但现有以准确率为中心的评估体系可能高估实际性能而忽略脆弱性；提出的行为基准和一致性指标有助于未来多模态架构设计，强调需同时评估性能与计算策略。

Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.

</details>


### [19] [SurveyEval: Towards Comprehensive Evaluation of LLM-Generated Academic Surveys](https://arxiv.org/abs/2512.02763)
*Jiahao Zhao,Shuaixing Zhang,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: 本文介绍了SurveyEval，这是一个用于评估基于LLM的自动调查系统的综合基准测试，通过三个维度：整体质量、大纲连贯性和参考准确性来进行评价。


<details>
  <summary>Details</summary>
Motivation: 随着LLM-based自动调查系统变得越来越复杂，如何评估这些系统成为一项重要挑战，现有方法未能充分解决这一问题。

Method: 提出SurveyEval，通过扩展7个主题，并使用LLM-as-a-Judge框架结合人类参考，来评估自动生成的调查报告。

Result: 通用长文本或论文写作系统生成的调查报告质量较低，而专门的调查生成系统能够产生较高质量的结果。

Conclusion: SurveyEval作为一个可扩展的测试平台，有助于理解和改进跨不同主题和评估标准的自动调查系统。

Abstract: LLM-based automatic survey systems are transforming how users acquire information from the web by integrating retrieval, organization, and content synthesis into end-to-end generation pipelines. While recent works focus on developing new generation pipelines, how to evaluate such complex systems remains a significant challenge. To this end, we introduce SurveyEval, a comprehensive benchmark that evaluates automatically generated surveys across three dimensions: overall quality, outline coherence, and reference accuracy. We extend the evaluation across 7 subjects and augment the LLM-as-a-Judge framework with human references to strengthen evaluation-human alignment. Evaluation results show that while general long-text or paper-writing systems tend to produce lower-quality surveys, specialized survey-generation systems are able to deliver substantially higher-quality results. We envision SurveyEval as a scalable testbed to understand and improve automatic survey systems across diverse subjects and evaluation criteria.

</details>


### [20] [PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](https://arxiv.org/abs/2512.02764)
*Robert Belanec,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: PEFT-Factory是一个统一的框架，用于使用现成和自定义的PEFT方法高效地微调大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决新引入的PEFT方法难以复制、部署或相互比较的问题。

Method: 模块化设计，支持19种PEFT方法、27个分类和文本生成数据集，以及标准和PEFT特定的评估指标。

Result: 提供了一个即用、受控和稳定的环境，提高PEFT方法的可复制性和基准测试。

Conclusion: PEFT-Factory源自LLaMA-Factory，是一个下游框架，公开可用，提升了PEFT方法在实践中的可行性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods address the increasing size of Large Language Models (LLMs). Currently, many newly introduced PEFT methods are challenging to replicate, deploy, or compare with one another. To address this, we introduce PEFT-Factory, a unified framework for efficient fine-tuning LLMs using both off-the-shelf and custom PEFT methods. While its modular design supports extensibility, it natively provides a representative set of 19 PEFT methods, 27 classification and text generation datasets addressing 12 tasks, and both standard and PEFT-specific evaluation metrics. As a result, PEFT-Factory provides a ready-to-use, controlled, and stable environment, improving replicability and benchmarking of PEFT methods. PEFT-Factory is a downstream framework that originates from the popular LLaMA-Factory, and is publicly available at https://github.com/kinit-sk/PEFT-Factory

</details>


### [21] [Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](https://arxiv.org/abs/2512.02772)
*Weihang Su,Jianming Long,Changyue Wang,Shiyu Lin,Jingyan Xu,Ziyi Ye,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: 本文提出了一种统一的评估框架UniFact，用于直接比较幻觉检测（HD）和事实核查（FV）两种方法，揭示了它们的互补性并展示了混合方法的优势。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）经常出现幻觉，即生成内容流畅但不真实。现有研究分为模型为中心的幻觉检测（HD）和以文本为中心的事实核查（FV），但两者独立发展，阻碍了共同进步。

Method: 引入了UniFact框架，通过动态生成模型输出和事实性标签，进行大规模实验，比较不同LLM家族和检测方法。

Result: （1）没有一种方法在所有情况下都更优；（2）HD和FV捕捉事实错误的不同方面；（3）混合方法始终实现最先进的性能。

Conclusion: 需要新的整合研究议程，以统一LLM中的幻觉检测和事实核查，提高模型输出的事实准确性。

Abstract: Large Language Models (LLMs) frequently exhibit hallucinations, generating content that appears fluent and coherent but is factually incorrect. Such errors undermine trust and hinder their adoption in real-world applications. To address this challenge, two distinct research paradigms have emerged: model-centric Hallucination Detection (HD) and text-centric Fact Verification (FV). Despite sharing the same goal, these paradigms have evolved in isolation, using distinct assumptions, datasets, and evaluation protocols. This separation has created a research schism that hinders their collective progress. In this work, we take a decisive step toward bridging this divide. We introduce UniFact, a unified evaluation framework that enables direct, instance-level comparison between FV and HD by dynamically generating model outputs and corresponding factuality labels. Through large-scale experiments across multiple LLM families and detection methods, we reveal three key findings: (1) No paradigm is universally superior; (2) HD and FV capture complementary facets of factual errors; and (3) hybrid approaches that integrate both methods consistently achieve state-of-the-art performance. Beyond benchmarking, we provide the first in-depth analysis of why FV and HD diverged, as well as empirical evidence supporting the need for their unification. The comprehensive experimental results call for a new, integrated research agenda toward unifying Hallucination Detection and Fact Verification in LLMs.
  We have open-sourced all the code, data, and baseline implementation at: https://github.com/oneal2000/UniFact/

</details>


### [22] [Making Dialogue Grounding Data Rich: A Three-Tier Data Synthesis Framework for Generalized Referring Expression Comprehension](https://arxiv.org/abs/2512.02791)
*Juexi Shao,Siyou Li,Yujian Gan,Chris Madge,Vanja Karan,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出了一种三层数据合成方法，以提高对话基础广义指代表达理解（GREC）模型在训练和评估领域之间分布转移下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的系统在训练和评估领域之间的分布转移方面表现不佳，并且标注的对话基础数据稀缺，这加剧了问题。

Method: 提出了一种三层数据合成方法，以平衡真实性和可控性，为对话条件基础生成可扩展的监督数据，并在合成数据上进行微调。

Result: 在标准评估指标下，与先前的方法相比，该方法带来了持续且显著的改进。

Conclusion: 通过三层数据合成方法生成的数据可以有效提高对话基础广义指代表达理解模型的性能，特别是在分布转移情况下。

Abstract: Dialogue-Based Generalized Referring Expressions Comprehension (GREC) requires models to ground the expression and unlimited targets in complex visual scenes while resolving coreference across a long dialogue context. However, existing systems struggle under distribution shift between training and evaluation domains, a gap exacerbated by the scarcity of annotated dialogue grounding data. We address this challenge with a three-tier data-synthesis method that balances realism and controllability to produce scalable supervision for dialogue-conditioned grounding. Fine-tuning on the synthesized data yields consistent, substantial improvements over prior approaches across standard evaluation metrics.

</details>


### [23] [TriLex: A Framework for Multilingual Sentiment Analysis in Low-Resource South African Languages](https://arxiv.org/abs/2512.02799)
*Mike Nkongolo,Hilton Vorster,Josh Warren,Trevor Naick,Deandre Vanmali,Masana Mashapha,Luke Brand,Alyssa Fernandes,Janco Calitz,Sibusiso Makhoba*

Main category: cs.CL

TL;DR: 提出TriLex框架，通过三阶段增强低资源非洲语言情感词典，提升多语言NLP性能。


<details>
  <summary>Details</summary>
Motivation: 低资源非洲语言在情感分析中代表性不足，限制了多语言NLP系统的词典覆盖率和性能。

Method: 采用三阶段检索增强框架TriLex，包括基于语料库的提取、跨语言映射和检索增强生成的词典精炼。

Result: AfroXLMR在情感分析中表现优于AfriBERTa，F1分数超过80%，而AfriBERTa在目标语言上未预训练，仍达到64%的F1分数。

Conclusion: TriLex是扩展低资源南非语言情感词典和进行情感建模的可扩展且有效的框架。

Abstract: Low-resource African languages remain underrepresented in sentiment analysis, limiting both lexical coverage and the performance of multilingual Natural Language Processing (NLP) systems. This study proposes TriLex, a three-stage retrieval augmented framework that unifies corpus-based extraction, cross lingual mapping, and retrieval augmented generation (RAG) driven lexical refinement to systematically expand sentiment lexicons for low-resource languages. Using the enriched lexicon, the performance of two prominent African pretrained language models (AfroXLMR and AfriBERTa) is evaluated across multiple case studies. Results demonstrate that AfroXLMR delivers superior performance, achieving F1-scores above 80% for isiXhosa and isiZulu and exhibiting strong cross-lingual stability. Although AfriBERTa lacks pre-training on these target languages, it still achieves reliable F1-scores around 64%, validating its utility in computationally constrained settings. Both models outperform traditional machine learning baselines, and ensemble analyses further enhance precision and robustness. The findings establish TriLex as a scalable and effective framework for multilingual sentiment lexicon expansion and sentiment modeling in low-resource South African languages.

</details>


### [24] [SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment](https://arxiv.org/abs/2512.02807)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: 提出了一种无需外部标注的LLM质量评估方法stable rank，并基于此设计了SR-GRPO强化学习框架，在多项任务上超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在人工标注稀缺主观、奖励模型易受攻击、自评估存在偏差等根本性缺陷，需要开发内在的质量评估信号。

Method: 1) 提出stable rank指标：通过计算隐藏状态总方差与主方向方差的比值，衡量信息在表示维度中的分布质量；2) 设计SR-GRPO框架：将stable rank作为强化学习的奖励信号。

Result: 1) stable rank在RewardBench上达到84.04%准确率；2) Best-of-N采样提升任务准确率11.3%；3) SR-GRPO使Qwen2.5-1.5B-Instruct在STEM提升10%，数学推理提升19%。

Conclusion: 模型内部几何结构可以提取有效的质量信号，为无外部监督的可扩展对齐提供了新范式。

Abstract: Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.

</details>


### [25] [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/abs/2512.02816)
*Kunning Li,Jianbin Guo,Zhaoyang Shang,Yiqing Liu,Hongmin Du,Lingling Liu,Yuping Zhao,Lifeng Dong*

Main category: cs.CL

TL;DR: 提出了一个新的综合性临床案例基准TCM-BEST4SDT，用于评估大型语言模型在传统中医领域的临床能力，包括四个任务，并采用三种评估机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在传统中医领域的应用能力急需评估，但现有基准局限于知识问答或辨证准确性，忽视了治疗决策的评估。

Method: 提出了一个由中医专家主导的综合临床案例基准TCM-BEST4SDT，包含四个任务，采用严格的数据标注流程，并结合三种评估机制。

Result: 通过15个主流大型语言模型的实验验证了TCM-BEST4SDT的有效性，并公开了该基准以促进中医智能化研究的发展。

Conclusion: TCM-BEST4SDT为大型语言模型在传统中医领域的评估提供了新的基准，并推动了中医智能化研究的发展。

Abstract: The emergence of Large Language Models (LLMs) within the Traditional Chinese Medicine (TCM) domain presents an urgent need to assess their clinical application capabilities. However, such evaluations are challenged by the individualized, holistic, and diverse nature of TCM's "Syndrome Differentiation and Treatment" (SDT). Existing benchmarks are confined to knowledge-based question-answering or the accuracy of syndrome differentiation, often neglecting assessment of treatment decision-making. Here, we propose a comprehensive, clinical case-based benchmark spearheaded by TCM experts, and a specialized reward model employed to quantify prescription-syndrome congruence. Data annotation follows a rigorous pipeline. This benchmark, designated TCM-BEST4SDT, encompasses four tasks, including TCM Basic Knowledge, Medical Ethics, LLM Content Safety, and SDT. The evaluation framework integrates three mechanisms, namely selected-response evaluation, judge model evaluation, and reward model evaluation. The effectiveness of TCM-BEST4SDT was corroborated through experiments on 15 mainstream LLMs, spanning both general and TCM domains. To foster the development of intelligent TCM research, TCM-BEST4SDT is now publicly available.

</details>


### [26] [Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs](https://arxiv.org/abs/2512.00663)
*Tanmay Agrawal*

Main category: cs.CL

TL;DR: 本文介绍了一个通过交互式可视化知识图谱减少大语言模型幻觉的框架。


<details>
  <summary>Details</summary>
Motivation: 企业环境中，大模型在有限上下文和知识不一致时常产生难以察觉的幻觉，现有缓解策略缺乏确定性保障。

Method: 提出将专有知识与模型生成内容组织成交互式可视化知识图谱，将模型断言与真实来源链接，并标注置信度，用户可诊断不一致、提供反馈。

Result: 实现了人机协同工作流，通过结构化反馈循环提升模型可靠性与响应质量。

Conclusion: 该框架通过可视化手段增强用户对模型输出的理解与控制，为减少幻觉提供了一种实用、可扩展的解决方案。

Abstract: Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.

</details>


### [27] [BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion](https://arxiv.org/abs/2512.02817)
*Sai Koneru,Fabian Retkowski,Christian Huber,Lukas Hilgert,Seymanur Akti,Enes Yavuz Ugan,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出BOOM，一个多模态多语言讲座伴侣，能够同步翻译讲座音频和幻灯片，生成翻译文本、本地化幻灯片和合成语音，以解决教育内容本地化的挑战。


<details>
  <summary>Details</summary>
Motivation: 全球化教育和在线学习的快速增长使得教育内容的本地化成为一个关键挑战，需要系统能够处理多种输入模式，以提供全面且可访问的学习体验。

Method: BOOM系统采用端到端方法，联合翻译讲座音频和幻灯片，生成同步的三模态输出：翻译文本、保留视觉元素的本地化幻灯片和合成语音。

Result: 实验表明，幻灯片感知的翻译对摘要和问答等下游任务也有级联好处。

Conclusion: BOOM通过保留原始内容的多模态，使学生能够以母语访问讲座，同时保持内容的完整性。

Abstract: The globalization of education and rapid growth of online learning have made localizing educational content a critical challenge. Lecture materials are inherently multimodal, combining spoken audio with visual slides, which requires systems capable of processing multiple input modalities. To provide an accessible and complete learning experience, translations must preserve all modalities: text for reading, slides for visual understanding, and speech for auditory learning. We present \textbf{BOOM}, a multimodal multilingual lecture companion that jointly translates lecture audio and slides to produce synchronized outputs across three modalities: translated text, localized slides with preserved visual elements, and synthesized speech. This end-to-end approach enables students to access lectures in their native language while aiming to preserve the original content in its entirety. Our experiments demonstrate that slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering. We release our Slide Translation code at https://github.com/saikoneru/image-translator and integrate it in Lecture Translator at https://gitlab.kit.edu/kit/isl-ai4lt/lt-middleware/ltpipeline}\footnote{All released code and models are licensed under the MIT License.

</details>


### [28] [promptolution: A Unified, Modular Framework for Prompt Optimization](https://arxiv.org/abs/2512.02840)
*Tom Zehle,Timo Heiß,Moritz Schlager,Matthias Aßenmacher,Matthias Feurer*

Main category: cs.CL

TL;DR: 本文介绍了一个名为 promptolution 的开源框架，旨在简化和统一提示优化的实现。


<details>
  <summary>Details</summary>
Motivation: 提示优化对于提升大型语言模型（LLMs）在各种任务中的表现至关重要，但现有实现通常依赖于孤立且缺乏维护的研究代码，这阻碍了实际应用。

Method: promptolution 框架通过集成多个现代离散提示优化器，提供一个模块化、可扩展且与底层 LLM 实现无关的解决方案。

Result: 该框架为从业者和研究人员提供了进行提示优化所需的全部组件，并能够在一个系统中实现。

Conclusion: promptolution 框架解决了提示优化在实际应用中面临的问题，为未来的研究和应用提供了一个统一的平台。

Abstract: Prompt optimization has become crucial for enhancing the performance of large language models (LLMs) across a broad range of tasks. Although many research papers show its effectiveness, practical adoption is hindered as existing implementations are often tied to unmaintained and isolated research codebases. To address this, we introduce promptolution, a unified and modular open-source framework that provides all components required for prompt optimization within a single extensible system for both practitioners and researchers. It integrates multiple contemporary discrete prompt optimizers while remaining agnostic to the underlying LLM implementation.

</details>


### [29] [Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages](https://arxiv.org/abs/2512.02841)
*Lechen Zhang,Yusheng Zhou,Tolga Ergen,Lajanugen Logeswaran,Moontae Lee,David Jurgens*

Main category: cs.CL

TL;DR: 该论文研究了系统提示在多语言环境中对大型语言模型（LLM）行为的影响，并提出了一个四维评估框架。


<details>
  <summary>Details</summary>
Motivation: 实际部署中，单个提示需要在不同语言下可靠工作，但以往的研究主要集中在英语上，缺乏多语言环境中的研究。

Method: 通过在一个四维评估框架内进行大规模实验，分析五种语言、三个LLM和三个基准测试下的系统提示效果，并提出了一个提示优化框架。

Result: 某些提示组件（如CoT、情感和场景）与稳健的多语言行为相关，提示优化框架可以自动发现改进所有指标5-10%的提示。

Conclusion: 系统提示优化是提高多语言LLM行为准确性和稳健性的可扩展路径，更高效的提示能带来更有条理和一致的推理模式，同时减少不必要的语言切换。

Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.

</details>


### [30] [Bangla Hate Speech Classification with Fine-tuned Transformer Models](https://arxiv.org/abs/2512.02845)
*Yalda Keivan Jafari,Krishno Dey*

Main category: cs.CL

TL;DR: 本文研究了在低资源语言（尤其是孟加拉语）中识别仇恨言论的问题，提出了多种模型并评估了它们在BLP 2025共享任务的表现。


<details>
  <summary>Details</summary>
Motivation: 由于数据集的不足、正字法异质性和语言多样性，低资源语言的仇恨言论识别仍然是一个难题。孟加拉语在计算资源中代表性不足，尽管其在社交媒体平台上自动化管理的需求不断增长。

Method: 研究复现了官方基线（例如多数类、随机、支持向量机），并考虑了逻辑回归、随机森林和决策树作为基线方法。同时使用了基于Transformer的模型，如DistilBERT、BanglaBERT、m-BERT和XLM-RoBERTa进行仇恨言论分类。

Result: 所有基于Transformer的模型在子任务中均优于基线方法（DistilBERT除外）。其中，BanglaBERT在两个子任务中表现最佳，尽管其规模较小，但性能优于m-BERT和XLM-RoBERTa。

Conclusion: 语言特定的预训练非常重要，BanglaBERT的结果突显了为低资源语言（如孟加拉语）开发预训练语言模型的潜力和必要性。

Abstract: Hate speech recognition in low-resource lan- guages remains a difficult problem due to in- sufficient datasets, orthographic heterogeneity, and linguistic variety. Bangla is spoken by more than 230 million people of Bangladesh and India (West Bengal). Despite the grow- ing need for automated moderation on social media platforms, Bangla is significantly under- represented in computational resources. In this work, we study Subtask 1A and Subtask 1B of the BLP 2025 Shared Task on hate speech detection. We reproduce the official base- lines (e.g., Majority, Random, Support Vec- tor Machine) and also produce and consider Logistic Regression, Random Forest, and De- cision Tree as baseline methods. We also uti- lized transformer-based models such as Dis- tilBERT, BanglaBERT, m-BERT, and XLM- RoBERTa for hate speech classification. All the transformer-based models outperformed base- line methods for the subtasks, except for Distil- BERT. Among the transformer-based models, BanglaBERT produces the best performance for both subtasks. Despite being smaller in size, BanglaBERT outperforms both m-BERT and XLM-RoBERTa, which suggests language- specific pre-training is very important. Our results highlight the potential and need for pre- trained language models for the low-resource Bangla language.

</details>


### [31] [Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules](https://arxiv.org/abs/2512.02892)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: 提出SchED，一种无需训练、模型无关的提前退出算法，通过聚合logit边际，在满足置信度阈值时停止解码，显著提升扩散大模型（dLLM）的解码效率。


<details>
  <summary>Details</summary>
Motivation: 扩散大模型在实际应用中因迭代采样过程导致解码速度慢，限制其实用性。

Method: SchED算法通过在解码过程中聚合全跨度logit边际，并根据平滑且与进度相关的置信度阈值判断是否提前退出。

Result: 在多个dLLM模型上，SchED实现了3.8-4.0倍加速，性能保留99.8-100%。在基础模型上也获得了一致的速度提升，性能保留99.1-100%。

Conclusion: SchED通过提前退出机制显著提升了dLLM的解码效率，同时在各种任务上保持了高性能，是扩散大模型实际应用中的一项有效优化。

Abstract: Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves $3.8$-$4.0\times$ speedups while retaining $99.8$-$100\%$ of the baseline score on average. On base models, SchED yields consistent speedup gains with $99.1$-$100\%$ performance retention, with up to $2.34\times$ under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, $γ{=}4$), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.

</details>


### [32] [Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic](https://arxiv.org/abs/2512.02987)
*Muyu Pan,Dheeraj Kodakandla,Mahfuza Farooque*

Main category: cs.CL

TL;DR: 本文提出了一种将自然语言自动翻译为逻辑表达式并转换为合取范式（CNF）的新框架，以减少大型语言模型的幻觉。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理（NLP）和大型语言模型（LLMs）的进步促使人们研究无需人工干预的自动逻辑翻译，以实现自动推理、调试和遵循软件系统规范。然而，LLMs产生的幻觉（不正确的输出）对逻辑翻译任务构成了挑战。

Method: 引入的新框架将英语句子转换为逻辑表达式，再转换为CNF，用于可满足性求解。该框架采用经典的NLP技术、自定义语法、符号计算库和经过微调的LLM来减少幻觉。

Result: 在早期实验中，经过微调并训练于不同语法设置的模型，可以有意地纠正原始模型产生的相同类型的幻觉。

Conclusion: 该方法提供了可靠的CNF生成，有效地减少了逻辑翻译任务中LLMs的幻觉问题。

Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.

</details>


### [33] [The Moral Consistency Pipeline: Continuous Ethical Evaluation for Large Language Models](https://arxiv.org/abs/2512.03026)
*Saeid Jamshidi,Kawser Wazed Nafi,Arghavan Moradi Dakhel,Negar Shahabi,Foutse Khomh*

Main category: cs.CL

TL;DR: 本文提出了一种名为MoCoP的无数据集闭环框架，用于持续评估和解释大型语言模型的道德稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展突显了道德一致性的重要性，现有方法依赖静态数据集和后验评估，无法充分捕捉伦理推理在不同情境或时间尺度下的演变。

Method: MoCoP框架包含三层支持：词汇完整性分析、语义风险估计和基于推理的判断建模，通过自我维持的架构自主生成、评估和优化伦理场景。

Result: 在GPT-4-Turbo和DeepSeek上的实验表明，MoCoP能够有效捕捉纵向伦理行为，揭示伦理与毒性维度之间的强烈负相关（rET = -0.81, p < 0.001），与响应延迟几乎无关（rEL ≈ 0）。

Conclusion: MoCoP为可拓展的持续审计提供了一个可复现的基础，推动了对自主AI系统中计算道德的研究，道德一致性和语言安全性是模型行为的稳定特征。

Abstract: The rapid advancement and adaptability of Large Language Models (LLMs) highlight the need for moral consistency, the capacity to maintain ethically coherent reasoning across varied contexts. Existing alignment frameworks, structured approaches designed to align model behavior with human ethical and social norms, often rely on static datasets and post-hoc evaluations, offering limited insight into how ethical reasoning may evolve across different contexts or temporal scales. This study presents the Moral Consistency Pipeline (MoCoP), a dataset-free, closed-loop framework for continuously evaluating and interpreting the moral stability of LLMs. MoCoP combines three supporting layers: (i) lexical integrity analysis, (ii) semantic risk estimation, and (iii) reasoning-based judgment modeling within a self-sustaining architecture that autonomously generates, evaluates, and refines ethical scenarios without external supervision. Our empirical results on GPT-4-Turbo and DeepSeek suggest that MoCoP effectively captures longitudinal ethical behavior, revealing a strong inverse relationship between ethical and toxicity dimensions (correlation rET = -0.81, p value less than 0.001) and a near-zero association with response latency (correlation rEL approximately equal to 0). These findings demonstrate that moral coherence and linguistic safety tend to emerge as stable and interpretable characteristics of model behavior rather than short-term fluctuations. Furthermore, by reframing ethical evaluation as a dynamic, model-agnostic form of moral introspection, MoCoP offers a reproducible foundation for scalable, continuous auditing and advances the study of computational morality in autonomous AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [34] [The 4/$δ$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee](https://arxiv.org/abs/2512.02080)
*PIerre Dantas,Lucas Cordeiro,Youcheng Sun,Waldir Junior*

Main category: cs.AI

TL;DR: 该论文提出了LLM-Verifier Convergence Theorem，为大型语言模型与验证工具结合提供了首个具有可证明终止和收敛保证的形式化框架。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型进行软件验证的方法缺乏理论基础，导致过程不稳定。本文旨在通过建立形式化理论框架，确保验证过程的稳定性和可靠性。

Method: 将LLM与验证器的互动建模为离散时间马尔可夫链，关键参数为错误减少概率（δ），并证明了程序终止性和收敛性。

Result: 通过90,000多次试验验证，所有运行均实现了验证，收敛因子接近1.0，理论与实证高度一致。

Conclusion: 理论和实验结果为LLM辅助验证提供了更清晰的结构基础，支持可预测资源规划和性能预算，为安全关键软件环境中的部署提供了框架。

Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($δ$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $δ> 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/δ$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.

</details>


### [35] [From monoliths to modules: Decomposing transducers for efficient world modelling](https://arxiv.org/abs/2512.02193)
*Alexander Boyd,Franz Nowak,David Hyland,Manuel Baltieri,Fernando E. Rosas*

Main category: cs.AI

TL;DR: 本文提出了一种通过分解复杂世界模型为子转换器，以实现更高效和可并行化建模的框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界模型计算成本高，但可以通过模块化分解来提升效率，从而满足AI安全和实际推理的需求。

Method: 开发了一种用于分解用转换器表示的复杂世界模型的框架，并推导出在分离的输入-输出子空间上操作的子转换器。

Result: 实现了可并行化和可解释的替代方法，以支持分布式推理，提高了计算效率。

Conclusion: 这些结果为AI安全所需的结构透明性和实际推理所需的计算效率之间架起了桥梁。

Abstract: World models have been recently proposed as sandbox environments in which AI agents can be trained and evaluated before deployment. Although realistic world models often have high computational demands, efficient modelling is usually possible by exploiting the fact that real-world scenarios tend to involve subcomponents that interact in a modular manner. In this paper, we explore this idea by developing a framework for decomposing complex world models represented by transducers, a class of models generalising POMDPs. Whereas the composition of transducers is well understood, our results clarify how to invert this process, deriving sub-transducers operating on distinct input-output subspaces, enabling parallelizable and interpretable alternatives to monolithic world modelling that can support distributed inference. Overall, these results lay a groundwork for bridging the structural transparency demanded by AI safety and the computational efficiency required for real-world inference.

</details>


### [36] [Bridging the Gap: Toward Cognitive Autonomy in Artificial Intelligence](https://arxiv.org/abs/2512.02280)
*Noorbakhsh Amiri Golilarz,Sindhuja Penchala,Shahram Rahimi*

Main category: cs.AI

TL;DR: 本文分析了现代AI系统的七个核心缺陷，并倡导向基于认知原理的AI架构转变，以实现更强的自我监控、自适应和目标导向能力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在多个领域取得进展，但现有系统缺乏自我监控、自我修正和动态行为调节能力，限制了其泛化和自主性。

Method: 通过比较人工系统与生物认知，并结合AI、认知科学和神经科学的见解，识别当前AI模型的局限性，提出基于神经认知原则的新架构。

Result: 指出了包括缺乏内在自我监控和元认知意识等在内的七项关键缺陷，并论证了这些缺陷阻碍了AI的鲁棒泛化和终身适应能力。

Conclusion: 主张向认知基础的AI（认知自主）转变，实现自我导向适应和目标导向行为，并引入改革性监督机制，以确保系统的可解释性、可管理性和与人类价值的一致性。

Abstract: Artificial intelligence has advanced rapidly across perception, language, reasoning, and multimodal domains. Yet despite these achievements, modern AI systems remain fun- damentally limited in their ability to self-monitor, self-correct, and regulate their behavior autonomously in dynamic contexts. This paper identifies and analyzes seven core deficiencies that constrain contemporary AI models: the absence of intrinsic self- monitoring, lack of meta-cognitive awareness, fixed and non- adaptive learning mechanisms, inability to restructure goals, lack of representational maintenance, insufficient embodied feedback, and the absence of intrinsic agency. Alongside identifying these limitations, we also outline a forward-looking perspective on how AI may evolve beyond them through architectures that mirror neurocognitive principles. We argue that these structural limitations prevent current architectures, including deep learning and transformer-based systems, from achieving robust general- ization, lifelong adaptability, and real-world autonomy. Drawing on a comparative analysis of artificial systems and biological cognition [7], and integrating insights from AI research, cognitive science, and neuroscience, we outline how these capabilities are absent in current models and why scaling alone cannot resolve them. We conclude by advocating for a paradigmatic shift toward cognitively grounded AI (cognitive autonomy) capable of self-directed adaptation, dynamic representation management, and intentional, goal-oriented behavior, paired with reformative oversight mechanisms [8] that ensure autonomous systems remain interpretable, governable, and aligned with human values.

</details>


### [37] [Model Recovery at the Edge under Resource Constraints for Physical AI](https://arxiv.org/abs/2512.02283)
*Bin Xu,Ayan Banerjee,Sandeep K. S. Gupta*

Main category: cs.AI

TL;DR: MERINDA是一种新型FPGA加速的模型恢复框架，通过可并行化的神经结构替代迭代求解器，解决了在边缘设备上部署MR的内存和能耗问题。


<details>
  <summary>Details</summary>
Motivation: 模型恢复（MR）在关键任务自主系统（MCAS）中可实现安全和可解释的决策，但由于神经常微分方程（NODEs）在FPGA上的低效性，其在边缘设备上的部署受到阻碍。内存和能耗是主要问题。

Method: 提出MERINDA框架，将NODEs的迭代求解器替换为可并行化的神经结构，从而在FPGA上实现高效计算。

Result: MERINDA实现了比移动GPU低近11倍的DRAM使用率和2.2倍的运行速度。

Conclusion: 在固定精度下，内存和能耗存在反比关系，MERINDA适用于资源受限的实时MCAS。

Abstract: Model Recovery (MR) enables safe, explainable decision making in mission-critical autonomous systems (MCAS) by learning governing dynamical equations, but its deployment on edge devices is hindered by the iterative nature of neural ordinary differential equations (NODEs), which are inefficient on FPGAs. Memory and energy consumption are the main concerns when applying MR on edge devices for real-time operation. We propose MERINDA, a novel FPGA-accelerated MR framework that replaces iterative solvers with a parallelizable neural architecture equivalent to NODEs. MERINDA achieves nearly 11x lower DRAM usage and 2.2x faster runtime compared to mobile GPUs. Experiments reveal an inverse relationship between memory and energy at fixed accuracy, highlighting MERINDA's suitability for resource-constrained, real-time MCAS.

</details>


### [38] [Breast Cell Segmentation Under Extreme Data Constraints: Quantum Enhancement Meets Adaptive Loss Stabilization](https://arxiv.org/abs/2512.02302)
*Varun Kumar Dasoju,Qingsu Cheng,Zeyun Yu*

Main category: cs.AI

TL;DR: 提出一种高效医学图像分割框架，仅用599张图像实现95.5% Dice分数


<details>
  <summary>Details</summary>
Motivation: 解决医学图像标注耗时问题，乳腺上皮细胞区域仅占图像0.1%-20%，存在严重类别不平衡

Method: 1) 量子启发多尺度Gabor边缘增强 2) 稳定化多组件损失函数 3) 基于复杂度的加权采样 4) EfficientNet-B7/UNet++混合架构 5) 指数移动平均验证

Result: 95.5% Dice分数(+/-0.3%)和91.2% IoU(+/-0.4%)，边界精度提升2.1%，小病灶检测提升3.8%

Conclusion: 显著降低医学专家标注时间，解决临床AI开发中的数据标注瓶颈问题

Abstract: Annotating medical images demands significant time and expertise, often requiring pathologists to invest hundreds of hours in labeling mammary epithelial nuclei datasets. We address this critical challenge by achieving 95.5% Dice score using just 599 training images for breast cell segmentation, where just 4% of pixels represent breast tissue and 60% of images contain no breast regions. Our framework uses quantum-inspired edge enhancement via multi-scale Gabor filters creating a fourth input channel, enhancing boundary detection where inter-annotator variations reach +/- 3 pixels. We present a stabilized multi-component loss function that integrates adaptive Dice loss with boundary-aware terms and automatic positive weighting to effectively address severe class imbalance, where mammary epithelial cell regions comprise only 0.1%-20% of the total image area. Additionally, a complexity-based weighted sampling strategy is introduced to prioritize the challenging mammary epithelial cell regions. The model employs an EfficientNet-B7/UNet++ architecture with a 4-to-3 channel projection, enabling the use of pretrained weights despite limited medical imaging data. Finally, robust validation is achieved through exponential moving averaging and statistical outlier detection, ensuring reliable performance estimates on a small validation set (129 images). Our framework achieves a Dice score of 95.5% +/- 0.3% and an IoU of 91.2% +/- 0.4%. Notably, quantum-based enhancement contributes to a 2.1% improvement in boundary accuracy, while weighted sampling increases small lesion detection by 3.8%. By achieving groundbreaking performance with limited annotations, our approach significantly reduces the medical expert time required for dataset creation, addressing a fundamental bottleneck in clinical perception AI development.

</details>


### [39] [OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning](https://arxiv.org/abs/2512.02306)
*Boyu Zhu,Xiaofei Wen,Wenjie Jacky Mo,Tinghui Zhu,Yanan Xie,Peng Qi,Muhao Chen*

Main category: cs.AI

TL;DR: OmniGuard是首个具备多模态保护能力的模型家族，能够处理文本、图像、视频和音频，并通过有意识的推理进行安全保护。


<details>
  <summary>Details</summary>
Motivation: 以往的安全防护研究主要集中在单模态环境下，通常将保护视为二分类问题，这限制了其在不同模态和任务中的鲁棒性。因此，作者提出了OmniGuard，以应对多模态大模型在安全方面的新挑战。

Method: 为了支持OmniGuard的训练，作者收集了一个包含超过210K个多样化样本的大型综合多模态安全数据集。每个样本都有结构化的安全标签和由专家模型通过目标蒸馏仔细策划的安全评论。

Result: 在15个基准测试上的大量实验表明，OmniGuard在广泛的多模态安全场景中表现出强大的有效性和泛化能力。

Conclusion: OmniGuard提供了一个统一的框架，在多模态环境中执行策略和降低风险，为构建更强大和功能更全面的多模态保护系统铺平了道路。

Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.

</details>


### [40] [Reasoning Path and Latent State Analysis for Multi-view Visual Spatial Reasoning: A Cognitive Science Perspective](https://arxiv.org/abs/2512.02340)
*Qiyao Xue,Weichen Liu,Shiqi Wang,Haoming Wang,Yuyang Wu,Wei Gao*

Main category: cs.AI

TL;DR: 本文介绍了一种新基准ReMindView-Bench，用于评估视觉语言模型在多视角空间推理中的表现，揭示了这些模型在跨视图对齐和视角转换中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在几何一致性和多视角空间推理方面存在挑战，缺乏细粒度基准来分离多视角推理与单视角感知及时间因素。

Method: 提出ReMindView-Bench，通过系统变化视角空间模式和查询类型来评估模型的空间认知能力，并使用LLM-as-a-judge和线性探测等技术进行详细分析。

Result: 评估发现15种当前模型在跨视图对齐和视角转换中表现不佳，尤其在信息整合时表现下降。

Conclusion: 该基准为VLM空间推理提供了认知基础诊断，揭示了多视角空间心理模型在推理阶段的形成与退化过程。

Abstract: Spatial reasoning is a core aspect of human intelligence that allows perception, inference and planning in 3D environments. However, current vision-language models (VLMs) struggle to maintain geometric coherence and cross-view consistency for spatial reasoning in multi-view settings. We attribute this gap to the lack of fine-grained benchmarks that isolate multi-view reasoning from single-view perception and temporal factors. To address this, we present ReMindView-Bench, a cognitively grounded benchmark for evaluating how VLMs construct, align and maintain spatial mental models across complementary viewpoints. ReMindView-Bench systematically varies viewpoint spatial pattern and query type to probe key factors of spatial cognition. Evaluations of 15 current VLMs reveals consistent failures in cross-view alignment and perspective-taking in multi-view spatial reasoning, motivating deeper analysis on the reasoning process. Explicit phase-wise analysis using LLM-as-a-judge and self-consistency prompting shows that VLMs perform well on in-frame perception but degrade sharply when integrating information across views. Implicit analysis, including linear probing and entropy dynamics, further show progressive loss of task-relevant information and uncertainty separation between correct and incorrect trajectories. These results provide a cognitively grounded diagnosis of VLM spatial reasoning and reveal how multi-view spatial mental models are formed, degraded and destabilized across reasoning phases. The ReMindView-Bench benchmark is available at https://huggingface.co/datasets/Xue0823/ReMindView-Bench, and the source codes of benchmark construction and VLM reasoning analysis are available at https://github.com/pittisl/ReMindView-Bench.

</details>


### [41] [Synthetic Error Injection Fails to Elicit Self-Correction In Language Models](https://arxiv.org/abs/2512.02389)
*David X. Wu,Shreyas Kapur,Anant Sahai,Stuart Russell*

Main category: cs.AI

TL;DR: 本文研究了通过监督学习和合成错误注入来诱导语言模型自我纠正能力的可行性，但发现其效果不佳。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大型语言模型中用于推理和自我纠正，但计算成本高，因此探索替代方案。

Method: 采用监督学习，通过合成错误注入，训练模型识别和纠正这些错误。

Result: 该方法未能显著提升性能，且模型在纠正错误时常常重复原错误。

Conclusion: 合成错误与真实错误的分布偏移显著降低了微调模型的错误纠正能力，解释了为何强化学习在此领域更有效。

Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.

</details>


### [42] [Guided Self-Evolving LLMs with Minimal Human Supervision](https://arxiv.org/abs/2512.02472)
*Wenhao Yu,Zhenwen Liang,Chengsong Huang,Kishan Panaganti,Tianqing Fang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: R-Few是一种引导式自我博弈框架，通过轻量级人工标注和混合训练实现AI模型的稳定可控自我进化。


<details>
  <summary>Details</summary>
Motivation: 解决无引导自我进化系统中的概念漂移、多样性丧失和错误进化问题，实现稳定可控的自主知识获取。

Method: 提出R-Few框架，包含：1)基于少量人工标注的上下文接地问题生成(Challenger)；2)采用在线难度课程混合训练求解器(Solver)。

Result: 在数学和通用推理任务上，Qwen3-8B-Base模型性能提升+3.0分，仅用1/20人工数据达到General-Reasoner同等水平。

Conclusion: 通过接地挑战者训练和课程求解器训练的协同作用，R-Few有效缓解了进化漂移，实现了更稳定的协同进化动态。

Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.

</details>


### [43] [COPE: Chain-Of-Thought Prediction Engine for Open-Source Large Language Model Based Stroke Outcome Prediction from Clinical Notes](https://arxiv.org/abs/2512.02499)
*Yongkai Liu,Helena Feng,Bin Jiang,Yixin Wang,Max Wintermark,David S. Liebeskind,Michael Moseley,Maarten Lansberg,Gregory Albers,Jeremy Heit,Greg Zaharchuk*

Main category: cs.AI

TL;DR: 提出了一种基于开源LLaMA-3-8B模型的Chain-of-Thought (CoT)框架，用于从未结构化临床笔记预测急性缺血性中风患者90天功能预后。


<details>
  <summary>Details</summary>
Motivation: 临床笔记包含丰富信息但因非结构化难以用于传统预测模型，需要一种能利用文本信息的轻量级、可解释且保护隐私的预测框架。

Method: 采用两阶段CoT框架（COPE）：首先生成临床推理，然后输出mRS预测；使用464例AIS患者出院小结和90天mRS评分，与GPT-4.1、ClinicalBERT、结构化变量模型和单步LLM对比。

Result: COPE的MAE为1.01，±1准确率74.4%，精确准确率32.8%，性能与GPT-4.1相当，优于其他对比模型；亚组分析显示在年龄、性别等分组中表现一致。

Conclusion: COPE是一种轻量级、可解释、保护隐私的开源框架，能准确从非结构化临床文本中预测中风预后，具有临床应用价值。

Abstract: Predicting outcomes in acute ischemic stroke (AIS) guides clinical decision-making, patient counseling, and resource allocation. Clinical notes contain rich contextual information, but their unstructured nature limits their use in traditional predictive models. We developed and evaluated the Chain-of-Thought (CoT) Outcome Prediction Engine (COPE), a reasoning-enhanced large language model framework, for predicting 90-day functional outcomes after AIS from unstructured clinical notes. This study included 464 AIS patients with discharge summaries and 90-day modified Rankin Scale (mRS) scores. COPE uses a two-step CoT framework based on sequential open-source LLaMA-3-8B models: the first generates clinical reasoning, and the second outputs an mRS prediction. We compared COPE with GPT-4.1, ClinicalBERT, a structured variable-based machine learning model (Clinical ML), and a single-step LLM without CoT. Performance was evaluated using mean absolute error (MAE), accuracy within +/-1 mRS point, and exact accuracy. COPE achieved an MAE of 1.01 (95% CI 0.92-1.11), +/-1 accuracy of 74.4% (69.9, 78.8%), and exact accuracy of 32.8% (28.0, 37.6%), comparable to GPT-4.1 and superior to ClinicalBERT [MAE 1.24 (1.13-1.36)], Clinical ML [1.28 (1.18-1.39)], and the single-step LLM [1.20 (1.09-1.33)]. Subgroup analyses showed consistent performance across sex and age, with slightly higher error among older patients, those undergoing thrombectomy, and those with longer summaries. These findings demonstrate that COPE, a lightweight, interpretable, and privacy-preserving open-source framework, provides an accurate and practical solution for outcome prediction from unstructured clinical text.

</details>


### [44] [IACT: A Self-Organizing Recursive Model for General AI Agents: A Technical White Paper on the Architecture Behind kragent.ai](https://arxiv.org/abs/2512.02605)
*Pengju Lu*

Main category: cs.AI

TL;DR: 介绍了一种新的计算模型IACT，通过动态、递归的智能体拓扑结构，以用户对话驱动，解决静态、硬编码智能体工作流的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统系统需要预定义图或专门编程，限制了智能体工作流的灵活性和可扩展性。IACT旨在通过用户对话驱动，实现通用、自主、动态调整的智能体系统。

Method: IACT采用动态、递归的智能体拓扑结构，以双向、有状态的对话取代刚性调用，引入交互冗余，实现运行时错误校正和模糊性解决。

Result: 在kragent.ai系统中实现了IACT模型的生产部署，并通过实际工作流程提供了定性证据，展示了系统的灵活性和可扩展性。

Conclusion: IACT通过动态和交互式的智能体拓扑结构，有效解决了传统静态工作流的局限性，为开放任务的智能体系统设计提供了新思路。

Abstract: This technical white paper introduces the Interactive Agents Call Tree (IACT), a computational model designed to address the limitations of static, hard-coded agent workflows. Unlike traditional systems that require pre-defined graphs or specialized programming, IACT operates as a general-purpose autonomous system driven purely by user dialogue. Given a high-level objective, the system autonomously grows a dynamic, recursive agent topology incrementally tailored to the problem's structure. This allows it to scale its organizational complexity to match open-ended tasks. To mitigate the error propagation inherent in unidirectional function calls, IACT introduces interactional redundancy by replacing rigid invocations with bidirectional, stateful dialogues. This mechanism enables runtime error correction and ambiguity resolution. We describe the architecture, design principles, and practical lessons behind the production deployment of this model in the kragent.ai system, presenting qualitative evidence from real-world workflows rather than exhaustive benchmark results.

</details>


### [45] [Target-specific Adaptation and Consistent Degradation Alignment for Cross-Domain Remaining Useful Life Prediction](https://arxiv.org/abs/2512.02610)
*Yubo Hou,Mohamed Ragab,Min Wu,Chee-Keong Kwoh,Xiaoli Li,Zhenghua Chen*

Main category: cs.AI

TL;DR: 提出了一种新的跨域剩余使用寿命预测方法TACDA，通过目标域重建和聚类配对策略，提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的RUL预测技术依赖于训练和测试数据来自同一分布，但现实中工业设备的数据分布不同，导致性能下降。

Method: 提出TACDA方法，包含目标域重建策略和聚类配对策略，以学习域不变特征并保留目标域特定信息。

Result: 实验表明，TACDA在两个不同评估指标下均优于现有技术。

Conclusion: TACDA方法通过保留目标特定信息和一致对齐退化阶段，有效解决了跨域RUL预测问题。

Abstract: Accurate prediction of the Remaining Useful Life (RUL) in machinery can significantly diminish maintenance costs, enhance equipment up-time, and mitigate adverse outcomes. Data-driven RUL prediction techniques have demonstrated commendable performance. However, their efficacy often relies on the assumption that training and testing data are drawn from the same distribution or domain, which does not hold in real industrial settings. To mitigate this domain discrepancy issue, prior adversarial domain adaptation methods focused on deriving domain-invariant features. Nevertheless, they overlook target-specific information and inconsistency characteristics pertinent to the degradation stages, resulting in suboptimal performance. To tackle these issues, we propose a novel domain adaptation approach for cross-domain RUL prediction named TACDA. Specifically, we propose a target domain reconstruction strategy within the adversarial adaptation process, thereby retaining target-specific information while learning domain-invariant features. Furthermore, we develop a novel clustering and pairing strategy for consistent alignment between similar degradation stages. Through extensive experiments, our results demonstrate the remarkable performance of our proposed TACDA method, surpassing state-of-the-art approaches with regard to two different evaluation metrics. Our code is available at https://github.com/keyplay/TACDA.

</details>


### [46] [Exploring Depth Generalization in Large Language Models for Solving Recursive Logic Tasks](https://arxiv.org/abs/2512.02677)
*Zhiyuan He*

Main category: cs.AI

TL;DR: 本文研究了标准Transformer架构在处理递归推理问题时存在的深度泛化局限，并提出了一种新颖的循环定位-替换流水线方法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在多种任务中表现出色，但在处理需要解决嵌套层次结构的递归推理问题时面临挑战。现有研究主要关注长度泛化，而深度泛化这一重要限制尚未被充分研究。

Method: 作者提出了一种循环定位-替换流水线方法，该方法将递归问题分解为可管理的子组件。系统包含两个专门模型：定位器识别可解子表达式，替换器在保持整体结构的同时评估这些组件。

Result: 在布尔代数、递归算术和命题逻辑三个领域的实验表明，该方法有效缓解了在超出训练分布递归深度测试时的性能下降问题。

Conclusion: 标准Transformer架构由于无法维持类似栈的行为而难以处理深度递归问题。所提出的循环定位-替换方法通过分解递归问题为子组件，显著改善了深度泛化能力。

Abstract: Large language models have demonstrated remarkable capabilities across many tasks, yet face significant challenges when dealing with recursive reasoning problems, those requiring the resolution of nested hierarchical structures. While prior research has extensively studied length generalization (a model's ability to handle longer sequences than seen during training), we investigate a distinct and underexplored limitation: depth generalization. Here, depth refers to the number of nested levels in a hierarchical problem, such as the layers of parentheses in a mathematical expression or the nesting of logical clauses in a Boolean formula. Our work reveals that standard transformer architectures struggle with problems involving deeper recursion than encountered during training, even when they perform well on longer but non-nested sequences. This limitation stems from their inability to maintain stack-like behavior, the capacity to track and resolve multiple levels of nested dependencies. Through systematic analysis, we demonstrate how this architectural constraint leads to rapid performance decay as the depth of the recursion increases. To address this challenge, we develop a novel looped locate-and-replace pipeline that decomposes recursive problems into manageable subcomponents. The approach employs two specialized models: a locator that identifies solvable subexpressions and a replacer that evaluates these components while preserving the overall structure. We evaluated this method in three carefully designed domains: Boolean algebra, recursive arithmetic, and propositional logic, each with a controllable depth of recursion. We show that our method effectively alleviates the performance decay when tested on out-of-distribution recursion depth.

</details>


### [47] [Learning What to Attend First: Modality-Importance-Guided Reasoning for Reliable Multimodal Emotion Understanding](https://arxiv.org/abs/2512.02699)
*Hyeongseop Rha,Jeong Hun Yeo,Junil Won,Se Jin Park,Yong Man Ro*

Main category: cs.AI

TL;DR: 提出了一个名为MIGR的框架，用于提升多模态大语言模型中基于推理的情感理解的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在情感理解中容易出现推理漂移，模型偏向于依赖自身生成的文本而非多模态证据，导致解释受视觉推理路径影响过大。

Method: 引入模态重要性（MI）机制识别主导情感的模态，并使用两阶段框架（模态对齐的监督微调和模态感知奖励优化）来优化推理路径。

Result: 在DFEW基准测试中，MIGR显著提高了推理可靠性，将情感不一致解释的正确预测从18.10%降低到7.37%。

Conclusion: 从情感主导模态开始推理可以显著改善多模态情感理解的效果。

Abstract: In this paper, we present Modality-Importance-Guided Reasoning (MIGR), a framework designed to improve the reliability of reasoning-based multimodal emotion understanding in multimodal large language models. Although existing methods have advanced emotion understanding, they often suffer from reasoning drift: models gradually rely on their own generated text instead of multimodal evidence, and their explanations are overly shaped by visually initiated reasoning paths. To address these issues, we introduce Modality Importance (MI), a simple yet effective mechanism for identifying the emotion-dominant modality. Using MI, MIGR reorganizes reasoning sequences so that explanations begin from the modality most critical to the target emotion, preventing early reasoning from being misled by less informative cues. Our two-stage framework-comprising modality-aligned supervised fine-tuning and modality-aware reward optimization-encourages models to generate emotionally grounded, causally relevant, and coherence-preserving explanations. Experimental results on the DFEW benchmark show that MIGR substantially improves reasoning reliability, decreasing instances of correct predictions accompanied by emotionally inconsistent explanations from 18.10% to 7.37%. These results confirm the benefit of initiating reasoning from the emotion-dominant modality.

</details>


### [48] [Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs](https://arxiv.org/abs/2512.02713)
*Theodoros Aivalis,Iraklis A. Klampanos,Antonis Troumpoukis,Joemon M. Jose*

Main category: cs.AI

TL;DR: 本文介绍了一种利用多模态大型语言模型从图像中自动构建与本体对齐的知识图谱框架，以解释生成模型的输出并分析训练数据的影响。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能力的增强，透明度、责任和版权问题日益突出，理解训练数据对模型输出的影响变得至关重要。

Method: 利用多模态大型语言模型从图像中提取结构化三元组，并与领域特定本体对齐，通过比较生成图像和训练图像的知识图谱来追踪潜在影响。

Result: 通过本地训练模型的遗忘实验和大规模模型的风格特定实验验证了该方法的有效性。

Conclusion: 该框架支持开发促进人类协作、创造力和激发好奇心的AI系统，有助于版权分析、数据集透明度和可解释AI。

Abstract: As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.

</details>


### [49] [Menta: A Small Language Model for On-Device Mental Health Prediction](https://arxiv.org/abs/2512.02716)
*Tianyi Zhang,Xiangyuan Xue,Lingyan Ruan,Shiya Fu,Feng Xia,Simon D'Alfonso,Vassilis Kostakos,Hong Jia*

Main category: cs.AI

TL;DR: 本文介绍了一种名为Menta的优化小型语言模型，专门用于社交媒体数据的多任务心理健康预测。


<details>
  <summary>Details</summary>
Motivation: 心理健康问题影响全球数亿人，但早期检测仍然有限。大型语言模型虽有应用前景，但由于其规模和计算需求，实际部署困难。小型语言模型提供了轻量级替代方案，但其在社交媒体心理健康预测中的应用尚未充分探索。

Method: Menta采用LoRA框架、跨数据集策略和平衡准确率导向的损失函数，在六个分类任务上联合训练。

Result: 与九个先进的小型语言模型基线相比，Menta在抑郁、压力和自杀倾向等任务上的平均性能提升了15.2%。与13B参数的LLMs相比，Menta在抑郁和压力分类任务上的准确率更高，同时模型大小约为其1/3.25。Menta可在iPhone 15 Pro Max上实时运行，仅需约3GB RAM。

Conclusion: Menta展示了可扩展且隐私保护的心理健康监测的潜力，为心理健康早期检测提供了新的技术路径。

Abstract: Mental health conditions affect hundreds of millions globally, yet early detection remains limited. While large language models (LLMs) have shown promise in mental health applications, their size and computational demands hinder practical deployment. Small language models (SLMs) offer a lightweight alternative, but their use for social media--based mental health prediction remains largely underexplored. In this study, we introduce Menta, the first optimized SLM fine-tuned specifically for multi-task mental health prediction from social media data. Menta is jointly trained across six classification tasks using a LoRA-based framework, a cross-dataset strategy, and a balanced accuracy--oriented loss. Evaluated against nine state-of-the-art SLM baselines, Menta achieves an average improvement of 15.2\% across tasks covering depression, stress, and suicidality compared with the best-performing non--fine-tuned SLMs. It also achieves higher accuracy on depression and stress classification tasks compared to 13B-parameter LLMs, while being approximately 3.25x smaller. Moreover, we demonstrate real-time, on-device deployment of Menta on an iPhone 15 Pro Max, requiring only approximately 3GB RAM. Supported by a comprehensive benchmark against existing SLMs and LLMs, Menta highlights the potential for scalable, privacy-preserving mental health monitoring. Code is available at: https://xxue752-nz.github.io/menta-project/

</details>


### [50] [StockMem: An Event-Reflection Memory Framework for Stock Forecasting](https://arxiv.org/abs/2512.02720)
*He Wang,Wenyilin Xiao,Songqiao Han,Hailiang Huang*

Main category: cs.AI

TL;DR: 提出StockMem，一个用于股票价格预测的事件-反思双层记忆框架，通过整合新闻事件和挖掘市场预期差异，提高了预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测由于市场波动和对实时事件的敏感性而具有挑战性。现有LLMs在金融领域应用受到嘈杂新闻数据和文本中缺乏明确答案的阻碍。

Method: StockMem通过事件-反思双层记忆框架，将新闻结构化并挖掘事件的横向整合和纵向跟踪，构建时间事件知识库和因果经验的反思知识库，通过检索和推理历史场景进行预测。

Result: 实验表明，StockMem在预测准确性上优于现有记忆架构，通过追踪影响价格的信息链，提供优越且可解释的推理。

Conclusion: StockMem通过构建事件和反思知识库，提高了股票价格预测的准确性和决策透明度，为金融预测提供了一种有效的解决方案。

Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.

</details>


### [51] [AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping](https://arxiv.org/abs/2512.02726)
*Md Abdul Kadir,Sai Suresh Macharla Vasu,Sidharth S. Nair,Daniel Sonntag*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在检测复式记账中的异常方面优于传统的基于规则的日记账测试（JETs）和经典的机器学习基线，并提供自然语言解释以增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 审计人员依赖JETs检测与税务相关的分类账记录中的异常，但基于规则的方法会产生大量误报，并且在处理细微的异常时存在困难。因此，研究探讨LLMs是否可以作为复式记账中的异常检测器。

Method: 研究使用LLaMA和Gemma等先进的大型语言模型，在合成和真实世界的匿名分类账上进行基准测试，并将其与JETs和机器学习基线进行比较。

Result: 结果表明，LLMs始终优于传统的基于规则的JETs和经典ML基线，同时还提供了增强可解释性的自然语言解释。

Conclusion: 这些结果突显了AI增强审计的潜力，即人类审计员与基础模型合作，以加强财务完整性。

Abstract: Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.

</details>


### [52] [Self-Improving AI Agents through Self-Play](https://arxiv.org/abs/2512.02731)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 本文将心理测量框架扩展至动态系统领域，通过递归GVU算子形式化智能体，并推导出自我改进稳定性条件（Variance Inequality），统一了语言自博弈、自我纠正等文献。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能体能力评估的静态局限性，建立动态系统理论框架以分析自我改进过程的稳定性机制。

Method: 采用模理论方法：1) 构建参数化流ν_r与GVU算子 2) 定义能力泛函的李导数κ 3) 推导谱条件形式的方差不等式

Result: 1) 证明GVU生成参数流形上的向量场 2) 获得κ>0的充分条件（生成-验证噪声控制）3) 成功将5类架构(STaR/SPIN等)映射为GVU的具体实现

Conclusion: 通过微分几何与动力系统方法建立了自我改进的稳定性理论，为LSP、自纠正等范式提供了统一分析框架，揭示了不同架构满足稳定性条件的拓扑机制。

Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $ν_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $Θ$, and we identify the coefficient of self-improvement $κ$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $κ> 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.

</details>


### [53] [A Framework for Causal Concept-based Model Explanations](https://arxiv.org/abs/2512.02735)
*Anna Rodum Bjøru,Jacob Lysnæs-Larsen,Oskar Jørgensen,Inga Strümke,Helge Langseth*

Main category: cs.AI

TL;DR: 提出一个基于因果概念的事后解释框架，确保对不可解释模型的解释既可理解又忠实。


<details>
  <summary>Details</summary>
Motivation: 解释不可解释模型需要满足可理解性和忠实性两个要求。

Method: 通过计算概念干预的充分性概率来生成局部和全局解释。

Result: 使用CelebA数据集训练的分类器，生成了可理解的、基于概念的示例解释。

Conclusion: 可理解性通过清晰的概念词汇和隐含的因果解释实现，忠实性通过强调框架假设和解释生成与解释解读的上下文一致性来解决。

Abstract: This work presents a conceptual framework for causal concept-based post-hoc Explainable Artificial Intelligence (XAI), based on the requirements that explanations for non-interpretable models should be understandable as well as faithful to the model being explained. Local and global explanations are generated by calculating the probability of sufficiency of concept interventions. Example explanations are presented, generated with a proof-of-concept model made to explain classifiers trained on the CelebA dataset. Understandability is demonstrated through a clear concept-based vocabulary, subject to an implicit causal interpretation. Fidelity is addressed by highlighting important framework assumptions, stressing that the context of explanation interpretation must align with the context of explanation generation.

</details>


### [54] [Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control](https://arxiv.org/abs/2512.02814)
*Yongrui Yu,Zhongzhen Huang,Linjie Mu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为Radiologist Copilot的AI助手，用于放射科报告生成与质量控制。


<details>
  <summary>Details</summary>
Motivation: 放射科报告生成耗时且易出错，现有自动化方法忽视了质量控制这一关键环节。

Method: 利用大型语言模型作为推理核心，结合一系列协调工具（如区域定位、图像分析规划、模板选择和质量评估）来模拟放射科医生的工作流程。

Result: 实验结果表明，Radiologist Copilot在放射科报告生成方面显著优于其他最新方法。

Conclusion: Radiologist Copilot能够促进准确、完整且高效的放射科报告生成，提高临床效率。

Abstract: Radiology reporting is an essential yet time-consuming and error-prone task for radiologists in clinical examinations, especially for volumetric medical images. Rigorous quality control is also critical but tedious, ensuring that the final report meets clinical standards. Existing automated approaches, including radiology report generation methods and medical vision-language models, focus mainly on the report generation phase and neglect the crucial quality control procedure, limiting their capability to provide comprehensive support to radiologists. We propose Radiologist Copilot, an agentic AI assistant equipped with orchestrated tools designed for automated radiology reporting with quality control. Leveraging large language models as the reasoning backbone, the agentic system autonomously selects tools, plans, and executes actions, emulating the behavior of radiologists throughout the holistic radiology reporting process. The orchestrated tools include region localization, think with image paradigm directed region analysis planning, strategic template selection for report generation, quality assessment and feedback-driven adaptive refinement for quality control. Therefore, Radiologist Copilot facilitates accurate, complete, and efficient radiology reporting, assisting radiologists and improving clinical efficiency. Experimental results demonstrate that Radiologist Copilot significantly surpasses other state-of-the-art methods in radiology reporting. The source code will be released upon acceptance.

</details>


### [55] [The future of AI in critical mineral exploration](https://arxiv.org/abs/2512.02879)
*Jef Caers*

Main category: cs.AI

TL;DR: 本文提出了一种结合人工智能与贝叶斯-证伪主义哲学的矿产勘探科学方法，旨在减少认知偏差、降低勘探成本并解决新发现递减的问题。


<details>
  <summary>Details</summary>
Motivation: 全球能源转型加剧了对关键矿产的需求，但近二十年新发现持续减少，现有勘探方法存在认知偏差和假阳性问题，亟需创新解决方案。

Method: 构建基于贝叶斯主义和证伪主义的新型科学方法：1) 将数据采集视为证伪人类假设的首要手段；2) 采用可验证指标进行数据决策；3) 提出需结合无监督学习（与领域专家协作生成地质假设）与人机协同AI（优化多类型勘探数据采集顺序）的实践协议。

Result: 设计了可应用于任何勘探活动的标准化流程框架，强调通过AI实现地质假设的不确定性优先于品位/储量的不确定性。

Conclusion: AI驱动的勘探方法能有效降低认知偏差和假阳性，其核心在于无监督学习生成竞争假设的能力与人机协同算法的优化决策，这将成为突破矿产勘探瓶颈的关键。

Abstract: The energy transition through increased electrification has put the worlds attention on critical mineral exploration Even with increased investments a decrease in new discoveries has taken place over the last two decades Here I propose a solution to this problem where AI is implemented as the enabler of a rigorous scientific method for mineral exploration that aims to reduce cognitive bias and false positives drive down the cost of exploration I propose a new scientific method that is based on a philosophical approach founded on the principles of Bayesianism and falsification In this approach data acquisition is in the first place seen as a means to falsify human generated hypothesis Decision of what data to acquire next is quantified with verifiable metrics and based on rational decision making A practical protocol is provided that can be used as a template in any exploration campaign However in order to make this protocol practical various form of artificial intelligence are needed I will argue that the most important form are one novel unsupervised learning methods that collaborate with domain experts to better understand data and generate multiple competing geological hypotheses and two humanintheloop AI algorithms that can optimally plan various geological geophysical geochemical and drilling data acquisition where uncertainty reduction of geological hypothesis precedes the uncertainty reduction on grade and tonnage

</details>
