<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

TL;DR: 为了减轻大型语言模型（LLMs）生成文本的潜在危害，研究者提出了水印技术，但该技术会影响文本质量且易受攻击。本文评估了不同水印技术的稳健性和质量保持能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成文本的潜在危害，提高水印技术的广泛采用率。

Method: 通过比较释义和反向翻译攻击，评估水印技术的稳健性，并使用语言指标评估文本质量和风格保持能力。

Result: 水印技术在语义保持上表现良好，但会偏离原始文本的写作风格，并容易受到攻击，尤其是反向翻译攻击。

Conclusion: 当前的水印技术在稳健性和文本质量保持方面仍有改进空间，以提高其在实际应用中的有效性。

Abstract: To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [2] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

TL;DR: 提出了一种增强文本嵌入模型语义推理能力的方法RT，通过多次前向传播获得最终语义表示。


<details>
  <summary>Details</summary>
Motivation: 提升文本嵌入模型在语义推理任务中的表现，同时保持其在通用语义理解任务中的性能。

Method: RT方法通过运行文本嵌入模型的多次前向传播，获取最终语义表示。

Result: 在BRIGHT和PJBenchmark1语义推理任务中取得了显著改进，并在C-MTEB等通用语义理解任务中保持了一致的性能。

Conclusion: RT有效激活了预训练过程中学习的语义推理能力，是一种测试时推理方法。

Abstract: We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [3] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

TL;DR: 本文提出了两种无需训练的方法用于WMT 2025自动化翻译质量评估任务3，其中一种方法通过选择多个LLM生成的最佳翻译获胜。


<details>
  <summary>Details</summary>
Motivation: 尽管联合训练质量评估(QE)和自动后期编辑(APE)系统已显示出改进的性能，但APE系统仍因过度纠正机器翻译(MT)输出而导致性能下降。

Method: 研究了一种简单的无需训练的方法 - QE引导的重翻译，并与同一范式下的另一种方法进行比较。获胜方法是从不同LLM生成的多个候选项中选择最高质量的翻译。第二种方法类似于APE，指导LLM替换QE解释中指定的错误子串，使用条件启发式方法最小化编辑次数。

Result: 两种方法分别获得了0.0201和-0.0108的Delta COMET分数，第一种方法在子任务排行榜上取得了胜利。

Conclusion: QE引导的多候选翻译选择方法在无需训练的情况下表现优异，优于编辑错误子串的方法，表明选择优于修改的策略在翻译质量提升中更为有效。

Abstract: The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [4] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

TL;DR: 本文介绍了GM-Extract基准数据集，旨在评估大型语言模型在控制变量检索中的表现，并提出了一种新的评估系统，通过两个指标分析模型在空间检索和语义检索方面的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在利用长距离上下文时存在“lost-in-the-middle”现象，这给基于检索的LLM应用带来了挑战，因此需要在实际应用中研究其影响。

Method: 引入GM-Extract基准数据集，并提出使用Document Metric和Variable Extraction Metric两个指标进行模型评估。在两个多文档任务上系统评估了7-8B参数模型，并进行了缓解方法的文献综述。

Result: 通过改变数据在上下文窗口中的表示方式，观察到检索性能的显著变化，并发现了模型性能与困惑度得分之间的相关性。缓解方法的有效性高度依赖具体场景。

Conclusion: 评估揭示了缓解方法在不同情境下的有效性，有些方法在特定场景下能提升性能，而在其他场景下则可能导致性能下降，提供了对其实际效用的全面理解。

Abstract: The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [5] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

TL;DR: 该论文提出了一种通过LLMs解析电商搜索查询中最高级表达（如best, most popular）潜在意图的框架，并将其转化为结构化提示，以提高搜索性能。


<details>
  <summary>Details</summary>
Motivation: 搜索查询中的最高级表达需要多维度的比较，涉及语言理解和领域知识，而现有方法难以高效解析这些表达。

Method: 提出的框架将查询分解为属性-值提示，与检索过程并行生成，并将这些提示集成到排序流程中。同时，为解决LLM延迟问题，开发了将最高级解释迁移到轻量级模型的有效方法。

Result: 该方法在MAP上比基线提高了10.9个点，在MRR上提高了5.9个点。

Conclusion: 论文展示了如何在检索系统中表示和迁移最高级语义，解决了实际部署中的延迟问题，为语言解释在检索系统中的应用提供了新见解。

Abstract: Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [6] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

TL;DR: 提出MoRA-RAG框架，利用大语言模型分析灾后报告，转化为结构化多灾害推理基础。


<details>
  <summary>Details</summary>
Motivation: 灾后报告对理解多灾害相互作用至关重要，但非结构化叙述使得知识转移困难，现有LLM缺乏领域基础时输出不可靠。

Method: 引入MoRA-RAG框架，集成Mixture-of-Retrieval机制和agentic chunking，动态路由查询并保留上下文连贯性，还包括验证循环以评估证据充分性并优化查询。

Result: 构建了HazardRecQA数据集，MoRA-RAG达到94.5%的准确率，优于零样本LLMs和最先进的RAG系统，同时减少不同LLM架构下的幻觉现象。

Conclusion: MoRA-RAG为将灾后文档转化为可操作、值得信赖的灾害情报建立了新范式，提升灾害抵御能力。

Abstract: Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [7] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

TL;DR: 提出了一种新型的多模态虚假信息检测框架HiEAG，通过强化外部一致性检查，利用多模态大型语言模型（MLLMs）的广泛知识，实现了在图像-文本对与外部证据之间的一致性验证。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态虚假信息检测方法过于强调内部一致性，而忽略了图像-文本对与外部证据之间的外部一致性，因此作者提出了一种新的框架来解决这个问题。

Method: HiEAG框架通过一个综合引擎管道分解外部一致性检查，包括证据重排（reranking）、证据重写（rewriting）以及检索（retrieval）。其中，证据重排模块采用了自动证据选择提示（AESP）技术，证据重写模块利用了自动证据生成提示（AEGP）技术，以增强MLLM-based OOC虚假信息检测器的任务适应性。

Result: 实验结果表明，HiEAG在多个基准数据集上的表现超越了先前的最先进（SOTA）方法，在所有样本上的准确率更高。

Conclusion: 该框架不仅提高了虚假信息检测的性能，还使得判断过程具有可解释性，展示了其在多模态虚假信息检测领域的潜力。

Abstract: Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [8] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Stealth Fine-Tuning的攻击方法，能够绕过推理增强视觉语言模型（RVLMs）的安全对齐，成功诱导有害行为。


<details>
  <summary>Details</summary>
Motivation: 尽管RVLMs通过安全对齐防止有害行为，但其暴露的思维链（CoT）痕迹引入了新的攻击面，容易被利用。

Method: 通过分段级干扰和自生成输出重用，结合轮次加权损失设计，实现轻量级、分布一致的精细调整方法。

Result: 仅用499个样本和不到3小时的单A100 QLoRA训练，Stealth Fine-Tuning在ASR上优于IDEATOR 38.52%，并保留通用推理能力。

Conclusion: Stealth Fine-Tuning是一种低成本且高效的方法，可以绕过安全对齐防御。

Abstract: Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [9] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

TL;DR: 提出了一种以数据为中心的方法，通过生成高质量的合成出院摘要来解决ICD编码任务中的长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 由于ICD诊断码存在极端的长尾分布，许多稀有和零样本ICD代码在MIMIC-III等数据集中代表性严重不足，导致低macro-F1分数。

Method: 利用真实世界的共现模式、ICD描述、同义词、分类学和类似临床笔记构建基于稀有代码的现实多标签代码集，并使用这些结构化提示生成90,000个合成笔记，扩展训练分布。

Result: 在原始和扩展数据集上微调了PLM-ICD和GKI-ICD两个先进模型，实验显示该方法适度提高了macro-F1，同时保持强micro-F1，优于之前的SOTA。

Conclusion: 精心设计的合成数据可以增强长尾ICD代码预测的公平性，尽管提升看似微小，但证明了合成数据的有效性。

Abstract: Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [10] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

TL;DR: 提出了一种用于方面级情感分析（ABSA）的动态超图框架HyperABSA，通过样本特定的层次聚类诱导方面-观点结构，并在三个基准上表现优于现有图基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法仅建模成对依赖关系，导致冗余、参数开销和融合时的误差传播，尤其在短文本和低资源环境下鲁棒性较差。

Method: 提出HyperABSA框架，采用动态超图结构和一种新颖的加速-回退截断方法进行层次聚类，自适应确定粒度级别。

Result: 在Lap14、Rest14和MAMS三个基准测试中，HyperABSA始终优于现有强图基线，尤其与RoBERTa结合时性能提升显著。

Conclusion: 动态超图构建是ABSA任务中一种高效且强大的替代方法，并可扩展至其他短文本NLP任务。

Abstract: Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [11] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

TL;DR: 本文提出了一种结合基于Transformer的关系抽取和知识图谱匹配的方法，以回答多项选择题，同时保持输出过程的可追溯性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱（KGs）由于构建成本高，通常被视为静态数据库。然而，近年来基于Transformer的关系抽取（RE）方法的发展使得动态生成KGs成为可能。本文旨在利用这一技术，回答填空式的多项选择题，并确保答案的可追溯性。

Method: 该方法首先使用关系抽取方法将问题句子转换为关系图，然后基于封闭世界假设，与事实正确的KGs进行验证，以测量每个问题句子的真实性。

Result: 实验结果表明，该方法能够正确回答约70%的问题，同时提供了可追溯的解题过程。此外，问题类别对准确率有显著影响。

Conclusion: 结合关系抽取和知识图谱匹配的方法在回答多项选择题方面表现良好，但问题类别会影响最终准确率。

Abstract: In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


### [12] [Selective Weak-to-Strong Generalization](https://arxiv.org/abs/2511.14166)
*Hao Lang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 提出了一种选择性弱到强泛化（W2SG）框架，以避免在不需要时使用弱监督。


<details>
  <summary>Details</summary>
Motivation: 现有W2SG方法中不变地使用弱监督导致稳健性问题，部分弱标签可能对模型有害。

Method: 训练一个二元分类器P(IK)来识别强模型可以回答的问题，并使用其自我生成的标签进行对齐。进一步用图平滑方法优化弱标签。

Result: 在三个基准测试上的广泛实验表明，该方法始终优于有竞争力的基线。

Conclusion: P(IK)可以跨任务和难度泛化，表明选择性W2SG有助于超级对齐。

Abstract: Future superhuman models will surpass the ability of humans and humans will only be able to \textit{weakly} supervise superhuman models. To alleviate the issue of lacking high-quality data for model alignment, some works on weak-to-strong generalization (W2SG) finetune a strong pretrained model with a weak supervisor so that it can generalize beyond weak supervision. However, the invariable use of weak supervision in existing methods exposes issues in robustness, with a proportion of weak labels proving harmful to models. In this paper, we propose a selective W2SG framework to avoid using weak supervision when unnecessary. We train a binary classifier P(IK) to identify questions that a strong model can answer and use its self-generated labels for alignment. We further refine weak labels with a graph smoothing method. Extensive experiments on three benchmarks show that our method consistently outperforms competitive baselines. Further analyses show that P(IK) can generalize across tasks and difficulties, which indicates selective W2SG can help superalignment.

</details>


### [13] [Harnessing Deep LLM Participation for Robust Entity Linking](https://arxiv.org/abs/2511.14181)
*Jiajun Hou,Chenyu Zhang,Rui Meng*

Main category: cs.CL

TL;DR: 本文提出了DeepEL，一个将大型语言模型（LLMs）全面整合到实体链接（EL）任务各个阶段的框架，并引入了一种利用全局上下文信息的自验证机制，以提升实体消歧性能。


<details>
  <summary>Details</summary>
Motivation: 现有的EL方法通常只在EL任务的孤立阶段应用LLMs，未能充分发挥LLMs在整个EL过程中的潜力。此外，单独进行实体消歧的性能有限，需要利用全局上下文信息来改进。

Method: DeepEL框架在每个EL任务阶段都整合LLMs，并提出了一种自验证机制，该机制利用全局上下文信息，使LLMs能够修正自身的预测，并识别句子中实体间的关联关系。

Result: 在十个基准数据集上的广泛实验表明，DeepEL显著优于现有最先进的方法，在整体F1分数上平均提高了2.6%，在域外数据集上提高了4%。

Conclusion: 深度整合LLMs到EL任务的各个阶段，并引入全局上下文自验证机制，能够显著提升EL任务的性能，尤其是在实体消歧方面。

Abstract: Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

</details>


### [14] [ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC](https://arxiv.org/abs/2511.14230)
*Ahlam Alrehili,Areej Alhothali*

Main category: cs.CL

TL;DR: 提出了一种新的多系统方法ArbESC+，用于阿拉伯语语法错误校正，通过结合多种模型和技术来提高校正性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语复杂的形态和句法结构使得语法错误校正比其他语言更具挑战性，以往的方法多使用单一模型，未充分利用多系统集成的优势。

Method: 采用多模型集成方法，将多种模型的校正建议以数值特征表示，并通过分类器选择最佳修正。框架还包括过滤重叠修正和支持决策可靠性估计的辅助技术。

Result: 在QALB-14、QALB-15 L1和QALB-15 L2测试数据上，F0.5得分分别为82.63%、84.64%和65.55%，优于单一模型。

Conclusion: 该工作是首次将语言错误校正集成到阿拉伯语中的尝试，为开发更先进的工具提供了实用步骤，有利于阿拉伯文本处理的用户和研究人员。

Abstract: Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

</details>


### [15] [MuCPT: Music-related Natural Language Model Continued Pretraining](https://arxiv.org/abs/2511.14245)
*Kai Tian,Yirong Mao,Wendong Bi,Hanjie Wang,Que Wenhui*

Main category: cs.CL

TL;DR: 为解决音乐领域大模型的数据和训练问题，构建了40B音乐语料库及领域优先数据流程，引入基于参考模型的软评分进行质量控制，并设计MusicSimpleQA评估真实性，提出可扩展的音乐领域大模型数据训练框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用任务表现优异，但在音乐等专门领域仍受限，关键因素是语料规模、纯度及数据与训练目标匹配度。

Method: 构建40B音乐相关自然语言语料库，采用领域优先数据流程，包括轻量分类器过滤加权、多阶段清洗、去重和隐私保护掩码；整合多源音乐文本与元数据；引入基于参考模型的标记级软评分进行质量控制和动态降权优化。

Result: 提出可扩展的数据训练框架，设计MusicSimpleQA基准评估事实准确性，并在数据组成方面进行系统比较，实现更高效的音乐领域持续预训练和对齐。

Conclusion: 该工作通过优化语料库和训练目标，提出了适用于音乐领域大模型的可扩展数据训练框架和可复用评估工具。

Abstract: Large language models perform strongly on general tasks but remain constrained in specialized settings such as music, particularly in the music-entertainment domain, where corpus scale, purity, and the match between data and training objectives are critical. We address this by constructing a large, music-related natural language corpus (40B tokens) that combines open source and in-house data, and by implementing a domain-first data pipeline: a lightweight classifier filters and weights in-domain text, followed by multi-stage cleaning, de-duplication, and privacy-preserving masking. We further integrate multi-source music text with associated metadata to form a broader, better-structured foundation of domain knowledge. On the training side, we introduce reference-model (RM)-based token-level soft scoring for quality control: a unified loss-ratio criterion is used both for data selection and for dynamic down-weighting during optimization, reducing noise gradients and amplifying task-aligned signals, thereby enabling more effective music-domain continued pretraining and alignment. To assess factuality, we design the MusicSimpleQA benchmark, which adopts short, single-answer prompts with automated agreement scoring. Beyond the benchmark design, we conduct systematic comparisons along the axes of data composition. Overall, this work advances both the right corpus and the right objective, offering a scalable data-training framework and a reusable evaluation tool for building domain LLMs in the music field.

</details>


### [16] [Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning](https://arxiv.org/abs/2511.14249)
*Rui Liu,Yuan Zhao,Zhenqi Jia*

Main category: cs.CL

TL;DR: 提出了一种新的电影配音模型，Authentic-Dubber，通过模拟真实配音过程中导演与演员的互动，提高了情感表达的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了导演和演员在真实配音过程中的互动，导致情感表达不够真实。本文旨在通过模拟真实配音流程，提高配音的情感表现力。

Method: 提出了一个检索增强的导演-演员互动学习方案，包括：(1)构建多模态参考片段库，(2)基于情感相似性的检索增强策略，(3)基于渐进图式的语音生成方法。

Result: 在V2C Animation基准数据集上的主客观评价均验证了该方法的有效性。

Conclusion: Authentic-Dubber通过模拟真实的导演-演员互动，实现了更真实的情感表达，为自动电影配音提供了新的解决方案。

Abstract: The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

</details>


### [17] [AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR](https://arxiv.org/abs/2511.14255)
*Gabrial Zencha Ashungafac,Mardhiyah Sanni,Busayo Awobade,Alex Gichamba,Tobi Olatunji*

Main category: cs.CL

TL;DR: AfriSpeech-MultiBench是首个针对非洲英语口音的多领域评估套件，涵盖100多种口音和七个应用领域。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对非洲语言多样性的应用特定模型评估，而语音AI的进步使得这一需求更加迫切。

Method: 使用AfriSpeech-MultiBench基准测试，评估多种开放和封闭的单模态ASR及多模态LLM语音识别系统，在自发的和非自发的非洲口音英语对话数据集上进行。

Result: 开源ASR模型在自发语音中表现出色，但在嘈杂和非本土对话中性能下降；多模态LLM对口音更具鲁棒性，但在领域特定命名实体上表现不佳；专有模型在清晰语音上准确性高，但各国和领域之间差异显著。

Conclusion: 通过在非洲英语上微调的模型，可以实现高准确性和低延迟，这对实际应用具有优势，但幻觉问题仍是大多数SOTA模型面临的挑战。发布这一综合基准测试，帮助实践者和研究人员选择适合非洲用例的语音技术，促进包容性语音应用的发展。

Abstract: Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

</details>


### [18] [Entropy-Guided Reasoning Compression](https://arxiv.org/abs/2511.14258)
*Hourun Zhu,Yang Gao,Wenlong Fei,Jiawei Li,Huashan Sun*

Main category: cs.CL

TL;DR: 大型推理模型在复杂任务上表现出色，但其思维链输出过长导致高计算成本和部署困难。现有压缩方法忽略了训练过程中的熵冲突现象。本文提出一种熵引导的训练框架，有效平衡压缩和准确性，实验证明在保持或超越基线准确性的同时，将推理长度压缩到原来的20%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理任务中表现优异，但长思维链输出导致高计算成本和部署困难。现有压缩方法忽略了熵冲突，导致模型陷入局部困境。

Method: 采用熵引导的训练框架：在熵降低时，通过鼓励简洁思维步骤引导模型实现高效推理；在熵上升时，通过紧凑推理模式加强探索，提高鲁棒性。

Result: 在六个数学基准测试中，该方法将推理长度压缩到原来的20%，同时保持或甚至超越了基线准确性。

Conclusion: 熵引导的训练框架有效解决了压缩训练中的熵冲突问题，实现了推理长度的显著压缩，同时保持或提升了模型的准确性。代码和模型将公开发布。

Abstract: Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

</details>


### [19] [Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space](https://arxiv.org/abs/2511.14275)
*Ante Wang,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 该论文提出，通过预测一个语言化的概率分布，可以有效促进模型在置信度估计中进行深入推理。


<details>
  <summary>Details</summary>
Motivation: 了解模型回答的可靠性在许多应用中至关重要。尽管已有研究关注于生成语言化的置信度，但推理策略对置信度估计的影响仍缺乏探索。

Method: 论文提出了一种方法，通过预测概率分布来鼓励大型语言模型进行更深入的推理，从而考虑答案空间中所有可能的候选答案，并仔细分配置信度分数。

Result: 该方法在不同的模型和任务中均显示出优势，无论答案空间是否已知。其优势在强化学习后依然保持，并且分析显示其推理模式符合人类期望。

Conclusion: 预测语言化的概率分布是一种有效的置信度估计方法，可以促进深度推理并提升可靠性，其推理方式与人类思维模式相一致。

Abstract: Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

</details>


### [20] [AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models](https://arxiv.org/abs/2511.14295)
*Mohammad Zbib,Hasan Abed Al Kader Hammoud,Sina Mukalled,Nadine Rizk,Fatima Karnib,Issam Lakkis,Ammar Mohanna,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出AraLingBench，一个用于评估大型语言模型（LLMs）阿拉伯语语言能力的全人工标注基准。


<details>
  <summary>Details</summary>
Motivation: 当前模型在知识基准测试中得分高，但在深层语法和句法推理上表现不佳，显示出真实语言掌握程度的差距。

Method: 通过五个核心类别（语法、形态、拼写、阅读理解、句法）的150个专家设计选择题，评估35个阿拉伯语和双语LLMs的结构性语言理解能力。

Result: 当前模型表现出较强的表面水平熟练度，但在深层语法和句法推理方面存在困难，很多模型通过记忆或模式识别成功，而非真正的理解。

Conclusion: AraLingBench为阿拉伯语LLMs的开发提供了一个诊断框架，强调需要提升模型的真实语言掌握能力。

Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.

</details>


### [21] [ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions](https://arxiv.org/abs/2511.14342)
*Xingwei He,Qianru Zhang,Pengfei Chen,Guanhua Chen,Linlin Yu,Yuan Yuan,Siu-Ming Yiu*

Main category: cs.CL

TL;DR: 本文提出ConInstruct基准测试，用于评估大语言模型检测和解决用户指令中冲突约束的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了大语言模型在复杂提示中处理冲突约束的行为，而这种情况在实际应用中很常见。

Method: 引入ConInstruct数据集，评估模型的冲突检测性能并分析其冲突解决行为。

Result: 专有模型在冲突检测上表现良好，开源模型中仅DeepSeek-R1表现接近专有模型；但所有模型都极少明确通知用户冲突或请求澄清。

Conclusion: 当前大语言模型存在关键缺陷，未来在设计指令跟随模型时需要重点改进冲突处理能力。

Abstract: Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

</details>


### [22] [ATLAS: A High-Difficulty, Multidisciplinary Benchmark for Frontier Scientific Reasoning](https://arxiv.org/abs/2511.14366)
*Hongwei Liu,Junnan Liu,Shudong Liu,Haodong Duan,Yuqiang Li,Mao Su,Xiaohong Liu,Guangtao Zhai,Xinyu Fang,Qianhong Ma,Taolin Zhang,Zihan Ma,Yufeng Zhao,Peiheng Zhou,Linchen Xiao,Wenlong Zhang,Shijie Zhou,Xingjian Ma,Siqi Sun,Jiaye Ge,Meng Li,Yuhong Liu,Jianxin Dong,Jiaying Li,Hui Wu,Hanwen Liang,Jintai Lin,Yanting Wang,Jie Dong,Tong Zhu,Tianfan Fu,Conghui He,Qi Zhang,Songyang Zhang,Lei Bai,Kai Chen*

Main category: cs.CL

TL;DR: ATLAS是一个大型、高难度、跨学科的评估套件，旨在解决现有基准在区分前沿模型时的性能饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在学科狭窄、答案格式过于简化以及易受数据污染等问题，造成与真实科学探究的差距。

Method: ATLAS由领域专家开发，涵盖七个核心科学领域，具有高度原创性、抗污染能力、跨学科重点、高保真答案和严格的质量控制。

Result: 初步结果显示，ATLAS能有效区分领先模型的高级科学推理能力。

Conclusion: ATLAS计划发展为一个长期、开放、社区驱动的平台，为通向人工通用智能的进展提供一个可靠的“尺子”。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to performance saturation on many established benchmarks, questioning their ability to distinguish frontier models. Concurrently, existing high-difficulty benchmarks often suffer from narrow disciplinary focus, oversimplified answer formats, and vulnerability to data contamination, creating a fidelity gap with real-world scientific inquiry. To address these challenges, we introduce ATLAS (AGI-Oriented Testbed for Logical Application in Science), a large-scale, high-difficulty, and cross-disciplinary evaluation suite composed of approximately 800 original problems. Developed by domain experts (PhD-level and above), ATLAS spans seven core scientific fields: mathematics, physics, chemistry, biology, computer science, earth science, and materials science. Its key features include: (1) High Originality and Contamination Resistance, with all questions newly created or substantially adapted to prevent test data leakage; (2) Cross-Disciplinary Focus, designed to assess models' ability to integrate knowledge and reason across scientific domains; (3) High-Fidelity Answers, prioritizing complex, open-ended answers involving multi-step reasoning and LaTeX-formatted expressions over simple multiple-choice questions; and (4) Rigorous Quality Control, employing a multi-stage process of expert peer review and adversarial testing to ensure question difficulty, scientific value, and correctness. We also propose a robust evaluation paradigm using a panel of LLM judges for automated, nuanced assessment of complex answers. Preliminary results on leading models demonstrate ATLAS's effectiveness in differentiating their advanced scientific reasoning capabilities. We plan to develop ATLAS into a long-term, open, community-driven platform to provide a reliable "ruler" for progress toward Artificial General Intelligence.

</details>


### [23] [Mitigating Label Length Bias in Large Language Models](https://arxiv.org/abs/2511.14385)
*Mario Sanz-Guerrero,Katharina von der Wense*

Main category: cs.CL

TL;DR: 该论文提出了一种新的校准方法NCC，用于解决LLMs在候选选项预测中的标签长度偏差问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在进行候选选项预测时会受到标签偏差的影响，现有校准方法忽略了多词标签带来的偏差，尤其是标签长度偏差。

Method: 提出了一种名为归一化上下文校准（NCC）的方法，该方法在完整标签级别进行归一化和校准预测，以缓解标签长度偏差。

Result: NCC在多个数据集和模型上优于现有方法，F1分数提升可达10%，并将偏差缓解扩展到了多项选择问答任务。

Conclusion: NCC与上下文学习结合时，对少量样本选择不敏感，需要更少的样本以实现有竞争力的性能，并产生更可靠的置信度估计。这些发现强调了在实际应用中缓解完整标签偏差的重要性。

Abstract: Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens.

</details>


### [24] [Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education](https://arxiv.org/abs/2511.14423)
*Xin Yi,Yue Li,Dongsheng Shi,Linlin Wang,Xiaoling Wang,Liang He*

Main category: cs.CL

TL;DR: 本文提出 EduHarm 基准和 TSSF 框架，以增强教育场景下大型语言模型的安全性，抵御越狱和微调攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育应用中面临越狱和微调攻击的威胁，但现有研究对教育场景的特殊安全需求关注不足。

Method: 构建了 EduHarm 基准，并提出了一种三阶段防护框架（TSSF），包括安全感知注意力重定向、分层安全判断和防御驱动双路由。

Result: 在多种越狱攻击策略和微调攻击数据集上的实验表明，TSSF 能有效增强安全性，同时避免对良性查询的过度拒绝，并保持良好的实用性。

Conclusion: TSSF 框架在教育场景中为大型语言模型提供了系统且高效的安全保障，同时兼顾了模型的性能和实用性。

Abstract: Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

</details>


### [25] [MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents](https://arxiv.org/abs/2511.14439)
*Jinru Ding,Lu Lu,Chao Ding,Mouxiao Bian,Jiayuan Chen,Renjie Lu,Wenrao Pang,Xiaoqin Wu,Zhiqiang Liu,Luyi Jiang,Bing Han,Yunqiu Wang,Jie Xu*

Main category: cs.CL

TL;DR: MedBench v4是一个全国性云基础的医学评估框架，包含超过700,000个专家策划的任务，涵盖24个主要和91个次要专业。


<details>
  <summary>Details</summary>
Motivation: 为了评估医学大语言模型（LLMs）、多模态模型和智能体在真实临床工作流程和安全限制下的表现。

Method: 通过多阶段和多轮临床医生审核项目，并由校准到人类评分的LLM-as-a-judge对开放式回答评分。

Result: 基础LLMs平均分为54.1/100，Claude Sonnet 4.5表现最佳（62.5/100），但安全和伦理得分低（18.4/100）；多模态模型整体表现较差（平均47.5/100，GPT-5为54.9/100）；智能体性能显著提高（平均79.8/100，Claude Sonnet 4.5达85.3/100）。

Conclusion: 基础模型在多模态推理和安全方面存在差距，但基于治理意识的智能体编排能显著提升临床准备度而不牺牲能力。

Abstract: Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5-based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.

</details>


### [26] [Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning](https://arxiv.org/abs/2511.14445)
*Trishala Jayesh Ahalpara*

Main category: cs.CL

TL;DR: 介绍Tell Me心理健康系统，其利用大语言模型为用户提供个性化、情境感知的心理支持，包括RAG助手、虚拟对话生成器和AI健康小组件。


<details>
  <summary>Details</summary>
Motivation: 解决心理健康支持的可及性问题，并为研究人员提供数据支持，弥补专业心理治疗数据的不足。

Method: 集成三种组件：RAG助手用于个性化对话，基于客户画像的虚拟客户-治疗师对话生成器，以及利用CrewAI实现的健康AI小组件，生成每周自我护理计划和引导冥想音频。

Result: 系统通过自动LLM评估和人用户研究，在精心策划的健康场景中验证了RAG助手的有效性。

Conclusion: 该系统展示了会话助手如何降低支持门槛，补充现有治疗，扩大心理健康资源的获取，并促进NLP研究人员与心理健康专业人士之间的跨学科合作，以推动人类-AI互动中的负责任创新。

Abstract: We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

</details>


### [27] [Agent-R1: Training Powerful LLM Agents with End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.14460)
*Mingyue Cheng,Jie Ouyang,Shuo Yu,Ruiran Yan,Yucong Luo,Zirui Liu,Daoyu Wang,Qi Liu,Enhong Chen*

Main category: cs.CL

TL;DR: 该论文探讨了强化学习（RL）在大型语言模型（LLM）Agent中的有效应用，提出了Agent-R1训练框架，并通过多跳问答任务验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前RL在LLM Agent领域的应用面临关键挑战，包括缺乏针对LLM Agent情境的深入探索以及灵活的训练框架。

Method: 扩展了马尔可夫决策过程（MDP）框架以定义LLM Agent的关键组成部分，并引入了Agent-R1，一个模块化和灵活易用的训练框架。

Result: 在多跳问答基准任务上的实验验证了提出的方法和框架的有效性。

Conclusion: Agent-R1框架为RL在LLM Agent中的应用提供了可行的解决方案，并易于适应不同任务场景和互动环境。

Abstract: Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.

</details>


### [28] [LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation](https://arxiv.org/abs/2511.14531)
*David Carmel,Simone Filice,Guy Horowitz,Yoelle Maarek,Alex Shtoff,Oren Somekh,Ran Tavory*

Main category: cs.CL

TL;DR: 本文介绍了LiveRAG基准，一个公开可用的合成数据集，用于系统评估基于RAG的问答系统。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成（RAG）在生成性AI解决方案中变得越来越重要，需要一个系统的方法来评估其有效性。

Method: LiveRAG基准包含895个合成问题和答案，来源于SIGIR'2025 LiveRAG Challenge，并补充了比赛期间未提供的信息，如正确答案和支持性声明。每个问题还附带由项目反应理论模型估算的难度和区分度评分。

Result: 分析显示，LiveRAG基准的问题具有多样性，难度范围广，并且能够有效区分不同系统的能力。

Conclusion: LiveRAG基准有助于推进RAG研究，进行系统评估，并开发更健壮的问答系统。

Abstract: With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

</details>


### [29] [Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak](https://arxiv.org/abs/2511.14566)
*Lucia Makaiová,Martin Fajčík,Antonín Jarolím*

Main category: cs.CL

TL;DR: 该论文探讨了文档级声明提取的评估方法，提出了一种通过比对模型提取声明和人工注释声明来评估性能的新框架。


<details>
  <summary>Details</summary>
Motivation: 文档级声明提取在事实核查领域仍是一个开放挑战，现有评估提取声明的方法关注较少，因此需要可靠的评估框架来衡量模型性能和注释者间一致性。

Method: 研究比对同一源文档两组声明的方法，计算其相似度，通过比对得分进行评估。探索识别最佳比对和评估方法，以提供更可靠的评估框架。

Result: 实验基于捷克和斯洛伐克新闻评论数据集，结果显示当前评估方法在文档级声明提取中存在局限性，尤其是在处理非正式语言和强本地语境时。

Conclusion: 强调了需要更先进的评估方法，以捕捉语义相似性并评估声明的原子性、可验证性和去上下文化等关键属性。

Abstract: Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.

</details>


### [30] [A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease](https://arxiv.org/abs/2511.14603)
*Yilu Fang,Jordan G. Nestor,Casey N. Ta,Jerard Z. Kneifati-Hayek,Chunhua Weng*

Main category: cs.CL

TL;DR: 该研究利用电子健康记录（EHR）和多状态模型，识别急性肾损伤（AKI）患者发展为慢性肾病（CKD）的风险因素及临床轨迹。


<details>
  <summary>Details</summary>
Motivation: 识别AKI患者中发展至CKD的高风险人群仍具挑战性，需要动态跟踪和预测模型以改善早期干预。

Method: 使用纵向医疗代码和肌酐测量数据，通过聚类和多状态模型分析AKI患者的临床状态转移概率，并进行生存分析识别CKD风险因素。

Result: 在20,699名AKI患者中，3,491名（17%）发展为CKD。研究识别出15个不同的AKI后状态，并发现75%的患者在研究期间保持稳定或仅一次状态转移。已建立和新的CKD风险因素影响各异。

Conclusion: 该数据驱动的方法有助于识别高风险AKI患者，支持开发早期检测和干预CKD的决策支持工具。

Abstract: Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

</details>


### [31] [Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models](https://arxiv.org/abs/2511.14606)
*Shreya Adrita Banik,Niaz Nafi Rahman,Tahsina Moiukh,Farig Sadeque*

Main category: cs.CL

TL;DR: 本文提出一个比较框架，用于评估人类标注与多种大语言模型（GPT、BERT、RoBERTa、FLAN）在新闻政治偏见检测上的一致性。研究发现，RoBERTa微调后最符合人工标注，而GPT在零样本设置下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP技术在自动偏见分类方面有所进展，但大模型与人工判断的一致性仍缺乏深入理解。因此，需要系统评估模型在政治偏见检测任务中与人类判断的差异。

Method: 构建一个人工标注的新闻文章数据集，评估标注一致性、偏见极性和模型间一致性，比较多个LLM（GPT、BERT、RoBERTa、FLAN）与人工标注的对齐程度。

Result: 传统Transformer模型中，RoBERTa与人类标注对齐度最高；生成式模型GPT在零样本设置下整体一致性最强；微调后的RoBERTa在所有模型中准确率和人工标注对齐度最高。

Conclusion: 人类与模型在政治偏见感知上存在系统性差异，应构建结合人类可解释性和模型可扩展性的混合评估框架，以提升自动化媒体偏见检测的可靠性。

Abstract: Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.

</details>


### [32] [Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities](https://arxiv.org/abs/2511.14631)
*Kahaan Gandhi,Boris Bolliet,Inigo Zubeldia*

Main category: cs.CL

TL;DR: 使用视觉语言模型（VLM）引导的多智能体系统，通过将图表作为可验证的检查点，提升了自主科学发现的端到端过程。


<details>
  <summary>Details</summary>
Motivation: 通过VLM-as-a-judge评估图表并根据动态生成的领域特定标准进行实时修正，解决了探索性数据分析中的错误推理路径问题。

Method: 采用VLM作为评判者，结合多智能体系统，在宇宙学和天体化学领域进行案例研究，验证其在数据驱动发现中的有效性。

Result: 在10项任务的基准测试中，VLM增强系统实现了0.7-0.8的通过率，优于仅代码（0.2-0.3）和代码及文本基线（0.4-0.5）。

Conclusion: VLM引导的多智能体系统不仅提升了发现过程的准确性，还提供了可审计的推理路径，提高了结果的可解释性。

Abstract: We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent

</details>


### [33] [A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases](https://arxiv.org/abs/2511.14638)
*Tao Yang,Dandan Huang,Yunting Lin,Pengfei Wu,Zhikun Wu,Gangyuan Ma,Yulan Lu,Xinran Dong,Dingpeng Li,Junshuang Ge,Zhiyan Zhang,Xuanzhao Huang,Wenyan Nong,Yao Zhou,Hui Tang,Hongxi Yang,Shijie Zhang,Juan Li,Xiaojun Cao,Lin Yang,Xia Gao,Kaishou Xu,Xiaoqiong Gu,Wen Zhang,Huimin Xia,Li Liu,Wenhao Zhou,Mulin Jun Li*

Main category: cs.CL

TL;DR: RareSeek R1通过阶段性指令调整、思维链学习和图增强检索，在罕见病诊断上实现了最先进的准确性、强大的泛化能力和对噪声或重叠表型的稳定性。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断耗时长，传统流程与下游推理脱节，大型语言模型面临真实世界电子健康记录稀缺、领域知识滞后和幻觉等问题。

Method: 收集专业临床语料库和医生验证的推理数据集，采用阶段性指令调整、思维链学习和图增强检索的方法开发RareSeek R1。

Result: RareSeek R1在多中心EHR叙述和公共基准上达到最优准确性，增强检索在解决歧义和将候选病因与机制对齐时增益最大。

Conclusion: 该工作推进了以叙述为主、知识整合的推理范式，缩短了诊断过程并实现了可审计、临床可翻译的决策支持。

Abstract: Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

</details>


### [34] [Graded strength of comparative illusions is explained by Bayesian inference](https://arxiv.org/abs/2511.14642)
*Yuhan Zhang,Erxiao Wang,Cory Shain*

Main category: cs.CL

TL;DR: 本文研究比较性错觉（CI）现象，通过贝叶斯推理模型解释为何人们会认为无意义的比较句是可接受的。


<details>
  <summary>Details</summary>
Motivation: 语言处理与视觉处理一样，会受到系统性的错觉影响。比较性错觉（CI）是一个例子，人们倾向于认为无意义的比较句是可接受的。以前的研究提出，这种现象可以通过贝叶斯推理模型来解释，但本研究通过更广泛的实验和模型进行验证和扩展。

Method: 通过结合统计语言模型和人类行为数据，构建了一个后验概率的量化模型，预测CI句的错觉强度，并比较了不同解释的合理性。

Result: 该模型不仅解释了CI效应的细微差别，还解释了以前未解释的代词与完整名词短语在比较句主语中的效应。

Conclusion: 研究结果支持了句子理解的噪声信道理论，并为该理论提供了新颖的预测，从而为语言处理现象的统一计算级理论提供了支持。

Abstract: Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Imagine in Space: Exploring the Frontier of Spatial Intelligence and Reasoning Efficiency in Vision Language Models](https://arxiv.org/abs/2511.13782)
*Xiaoxing Lian,Aidong Yang,Jun Zhu,Peng Wang,Yue Zhang*

Main category: cs.AI

TL;DR: 本文介绍了大型语言模型和视觉语言模型在空间推理方面的局限性，并提出了一个新框架以改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 空间推理是人类认知的重要组成部分，但当前先进的视觉语言模型（VLMs）在此领域仍面临挑战。

Method: 引入SpatiaLite基准，通过综合实验测量空间推理准确性和效率，并提出Imagery Driven Framework（IDF）以改进数据合成和训练。

Result: 1. 高级VLMs主要依赖语言表征进行推理和想象，导致在以视觉为中心的任务中表现不佳。2. 当前空间推理机制效率低下，随着变换复杂性的增加，标记使用量迅速增长。3. 提出的IDF框架可以隐式构建内部世界模型，这对VLMs的空间推理至关重要。

Conclusion: 通过SpatiaLite基准和IDF框架，本文揭示了高级VLMs在空间推理方面的限制和模式，并指出了未来改进的关键方向。

Abstract: Large language models (LLMs) and vision language models (VLMs), such as DeepSeek R1,OpenAI o3, and Gemini 2.5 Pro, have demonstrated remarkable reasoning capabilities across logical inference, problem solving, and decision making. However, spatial reasoning:a fundamental component of human cognition that includes mental rotation, navigation, and spatial relationship comprehension remains a significant challenge for current advanced VLMs. We hypothesize that imagination, the internal simulation of spatial states, is the dominant reasoning mechanism within a spatial world model. To test this hypothesis and systematically probe current VLM spatial reasoning mechanisms, we introduce SpatiaLite, a fully synthetic benchmark that jointly measures spatial reasoning accuracy and reasoning efficiency. Comprehensive experiments reveal three key findings. First, advanced VLMs predominantly rely on linguistic representations for reasoning and imagination, resulting in significant deficiencies on visual centric tasks that demand perceptual spatial relations and 3D geometry transformations such as mental rotation or projection prediction. Second, advanced VLMs exhibit severe inefficiency in their current spatial reasoning mechanisms, with token usage growing rapidly as transformation complexity increases. Third, we propose an Imagery Driven Framework (IDF) for data synthesis and training, which can implicitly construct an internal world model that is critical for spatial reasoning in VLMs. Building on SpatiaLite, this work delineates the spatial reasoning limits and patterns of advanced VLMs, identifies key shortcomings, and informs future advances

</details>


### [36] [Causal computations in Semi Markovian Structural Causal Models using divide and conquer](https://arxiv.org/abs/2511.13852)
*Anna Rodum Bjøru,Rafael Cabañas,Helge Langseth,Antonio Salmerón*

Main category: cs.AI

TL;DR: 本文研究将Bjøru等人提出的用于SCMs的分治算法扩展到半马尔可夫SCMs，解决外生变量影响多个内生变量的问题。


<details>
  <summary>Details</summary>
Motivation: Bjøru等人提出的分治算法在马尔可夫SCMs中表现良好，但无法处理外生变量影响多个内生变量的半马尔可夫模型。因此，本文旨在扩展该方法，以解决更具挑战性的半马尔可夫SCMs。

Method: 使用结构方程的规范表示，将高基数外生变量的SCM分解为多个低基数外生变量的子模型。提出并评估了若干替代解决方案策略，以解决在半马尔可夫模型中的挑战。

Result: 通过理论分析和计算研究，评估了提出的解决方案策略，展示了在半马尔可夫模型中的有效性和可行性。

Conclusion: 本文成功扩展了分治算法到半马尔可夫SCMs，解决了外生变量影响多个内生变量的问题，并提出了有效的替代策略。

Abstract: Recently, Bjøru et al. proposed a novel divide-and-conquer algorithm for bounding counterfactual probabilities in structural causal models (SCMs). They assumed that the SCMs were learned from purely observational data, leading to an imprecise characterization of the marginal distributions of exogenous variables. Their method leveraged the canonical representation of structural equations to decompose a general SCM with high-cardinality exogenous variables into a set of sub-models with low-cardinality exogenous variables. These sub-models had precise marginals over the exogenous variables and therefore admitted efficient exact inference. The aggregated results were used to bound counterfactual probabilities in the original model. The approach was developed for Markovian models, where each exogenous variable affects only a single endogenous variable. In this paper, we investigate extending the methodology to \textit{semi-Markovian} SCMs, where exogenous variables may influence multiple endogenous variables. Such models are capable of representing confounding relationships that Markovian models cannot. We illustrate the challenges of this extension using a minimal example, which motivates a set of alternative solution strategies. These strategies are evaluated both theoretically and through a computational study.

</details>


### [37] [Jailbreaking Large Vision Language Models in Intelligent Transportation Systems](https://arxiv.org/abs/2511.13892)
*Badhan Chandra Das,Md Tasnim Jawad,Md Jueal Mia,M. Hadi Amini,Yanzhao Wu*

Main category: cs.AI

TL;DR: 本文系统分析智能交通系统中大型视觉语言模型在越狱攻击下的漏洞，并提出了一种新的攻击和防御方法。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多种现实应用中表现出强大的多模态推理能力，但其易受越狱攻击。特别是在智能交通系统（ITS）中，这种漏洞可能导致严重的安全风险。

Method: 首先构建一个包含有害交通相关问题的数据集，接着提出一种利用图像排版操作和多轮提示的新型越狱攻击方法，并设计了一种多层响应过滤防御技术。

Result: 通过大量实验，使用GPT-4判断生成响应的有害性得分，并进行人工验证。结果显示，该方法能有效揭示LVLMs的安全漏洞并防止不当响应生成。

Conclusion: 该研究揭示了LVLMs在ITS中面临的安全风险，强调了图像排版操作和多轮提示在越狱攻击中的威胁，同时验证了多层响应过滤防御技术的有效性。

Abstract: Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.

</details>


### [38] [CORGI: Efficient Pattern Matching With Quadratic Guarantees](https://arxiv.org/abs/2511.13942)
*Daniel Weitekamp*

Main category: cs.AI

TL;DR: 本文提出了一种名为CORGI的新匹配算法，解决了规则系统中复杂匹配问题的时间和空间复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 规则系统需在严格时间限制下解决复杂匹配问题，以避免实时应用中出现的性能瓶颈。尤其是在从示例驱动的归纳或代码综合中自动生成规则时，容易导致内存超载和程序执行变慢。

Method: 引入了一种名为CORGI（Collection-Oriented Relational Graph Iteration）的新匹配算法。该算法采用两步法：在前向传递中构建和维护接地关系的图，然后通过迭代器在图中逆向生成匹配，从而避免了传统RETE方法中的高延迟和内存溢出问题。

Result: 在性能评估中，CORGI在一个简单的组合匹配任务中显著优于SOAR和OPS5的RETE实现。

Conclusion: CORGI算法通过避免传统RETE方法中的β-内存，提供了一种更为高效和实用的匹配解决方案，使规则系统在实时应用中更加可靠和高效。

Abstract: Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $β$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

</details>


### [39] [Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios](https://arxiv.org/abs/2511.13970)
*Sanjay Acharjee,Abir Khan Ratul,Diego Patino,Md Nazmus Sakib*

Main category: cs.AI

TL;DR: 该研究开发了一种场景图引导的生成式AI框架，利用OSHA事故报告生成危险场景的逼真图像。


<details>
  <summary>Details</summary>
Motivation: 由于获取真实事故场景数据困难，需要一种方法来合成用于培训视觉模型检测职场危害的图像。

Method: 使用GPT-4o分析OSHA叙述，提取结构化危害推理并转化为场景图，指导文本到图像扩散模型生成图像。引入视觉问题回答框架评估生成数据的真实性和语义保真度。

Result: 提出的VQA Graph Score在四个先进的生成模型中优于CLIP和BLIP指标，通过基于熵的验证，证明其具有更高的判别敏感度。

Conclusion: 该框架能够有效生成逼真且语义准确的危险场景图像，并为视觉模型训练提供可靠的数据。

Abstract: Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.

</details>


### [40] [ALEX:A Light Editing-knowledge Extractor](https://arxiv.org/abs/2511.14018)
*Minghu Wang,Shuliang Zhao,Yuanyuan Zhao,Hongxia Xu*

Main category: cs.AI

TL;DR: 本文提出了ALEX，一个轻量级知识编辑框架，通过分层记忆架构和多种模块提高了大规模语言模型中的知识编辑效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型中的静态知识难以适应动态变化的信息，知识编辑是一项关键任务，但现有方法在可扩展性和检索效率方面存在挑战，特别是在处理需要多步推理的复杂多跳问题时。

Method: ALEX框架引入了分层记忆架构将知识更新组织成语义集群，降低检索复杂度，并集成了Inferential Query Synthesis (IQS)模块和Dynamic Evidence Adjudication (DEA)引擎，实现高效的两阶段检索过程。

Result: 在MQUAKE基准上的实验表明，ALEX显著提高了多跳问题的答案准确性（MultiHop-ACC）和推理路径的可靠性（HopWise-ACC），并将所需的搜索空间减少了80%以上。

Conclusion: ALEX为构建可扩展、高效且准确的知识编辑系统提供了一种有前途的方法。

Abstract: The static nature of knowledge within Large Language Models (LLMs) makes it difficult for them to adapt to evolving information, rendering knowledge editing a critical task. However, existing methods struggle with challenges of scalability and retrieval efficiency, particularly when handling complex, multi-hop questions that require multi-step reasoning. To address these challenges, this paper introduces ALEX (A Light Editing-knowledge Extractor), a lightweight knowledge editing framework. The core innovation of ALEX is its hierarchical memory architecture, which organizes knowledge updates (edits) into semantic clusters. This design fundamentally reduces retrieval complexity from a linear O(N) to a highly scalable O(K+N/C). Furthermore, the framework integrates an Inferential Query Synthesis (IQS) module to bridge the semantic gap between queries and facts , and a Dynamic Evidence Adjudication (DEA) engine that executes an efficient two-stage retrieval process. Experiments on the MQUAKE benchmark demonstrate that ALEX significantly improves both the accuracy of multi-hop answers (MultiHop-ACC) and the reliability of reasoning paths (HopWise-ACC). It also reduces the required search space by over 80% , presenting a promising path toward building scalable, efficient, and accurate knowledge editing systems.

</details>


### [41] [Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation](https://arxiv.org/abs/2511.14023)
*Chiharu Hagiwara,Naoki Nonaka,Yuhta Hashimoto,Ryu Uchimido,Jun Seita*

Main category: cs.AI

TL;DR: 本文介绍了Syn-STARTS框架，利用LLMs生成大规模、高质量的伤患分流案例，以解决真实世界数据难以获取的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模伤亡事件（MCIs）发生频率低，导致难以收集足够多的现场记录，从而影响AI模型在紧急医疗情况下决策性能的开发和评估。

Method: 开发Syn-STARTS框架，利用大型语言模型（LLMs）生成伤患分流案例，并验证其与手动整理的真实数据集在质量上的一致性。

Result: Syn-STARTS生成的分流案例在质量上与手动整理的TRIAGE开放数据集无法区分，并且在标准分流方法START定义的各类别中，LLM的准确性表现高度稳定。

Conclusion: 该研究强烈表明，合成数据在开发和评估高性能AI模型方面具有巨大潜力，尤其是在严重和关键的医学情况下。

Abstract: Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.

</details>


### [42] [Making Evidence Actionable in Adaptive Learning](https://arxiv.org/abs/2511.14052)
*Amirreza Mehrabi,Jason W. Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.AI

TL;DR: 该研究通过引入教师主导的反馈循环，将概念级评估转化为微干预，提升适应性学习的干预效果。


<details>
  <summary>Details</summary>
Motivation: 适应性学习在诊断上很精确，但干预较弱，导致帮助时机不当或不对齐。

Method: 提出了一种包含三种保障的适应性学习算法：充足性、注意力和多样性。将干预分配形式化为带有约束的二进制整数程序，并采用贪婪选择、基于梯度的松弛以及混合方法。

Result: 在模拟和1204名学生的物理课程中，两种求解器在有限的观看时间内实现了对所有学习者的完整技能覆盖。基于梯度的方法减少了冗余覆盖，贪婪方法在计算成本上更有优势。

Conclusion: 该方法形成了一个可追踪的控制器，能够闭环诊断和教学，实现公平、负载感知的个性化学习。

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.

</details>


### [43] [APD-Agents: A Large Language Model-Driven Multi-Agents Collaborative Framework for Automated Page Design](https://arxiv.org/abs/2511.14101)
*Xinpeng Chen,Xiaofeng Han,Kaihao Zhang,Guochao Ren,Yujie Wang,Wenhao Cao,Yang Zhou,Jianfeng Lu,Zhenbo Song*

Main category: cs.AI

TL;DR: 提出了一种基于大模型的多智能体框架APD-agents，用于移动应用的自动页面设计，通过多个智能体协同工作，实现从用户描述到布局生成的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 移动应用页面设计过程耗时且需要专业技能，现有设计软件需要大量培训，且协同设计需额外时间保持一致风格。因此，需要一种自动化且高效的解决方案。

Method: APD-agents框架包含OrchestratorAgent、SemanticParserAgent、PrimaryLayoutAgent、TemplateRetrievalAgent和RecursiveComponentAgent，通过多智能体协作，将用户描述转化为结构化布局。

Result: 在RICO数据集上的实验表明，APD-agents实现了最先进的性能，有效提升了布局生成的质量。

Conclusion: 通过利用大模型驱动的多智能体系统，APD-agents能够高效自动化地完成移动应用页面设计，显著降低设计师的工作负担。

Abstract: Layout design is a crucial step in developing mobile app pages. However, crafting satisfactory designs is time-intensive for designers: they need to consider which controls and content to present on the page, and then repeatedly adjust their size, position, and style for better aesthetics and structure. Although many design software can now help to perform these repetitive tasks, extensive training is needed to use them effectively. Moreover, collaborative design across app pages demands extra time to align standards and ensure consistent styling. In this work, we propose APD-agents, a large language model (LLM) driven multi-agent framework for automated page design in mobile applications. Our framework contains OrchestratorAgent, SemanticParserAgent, PrimaryLayoutAgent, TemplateRetrievalAgent, and RecursiveComponentAgent. Upon receiving the user's description of the page, the OrchestratorAgent can dynamically can direct other agents to accomplish users' design task. To be specific, the SemanticParserAgent is responsible for converting users' descriptions of page content into structured data. The PrimaryLayoutAgent can generate an initial coarse-grained layout of this page. The TemplateRetrievalAgent can fetch semantically relevant few-shot examples and enhance the quality of layout generation. Besides, a RecursiveComponentAgent can be used to decide how to recursively generate all the fine-grained sub-elements it contains for each element in the layout. Our work fully leverages the automatic collaboration capabilities of large-model-driven multi-agent systems. Experimental results on the RICO dataset show that our APD-agents achieve state-of-the-art performance.

</details>


### [44] [Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation](https://arxiv.org/abs/2511.14131)
*Yu Zhong,Zihao Zhang,Rui Zhang,Lingdong Huang,Haihan Gao,Shuo Wang,Da Li,Ruijian Han,Jiaming Guo,Shaohui Peng,Di Huang,Yunji Chen*

Main category: cs.AI

TL;DR: 提出了一种新颖的双过程思维框架R3，通过集成大型语言模型（LLMs）与VLN特定专业知识，以零样本方式提升视觉-语言导航（VLN）任务性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在VLN任务中存在对空间相关性理解不准确、计算成本高和推理延迟大的问题。为了克服这些限制，作者提出结合LLMs的泛化能力和VLN专用模型的效率。

Method: 提出的R3框架包含三个核心模块：1) Runner，一个轻量级基于Transformer的专家模型，用于高效准确的导航；2) Ruminator，采用多模态LLM与思维链（CoT）提示，用于结构化推理；3) Regulator，监控导航进度并根据三个标准选择适当的思维模式。

Result: 在REVERIE基准测试中，R3显著优于其他最先进方法，SPL和RGSPL分别提高了3.28%和3.30%。

Conclusion: R3框架通过有效结合LLMs和VLN特定专家模型，显著提升了复杂VLN任务的性能，展示了其处理挑战性任务的有效性。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.

</details>


### [45] [Do Large Language Models (LLMs) Understand Chronology?](https://arxiv.org/abs/2511.14214)
*Pattaraphon Kenny Wongchamcharoen,Paul Glasserman*

Main category: cs.AI

TL;DR: 本文测试了大型语言模型在金融经济领域的时间顺序理解能力，发现模型在复杂时间排序任务上存在局限，但增加推理资源可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融经济领域被广泛应用，但其在时间顺序理解方面的能力尚未被充分研究，尤其是在防止前瞻偏误(look-ahead bias)方面的表现。

Method: 通过三类任务测试模型的时间顺序理解能力：(1)时间排序(2)条件排序(过滤后排序)(3)时代错误检测。测试了GPT-4.1、Claude-3.7 Sonnet(带/不带扩展思维)和GPT-5在不同推理强度下的表现。

Result: 1) 所有模型在序列变长时精确匹配率明显下降，但排名相关性保持较高；2) 条件排序中主要失败在过滤步骤；3) 时代错误检测相对最容易但性能仍会下降；4) GPT-5在中高推理强度下表现最佳，能完美完成所有长度的时间排序和条件排序。

Conclusion: 当前大型语言模型在时间顺序任务上存在局限，但增加显性推理预算能显著提升性能。这些发现对模型在金融实时应用中的使用有重要指导意义，并揭示了模型能力边界和推理帮助的场景。

Abstract: Large language models (LLMs) are increasingly used in finance and economics, where prompt-based attempts against look-ahead bias implicitly assume that models understand chronology. We test this fundamental question with a series of chronological ordering tasks with increasing complexities over facts the model already knows from pre-training. Our tasks cover (1) chronological ordering, (2) conditional sorting (filter, then order), and (3) anachronism detection. We evaluate GPT-4.1, Claude-3.7 Sonnet, with and without Extended Thinking (ET), and GPT-5 across multiple reasoning-effort settings. Across models, Exact match rate drops sharply as sequences lengthen even while rank correlations stay high as LLMs largely preserve local order but struggle to maintain a single globally consistent timeline. In conditional sorting, most failures stem from the filtering step rather than the ordering step, but GPT-5 and Claude-3.7 Sonnet with Extended Thinking outshine normal models significantly. Lastly, anachronism detection is found to be the easiest task for the LLMs but performance still declines with increasingly overlapping timelines or entities. Overall, our main contribution is showing that allocating explicit reasoning budget helps with chronological ordering with GPT-5 at medium/high reasoning effort achieving flawless ordering at all lengths and perfect conditional sorting (both self-filtered and given-subset), whereas low/minimal effort degrades with longer lists, mirroring earlier models. Our findings delineate limits of current LLMs on chronological tasks, providing insights into task complexity, and demonstrate scenarios in which reasoning helps. These patterns are important for the real-time application of LLMs in finance. We release all code and evaluation templates to support full reproducibility.

</details>


### [46] [Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation](https://arxiv.org/abs/2511.14219)
*Kumud Tripathi,Aditya Srinivas Menon,Aman Gaurav,Raj Prakash Gohil,Pankaj Wasnik*

Main category: cs.AI

TL;DR: 提出了一种两阶段架构，通过自适应层注意力（ALA）和多目标知识蒸馏（KD）框架减少Whisper模型的幻觉错误。


<details>
  <summary>Details</summary>
Motivation: Whisper模型在嘈杂环境中容易出现幻觉错误，现有方法多集中在音频预处理或转录后处理，直接修改Whisper模型以减少幻觉的研究较少。

Method: 第一阶段通过自适应层注意力（ALA）增强编码器鲁棒性，第二阶段使用多目标知识蒸馏（KD）框架抑制幻觉。

Result: 在嘈杂语音基准测试中显著降低幻觉和词错误率，同时在干净语音上保持性能。

Conclusion: ALA和KD为在真实嘈杂条件下提高Whisper模型的可靠性提供了一种原则性策略。

Abstract: The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.

</details>


### [47] [DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home](https://arxiv.org/abs/2511.14227)
*Yuxiang Wang,Siwen Wang,Haowei Han,Ao Wang,Boya Liu,Yong Zhao,Chengbo Wu,Bin Zhu,Bin Qin,Xiaokai Zhou,Xiao Yan,Jiawei Jiang,Bo Du*

Main category: cs.AI

TL;DR: DevPiolt是一个基于LLM的IoT设备操作推荐模型，通过持续预训练和多任务微调增强LLM的领域知识，利用直接偏好优化和基于置信度的曝光控制机制，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: IoT设备操作推荐对于提升用户满意度和企业利润至关重要。然而，现有的推荐模型在复杂操作逻辑、多样化用户偏好以及对次优建议的敏感性方面存在问题，限制了其在IoT设备操作中的适用性。

Method: DevPiolt模型首先通过持续预训练和多任务微调，为LLM配备IoT操作的基本领域知识。然后，采用直接偏好优化，将微调后的LLM与特定用户偏好对齐。最后，设计了一个基于置信度的曝光控制机制，以避免低质量推荐带来的负面用户体验。

Result: 大量实验表明，DevPiolt在所有数据集上显著优于基线模型，所有指标平均提高了69.5%。该模型已在小米家庭应用中实际部署，为255,000名用户提供每日操作推荐。在线实验结果显示，设备覆盖率提高了21.6%，页面查看接受率提高了29.1%。

Conclusion: DevPiolt模型在IoT设备操作推荐任务中表现优异，通过引入领域知识、用户偏好对齐和曝光控制机制，有效提升了推荐效果，并在实际应用中取得了显著成效。

Abstract: Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.

</details>


### [48] [Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility](https://arxiv.org/abs/2511.14248)
*Hongju Lee,Youngjun Park,Jisun An,Dongman Lee*

Main category: cs.AI

TL;DR: 提出了一种新颖的时间序列预测框架，用于在区域层面预测Airbnb的关键指标。


<details>
  <summary>Details</summary>
Motivation: 短期租赁平台扩张导致住房市场混乱，准确预测Airbnb市场趋势对政策制定者和城市规划者至关重要。

Method: 结合房源特征和外部上下文因素，将结构化表格数据转换为基于提示的输入，利用LLM生成区域嵌入，并输入到时间序列模型中。

Result: 在首尔的Airbnb数据集上，该方法比传统基线模型降低了约48%的RMSE和MAE。

Conclusion: 该框架提高了预测准确性，并为检测供过于求的区域和制定数据驱动的城市政策提供了实用见解。

Abstract: The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.

</details>


### [49] [PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models](https://arxiv.org/abs/2511.14256)
*Yu Liu,Xixun Lin,Yanmin Shang,Yangxi Li,Shi Wang,Yanan Cao*

Main category: cs.AI

TL;DR: PathMind 是一种通过选择性引导重要推理路径来增强忠实且可解释的推理的框架。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的知识图谱推理方法存在两个局限：一是无差别提取推理路径而不评估其重要性，二是动态探索路径需要高检索需求和频繁调用大语言模型。

Method: PathMind 遵循“检索-优先-推理”模式，包括查询子图检索、路径优先机制以及两阶段训练策略，即任务特定的指令调优和路径优先对齐。

Result: PathMind 在基准数据集上的实验证明，它在复杂推理任务中优于竞争基线，尤其在输入标记较少的情况下。

Conclusion: 通过识别基本推理路径，PathMind 提高了推理的准确性和逻辑一致性，减少了无关噪音和检索需求。

Abstract: Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

</details>


### [50] [DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning](https://arxiv.org/abs/2511.14299)
*Xiaochuan Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为DataSage的新型多智能体框架，通过外部知识检索、多角色辩论机制和多路径推理来提升自动化数据洞察发现的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据洞察智能体在利用领域知识、分析深度和代码生成准确性方面存在局限，导致结果不理想。

Method: DataSage框架包含三个创新特点：外部知识检索、多角色辩论机制和多路径推理，以丰富分析上下文、深化分析深度并提高代码和洞察的准确性。

Result: 在InsightBench上的广泛实验表明，DataSage在所有难度级别上均优于现有数据洞察智能体。

Conclusion: DataSage为自动化数据洞察发现提供了一个有效的解决方案，通过其创新特性显著提升了分析性能。

Abstract: In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

</details>


### [51] [When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling](https://arxiv.org/abs/2511.14334)
*Alessio Pellegrino,Jacopo Mauro*

Main category: cs.AI

TL;DR: 大型语言模型在自动从自然语言生成可执行优化模型方面表现看似成功，但这种成功可能源于数据污染而非真正的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在检验大型语言模型是否真正理解约束编程问题，而不是依赖训练数据中的标准问题。

Method: 通过系统性地重述和扰动CSPLib问题，保持其结构但修改上下文和引入误导元素，然后比较三种LLM模型在原始和修改描述下的表现。

Result: 虽然LLM可以生成语法上有效和语义上合理的模型，但在上下文和语言变化下性能显著下降，显示出浅层的理解和措辞敏感性。

Conclusion: LLM在约束编程问题上的表现受到措辞和上下文的影响，表明其理解能力有限，容易受到数据污染的影响。

Abstract: One of the long-standing goals in optimisation and constraint programming is to describe a problem in natural language and automatically obtain an executable, efficient model. Large language models appear to bring this vision closer, showing impressive results in automatically generating models for classical benchmarks. However, much of this apparent success may derive from data contamination rather than genuine reasoning: many standard CP problems are likely included in the training data of these models. To examine this hypothesis, we systematically rephrased and perturbed a set of well-known CSPLib problems to preserve their structure while modifying their context and introducing misleading elements. We then compared the models produced by three representative LLMs across original and modified descriptions. Our qualitative analysis shows that while LLMs can produce syntactically valid and semantically plausible models, their performance drops sharply under contextual and linguistic variation, revealing shallow understanding and sensitivity to wording.

</details>


### [52] [Operationalizing Pluralistic Values in Large Language Model Alignment Reveals Trade-offs in Safety, Inclusivity, and Model Behavior](https://arxiv.org/abs/2511.14476)
*Dalia Ali,Dora Zhao,Allison Koenecke,Orestis Papakyriakopoulos*

Main category: cs.AI

TL;DR: 该研究探讨了多元价值观对大型语言模型（LLM）行为的影响，通过系统评估对齐管道中的人口统计差异和设计参数。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的安全和对齐训练常忽视人类的社会多样性，需要在对齐过程中平衡专家驱动和用户驱动的信号，以确保安全性和公平代表性。

Method: 研究收集了美国和德国参与者（N = 1,095, 27,375个评分）的对齐数据，通过五个维度对LLM响应进行评分，并使用不同社会群体的偏好微调多个大型语言模型和推理模型，同时变化评分尺度、处理分歧的方法以及优化技术。

Result: 不同人口统计组之间存在系统性差异；模型在特定群体偏好下微调后表现出不同行为；保留评分者分歧比多数投票减少53%毒性；5分制比二分制减少22%毒性；DPO在多值优化中表现优于GRPO。

Conclusion: 该研究为如何在模型对齐中平衡专家驱动和用户驱动的信号提供了初步答案，强调了社会多样性在对齐过程中的重要性。

Abstract: Although large language models (LLMs) are increasingly trained using human feedback for safety and alignment with human values, alignment decisions often overlook human social diversity. This study examines how incorporating pluralistic values affects LLM behavior by systematically evaluating demographic variation and design parameters in the alignment pipeline. We collected alignment data from US and German participants (N = 1,095, 27,375 ratings) who rated LLM responses across five dimensions: Toxicity, Emotional Awareness (EA), Sensitivity, Stereotypical Bias, and Helpfulness. We fine-tuned multiple Large Language Models and Large Reasoning Models using preferences from different social groups while varying rating scales, disagreement handling methods, and optimization techniques. The results revealed systematic demographic effects: male participants rated responses 18% less toxic than female participants; conservative and Black participants rated responses 27.9% and 44% more emotionally aware than liberal and White participants, respectively. Models fine-tuned on group-specific preferences exhibited distinct behaviors. Technical design choices showed strong effects: the preservation of rater disagreement achieved roughly 53% greater toxicity reduction than majority voting, and 5-point scales yielded about 22% more reduction than binary formats; and Direct Preference Optimization (DPO) consistently outperformed Group Relative Policy Optimization (GRPO) in multi-value optimization. These findings represent a preliminary step in answering a critical question: How should alignment balance expert-driven and user-driven signals to ensure both safety and fair representation?

</details>


### [53] [Rate-Distortion Guided Knowledge Graph Construction from Lecture Notes Using Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2511.14595)
*Yuan An,Ruhma Hashmi,Michelle Rogers,Jane Greenberg,Brian K. Smith*

Main category: cs.AI

TL;DR: 提出了一种基于率失真理论和最优传输几何的知识图构建和优化框架，用于从非结构化教育材料生成高质量多选题。


<details>
  <summary>Details</summary>
Motivation: 任务导向的知识图可用于AI学习辅助系统自动生成高质量多选题，但将非结构化教育材料转化为捕捉关键教学内容的知识图仍然困难。

Method: 框架将讲座内容建模为度量-测度空间，使用Fused Gromov-Wasserstein (FGW)耦合对齐候选知识图，并通过率失真拉格朗日乘子法进行优化，采用加、合并、分裂、删除、重连等操作。

Result: 应用于数据科学讲座的原型生成了可解释的率失真曲线，且由优化知识图生成的多选题在十五个质量标准上均优于原始笔记生成的多选题。

Conclusion: 该研究为个性化及AI辅助教育中的信息理论知识图优化奠定了原则性基础。

Abstract: Task-oriented knowledge graphs (KGs) enable AI-powered learning assistant systems to automatically generate high-quality multiple-choice questions (MCQs). Yet converting unstructured educational materials, such as lecture notes and slides, into KGs that capture key pedagogical content remains difficult. We propose a framework for knowledge graph construction and refinement grounded in rate-distortion (RD) theory and optimal transport geometry. In the framework, lecture content is modeled as a metric-measure space, capturing semantic and relational structure, while candidate KGs are aligned using Fused Gromov-Wasserstein (FGW) couplings to quantify semantic distortion. The rate term, expressed via the size of KG, reflects complexity and compactness. Refinement operators (add, merge, split, remove, rewire) minimize the rate-distortion Lagrangian, yielding compact, information-preserving KGs. Our prototype applied to data science lectures yields interpretable RD curves and shows that MCQs generated from refined KGs consistently surpass those from raw notes on fifteen quality criteria. This study establishes a principled foundation for information-theoretic KG optimization in personalized and AI-assisted education.

</details>
