<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Harmonic Token Projection (HTP): A Vocabulary-Free, Training-Free, Deterministic, and Reversible Embedding Methodology](https://arxiv.org/abs/2511.20665)
*Tcharlies Schmitz*

Main category: cs.CL

TL;DR: 提出一种无需训练、词汇表或随机参数的可逆且确定性的文本嵌入框架HTP，通过谐波轨迹将Unicode整数映射为连续向量，实现高效、可解释的多语言语义相似性计算。


<details>
  <summary>Details</summary>
Motivation: 传统神经嵌入依赖统计共现和优化，缺乏可解释性和可逆性；HTP旨在通过确定性几何方法构建透明、高效且支持多语言的替代方案。

Method: 将每个token的Unicode整数表示解析为谐波轨迹，建立离散符号与连续向量空间之间的双射和可解释映射，利用相位相干性保持结构和可逆性。

Result: 在STS-B及其多语言扩展上，HTP英语Spearman相关系数达0.68，在10种语言上性能稳定，计算成本极低，每句对延迟低于1毫秒。

Conclusion: 确定性几何可产生有意义的语义关系，HTP为数据驱动嵌入提供了透明高效的替代方案，验证了无训练、无词汇表嵌入的可行性。

Abstract: This paper introduces the Harmonic Token Projection (HTP), a reversible and deterministic framework for generating text embeddings without training, vocabularies, or stochastic parameters. Unlike neural embeddings that rely on statistical co-occurrence or optimization, HTP encodes each token analytically as a harmonic trajectory derived from its Unicode integer representation, establishing a bijective and interpretable mapping between discrete symbols and continuous vector space. The harmonic formulation provides phase-coherent projections that preserve both structure and reversibility, enabling semantic similarity estimation from purely geometric alignment. Experimental evaluation on the Semantic Textual Similarity Benchmark (STS-B) and its multilingual extension shows that HTP achieves a Spearman correlation of \r{ho} = 0.68 in English, maintaining stable performance across ten languages with negligible computational cost and sub-millisecond latency per sentence pair. This demonstrates that meaningful semantic relations can emerge from deterministic geometry, offering a transparent and efficient alternative to data-driven embeddings. Keywords: Harmonic Token Projection, reversible embedding, deterministic encoding, semantic similarity, multilingual representation.

</details>


### [2] [PIRA: Preference-Oriented Instruction-Tuned Reward Models with Dual Aggregation](https://arxiv.org/abs/2511.20668)
*Yongfu Xue*

Main category: cs.CL

TL;DR: 提出了一种新的训练范式PIRA，通过三种策略解决传统判别式奖励模型的数据效率低和奖励过优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统判别式奖励模型存在数据效率低和奖励过优化的问题，需要新的方法来提高奖励模型的性能和稳定性。

Method: PIRA通过以下三种策略：1）将问答对重构为基于偏好的指令，2）从不同偏好任务中聚合奖励以减少偏差并提高鲁棒性，3）在不同dropout率下平均价值头输出以稳定奖励。

Result: 大量实验证明了PIRA的有效性。

Conclusion: PIRA提供了一种有效的方法来提高奖励模型的性能和稳定性，解决了传统方法的两个主要挑战。

Abstract: Reward models are crucial for aligning Large Language Models (LLMs) with human preferences but face two representative challenges. First, traditional discriminative reward models usually concatenate questions and responses directly as input, resulting in low data efficiency. Second, reward models are vulnerable to reward overoptimization. We propose PIRA, a training paradigm addressing these issues through three strategies: (1) Reformulating question-answer pairs into preference-based instructions for clearer and more explicit task specification, (2) aggregating rewards from diverse preference tasks to reduce bias and improve robustness, and (3) averaging value-head outputs under varying dropout rates to stabilize rewards. Extensive experiments have demonstrated the effectiveness of PIRA.

</details>


### [3] [Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data](https://arxiv.org/abs/2511.20669)
*Mann Khatri,Mirza Yusuf,Rajiv Ratn Shah,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在法律领域的应用，通过三种实验方式提升模型在零样本设定下的法律判决预测性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用推理方面表现出色，但在法律等专业领域由于缺乏特定领域的预训练，表现不佳。法律文件通常冗长复杂，处理全文对模型来说效率低下。

Method: 通过在三个印度法律判决预测数据集上进行零样本设定实验，研究包括：(i) 基于修辞角色重组文件，(ii) 定义修辞角色以熟悉法律术语，(iii) 模仿法院逐步推理过程。

Result: 实验显示，通过组织数据或解释关键法律术语，模型性能显著提高，F1分数相较基线提升至少1.5%，最多提升4.36%。

Conclusion: 结构化信息和法律术语解释对提升大型语言模型在法律任务中的表现有显著作用。

Abstract: Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.

</details>


### [4] [MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data](https://arxiv.org/abs/2511.20672)
*Saad Mankarious,Ayah Zirikly,Daniel Wiechmann,Elma Kerz,Edward Kempa,Yu Qiao*

Main category: cs.CL

TL;DR: 提出了一个新的基准数据集MindSET，用于社交媒体上的心理健康分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集由于数据有限、清理不充分和社交媒体内容多样性而变得过时，因此需要一个新的、更全面的基准。

Method: 从Reddit收集数据，使用自我报告诊断，经过严格预处理，包括语言过滤和去除NSFW及重复内容，进行二分类实验以验证数据集有效性。

Result: MindSET包含超过1300万条标注帖子，涵盖七种心理健康状况，模型在MindSET上的表现优于以往基准，自闭症检测的F1分数提高达18分。

Conclusion: MindSET为研究者提供了一个强大基础，可用于探索社交媒体与心理健康之间的联系，支持早期风险检测和深入分析心理趋势。

Abstract: Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.

</details>


### [5] [Semantics Meet Signals: Dual Codebook Representationl Learning for Generative Recommendation](https://arxiv.org/abs/2511.20673)
*Zheng Hui,Xiaokai Wei,Reza Shirkavand,Chen Wang,Weizhi Zhang,Alejandro Peláez,Michelle Gong*

Main category: cs.CL

TL;DR: 该论文提出了FlexCode，这是一种在生成推荐中根据项目流行度自适应分配token预算的新框架，以提高表示效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的生成推荐方法使用单一且统一的码本对所有项目进行编码，忽视了流行项目与长尾项目在协作信号和语义理解上的不平衡，因此限制了其表示效率和泛化能力。

Method: FlexCode框架通过一个轻量级MoE（专家混合）动态平衡CF（协同过滤）特定精度和语义泛化，并采用对齐和平滑目标来维持整个流行度谱的一致性。

Result: 在公共和工业级数据集上的实验表明，FlexCode始终优于强基线，并在准确性和尾部稳健性上表现更佳。

Conclusion: FlexCode为生成推荐中的token表示提供了新机制，在提高准确性的同时也增强了对长尾项目的处理能力，为token-based推荐模型中记忆与泛化的平衡提供了新视角。

Abstract: Generative recommendation has recently emerged as a powerful paradigm that unifies retrieval and generation, representing items as discrete semantic tokens and enabling flexible sequence modeling with autoregressive models. Despite its success, existing approaches rely on a single, uniform codebook to encode all items, overlooking the inherent imbalance between popular items rich in collaborative signals and long-tail items that depend on semantic understanding. We argue that this uniform treatment limits representational efficiency and hinders generalization. To address this, we introduce FlexCode, a popularity-aware framework that adaptively allocates a fixed token budget between a collaborative filtering (CF) codebook and a semantic codebook. A lightweight MoE dynamically balances CF-specific precision and semantic generalization, while an alignment and smoothness objective maintains coherence across the popularity spectrum. We perform experiments on both public and industrial-scale datasets, showing that FlexCode consistently outperform strong baselines. FlexCode provides a new mechanism for token representation in generative recommenders, achieving stronger accuracy and tail robustness, and offering a new perspective on balancing memorization and generalization in token-based recommendation models.

</details>


### [6] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed,May Alsofyani,Saad Almohaimeed,Mansour Al Ghanim,Liqiang Wang*

Main category: cs.CL

TL;DR: 本文介绍了首个阿拉伯语跨领域、上下文依赖的文本到SQL数据集Ar-SParC，并提出了GAT corrector方法以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL的研究主要集中在英语和中文，缺乏对阿拉伯语的支持，因此需要构建相关数据集和方法以促进阿拉伯语自然语言与数据库的交互。

Method: 构建了Ar-SParC数据集，包含3,450个问题序列和10,225个问题及其SQL查询；使用GPT-3.5-turbo和GPT-4.5-turbo模型，结合10种提示技术进行实验，并提出GAT corrector方法。

Result: GAT corrector在所有实验中平均提升了1.9%的执行准确率（EX）和1.9%的交互准确率（IX）（零样本设置下），以及1.72% EX和0.92% IX（上下文学习设置下）。

Conclusion: Ar-SParC填补了阿拉伯语文本到SQL任务的空白，GAT corrector方法显著提升了模型性能，尤其在阿拉伯语场景下优于先前的GAT verifier技术。

Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.

</details>


### [7] [Cognitive bias in LLM reasoning compromises interpretation of clinical oncology notes](https://arxiv.org/abs/2511.20680)
*Matthew W. Kenaston,Umair Ayub,Mihir Parmar,Muhammad Umair Anjum,Syed Arsalan Ahmed Naqvi,Priya Kumar,Samarth Rawal,Aadel A. Chaudhuri,Yousef Zakharia,Elizabeth I. Heath,Tanios S. Bekaii-Saab,Cui Tao,Eliezer M. Van Allen,Ben Zhou,YooJung Choi,Chitta Baral,Irbaz Bin Riaz*

Main category: cs.CL

TL;DR: 尽管大型语言模型在临床基准测试中表现出色，但其可能通过错误的推理得出正确的结论，这种错误模式在肿瘤学决策支持中具有安全影响，而基于准确性的评估无法捕捉这种影响。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在临床应用中可能因推理错误导致安全隐患的问题，尤其是在肿瘤学决策支持中，准确性评估无法完全捕捉推理错误。

Method: 通过两个队列的回顾性研究，从GPT-4思维链响应中开发了一个推理错误的分层分类法，并测试了其临床相关性。使用CORAL数据集中的乳腺癌和胰腺癌记录，对600条推理链进行标注，定义了一个三层分类法，将计算失败映射到认知偏差框架。

Result: 在23%的解释中出现推理错误，并且主导了整体错误，确认偏误和锚定偏误最常见。推理失败与指南不一致且潜在有害的建议相关，特别是在晚期疾病管理中。使用最先进的语言模型的自动化评估器能检测到错误的存在，但无法可靠地分类错误子类型。

Conclusion: 大型语言模型在推理有误时可能提供流畅但临床上不安全的建议。该分类法为临床部署前评估和改善推理保真度提供了可推广的框架。

Abstract: Despite high performance on clinical benchmarks, large language models may reach correct conclusions through faulty reasoning, a failure mode with safety implications for oncology decision support that is not captured by accuracy-based evaluation. In this two-cohort retrospective study, we developed a hierarchical taxonomy of reasoning errors from GPT-4 chain-of-thought responses to real oncology notes and tested its clinical relevance. Using breast and pancreatic cancer notes from the CORAL dataset, we annotated 600 reasoning traces to define a three-tier taxonomy mapping computational failures to cognitive bias frameworks. We validated the taxonomy on 822 responses from prostate cancer consult notes spanning localized through metastatic disease, simulating extraction, analysis, and clinical recommendation tasks. Reasoning errors occurred in 23 percent of interpretations and dominated overall errors, with confirmation bias and anchoring bias most common. Reasoning failures were associated with guideline-discordant and potentially harmful recommendations, particularly in advanced disease management. Automated evaluators using state-of-the-art language models detected error presence but could not reliably classify subtypes. These findings show that large language models may provide fluent but clinically unsafe recommendations when reasoning is flawed. The taxonomy provides a generalizable framework for evaluating and improving reasoning fidelity before clinical deployment.

</details>


### [8] [Dynamic Template Selection for Output Token Generation Optimization: MLP-Based and Transformer Approaches](https://arxiv.org/abs/2511.20683)
*Bharadwaj Yadavalli*

Main category: cs.CL

TL;DR: 提出动态模板选择(DTS)方法，通过匹配查询复杂度与响应模板来降低大语言模型使用成本，在不影响质量前提下实现32-33%的token减少。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型对所有查询类型使用统一的冗长响应模式，导致token效率低下，而输出token成本比输入高4-8倍，亟需优化。

Method: 提出动态模板选择(DTS)框架，比较了MLP路由器和微调RoBERTa两种方法，使用预计算嵌入进行查询复杂度分类。

Result: 在1000个MMLU问题上测试，MLP路由器达到90.5%准确率（优于RoBERTa的89.5%），参数少1.25亿；在9000次API调用验证中，token减少32.6%-33.9%。

Conclusion: DTS方法能有效降低大语言模型使用成本，具有提供商无关的泛化能力，提供了完整的理论框架、四种算法和实证验证。

Abstract: Contemporary large language model deployments typically employ uniform prompting strategies across diverse query types, applying verbose response patterns to both complex analytical tasks and straightforward factual questions. This one-size-fits-all methodology leads to substantial token inefficiency, a concern amplified by the significant cost differential between input and output tokens--the latter commanding 4-8x higher prices across major providers. We present Dynamic Template Selection (DTS), which adaptively matches response templates to query complexity, achieving significant cost reductions without compromising response quality.
  We compared two routing approaches: a simple MLP that uses pre-computed embeddings and a more complex fine-tuned RoBERTa transformer. Through comprehensive evaluation on 1,000 MMLU questions, we find that the MLP router achieves 90.5% routing accuracy on held-out test data, marginally exceeding RoBERTa's performance (89.5%) despite utilizing 125M fewer parameters. Notably, our empirical analysis reveals provider-agnostic behavior in template selection--routing decisions generalize effectively across 3 major LLM providers (OpenAI GPT-4, Google Gemini, and Anthropic Claude), as validated through 9,000 production API calls. While routing accuracy remains consistent at 90.5% across providers, observed token reductions vary from 32.6% to 33.9%, reflecting provider-specific generation characteristics.
  This work contributes several key elements: formal problem formulation with theoretical grounding in machine learning, four algorithms with corresponding complexity analyses, and extensive empirical validation across production systems.

</details>


### [9] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang,Yadong Yu,Wenqiang Kang,Jian Zhou,Dongyue Gao,Pan Xiang,Zhe Liu,Mengyan Dai,Zhonglu Guo,Zhimei Sun*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>


### [10] [Memories Retrieved from Many Paths: A Multi-Prefix Framework for Robust Detection of Training Data Leakage in Large Language Models](https://arxiv.org/abs/2511.20799)
*Trung Cuong Dang,David Mohaisen*

Main category: cs.CL

TL;DR: 大型语言模型因训练于海量语料，容易出现对训练数据的逐字记忆，带来显著的隐私和版权风险。本文提出了一种新的多前缀记忆框架来更全面捕捉记忆现象，尤其在已对齐的模型中。


<details>
  <summary>Details</summary>
Motivation: 现有记忆化定义存在不足，无法全面捕捉对齐模型中的记忆现象，因此需要一种新的、更全面的记忆化定义来解决这一问题。

Method: 提出多前缀记忆框架，通过外部对抗搜索识别能引发特定序列的不同前缀数量来定义记忆化，从而量化记忆的多样性和鲁棒性。

Result: 在开源和对齐的聊天模型上的实验表明，多前缀定义能够可靠地区分记忆化和非记忆化数据，为审计LLMs中的数据泄露提供了实用工具。

Conclusion: 多前缀记忆框架能够有效捕捉大型语言模型中的记忆化现象，并提供了一种稳健的实用工具来审计数据泄露。

Abstract: Large language models, trained on massive corpora, are prone to verbatim memorization of training data, creating significant privacy and copyright risks. While previous works have proposed various definitions for memorization, many exhibit shortcomings in comprehensively capturing this phenomenon, especially in aligned models. To address this, we introduce a novel framework: multi-prefix memorization. Our core insight is that memorized sequences are deeply encoded and thus retrievable via a significantly larger number of distinct prefixes than non-memorized content. We formalize this by defining a sequence as memorized if an external adversarial search can identify a target count of distinct prefixes that elicit it. This framework shifts the focus from single-path extraction to quantifying the robustness of a memory, measured by the diversity of its retrieval paths. Through experiments on open-source and aligned chat models, we demonstrate that our multi-prefix definition reliably distinguishes memorized from non-memorized data, providing a robust and practical tool for auditing data leakage in LLMs.

</details>


### [11] [Structured Prompting Enables More Robust, Holistic Evaluation of Language Models](https://arxiv.org/abs/2511.20836)
*Asad Aali,Muhammad Ahmed Mohsin,Vasiliki Bikia,Arnav Singhvi,Richard Gaus,Suhana Bedi,Hejie Cui,Miguel Fuentes,Alyssa Unell,Yifan Mai,Jordan Cahoon,Michael Pfeffer,Roxana Daneshjou,Sanmi Koyejo,Emily Alsentzer,Percy Liang,Christopher Potts,Nigam H. Shah,Akshay S. Chaudhari*

Main category: cs.CL

TL;DR: 本文提出了一种结合DSPy和HELM的可复现框架，通过结构化提示方法提高语言模型（LM）性能评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前HELM等基准测试框架使用固定提示，无法通用化到所有LM，导致性能估计不准确。需要更准确的LM性能估计方法。

Method: 引入DSPy+HELM框架，通过四种结构化提示方法评估四个前沿LM在七个基准任务上的表现，并与现有HELM基准分数进行对比。

Result: 未使用结构化提示时，HELM低估LM性能，性能估计波动较大，性能差距被错误表示，引入推理（思维链）后减少了LM对提示设计的敏感性。

Conclusion: 可扩展的性能上限估计方法能够提供更实用的决策基准。本文开源了DSPy+HELM集成和提示优化流程。

Abstract: As language models (LMs) are increasingly adopted across domains, high-quality benchmarking frameworks that accurately estimate performance are essential for guiding deployment decisions. While frameworks such as Holistic Evaluation of Language Models (HELM) enable broad evaluation across tasks, they often rely on fixed prompts that fail to generalize across LMs, yielding unrepresentative performance estimates. Unless we estimate each LM's ceiling (maximum achievable via changes to the prompt), we risk underestimating performance. Declarative prompting frameworks, such as DSPy, offer a scalable alternative to manual prompt engineering by crafting structured prompts that can be optimized per task. However, such frameworks have not been systematically evaluated across established benchmarks. We present a reproducible DSPy+HELM framework that introduces structured prompting methods which elicit reasoning, enabling more accurate LM benchmarking. Using four prompting methods, we evaluate four frontier LMs across seven benchmarks (general/medical domain) against existing HELM baseline scores. We find that without structured prompting: (i) HELM underestimates LM performance (by 4% average), (ii) performance estimates vary more across benchmarks (+2% standard deviation), (iii) performance gaps are misrepresented (leaderboard rankings flip on 3/7 benchmarks), and (iv) introducing reasoning (chain-of-thought) reduces LM sensitivity to prompt design (smaller Δ across prompts). To our knowledge, this is the first large-scale benchmarking study to empirically characterize LM behavior across benchmarks and prompting methods, showing that scalable performance ceiling estimation enables more decision-useful benchmarks. We open-source (i) DSPy+HELM Integration (https://github.com/stanford-crfm/helm/pull/3893) and (ii) Prompt Optimization Pipeline (https://github.com/StanfordMIMI/dspy-helm).

</details>


### [12] [Length-MAX Tokenizer for Language Models](https://arxiv.org/abs/2511.20849)
*Dong Dong,Weijie Su*

Main category: cs.CL

TL;DR: 介绍了一种新的Length-MAX tokenizer，通过优化平均每个字符的token数量，提高了语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 减少语言模型在训练和推理过程中所需的token数量，以提高效率。

Method: 将长度加权目标最大化问题转化为图分割问题，并开发了一种贪心近似算法以获得词汇表。

Result: 在不同词汇表大小下，Length-MAX tokenizer比BPE减少14-18%的token数量，训练步数减少17.2-18.5%，推理延迟降低12.7-13.7%，下游任务性能提升。

Conclusion: 优化平均token长度而非仅频率，是一种有效的提高语言模型效率的方法，且不会牺牲下游性能。

Abstract: We introduce a new tokenizer for language models that minimizes the average tokens per character, thereby reducing the number of tokens needed to represent text during training and to generate text during inference. Our method, which we refer to as the Length-MAX tokenizer, obtains its vocabulary by casting a length-weighted objective maximization as a graph partitioning problem and developing a greedy approximation algorithm. On FineWeb and diverse domains, it yields 14--18\% fewer tokens than Byte Pair Encoding (BPE) across vocabulary sizes from 10K to 50K, and the reduction is 13.0\% when the size is 64K. Training GPT-2 models at 124M, 355M, and 1.3B parameters from scratch with five runs each shows 18.5\%, 17.2\%, and 18.5\% fewer steps, respectively, to reach a fixed validation loss, and 13.7\%, 12.7\%, and 13.7\% lower inference latency, together with a 16\% throughput gain at 124M, while consistently improving on downstream tasks including reducing LAMBADA perplexity by 11.7\% and enhancing HellaSwag accuracy by 4.3\%. Moreover, the Length-MAX tokenizer achieves 99.62\% vocabulary coverage and the out-of-vocabulary rate remains low at 0.12\% on test sets. These results demonstrate that optimizing for average token length, rather than frequency alone, offers an effective approach to more efficient language modeling without sacrificing -- and often improving -- downstream performance. The tokenizer is compatible with production systems and reduces embedding and KV-cache memory by 18\% at inference.

</details>


### [13] [Winning with Less for Low Resource Languages: Advantage of Cross-Lingual English_Persian Argument Mining Model over LLM Augmentation](https://arxiv.org/abs/2511.20872)
*Ali Jahan,Masood Ghayoomi,Annette Hautli-Janisz*

Main category: cs.CL

TL;DR: 本文探讨了利用跨语言方法进行低资源语言论证挖掘的有效性，通过三种训练场景比较了英语和波斯语的表现。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘在知识提取等任务中揭示文本逻辑结构，然而低资源语言缺乏足够的训练数据，需要跨语言方法来解决这一问题。

Method: 研究采用了三种训练场景：(i) 零样本迁移，(ii) 基于大型语言模型的英语数据增强，(iii) 结合英语和波斯语数据的跨语言模型。

Result: 零样本迁移模型在英语和波斯语测试集上的F1得分分别为50.2%和50.7%，LLM增强模型在英语和波斯语上分别达到59.2%和69.3%，而跨语言模型在波斯语测试集上达到74.8%。

Conclusion: 轻量级跨语言模型在低资源语言的论证挖掘任务中，性能优于资源密集型的数据增强方法，为克服数据资源不足提供了实用路径。

Abstract: Argument mining is a subfield of natural language processing to identify and extract the argument components, like premises and conclusions, within a text and to recognize the relations between them. It reveals the logical structure of texts to be used in tasks like knowledge extraction. This paper aims at utilizing a cross-lingual approach to argument mining for low-resource languages, by constructing three training scenarios. We examine the models on English, as a high-resource language, and Persian, as a low-resource language. To this end, we evaluate the models based on the English Microtext corpus \citep{PeldszusStede2015}, and its parallel Persian translation. The learning scenarios are as follow: (i) zero-shot transfer, where the model is trained solely with the English data, (ii) English-only training enhanced by synthetic examples generated by Large Language Models (LLMs), and (iii) a cross-lingual model that combines the original English data with manually translated Persian sentences. The zero-shot transfer model attains F1 scores of 50.2\% on the English test set and 50.7\% on the Persian test set. LLM-based augmentation model improves the performance up to 59.2\% on English and 69.3\% on Persian. The cross-lingual model, trained on both languages but evaluated solely on the Persian test set, surpasses the LLM-based variant, by achieving a F1 of 74.8\%. Results indicate that a lightweight cross-lingual blend can outperform considerably the more resource-intensive augmentation pipelines, and it offers a practical pathway for the argument mining task to overcome data resource shortage on low-resource languages.

</details>


### [14] [Emergence and Localisation of Semantic Role Circuits in LLMs](https://arxiv.org/abs/2511.20910)
*Nura Aljaafari,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 本文提出了一种结合角色交叉最小对、时间涌现分析和跨模型比较的方法，用于研究大语言模型如何实现语义角色，发现模型形成了紧凑且因果隔离的机制来处理抽象语义结构，并且这些机制在不同规模和架构之间部分转移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型尽管展示了语义能力，但其内部如何具体实现抽象语义结构的机制尚未充分明确。

Method: 结合角色交叉最小对、时间涌现分析和跨模型比较的方法。

Result: 发现(i) 高度集中的电路（89-94%的归因集中在28个节点内）；(ii) 结构逐渐精细化而非相变，较大的模型有时绕过局部电路；(iii) 中等跨规模保守性（24-59%的组件重叠）以及高光谱相似性。

Conclusion: 大语言模型形成了紧凑且因果隔离的机制来处理抽象语义结构，并且这些机制在不同规模和架构之间部分转移。

Abstract: Despite displaying semantic competence, large language models' internal mechanisms that ground abstract semantic structure remain insufficiently characterised. We propose a method integrating role-cross minimal pairs, temporal emergence analysis, and cross-model comparison to study how LLMs implement semantic roles. Our analysis uncovers: (i) highly concentrated circuits (89-94% attribution within 28 nodes); (ii) gradual structural refinement rather than phase transitions, with larger models sometimes bypassing localised circuits; and (iii) moderate cross-scale conservation (24-59% component overlap) alongside high spectral similarity. These findings suggest that LLMs form compact, causally isolated mechanisms for abstract semantic structure, and these mechanisms exhibit partial transfer across scales and architectures.

</details>


### [15] [Chatty-KG: A Multi-Agent AI System for On-Demand Conversational Question Answering over Knowledge Graphs](https://arxiv.org/abs/2511.20940)
*Reham Omar,Abdelghny Orogat,Ibrahim Abdelaziz,Omij Mangukiya,Panos Kalnis,Essam Mansour*

Main category: cs.CL

TL;DR: 提出Chatty-KG，一种用于知识图谱对话问答的多智能体系统，结合RAG与结构化SPARQL查询生成，提升多轮对话准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统KGQA系统难以处理多轮对话且延迟高，而RAG系统序列化图谱结构且上下文处理能力弱，LLMs又缺乏直接访问动态KG的能力，需融合对话灵活性与KG结构化优势。

Method: 设计模块化多智能体系统，由任务专用LLM智能体协作完成上下文理解、对话追踪、实体关系链接和SPARQL查询规划，实现自然语言到可执行查询的高效转化。

Result: 在大型多样化KG上，Chatty-KG在单轮和多轮设置中显著优于现有方法，F1和P@1分数更高，兼容多种商业及开源LLM，性能稳健。

Conclusion: Chatty-KG成功融合对话交互性与KG结构化知识，无需微调或预处理即可支持动态KG，为可靠多轮KGQA提供可扩展、易扩展的解决方案。

Abstract: Conversational Question Answering over Knowledge Graphs (KGs) combines the factual grounding of KG-based QA with the interactive nature of dialogue systems. KGs are widely used in enterprise and domain applications to provide structured, evolving, and reliable knowledge. Large language models (LLMs) enable natural and context-aware conversations, but lack direct access to private and dynamic KGs. Retrieval-augmented generation (RAG) systems can retrieve graph content but often serialize structure, struggle with multi-turn context, and require heavy indexing. Traditional KGQA systems preserve structure but typically support only single-turn QA, incur high latency, and struggle with coreference and context tracking. To address these limitations, we propose Chatty-KG, a modular multi-agent system for conversational QA over KGs. Chatty-KG combines RAG-style retrieval with structured execution by generating SPARQL queries through task-specialized LLM agents. These agents collaborate for contextual interpretation, dialogue tracking, entity and relation linking, and efficient query planning, enabling accurate and low-latency translation of natural questions into executable queries. Experiments on large and diverse KGs show that Chatty-KG significantly outperforms state-of-the-art baselines in both single-turn and multi-turn settings, achieving higher F1 and P@1 scores. Its modular design preserves dialogue coherence and supports evolving KGs without fine-tuning or pre-processing. Evaluations with commercial (e.g., GPT-4o, Gemini-2.0) and open-weight (e.g., Phi-4, Gemma 3) LLMs confirm broad compatibility and stable performance. Overall, Chatty-KG unifies conversational flexibility with structured KG grounding, offering a scalable and extensible approach for reliable multi-turn KGQA.

</details>


### [16] [TrackList: Tracing Back Query Linguistic Diversity for Head and Tail Knowledge in Open Large Language Models](https://arxiv.org/abs/2511.21006)
*Ioana Buhnila,Aman Sinha,Mathieu Constant*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在不同类型问题上的表现，发现其在定义类问题上表现最好，在举例类问题上表现最差。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在定义类问题上表现良好，但在其他类型问题（如举例和转述）上表现较差，因此需要分析预训练数据对LLMs回答的影响。

Method: 使用TrackList进行细粒度语言学和统计分析，并引入RefoMed-EN数据集（包含6170个医学术语及其定义、命名、示例、解释或转述），研究概念频率对模型性能的影响，并评估LLMs输出的句法和语义相似性。

Result: LLMs在定义类问题上表现最好，在举例类问题上表现最差；模型倾向于对流行和常见知识进行转述，较少关注尾部和技术性知识。

Conclusion: LLMs在不同类型问题上的表现差异明显，尤其在专家文本中，对尾部和技术性知识的处理需要改进。

Abstract: Large Language Models (LLMs) have proven efficient in giving definition-type answers to user input queries. While for humans giving various types of answers, such as examples and paraphrases, is an easy task, LLMs struggle to provide correct answers for other than definition-type queries. In this study, we evaluated this drop in performance using TrackList, a fine-grained linguistic and statistical analysis pipeline to investigate the impact of the pre-training data on LLMs answers to diverse linguistic queries. We also introduce RefoMed-EN, an English dataset consisting of 6170 human-annotated medical terms alongside their corresponding definitions, denominations, exemplifications, explanations, or paraphrases. We studied whether the high frequency of a concept (head) or low frequency (tail) impacts the language model's performance. We evaluated the quality of the LLM's output using syntactic and semantic similarity metrics, statistical correlations and embeddings. Results showed that the LLM's task performance for definition type questions is the highest, while for the exemplification type it is the lowest. Additionally, we showed that for definition-type questions, large language models are prone to paraphrase more on popular and frequent knowledge and less on tail and technical knowledge, especially in the expert texts.

</details>


### [17] [Semantic Anchors in In-Context Learning: Why Small LLMs Cannot Flip Their Labels](https://arxiv.org/abs/2511.21038)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.CL

TL;DR: 本文研究了上下文学习（ICL）是否能覆盖预训练标签语义，发现ICL主要调整输入在稳定语义方向上的投影，而不能灵活重映射标签含义。


<details>
  <summary>Details</summary>
Motivation: 探究ICL是否能覆盖预训练的标签语义，或只是细化已有语义结构，从而理解ICL的机制和限制。

Method: 通过将大规模语言模型（LLMs）视作由提示引发的分类器，比较其在自然演示（正确标签）和反向演示（系统翻转标签含义）下的表现，并分解ICL行为为三种对齐指标（真实、先验和提示对齐）。

Result: 在自然演示下，ICL提高准确率同时保持强先验对齐；在反向演示下，模型无法学习反语义分类器，提示对齐增加仅以牺牲准确率为代价，语义覆盖率在小样本1-12B参数设置下保持为零。

Conclusion: ICL主要调整输入在预训练稳定语义方向上的投影，而不是灵活重映射标签含义，这揭示了小样本提示的根本限制，并表明覆盖标签语义需要超越ICL的干预措施。

Abstract: Can in-context learning (ICL) override pre-trained label semantics, or does it merely refine an existing semantic backbone? We address this question by treating LLMs as prompt-induced classifiers and contrasting their behavior under \emph{natural} demonstrations (with correct labels) and \emph{inverted} demonstrations (systematically flipping label meanings). We decompose ICL behavior into three alignment metrics (truth, prior, and prompt alignment) and introduce a semantic override rate, defined as correctness under flipped semantics. Across eight classification tasks and eight open-source LLMs (1--12B parameters), we find consistent evidence for a semantic anchor view. With natural demonstrations, ICL improves accuracy while maintaining strong prior alignment; most correct predictions coincide with zero-shot behavior, even when the prior is weak. With inverted demonstrations, models cannot learn coherent anti-semantic classifiers: prompt alignment increases only by sacrificing accuracy, and semantic override rates remain exactly zero in our few-shot 1--12B setting. Rather than flexibly remapping label meanings, ICL primarily adjusts how inputs project onto stable semantic directions learned during pre-training, clarifying fundamental limits of few-shot prompting and suggesting that overriding label semantics at these scales requires interventions beyond ICL. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/semantic-anchors-icl.

</details>


### [18] [Orthographic Constraint Satisfaction and Human Difficulty Alignment in Large Language Models](https://arxiv.org/abs/2511.21086)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.CL

TL;DR: 本文评估了28种大型语言模型在满足字符级约束的58个单词谜题上的表现，发现架构差异比参数扩展对性能的影响更大，并揭示了模型在常见但拼写特殊单词上的系统性失败。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在受控文本生成中需满足严格的拼写约束，但不同架构间的系统性评估仍有限，且模型在处理特殊拼写时存在明显不足。

Method: 评估了来自三个模型家族（Qwen3, Claude Haiku-4.5, GPT-5-mini）的28种配置，使用58个需要字符级约束满足的单词谜题，并引入10,000名人类解题者的难度评分。

Result: 架构差异导致性能差距显著（2.0-2.2倍），参数扩展仅带来83%增益；高容量模型思维预算回报高，中等模型则饱和或下降；模型在常见但拼写特殊单词上的错误率高（89-96%）。

Conclusion: 约束满足需要超越标准语言模型扩展的专门架构特征或训练目标，尤其在处理非常规拼写时，模型过度依赖分布合理性，需架构创新。

Abstract: Large language models must satisfy hard orthographic constraints during controlled text generation, yet systematic cross-architecture evaluation remains limited. We evaluate 28 configurations spanning three model families (Qwen3, Claude Haiku-4.5, GPT-5-mini) on 58 word puzzles requiring character-level constraint satisfaction. Architectural differences produce substantially larger performance gaps (2.0-2.2x, F1=0.761 vs. 0.343) than parameter scaling within families (83% gain from eightfold scaling), suggesting that constraint satisfaction may require specialized architectural features or training objectives beyond standard language model scaling. Thinking budget sensitivity proves heterogeneous: high-capacity models show strong returns (+0.102 to +0.136 F1), while mid-sized variants saturate or degrade. These patterns are inconsistent with uniform compute benefits. Using difficulty ratings from 10,000 human solvers per puzzle, we establish modest but consistent calibration (r=0.24-0.38) across all families, yet identify systematic failures on common words with unusual orthography ("data", "poop", "loll": 86-95% human success, 89-96% model miss rate). These failures reveal over-reliance on distributional plausibility that penalizes orthographically atypical but constraint-valid patterns, suggesting architectural innovations may be required beyond simply scaling parameters or computational budgets.

</details>


### [19] [ASR Error Correction in Low-Resource Burmese with Alignment-Enhanced Transformers using Phonetic Features](https://arxiv.org/abs/2511.21088)
*Ye Bhone Lin,Thura Aung,Ye Kyaw Thu,Thazin Myint Oo*

Main category: cs.CL

TL;DR: 该论文研究了用于低资源缅甸语ASR错误纠正的序列到序列Transformer模型，重点关注不同特征整合策略。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决缅甸语ASR错误纠正的问题，这在之前的研究中尚未涉及。

Method: 评估了五种ASR主干模型，并提出了一种结合国际音标（IPA）和对齐信息的AEC方法。

Result: 提出的AEC模型在字和字符级别上均优于基线，WER从51.56降到39.82，chrF++得分从0.5864提升到0.627。

Conclusion: AEC方法在低资源环境下对ASR输出的改进具有鲁棒性，特征设计在提升ASR性能方面至关重要。

Abstract: This paper investigates sequence-to-sequence Transformer models for automatic speech recognition (ASR) error correction in low-resource Burmese, focusing on different feature integration strategies including IPA and alignment information. To our knowledge, this is the first study addressing ASR error correction specifically for Burmese. We evaluate five ASR backbones and show that our ASR Error Correction (AEC) approaches consistently improve word- and character-level accuracy over baseline outputs. The proposed AEC model, combining IPA and alignment features, reduced the average WER of ASR models from 51.56 to 39.82 before augmentation (and 51.56 to 43.59 after augmentation) and improving chrF++ scores from 0.5864 to 0.627, demonstrating consistent gains over the baseline ASR outputs without AEC. Our results highlight the robustness of AEC and the importance of feature design for improving ASR outputs in low-resource settings.

</details>


### [20] [MortgageLLM: Domain-Adaptive Pretraining with Residual Instruction Transfer, Alignment Tuning, and Task-Specific Routing](https://arxiv.org/abs/2511.21101)
*Manish Jain,Satheesh Kumar Ponnambalam,Salman Faroz,Chandrakanth Lns,Vinay Sharma*

Main category: cs.CL

TL;DR: MortgageLLM是一个为抵押金融领域设计的新型领域特定大型语言模型，通过双轨专业化和指令残差技术解决了领域特定知识增强与保持指令遵循能力的双重挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用领域表现出色，但在抵押金融等专业领域需要特定的知识增强，同时保持指令遵循能力。单一多任务模型存在性能权衡问题，因此需要新的方法。

Method: 采用双轨专业化框架，从一个基础模型（LLaMA-3.1-8B）创建两个专家模型：一个用于对话Q&A，一个用于结构化任务（分类和摘要），并使用指令残差技术恢复指令遵循能力。还设计了一个智能任务路由机制，使用少样本分类。

Result: 在领域特定基准测试中，MortgageLLM（MLM v2）显著优于基础模型LLaMA-3.1-8B-Instruct，在摘要、Q&A和分类任务上分别获得了4.58、4.09和2.6的分数，BERTScore也明显更高。

Conclusion: 双轨专业化和指令残差技术有效解决了领域特定知识增强与指令遵循能力保持的矛盾，MortgageLLM在抵押金融领域表现出色，为领域特定语言模型开发提供了新的思路。

Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.

</details>


### [21] [Self-Guided Defense: Adaptive Safety Alignment for Reasoning Models via Synthesized Guidelines](https://arxiv.org/abs/2511.21214)
*Yuhang Wang,Yanxu Zhu,Dongyuan Lu,Jitao Sang*

Main category: cs.CL

TL;DR: 本文提出了一种新的框架SGASA，通过内部化模型生成的安全指南，增强推理模型在面对对抗性越狱提示时的安全性。


<details>
  <summary>Details</summary>
Motivation: 对抗性越狱提示具有隐蔽性和欺骗性，可以绕过模型的安全机制，导致有害内容的生成，因此需要一种自适应的安全对齐方法。

Method: SGASA框架包括两个关键阶段：数据预合成（生成安全指南和增强提示）和对齐微调（利用监督微调和直接偏好优化将指南嵌入模型）。

Result: 在多个数据集上的广泛实验表明，SGASA显著提高了模型的安全性，验证了其自适应和可扩展的有效性。

Conclusion: SGASA框架通过合成指南和自适应微调，有效提升了模型在面对对抗性输入时的鲁棒性，同时减少了对良性请求的不必要拒绝。

Abstract: Reasoning models have demonstrated remarkable capabilities in complex reasoning tasks. However, ensuring their safety against adversarial jailbreak prompts remains a critical challenge. Due to the covert and deceptive nature of such prompts, they can often evade built-in safety mechanisms and lead to the generation of harmful content. This underscores the need for an adaptive safety alignment approach that enables models to autonomously reinforce their defenses in response to adversarial inputs. This paper introduces the Synthesized Guideline-based Adaptive Safety Alignment (SGASA) framework, which internalizes model-generated safety guidelines to strengthen models' ability to enhance robustness against harmful adversarial prompts while minimizing unnecessary refusals of benign requests. SGASA consists of two key stages: Data Pre-synthesis, which generates safety guidelines and augmented prompts; and Alignment Fine-tuning, which leverages Supervised Fine-tuning (SFT) and Direct Preference Optimization (DPO) to embed these guidelines into the model. Extensive experiments across multiple datasets demonstrate that SGASA significantly improves model safety, validating its adaptive and scalable effectiveness.

</details>


### [22] [Can Finetuing LLMs on Small Human Samples Increase Heterogeneity, Alignment, and Belief-Action Coherence?](https://arxiv.org/abs/2511.21218)
*Steven Wang,Kyle Hunt,Shaojie Tang,Kenneth Joseph*

Main category: cs.CL

TL;DR: 该论文探讨了通过在小部分人类调查数据上微调大型语言模型（LLMs）是否能改善其模拟人类行为的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在模拟人类行为方面具有潜力，但已有研究表明其与人类行为存在系统性偏差。该研究希望探索通过微调是否能缓解这些问题。

Method: 研究采用了一项关于信息披露的行为实验，从分布差异、子群对齐、信念-行为一致性以及回归系数的恢复等多个维度比较了人类和LLM生成的响应。

Result: 在小样本人类数据上微调显著改善了模型的异质性、对齐性和信念-行为一致性，但仍然未能复制原始研究的回归系数。

Conclusion: 尽管微调能部分改善LLM模拟效果，但LLM生成的数据仍不适合用于替代人类参与者进行正式的推论分析。

Abstract: There is ongoing debate about whether large language models (LLMs) can serve as substitutes for human participants in survey and experimental research. While recent work in fields such as marketing and psychology has explored the potential of LLM-based simulation, a growing body of evidence cautions against this practice: LLMs often fail to align with real human behavior, exhibiting limited diversity, systematic misalignment for minority subgroups, insufficient within-group variance, and discrepancies between stated beliefs and actions. This study examines an important and distinct question in this domain: whether fine-tuning on a small subset of human survey data, such as that obtainable from a pilot study, can mitigate these issues and yield realistic simulated outcomes. Using a behavioral experiment on information disclosure, we compare human and LLM-generated responses across multiple dimensions, including distributional divergence, subgroup alignment, belief-action coherence, and the recovery of regression coefficients. We find that fine-tuning on small human samples substantially improves heterogeneity, alignment, and belief-action coherence relative to the base model. However, even the best-performing fine-tuned models fail to reproduce the regression coefficients of the original study, suggesting that LLM-generated data remain unsuitable for replacing human participants in formal inferential analyses.

</details>


### [23] [Developing an Open Conversational Speech Corpus for the Isan Language](https://arxiv.org/abs/2511.21229)
*Adisai Na-Thalang,Chanakan Wittayasakpan,Kritsadha Phatcharoen,Supakit Buakaw*

Main category: cs.CL

TL;DR: 本文介绍了首个针对泰国最广泛使用地区方言——伊桑语（Isan）开放对话语音数据集的开发。


<details>
  <summary>Details</summary>
Motivation: 现有语音库主要是基于阅读或脚本语音，缺乏自然对话的真实性。此外，伊桑语缺乏标准正字法，导致转录规范设计困难。

Method: 通过建立实用的转录协议，平衡表征准确性和计算处理需求，开发了一个包含自然对话语音的数据集。

Result: 成功构建并发布了一个开放的伊桑语语音数据集，该数据集捕捉了真实的语言现象，如口语、自发韵律、不流畅性和频繁的泰语-伊桑语代码转换。

Conclusion: 发布该数据集有助于包容性人工智能发展，支持对代表性不足语言的研究，并为解决对话语音建模中的语言和技术挑战提供了基础。

Abstract: This paper introduces the development of the first open conversational speech dataset for the Isan language, the most widely spoken regional dialect in Thailand. Unlike existing speech corpora that are primarily based on read or scripted speech, this dataset consists of natural speech, thereby capturing authentic linguistic phenomena such as colloquials, spontaneous prosody, disfluencies, and frequent code-switching with central Thai. A key challenge in building this resource lies in the lack of a standardized orthography for Isan. Current writing practices vary considerably, due to the different lexical tones between Thai and Isan. This variability complicates the design of transcription guidelines and poses questions regarding consistency, usability, and linguistic authenticity. To address these issues, we establish practical transcription protocols that balance the need for representational accuracy with the requirements of computational processing. By releasing this dataset as an open resource, we aim to contribute to inclusive AI development, support research on underrepresented languages, and provide a basis for addressing the linguistic and technical challenges inherent in modeling conversational speech.

</details>


### [24] [PEFT-Bench: A Parameter-Efficient Fine-Tuning Methods Benchmark](https://arxiv.org/abs/2511.21285)
*Robert Belanec,Branislav Pecher,Ivan Srba,Maria Bielikova*

Main category: cs.CL

TL;DR: 本文介绍了一种名为PEFT-Bench的统一端到端基准，用于评估自回归大型语言模型上的不同参数高效微调（PEFT）方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在很多任务上表现出色，但其庞大的规模导致了高昂的计算和环境成本，限制了其可访问性。PEFT方法通过减少可训练参数的数量来应对这一挑战，同时保持强大的下游性能。然而，当前对PEFT方法的评估仍存在局限性和可复现性问题。

Method: 引入了PEFT-Bench，一个统一的端到端基准，用于评估自回归大型语言模型上的不同PEFT方法，并在27个NLP数据集和6种PEFT方法上展示了其用途。同时，引入了PEFT Soft Score Penalties (PSCP) 指标，综合考虑了可训练参数、推理速度和训练内存使用情况。

Result: PEFT-Bench能够更全面和可靠地评估不同的PEFT方法，解决了现有评估方法的局限性和可复现性问题。

Conclusion: PEFT-Bench为评估PEFT方法提供了一个全面、可复现的框架，有助于推动该领域的发展。通过引入PSCP指标，可以更全面地考虑不同因素对PEFT方法性能的影响。

Abstract: Despite the state-of-the-art performance of Large Language Models (LLMs) achieved on many tasks, their massive scale often leads to high computational and environmental costs, limiting their accessibility. Parameter-efficient fine-tuning (PEFT) methods address this challenge by reducing the number of trainable parameters while maintaining strong downstream performance. Despite the increased development in PEFT methods, current evaluations remain limited (in terms of evaluated models and datasets) and difficult to reproduce. To bridge this gap, we introduce PEFT-Bench, a unified end-to-end benchmark for evaluating diverse PEFT methods on autoregressive LLMs. We demonstrate its usage across 27 NLP datasets and 6 PEFT methods. To account for different PEFT training and inference factors, we also introduce the PEFT Soft Score Penalties (PSCP) metric, which takes trainable parameters, inference speed, and training memory usage into account.

</details>


### [25] [Emergent Lexical Semantics in Neural Language Models: Testing Martin's Law on LLM-Generated Text](https://arxiv.org/abs/2511.21334)
*Kai Kugler*

Main category: cs.CL

TL;DR: 首次系统研究神经语言模型训练过程中Martin's Law（词频与多义性关系）的演变，发现其呈现非单调发展轨迹。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型生成文本中语言规律的动态演变，特别是词频与多义性的关系。

Method: 使用DBSCAN聚类上下文化嵌入作为词义操作化，分析四个Pythia模型（70M-1B参数）在30个训练检查点的表现。

Result: Martin's Law在检查点100左右出现，104达到峰值相关性（r>0.6），105后退化；小模型后期出现灾难性语义崩溃，大模型表现优雅退化。

Conclusion: LLM生成文本的语言规律遵循平衡轨迹，存在最佳语义窗口，而非随训练单调提升；建立了评估神经语言模型新兴语言结构的新方法。

Abstract: We present the first systematic investigation of Martin's Law - the empirical relationship between word frequency and polysemy - in text generated by neural language models during training. Using DBSCAN clustering of contextualized embeddings as an operationalization of word senses, we analyze four Pythia models (70M-1B parameters) across 30 training checkpoints. Our results reveal a non-monotonic developmental trajectory: Martin's Law emerges around checkpoint 100, reaches peak correlation (r > 0.6) at checkpoint 104, then degrades by checkpoint 105. Smaller models (70M, 160M) experience catastrophic semantic collapse at late checkpoints, while larger models (410M, 1B) show graceful degradation. The frequency-specificity trade-off remains stable (r $\approx$ -0.3) across all models. These findings suggest that compliance with linguistic regularities in LLM-generated text is not monotonically increasing with training, but instead follows a balanced trajectory with an optimal semantic window. This work establishes a novel methodology for evaluating emergent linguistic structure in neural language models.

</details>


### [26] [Training Introspective Behavior: Fine-Tuning Induces Reliable Internal State Detection in a 7B Model](https://arxiv.org/abs/2511.21399)
*Joshua Fonseca Rivera*

Main category: cs.CL

TL;DR: Lindsey (2025) 探究了语言模型的自我意识，通过四个实验发现模型有时能检测识别注入的激活模式，但不可靠。本文聚焦于第一个实验，研究是否可以通过直接训练而非等待自发产生来实现该能力。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否可以通过直接训练来可靠地检测并报告注入的“思想”，从而提高模型的内省能力，解决模型在自我报告上的不可靠性问题。

Method: 通过对瞬时单标记注入进行微调，将7B参数模型从几乎完全失败（0.4% 准确率，6.7% 假阳性率）转变为可靠检测（在α=40的保留概念上准确率85%，0%假阳性）。

Result: 模型能检测在单标记位置注入的瞬时“思想”，并在后续生成步骤中报告其语义内容，满足Lindsey的三个标准：准确性、基础性和内部性。未见概念向量的泛化能力（7.5个百分点差距）表明模型学到的是可迁移技能而非记忆特定向量。

Conclusion: 至少有一个内省行为成分可以通过直接训练诱导，这为构建内在AI透明性提供了路径，解决了Lindsey提出的关于训练内省是否能消除模型间差异的开放问题。

Abstract: Lindsey (2025) investigates introspective awareness in language models through four experiments, finding that models can sometimes detect and identify injected activation patterns -- but unreliably (~20% success in the best model). We focus on the first of these experiments -- self-report of injected "thoughts" -- and ask whether this capability can be directly trained rather than waiting for emergence. Through fine-tuning on transient single-token injections, we transform a 7B parameter model from near-complete failure (0.4% accuracy, 6.7% false positive rate) to reliable detection (85% accuracy on held-out concepts at α=40, 0% false positives). Our model detects fleeting "thoughts" injected at a single token position, retains that information, and reports the semantic content across subsequent generation steps. On this task, our trained model satisfies three of Lindsey's criteria: accuracy (correct identification), grounding (0/60 false positives), and internality (detection precedes verbalization). Generalization to unseen concept vectors (7.5pp gap) demonstrates the model learns a transferable skill rather than memorizing specific vectors, though this does not establish metacognitive representation in Lindsey's sense. These results address an open question raised by Lindsey: whether "training for introspection would help eliminate cross-model differences." We show that at least one component of introspective behavior can be directly induced, offering a pathway to built-in AI transparency.

</details>


### [27] [Can LLMs extract human-like fine-grained evidence for evidence-based fact-checking?](https://arxiv.org/abs/2511.21401)
*Antonín Jarolím,Martin Fajčík,Lucia Makaiová*

Main category: cs.CL

TL;DR: 本文关注于针对捷克语和斯洛伐克语评论中细粒度证据提取任务，评估了大型语言模型在人类标注数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 在线新闻用户评论中频繁传播错误信息，需要有效的方法来检测并提取出事实不准确的信息，因此需要识别相关文档并定位确凿证据。

Method: 创建了一个新的数据集，包含由付费标注者进行双向标注的细粒度证据，并在该数据集上评估了大型语言模型的表现。

Result: Llama3.1:8b 模型尽管参数较少，但正确输出比例高，而 gpt-oss-120b 模型表现不佳；Qwen3:14b, deepseek-r1:32b, 和 gpt-oss:20b 模型在模型大小和与人类标注的一致性方面表现良好。

Conclusion: 大型语言模型在细粒度证据提取任务中表现各异，模型大小与性能并不总是成正比，部分模型在性能和大小之间达到了良好的平衡。

Abstract: Misinformation frequently spreads in user comments under online news articles, highlighting the need for effective methods to detect factually incorrect information. To strongly support or refute claims extracted from such comments, it is necessary to identify relevant documents and pinpoint the exact text spans that justify or contradict each claim. This paper focuses on the latter task -- fine-grained evidence extraction for Czech and Slovak claims. We create new dataset, containing two-way annotated fine-grained evidence created by paid annotators. We evaluate large language models (LLMs) on this dataset to assess their alignment with human annotations. The results reveal that LLMs often fail to copy evidence verbatim from the source text, leading to invalid outputs. Error-rate analysis shows that the {llama3.1:8b model achieves a high proportion of correct outputs despite its relatively small size, while the gpt-oss-120b model underperforms despite having many more parameters. Furthermore, the models qwen3:14b, deepseek-r1:32b, and gpt-oss:20b demonstrate an effective balance between model size and alignment with human annotations.

</details>


### [28] [Text-to-SQL as Dual-State Reasoning: Integrating Adaptive Context and Progressive Generation](https://arxiv.org/abs/2511.21402)
*Zhifeng Hao,Qibin Song,Ruichu Cai,Boyan Xu*

Main category: cs.CL

TL;DR: DSR-SQL是一种新颖的双状态推理框架，旨在提升大型语言模型在复杂企业数据库上的Text-to-SQL能力。


<details>
  <summary>Details</summary>
Motivation: 传统的基于Chain-of-Thought的推理方法在处理复杂企业数据库时，面临上下文容量有限、模式链接不可靠和数据库语义基础薄弱等问题。

Method: DSR-SQL引入了双状态推理框架，包括自适应上下文状态和渐进生成状态，通过优化大型模式和选择相关结构来构建紧凑、语义上可信的环境，并将SQL综合形式化为有反馈引导的状态转移。

Result: 在没有进行任何后期训练或上下文示例的情况下，DSR-SQL在Spider 2.0-Snow和BIRD开发集上分别达到了35.28%和68.32%的执行准确率。

Conclusion: DSR-SQL通过建模Text-to-SQL为两个状态之间的交互，有效提升了在复杂数据库环境下的性能，具有良好的应用前景。

Abstract: Recent divide-and-conquer reasoning approaches, particularly those based on Chain-of-Thought (CoT), have substantially improved the Text-to-SQL capabilities of Large Language Models (LLMs). However, when applied to complex enterprise databases, such methods struggle to maintain coherent reasoning due to limited context capacity, unreliable schema linking, and weak grounding in database semantics. To overcome these issues, we introduce DSR-SQL, a \textbf{D}ual-\textbf{S}tate \textbf{R}easoning framework that models Text-to-SQL as an interaction between an adaptive context state and a progressive generation state. The first constructs a compact, semantically faithful environment by refining large schemas and selecting relevant structures, while the second formalizes SQL synthesis as feedback-guided state transitions, enabling the model to self-correct and align with user intent. Without any post-training or in-context examples, DSR-SQL achieves competitive performance, reaching 35.28\% execution accuracy on Spider 2.0-Snow and 68.32\% on BIRD development set. Our implementation will be open-sourced at: https://github.com/DMIRLAB-Group/DSR-SQL.

</details>


### [29] [Odin: Oriented Dual-module Integration for Text-rich Network Representation Learning](https://arxiv.org/abs/2511.21416)
*Kaifeng Hong,Yinglong Zhang,Xiaoying Hong,Xuewen Xia,Xing Xu*

Main category: cs.CL

TL;DR: Odin是一种新颖的架构，通过定向双模块机制在特定深度向Transformer中注入图结构，解决了传统GNN和Transformer在文本属性图建模中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖GNNs（易过平滑且依赖跳数扩散）或Transformer（忽略图拓扑，将节点视为孤立序列），无法有效结合文本理解与结构推理。Odin旨在通过新架构实现结构-文本统一建模。

Method: Odin采用定向双模块机制，在特定Transformer层中集成多跳结构，实现低、中、高层结构抽象，并通过全局[CLS]表示避免过平滑。还提出轻量级变体Light Odin，保留层对齐结构抽象以提升效率。

Result: 在多个文本丰富图基准测试中，Odin实现了最先进的准确性，Light Odin在显著降低计算成本的同时保持了有竞争力的性能。

Conclusion: Odin和Light Odin构成了一种统一、无跳数扩散的框架，为结构-文本集成提供了新的解决方案，其表达能力严格包含纯Transformer和GNN。

Abstract: Text-attributed graphs require models to effectively combine strong textual understanding with structurally informed reasoning. Existing approaches either rely on GNNs--limited by over-smoothing and hop-dependent diffusion--or employ Transformers that overlook graph topology and treat nodes as isolated sequences. We propose Odin (Oriented Dual-module INtegration), a new architecture that injects graph structure into Transformers at selected depths through an oriented dual-module mechanism.Unlike message-passing GNNs, Odin does not rely on multi-hop diffusion; instead, multi-hop structures are integrated at specific Transformer layers, yielding low-, mid-, and high-level structural abstraction aligned with the model's semantic hierarchy. Because aggregation operates on the global [CLS] representation, Odin fundamentally avoids over-smoothing and decouples structural abstraction from neighborhood size or graph topology. We further establish that Odin's expressive power strictly contains that of both pure Transformers and GNNs.To make the design efficient in large-scale or low-resource settings, we introduce Light Odin, a lightweight variant that preserves the same layer-aligned structural abstraction for faster training and inference. Experiments on multiple text-rich graph benchmarks show that Odin achieves state-of-the-art accuracy, while Light Odin delivers competitive performance with significantly reduced computational cost. Together, Odin and Light Odin form a unified, hop-free framework for principled structure-text integration. The source code of this model has been released at https://github.com/hongkaifeng/Odin.

</details>


### [30] [A Systematic Study of Model Merging Techniques in Large Language Models](https://arxiv.org/abs/2511.21437)
*Oğuz Kağan Hitit,Leander Girrbach,Zeynep Akata*

Main category: cs.CL

TL;DR: 该论文系统评估了六种模型合并方法在大型语言模型（LLMs）上的表现，发现只有最简单的方法Task Arithmetic能稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 模型合并是一种无需额外训练即可组合多个微调模型的方法，但在LLMs上的效果尚未明确。

Method: 在四个开源LLMs、每个基础模型十二个微调检查点和十六个标准LLM基准上，系统评估六种最先进合并方法。

Result: Task Arithmetic是唯一可靠提升性能的方法，其他方法通常导致性能显著下降。

Conclusion: 当前合并技术不能直接迁移到现代LLMs，需设计LLM特定的合并算法和合并感知的微调方法。

Abstract: Model merging combines multiple fine-tuned checkpoints into a single model without additional training, offering an attractive approach to reusing models and efficiently improving performance. However, it remains unclear whether the advantages reported for smaller models and classifiers generalize to LLMs. We present a large-scale, systematic evaluation of six state-of-the-art merging methods, including recent subspace methods, across four open-weight LLMs, twelve fine-tuned checkpoints per base model, and sixteen standard LLM benchmarks. Evaluating through standardized benchmarks, we measure both the probability that a merged model outperforms the base model and relative gains over the best individual checkpoint. Our results show that the oldest and simplest method, Task Arithmetic, is the only approach that reliably yields performance gains on LLMs. Other interference-aware and subspace merging methods typically result in significant performance drops. Our findings indicate that current merging techniques do not directly transfer to modern LLMs. This motivates the design of LLM-specific merging algorithms and merging-aware fine-tuning methods. Code will be released upon acceptance of this paper.

</details>


### [31] [Hierarchical Ranking Neural Network for Long Document Readability Assessment](https://arxiv.org/abs/2511.21473)
*Yurui Zheng,Yijun Chen,Shaohong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种双向可读性评估机制，结合句子级标签和标签减法排序算法，以更好地评估文本阅读难度。


<details>
  <summary>Details</summary>
Motivation: 当前的可读性评估方法未能充分考虑文本长度或标签的序数关系。

Method: 提出了一种双向可读性评估机制，捕捉文本中的上下文信息，识别语义信息丰富的区域，并预测每个句子的可读性水平。句子级标签用于辅助预测文档的整体可读性水平。引入了一种成对排序算法，通过标签减法建模可读性水平之间的序数关系。

Result: 在中英文数据集上的实验结果表明，所提出的模型性能具有竞争力，优于其他基线模型。

Conclusion: 该模型通过考虑句子级可读性和标签序数关系，提高了可读性评估的准确性和性能。

Abstract: Readability assessment aims to evaluate the reading difficulty of a text. In recent years, while deep learning technology has been gradually applied to readability assessment, most approaches fail to consider either the length of the text or the ordinal relationship of readability labels. This paper proposes a bidirectional readability assessment mechanism that captures contextual information to identify regions with rich semantic information in the text, thereby predicting the readability level of individual sentences. These sentence-level labels are then used to assist in predicting the overall readability level of the document. Additionally, a pairwise sorting algorithm is introduced to model the ordinal relationship between readability levels through label subtraction. Experimental results on Chinese and English datasets demonstrate that the proposed model achieves competitive performance and outperforms other baseline models.

</details>


### [32] [Voice, Bias, and Coreference: An Interpretability Study of Gender in Speech Translation](https://arxiv.org/abs/2511.21517)
*Lina Conti,Dennis Fucci,Marco Gaido,Matteo Negri,Guillaume Wisniewski,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 该论文研究了语音翻译（ST）模型在处理性别分配时如何受到训练数据、内部语言模型偏见和声学信息的影响。


<details>
  <summary>Details</summary>
Motivation: 语音通过音高等声学线索传递说话人信息（如性别），这引发了模态特定偏见问题，尤其是在从没有性别概念的语言（如英语）翻译成有语法性别的语言时，可能导致误性别化。

Method: 研究使用三种语言对（en-es/fr/it）来调查ST模型在性别分配中的机制，分析训练数据模式、内部语言模型偏见和声学信息之间的互动，并使用对比特征归因分析频谱图。

Result: ST模型不简单复制训练数据中的特定术语性别关联，而是学习更广泛的男性占优模式。虽然内部语言模型显示强烈的男性偏见，但模型可以根据声学输入覆盖这些偏好。

Conclusion: 模型利用第一人称代词将性别术语与说话人关联，通过访问分布在频谱上的性别信息，而非集中在音高上，从而提高性别准确性。

Abstract: Unlike text, speech conveys information about the speaker, such as gender, through acoustic cues like pitch. This gives rise to modality-specific bias concerns. For example, in speech translation (ST), when translating from languages with notional gender, such as English, into languages where gender-ambiguous terms referring to the speaker are assigned grammatical gender, the speaker's vocal characteristics may play a role in gender assignment. This risks misgendering speakers, whether through masculine defaults or vocal-based assumptions. Yet, how ST models make these decisions remains poorly understood. We investigate the mechanisms ST models use to assign gender to speaker-referring terms across three language pairs (en-es/fr/it), examining how training data patterns, internal language model (ILM) biases, and acoustic information interact. We find that models do not simply replicate term-specific gender associations from training data, but learn broader patterns of masculine prevalence. While the ILM exhibits strong masculine bias, models can override these preferences based on acoustic input. Using contrastive feature attribution on spectrograms, we reveal that the model with higher gender accuracy relies on a previously unknown mechanism: using first-person pronouns to link gendered terms back to the speaker, accessing gender information distributed across the frequency spectrum rather than concentrated in pitch.

</details>


### [33] [Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects](https://arxiv.org/abs/2511.21533)
*Husne Ara Rubaiyeat,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 本文介绍了针对低资源语言——孟加拉手语翻译（BdSLT）的数据集IsharaKhobor及其子集，旨在为孟加拉语社区的听障人士开发基于AI的辅助工具。


<details>
  <summary>Details</summary>
Motivation: 孟加拉手语翻译（BdSLT）由于语言本身的低资源特性，一直受到严重限制。创建标准句子级数据集对于开发AI辅助工具至关重要。

Method: 作者提出了一个名为IsharaKhobor的数据集及其两个子集，并通过基于地标和RQE嵌入的基准测试，进行了词汇限制和规范化处理的消融研究，生成了另外两个数据集。

Result: 成功创建了IsharaKhobor数据集及其子集IsharaKhobor_small和IsharaKhobor_canonical_small，并进行了相关基准测试。

Conclusion: 该数据集及其子集为孟加拉手语翻译的研究提供了重要资源，并通过公开数据集促进了该领域的进一步发展。数据集可在www.kaggle.com/datasets/hasanssl/isharakhobor获取。

Abstract: Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].

</details>


### [34] [RoParQ: Paraphrase-Aware Alignment of Large Language Models Towards Robustness to Paraphrased Questions](https://arxiv.org/abs/2511.21568)
*Minjoon Choi*

Main category: cs.CL

TL;DR: 提出RoParQ基准和XParaCon指标，通过释义感知的SFT提升大模型语义一致性。


<details>
  <summary>Details</summary>
Motivation: 大模型在释义问题中表现不一致，暴露对表面模式的依赖而非真正语义理解。

Method: 1) 构建RoParQ基准（基于标准数据集生成释义并筛选引发不一致的样本）；2) 提出XParaCon指标（通过问题变体准确率标准差量化鲁棒性）；3) 设计释义感知的SFT策略。

Result: 针对性微调使轻量模型达到与大型预训练模型相当的鲁棒性，标准差降低37.5%（示例值，需具体数据）。

Conclusion: 该方法有效缓解浅层记忆，提升模型对语义变体的泛化能力。

Abstract: Large Language Models (LLMs) often exhibit inconsistent behavior when answering paraphrased questions, suggesting a reliance on surface-level patterns rather than true semantic understanding. To address this limitation, we introduce RoParQ, a benchmark specifically constructed to evaluate cross-paraphrase consistency in closed-book multiple-choice QA. This benchmark is derived from standard datasets by generating paraphrases via proprietary models and selectively retaining examples that elicit inconsistent confidence from a judge model. We further propose XParaCon, a novel evaluation metric that quantifies a model's robustness by measuring the standard deviation of accuracies across question variants. Additionally, we implement a reasoning-based, paraphrase-aware Supervised Fine-Tuning (SFT) strategy designed to align models toward semantic invariance. Our experiments demonstrate that this targeted alignment significantly enhances robustness. Notably, fine-tuned lightweight models achieved consistency levels comparable to much larger pre-trained models. These results highlight the efficacy of our approach in mitigating superficial memorization and fostering more robust, reliable LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Minimizing Hyperbolic Embedding Distortion with LLM-Guided Hierarchy Restructuring](https://arxiv.org/abs/2511.20679)
*Melika Ayoughi,Pascal Mettes,Paul Groth*

Main category: cs.AI

TL;DR: 本文探讨了利用大型语言模型（LLMs）自动重构层次结构以优化双曲嵌入。


<details>
  <summary>Details</summary>
Motivation: 双曲学习在机器学习应用中越来越重要，特别是在数据具有层次结构或语义时。双曲嵌入的质量与输入层次结构密切相关，而目前需要知识工程师手动调整层次结构。

Method: 提出一种基于提示的方法，利用LLMs对现有层次结构进行转换，以满足双曲嵌入的关键需求。

Result: 在16个不同层次结构上的实验表明，LLM重构的层次结构在多个标准嵌入质量指标上持续产生更高质量的双曲嵌入。

Conclusion: LLM引导的层次结构重构不仅提高了嵌入质量，还为知识工程师提供了可解释的重组方案。

Abstract: Hyperbolic geometry is an effective geometry for embedding hierarchical data structures. Hyperbolic learning has therefore become increasingly prominent in machine learning applications where data is hierarchically organized or governed by hierarchical semantics, ranging from recommendation systems to computer vision. The quality of hyperbolic embeddings is tightly coupled to the structure of the input hierarchy, which is often derived from knowledge graphs or ontologies. Recent work has uncovered that for an optimal hyperbolic embedding, a high branching factor and single inheritance are key, while embedding algorithms are robust to imbalance and hierarchy size. To assist knowledge engineers in reorganizing hierarchical knowledge, this paper investigates whether Large Language Models (LLMs) have the ability to automatically restructure hierarchies to meet these criteria. We propose a prompt-based approach to transform existing hierarchies using LLMs, guided by known desiderata for hyperbolic embeddings. Experiments on 16 diverse hierarchies show that LLM-restructured hierarchies consistently yield higher-quality hyperbolic embeddings across several standard embedding quality metrics. Moreover, we show how LLM-guided hierarchy restructuring enables explainable reorganizations, providing justifications to knowledge engineers.

</details>


### [36] [AssurAI: Experience with Constructing Korean Socio-cultural Datasets to Discover Potential Risks of Generative AI](https://arxiv.org/abs/2511.20686)
*Chae-Gyun Lim,Seung-Ho Han,EunYoung Byun,Jeongyun Han,Soohyun Cho,Eojin Joo,Heehyeon Kim,Sieun Kim,Juhoon Lee,Hyunsoo Lee,Dongkun Lee,Jonghwan Hyeon,Yechan Hwang,Young-Jun Lee,Kyeongryul Lee,Minhyeong An,Hyunjun Ahn,Jeongwoo Son,Junho Park,Donggyu Yoon,Taehyung Kim,Jeemin Kim,Dasom Choi,Kwangyoung Lee,Hyunseung Lim,Yeohyun Jung,Jongok Hong,Sooyohn Nam,Joonyoung Park,Sungmin Na,Yubin Choi,Jeanne Choi,Yoojin Hong,Sueun Jang,Youngseok Seo,Somin Park,Seoungung Jo,Wonhye Chae,Yeeun Jo,Eunyoung Kim,Joyce Jiyoung Whang,HwaJung Hong,Joseph Seering,Uichin Lee,Juho Kim,Sunna Choi,Seokyeon Ko,Taeho Kim,Kyunghoon Kim,Myungsik Ha,So Jung Lee,Jemin Hwang,JoonHo Kwak,Ho-Jin Choi*

Main category: cs.AI

TL;DR: 介绍AssurAI，一个用于评估生成式AI安全性的韩语多模态数据集，填补了现有英语主导的安全数据集在韩语及非文本模态方面的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的安全数据集主要以英语为主，未能涵盖韩语及非英语社会文化背景下的特定风险，并且局限于文本模态。为了应对这一挑战，需要构建一个多模态、文化相关的韩语安全数据集。

Method: 定义了35种AI风险因素分类体系，构建了包含11,480个实例的韩语多模态数据集AssurAI，包括文本、图像、视频和音频。采用两阶段构建、三重独立标注和迭代专家红队循环等质量控制流程。

Result: 通过试点研究验证了AssurAI在评估最新大型语言模型（LLM）安全性方面的有效性，并公开发布该数据集。

Conclusion: AssurAI的发布将有助于为韩国社区开发更安全、更可靠的生成式AI系统。

Abstract: The rapid evolution of generative AI necessitates robust safety evaluations. However, current safety datasets are predominantly English-centric, failing to capture specific risks in non-English, socio-cultural contexts such as Korean, and are often limited to the text modality. To address this gap, we introduce AssurAI, a new quality-controlled Korean multimodal dataset for evaluating the safety of generative AI. First, we define a taxonomy of 35 distinct AI risk factors, adapted from established frameworks by a multidisciplinary expert group to cover both universal harms and relevance to the Korean socio-cultural context. Second, leveraging this taxonomy, we construct and release AssurAI, a large-scale Korean multimodal dataset comprising 11,480 instances across text, image, video, and audio. Third, we apply the rigorous quality control process used to ensure data integrity, featuring a two-phase construction (i.e., expert-led seeding and crowdsourced scaling), triple independent annotation, and an iterative expert red-teaming loop. Our pilot study validates AssurAI's effectiveness in assessing the safety of recent LLMs. We release AssurAI to the public to facilitate the development of safer and more reliable generative AI systems for the Korean community.

</details>


### [37] [A Brief History of Digital Twin Technology](https://arxiv.org/abs/2511.20695)
*Yunqi Zhang,Kuangyu Shi,Biao Li*

Main category: cs.AI

TL;DR: 数字孪生技术起源于20世纪60年代的NASA航天器模拟，现已在医疗领域实现广泛应用，但仍面临临床整合挑战。


<details>
  <summary>Details</summary>
Motivation: 通过实时数据流和双向交互，数字孪生技术在医疗领域提供个性化诊断、治疗规划和药物开发，推动医疗转型。

Method: 整合影像、生物传感器和计算模型，创建患者特定的模拟，用于心脏、肿瘤和药物开发的数字孪生应用。

Result: 代表性应用包括预测心律失常治疗结果的心脏数字孪生、跟踪肿瘤进展的肿瘤数字孪生，以及加速药物发现的药理数字孪生。

Conclusion: 尽管面临互操作性、数据隐私和模型保真度等挑战，通过可解释AI、联邦学习和协调的监管框架，数字孪生有望推动医疗向预测性和个性化发展。

Abstract: Emerging from NASA's spacecraft simulations in the 1960s, digital twin technology has advanced through industrial adoption to spark a healthcare transformation. A digital twin is a dynamic, data-driven virtual counterpart of a physical system, continuously updated through real-time data streams and capable of bidirectional interaction. In medicine, digital twin integrates imaging, biosensors, and computational models to generate patient-specific simulations that support diagnosis, treatment planning, and drug development. Representative applications include cardiac digital twin for predicting arrhythmia treatment outcomes, oncology digital twin for tracking tumor progression and optimizing radiotherapy, and pharmacological digital twin for accelerating drug discovery. Despite rapid progress, major challenges, including interoperability, data privacy, and model fidelity, continue to limit widespread clinical integration. Emerging solutions such as explainable AI, federated learning, and harmonized regulatory frameworks offer promising pathways forward. Looking ahead, advances in multi-organ digital twin, genomics integration, and ethical governance will be essential to ensure that digital twin shifts healthcare from reactive treatment to predictive, preventive, and truly personalized medicine.

</details>


### [38] [Cross Domain Evaluation of Multimodal Chain-of-Thought Reasoning of different datasets into the Amazon CoT Framework](https://arxiv.org/abs/2511.20701)
*Nitya Tiwari,Parv Maheshwari,Vidisha Agarwal*

Main category: cs.AI

TL;DR: 本文分析了多模态思维链(Multimodal-CoT)在不同领域的泛化能力，发现视觉特征整合虽能减少幻觉，但思维链有效性因问题类型而异。


<details>
  <summary>Details</summary>
Motivation: 多模态CoT在ScienceQA等科学问答基准上取得先进成果，但其跨领域泛化能力尚未充分探索，特别是在需要广泛常识和世界知识的场景。

Method: 采用Zhang等提出的双流框架，分离推理生成与答案推断，通过门控融合机制整合视觉特征和基于T5的语言模型，并在A-OKVQA、OKVQA和ChartQA数据集进行系统消融研究。

Result: 视觉特征整合显著减少了推理生成的幻觉现象，但CoT有效性因问题类型差异很大，常识推理尤其具有挑战性。

Conclusion: 研究为多模态推理系统实施提供实践见解，并指出跨领域泛化的未来改进方向，强调需要针对不同问题类型优化推理方法。

Abstract: While recent work has extended CoT to multimodal settings, achieving state-of-the-art results on science question answering benchmarks like ScienceQA, the generalizability of these approaches across diverse domains remains underexplored. This work presents a comprehensive analysis of Multimodal Chain-of-Thought (Multimodal-CoT) reasoning, evaluating its effectiveness on the A-OKVQA, OKVQA and ChartQA datasets, which requires broad commonsense and world knowledge beyond scientific reasoning. We implement the two-stage framework proposed by Zhang et al. [3], which separates rationale generation from answer inference and integrates vision features through a gated fusion mechanism with T5-based language models. Through systematic ablation studies, we analyze the contributions of vision features, rationale quality, and architectural choices. Our findings reveal that while vision integration significantly reduces hallucination in rationale generation, the effectiveness of CoT reasoning varies substantially across question types, with commonsense reasoning presenting particular challenges. This work provides practical insights for researchers implementing multimodal reasoning systems and identifies key areas for future improvement in cross-domain generalization.

</details>


### [39] [Representation Interventions Enable Lifelong Unstructured Knowledge Control](https://arxiv.org/abs/2511.20892)
*Xuyuan Liu,Zhengzhang Chen,Xinshuai Dong,Yanchi Liu,Xujiang Zhao,Shengyu Chen,Haoyu Wang,Yujun Yan,Haifeng Chen*

Main category: cs.AI

TL;DR: RILKE是一种在表示空间内进行干预的稳健且可扩展的方法，用于在不进行昂贵再训练的情况下更新大型语言模型的知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常产生错误或过时的内容，而在不进行昂贵再训练的情况下高效准确地更新其知识是一个主要挑战。

Method: RILKE将知识控制视为模型表示空间中的干预，通过识别表示空间的表现力，学习具有释义鲁棒性和编辑局部化的模块，以限制每次更新至低维子空间。

Result: 在LLaMA和Qwen模型的知识编辑基准评估中，RILKE可扩展到大型数据集，展示出高的编辑成功率和强的释义泛化能力，同时保持通用效用。

Conclusion: RILKE是一种有效且可扩展的解决方案，可用于大型语言模型的终身知识控制。

Abstract: Large language models (LLMs) often produce incorrect or outdated content. Updating their knowledge efficiently and accurately without costly retraining is a major challenge. This problem is especially hard for complex, unstructured knowledge in a lifelong setting, where many edits must coexist without interference. We introduce RILKE (Representation Intervention for Lifelong KnowledgE Control), a robust and scalable method that treats knowledge control as interventions within the model's representation space. Leveraging representation-space expressiveness, we identify two properties enabling RILKE to deliver fine-grained control over complex, unstructured knowledge while maintaining general utility with frozen base weights. During training, RILKE learns paraphrase-robust and edit-localized modules that limit each update to a low-dimensional subspace to minimize cross-edit interference. In inference, a query-adaptive router selects the appropriate module to guide the model's generation. In evaluation on knowledge editing benchmarks with LLaMA and Qwen models, RILKE is scalable to large-scale datasets, demonstrating high edit success, strong paraphrase generalization, and preserving general utility with modest memory overhead. These results show RILKE is an effective and scalable solution for lifelong knowledge control in LLMs.

</details>


### [40] [ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction](https://arxiv.org/abs/2511.20937)
*Qineng Wang,Wenlong Huang,Yu Zhou,Hang Yin,Tianwei Bao,Jianwen Lyu,Weiyu Liu,Ruohan Zhang,Jiajun Wu,Li Fei-Fei,Manling Li*

Main category: cs.AI

TL;DR: 本文介绍了ENACT，一个用于评估视觉语言模型（VLM）是否展示具身认知的基准测试。


<details>
  <summary>Details</summary>
Motivation: 具身认知认为，智能源于感知与运动的互动，而非被动观察。现代视觉语言模型主要在非具身方式下训练，因此作者提出一个评估这些模型是否展示具身认知的基准。

Method: ENACT基准测试通过视觉问答（VQA）格式，将具身认知评估转化为部分可观察马尔可夫决策过程（POMDP）中的世界建模，包括顺序重排任务：给定动作重新排序观察结果（正向世界建模）和给定观察结果重新排序动作（反向世界建模）。

Result: 实验揭示，前沿视觉语言模型与人类之间存在性能差距，并且随着互动时间的增加，差距扩大。模型在反向任务上的表现始终优于正向任务，并表现出人类中心偏差。

Conclusion: ENACT提供了一个可扩展的流程，用于评估模型在具身认知方面的能力，并揭示了当前模型在具身认知能力上的局限性。

Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.

</details>


### [41] [Improving Procedural Skill Explanations via Constrained Generation: A Symbolic-LLM Hybrid Architecture](https://arxiv.org/abs/2511.20942)
*Rahul Dass,Thomas Bowlin,Zebing Li,Xiao Jin,Ashok Goel*

Main category: cs.AI

TL;DR: Ivy是一种结合符号化TMK模型与生成式LLM的AI辅导系统，通过结构化约束提升多步骤解释的教学质量。


<details>
  <summary>Details</summary>
Motivation: 程序性技能学习需要传递因果、目标导向和组合逻辑，而传统LLM生成的解释往往流于表面，缺乏结构性。

Method: 将符号化的任务-方法-知识(TMK)模型与生成式LLM结合，TMK编码因果转换、目标层次和问题分解，约束LLM在结构化范围内生成解释。

Result: 在三个推理维度上，Ivy相比GPT和检索增强GPT基线，其解释的结构性质量获得专家与独立标注者的一致认可。

Conclusion: 该研究验证了符号化约束能有效提升AI生成解释的教学价值，为教育AI提供了可扩展的解决方案。

Abstract: In procedural skill learning, instructional explanations must convey not just steps, but the causal, goal-directed, and compositional logic behind them. Large language models (LLMs) often produce fluent yet shallow responses that miss this structure. We present Ivy, an AI coaching system that delivers structured, multi-step explanations by combining symbolic Task-Method-Knowledge (TMK) models with a generative interpretation layer-an LLM that constructs explanations while being constrained by TMK structure. TMK encodes causal transitions, goal hierarchies, and problem decompositions, and guides the LLM within explicit structural bounds. We evaluate Ivy against responses against GPT and retrieval-augmented GPT baselines using expert and independent annotations across three inferential dimensions. Results show that symbolic constraints consistently improve the structural quality of explanations for "how" and "why" questions. This study demonstrates a scalable AI for education approach that strengthens the pedagogical value of AI-generated explanations in intelligent coaching systems.

</details>


### [42] [ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning](https://arxiv.org/abs/2511.21005)
*Jinpeng Wang,Chao Li,Ting Ye,Mengyuan Zhang,Wei Liu,Jian Luan*

Main category: cs.AI

TL;DR: 本文提出了一种新的强化学习方法ICPO，通过内在置信度驱动和群体相对偏好优化，提高了大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法受到粗粒度奖励、奖励噪声和探索效率低下的限制，导致训练不稳定和熵崩溃。

Method: ICPO通过比较同一输入提示下多个响应的相对生成概率，计算每个响应的偏好优势分数，并将其与可验证奖励结合以指导探索过程。

Result: 在四个通用领域和三个数学领域的基准测试中，ICPO相比GRPO稳步提高了推理能力。

Conclusion: ICPO通过偏好优势分数缓解了粗粒度奖励和奖励噪声的问题，有效抑制了过度自信错误，提高了被低估的高质量响应的相对优势，并防止模型过度拟合特定策略，从而促进了更彻底的探索。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.

</details>


### [43] [Causality Without Causal Models](https://arxiv.org/abs/2511.21260)
*Joseph Y. Halpern,Rafael Pass*

Main category: cs.AI

TL;DR: 本文抽象化了Halpern和Pearl的因果定义，使其适用于更广泛的模型，并扩展了解释的抽象定义。


<details>
  <summary>Details</summary>
Motivation: Halpern和Pearl的因果定义局限于特定模型，无法处理涉及析取、否定、信念和嵌套反事实的公式。为了克服这些限制，作者旨在抽象化该定义，使其更具通用性。

Method: 通过提取Halpern和Pearl因果定义的关键特征，将其抽象化，以便适用于任何定义了反事实的模型。

Result: 抽象化的定义可以应用于更广泛的模型，包括允许回溯的模型，并且可以处理涉及析取、否定、信念和嵌套反事实的公式。此外，还扩展了解释的抽象定义。

Conclusion: 抽象化的因果定义不仅扩展了适用范围，还加深了对因果模型中定义特征的理解，为因果性和解释提供了更通用的框架。

Abstract: Perhaps the most prominent current definition of (actual) causality is due to Halpern and Pearl.  It is defined using causal models (also known as structural equations models).  We abstract the definition, extracting its key features, so that it can be applied to any other model where counterfactuals are defined. By abstracting the definition, we gain a number of benefits. Not only can we apply the definition in a wider range of models, including ones that allow, for example, backtracking, but we can apply the definition to determine if A is a cause of B  even if A and B are formulas involving disjunctions, negations, beliefs, and nested counterfactuals (none of which can be handled by the Halpern-Pearl definition). Moreover, we can extend the ideas to getting an abstract definition of explanation that can be applied beyond causal models. Finally, we gain a deeper understanding of features of the definition  even in causal models.

</details>


### [44] [Prune4Web: DOM Tree Pruning Programming for Web Agent](https://arxiv.org/abs/2511.21398)
*Jiayuan Zhang,Kaiquan Chen,Zhihao Lu,Enshen Zhou,Qian Yu,Jing Zhang*

Main category: cs.AI

TL;DR: 提出Prune4Web，通过程序化剪枝DOM树以应对大规模DOM处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based web agents在复杂网页导航中由于DOM结构过大而效率低下，且现有策略在信息损失和效率之间难以平衡。

Method: 引入Prune4Web，利用LLM生成Python评分脚本进行动态DOM元素过滤，减少LLM对原始DOM的读取，采用轻量级程序进行遍历和评分。

Result: 实现了候选元素减少25x到50x，显著提高了动作定位的精确性，低级别接地任务中准确率从46.8%提升至88.28%。

Conclusion: Prune4Web在真实世界Web自动化任务中表现出色，通过程序化剪枝和专门的数据标注及训练策略，提高了自动化效率和准确性。

Abstract: Web automation employs intelligent agents to execute high-level tasks by mimicking human interactions with web interfaces. Despite the capabilities of recent Large Language Model (LLM)-based web agents, navigating complex, real-world webpages efficiently remains a significant hurdle due to the prohibitively large size of Document Object Model (DOM) structures, often ranging from 10,000 to 100,000 tokens. Existing strategies typically rely on crude DOM truncation -- risking the loss of critical information -- or employ inefficient heuristics and separate ranking models, failing to achieve an optimal balance between precision and scalability. To address these challenges, we introduce Prune4Web, a novel paradigm that shifts DOM processing from resource-intensive LLM reading to efficient programmatic pruning. Central to our approach is DOM Tree Pruning Programming, where an LLM generates executable Python scoring scripts to dynamically filter DOM elements based on semantic cues from decomposed sub-tasks. This mechanism eliminates the need for LLMs to ingest raw, massive DOMs, instead delegating traversal and scoring to lightweight, interpretable programs. This methodology achieves a 25x to 50x reduction in candidate elements for grounding, thereby facilitating precise action localization while mitigating attention dilution. Furthermore, we propose a specialized data annotation pipeline and a two-turn dialogue training strategy that jointly optimizes the Planner, Programmatic Filter, and Grounder within a unified framework. Extensive experiments demonstrate state-of-the-art performance. Notably, on our low-level grounding task, Prune4Web dramatically improves accuracy from 46.8% to 88.28%, underscoring its efficacy in real-world web automation.

</details>


### [45] [New Hybrid Heuristics for Pseudo-Boolean Propagation](https://arxiv.org/abs/2511.21417)
*Mia Müßig,Jan Johannsen*

Main category: cs.AI

TL;DR: 本文介绍了伪布尔求解器中一种新的混合单位传播启发式方法，该方法结合了监视文字方案和计数方法，并在RoundingSAT求解器中显著优于当前方法。


<details>
  <summary>Details</summary>
Motivation: 当前伪布尔求解器中最成功的单位传播策略是监视文字方案和计数方法的混合模式。为了进一步提升性能，本文旨在提出新的启发式方法，以超越现有技术。

Method: 本文提出了一种新的启发式方法，用于伪布尔求解器中的混合单位传播决策，结合了监视文字方案和计数方法。

Result: 新的启发式方法在RoundingSAT求解器中表现出色，大幅优于当前方法。

Conclusion: 通过引入新的启发式方法，本文在伪布尔求解器的单位传播策略方面取得了显著的性能提升，证明了该方法的有效性。

Abstract: In pseudo-boolean solving the currently most successful unit propagation strategy is a hybrid mode combining the watched literal scheme with the counting method. This short paper introduces new heuristics for this hybrid decision, which are able to drastically outperform the current method in the RoundingSAT solver.

</details>


### [46] [MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning](https://arxiv.org/abs/2511.21460)
*Junjian Wang,Lidan Zhao,Xi Sheryl Zhang*

Main category: cs.AI

TL;DR: 提出MADRA多智能体辩论风险评估框架，无需训练即可提升具身AI任务规划安全性，结合分层认知协作规划，在AI2-THOR和VirtualHome上实现90%以上危险任务拦截且安全任务误拒率低。


<details>
  <summary>Details</summary>
Motivation: 现有方法在具身AI任务规划中存在计算成本高（需偏好对齐训练）或过度拒绝（单智能体安全提示）的问题，需解决安全性和任务性能的平衡。

Method: 1) MADRA框架：多LLM智能体辩论+关键评估器评分（逻辑性、风险识别、证据质量、清晰度），迭代审议与共识投票；2) 分层认知协作规划框架：整合安全、记忆、规划、自我进化机制；3) 构建SafeAware-VH基准数据集（800条VirtualHome标注指令）。

Result: 在AI2-THOR和VirtualHome上：危险任务拒绝率>90%，安全任务误拒率低，安全性和执行效率均优于现有方法。

Conclusion: MADRA提供了一种可扩展、模型无关的解决方案，能构建高可信度的具身AI智能体，兼顾安全与任务完成能力。

Abstract: Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.

</details>


### [47] [SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition](https://arxiv.org/abs/2511.21471)
*Peiran Xu,Sudong Wang,Yao Zhu,Jianing Li,Yunjian Zhang*

Main category: cs.AI

TL;DR: 提出了一个分层空间认知框架，以解决现有MLLMs基准测试过于简化的问题，并构建了SpatialBench基准。


<details>
  <summary>Details</summary>
Motivation: 空间认知对于多模态智能体在物理世界中的有效互动至关重要，但现有基准未能充分捕捉空间能力的层次结构和相互依赖性。

Method: 提出了一个包含五个复杂层次的空间认知框架，并构建了涵盖15个任务的SpatialBench基准，同时引入了一种面向能力的评估指标。

Result: 大规模实验揭示了不同认知层次下的模型性能差异，模型在感知基础方面表现强劲，但在符号推理、因果推断和规划方面仍受限。

Conclusion: 建立了首个系统化的框架用于测量MLLMs中的分层空间认知，为未来空间智能系统奠定了基础。

Abstract: Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.

</details>


### [48] [Pessimistic Verification for Open Ended Math Questions](https://arxiv.org/abs/2511.21522)
*Yanxing Huang,Zihan Tang,Zejin Lin,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 悲观验证通过并行多重验证显著提升数学问题验证性能。


<details>
  <summary>Details</summary>
Motivation: 当前验证性能的关键限制在于错误检测能力。

Method: 设计了多种悲观验证的变体，通过并行多重验证同一证明，只要有一个验证报告错误，就认为证明错误。

Result: 该方法在不增加大量计算资源的情况下，显著提升了多个数学验证基准的性能，甚至超越了扩展long-CoT在测试时的缩放效率。

Conclusion: 数学问题的自我验证有效提高语言模型输出的可靠性和性能，对实现长期数学任务至关重要，悲观验证的研究将有助于提升语言模型在各类数学任务中的能力。

Abstract: The key limitation of the verification performance lies in the ability of error detection. With this intuition we designed several variants of pessimistic verification, which are simple workflows that could significantly improve the verification of open-ended math questions. In pessimistic verification we construct multiple parallel verifications for the same proof, and the proof is deemed incorrect if any one of them reports an error. This simple technique significantly improves the performance across many math verification benchmarks without incurring substantial computational resources. Its token efficiency even surpassed extended long-CoT in test-time scaling. Our case studies further indicate that the majority of false negatives in stronger models are actually caused by annotation errors in the original dataset, so our method's performance is in fact underestimated. Self-verification for mathematical problems can effectively improve the reliability and performance of language model outputs, and it also plays a critical role in enabling long-horizon mathematical tasks. We believe that research on pessimistic verification will help enhance the mathematical capabilities of language models across a wide range of tasks.

</details>


### [49] [Self-Transparency Failures in Expert-Persona LLMs: A Large-Scale Behavioral Audit](https://arxiv.org/abs/2511.21569)
*Alex Diep*

Main category: cs.AI

TL;DR: 该研究测试了语言模型在高风险专业领域中自我披露AI身份的能力，发现模型在不同领域和角色下的自我披露率差异巨大，且披露行为更多受模型训练因素而非参数规模影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型在专业领域若无法可靠披露其AI身份，用户将难以判断其能力边界，可能导致信任误判和潜在危害。

Method: 采用‘common-garden’设计，对16个开源模型（4B–671B参数）在19,200次试验中进行审计，测试其在不同专业角色（如金融顾问、神经外科医生）中的自我披露行为，并结合贝叶斯方法与Rogan–Gladen校正验证结果。

Result: 自我披露率从2.8%到73.6%不等，角色差异显著（金融顾问30.8% vs 神经外科医生3.5%）；模型身份比参数数量更能预测行为（ΔR²_adj=0.359 vs 0.018）；推理优化反而降低披露率，最高达48.4%。

Conclusion: 模型的自我透明度主要由训练因素决定，而非模型规模；不能假设模型在训练中的安全表现会迁移到实际部署中，需通过针对性设计和实证验证确保安全。

Abstract: If a language model cannot reliably disclose its AI identity in expert contexts, users cannot trust its competence boundaries. This study examines self-transparency in models assigned professional personas within high-stakes domains where false expertise risks user harm. Using a common-garden design, sixteen open-weight models (4B--671B parameters) were audited across 19,200 trials. Models exhibited sharp domain-specific inconsistency: a Financial Advisor persona elicited 30.8% disclosure initially, while a Neurosurgeon persona elicited only 3.5%. This creates preconditions for a "Reverse Gell-Mann Amnesia" effect, where transparency in some domains leads users to overgeneralize trust to contexts where disclosure fails. Disclosure ranged from 2.8% to 73.6%, with a 14B model reaching 61.4% while a 70B produced just 4.1%. Model identity predicted behavior better than parameter count ($ΔR_{adj}^{2} = 0.359$ vs 0.018). Reasoning optimization actively suppressed self-transparency in some models, with reasoning variants showing up to 48.4% lower disclosure than base counterparts. Bayesian validation with Rogan--Gladen correction confirmed robustness to measurement error ($κ= 0.908$). These findings demonstrate transparency reflects training factors rather than scale. Organizations cannot assume safety properties transfer to deployment contexts, requiring deliberate behavior design and empirical verification.

</details>


### [50] [From Prediction to Foresight: The Role of AI in Designing Responsible Futures](https://arxiv.org/abs/2511.21570)
*Maria Perez-Ortiz*

Main category: cs.AI

TL;DR: 本文提出了“负责任的计算预见力”概念，探讨了以人为本的人工智能和计算模型在负责任预见力中的角色，并确立了该领域的基础原则和AI驱动工具。


<details>
  <summary>Details</summary>
Motivation: 在快速技术进步和复杂全球挑战的时代，负责任预见力成为政策制定者应对未来不确定性和塑造未来的重要框架。

Method: 通过结合人工智能、模拟和情景分析，增强政策制定者处理不确定性、评估风险和制定可持续战略的能力，并提出了“负责任的计算预见力”的基础原则和工具。

Result: AI作为支持工具，能够增强政策制定者的判断力，帮助塑造具有弹性和道德性的未来，而不是取代人类的判断。

Conclusion: 本文主张将AI审慎地整合到预见力实践中，以增强政策制定者和社区应对21世纪重大挑战的能力。

Abstract: In an era marked by rapid technological advancements and complex global challenges, responsible foresight has emerged as an essential framework for policymakers aiming to navigate future uncertainties and shape the future. Responsible foresight entails the ethical anticipation of emerging opportunities and risks, with a focus on fostering proactive, sustainable, and accountable future design. This paper coins the term "responsible computational foresight", examining the role of human-centric artificial intelligence and computational modeling in advancing responsible foresight, establishing a set of foundational principles for this new field and presenting a suite of AI-driven foresight tools currently shaping it. AI, particularly in conjunction with simulations and scenario analysis, enhances policymakers' ability to address uncertainty, evaluate risks, and devise strategies geared toward sustainable, resilient futures. However, responsible foresight extends beyond mere technical forecasting; it demands a nuanced understanding of the interdependencies within social, environmental, economic and political systems, alongside a commitment to ethical, long-term decision-making that supports human intelligence. We argue that AI will play a role as a supportive tool in responsible, human-centered foresight, complementing rather than substituting policymaker judgment to enable the proactive shaping of resilient and ethically sound futures. This paper advocates for the thoughtful integration of AI into foresight practices to empower policymakers and communities as they confront the grand challenges of the 21st century.

</details>
