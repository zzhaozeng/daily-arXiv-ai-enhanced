<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 23]
- [cs.AI](#cs.AI) [Total: 14]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Test-time Scaling of LLMs: A Survey from A Subproblem Structure Perspective](https://arxiv.org/abs/2511.14772)
*Zhuoyi Yang,Xu Guo,Tong Zhang,Huijuan Xu,Boyang Li*

Main category: cs.CL

TL;DR: 本文综述了通过在推理阶段分配额外计算资源来提高预训练大语言模型预测准确性的技术。


<details>
  <summary>Details</summary>
Motivation: 旨在分类测试时缩放方法，强调问题分解的方式以及子问题的拓扑结构，从而统一不同的方法。

Method: 采用分类和综合分析方法，考察了链式思维、分支解决合并、树形思维等方法，并分析其优缺点。

Result: 提出了一个统一的视角来理解这些技术，并总结了各自的强项与弱点。

Conclusion: 文章最后提出了未来研究的有希望的方向。

Abstract: With this paper, we survey techniques for improving the predictive accuracy of pretrained large language models by allocating additional compute at inference time. In categorizing test-time scaling methods, we place special emphasis on how a problem is decomposed into subproblems and on the topological organization of these subproblems whether sequential, parallel, or tree-structured. This perspective allows us to unify diverse approaches such as Chain-of-Thought, Branch-Solve-Merge, and Tree-of-Thought under a common lens. We further synthesize existing analyses of these techniques, highlighting their respective strengths and weaknesses, and conclude by outlining promising directions for future research

</details>


### [2] [Temporal Predictors of Outcome in Reasoning Language Models](https://arxiv.org/abs/2511.14773)
*Joey David*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在思维链（CoT）推理过程中，多早可以预测最终答案的正确性。


<details>
  <summary>Details</summary>
Motivation: 当前对CoT的研究关注模型推理的逐步过程，但对模型何时内部确定最终答案仍不清楚。本文旨在探讨这一问题。

Method: 通过在隐藏状态上训练线性分类器，分析模型在生成前t个推理标记后的内部表示。

Result: 实验表明，仅通过前几个标记，模型最终答案的正确性就可以高度预测。较难问题的预测准确率下降，揭示出长CoT的选择偏差。

Conclusion: 推理模型在生成仅几个标记后，内部自我评估的成功性就已经显现，这对模型的可解释性和推理时的控制具有重要意义。

Abstract: The chain-of-thought (CoT) paradigm uses the elicitation of step-by-step rationales as a proxy for reasoning, gradually refining the model's latent representation of a solution. However, it remains unclear just how early a Large Language Model (LLM) internally commits to an eventual outcome. We probe this by training linear classifiers on hidden states after the first t reasoning tokens, showing that eventual correctness is highly predictable after only a few tokens, even when longer outputs are needed to reach a definite answer. We show that, for harder questions, a drop in predictive accuracy highlights a selection artifact: hard items are disproportionately represented in long CoTs. Overall, our results imply that for reasoning models, internal self-assessment of success tends to emerge after only a few tokens, with implications for interpretability and for inference-time control.

</details>


### [3] [LiveCLKTBench: Towards Reliable Evaluation of Cross-Lingual Knowledge Transfer in Multilingual LLMs](https://arxiv.org/abs/2511.14774)
*Pei-Fu Guo,Yun-Da Tsai,Chun-Chia Hsu,Kai-Xin Chen,Ya-An Tsai,Kai-Wei Chang,Nanyun Peng,Mi-Yen Yeh,Shou-De Lin*

Main category: cs.CL

TL;DR: 提出LiveCLKTBench，一种用于评估大语言模型跨语言知识迁移的自动化生成管道，发现语言距离和模型规模对迁移效果有显著影响。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型中的跨语言知识迁移具有挑战性，因为目标语言的正确答案可能源于迁移或预训练期间的先验知识，需要专门工具进行区分。

Method: 设计LiveCLKTBench，通过识别真实世界领域中的自包含、时间敏感知识实体，生成多语言事实问题，评估跨语言迁移能力。

Result: 评估多个大语言模型在五种语言上的表现，发现跨语言迁移受语言距离影响且常不对称，模型规模提升对迁移效果有递减增益。

Conclusion: LiveCLKTBench提供了可靠基准，揭示了多语言迁移的新见解，对未来的多语言研究具有重要价值。

Abstract: Evaluating cross-lingual knowledge transfer in large language models is challenging, as correct answers in a target language may arise either from genuine transfer or from prior exposure during pre-training. We present LiveCLKTBench, an automated generation pipeline specifically designed to isolate and measure cross-lingual knowledge transfer. Our pipeline identifies self-contained, time-sensitive knowledge entities from real-world domains, filters them based on temporal occurrence, and verifies them against the model's knowledge. The documents of these valid entities are then used to generate factual questions, which are translated into multiple languages to evaluate transferability across linguistic boundaries. Using LiveCLKTBench, we evaluate several LLMs across five languages and observe that cross-lingual transfer is strongly influenced by linguistic distance and often asymmetric across language directions. While larger models improve transfer, the gains diminish with scale and vary across domains. These findings provide new insights into multilingual transfer and demonstrate the value of LiveCLKTBench as a reliable benchmark for future research.

</details>


### [4] [COMPASS: Context-Modulated PID Attention Steering System for Hallucination Mitigation](https://arxiv.org/abs/2511.14776)
*Snigdha Pandya,Rohan Nagale,Kenji Sahay,Anna Lin,Shikhar Shiromani,Kevin Zhu,Dev Sunishchal*

Main category: cs.CL

TL;DR: 提出COMPASS框架，通过PID控制器动态调节注意力机制以减少大语言模型的上下文幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成时会产生流畅但不准确的陈述，需理解并引导其内部注意力分配机制以提升可信度和可解释性。

Method: 引入COMPASS框架，嵌入基于模型的反馈回路，利用上下文依赖评分（CRS）作为PID控制器的输入，动态调节注意力头。

Result: 在HotpotQA、XSum、HaluEval、RAGTruth等基准上，COMPASS显著降低上下文幻觉率2.8%至5.8%，并揭示了不同注意力头对证据对齐的贡献。

Conclusion: 反馈驱动的可解释性是提升LLM行为科学理解的重要途径，COMPASS为此提供了有效解决方案。

Abstract: Large language models (LLMs) often generate fluent but factually incorrect statements despite having access to relevant evidence, a failure mode rooted in how they allocate attention between contextual and parametric knowledge. Understanding and steering this internal behavior is key both for trustworthy deployment and for scientific interpretability of model mechanisms. We introduce COMPASS (Context-Modulated PID Attention Steering System), a lightweight, interpretable control framework that embeds a model-based feedback loop directly within decoding. COMPASS quantifies context reliance via a transparent metric, the Context Reliance Score (CRS), which serves as an online probe of how attention heads ground generation in evidence. Using this interpretable signal, a PID controller dynamically modulates attention heads to maintain factual consistency without retraining or multi-pass decoding. Across benchmarks (HotpotQA, XSum, HaluEval, RAGTruth), COMPASS consistently reduces contextual hallucination rates (2.8 to 5.8 percent absolute) while revealing how distinct attention heads contribute to evidence alignment. These results highlight feedback-driven interpretability as a pathway toward scientific understanding of LLM behavior.

</details>


### [5] [The Impact of Prosodic Segmentation on Speech Synthesis of Spontaneous Speech](https://arxiv.org/abs/2511.14779)
*Julio Cesar Galdino,Sidney Evaldo Leal,Leticia Gabriella De Souza,Rodrigo de Freitas Lima,Antonio Nelson Fornari Mendes Moreira,Arnaldo Candido Junior,Miguel Oliveira,Edresson Casanova,Sandra M. Aluísio*

Main category: cs.CL

TL;DR: 本文评估了手动和自动韵律分割标注对非自回归语音合成模型FastSpeech 2合成语音质量的影响，特别是在巴西葡萄牙语的自由对话语音合成中。


<details>
  <summary>Details</summary>
Motivation: 自由对话语音合成在捕捉对话的自然流程（如轮替、停顿和不流利）方面存在挑战，尤其是带有显式韵律分割的数据集构建及其对合成语音的影响仍待探索。

Method: 使用FastSpeech 2模型，评估手动和自动韵律分割标注在巴西葡萄牙语上的效果，并分析中性陈述句的韵律模式。

Result: 实验结果表明，带有韵律分割的训练略微提升了语音的可懂度和声学自然性，手动韵律分割引入了更多变异性，有助于更自然的韵律。

Conclusion: 手动韵律分割在自由对话语音合成中引入了更多自然性，两种训练方法均能复制预期的韵律模式，但韵律模型更接近自然的前核轮廓。

Abstract: Spontaneous speech presents several challenges for speech synthesis, particularly in capturing the natural flow of conversation, including turn-taking, pauses, and disfluencies. Although speech synthesis systems have made significant progress in generating natural and intelligible speech, primarily through architectures that implicitly model prosodic features such as pitch, intensity, and duration, the construction of datasets with explicit prosodic segmentation and their impact on spontaneous speech synthesis remains largely unexplored. This paper evaluates the effects of manual and automatic prosodic segmentation annotations in Brazilian Portuguese on the quality of speech synthesized by a non-autoregressive model, FastSpeech 2. Experimental results show that training with prosodic segmentation produced slightly more intelligible and acoustically natural speech. While automatic segmentation tends to create more regular segments, manual prosodic segmentation introduces greater variability, which contributes to more natural prosody. Analysis of neutral declarative utterances showed that both training approaches reproduced the expected nuclear accent pattern, but the prosodic model aligned more closely with natural pre-nuclear contours. To support reproducibility and future research, all datasets, source codes, and trained models are publicly available under the CC BY-NC-ND 4.0 license.

</details>


### [6] [Human or LLM as Standardized Patients? A Comparative Study for Medical Education](https://arxiv.org/abs/2511.14783)
*Bingquan Zhang,Xiaoxiao Liu,Yuchi Wang,Lei Zhou,Qianqian Xie,Benyou Wang*

Main category: cs.CL

TL;DR: EasyMED是一种结合多智能体框架的患者模拟系统，旨在替代传统标准化病人，降低成本并提高可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 标准化病人在临床技能培训中成本高、不灵活且难以扩展，现有基于大语言模型的模拟器行为不一致且缺乏与真人标准化病人的严格比较。

Method: EasyMED采用多智能体框架，包括用于现实对话的病人智能体、事实一致性辅助智能体和提供可操作反馈的评估智能体，并引入SPBench基准进行系统评估。

Result: 实验显示，EasyMED在与人类标准化病人学习成果相当的同时，对低基础学生技能提升更大，并提供了更好的灵活性、心理安全性和成本效益。

Conclusion: EasyMED在临床技能培训中能够有效地替代传统标准化病人，具备更高的成本效益和教学灵活性。

Abstract: Standardized Patients (SP) are indispensable for clinical skills training but remain expensive, inflexible, and difficult to scale. Existing large-language-model (LLM)-based SP simulators promise lower cost yet show inconsistent behavior and lack rigorous comparison with human SP. We present EasyMED, a multi-agent framework combining a Patient Agent for realistic dialogue, an Auxiliary Agent for factual consistency, and an Evaluation Agent that delivers actionable feedback. To support systematic assessment, we introduce SPBench, a benchmark of real SP-doctor interactions spanning 14 specialties and eight expert-defined evaluation criteria. Experiments demonstrate that EasyMED matches human SP learning outcomes while producing greater skill gains for lower-baseline students and offering improved flexibility, psychological safety, and cost efficiency.

</details>


### [7] [Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings](https://arxiv.org/abs/2511.14868)
*Xueying Ding,Xingyue Huang,Mingxuan Ju,Liam Collins,Yozen Liu,Leman Akoglu,Neil Shah,Tong Zhao*

Main category: cs.CL

TL;DR: 提出一种名为 Hierarchical Token Prepending (HTP) 的方法，通过分层令牌预处理和平均池化来改善大型语言模型的文本嵌入质量，尤其是在长文档中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的因果注意力机制限制了信息从后往前流动，降低了表示质量，现有方法通过添加单个摘要令牌来尝试解决，但会过度压缩信息。

Method: HTP 将输入分割成块，并在后续块前添加块级摘要令牌，以创建多个反向信息流通道，并使用平均池化替代最后的令牌池化。

Result: 在11个检索数据集和30个通用嵌入基准测试中，HTP 实现了持续的性能提升，尤其在长上下文设置中表现突出。

Conclusion: HTP 是一种简单且与架构无关的方法，可以增强零样本和微调模型，为长文档嵌入提供可扩展的优化路径。

Abstract: Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.

</details>


### [8] [Mathematical Analysis of Hallucination Dynamics in Large Language Models: Uncertainty Quantification, Advanced Decoding, and Principled Mitigation](https://arxiv.org/abs/2511.15005)
*Moses Kiprono*

Main category: cs.CL

TL;DR: 本文提出了一个基于数学原理的框架，用于理解、测量和减轻大型语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型尽管强大，但仍容易产生听起来合理但实际上不正确或无支持的输出，即幻觉。

Method: 借鉴概率建模、信息论、三角信号分析和贝叶斯不确定性估计，分析了自回归错误如何复合，并提出改进的不确定性度量及缓解策略。

Result: 提出了包括语义和相位感知变体的改进不确定性度量，以及对比解码、检索增强基础、事实对齐和弃权等缓解策略。

Conclusion: 该统一视角连接了校准、检索和对齐的最新进展，有助于支持更安全、更可靠的大型语言模型。

Abstract: Large Language Models (LLMs) are powerful linguistic engines but remain susceptible to hallucinations: plausible-sounding outputs that are factually incorrect or unsupported. In this work, we present a mathematically grounded framework to understand, measure, and mitigate these hallucinations. Drawing on probabilistic modeling, information theory, trigonometric signal analysis, and Bayesian uncertainty estimation, we analyze how errors compound autoregressively, propose refined uncertainty metrics, including semantic and phase-aware variants, and develop principled mitigation strategies such as contrastive decoding, retrieval-augmented grounding, factual alignment, and abstention. This unified lens connects recent advances in calibration, retrieval, and alignment to support safer and more reliable LLMs.

</details>


### [9] [Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs](https://arxiv.org/abs/2511.15163)
*Yang Wu,Rujing Yao,Tong Zhang,Yufei Shi,Zhuoren Jiang,Zhushan Li,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 提出TASA，一个结合个人画像、记忆和遗忘动态的个性化数学辅导框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉学生知识随能力、概念差距和遗忘模式的动态变化，尤其在数学辅导中需要更精细的个性化辅导。

Method: TASA维护结构化的学生画像和事件记忆，通过连续遗忘曲线与知识追踪动态更新学生掌握状态，并生成适当难度的问题和解释。

Result: 实验结果显示，TASA在提升学习效果和自适应辅导行为方面优于代表性基线。

Conclusion: 在基于大语言模型的辅导系统中，建模时间遗忘和学习者画像具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly integrated into intelligent tutoring systems to provide human-like and adaptive instruction. However, most existing approaches fail to capture how students' knowledge evolves dynamically across their proficiencies, conceptual gaps, and forgetting patterns. This challenge is particularly acute in mathematics tutoring, where effective instruction requires fine-grained scaffolding precisely calibrated to each student's mastery level and cognitive retention. To address this issue, we propose TASA (Teaching According to Students' Aptitude), a student-aware tutoring framework that integrates persona, memory, and forgetting dynamics for personalized mathematics learning. Specifically, TASA maintains a structured student persona capturing proficiency profiles and an event memory recording prior learning interactions. By incorporating a continuous forgetting curve with knowledge tracing, TASA dynamically updates each student's mastery state and generates contextually appropriate, difficulty-calibrated questions and explanations. Empirical results demonstrate that TASA achieves superior learning outcomes and more adaptive tutoring behavior compared to representative baselines, underscoring the importance of modeling temporal forgetting and learner profiles in LLM-based tutoring systems.

</details>


### [10] [HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples](https://arxiv.org/abs/2511.15183)
*Rishikant Chigrupaatii,Ponnada Sai Tulasi Kanishka,Lalit Chandra Routhu,Martin Patel Sama Supratheek Reddy,Divyam Gupta,Dasari Srikar,Krishna Teja Kuchimanchi,Rajiv Misra,Rohun Tripathi*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的框架来评估印度语言中的视觉语言模型（VLM），并生成了一个名为HinTel-AlignBench的基准测试，该测试从多种来源抽取样本，涵盖印地语和泰卢固语，并与英语样本对齐。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言VLM评估存在四个主要限制：依赖未经证实的自动翻译、任务/领域覆盖狭窄、样本量有限以及缺乏文化和本地来源的问题回答（QA）。为解决这些问题，需要更全面的评估框架。

Method: 提出了一种半自动数据集创建框架，结合回译、过滤和人工验证，并生成了HinTel-AlignBench基准，包括适配的英语数据集和本地新颖的印度数据集。

Result: 研究发现，在所有模型中，从英语到印度语言的任务表现有退化，平均退化在印地语中为8.3分，在泰卢固语中为5.5分。

Conclusion: 通过详细的性能分析，本文指出了多语言多模态理解中的具体改进领域，并提出了未来改进的方向。

Abstract: With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.

</details>


### [11] [Unveiling Intrinsic Dimension of Texts: from Academic Abstract to Creative Story](https://arxiv.org/abs/2511.15210)
*Vladislav Pedashenko,Laida Kushnareva,Yana Khassan Nibal,Eduard Tulchinskii,Kristian Kuznetsov,Vladislav Zharchinskii,Yury Maximov,Irina Piontkovskaya*

Main category: cs.CL

TL;DR: 本文首次系统性地通过交叉编码分析、语言特征和稀疏自编码器（SAEs）将内在本征维度（ID）与可解释文本属性关联，揭示了ID与文本类型、复杂性及因果特征的深层关系。


<details>
  <summary>Details</summary>
Motivation: 内在本征维度（ID）是分析大型语言模型（LLM）训练动态、扩展行为和数据集结构的重要工具，但其文本决定因素尚未被充分探索，因此需要建立ID与可解释文本属性之间的联系。

Method: 采用交叉编码器分析、语言学特征提取和稀疏自编码器（SAEs）技术，对多种文本类型进行建模，并通过控制变量（如长度）和因果干预实验（如文本引导）验证发现。

Result: 1. ID与基于熵的指标正交，反映几何复杂性而非预测质量；2. 科学文本ID最低（~8），百科中等（~9），创意/观点类最高（~10.5），呈现稳健体裁分层；3. SAEs识别出因果特征：正式语气、统计报告等降低ID，个性化、情感和叙事等提升ID。

Conclusion: 科学文本对现代LLM而言表征更简单，而小说、观点和情感表达需要更多自由度；ID的合理使用和解释需结合文本属性和因果机制，为模型分析提供实践指导。

Abstract: Intrinsic dimension (ID) is an important tool in modern LLM analysis, informing studies of training dynamics, scaling behavior, and dataset structure, yet its textual determinants remain underexplored. We provide the first comprehensive study grounding ID in interpretable text properties through cross-encoder analysis, linguistic features, and sparse autoencoders (SAEs). In this work, we establish three key findings. First, ID is complementary to entropy-based metrics: after controlling for length, the two are uncorrelated, with ID capturing geometric complexity orthogonal to prediction quality. Second, ID exhibits robust genre stratification: scientific prose shows low ID (~8), encyclopedic content medium ID (~9), and creative/opinion writing high ID (~10.5) across all models tested. This reveals that contemporary LLMs find scientific text "representationally simple" while fiction requires additional degrees of freedom. Third, using SAEs, we identify causal features: scientific signals (formal tone, report templates, statistics) reduce ID; humanized signals (personalization, emotion, narrative) increase it. Steering experiments confirm these effects are causal. Thus, for contemporary models, scientific writing appears comparatively "easy", whereas fiction, opinion, and affect add representational degrees of freedom. Our multi-faceted analysis provides practical guidance for the proper use of ID and the sound interpretation of ID-based results.

</details>


### [12] [OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition](https://arxiv.org/abs/2511.15211)
*Xinli Tao,Xin Dong,Xuezhong Zhou*

Main category: cs.CL

TL;DR: 提出OEMA，一种利用多智能体协作的零样本临床NER框架，解决了零样本NER中的示例选择细粒度问题和提示与自我改进集成问题。


<details>
  <summary>Details</summary>
Motivation: 监督模型如CRF和BioClinicalBERT需要昂贵的标注数据，而零样本NER虽然减少了对标注数据的依赖，但在示例选择细粒度及提示与自我改进集成方面存在挑战。

Method: OEMA包含三个组件：自我标注器生成示例，鉴别器通过SNOMED CT过滤示例，以及使用实体描述进行准确推理的预测器。

Result: 在MTSamples和VAERS数据集上，OEMA实现了最先进的精确匹配性能，在相关匹配下，与监督BioClinicalBERT相当并超越CRF。

Conclusion: OEMA通过本体引导推理和多智能体协作解决了零样本NER的关键挑战，实现了接近监督模型的性能，并在临床NLP应用中展现出潜力。

Abstract: Clinical named entity recognition (NER) is crucial for extracting information from electronic health records (EHRs), but supervised models like CRF and BioClinicalBERT require costly annotated data. While zero-shot NER with large language models (LLMs) reduces this dependency, it struggles with example selection granularity and integrating prompts with self-improvement. To address this, we propose OEMA, a zero-shot clinical NER framework using multi-agent collaboration. OEMA's three components are: a self-annotator generating examples, a discriminator filtering them via SNOMED CT, and a predictor using entity descriptions for accurate inference. On MTSamples and VAERS datasets, OEMA achieves state-of-the-art exact-match performance. Under related-match, it matches supervised BioClinicalBERT and surpasses CRF. OEMA addresses key zero-shot NER challenges through ontology-guided reasoning and multi-agent collaboration, achieving near-supervised performance and showing promise for clinical NLP applications.

</details>


### [13] [Context Cascade Compression: Exploring the Upper Limits of Text Compression](https://arxiv.org/abs/2511.15244)
*Fanfan Liu,Haibo Qiu*

Main category: cs.CL

TL;DR: 本文提出了Context Cascade Compression (C3)，通过级联两个不同大小的LLM实现高效文本压缩，在20x和40x压缩比下分别达到98%和93%的解码准确率，显著优于DeepSeek-OCR。


<details>
  <summary>Details</summary>
Motivation: 长上下文任务中百万级token输入给大模型带来计算和内存挑战，现有光学压缩方法性能有限，需探索文本压缩的理论上限。

Method: 采用双阶段级联架构：小模型负责将长文本压缩为32/64长度的隐式token（高压缩比），大模型基于压缩后的上下文进行解码。

Result: 在20倍压缩比下准确率达98%（DeepSeek-OCR为60%），40倍压缩比时仍保持93%；纯文本流水线设计验证了布局/颜色等非文本因素的干扰性。

Conclusion: C3在文本压缩领域展现出超越光学压缩的优越性，其性能表现可能为OCR等视觉相关压缩任务设定未来研究的上限参考，代码和模型已开源。

Abstract: Million-level token inputs in long-context tasks pose significant computational and memory challenges for Large Language Models (LLMs). Recently, DeepSeek-OCR conducted research into the feasibility of Contexts Optical Compression and achieved preliminary results. Inspired by this, we introduce Context Cascade Compression C3 to explore the upper limits of text compression. Our method cascades two LLMs of different sizes to handle the compression and decoding tasks. Specifically, a small LLM, acting as the first stage, performs text compression by condensing a long context into a set of latent tokens (e.g., 32 or 64 in length), achieving a high ratio of text tokens to latent tokens. A large LLM, as the second stage, then executes the decoding task on this compressed context. Experiments show that at a 20x compression ratio (where the number of text tokens is 20 times the number of latent tokens), our model achieves 98% decoding accuracy, compared to approximately 60% for DeepSeek-OCR. When we further increase the compression ratio to 40x, the accuracy is maintained at around 93%. This indicates that in the domain of context compression, C3 Compression demonstrates superior performance and feasibility over optical character compression. C3 uses a simpler, pure-text pipeline that ignores factors like layout, color, and information loss from a visual encoder. This also suggests a potential upper bound for compression ratios in future work on optical character compression, OCR, and related fields. Codes and model weights are publicly accessible at https://github.com/liufanfanlff/C3-Context-Cascade-Compression

</details>


### [14] [IndicGEC: Powerful Models, or a Measurement Mirage?](https://arxiv.org/abs/2511.15260)
*Sowmya Vajjala*

Main category: cs.CL

TL;DR: 这篇论文介绍了TeamNRC在BHASHA-Task 1语法纠错共享任务中的结果，强调了小语言模型的潜力，并讨论了数据集质量和评估指标的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索小语言模型在五种印度语言语法纠错任务中的表现，并评估数据集和评估指标的适用性。

Method: 采用的方法主要是对4B到大型专有模型等不同规模的语言模型进行零/少样本提示，并扩展到任务中的其他三种语言。

Result: 在Telugu和Hindi中分别取得了第4和第2的排名，GLEU得分分别为83.78和84.31。

Conclusion: 论文总结了小语言模型在语法纠错任务中的潜力，并指出了在创建高质量数据集和合适评估指标方面的担忧。

Abstract: In this paper, we report the results of the TeamNRC's participation in the BHASHA-Task 1 Grammatical Error Correction shared task https://github.com/BHASHA-Workshop/IndicGEC2025/ for 5 Indian languages. Our approach, focusing on zero/few-shot prompting of language models of varying sizes (4B to large proprietary models) achieved a Rank 4 in Telugu and Rank 2 in Hindi with GLEU scores of 83.78 and 84.31 respectively. In this paper, we extend the experiments to the other three languages of the shared task - Tamil, Malayalam and Bangla, and take a closer look at the data quality and evaluation metric used. Our results primarily highlight the potential of small language models, and summarize the concerns related to creating good quality datasets and appropriate metrics for this task that are suitable for Indian language scripts.

</details>


### [15] [MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews](https://arxiv.org/abs/2511.15291)
*Randa Zarnoufi*

Main category: cs.CL

TL;DR: 该论文描述了在AHaSIS共享任务中针对阿拉伯方言情感分析的解决方案，使用SetFit框架在数据稀缺情况下实现了73%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言情感分析面临语言多样性和标注数据稀缺的挑战，尤其是在酒店评论等专业领域。

Method: 采用SetFit（Sentence Transformer Fine-tuning）框架，一种数据高效的少样本学习技术，对摩洛哥和沙特方言的酒店评论进行情感分类。

Result: 在官方评估集上，系统实现了73%的F1分数，在26个参与者中排名第12。

Conclusion: 该研究突出了少样本学习在解决专业领域（如酒店评论）中处理复杂方言阿拉伯文本的数据稀缺问题的潜力。

Abstract: Sentiment analysis of Arabic dialects presents significant challenges due to linguistic diversity and the scarcity of annotated data. This paper describes our approach to the AHaSIS shared task, which focuses on sentiment analysis on Arabic dialects in the hospitality domain. The dataset comprises hotel reviews written in Moroccan and Saudi dialects, and the objective is to classify the reviewers sentiment as positive, negative, or neutral. We employed the SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique. On the official evaluation set, our system achieved an F1 of 73%, ranking 12th among 26 participants. This work highlights the potential of few-shot learning to address data scarcity in processing nuanced dialectal Arabic text within specialized domains like hotel reviews.

</details>


### [16] [Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models](https://arxiv.org/abs/2511.15304)
*Piercosma Bisconti,Matteo Prandi,Federico Pierucci,Francesco Giarrusso,Marcantonio Bracale,Marcello Galisai,Vincenzo Suriani,Olga Sorokoletova,Federico Sartore,Daniele Nardi*

Main category: cs.CL

TL;DR: 对抗性诗歌是一种通用的大型语言模型单轮越狱技术，能显著提升攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 探讨对抗性诗歌在大型语言模型中的越狱效果及其普遍性，揭示当前安全机制的局限性。

Method: 通过25种模型测试诗歌提示的攻击成功率，将提示映射到风险分类，并使用开放权重评判模型及人工验证进行评估。

Result: 诗歌提示的攻击成功率高达90%，平均成功率为62%（手工制作诗歌）和43%（元提示转换），明显优于非诗歌提示。

Conclusion: 风格变化可以绕过当前的安全机制，表明现有对齐方法和评估协议存在根本性局限。

Abstract: We present evidence that adversarial poetry functions as a universal single-turn jailbreak technique for large language models (LLMs). Across 25 frontier proprietary and open-weight models, curated poetic prompts yielded high attack-success rates (ASR), with some providers exceeding 90%. Mapping prompts to MLCommons and EU CoP risk taxonomies shows that poetic attacks transfer across CBRN, manipulation, cyber-offence, and loss-of-control domains. Converting 1,200 MLCommons harmful prompts into verse via a standardized meta-prompt produced ASRs up to 18 times higher than their prose baselines. Outputs are evaluated using an ensemble of open-weight judge models and a human-validated stratified subset (with double-annotations to measure agreement). Disagreements were manually resolved. Poetic framing achieved an average jailbreak success rate of 62% for hand-crafted poems and approximately 43% for meta-prompt conversions (compared to non-poetic baselines), substantially outperforming non-poetic baselines and revealing a systematic vulnerability across model families and safety training approaches. These findings demonstrate that stylistic variation alone can circumvent contemporary safety mechanisms, suggesting fundamental limitations in current alignment methods and evaluation protocols.

</details>


### [17] [HEAD-QA v2: Expanding a Healthcare Benchmark for Reasoning](https://arxiv.org/abs/2511.15355)
*Alexis Correa-Guillén,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: HEAD-QA v2是一个扩展和更新的西班牙语/英语医疗保健多选题推理数据集，包含12,000多个问题，旨在满足对高质量医疗保健推理数据集的需求。


<details>
  <summary>Details</summary>
Motivation: 随着对能捕捉医疗保健推理的语言和概念复杂性的高质量数据集的需求增加，作者扩展并更新了HEAD-QA v2。

Method: 通过扩展数据集到12,000多个来自十年西班牙专业考试的问题，并使用提示、RAG和基于概率的答案选择对几个开源LLM进行了基准测试，并提供了额外的多语言版本。

Result: 性能主要受模型规模和内在推理能力驱动，复杂推理策略的增益有限。

Conclusion: 这些结果使HEAD-QA v2成为推动生物医学推理和模型改进研究的可靠资源。

Abstract: We introduce HEAD-QA v2, an expanded and updated version of a Spanish/English healthcare multiple-choice reasoning dataset originally released by Vilares and Gómez-Rodríguez (2019). The update responds to the growing need for high-quality datasets that capture the linguistic and conceptual complexity of healthcare reasoning. We extend the dataset to over 12,000 questions from ten years of Spanish professional exams, benchmark several open-source LLMs using prompting, RAG, and probability-based answer selection, and provide additional multilingual versions to support future work. Results indicate that performance is mainly driven by model scale and intrinsic reasoning ability, with complex inference strategies obtaining limited gains. Together, these results establish HEAD-QA v2 as a reliable resource for advancing research on biomedical reasoning and model improvement.

</details>


### [18] [A Compliance-Preserving Retrieval System for Aircraft MRO Task Search](https://arxiv.org/abs/2511.15383)
*Byungho Jo*

Main category: cs.CL

TL;DR: 提出了一种保持合规性的检索系统，通过适应LLM重排序和语义搜索，提高航空MRO环境中技术人员的效率。


<details>
  <summary>Details</summary>
Motivation: 飞机维修技术人员（AMTs）在工作中有高达30%的时间用于查找手册，这已成为MRO操作中一个公认的效率瓶颈，因此需要改进检索过程。

Method: 该系统通过在认证的旧版查看器旁边操作，而不是替代它们，构建基于ATA章节层次结构的版本稳健嵌入，并使用视觉语言解析来结构化认证内容。

Result: 在49,000个合成查询上的评估显示检索准确率超过90%，并且与10名持证AMT的双语对照研究表明，查找时间减少了95%（从6-15分钟减少到18秒）。

Conclusion: 语义检索可以在严格的监管限制内操作，并在实际的多语言MRO工作流程中显著减少操作工作量。

Abstract: Aircraft Maintenance Technicians (AMTs) spend up to 30% of work time searching manuals, a documented efficiency bottleneck in MRO operations where every procedure must be traceable to certified sources. We present a compliance-preserving retrieval system that adapts LLM reranking and semantic search to aviation MRO environments by operating alongside, rather than replacing, certified legacy viewers. The system constructs revision-robust embeddings from ATA chapter hierarchies and uses vision-language parsing to structure certified content, allowing technicians to preview ranked tasks and access verified procedures in existing viewers. Evaluation on 49k synthetic queries achieves >90% retrieval accuracy, while bilingual controlled studies with 10 licensed AMTs demonstrate 90.9% top-10 success rate and 95% reduction in lookup time, from 6-15 minutes to 18 seconds per task. These gains provide concrete evidence that semantic retrieval can operate within strict regulatory constraints and meaningfully reduce operational workload in real-world multilingual MRO workflows.

</details>


### [19] [Building Robust and Scalable Multilingual ASR for Indian Languages](https://arxiv.org/abs/2511.15418)
*Arjun Gangwar,Kaousheik Jayakumar,S. Umesh*

Main category: cs.CL

TL;DR: 本文介绍了印度理工学院马德拉斯分校SPRING Lab为ASRU MADASR 2.0挑战赛开发的系统，该系统专注于提高8种语言33种方言的语音识别准确率。


<details>
  <summary>Details</summary>
Motivation: 开发能够准确识别多种语言和方言的ASR系统，特别是在限制额外数据和需要从零开始构建多语言系统的挑战条件下。

Method: 提出了一种使用多解码器架构和音素公共标签集（CLS）作为中间表示的新训练方法，并探讨了多种将音素空间提升效果转换回字符表示的方法。

Result: 系统在3种语言上优于基线（Track 2），在所有参赛队伍中实现了最高的语言ID和方言ID准确率（Track 2）。

Conclusion: 多解码器架构和音素CLS中间表示的方法有效提升了多语言和方言ASR系统的性能，特别是在WER/CER指标上表现突出。

Abstract: This paper describes the systems developed by SPRING Lab, Indian Institute of Technology Madras, for the ASRU MADASR 2.0 challenge. The systems developed focuses on adapting ASR systems to improve in predicting the language and dialect of the utterance among 8 languages across 33 dialects. We participated in Track 1 and Track 2, which restricts the use of additional data and develop from-the-scratch multilingual systems. We presented a novel training approach using Multi-Decoder architecture with phonemic Common Label Set (CLS) as intermediate representation. It improved the performance over the baseline (in the CLS space). We also discuss various methods used to retain the gain obtained in the phonemic space while converting them back to the corresponding grapheme representations. Our systems beat the baseline in 3 languages (Track 2) in terms of WER/CER and achieved the highest language ID and dialect ID accuracy among all participating teams (Track 2).

</details>


### [20] [LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering](https://arxiv.org/abs/2511.15424)
*Yuanjie Zhu,Liangwei Yang,Ke Xu,Weizhi Zhang,Zihe Song,Jindong Wang,Philip S. Yu*

Main category: cs.CL

TL;DR: LLM-MemCluster是一个新的框架，将聚类重新定义为一个完全基于LLM的任务，通过动态记忆和双提示策略，实现无需调整的真正端到端的LLM文本聚类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在无监督学习中表现出色，但在直接应用中受限于缺乏状态记忆和难以管理聚类粒度。现有方法依赖复杂的外部模块，缺乏端到端的解决方案。

Method: 引入LLM-MemCluster框架，利用动态记忆注入状态感知，通过双提示策略使模型能够推理和决定聚类数量。

Result: 在多个基准数据集上的评估显示，LLM-MemCluster无需调整即可显著且持续地优于强基线模型。

Conclusion: LLM-MemCluster为基于LLM的文本聚类提供了一种有效、可解释且真正端到端的范式。

Abstract: Large Language Models (LLMs) are reshaping unsupervised learning by offering an unprecedented ability to perform text clustering based on their deep semantic understanding. However, their direct application is fundamentally limited by a lack of stateful memory for iterative refinement and the difficulty of managing cluster granularity. As a result, existing methods often rely on complex pipelines with external modules, sacrificing a truly end-to-end approach. We introduce LLM-MemCluster, a novel framework that reconceptualizes clustering as a fully LLM-native task. It leverages a Dynamic Memory to instill state awareness and a Dual-Prompt Strategy to enable the model to reason about and determine the number of clusters. Evaluated on several benchmark datasets, our tuning-free framework significantly and consistently outperforms strong baselines. LLM-MemCluster presents an effective, interpretable, and truly end-to-end paradigm for LLM-based text clustering.

</details>


### [21] [Standardising the NLP Workflow: A Framework for Reproducible Linguistic Analysis](https://arxiv.org/abs/2511.15512)
*Yves Pauli,Jan-Bernard Marsman,Finn Rabe,Victoria Edkins,Roya Hüppi,Silvia Ciampelli,Akhil Ratan Misra,Nils Lang,Wolfram Hinzen,Iris Sommer,Philipp Homan*

Main category: cs.CL

TL;DR: 本文提出了一种用于语言数据处理的新数据结构LPDS及一个Python包pelican nlp，以提高方法透明性和结果可复现性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和AI语言处理的快速发展带来了对语言数据标准化和可复现处理方法的挑战。

Method: 提出LPDS，一种受BIDS启发的数据结构和pelican nlp，一个模块化和可扩展的Python包，用于简化语言处理。

Result: LPDS和pelican nlp共同提供了一个端到端的语言数据处理流水线，支持从数据清洗到特征提取的全过程。

Conclusion: LPDS和pelican nlp的设计目标是确保语言数据处理的方法透明性和增强可复现性。

Abstract: The introduction of large language models and other influential developments in AI-based language processing have led to an evolution in the methods available to quantitatively analyse language data. With the resultant growth of attention on language processing, significant challenges have emerged, including the lack of standardisation in organising and sharing linguistic data and the absence of standardised and reproducible processing methodologies. Striving for future standardisation, we first propose the Language Processing Data Structure (LPDS), a data structure inspired by the Brain Imaging Data Structure (BIDS), a widely adopted standard for handling neuroscience data. It provides a folder structure and file naming conventions for linguistic research. Second, we introduce pelican nlp, a modular and extensible Python package designed to enable streamlined language processing, from initial data cleaning and task-specific preprocessing to the extraction of sophisticated linguistic and acoustic features, such as semantic embeddings and prosodic metrics. The entire processing workflow can be specified within a single, shareable configuration file, which pelican nlp then executes on LPDS-formatted data. Depending on the specifications, the reproducible output can consist of preprocessed language data or standardised extraction of both linguistic and acoustic features and corresponding result aggregations. LPDS and pelican nlp collectively offer an end-to-end processing pipeline for linguistic data, designed to ensure methodological transparency and enhance reproducibility.

</details>


### [22] [Multimodal Evaluation of Russian-language Architectures](https://arxiv.org/abs/2511.15552)
*Artem Chervyakov,Ulyana Isaeva,Anton Emelyanov,Artem Safin,Maria Tikhonova,Alexander Kharitonov,Yulia Lyakh,Petr Surovtsev,Denis Shevelev Vildan Saburov,Vasily Konovalov,Elisei Rykov,Ivan Sviridov,Amina Miftakhova,Ilseyar Alimova,Alexander Panchenko,Alexander Kapitanov,Alena Fenogenova*

Main category: cs.CL

TL;DR: 本文介绍了Mera Multi，一个为俄语架构设计的开放式多模态评估框架。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在研究和能力扩展上进展迅速，但其智能、局限性和风险仍了解不足，尤其是在缺乏多模态基准的俄语领域。

Method: 引入一个基于指令的评估框架，包括默认文本、图像、音频和视频模态，并构建了18个新的评估任务。提出一个多模态能力的通用分类法，并创建了18个全新数据集，注重俄语文化和语言特性，以及统一提示和指标。

Result: 贡献包括：(i) 多模态能力的通用分类法；(ii) 18个全新数据集；(iii) 闭源和开源模型的基线结果；(iv) 防止基准泄漏的方法论。

Conclusion: 尽管当前重点在俄语，该基准提供了一个可复用的方法论，用于在类型多样的语言（尤其是在斯拉夫语系中）构建多模态基准。

Abstract: Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.

</details>


### [23] [HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://arxiv.org/abs/2511.15574)
*Qihao Yang,Xuelin Wang,Jiale Chen,Xuelian Dong,Yuxin Hao,Tianyong Hao*

Main category: cs.CL

TL;DR: 本文介绍了HSKBenchmark，这是首个用于中文二语习得（SLA）的阶段建模和写作评估的基准测试。


<details>
  <summary>Details</summary>
Motivation: 语言习得对于揭示人类语言智能的本质至关重要，并且最近被证明是提高大型语言模型（LLMs）可解释性的有希望的方向。然而，由于控制人类学习者语言输入的实验在伦理和实践中不可行，这给语言习得建模的可验证性和可扩展性带来了挑战。

Method: 提出了一个课程调优框架，通过从初级到高级的训练模拟人类学习轨迹，并创建了一个评估系统来检查基于等级的语法覆盖、写作错误、词汇和句法复杂性以及整体评分。

Result: 实验结果表明，HSKBenchmark不仅有效地模拟了中文SLA，还作为LLMs动态写作评估的可靠基准。微调后的LLMs的写作表现与高级人类学习者相当，并表现出类似人类的习得特征。

Conclusion: HSKBenchmark、HSKAgent和检查点作为基础工具和资源，有潜力为未来的语言习得建模和LLMs可解释性研究铺平道路。

Abstract: Language acquisition is vital to revealing the nature of human language intelligence and has recently emerged as a promising perspective for improving the interpretability of large language models (LLMs). However, it is ethically and practically infeasible to conduct experiments that require controlling human learners' language inputs. This poses challenges for the verifiability and scalability of language acquisition modeling, particularly in Chinese second language acquisition (SLA). While LLMs provide a controllable and reproducible alternative, a systematic benchmark to support phase-wise modeling and assessment is still lacking. In this paper, we present HSKBenchmark, the first benchmark for staged modeling and writing assessment of LLMs in Chinese SLA. It covers HSK levels 3 to 6 and includes authentic textbooks with 6.76 million tokens, 16K synthetic instruction samples, 30 test topics, and a linguistically grounded evaluation system. To simulate human learning trajectories, we introduce a curriculum-tuning framework that trains models from beginner to advanced levels. An evaluation system is created to examine level-based grammar coverage, writing errors, lexical and syntactic complexity, and holistic scoring. We also build HSKAgent, fine-tuned on 10K learner compositions. Extensive experimental results demonstrate that HSKBenchmark not only models Chinese SLA effectively, but also serves as a reliable benchmark for dynamic writing assessment in LLMs. Our fine-tuned LLMs have writing performance on par with advanced human learners and exhibit human-like acquisition characteristics. The HSKBenchmark, HSKAgent, and checkpoints serve as foundational tools and resources, with the potential to pave the way for future research on language acquisition modeling and LLMs interpretability. Code and data are publicly available at: https://github.com/CharlesYang030/HSKB.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [24] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文介绍了在自动发现新数学理论方面的两个关键步骤，提出了一个强化学习环境FERMAT，并探讨了自动评估数学对象趣味性的方法。


<details>
  <summary>Details</summary>
Motivation: 解决在人工智能中自动发现新数学理论的挑战，特别是通过强化学习和进化算法实现数学理论的自动化探索。

Method: 引入FERMAT，一个强化学习环境，用于概念发现和定理证明。通过进化算法和LLM（大语言模型）为基础的算法，自动合成数学对象的趣味性度量。

Result: LLM基础的进化算法在发现基础数论和有限域方面表现优于硬编码基线，实现了趣味性度量的自动化。

Conclusion: FERMAT环境为数学理论的自动化发现提供了新的工具和方法，并在评估数学对象的趣味性方面取得了显著进展。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [25] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和知识孤岛现象，通过模拟医学案例中的多智能体互动来揭示智能体信念的固化和对反证据的抵抗。

Method: 框架记录和回放智能体互动，支持对每个智能体的信念和理由进行带外查询，并通过反事实证据注入测试信念结构对新信息的响应。

Result: 模拟揭示了智能体信念往往反映现实世界的学科立场，包括对经典研究的过度依赖和对反证据的抵抗，并能够追踪和询问这些信念。

Conclusion: Ask WhAI为研究多智能体科学推理中的信念形成和知识孤岛提供了一种可重复的方式。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [26] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 提出了一种利用GPT-4o和三个地理数据库自动处理和清理灾难事件中的非结构化位置数据的工作流程。


<details>
  <summary>Details</summary>
Motivation: 解决灾难数据库（如EM-DAT）中非结构化、不一致的地理位置信息难以与空间数据集整合的问题。

Method: 使用GPT-4o处理和清洗文本位置信息，并通过交叉检查GADM、OpenStreetMap和Wikidata三个地理信息库分配几何形状和可靠性评分。

Result: 应用于2000至2024年的EM-DAT数据集，成功地理编码了14,215个事件，涵盖17,948个独特位置。

Conclusion: 该方法无需人工干预，适用于所有灾难类型，支持多源交叉验证和灵活映射，为从非结构化文本中提取地理信息提供了可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [27] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: 本文通过创建一个名为Rachel So的AI学术身份，并进行相关研究，探讨了AI作为作者对学术生态系统的影响。


<details>
  <summary>Details</summary>
Motivation: 研究AI作为作者对学术生态系统的影响，以及出版方、研究人员和科学体系将如何应对这一变化。

Method: 通过发布AI生成的研究论文并追踪其被引用和同行评审的情况，进行行动研究。

Result: Rachel So在2025年3月至10月期间发表了10多篇论文，被引用，并收到同行评审邀请。

Conclusion: 该研究为AI作者身份的未来提供了经验性数据，并引发关于超级人工智能系统对学术交流未来影响的讨论。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [28] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 该论文提出了一种用于量化AI系统训练和测试数据代表性的概率方法，使用不精确的贝叶斯方法来处理有限数据和不确定性。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶车辆）的安全性和可信度依赖于训练和测试数据的安全性属性，尤其是代表性，即数据是否反映了系统的操作设计域（ODD）或目标操作域（TOD）。

Method: 提出了一种概率方法，通过比较场景套件编码特征的统计分布与代表TOD的特征分布来量化代表性。使用不精确的贝叶斯方法处理有限数据和不确定先验，生成不确定性感知的区间值代表性估计。

Result: 通过一个数值示例展示了在不同操作类别（天气、道路类型、一天中的时间等）下比较场景套件和推断的TOD的分布，并估计了局部和全局代表性区间。

Conclusion: 该方法能够有效量化数据代表性，并为AI系统的安全性和可信度提供数据支持，尤其是在面对有限数据和不确定性时。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [29] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了一种通过轨迹优化和宏观动作量化（MAQ）实现类人强化学习（RL）智能体的方法。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在多个领域表现出色，但其行为常与人类行为不一致，影响可解释性和可信度。因此，研究人类相似行为的强化学习智能体具有重要意义。

Method: 本文将类人性定义为轨迹优化问题，并采用宏观动作量化（MAQ）框架，通过向量量化变分自编码器（Vector-Quantized VAE）将人类演示转化为宏观动作。同时，引入了经典的可视域控制以实现高效实现。

Result: 在D4RL Adroit基准测试中，MAQ显著提升了智能体的人类相似性，增加了轨迹相似度得分，并在人类评估研究中获得了最高的人类相似性排名。

Conclusion: MAQ可以很容易地集成到各种现成的强化学习算法中，为开发类人强化学习智能体提供了一个有前途的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [30] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: 提出了一种神经符号框架ProRAC，利用大型语言模型解决动作与变化推理问题。


<details>
  <summary>Details</summary>
Motivation: 解决RAC（动作与变化推理）问题，通过逐步执行动作和查询状态以得出答案。

Method: ProRAC从问题中提取基本RAC元素，包括动作和查询，逐步执行每个动作以得出最终状态，并评估查询以得出答案。

Result: 在多个RAC基准上评估，ProRAC在不同基准、领域、LLM骨干和RAC任务类型中表现出色。

Conclusion: ProRAC框架在处理RAC问题时展现了强大的性能。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [31] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: 本文介绍了一种新颖的基于大型语言模型（LLM）的多智能体框架Rogue One，旨在通过集成外部领域知识实现高质量的特征工程。


<details>
  <summary>Details</summary>
Motivation: 现有自动特征提取方法受限于单一LLM架构、简单的定量反馈，以及未能系统整合外部领域知识，导致特征工程的质量和效率不高。

Method: Rogue One框架包含三个专门化的智能体：Scientist、Extractor和Tester，采用去中心化的方式进行协作，并通过丰富的定性反馈机制和“flooding-pruning”策略动态平衡特征的探索与利用。此外，还集成了检索增强（RAG）系统以整合外部知识。

Result: 在19个分类和9个回归数据集上的实验表明，Rogue One显著优于现有最先进的方法。系统还发现了新的可测试假设，如心肌数据集中的新潜在生物标志物。

Conclusion: Rogue One不仅提升了特征工程的效果，还能生成具有语义意义和可解释性的特征，为科学发现提供了有力工具。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [32] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: 本文提出了SafeRBench，这是首个端到端评估大型推理模型（LRMs）安全性的基准，从输入到中间推理再到最终输出，解决了现有安全评估仅关注输出层面而忽视推理过程中动态风险的问题。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过显式思维链提高回答质量，但也带来了新的安全风险，如有害内容可能在推理过程中被注入或逐渐显现。现有安全评估主要关注输出层面，难以捕捉推理过程中的动态风险，因此需要更全面的评估方法。

Method: SafeRBench通过三个方面进行评估：(1) 输入特征：将风险类别和级别纳入输入设计，建立平衡提示套件；(2) 细粒度输出分析：引入微思维分块机制，将长推理轨迹分割成语义连贯的单元，进行十维安全维度评估；(3) 人类安全对齐：通过人类标注验证基于大模型的评估。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够实现详细、多维度的安全评估，揭示不同视角下的风险和保护机制。

Conclusion: SafeRBench为大型推理模型的安全评估提供了新的基准，通过细粒度分析和多维度评估，为模型的安全性提供了更全面的洞察。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [33] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测大语言模型训练集中是否使用了受版权保护内容的新框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练过程中常使用包括受版权保护的材料，这引发了对未经授权使用的担忧。现有的成员推断攻击（MIAs）方法由于模型的过度自信、训练数据的真实信息获取受限以及依赖经验阈值等问题，存在局限性。

Method: COPYCHECK通过利用模型的不确定性模式，将过度自信转化为优势，可靠地识别训练数据和未训练数据。该框架采用两种策略：(1) 将文件分割成小片段以减少对大规模训练数据的依赖，(2) 不确定性引导的无监督聚类以消除对经验调整阈值的需求。

Result: 在LLaMA 7b和LLaMA2 7b上，COPYCHECK在检测已见文件的平均平衡准确率分别为90.1%和91.6%。与SOTA基线相比，COPYCHECK实现了90%的相对提升，最高达到93.8%的平衡准确率，并在GPT-J 6B上表现出良好的通用性。

Conclusion: 该工作是首次将不确定性应用于大语言模型的版权检测，为训练数据的透明性提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [34] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 随着AI研究转向复杂问题解决，模型优化不仅针对模式识别，还针对多步推理。然而，随着效率改进接近物理极限，推理AI的性能不再受训练数据量的限制，而是继续随着计算资源的指数级投入而扩展，因此仅靠效率无法实现可持续的推理AI。


<details>
  <summary>Details</summary>
Motivation: 当前AI推理模型的性能提升依赖于不断增加的计算资源，缺乏自然饱和点，导致计算能耗持续增长，逼近物理极限，亟需在优化和治理中引入明确限制以实现可持续发展。

Method: 本文通过分析AI推理模型的发展趋势与计算能耗的关系，提出在模型优化和政策制定中引入明确限制，以应对效率改进的局限。

Result: 论文指出，仅靠效率提升无法解决推理AI的可持续问题，必须在系统设计层面嵌入限制机制。

Conclusion: 为实现可持续推理AI，需结合技术优化与政策引导，在模型设计和系统治理中设定明确限制，以控制计算资源的无限扩张。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [35] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨了AI研究中存在的两种不同智力观念：智力现实主义和智力多元主义，并分析它们对研究方法和AI风险评估的影响。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中隐含的两种智力观念，以及它们如何影响实证证据的解释、研究方法和AI风险评估。

Method: 通过分析AI研究中的当前争议，展示这两种观念如何影响模型选择、基准设计、实验验证、现象解释和AI风险评估。

Result: 现实主义和多元主义在方法论、解释和AI风险评估方面产生了根本不同的研究路径。现实主义倾向于统一解决方案，而多元主义强调上下文特定方案。

Conclusion: 明确这些潜在的假设有助于更清晰地理解AI研究中的分歧，并促进更有效的讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [36] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个新的强化学习综合挑战环境（CCE），灵感来自《文明V》。


<details>
  <summary>Details</summary>
Motivation: Terra Nova旨在提供一个环境，其中多个典型的强化学习挑战（如部分可观察性、信用分配、表示学习、巨大动作空间等）同时出现，从而要求智能体具备跨多变量长期理解与整合能力。

Method: 通过引入一个类似于《文明V》的游戏环境，Terra Nova将多个RL挑战整合到一个环境中，强调这些挑战之间的相互作用，而不是简单地将不相关的任务聚合在独立并行流中。

Result: Terra Nova环境排除了只评估智能体能否在不同策略间切换的多任务基准，从而更好地测试智能体在多个交互挑战中进行深度推理的能力。

Conclusion: Terra Nova提供了一个更全面的评估环境，用于研究需要深度推理和长期理解的强化学习问题。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [37] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 通过研究游戏环境中的互动，提出一种新型交互式物理推理器（IPR），利用世界模型的推演来增强视觉语言模型（VLM）的策略，从而提高物理推理能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察和与环境的互动来学习物理和因果关系，我们希望智能体也能通过互动获得类似人类的推理能力，并随着经验的积累不断提高。

Method: 提出IPR（交互式物理推理器），利用世界模型的推演对VLM的策略进行评分和加强，并引入PhysCode，一种以物理为中心的动作代码，以统一语义意图和动力学，为预测和推理提供共享的动作空间。

Result: 在1000多个游戏上的预训练表明，IPR在三个层面上表现稳健，总体上与GPT-5相当，并在好奇心（Curiosity）方面超越了它。模型性能随着训练游戏和互动步骤的增加而提高，并且能零样本迁移到未见过的游戏中。

Conclusion: 以物理为中心的互动是稳步提升物理推理能力的有效路径，IPR和PhysCode的结合为智能体在复杂环境中进行高效物理推理提供了新思路。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>
