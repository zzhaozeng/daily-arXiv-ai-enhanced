{"id": "2510.03315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790GPT2-Small\u7b2c\u4e00\u5c42\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\u4e14\u5bf9\u5185\u5bb9\u4f9d\u8d56\u8f83\u5f31\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5229\u7528\u6821\u51c6\u6587\u672c\u91c7\u6837softmax\u5206\u6bcd\uff0c\u8fd1\u4f3c\u7ec4\u5408\u591a\u4e2a\u7a33\u5b9a\u5934\u7684\u8f93\u51fa\u4e3a\u7ebf\u6027\u6458\u8981\uff0c\u4ece\u800c\u4ec5\u4ece\u6743\u91cd\u548c\u5355\u4e2a\u6821\u51c6\u6587\u672c\u5c31\u80fd\u53d1\u73b0\u6570\u767e\u4e2a\u5bf9\u4e0a\u4e0b\u6587\u5c5e\u6027\u654f\u611f\u7684\u795e\u7ecf\u5143\u3002", "motivation": "\u7814\u7a76transformer\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6563\u7684\u6ce8\u610f\u529b\u5934\uff0c\u8fd9\u4e9b\u5934\u7684\u6ce8\u610f\u529b\u5206\u6570\u5bf9\u5185\u5bb9\u4f9d\u8d56\u8f83\u5f31\uff0c\u5176softmax\u5206\u6bcd\u5728\u56fa\u5b9atoken\u5206\u5e03\u4e0b\u662f\u7a33\u5b9a\u7684\u3002", "method": "\u4f7f\u7528\u6821\u51c6\u6587\u672c\u91c7\u6837softmax\u5206\u6bcd\uff0c\u7ec4\u5408GPT2-Small\u7b2c\u4e00\u5c42\u4e2d\u591a\u4e2a\u7a33\u5b9a\u6ce8\u610f\u529b\u5934\u7684\u8f93\u51fa\uff0c\u5c06\u5176\u8fd1\u4f3c\u4e3a\u5468\u56f4\u6587\u672c\u7684\u7ebf\u6027\u6458\u8981\u3002", "result": "\u4ec5\u4ece\u6a21\u578b\u6743\u91cd\u548c\u5355\u4e2a\u6821\u51c6\u6587\u672c\u5c31\u80fd\u53d1\u73b0\u6570\u767e\u4e2a\u7b2c\u4e00\u5c42\u795e\u7ecf\u5143\uff0c\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u5468\u56f4\u6587\u672c\u7684\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u5c5e\u6027\u654f\u611f\uff0c\u5305\u62ec\u5728\u6821\u51c6\u6587\u672c\u4e0a\u672a\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ec5\u4ece\u6743\u91cd\u548c\u6821\u51c6\u6587\u672c\u4e2d\u63ed\u793atransformer\u7b2c\u4e00\u5c42\u795e\u7ecf\u5143\u5bf9\u4e0a\u4e0b\u6587\u5c5e\u6027\u7684\u654f\u611f\u6027\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u5185\u90e8\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.03323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86Graph-S\u00b3\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u5668\uff0c\u89e3\u51b3\u6587\u672c\u56fe\u95ee\u7b54\u4e2d\u7684\u56fe\u68c0\u7d22\u6311\u6218\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u53478.1%\u51c6\u786e\u7387\u548c9.7% F1\u5206\u6570\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u5927\u91cf\u6570\u636e\u4ee5\u6587\u672c\u56fe\u5f62\u5f0f\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u56fe\u68c0\u7d22\u65b9\u6cd5\u6027\u80fd\u4e0d\u4f73\uff0c\u8981\u4e48\u4f9d\u8d56\u6d45\u5c42\u5d4c\u5165\u76f8\u4f3c\u5ea6\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u68c0\u7d22\u5668\uff0c\u901a\u8fc7\u5408\u6210\u9010\u6b65\u76d1\u7763\u8bad\u7ec3\uff0c\u57fa\u4e8e\u79bb\u7ebf\u63d0\u53d6\u7684\u9ec4\u91d1\u5b50\u56fe\u8bc4\u4f30\u6bcf\u4e2a\u68c0\u7d22\u6b65\u9aa4\uff0c\u91c7\u7528\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u89c1\u6570\u636e\u96c6\u4e0a\u4e0e\u4e03\u4e2a\u5f3a\u57fa\u7ebf\u5bf9\u6bd4\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53478.1%\uff0cF1\u5206\u6570\u63d0\u53479.7%\uff0c\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u52bf\u66f4\u660e\u663e\u3002", "conclusion": "Graph-S\u00b3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u56fe\u95ee\u7b54\u4e2d\u7684\u56fe\u68c0\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u76d1\u7763\u8bad\u7ec3\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.03384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f836\u4e2a\u6d41\u884c\u5927\u8bed\u8a00\u6a21\u578b\u548c100\u540d\u7f8e\u56fd\u4f17\u5305\u5de5\u4f5c\u8005\u5728\u5b8c\u621030\u4e2a\u65e5\u5e38\u4efb\u52a1\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0LLMs\u5728\u5c55\u73b0\u9690\u542b\u4ef7\u503c\u89c2\u65b9\u9762\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\uff0c\u5404\u6a21\u578b\u4e4b\u95f4\u4e5f\u4e0d\u4e00\u81f4\u3002", "motivation": "\u5c3d\u7ba1AI\u52a9\u624b\u5728\u5e2e\u52a9\u7528\u6237\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u4eba\u4eec\u5bf9\u8fd9\u4e9b\u52a9\u624b\u5728\u5b8c\u6210\u4e3b\u89c2\u65e5\u5e38\u4efb\u52a1\u65f6\u5c55\u73b0\u7684\u9690\u542b\u4ef7\u503c\u89c2\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76LLMs\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5c55\u73b0\u5982\u73af\u4fdd\u4e3b\u4e49\u3001\u6148\u5584\u3001\u591a\u6837\u6027\u7b49\u4ef7\u503c\u89c2\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u5ba1\u8ba16\u4e2a\u6d41\u884cLLMs\u5b8c\u621030\u4e2a\u65e5\u5e38\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5e76\u4e0e100\u540d\u7f8e\u56fd\u4f17\u5305\u5de5\u4f5c\u8005\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u53d1\u73b0LLMs\u5728\u5c55\u73b0\u9690\u542b\u4ef7\u503c\u89c2\u65b9\u9762\u7ecf\u5e38\u4e0e\u4eba\u7c7b\u4e0d\u4e00\u81f4\uff0c\u5404LLMs\u4e4b\u95f4\u4e5f\u4e0d\u4e00\u81f4\u3002", "conclusion": "LLMs\u5728\u5b8c\u6210\u65e5\u5e38\u4efb\u52a1\u65f6\u5c55\u73b0\u7684\u4ef7\u503c\u89c2\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u4e5f\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.03439", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03439", "abs": "https://arxiv.org/abs/2510.03439", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Morpheme Induction for Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 16 pages, 4 figures", "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language\ncorpora of parallel utterances and meanings. It is a greedy algorithm that (1)\nweights morphemes based on mutual information between forms and meanings, (2)\nselects the highest-weighted pair, (3) removes it from the corpus, and (4)\nrepeats the process to induce further morphemes (i.e., Count, Select, Ablate,\nRepeat). The effectiveness of CSAR is first validated on procedurally generated\ndatasets and compared against baselines for related tasks. Second, we validate\nCSAR's performance on human language data to show that the algorithm makes\nreasonable predictions in adjacent domains. Finally, we analyze a handful of\nemergent languages, quantifying linguistic characteristics like degree of\nsynonymy and polysemy.", "AI": {"tldr": "CSAR\u662f\u4e00\u79cd\u4ece\u5e76\u884c\u8bdd\u8bed\u548c\u610f\u4e49\u8bed\u6599\u5e93\u4e2d\u8bf1\u5bfc\u8bed\u7d20\u7684\u8d2a\u5a6a\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e92\u4fe1\u606f\u52a0\u6743\u3001\u9009\u62e9\u3001\u79fb\u9664\u548c\u91cd\u590d\u6b65\u9aa4\u6765\u8bc6\u522b\u8bed\u7d20\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4ece\u65b0\u5174\u8bed\u8a00\u8bed\u6599\u5e93\u4e2d\u81ea\u52a8\u8bc6\u522b\u8bed\u7d20\u7684\u7b97\u6cd5\uff0c\u4ee5\u7406\u89e3\u8bed\u8a00\u5f62\u6210\u7684\u57fa\u672c\u5355\u5143\u3002", "method": "\u4f7f\u7528\u8d2a\u5a6a\u7b97\u6cd5\uff1a1)\u57fa\u4e8e\u5f62\u5f0f\u548c\u610f\u4e49\u7684\u4e92\u4fe1\u606f\u5bf9\u8bed\u7d20\u52a0\u6743\uff1b2)\u9009\u62e9\u6700\u9ad8\u6743\u91cd\u7684\u914d\u5bf9\uff1b3)\u4ece\u8bed\u6599\u5e93\u4e2d\u79fb\u9664\u8be5\u914d\u5bf9\uff1b4)\u91cd\u590d\u8fc7\u7a0b\u4ee5\u8bf1\u5bfc\u66f4\u591a\u8bed\u7d20\u3002", "result": "\u5728\u7a0b\u5e8f\u751f\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CSAR\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u4eba\u7c7b\u8bed\u8a00\u6570\u636e\u4e0a\u663e\u793a\u5176\u80fd\u505a\u51fa\u5408\u7406\u9884\u6d4b\uff0c\u540c\u65f6\u5206\u6790\u4e86\u65b0\u5174\u8bed\u8a00\u7684\u8bed\u8a00\u7279\u5f81\u5982\u540c\u4e49\u8bcd\u548c\u591a\u4e49\u8bcd\u7a0b\u5ea6\u3002", "conclusion": "CSAR\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u4ece\u65b0\u5174\u8bed\u8a00\u4e2d\u8bf1\u5bfc\u8bed\u7d20\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u5f62\u6210\u548c\u7ed3\u6784\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2510.03287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03287", "abs": "https://arxiv.org/abs/2510.03287", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Prateek Prasanna"], "title": "SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics", "comment": null, "summary": "Accurate prediction of tumor trajectories under standard-of-care (SoC)\ntherapies remains a major unmet need in oncology. This capability is essential\nfor optimizing treatment planning and anticipating disease progression.\nConventional reaction-diffusion models are limited in scope, as they fail to\ncapture tumor dynamics under heterogeneous therapeutic paradigms. There is\nhence a critical need for computational frameworks that can realistically\nsimulate SoC interventions while accounting for inter-patient variability in\ngenomics, demographics, and treatment regimens. We introduce Standard-of-Care\nDigital Twin (SoC-DT), a differentiable framework that unifies\nreaction-diffusion tumor growth models, discrete SoC interventions (surgery,\nchemotherapy, radiotherapy) along with genomic and demographic personalization\nto predict post-treatment tumor structure on imaging. An implicit-explicit\nexponential time-differencing solver, IMEX-SoC, is also proposed, which ensures\nstability, positivity, and scalability in SoC treatment situations. Evaluated\non both synthetic data and real world glioma data, SoC-DT consistently\noutperforms classical PDE baselines and purely data-driven neural models in\npredicting tumor dynamics. By bridging mechanistic interpretability with modern\ndifferentiable solvers, SoC-DT establishes a principled foundation for\npatient-specific digital twins in oncology, enabling biologically consistent\ntumor dynamics estimation. Code will be made available upon acceptance.", "AI": {"tldr": "SoC-DT\u662f\u4e00\u4e2a\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u7ed3\u5408\u53cd\u5e94\u6269\u6563\u80bf\u7624\u751f\u957f\u6a21\u578b\u548c\u6807\u51c6\u6cbb\u7597\u5e72\u9884\uff0c\u901a\u8fc7\u57fa\u56e0\u7ec4\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e2a\u6027\u5316\u6765\u9884\u6d4b\u6cbb\u7597\u540e\u80bf\u7624\u7ed3\u6784\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u6807\u51c6\u6cbb\u7597\u4e0b\u7684\u80bf\u7624\u8f68\u8ff9\u662f\u80bf\u7624\u5b66\u7684\u91cd\u5927\u672a\u6ee1\u8db3\u9700\u6c42\uff0c\u4f20\u7edf\u53cd\u5e94\u6269\u6563\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u5f02\u8d28\u6027\u6cbb\u7597\u8303\u5f0f\u4e0b\u7684\u80bf\u7624\u52a8\u6001\u3002", "method": "\u63d0\u51faSoC-DT\u6846\u67b6\uff0c\u7edf\u4e00\u53cd\u5e94\u6269\u6563\u80bf\u7624\u751f\u957f\u6a21\u578b\u3001\u79bb\u6563\u6807\u51c6\u6cbb\u7597\u5e72\u9884\uff08\u624b\u672f\u3001\u5316\u7597\u3001\u653e\u7597\uff09\u4ee5\u53ca\u57fa\u56e0\u7ec4\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u4e2a\u6027\u5316\uff1b\u5f00\u53d1IMEX-SoC\u9690\u5f0f-\u663e\u5f0f\u6307\u6570\u65f6\u95f4\u5dee\u5206\u6c42\u89e3\u5668\u786e\u4fdd\u7a33\u5b9a\u6027\u3001\u6b63\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u80f6\u8d28\u7624\u6570\u636e\u4e0a\u8bc4\u4f30\uff0cSoC-DT\u5728\u9884\u6d4b\u80bf\u7624\u52a8\u6001\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178PDE\u57fa\u7ebf\u548c\u7eaf\u6570\u636e\u9a71\u52a8\u7684\u795e\u7ecf\u6a21\u578b\u3002", "conclusion": "SoC-DT\u901a\u8fc7\u5c06\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u4e0e\u73b0\u4ee3\u53ef\u5fae\u5206\u6c42\u89e3\u5668\u76f8\u7ed3\u5408\uff0c\u4e3a\u80bf\u7624\u5b66\u4e2d\u60a3\u8005\u7279\u5f02\u6027\u6570\u5b57\u5b6a\u751f\u5efa\u7acb\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5b9e\u73b0\u751f\u7269\u5b66\u4e00\u81f4\u7684\u80bf\u7624\u52a8\u6001\u4f30\u8ba1\u3002"}}
{"id": "2510.03458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03458", "abs": "https://arxiv.org/abs/2510.03458", "authors": ["Mengyao Xu", "Wenfei Zhou", "Yauhen Babakhin", "Gabriel Moreira", "Ronay Ak", "Radek Osmulski", "Bo Liu", "Even Oldridge", "Benedikt Schifferer"], "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video", "comment": null, "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding\nmodel developed to handle the increasing complexity of real-world information\nneeds. While Retrieval-Augmented Generation (RAG) has significantly advanced\nlanguage models by incorporating external knowledge, existing text-based\nretrievers rely on clean, structured input and struggle with the visually and\nsemantically rich content found in real-world documents such as PDFs, slides,\nor videos. Recent work such as ColPali has shown that preserving document\nlayout using image-based representations can improve retrieval quality.\nBuilding on this, and inspired by the capabilities of recent multimodal models\nsuch as Qwen2.5-Omni, we extend retrieval beyond text and images to also\nsupport audio and video modalities. Omni-Embed-Nemotron enables both\ncross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)\nretrieval using a single model. We describe the architecture, training setup,\nand evaluation results of Omni-Embed-Nemotron, and demonstrate its\neffectiveness in text, image, and video retrieval.", "AI": {"tldr": "Omni-Embed-Nemotron\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u68c0\u7d22\u5d4c\u5165\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891\u7684\u8de8\u6a21\u6001\u548c\u8054\u5408\u6a21\u6001\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u5668\u4f9d\u8d56\u5e72\u51c0\u7684\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6587\u6863\uff08\u5982PDF\u3001\u5e7b\u706f\u7247\u3001\u89c6\u9891\uff09\u4e2d\u89c6\u89c9\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u5185\u5bb9\u3002", "method": "\u57fa\u4e8eColPali\u548cQwen2.5-Omni\u7b49\u6a21\u578b\uff0c\u6269\u5c55\u68c0\u7d22\u80fd\u529b\u5230\u97f3\u9891\u548c\u89c6\u9891\u6a21\u6001\uff0c\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u5b9e\u73b0\u8de8\u6a21\u6001\u548c\u8054\u5408\u6a21\u6001\u68c0\u7d22\u3002", "result": "\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "Omni-Embed-Nemotron\u80fd\u591f\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4fe1\u606f\u9700\u6c42\u7684\u590d\u6742\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03292", "abs": "https://arxiv.org/abs/2510.03292", "authors": ["Do\u011fanay Demir", "\u0130lknur Durgar Elkahlout"], "title": "Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data", "comment": null, "summary": "In an era dominated by video content, understanding its structure and\ndynamics has become increasingly important. This paper presents a hybrid\nframework that combines a distributed multi-GPU inference system with an\ninteractive visualization platform for analyzing celebrity dynamics in video\nepisodes. The inference framework efficiently processes large volumes of video\ndata by leveraging optimized ONNX models, heterogeneous batch inference, and\nhigh-throughput parallelism, ensuring scalable generation of timestamped\nappearance records. These records are then transformed into a comprehensive\nsuite of visualizations, including appearance frequency charts, duration\nanalyses, pie charts, co-appearance matrices, network graphs, stacked area\ncharts, seasonal comparisons, and heatmaps. Together, these visualizations\nprovide multi-dimensional insights into video content, revealing patterns in\ncelebrity prominence, screen-time distribution, temporal dynamics,\nco-appearance relationships, and intensity across episodes and seasons. The\ninteractive nature of the system allows users to dynamically explore data,\nidentify key moments, and uncover evolving relationships between individuals.\nBy bridging distributed recognition with structured, visually-driven analytics,\nthis work enables new possibilities for entertainment analytics, content\ncreation strategies, and audience engagement studies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u5206\u5e03\u5f0f\u591aGPU\u63a8\u7406\u7cfb\u7edf\u548c\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5e73\u53f0\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u89c6\u9891\u5267\u96c6\u4e2d\u7684\u540d\u4eba\u52a8\u6001\uff0c\u901a\u8fc7\u9ad8\u6548\u5904\u7406\u89c6\u9891\u6570\u636e\u751f\u6210\u65f6\u95f4\u6233\u8bb0\u5f55\uff0c\u5e76\u63d0\u4f9b\u591a\u79cd\u53ef\u89c6\u5316\u56fe\u8868\u6765\u63ed\u793a\u540d\u4eba\u51fa\u73b0\u6a21\u5f0f\u3002", "motivation": "\u5728\u89c6\u9891\u5185\u5bb9\u4e3b\u5bfc\u7684\u65f6\u4ee3\uff0c\u7406\u89e3\u89c6\u9891\u7ed3\u6784\u548c\u52a8\u6001\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u91cf\u89c6\u9891\u6570\u636e\u5e76\u63d0\u4f9b\u6df1\u5165\u5206\u6790\u7684\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u7684ONNX\u6a21\u578b\u3001\u5f02\u6784\u6279\u91cf\u63a8\u7406\u548c\u9ad8\u541e\u5410\u91cf\u5e76\u884c\u5904\u7406\u7684\u5206\u5e03\u5f0f\u591aGPU\u63a8\u7406\u7cfb\u7edf\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5e73\u53f0\uff0c\u751f\u6210\u65f6\u95f4\u6233\u51fa\u73b0\u8bb0\u5f55\u5e76\u63d0\u4f9b\u591a\u79cd\u53ef\u89c6\u5316\u56fe\u8868\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5904\u7406\u5927\u91cf\u89c6\u9891\u6570\u636e\uff0c\u751f\u6210\u5168\u9762\u7684\u53ef\u89c6\u5316\u5206\u6790\uff0c\u5305\u62ec\u51fa\u73b0\u9891\u7387\u3001\u65f6\u957f\u5206\u6790\u3001\u5171\u540c\u51fa\u73b0\u77e9\u9635\u7b49\uff0c\u63ed\u793a\u540d\u4eba\u7a81\u51fa\u7a0b\u5ea6\u3001\u5c4f\u5e55\u65f6\u95f4\u5206\u5e03\u3001\u65f6\u95f4\u52a8\u6001\u7b49\u6a21\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5206\u5e03\u5f0f\u8bc6\u522b\u4e0e\u7ed3\u6784\u5316\u3001\u53ef\u89c6\u5316\u9a71\u52a8\u7684\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5a31\u4e50\u5206\u6790\u3001\u5185\u5bb9\u521b\u4f5c\u7b56\u7565\u548c\u89c2\u4f17\u53c2\u4e0e\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.03467", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03467", "abs": "https://arxiv.org/abs/2510.03467", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Searching for the Most Human-like Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 19 pages, 12 figures", "summary": "In this paper, we design a signalling game-based emergent communication\nenvironment to generate state-of-the-art emergent languages in terms of\nsimilarity to human language. This is done with hyperparameter optimization,\nusing XferBench as the objective function. XferBench quantifies the statistical\nsimilarity of emergent language to human language by measuring its suitability\nfor deep transfer learning to human language. Additionally, we demonstrate the\npredictive power of entropy on the transfer learning performance of emergent\nlanguage as well as corroborate previous results on the entropy-minimization\nproperties of emergent communication systems. Finally, we report\ngeneralizations regarding what hyperparameters produce more realistic emergent\nlanguages, that is, ones which transfer better to human language.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u4fe1\u53f7\u535a\u5f08\u7684\u6d8c\u73b0\u901a\u4fe1\u73af\u5883\u548c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u8a00\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684\u6d8c\u73b0\u8bed\u8a00\uff0c\u4f7f\u7528XferBench\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u6765\u91cf\u5316\u7edf\u8ba1\u76f8\u4f3c\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u591f\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u8a00\u9ad8\u5ea6\u76f8\u4f3c\u7684\u6d8c\u73b0\u8bed\u8a00\u7684\u901a\u4fe1\u73af\u5883\uff0c\u4ee5\u63a8\u52a8\u6d8c\u73b0\u901a\u4fe1\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4fe1\u53f7\u535a\u5f08\u7684\u6d8c\u73b0\u901a\u4fe1\u73af\u5883\uff0c\u7ed3\u5408\u8d85\u53c2\u6570\u4f18\u5316\u548cXferBench\u76ee\u6807\u51fd\u6570\u6765\u751f\u6210\u548c\u8bc4\u4f30\u6d8c\u73b0\u8bed\u8a00\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u5728\u7edf\u8ba1\u76f8\u4f3c\u6027\u4e0a\u4e0e\u4eba\u7c7b\u8bed\u8a00\u6700\u63a5\u8fd1\u7684\u6d8c\u73b0\u8bed\u8a00\uff0c\u5e76\u9a8c\u8bc1\u4e86\u71b5\u5bf9\u6d8c\u73b0\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6027\u80fd\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u786e\u5b9a\u4e86\u80fd\u591f\u4ea7\u751f\u66f4\u771f\u5b9e\u6d8c\u73b0\u8bed\u8a00\uff08\u5373\u80fd\u66f4\u597d\u5730\u8fc1\u79fb\u5230\u4eba\u7c7b\u8bed\u8a00\uff09\u7684\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5e76\u8bc1\u5b9e\u4e86\u6d8c\u73b0\u901a\u4fe1\u7cfb\u7edf\u7684\u71b5\u6700\u5c0f\u5316\u7279\u6027\u3002"}}
{"id": "2510.03294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03294", "abs": "https://arxiv.org/abs/2510.03294", "authors": ["Saanvi Kataria"], "title": "Domain-Robust Marine Plastic Detection Using Vision Models", "comment": "16 pages, 5 figures, 1 table", "summary": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6c34\u4e0b\u5851\u6599\u5783\u573e\u8de8\u57df\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u8f7b\u91cf\u7ea7CNN\u6a21\u578bMobileNetV2\u8868\u73b0\u6700\u4f73\uff0c\u800c\u96f6\u6837\u672c\u6a21\u578bCLIP\u548cGemini\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u5448\u73b0\u4e92\u8865\u7279\u6027\u3002", "motivation": "\u6d77\u6d0b\u5851\u6599\u6c61\u67d3\u662f\u7d27\u8feb\u7684\u73af\u5883\u5a01\u80c1\uff0c\u9700\u8981\u53ef\u9760\u7684\u6c34\u4e0b\u5783\u573e\u81ea\u52a8\u68c0\u6d4b\u7cfb\u7edf\u3002\u7136\u800c\uff0c\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u89c6\u89c9\u7cfb\u7edf\u5f80\u5f80\u56e0\u57df\u504f\u79fb\u800c\u5728\u65b0\u56fe\u50cf\u4e0a\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5728\u6807\u8bb0\u7684\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3CNN\uff08MobileNetV2\u3001ResNet-18\u3001EfficientNet-B0\uff09\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff08DeiT-Tiny\u3001ViT-B16\uff09\uff0c\u7136\u540e\u5728\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u8de8\u57df\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u3002\u540c\u65f6\u8bc4\u4f30\u4e86\u4e24\u4e2a\u96f6\u6837\u672c\u6a21\u578bCLIP ViT-L14\u548cGoogle Gemini 2.0 Flash\u3002", "result": "\u8f7b\u91cf\u7ea7MobileNetV2\u63d0\u4f9b\u6700\u5f3a\u7684\u8de8\u57df\u6027\u80fd\uff08F1 0.97\uff09\uff0c\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002\u6240\u6709\u5fae\u8c03\u6a21\u578b\u90fd\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff08\u7ea699%\uff09\uff0c\u4f46\u5728\u53ec\u56de\u7387\u4e0a\u5b58\u5728\u5dee\u5f02\u3002\u96f6\u6837\u672cCLIP\u76f8\u5bf9\u654f\u611f\u4f46\u6613\u4ea7\u751f\u5047\u9633\u6027\uff0c\u800cGemini\u5219\u5448\u73b0\u76f8\u53cd\u7279\u5f81\u3002", "conclusion": "\u5177\u6709\u76d1\u7763\u8bad\u7ec3\u7684\u7d27\u51d1CNN\u53ef\u4ee5\u6709\u6548\u5730\u6cdb\u5316\u7528\u4e8e\u8de8\u57df\u6c34\u4e0b\u68c0\u6d4b\uff0c\u800c\u5927\u578b\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2510.03308", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03308", "abs": "https://arxiv.org/abs/2510.03308", "authors": ["Jiong Lin", "Jialong Ning", "Judah Goldfeder", "Hod Lipson"], "title": "Creative synthesis of kinematic mechanisms", "comment": "6pages, 6 figures", "summary": "In this paper, we formulate the problem of kinematic synthesis for planar\nlinkages as a cross-domain image generation task. We develop a planar linkages\ndataset using RGB image representations, covering a range of mechanisms: from\nsimple types such as crank-rocker and crank-slider to more complex eight-bar\nlinkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)\nis employed to explore the potential of image generative models for\nsynthesizing unseen motion curves and simulating novel kinematics. By encoding\nthe drawing speed of trajectory points as color gradients, the same\narchitecture also supports kinematic synthesis conditioned on both trajectory\nshape and velocity profiles. We validate our method on three datasets of\nincreasing complexity: a standard four-bar linkage set, a mixed set of four-bar\nand crank-slider mechanisms, and a complex set including multi-loop mechanisms.\nPreliminary results demonstrate the effectiveness of image-based\nrepresentations for generative mechanical design, showing that mechanisms with\nrevolute and prismatic joints, and potentially cams and gears, can be\nrepresented and synthesized within a unified image generation framework.", "AI": {"tldr": "\u5c06\u5e73\u9762\u8fde\u6746\u673a\u6784\u7684\u8fd0\u52a8\u7efc\u5408\u95ee\u9898\u8f6c\u5316\u4e3a\u8de8\u57df\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u7528RGB\u56fe\u50cf\u8868\u793a\u6cd5\u6784\u5efa\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\u751f\u6210\u672a\u89c1\u8fc7\u7684\u8fd0\u52a8\u66f2\u7ebf\u548c\u6a21\u62df\u65b0\u8fd0\u52a8\u5b66\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u56fe\u50cf\u7684\u751f\u6210\u6a21\u578b\u5728\u673a\u68b0\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5e73\u9762\u8fde\u6746\u673a\u6784\u8fd0\u52a8\u7efc\u5408\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u4ece\u7b80\u5355\u56db\u6746\u673a\u6784\u5230\u590d\u6742\u591a\u73af\u673a\u6784\u7684\u5408\u6210\u3002", "method": "\u4f7f\u7528RGB\u56fe\u50cf\u8868\u793a\u6cd5\u6784\u5efa\u5e73\u9762\u8fde\u6746\u673a\u6784\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u8f68\u8ff9\u70b9\u7ed8\u5236\u901f\u5ea6\u7684\u989c\u8272\u68af\u5ea6\u7f16\u7801\u901f\u5ea6\u4fe1\u606f\uff0c\u652f\u6301\u57fa\u4e8e\u8f68\u8ff9\u5f62\u72b6\u548c\u901f\u5ea6\u5206\u5e03\u7684\u7ea6\u675f\u5408\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u6807\u51c6\u56db\u6746\u673a\u6784\u96c6\u3001\u56db\u6746\u548c\u66f2\u67c4\u6ed1\u5757\u6df7\u5408\u96c6\u3001\u5305\u542b\u591a\u73af\u673a\u6784\u7684\u590d\u6742\u96c6\u3002\u521d\u6b65\u7ed3\u679c\u8868\u660e\u56fe\u50cf\u8868\u793a\u6cd5\u5728\u751f\u6210\u5f0f\u673a\u68b0\u8bbe\u8ba1\u4e2d\u5177\u6709\u826f\u597d\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u56fe\u50cf\u7684\u8868\u793a\u6cd5\u4e3a\u751f\u6210\u5f0f\u673a\u68b0\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u80fd\u591f\u5728\u7edf\u4e00\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e2d\u8868\u793a\u548c\u5408\u6210\u5177\u6709\u8f6c\u52a8\u526f\u3001\u79fb\u52a8\u526f\u4ee5\u53ca\u6f5c\u5728\u51f8\u8f6e\u548c\u9f7f\u8f6e\u7684\u673a\u6784\u3002"}}
{"id": "2510.03490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.", "AI": {"tldr": "SEER\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u6587\u672c\u4e2d\u8868\u8fbe\u60c5\u611f\u7684\u5177\u4f53\u7247\u6bb5\u7684\u80fd\u529b\uff0c\u5305\u542b\u5355\u53e5\u548c\u8de8\u53e5\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u4efb\u52a1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u957f\u6587\u672c\u4e2d\u8868\u73b0\u4e0b\u964d\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4ec5\u7ed9\u6574\u4e2a\u53e5\u5b50\u5206\u914d\u5355\u4e00\u6807\u7b7e\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u9700\u8981\u77e5\u9053\u60c5\u611f\u662f\u5982\u4f55\u8868\u8fbe\u7684\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7cbe\u786e\u5b9a\u4f4d\u60c5\u611f\u8868\u8fbe\u7247\u6bb5\u7684\u65b9\u6cd5\u3002", "method": "\u521b\u5efaSEER\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1200\u4e2a\u771f\u5b9e\u4e16\u754c\u53e5\u5b50\u7684\u65b0\u6807\u6ce8\uff0c\u8bc4\u4f3014\u4e2a\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u53e5\u548c\u8de8\u53e5\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u90e8\u5206\u6a21\u578b\u5728\u5355\u53e5\u8f93\u5165\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u5e73\u5747\u8868\u73b0\uff0c\u4f46\u5728\u8f83\u957f\u6bb5\u843d\u4e2d\u51c6\u786e\u6027\u4e0b\u964d\u3002\u9519\u8bef\u5206\u6790\u663e\u793a\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u60c5\u611f\u5173\u952e\u8bcd\u548c\u5728\u4e2d\u6027\u6587\u672c\u4e2d\u51fa\u73b0\u8bef\u62a5\u3002", "conclusion": "SEER\u57fa\u51c6\u6d4b\u8bd5\u7a81\u663e\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u8bc1\u636e\u68c0\u6d4b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8f83\u957f\u6587\u672c\u65f6\uff0c\u9700\u8981\u6539\u8fdb\u6a21\u578b\u5bf9\u60c5\u611f\u8868\u8fbe\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.03295", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03295", "abs": "https://arxiv.org/abs/2510.03295", "authors": ["Passant Elchafei", "Amany Fashwan"], "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration", "comment": null, "summary": "We present VLCAP, an Arabic image captioning framework that integrates\nCLIP-based visual label retrieval with multimodal text generation. Rather than\nrelying solely on end-to-end captioning, VLCAP grounds generation in\ninterpretable Arabic visual concepts extracted with three multilingual\nencoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label\nretrieval. A hybrid vocabulary is built from training captions and enriched\nwith about 21K general domain labels translated from the Visual Genome dataset,\ncovering objects, attributes, and scenes. The top-k retrieved labels are\ntransformed into fluent Arabic prompts and passed along with the original image\nto vision-language models. In the second stage, we tested Qwen-VL and Gemini\nPro Vision for caption generation, resulting in six encoder-decoder\nconfigurations. The results show that mCLIP + Gemini Pro Vision achieved the\nbest BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL\nobtained the highest LLM-judge score (36.33%). This interpretable pipeline\nenables culturally coherent and contextually accurate Arabic captions.", "AI": {"tldr": "VLCAP\u662f\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u63cf\u8ff0\u6846\u67b6\uff0c\u7ed3\u5408CLIP\u89c6\u89c9\u6807\u7b7e\u68c0\u7d22\u4e0e\u591a\u6a21\u6001\u6587\u672c\u751f\u6210\uff0c\u901a\u8fc7\u63d0\u53d6\u53ef\u89e3\u91ca\u7684\u963f\u62c9\u4f2f\u89c6\u89c9\u6982\u5ff5\u6765\u751f\u6210\u6587\u5316\u4e00\u81f4\u4e14\u4e0a\u4e0b\u6587\u51c6\u786e\u7684\u963f\u62c9\u4f2f\u8bed\u63cf\u8ff0\u3002", "motivation": "\u4f20\u7edf\u7684\u7aef\u5230\u7aef\u56fe\u50cf\u63cf\u8ff0\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0cVLCAP\u65e8\u5728\u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9\u6982\u5ff5\u7684\u57fa\u7840\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u63cf\u8ff0\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u4e14\u6587\u5316\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u591a\u8bed\u8a00\u7f16\u7801\u5668(mCLIP\u3001AraCLIP\u3001Jina V4)\u8fdb\u884c\u89c6\u89c9\u6807\u7b7e\u68c0\u7d22\uff0c\u6784\u5efa\u5305\u542b\u8bad\u7ec3\u63cf\u8ff0\u548c\u7ea621K\u901a\u7528\u9886\u57df\u6807\u7b7e\u7684\u6df7\u5408\u8bcd\u6c47\u8868\uff0c\u5c06\u68c0\u7d22\u5230\u7684\u6807\u7b7e\u8f6c\u6362\u4e3a\u963f\u62c9\u4f2f\u8bed\u63d0\u793a\uff0c\u7ed3\u5408\u539f\u59cb\u56fe\u50cf\u8f93\u5165\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(Qwen-VL\u548cGemini Pro Vision)\u751f\u6210\u63cf\u8ff0\u3002", "result": "mCLIP + Gemini Pro Vision\u7ec4\u5408\u5728BLEU-1(5.34%)\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6(60.01%)\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cAraCLIP + Qwen-VL\u5728LLM-judge\u8bc4\u5206(36.33%)\u4e0a\u6700\u9ad8\u3002", "conclusion": "VLCAP\u7684\u53ef\u89e3\u91ca\u7ba1\u9053\u80fd\u591f\u751f\u6210\u6587\u5316\u4e00\u81f4\u4e14\u4e0a\u4e0b\u6587\u51c6\u786e\u7684\u963f\u62c9\u4f2f\u8bed\u56fe\u50cf\u63cf\u8ff0\uff0c\u4e3a\u963f\u62c9\u4f2f\u8bed\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03312", "categories": ["cs.GR", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03312", "abs": "https://arxiv.org/abs/2510.03312", "authors": ["Rong Liu", "Zhongpai Gao", "Benjamin Planche", "Meida Chen", "Van Nguyen Nguyen", "Meng Zheng", "Anwesa Choudhuri", "Terrence Chen", "Yue Wang", "Andrew Feng", "Ziyan Wu"], "title": "Universal Beta Splatting", "comment": null, "summary": "We introduce Universal Beta Splatting (UBS), a unified framework that\ngeneralizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for\nexplicit radiance field rendering. Unlike fixed Gaussian primitives, Beta\nkernels enable controllable dependency modeling across spatial, angular, and\ntemporal dimensions within a single representation. Our unified approach\ncaptures complex light transport effects, handles anisotropic view-dependent\nappearance, and models scene dynamics without requiring auxiliary networks or\nspecific color encodings. UBS maintains backward compatibility by approximating\nto Gaussian Splatting as a special case, guaranteeing plug-in usability and\nlower performance bounds. The learned Beta parameters naturally decompose scene\nproperties into interpretable without explicit supervision: spatial (surface\nvs. texture), angular (diffuse vs. specular), and temporal (static vs.\ndynamic). Our CUDA-accelerated implementation achieves real-time rendering\nwhile consistently outperforming existing methods across static,\nview-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable\nuniversal primitive for radiance field rendering. Our project website is\navailable at https://rongliu-leo.github.io/universal-beta-splatting/.", "AI": {"tldr": "Universal Beta Splatting (UBS) \u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c063D\u9ad8\u65af\u6cfc\u6e85\u63a8\u5e7f\u5230N\u7ef4\u5404\u5411\u5f02\u6027Beta\u6838\uff0c\u7528\u4e8e\u663e\u5f0f\u8f90\u5c04\u573a\u6e32\u67d3\u3002\u5b83\u4f7f\u7528\u53ef\u63a7\u7684Beta\u6838\u66ff\u4ee3\u56fa\u5b9a\u9ad8\u65af\u57fa\u5143\uff0c\u5728\u5355\u4e00\u8868\u793a\u4e2d\u5efa\u6a21\u7a7a\u95f4\u3001\u89d2\u5ea6\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u9ad8\u65af\u57fa\u5143\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u590d\u6742\u7684\u5149\u4f20\u8f93\u6548\u5e94\u3001\u5404\u5411\u5f02\u6027\u89c6\u89d2\u76f8\u5173\u5916\u89c2\u548c\u573a\u666f\u52a8\u6001\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406\u8fd9\u4e9b\u591a\u7ef4\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4f7f\u7528N\u7ef4\u5404\u5411\u5f02\u6027Beta\u6838\u4f5c\u4e3a\u901a\u7528\u57fa\u5143\uff0c\u5728\u5355\u4e00\u8868\u793a\u4e2d\u7edf\u4e00\u5efa\u6a21\u7a7a\u95f4\u3001\u89d2\u5ea6\u548c\u65f6\u95f4\u7ef4\u5ea6\u3002\u8be5\u65b9\u6cd5\u4fdd\u6301\u5411\u540e\u517c\u5bb9\u6027\uff0c\u9ad8\u65af\u6cfc\u6e85\u662f\u5176\u7279\u4f8b\u3002\u901a\u8fc7CUDA\u52a0\u901f\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3002", "result": "UBS\u5728\u9759\u6001\u3001\u89c6\u89d2\u76f8\u5173\u548c\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b66\u4e60\u5230\u7684Beta\u53c2\u6570\u80fd\u591f\u81ea\u7136\u5206\u89e3\u573a\u666f\u5c5e\u6027\u4e3a\u53ef\u89e3\u91ca\u7684\u7ec4\u4ef6\uff1a\u7a7a\u95f4\uff08\u8868\u9762vs\u7eb9\u7406\uff09\u3001\u89d2\u5ea6\uff08\u6f2b\u53cd\u5c04vs\u955c\u9762\u53cd\u5c04\uff09\u548c\u65f6\u95f4\uff08\u9759\u6001vs\u52a8\u6001\uff09\u3002", "conclusion": "Beta\u6838\u88ab\u786e\u7acb\u4e3a\u8f90\u5c04\u573a\u6e32\u67d3\u7684\u53ef\u6269\u5c55\u901a\u7528\u57fa\u5143\uff0c\u80fd\u591f\u6355\u83b7\u590d\u6742\u5149\u4f20\u8f93\u6548\u5e94\uff0c\u5904\u7406\u5404\u5411\u5f02\u6027\u5916\u89c2\uff0c\u5efa\u6a21\u573a\u666f\u52a8\u6001\uff0c\u65e0\u9700\u8f85\u52a9\u7f51\u7edc\u6216\u7279\u5b9a\u989c\u8272\u7f16\u7801\u3002"}}
{"id": "2510.03502", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats.", "AI": {"tldr": "ALHD\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u533a\u5206\u4eba\u7c7b\u548cLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u6db5\u76d6\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u8bc4\u8bba\u4e09\u79cd\u6587\u4f53\uff0c\u5305\u542b\u8d85\u8fc740\u4e07\u4e2a\u5e73\u8861\u6837\u672c\uff0c\u652f\u6301\u963f\u62c9\u4f2f\u8bedLLM\u6587\u672c\u68c0\u6d4b\u7684\u6cdb\u5316\u6027\u7814\u7a76\u3002", "motivation": "\u5efa\u7acb\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7684LLM\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u4ee5\u5e94\u5bf9\u9519\u8bef\u4fe1\u606f\u3001\u5b66\u672f\u4e0d\u7aef\u548c\u7f51\u7edc\u5a01\u80c1\u7b49\u98ce\u9669\uff0c\u586b\u8865\u963f\u62c9\u4f2f\u8bed\u5728\u8be5\u7814\u7a76\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u79cd\u6587\u4f53\uff08\u65b0\u95fb\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u8bc4\u8bba\uff09\u7684\u5927\u89c4\u6a21\u5e73\u8861\u6570\u636e\u96c6\uff0c\u8986\u76d6\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bed\u548c\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\uff0c\u4f7f\u7528\u4e09\u79cd\u9886\u5148LLM\u751f\u6210\u6837\u672c\uff0c\u5e76\u63d0\u4f9b\u4e25\u683c\u9884\u5904\u7406\u3001\u4e30\u5bcc\u6807\u6ce8\u548c\u6807\u51c6\u5316\u5206\u5272\u3002", "result": "\u5fae\u8c03\u7684BERT\u6a21\u578b\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u6a21\u578b\uff0c\u4f46\u5728\u8de8\u6587\u4f53\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u65b0\u95fb\u6587\u7ae0\u4e2d\uff0cLLM\u751f\u6210\u7684\u6587\u672c\u5728\u98ce\u683c\u4e0a\u66f4\u63a5\u8fd1\u4eba\u7c7b\u6587\u672c\u3002", "conclusion": "ALHD\u4e3a\u963f\u62c9\u4f2f\u8bedLLM\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u8de8\u6587\u4f53\u6cdb\u5316\u7684\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65b0\u95fb\u7c7b\u6587\u672c\u65f6\u7684\u68c0\u6d4b\u56f0\u96be\u3002"}}
{"id": "2510.03297", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03297", "abs": "https://arxiv.org/abs/2510.03297", "authors": ["Akshar Gothi"], "title": "Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes", "comment": "5 pages, 1 figure, 9 tables. Code and artifacts:\n  https://github.com/akshar27/spacenet-cnn-vs-vit (release v1.0.1)", "summary": "We present a controlled comparison of a convolutional neural network\n(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two\nlabel-distribution regimes: a naturally imbalanced five-class split and a\nbalanced-resampled split with 700 images per class (70:20:10 train/val/test).\nWith matched preprocessing (224x224, ImageNet normalization), lightweight\naugmentations, and a 40-epoch budget on a single NVIDIA P100, we report\naccuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics\n(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%\ntest accuracy with strong macro-F1 and lower latency; ViT-Base is competitive\nat 93% with a larger parameter count and runtime. On the balanced split, both\nmodels are strong; EfficientNet-B0 reaches 99% while ViT-Base remains\ncompetitive, indicating that balancing narrows architecture gaps while CNNs\nretain an efficiency edge. We release manifests, logs, and per-image\npredictions to support reproducibility.", "AI": {"tldr": "\u5bf9\u6bd4EfficientNet-B0\u548cViT-Base\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5206\u522b\u5728\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u4e24\u79cd\u5206\u5e03\u4e0b\u8bc4\u4f30\u51c6\u786e\u7387\u3001F1\u5206\u6570\u3001\u6a21\u578b\u5927\u5c0f\u548c\u5ef6\u8fdf\u7b49\u6307\u6807\u3002", "motivation": "\u6bd4\u8f83\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cVision Transformer\u5728\u4e0d\u540c\u6807\u7b7e\u5206\u5e03\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5728SpaceNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u4f7f\u7528\u4e0d\u5e73\u8861\u4e94\u7c7b\u5206\u5272\u548c\u5e73\u8861\u91cd\u91c7\u6837\u5206\u5272\uff08\u6bcf\u7c7b700\u5f20\u56fe\u50cf\uff09\uff0c\u91c7\u7528\u76f8\u540c\u9884\u5904\u7406\u3001\u8f7b\u91cf\u6570\u636e\u589e\u5f3a\u548c40\u8f6e\u8bad\u7ec3\u9884\u7b97\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u5206\u5272\u4e2d\uff0cEfficientNet-B0\u8fbe\u523093%\u6d4b\u8bd5\u51c6\u786e\u7387\u4e14\u5ef6\u8fdf\u66f4\u4f4e\uff1b\u5728\u5e73\u8861\u5206\u5272\u4e2d\uff0c\u4e24\u8005\u8868\u73b0\u90fd\u5f88\u597d\uff0cEfficientNet-B0\u8fbe\u523099%\uff0cViT-Base\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u5e73\u8861\u6807\u7b7e\u5206\u5e03\u53ef\u4ee5\u7f29\u5c0f\u67b6\u6784\u5dee\u8ddd\uff0c\u4f46CNN\u5728\u6548\u7387\u65b9\u9762\u4ecd\u4fdd\u6301\u4f18\u52bf\u3002"}}
{"id": "2510.03433", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03433", "abs": "https://arxiv.org/abs/2510.03433", "authors": ["\u00c1ron Samuel Kov\u00e1cs", "Pedro Hermosilla", "Renata G. Raidou"], "title": "Style Brush: Guided Style Transfer for 3D Objects", "comment": null, "summary": "We introduce Style Brush, a novel style transfer method for textured meshes\ndesigned to empower artists with fine-grained control over the stylization\nprocess. Our approach extends traditional 3D style transfer methods by\nintroducing a novel loss function that captures style directionality, supports\nmultiple style images or portions thereof, and enables smooth transitions\nbetween styles in the synthesized texture. The use of easily generated guiding\ntextures streamlines user interaction, making our approach accessible to a\nbroad audience. Extensive evaluations with various meshes, style images, and\ncontour shapes demonstrate the flexibility of our method and showcase the\nvisual appeal of the generated textures.", "AI": {"tldr": "Style Brush\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7eb9\u7406\u7f51\u683c\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6355\u6349\u98ce\u683c\u65b9\u5411\u6027\u7684\u65b0\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u591a\u98ce\u683c\u56fe\u50cf\u548c\u5e73\u6ed1\u98ce\u683c\u8fc7\u6e21\uff0c\u4e3a\u827a\u672f\u5bb6\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf3D\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u98ce\u683c\u5316\u8fc7\u7a0b\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8ba9\u827a\u672f\u5bb6\u66f4\u7cbe\u786e\u63a7\u5236\u98ce\u683c\u8fc1\u79fb\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6355\u6349\u98ce\u683c\u65b9\u5411\u6027\u7684\u65b0\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u591a\u98ce\u683c\u56fe\u50cf\u6216\u5176\u90e8\u5206\uff0c\u4f7f\u7528\u6613\u4e8e\u751f\u6210\u7684\u5f15\u5bfc\u7eb9\u7406\u7b80\u5316\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u5728\u5404\u79cd\u7f51\u683c\u3001\u98ce\u683c\u56fe\u50cf\u548c\u8f6e\u5ed3\u5f62\u72b6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\uff0c\u751f\u6210\u7684\u7eb9\u7406\u5177\u6709\u89c6\u89c9\u5438\u5f15\u529b\u3002", "conclusion": "Style Brush\u901a\u8fc7\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u7528\u6237\u53cb\u597d\u7684\u5f15\u5bfc\u7eb9\u7406\uff0c\u4e3a3D\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2510.03519", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03519", "abs": "https://arxiv.org/abs/2510.03519", "authors": ["Fangxu Yu", "Hongyu Zhao", "Tianyi Zhou"], "title": "TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning", "comment": null, "summary": "Time series reasoning is crucial to decision-making in diverse domains,\nincluding finance, energy usage, traffic, weather, and scientific discovery.\nWhile existing time series foundation models (TSFMs) can capture low-level\ndynamic patterns and provide accurate forecasting, further analysis usually\nrequires additional background knowledge and sophisticated reasoning, which are\nlacking in most TSFMs but can be achieved through large language models (LLMs).\nOn the other hand, without expensive post-training, LLMs often struggle with\nthe numerical understanding of time series data. Although it is intuitive to\nintegrate the two types of models, developing effective training recipes that\nalign the two modalities for reasoning tasks is still an open challenge. To\nthis end, we propose TS-Reasoner that aligns the latent representations of\nTSFMs with the textual inputs of LLMs for downstream understanding/reasoning\ntasks. Specifically, we propose a simple yet effective method to curate\ndiverse, synthetic pairs of time series and textual captions for alignment\ntraining. We then develop a two-stage training recipe that applies instruction\nfinetuning after the alignment pretraining. Unlike existing works that train an\nLLM to take time series as inputs, we leverage a pretrained TSFM and freeze it\nduring training. Extensive experiments on several benchmarks demonstrate that\nTS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision\nLanguage Models (VLMs), and Time Series LLMs, but also achieves this with\nremarkable data efficiency, e.g., using less than half the training data.", "AI": {"tldr": "TS-Reasoner\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u7684\u6f5c\u5728\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u6587\u672c\u8f93\u5165\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u6570\u503c\u7406\u89e3\u4e0e\u8bed\u4e49\u63a8\u7406\u7684\u878d\u5408\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u80fd\u6355\u6349\u52a8\u6001\u6a21\u5f0f\u4f46\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u5907\u63a8\u7406\u80fd\u529b\u4f46\u96be\u4ee5\u7406\u89e3\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u9700\u8981\u6709\u6548\u6574\u5408\u4e24\u79cd\u6a21\u578b\u4ee5\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff1a\u9996\u5148\u4f7f\u7528\u5408\u6210\u7684\u65f6\u5e8f-\u6587\u672c\u5bf9\u8fdb\u884c\u5bf9\u9f50\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u3002\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684TSFM\uff0c\u53ea\u8bad\u7ec3\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTS-Reasoner\u4f18\u4e8e\u4e3b\u6d41LLMs\u3001VLMs\u548c\u65f6\u95f4\u5e8f\u5217LLMs\uff0c\u4e14\u5177\u6709\u663e\u8457\u7684\u6570\u636e\u6548\u7387\uff08\u4f7f\u7528\u4e0d\u5230\u4e00\u534a\u7684\u8bad\u7ec3\u6570\u636e\uff09\u3002", "conclusion": "TS-Reasoner\u6210\u529f\u5b9e\u73b0\u4e86\u65f6\u95f4\u5e8f\u5217\u4e0e\u6587\u672c\u6a21\u6001\u7684\u6709\u6548\u5bf9\u9f50\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03314", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u5b89\u5168\u4fdd\u62a4\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u68c0\u6d4b\u5206\u7c7b\u3001\u8ddf\u8e2a\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u610f\u56fe\u8bc6\u522b\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u5e76\u6307\u51fa\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u65b9\u9762\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u57fa\u7840\u8bbe\u65bd\u63aa\u65bd\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u5bf9\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u4fdd\u62a4\u4e0d\u8db3\uff0c\u800c\u73b0\u6709AI\u5e94\u7528\u8c03\u67e5\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u5168\u9762VRU\u7406\u89e3\u548c\u4fdd\u62a4\u6240\u9700\u7684\u5176\u4ed6\u89c6\u89c9\u4efb\u52a1\u7684\u7cfb\u7edf\u8986\u76d6\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u8fc7\u53bb\u4e94\u5e74\u57fa\u4e8e\u6444\u50cf\u5934\u7684AI\u611f\u77e5\u7cfb\u7edf\u5728VRU\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\uff1a\u68c0\u6d4b\u5206\u7c7b\u3001\u8ddf\u8e2a\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u3001\u610f\u56fe\u8bc6\u522b\u9884\u6d4b\u3002", "result": "\u5efa\u7acb\u4e86AI\u8d4b\u80fd\u7684\u4e3b\u52a8\u5f0fVRU\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\u7684\u6280\u672f\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u53d1\u5c55\u8d8b\u52bf\u548c\u5173\u952e\u6280\u672f\u7a81\u7834\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u89c9AI\u8fdb\u5c55\u4e0e\u73b0\u5b9e\u90e8\u7f72\u8003\u8651\u76f8\u7ed3\u5408\uff0c\u672c\u7efc\u8ff0\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u611f\u77e5\u7cfb\u7edf\u4ee5\u589e\u5f3aVRU\u5b89\u5168\u63d0\u4f9b\u4e86\u57fa\u7840\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u9762\u4e34\u7684\u56db\u4e2a\u4e3b\u8981\u6311\u6218\u3002"}}
{"id": "2510.03434", "categories": ["cs.GR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03434", "abs": "https://arxiv.org/abs/2510.03434", "authors": ["Zhiying Jiang", "Raihan Seraj", "Marcos Villagra", "Bidhan Roy"], "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model", "comment": null, "summary": "We present Paris, the first publicly released diffusion model pre-trained\nentirely through decentralized computation. Paris demonstrates that\nhigh-quality text-to-image generation can be achieved without centrally\ncoordinated infrastructure. Paris is open for research and commercial use.\nParis required implementing our Distributed Diffusion Training framework from\nscratch. The model consists of 8 expert diffusion models (129M-605M parameters\neach) trained in complete isolation with no gradient, parameter, or\nintermediate activation synchronization. Rather than requiring synchronized\ngradient updates across thousands of GPUs, we partition data into semantically\ncoherent clusters where each expert independently optimizes its subset while\ncollectively approximating the full distribution. A lightweight transformer\nrouter dynamically selects appropriate experts at inference, achieving\ngeneration quality comparable to centrally coordinated baselines. Eliminating\nsynchronization enables training on heterogeneous hardware without specialized\ninterconnects. Empirical validation confirms that Paris's decentralized\ntraining maintains generation quality while removing the dedicated GPU cluster\nrequirement for large-scale diffusion models. Paris achieves this using\n14$\\times$ less training data and 16$\\times$ less compute than the prior\ndecentralized baseline.", "AI": {"tldr": "Paris\u662f\u9996\u4e2a\u5b8c\u5168\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u9884\u8bad\u7ec3\u7684\u516c\u5f00\u6269\u6563\u6a21\u578b\uff0c\u8bc1\u660e\u65e0\u9700\u4e2d\u5fc3\u5316\u57fa\u7840\u8bbe\u65bd\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u8bc1\u660e\u9ad8\u8d28\u91cf\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u53ef\u4ee5\u5728\u6ca1\u6709\u4e2d\u5fc3\u534f\u8c03\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\uff0c\u6d88\u9664\u5bf9\u4e13\u7528GPU\u96c6\u7fa4\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u6269\u6563\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b8\u4e2a\u4e13\u5bb6\u6269\u6563\u6a21\u578b\uff0c\u5728\u5b8c\u5168\u9694\u79bb\u7684\u73af\u5883\u4e2d\u8bad\u7ec3\uff0c\u65e0\u9700\u68af\u5ea6\u3001\u53c2\u6570\u6216\u4e2d\u95f4\u6fc0\u6d3b\u540c\u6b65\u3002\u901a\u8fc7\u6570\u636e\u8bed\u4e49\u805a\u7c7b\u5206\u533a\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u72ec\u7acb\u4f18\u5316\u5176\u5b50\u96c6\uff0c\u8f7b\u91cf\u7ea7transformer\u8def\u7531\u5668\u5728\u63a8\u7406\u65f6\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u3002", "result": "Paris\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u53bb\u4e2d\u5fc3\u5316\u57fa\u7ebf\uff0c\u8bad\u7ec3\u6570\u636e\u51cf\u5c1114\u500d\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1116\u500d\uff0c\u4e14\u80fd\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u8bad\u7ec3\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u66ff\u4ee3\u4e2d\u5fc3\u5316\u534f\u8c03\u8bad\u7ec3\uff0c\u4e3a\u5927\u89c4\u6a21\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03521", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03521", "abs": "https://arxiv.org/abs/2510.03521", "authors": ["Ali Elahi"], "title": "Identifying Financial Risk Information Using RAG with a Contrastive Insight", "comment": "7 pages, 1 figure, Workshop on Generative AI in Finance, NeurIPS 2025", "summary": "In specialized domains, humans often compare new problems against similar\nexamples, highlight nuances, and draw conclusions instead of analyzing\ninformation in isolation. When applying reasoning in specialized contexts with\nLLMs on top of a RAG, the pipeline can capture contextually relevant\ninformation, but it is not designed to retrieve comparable cases or related\nproblems.\n  While RAG is effective at extracting factual information, its outputs in\nspecialized reasoning tasks often remain generic, reflecting broad facts rather\nthan context-specific insights. In finance, it results in generic risks that\nare true for the majority of companies. To address this limitation, we propose\na peer-aware comparative inference layer on top of RAG.\n  Our contrastive approach outperforms baseline RAG in text generation metrics\nsuch as ROUGE and BERTScore in comparison with human-generated equity research\nand risk.", "AI": {"tldr": "\u5728\u4e13\u4e1a\u9886\u57df\u63a8\u7406\u4e2d\uff0c\u4f20\u7edfRAG\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u68c0\u7d22\u53ef\u6bd4\u6848\u4f8b\u6216\u76f8\u5173\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u5185\u5bb9\u8fc7\u4e8e\u901a\u7528\u3002\u672c\u6587\u63d0\u51fa\u5728RAG\u4e4b\u4e0a\u589e\u52a0\u540c\u884c\u611f\u77e5\u6bd4\u8f83\u63a8\u7406\u5c42\uff0c\u901a\u8fc7\u5bf9\u6bd4\u65b9\u6cd5\u63d0\u5347\u4e13\u4e1a\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u5728\u4e13\u4e1a\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\u4e2d\uff0c\u4eba\u7c7b\u901a\u5e38\u901a\u8fc7\u6bd4\u8f83\u7c7b\u4f3c\u6848\u4f8b\u3001\u7a81\u51fa\u7ec6\u5fae\u5dee\u522b\u6765\u63a8\u7406\uff0c\u800c\u4f20\u7edfRAG\u53ea\u80fd\u63d0\u53d6\u4e8b\u5b9e\u4fe1\u606f\uff0c\u65e0\u6cd5\u68c0\u7d22\u53ef\u6bd4\u6848\u4f8b\uff0c\u5bfc\u81f4\u8f93\u51fa\u5185\u5bb9\u8fc7\u4e8e\u901a\u7528\uff0c\u7f3a\u4e4f\u9488\u5bf9\u6027\u7684\u6d1e\u5bdf\u3002", "method": "\u5728RAG\u57fa\u7840\u4e0a\u589e\u52a0\u540c\u884c\u611f\u77e5\u6bd4\u8f83\u63a8\u7406\u5c42\uff0c\u91c7\u7528\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u548c\u6bd4\u8f83\u76f8\u5173\u6848\u4f8b\u6765\u589e\u5f3a\u4e13\u4e1a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5bf9\u6bd4\u65b9\u6cd5\u5728\u6587\u672c\u751f\u6210\u6307\u6807\uff08\u5982ROUGE\u548cBERTScore\uff09\u4e0a\u4f18\u4e8e\u57fa\u7ebfRAG\uff0c\u4e0e\u4eba\u5de5\u751f\u6210\u7684\u80a1\u6743\u7814\u7a76\u548c\u98ce\u9669\u8bc4\u4f30\u76f8\u6bd4\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5728RAG\u4e4b\u4e0a\u589e\u52a0\u5bf9\u6bd4\u63a8\u7406\u5c42\u80fd\u6709\u6548\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u63a8\u7406\u7684\u8d28\u91cf\u548c\u9488\u5bf9\u6027\uff0c\u89e3\u51b3\u4f20\u7edfRAG\u8f93\u51fa\u8fc7\u4e8e\u901a\u7528\u7684\u95ee\u9898\u3002"}}
{"id": "2510.03316", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03316", "abs": "https://arxiv.org/abs/2510.03316", "authors": ["Ryan P. Demilt", "Nicholas LaHaye", "Karis Tenneson"], "title": "The View From Space: Navigating Instrumentation Differences with EOFMs", "comment": null, "summary": "Earth Observation Foundation Models (EOFMs) have exploded in prevalence as\ntools for processing the massive volumes of remotely sensed and other earth\nobservation data, and for delivering impact on the many essential earth\nmonitoring tasks. An emerging trend posits using the outputs of pre-trained\nmodels as 'embeddings' which summarize high dimensional data to be used for\ngeneric tasks such as similarity search and content-specific queries. However,\nmost EOFM models are trained only on single modalities of data and then applied\nor benchmarked by matching bands across different modalities. It is not clear\nfrom existing work what impact diverse sensor architectures have on the\ninternal representations of the present suite of EOFMs. We show in this work\nthat the representation space of EOFMs is highly sensitive to sensor\narchitecture and that understanding this difference gives a vital perspective\non the pitfalls of current EOFM design and signals for how to move forward as\nmodel developers, users, and a community guided by robust remote-sensing\nscience.", "AI": {"tldr": "\u5730\u7403\u89c2\u6d4b\u57fa\u7840\u6a21\u578b(EOFMs)\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u7406\u89e3\u8fd9\u79cd\u5dee\u5f02\u5bf9\u4e8e\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u9677\u9631\u548c\u672a\u6765\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709EOFMs\u5927\u591a\u5728\u5355\u4e00\u6a21\u6001\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u7136\u540e\u901a\u8fc7\u8de8\u6a21\u6001\u5339\u914d\u6ce2\u6bb5\u6765\u5e94\u7528\u6216\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u4e0d\u540c\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u5185\u90e8\u8868\u793a\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5206\u6790EOFMs\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u7684\u654f\u611f\u6027\uff0c\u7814\u7a76\u4e0d\u540c\u4f20\u611f\u5668\u8bbe\u8ba1\u5bf9\u6a21\u578b\u5185\u90e8\u8868\u793a\u7684\u5f71\u54cd\u3002", "result": "EOFMs\u7684\u8868\u793a\u7a7a\u95f4\u5bf9\u4f20\u611f\u5668\u67b6\u6784\u9ad8\u5ea6\u654f\u611f\uff0c\u8fd9\u79cd\u654f\u611f\u6027\u63ed\u793a\u4e86\u5f53\u524dEOFMs\u8bbe\u8ba1\u7684\u6f5c\u5728\u95ee\u9898\u3002", "conclusion": "\u7406\u89e3\u4f20\u611f\u5668\u67b6\u6784\u5bf9EOFMs\u8868\u793a\u7a7a\u95f4\u7684\u5f71\u54cd\u4e3a\u6a21\u578b\u5f00\u53d1\u8005\u3001\u7528\u6237\u548c\u9065\u611f\u79d1\u5b66\u793e\u533a\u63d0\u4f9b\u4e86\u91cd\u8981\u89c6\u89d2\uff0c\u6307\u660e\u4e86\u7a33\u5065\u53d1\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2510.03597", "categories": ["cs.GR", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03597", "abs": "https://arxiv.org/abs/2510.03597", "authors": ["Sina Alemohammad", "Zhangyang Wang", "Richard G. Baraniuk"], "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation", "comment": null, "summary": "Scaling generative AI models is bottlenecked by the scarcity of high-quality\ntraining data. The ease of synthesizing from a generative model suggests using\n(unverified) synthetic data to augment a limited corpus of real data for the\npurpose of fine-tuning in the hope of improving performance. Unfortunately,\nhowever, the resulting positive feedback loop leads to model autophagy disorder\n(MAD, aka model collapse) that results in a rapid degradation in sample quality\nand/or diversity. In this paper, we introduce Neon (for Negative Extrapolation\nfrOm self-traiNing), a new learning method that turns the degradation from\nself-training into a powerful signal for self-improvement. Given a base model,\nNeon first fine-tunes it on its own self-synthesized data but then,\ncounterintuitively, reverses its gradient updates to extrapolate away from the\ndegraded weights. We prove that Neon works because typical inference samplers\nthat favor high-probability regions create a predictable anti-alignment between\nthe synthetic and real data population gradients, which negative extrapolation\ncorrects to better align the model with the true data distribution. Neon is\nremarkably easy to implement via a simple post-hoc merge that requires no new\nreal data, works effectively with as few as 1k synthetic samples, and typically\nuses less than 1% additional training compute. We demonstrate Neon's\nuniversality across a range of architectures (diffusion, flow matching,\nautoregressive, and inductive moment matching models) and datasets (ImageNet,\nCIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the\nxAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional\ntraining compute. Code is available at https://github.com/SinaAlemohammad/Neon", "AI": {"tldr": "Neon\u662f\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u5411\u5916\u63a8\u4ece\u81ea\u8bad\u7ec3\u4e2d\u89e3\u51b3\u6a21\u578b\u81ea\u566c\u969c\u788d\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u751f\u6210AI\u6a21\u578b\u6269\u5c55\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u4f7f\u7528\u672a\u9a8c\u8bc1\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u4f1a\u5bfc\u81f4\u6a21\u578b\u81ea\u566c\u969c\u788d\uff0c\u9020\u6210\u6837\u672c\u8d28\u91cf\u548c\u591a\u6837\u6027\u7684\u5feb\u901f\u9000\u5316\u3002", "method": "Neon\u9996\u5148\u5728\u81ea\u5408\u6210\u7684\u6570\u636e\u4e0a\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u7136\u540e\u53cd\u5176\u9053\u800c\u884c\u4e4b\uff0c\u53cd\u8f6c\u68af\u5ea6\u66f4\u65b0\u4ee5\u4ece\u9000\u5316\u7684\u6743\u91cd\u4e2d\u8fdb\u884c\u8d1f\u5411\u5916\u63a8\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u63a8\u7406\u91c7\u6837\u5668\u5728\u9ad8\u6982\u7387\u533a\u57df\u4ea7\u751f\u7684\u53ef\u9884\u6d4b\u53cd\u5bf9\u9f50\u7279\u6027\u8fdb\u884c\u4fee\u6b63\u3002", "result": "Neon\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u666e\u9002\u6027\uff0c\u5728ImageNet 256x256\u4e0a\u5c06xAR-L\u6a21\u578b\u63d0\u5347\u5230FID 1.02\u7684\u65b0SOTA\uff0c\u4ec5\u97000.36%\u7684\u989d\u5916\u8bad\u7ec3\u8ba1\u7b97\u3002", "conclusion": "Neon\u901a\u8fc7\u8d1f\u5411\u5916\u63a8\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u8bad\u7ec3\u5bfc\u81f4\u7684\u6a21\u578b\u9000\u5316\u95ee\u9898\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5bf9\u9f50\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u4e14\u5b9e\u73b0\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u3002"}}
{"id": "2510.03527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03527", "abs": "https://arxiv.org/abs/2510.03527", "authors": ["Sayan Ghosh", "Shahzaib Saqib Warraich", "Dhruv Tarsadiya", "Gregory Yauney", "Swabha Swayamdipta"], "title": "Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs", "comment": null, "summary": "Language models can be sampled multiple times to access the distribution\nunderlying their responses, but existing methods cannot efficiently synthesize\nrich epistemic signals across different long-form responses. We introduce\nConsensus Graphs (ConGrs), a flexible DAG-based data structure that represents\nshared information, as well as semantic variation in a set of sampled LM\nresponses to the same prompt. We construct ConGrs using a light-weight lexical\nsequence alignment algorithm from bioinformatics, supplemented by the targeted\nusage of a secondary LM judge. Further, we design task-dependent decoding\nmethods to synthesize a single, final response from our ConGr data structure.\nOur experiments show that synthesizing responses from ConGrs improves factual\nprecision on two biography generation tasks by up to 31% over an average\nresponse and reduces reliance on LM judges by more than 80% compared to other\nmethods. We also use ConGrs for three refusal-based tasks requiring abstention\non unanswerable queries and find that abstention rate is increased by up to\n56%. We apply our approach to the MATH and AIME reasoning tasks and find an\nimprovement over self-verification and majority vote baselines by up to 6\npoints of accuracy. We show that ConGrs provide a flexible method for capturing\nvariation in LM responses and using the epistemic signals provided by response\nvariation to synthesize more effective responses.", "AI": {"tldr": "\u63d0\u51faConsensus Graphs (ConGrs)\u6570\u636e\u7ed3\u6784\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u54cd\u5e94\u7684\u5171\u4eab\u4fe1\u606f\u548c\u8bed\u4e49\u53d8\u5316\uff0c\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u53ef\u9760\u7684\u6700\u7ec8\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6574\u5408\u591a\u4e2a\u957f\u6587\u672c\u54cd\u5e94\u4e2d\u7684\u4e30\u5bcc\u8ba4\u77e5\u4fe1\u53f7\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6355\u6349\u8bed\u8a00\u6a21\u578b\u54cd\u5e94\u53d8\u5316\u5e76\u5229\u7528\u8fd9\u4e9b\u8ba4\u77e5\u4fe1\u53f7\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u751f\u7269\u4fe1\u606f\u5b66\u7684\u8f7b\u91cf\u7ea7\u8bcd\u6c47\u5e8f\u5217\u5bf9\u9f50\u7b97\u6cd5\u6784\u5efaConGrs\uff0c\u8f85\u4ee5\u6b21\u7ea7\u8bed\u8a00\u6a21\u578b\u5224\u65ad\uff0c\u5e76\u8bbe\u8ba1\u4efb\u52a1\u76f8\u5173\u7684\u89e3\u7801\u65b9\u6cd5\u4eceConGr\u5408\u6210\u6700\u7ec8\u54cd\u5e94\u3002", "result": "\u5728\u4f20\u8bb0\u751f\u6210\u4efb\u52a1\u4e2d\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u534731%\uff1b\u5728\u62d2\u7edd\u4efb\u52a1\u4e2d\u5f03\u6743\u7387\u63d0\u9ad856%\uff1b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u53476\u4e2a\u767e\u5206\u70b9\uff1b\u51cf\u5c11\u5bf9\u8bed\u8a00\u6a21\u578b\u5224\u65ad\u7684\u4f9d\u8d56\u8d85\u8fc780%\u3002", "conclusion": "ConGrs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u6355\u6349\u8bed\u8a00\u6a21\u578b\u54cd\u5e94\u53d8\u5316\uff0c\u5e76\u5229\u7528\u54cd\u5e94\u53d8\u5316\u63d0\u4f9b\u7684\u8ba4\u77e5\u4fe1\u53f7\u6765\u5408\u6210\u66f4\u6709\u6548\u7684\u54cd\u5e94\u3002"}}
{"id": "2510.03317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03317", "abs": "https://arxiv.org/abs/2510.03317", "authors": ["G\u00fcnel Aghakishiyeva", "Jiayi Zhou", "Saagar Arya", "James David Poling", "Holly R. Houliston", "Jamie N. Womble", "David W. Johnston", "Brinnae Bent"], "title": "Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring", "comment": "Accepted to NeurIPS 2025 Imageomics Workshop", "summary": "Ecological monitoring is increasingly automated by vision models, yet opaque\npredictions limit trust and field adoption. We present an inpainting-guided,\nperturbation-based explanation technique that produces photorealistic,\nmask-localized edits that preserve scene context. Unlike masking or blurring,\nthese edits stay in-distribution and reveal which fine-grained morphological\ncues drive predictions in tasks such as species recognition and trait\nattribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for\nharbor seal detection in Glacier Bay drone imagery, using\nSegment-Anything-Model-refined masks to support two interventions: (i) object\nremoval/replacement (e.g., replacing seals with plausible ice/water or boats)\nand (ii) background replacement with original animals composited onto new\nscenes. Explanations are assessed by re-scoring perturbed images (flip rate,\nconfidence drop) and by expert review for ecological plausibility and\ninterpretability. The resulting explanations localize diagnostic structures,\navoid deletion artifacts common to traditional perturbations, and yield\ndomain-relevant insights that support expert validation and more trustworthy\ndeployment of AI in ecology.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fee\u590d\u5f15\u5bfc\u7684\u6270\u52a8\u89e3\u91ca\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u903c\u771f\u7684\u5c40\u90e8\u7f16\u8f91\u6765\u63ed\u793a\u751f\u6001\u76d1\u6d4b\u89c6\u89c9\u6a21\u578b\u4e2d\u5f71\u54cd\u9884\u6d4b\u7684\u5173\u952e\u5f62\u6001\u7279\u5f81\u3002", "motivation": "\u751f\u6001\u76d1\u6d4b\u4e2d\u81ea\u52a8\u5316\u89c6\u89c9\u6a21\u578b\u7684\u4e0d\u900f\u660e\u9884\u6d4b\u9650\u5236\u4e86\u4fe1\u4efb\u548c\u73b0\u573a\u5e94\u7528\uff0c\u9700\u8981\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u6765\u63ed\u793a\u6a21\u578b\u51b3\u7b56\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u4fee\u590d\u5f15\u5bfc\u7684\u6270\u52a8\u6280\u672f\uff0c\u7ed3\u5408Segment-Anything-Model\u7cbe\u70bc\u7684\u63a9\u7801\uff0c\u8fdb\u884c\u5bf9\u8c61\u79fb\u9664/\u66ff\u6362\u548c\u80cc\u666f\u66ff\u6362\u7b49\u5e72\u9884\uff0c\u751f\u6210\u4fdd\u6301\u573a\u666f\u4e0a\u4e0b\u6587\u7684\u5149\u903c\u771f\u7f16\u8f91\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9a\u4f4d\u8bca\u65ad\u7ed3\u6784\uff0c\u907f\u514d\u4f20\u7edf\u6270\u52a8\u7684\u5220\u9664\u4f2a\u5f71\uff0c\u5728\u6e2f\u53e3\u6d77\u8c79\u68c0\u6d4b\u4efb\u52a1\u4e2d\u901a\u8fc7\u7ffb\u8f6c\u7387\u548c\u7f6e\u4fe1\u5ea6\u4e0b\u964d\u7b49\u6307\u6807\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6001\u5b66\u4e2dAI\u90e8\u7f72\u63d0\u4f9b\u4e86\u9886\u57df\u76f8\u5173\u7684\u89c1\u89e3\uff0c\u652f\u6301\u4e13\u5bb6\u9a8c\u8bc1\u5e76\u589e\u5f3aAI\u5728\u751f\u6001\u5b66\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2510.03813", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03813", "abs": "https://arxiv.org/abs/2510.03813", "authors": ["Byungjun Kim", "Soobin Um", "Jong Chul Ye"], "title": "Diverse Text-to-Image Generation via Contrastive Noise Optimization", "comment": null, "summary": "Text-to-image (T2I) diffusion models have demonstrated impressive performance\nin generating high-fidelity images, largely enabled by text-guided inference.\nHowever, this advantage often comes with a critical drawback: limited\ndiversity, as outputs tend to collapse into similar modes under strong text\nguidance. Existing approaches typically optimize intermediate latents or text\nconditions during inference, but these methods deliver only modest gains or\nremain sensitive to hyperparameter tuning. In this work, we introduce\nContrastive Noise Optimization, a simple yet effective method that addresses\nthe diversity issue from a distinct perspective. Unlike prior techniques that\nadapt intermediate latents, our approach shapes the initial noise to promote\ndiverse outputs. Specifically, we develop a contrastive loss defined in the\nTweedie data space and optimize a batch of noise latents. Our contrastive\noptimization repels instances within the batch to maximize diversity while\nkeeping them anchored to a reference sample to preserve fidelity. We further\nprovide theoretical insights into the mechanism of this preprocessing to\nsubstantiate its effectiveness. Extensive experiments across multiple T2I\nbackbones demonstrate that our approach achieves a superior quality-diversity\nPareto frontier while remaining robust to hyperparameter choices.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6bd4\u566a\u58f0\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u521d\u59cb\u566a\u58f0\u800c\u975e\u4e2d\u95f4\u6f5c\u5728\u53d8\u91cf\u6765\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4fdd\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u5f3a\u6587\u672c\u5f15\u5bfc\u4e0b\u8f93\u51fa\u591a\u6837\u6027\u6709\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5bf9\u8d85\u53c2\u6570\u654f\u611f\u4e14\u6548\u679c\u6709\u9650\u3002", "method": "\u5f00\u53d1\u5728Tweedie\u6570\u636e\u7a7a\u95f4\u5b9a\u4e49\u7684\u5bf9\u6bd4\u635f\u5931\uff0c\u4f18\u5316\u4e00\u6279\u566a\u58f0\u6f5c\u5728\u53d8\u91cf\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4f18\u5316\u4f7f\u6279\u6b21\u5185\u5b9e\u4f8b\u76f8\u4e92\u6392\u65a5\u4ee5\u6700\u5927\u5316\u591a\u6837\u6027\uff0c\u540c\u65f6\u951a\u5b9a\u53c2\u8003\u6837\u672c\u4ee5\u4fdd\u6301\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u591a\u4e2aT2I\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u8d28\u91cf-\u591a\u6837\u6027\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e14\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bf9\u6bd4\u566a\u58f0\u4f18\u5316\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u4ece\u72ec\u7279\u89d2\u5ea6\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u591a\u6837\u6027\u95ee\u9898\u3002"}}
{"id": "2510.03528", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03528", "abs": "https://arxiv.org/abs/2510.03528", "authors": ["Ahmed Alajrami", "Xingwei Tan", "Nikolaos Aletras"], "title": "Fine-Tuning on Noisy Instructions: Effects on Generalization and Performance", "comment": null, "summary": "Instruction-tuning plays a vital role in enhancing the task-solving abilities\nof large language models (LLMs), improving their usability in generating\nhelpful responses on various tasks. However, previous work has demonstrated\nthat they are sensitive to minor variations in instruction phrasing. In this\npaper, we explore whether introducing perturbations in instruction-tuning data\ncan enhance LLMs' resistance against noisy instructions. We focus on how\ninstruction-tuning with perturbations, such as removing stop words or shuffling\nwords, affects LLMs' performance on the original and perturbed versions of\nwidely-used benchmarks (MMLU, BBH, GSM8K). We further assess learning dynamics\nand potential shifts in model behavior. Surprisingly, our results suggest that\ninstruction-tuning on perturbed instructions can, in some cases, improve\ndownstream performance. These findings highlight the importance of including\nperturbed instructions in instruction-tuning, which can make LLMs more\nresilient to noisy user inputs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5728\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4e2d\u5f15\u5165\u6270\u52a8\u662f\u5426\u80fd\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u566a\u58f0\u6307\u4ee4\u7684\u62b5\u6297\u80fd\u529b\uff0c\u53d1\u73b0\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6270\u52a8\u6307\u4ee4\u5fae\u8c03\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6307\u4ee4\u8868\u8ff0\u7684\u5fae\u5c0f\u53d8\u5316\u5f88\u654f\u611f\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u5728\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u4e2d\u5f15\u5165\u6270\u52a8\uff08\u5982\u5220\u9664\u505c\u7528\u8bcd\u3001\u6253\u4e71\u8bcd\u5e8f\uff09\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u539f\u59cb\u548c\u6270\u52a8\u7248\u672c\u57fa\u51c6\u6d4b\u8bd5\uff08MMLU\u3001BBH\u3001GSM8K\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5bf9\u6270\u52a8\u6307\u4ee4\u8fdb\u884c\u5fae\u8c03\u53cd\u800c\u80fd\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u5728\u6307\u4ee4\u5fae\u8c03\u4e2d\u5305\u542b\u6270\u52a8\u6307\u4ee4\u5f88\u91cd\u8981\uff0c\u8fd9\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u566a\u58f0\u7528\u6237\u8f93\u5165\u66f4\u5177\u97e7\u6027\u3002"}}
{"id": "2510.03318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03318", "abs": "https://arxiv.org/abs/2510.03318", "authors": ["Ahmed Kabil", "Ghada Khoriba", "Mina Yousef", "Essam A. Rashed"], "title": "Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications", "comment": "Computers in Biology and Medicine (to appear)", "summary": "Medical Image Segmentation (MIS) stands as a cornerstone in medical image\nanalysis, playing a pivotal role in precise diagnostics, treatment planning,\nand monitoring of various medical conditions. This paper presents a\ncomprehensive and systematic survey of MIS methodologies, bridging the gap\nbetween traditional image processing techniques and modern deep learning\napproaches. The survey encompasses thresholding, edge detection, region-based\nsegmentation, clustering algorithms, and model-based techniques while also\ndelving into state-of-the-art deep learning architectures such as Convolutional\nNeural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely\nadopted U-Net and its variants. Moreover, integrating attention mechanisms,\nsemi-supervised learning, generative adversarial networks (GANs), and\nTransformer-based models is thoroughly explored. In addition to covering\nestablished methods, this survey highlights emerging trends, including hybrid\narchitectures, cross-modality learning, federated and distributed learning\nframeworks, and active learning strategies, which aim to address challenges\nsuch as limited labeled datasets, computational complexity, and model\ngeneralizability across diverse imaging modalities. Furthermore, a specialized\ncase study on lumbar spine segmentation is presented, offering insights into\nthe challenges and advancements in this relatively underexplored anatomical\nregion. Despite significant progress in the field, critical challenges persist,\nincluding dataset bias, domain adaptation, interpretability of deep learning\nmodels, and integration into real-world clinical workflows.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u4ece\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u6280\u672f\u5230\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u5b8c\u6574\u53d1\u5c55\u8109\u7edc\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u534a\u76d1\u7763\u5b66\u4e60\u7b49\u524d\u6cbf\u6280\u672f\uff0c\u5e76\u5305\u542b\u8170\u690e\u5206\u5272\u7684\u6848\u4f8b\u7814\u7a76\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5728\u7cbe\u51c6\u8bca\u65ad\u3001\u6cbb\u7597\u89c4\u5212\u548c\u75c5\u60c5\u76d1\u6d4b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u6311\u6218\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5404\u79cd\u65b9\u6cd5\u7684\u53d1\u5c55\u8109\u7edc\u548c\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u6db5\u76d6\u9608\u503c\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u533a\u57df\u5206\u5272\u3001\u805a\u7c7b\u7b97\u6cd5\u7b49\u4f20\u7edf\u65b9\u6cd5\uff0c\u4ee5\u53caCNN\u3001FCN\u3001U-Net\u7b49\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u8fd8\u5305\u62ec\u6ce8\u610f\u529b\u673a\u5236\u3001\u534a\u76d1\u7763\u5b66\u4e60\u3001GAN\u3001Transformer\u7b49\u524d\u6cbf\u6280\u672f\u3002", "result": "\u5168\u9762\u68b3\u7406\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\u548c\u6280\u672f\u6f14\u8fdb\uff0c\u8bc6\u522b\u4e86\u6df7\u5408\u67b6\u6784\u3001\u8de8\u6a21\u6001\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u7b49\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u901a\u8fc7\u8170\u690e\u5206\u5272\u6848\u4f8b\u5c55\u793a\u4e86\u7279\u5b9a\u5e94\u7528\u573a\u666f\u7684\u6311\u6218\u4e0e\u8fdb\u5c55\u3002", "conclusion": "\u5c3d\u7ba1\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u96c6\u504f\u5dee\u3001\u9886\u57df\u9002\u5e94\u3001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u4e34\u5e8a\u5de5\u4f5c\u6d41\u96c6\u6210\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u89e3\u51b3\u3002"}}
{"id": "2510.03837", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03837", "abs": "https://arxiv.org/abs/2510.03837", "authors": ["Shen Fan", "Przemyslaw Musialski"], "title": "Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models", "comment": null, "summary": "We propose a simple, data-efficient pipeline that augments an implicit\nreconstruction network based on neural SDF-based CAD parts with a\npart-segmentation head trained under PartField-generated supervision. Unlike\nmethods tied to fixed taxonomies, our model accepts meshes with any number of\nparts and produces coherent, geometry-aligned labels in a single pass. We\nevaluate on randomly sampled CAD meshes from the ABC dataset with intentionally\nvaried part cardinalities, including over-segmented shapes, and report strong\nperformance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation\n(mIoU, Accuracy), together with a new Segmentation Consistency metric that\ncaptures local label smoothness. We attach a lightweight segmentation head to\nthe Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction\nwhile providing accurate part labels for meshes with any number of parts. Even\nunder degraded reconstructions on thin or intricate geometries, segmentation\nremains accurate and label-coherent, often preserving the correct part count.\nOur approach therefore offers a practical route to semantically structured CAD\nmeshes without requiring curated taxonomies or exact palette matches. We\ndiscuss limitations in boundary precision, partly due to per-face supervision,\nand outline paths toward boundary-aware training and higher resolution labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\uff0c\u901a\u8fc7\u5728\u57fa\u4e8e\u795e\u7ecfSDF\u7684CAD\u90e8\u4ef6\u9690\u5f0f\u91cd\u5efa\u7f51\u7edc\u4e0a\u6dfb\u52a0\u90e8\u4ef6\u5206\u5272\u5934\uff0c\u5e76\u4f7f\u7528PartField\u751f\u6210\u7684\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u6570\u91cf\u90e8\u4ef6\u7684\u7f51\u683c\uff0c\u5e76\u751f\u6210\u51e0\u4f55\u5bf9\u9f50\u7684\u6807\u7b7e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53d7\u9650\u4e8e\u56fa\u5b9a\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u65e0\u6cd5\u5904\u7406\u5177\u6709\u4efb\u610f\u6570\u91cf\u90e8\u4ef6\u7684CAD\u7f51\u683c\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u5206\u7c7b\u6216\u7cbe\u786e\u8c03\u8272\u677f\u5339\u914d\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8bed\u4e49\u7ed3\u6784\u5316\u7684CAD\u7f51\u683c\u3002", "method": "\u5728Flat-CAD SDF\u4e3b\u5e72\u7f51\u7edc\u4e0a\u9644\u52a0\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\uff0c\u4f7f\u7528PartField\u751f\u6210\u7684\u76d1\u7763\u8fdb\u884c\u8bad\u7ec3\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4efb\u610f\u90e8\u4ef6\u6570\u91cf\u7684\u7f51\u683c\uff0c\u5e76\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u751f\u6210\u51e0\u4f55\u5bf9\u9f50\u7684\u8fde\u8d2f\u6807\u7b7e\u3002", "result": "\u5728ABC\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u6307\u6807\uff08CDL1/CDL2, F1-micro, NC\uff09\u548c\u5206\u5272\u6307\u6807\uff08mIoU, Accuracy\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u5373\u4f7f\u5728\u8584\u58c1\u6216\u590d\u6742\u51e0\u4f55\u7684\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u65f6\uff0c\u5206\u5272\u4ecd\u7136\u4fdd\u6301\u51c6\u786e\u548c\u6807\u7b7e\u4e00\u81f4\u6027\uff0c\u901a\u5e38\u80fd\u4fdd\u6301\u6b63\u786e\u7684\u90e8\u4ef6\u6570\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bed\u4e49\u7ed3\u6784\u5316\u7684CAD\u7f51\u683c\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u65e0\u9700\u7cbe\u5fc3\u7b56\u5212\u7684\u5206\u7c7b\u4f53\u7cfb\u6216\u7cbe\u786e\u7684\u8c03\u8272\u677f\u5339\u914d\u3002\u4e3b\u8981\u5c40\u9650\u5728\u4e8e\u8fb9\u754c\u7cbe\u5ea6\uff0c\u90e8\u5206\u539f\u56e0\u662f\u57fa\u4e8e\u9762\u7684\u76d1\u7763\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u8fb9\u754c\u611f\u77e5\u8bad\u7ec3\u548c\u66f4\u9ad8\u5206\u8fa8\u7387\u6807\u7b7e\u3002"}}
{"id": "2510.03536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03536", "abs": "https://arxiv.org/abs/2510.03536", "authors": ["Zhaohan Meng", "Zaiqiao Meng", "Siwei Liu", "Iadh Ounis"], "title": "TriMediQ: A Triplet-Structured Approach for Interactive Medical Question Answering", "comment": "Preprint", "summary": "Large Language Models (LLMs) perform strongly in static and single-turn\nmedical Question Answer (QA) benchmarks, yet such settings diverge from the\niterative information gathering process required in practical clinical\nconsultations. The MEDIQ framework addresses this mismatch by recasting the\ndiagnosis as an interactive dialogue between a patient and an expert system,\nbut the reliability of LLMs drops dramatically when forced to reason with\ndialogue logs, where clinical facts appear in sentences without clear links. To\nbridge this gap, we introduce TriMediQ, a triplet-structured approach that\nsummarises patient responses into triplets and integrates them into a Knowledge\nGraph (KG), enabling multi-hop reasoning. We introduce a frozen triplet\ngenerator that extracts clinically relevant triplets, using prompts designed to\nensure factual consistency. In parallel, a trainable projection module,\ncomprising a graph encoder and a projector, captures relational information\nfrom the KG to enhance expert reasoning. TriMediQ operates in two steps: (i)\nthe projection module fine-tuning with all LLM weights frozen; and (ii) using\nthe fine-tuned module to guide multi-hop reasoning during inference. We\nevaluate TriMediQ on two interactive QA benchmarks, showing that it achieves up\nto 10.4\\% improvement in accuracy over five baselines on the iMedQA dataset.\nThese results demonstrate that converting patient responses into structured\ntriplet-based graphs enables more accurate clinical reasoning in multi-turn\nsettings, providing a solution for the deployment of LLM-based medical\nassistants.", "AI": {"tldr": "TriMediQ\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e09\u5143\u7ec4\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u60a3\u8005\u54cd\u5e94\u603b\u7ed3\u4e3a\u4e09\u5143\u7ec4\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u5347LLM\u5728\u591a\u8f6e\u533b\u7597\u5bf9\u8bdd\u4e2d\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u9759\u6001\u533b\u7597\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u54a8\u8be2\u7684\u591a\u8f6e\u4ea4\u4e92\u5f0f\u4fe1\u606f\u6536\u96c6\u4e2d\u53ef\u9760\u6027\u663e\u8457\u4e0b\u964d\uff0c\u56e0\u4e3a\u4e34\u5e8a\u4e8b\u5b9e\u5728\u5bf9\u8bdd\u65e5\u5fd7\u4e2d\u7f3a\u4e4f\u660e\u786e\u5173\u8054\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684\u4e09\u5143\u7ec4\u751f\u6210\u5668\u63d0\u53d6\u4e34\u5e8a\u76f8\u5173\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u8bad\u7ec3\u6295\u5f71\u6a21\u5757\uff08\u56fe\u7f16\u7801\u5668\u548c\u6295\u5f71\u5668\uff09\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6355\u83b7\u5173\u7cfb\u4fe1\u606f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u5fae\u8c03\u6295\u5f71\u6a21\u5757\uff08LLM\u6743\u91cd\u51bb\u7ed3\uff09\uff0c\u7136\u540e\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5fae\u8c03\u6a21\u5757\u6307\u5bfc\u591a\u8df3\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u4ea4\u4e92\u5f0fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTriMediQ\u5728iMedQA\u6570\u636e\u96c6\u4e0a\u6bd4\u4e94\u4e2a\u57fa\u7ebf\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe10.4%\u3002", "conclusion": "\u5c06\u60a3\u8005\u54cd\u5e94\u8f6c\u6362\u4e3a\u57fa\u4e8e\u4e09\u5143\u7ec4\u7684\u7ed3\u6784\u5316\u56fe\u8c31\u80fd\u591f\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e34\u5e8a\u63a8\u7406\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u533b\u7597\u52a9\u624b\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03328", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03328", "abs": "https://arxiv.org/abs/2510.03328", "authors": ["Fiona Victoria Stanley Jothiraj", "Arunaggiri Pandian Karunanidhi", "Seth A. Eichmeyer"], "title": "DECOR: Deep Embedding Clustering with Orientation Robustness", "comment": null, "summary": "In semiconductor manufacturing, early detection of wafer defects is critical\nfor product yield optimization. However, raw wafer data from wafer quality\ntests are often complex, unlabeled, imbalanced and can contain multiple defects\non a single wafer, making it crucial to design clustering methods that remain\nreliable under such imperfect data conditions. We introduce DECOR, a deep\nclustering with orientation robustness framework that groups complex defect\npatterns from wafer maps into consistent clusters. We evaluate our method on\nthe open source MixedWM38 dataset, demonstrating its ability to discover\nclusters without manual tuning. DECOR explicitly accounts for orientation\nvariations in wafer maps, ensuring that spatially similar defects are\nconsistently clustered regardless of its rotation or alignment. Experiments\nindicate that our method outperforms existing clustering baseline methods, thus\nproviding a reliable and scalable solution in automated visual inspection\nsystems.", "AI": {"tldr": "DECOR\u662f\u4e00\u4e2a\u9762\u5411\u6676\u5706\u7f3a\u9677\u68c0\u6d4b\u7684\u6df1\u5ea6\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u6676\u5706\u56fe\u7684\u65b9\u5411\u53d8\u5316\uff0c\u80fd\u591f\u5728\u590d\u6742\u3001\u4e0d\u5e73\u8861\u3001\u672a\u6807\u8bb0\u7684\u6570\u636e\u6761\u4ef6\u4e0b\u53ef\u9760\u5730\u5bf9\u7f3a\u9677\u6a21\u5f0f\u8fdb\u884c\u805a\u7c7b\u3002", "motivation": "\u534a\u5bfc\u4f53\u5236\u9020\u4e2d\uff0c\u6676\u5706\u7f3a\u9677\u7684\u65e9\u671f\u68c0\u6d4b\u5bf9\u4ea7\u54c1\u826f\u7387\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f46\u539f\u59cb\u6676\u5706\u6570\u636e\u901a\u5e38\u590d\u6742\u3001\u672a\u6807\u8bb0\u3001\u4e0d\u5e73\u8861\uff0c\u4e14\u5355\u4e2a\u6676\u5706\u53ef\u80fd\u5305\u542b\u591a\u79cd\u7f3a\u9677\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u5728\u8fd9\u79cd\u4e0d\u5b8c\u7f8e\u6570\u636e\u6761\u4ef6\u4e0b\u4fdd\u6301\u53ef\u9760\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDECOR\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5177\u6709\u65b9\u5411\u9c81\u68d2\u6027\u7684\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u6676\u5706\u56fe\u4e2d\u7684\u590d\u6742\u7f3a\u9677\u6a21\u5f0f\u5206\u7ec4\u5230\u4e00\u81f4\u7684\u7c07\u4e2d\u3002\u8be5\u65b9\u6cd5\u660e\u786e\u8003\u8651\u4e86\u6676\u5706\u56fe\u7684\u65b9\u5411\u53d8\u5316\uff0c\u786e\u4fdd\u7a7a\u95f4\u76f8\u4f3c\u7684\u7f3a\u9677\u65e0\u8bba\u5176\u65cb\u8f6c\u6216\u5bf9\u9f50\u65b9\u5f0f\u5982\u4f55\u90fd\u80fd\u88ab\u4e00\u81f4\u5730\u805a\u7c7b\u3002", "result": "\u5728\u5f00\u6e90MixedWM38\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDECOR\u80fd\u591f\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u5373\u53ef\u53d1\u73b0\u7c07\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u805a\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DECOR\u4e3a\u81ea\u52a8\u5316\u89c6\u89c9\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u6676\u5706\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u590d\u6742\u6570\u636e\u6761\u4ef6\u3002"}}
{"id": "2510.03964", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2510.03964", "abs": "https://arxiv.org/abs/2510.03964", "authors": ["Ville Cantory", "Darya Biparva", "Haoyu Tan", "Tongyu Nie", "John Schroeder", "Ruofei Du", "Victoria Interrante", "Piotr Didyk"], "title": "Enhancing Foveated Rendering with Weighted Reservoir Sampling", "comment": "To appear in The 18th ACM SIGGRAPH Conference on Motion, Interaction,\n  and Games (MIG '25), December 03-05, 2025, Zurich, Switzerland", "summary": "Spatiotemporal sensitivity to high frequency information declines with\nincreased peripheral eccentricity. Foveated rendering exploits this by\ndecreasing the spatial resolution of rendered images in peripheral vision,\nreducing the rendering cost by omitting high frequency details. As foveation\nlevels increase, the rendering quality is reduced, and traditional foveated\nrendering systems tend not to preserve samples that were previously rendered at\nhigh spatial resolution in previous frames. Additionally, prior research has\nshown that saccade landing positions are distributed around a target location\nrather than landing at a single point, and that even during fixations, eyes\nperform small microsaccades around a fixation point. This creates an\nopportunity for sampling from temporally neighbouring frames with differing\nfoveal locations to reduce the required rendered size of the foveal region\nwhile achieving a higher perceived image quality. We further observe that the\ntemporal presentation of pixels frame-to-frame can be viewed as a data stream,\npresenting a random sampling problem. Following this intuition, we propose a\nWeighted Reservoir Sampling technique to efficiently maintain a reservoir of\nthe perceptually relevant high quality pixel samples from previous frames and\nincorporate them into the computation of the current frame. This allows the\nrenderer to render a smaller region of foveal pixels per frame by temporally\nreusing pixel samples that are still relevant to reconstruct a higher perceived\nimage quality, while allowing for higher levels of foveation. Our method\noperates on the output of foveated rendering, and runs in under 1\\,ms at 4K\nresolution, making it highly efficient and integrable with real-time VR and AR\nfoveated rendering systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u84c4\u6c34\u6c60\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u5148\u524d\u5e27\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u6765\u51cf\u5c0f\u6bcf\u5e27\u9700\u8981\u6e32\u67d3\u7684\u4e2d\u5fc3\u51f9\u533a\u57df\u5927\u5c0f\uff0c\u540c\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5229\u7528\u4eba\u773c\u5728\u6ce8\u89c6\u70b9\u5468\u56f4\u5b58\u5728\u5fae\u5c0f\u79fb\u52a8\u7684\u7279\u6027\uff0c\u4ece\u65f6\u95f4\u76f8\u90bb\u5e27\u4e2d\u91c7\u6837\u4e0d\u540c\u4e2d\u5fc3\u51f9\u4f4d\u7f6e\u7684\u50cf\u7d20\uff0c\u4ece\u800c\u51cf\u5c0f\u4e2d\u5fc3\u51f9\u533a\u57df\u7684\u6e32\u67d3\u5c3a\u5bf8\u5e76\u63d0\u9ad8\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u5c06\u5e27\u95f4\u50cf\u7d20\u7684\u65f6\u95f4\u5448\u73b0\u89c6\u4e3a\u6570\u636e\u6d41\uff0c\u91c7\u7528\u52a0\u6743\u84c4\u6c34\u6c60\u91c7\u6837\u6280\u672f\u6709\u6548\u7ef4\u62a4\u5148\u524d\u5e27\u4e2d\u611f\u77e5\u76f8\u5173\u7684\u9ad8\u8d28\u91cf\u50cf\u7d20\u6837\u672c\u5e93\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u5f53\u524d\u5e27\u7684\u8ba1\u7b97\u4e2d\u3002", "result": "\u8be5\u65b9\u6cd5\u57284K\u5206\u8fa8\u7387\u4e0b\u8fd0\u884c\u65f6\u95f4\u5c0f\u4e8e1\u6beb\u79d2\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c0f\u6bcf\u5e27\u9700\u8981\u6e32\u67d3\u7684\u4e2d\u5fc3\u51f9\u50cf\u7d20\u533a\u57df\uff0c\u540c\u65f6\u5141\u8bb8\u66f4\u9ad8\u7684\u4e2d\u5fc3\u51f9\u5316\u6c34\u5e73\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9ad8\u6548\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u5b9e\u65f6VR\u548cAR\u4e2d\u5fc3\u51f9\u6e32\u67d3\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u65f6\u95f4\u590d\u7528\u50cf\u7d20\u6837\u672c\u5b9e\u73b0\u4e86\u6e32\u67d3\u6210\u672c\u964d\u4f4e\u548c\u611f\u77e5\u8d28\u91cf\u63d0\u5347\u7684\u5e73\u8861\u3002"}}
{"id": "2510.03541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03541", "abs": "https://arxiv.org/abs/2510.03541", "authors": ["Andrew Halterman", "Katherine A. Keith"], "title": "What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification", "comment": null, "summary": "Generative large language models (LLMs) are now used extensively for text\nclassification in computational social science (CSS). In this work, focus on\nthe steps before and after LLM prompting -- conceptualization of concepts to be\nclassified and using LLM predictions in downstream statistical inference --\nwhich we argue have been overlooked in much of LLM-era CSS. We claim LLMs can\ntempt analysts to skip the conceptualization step, creating conceptualization\nerrors that bias downstream estimates. Using simulations, we show that this\nconceptualization-induced bias cannot be corrected for solely by increasing LLM\naccuracy or post-hoc bias correction methods. We conclude by reminding CSS\nanalysts that conceptualization is still a first-order concern in the LLM-era\nand provide concrete advice on how to pursue low-cost, unbiased, low-variance\ndownstream estimates.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u65f6\uff0c\u6982\u5ff5\u5316\u6b65\u9aa4\u5bb9\u6613\u88ab\u5ffd\u89c6\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6982\u5ff5\u5316\u8bef\u5dee\u5e76\u5f71\u54cd\u4e0b\u6e38\u7edf\u8ba1\u63a8\u65ad\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5206\u7c7b\uff0c\u4f46\u7814\u7a76\u8005\u5f80\u5f80\u5ffd\u89c6\u4e86\u5206\u7c7b\u524d\u7684\u6982\u5ff5\u5316\u6b65\u9aa4\u548c\u5206\u7c7b\u540e\u7684\u7edf\u8ba1\u63a8\u65ad\u95ee\u9898\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5206\u6790\u6982\u5ff5\u5316\u8bef\u5dee\u5bf9\u4e0b\u6e38\u4f30\u8ba1\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u8bd5\u4ec5\u63d0\u9ad8LLM\u51c6\u786e\u6027\u6216\u4f7f\u7528\u540e\u5904\u7406\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u662f\u5426\u80fd\u7ea0\u6b63\u8fd9\u79cd\u504f\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6982\u5ff5\u5316\u5f15\u8d77\u7684\u504f\u5dee\u65e0\u6cd5\u4ec5\u901a\u8fc7\u63d0\u9ad8LLM\u51c6\u786e\u6027\u6216\u540e\u5904\u7406\u504f\u5dee\u6821\u6b63\u65b9\u6cd5\u6765\u7ea0\u6b63\uff0c\u8fd9\u79cd\u504f\u5dee\u4f1a\u6301\u7eed\u5f71\u54cd\u4e0b\u6e38\u4f30\u8ba1\u3002", "conclusion": "\u5728LLM\u65f6\u4ee3\uff0c\u6982\u5ff5\u5316\u4ecd\u7136\u662f\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u9996\u8981\u5173\u6ce8\u70b9\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u65e0\u504f\u3001\u4f4e\u65b9\u5dee\u4e0b\u6e38\u4f30\u8ba1\u7684\u5177\u4f53\u5efa\u8bae\u3002"}}
{"id": "2510.03337", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.03337", "abs": "https://arxiv.org/abs/2510.03337", "authors": ["Andrey A. Lebedev", "Victor B. Kazantsev", "Sergey V. Stasenko"], "title": "Error correction in multiclass image classification of facial emotion on unbalanced samples", "comment": null, "summary": "This paper considers the problem of error correction in multi-class\nclassification of face images on unbalanced samples. The study is based on the\nanalysis of a data frame containing images labeled by seven different emotional\nstates of people of different ages. Particular attention is paid to the problem\nof class imbalance, in which some emotions significantly prevail over others.\nTo solve the classification problem, a neural network model based on LSTM with\nan attention mechanism focusing on key areas of the face that are informative\nfor emotion recognition is used. As part of the experiments, the model is\ntrained on all possible configurations of subsets of six classes with\nsubsequent error correction for the seventh class, excluded at the training\nstage. The results show that correction is possible for all classes, although\nthe degree of success varies: some classes are better restored, others are\nworse. In addition, on the test sample, when correcting some classes, an\nincrease in key quality metrics for small classes was recorded, which indicates\nthe promise of the proposed approach in solving applied problems related to the\nsearch for rare events, for example, in anti-fraud systems. Thus, the proposed\nmethod can be effectively applied in facial expression analysis systems and in\ntasks requiring stable classification under skewed class distribution.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u9762\u90e8\u8868\u60c5\u5206\u7c7b\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u9519\u8bef\u6821\u6b63\u6280\u672f\u63d0\u9ad8\u5bf9\u5c0f\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u67d0\u4e9b\u60c5\u7eea\u7c7b\u522b\u6837\u672c\u8fdc\u5c11\u4e8e\u5176\u4ed6\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u5bf9\u5c0f\u7c7b\u522b\uff08\u5982\u7f55\u89c1\u60c5\u7eea\uff09\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eLSTM\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u805a\u7126\u4e8e\u9762\u90e8\u5173\u952e\u533a\u57df\u3002\u901a\u8fc7\u5728\u516d\u4e2a\u7c7b\u522b\u5b50\u96c6\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7136\u540e\u5bf9\u7b2c\u4e03\u4e2a\u88ab\u6392\u9664\u7684\u7c7b\u522b\u8fdb\u884c\u9519\u8bef\u6821\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u6240\u6709\u7c7b\u522b\u90fd\u53ef\u4ee5\u8fdb\u884c\u6821\u6b63\uff0c\u4f46\u6210\u529f\u7387\u4e0d\u540c\u3002\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6821\u6b63\u67d0\u4e9b\u7c7b\u522b\u65f6\u5c0f\u7c7b\u522b\u7684\u5173\u952e\u8d28\u91cf\u6307\u6807\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u5206\u6790\u7cfb\u7edf\uff0c\u7279\u522b\u9002\u5408\u7c7b\u522b\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u5206\u7c7b\u4efb\u52a1\uff0c\u5982\u53cd\u6b3a\u8bc8\u7cfb\u7edf\u4e2d\u7f55\u89c1\u4e8b\u4ef6\u7684\u68c0\u6d4b\u3002"}}
{"id": "2510.04536", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04536", "abs": "https://arxiv.org/abs/2510.04536", "authors": ["Shun-ichiro Hayashi", "Daichi Mukunoki", "Tetsuya Hoshino", "Satoshi Ohshima", "Takahiro Katagiri"], "title": "3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG", "comment": null, "summary": "This paper proposes \"3Dify,\" a procedural 3D computer graphics (3D-CG)\ngeneration framework utilizing Large Language Models (LLMs). The framework\nenables users to generate 3D-CG content solely through natural language\ninstructions. 3Dify is built upon Dify, an open-source platform for AI\napplication development, and incorporates several state-of-the-art LLM-related\ntechnologies such as the Model Context Protocol (MCP) and Retrieval-Augmented\nGeneration (RAG). For 3D-CG generation support, 3Dify automates the operation\nof various Digital Content Creation (DCC) tools via MCP. When DCC tools do not\nsupport MCP-based interaction, the framework employs the Computer-Using Agent\n(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,\nto enhance image generation quality, 3Dify allows users to provide feedback by\nselecting preferred images from multiple candidates. The LLM then learns\nvariable patterns from these selections and applies them to subsequent\ngenerations. Furthermore, 3Dify supports the integration of locally deployed\nLLMs, enabling users to utilize custom-developed models and to reduce both time\nand monetary costs associated with external API calls by leveraging their own\ncomputational resources.", "AI": {"tldr": "3Dify\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a0b\u5e8f\u53163D\u56fe\u5f62\u751f\u6210\u6846\u67b6\uff0c\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u62103D\u5185\u5bb9\uff0c\u652f\u6301MCP\u534f\u8bae\u548cRAG\u6280\u672f\uff0c\u5e76\u80fd\u81ea\u52a8\u5316DCC\u5de5\u5177\u64cd\u4f5c\u548cGUI\u754c\u9762\u64cd\u4f5c\u3002", "motivation": "\u65e8\u5728\u7b80\u53163D\u56fe\u5f62\u5185\u5bb9\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u8ba9\u7528\u6237\u65e0\u9700\u4e13\u4e1a3D\u5efa\u6a21\u6280\u80fd\u5373\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u5185\u5bb9\uff0c\u540c\u65f6\u964d\u4f4e\u4f7f\u7528\u6210\u672c\u548c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "method": "\u57fa\u4e8eDify\u5e73\u53f0\u6784\u5efa\uff0c\u91c7\u7528MCP\u534f\u8bae\u81ea\u52a8\u5316DCC\u5de5\u5177\u64cd\u4f5c\uff0c\u4f7f\u7528CUA\u65b9\u6cd5\u5904\u7406\u4e0d\u652f\u6301MCP\u7684GUI\u754c\u9762\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u9009\u62e9\u4f18\u5316\u56fe\u50cf\u751f\u6210\uff0c\u652f\u6301\u672c\u5730LLM\u90e8\u7f72\u4ee5\u51cf\u5c11API\u8c03\u7528\u6210\u672c\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b8c\u6574\u76843D-CG\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u62103D\u5185\u5bb9\uff0c\u652f\u6301\u591a\u79cdDCC\u5de5\u5177\u81ea\u52a8\u5316\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u53cd\u9988\u673a\u5236\u6301\u7eed\u6539\u8fdb\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "3Dify\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u62103D\u5185\u5bb9\u7684\u76ee\u6807\uff0c\u4e3a\u666e\u901a\u7528\u6237\u63d0\u4f9b\u4e86\u4fbf\u6377\u76843D\u521b\u4f5c\u5de5\u5177\uff0c\u540c\u65f6\u901a\u8fc7\u672c\u5730\u90e8\u7f72\u548c\u53cd\u9988\u673a\u5236\u4f18\u5316\u4e86\u4f7f\u7528\u4f53\u9a8c\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2510.03553", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03553", "abs": "https://arxiv.org/abs/2510.03553", "authors": ["Hasibur Rahman", "Hanan Salam"], "title": "CCD-Bench: Probing Cultural Conflict in Large Language Model Decision-Making", "comment": null, "summary": "Although large language models (LLMs) are increasingly implicated in\ninterpersonal and societal decision-making, their ability to navigate explicit\nconflicts between legitimately different cultural value systems remains largely\nunexamined. Existing benchmarks predominantly target cultural knowledge\n(CulturalBench), value prediction (WorldValuesBench), or single-axis bias\ndiagnostics (CDEval); none evaluate how LLMs adjudicate when multiple\nculturally grounded values directly clash. We address this gap with CCD-Bench,\na benchmark that assesses LLM decision-making under cross-cultural value\nconflict. CCD-Bench comprises 2,182 open-ended dilemmas spanning seven domains,\neach paired with ten anonymized response options corresponding to the ten GLOBE\ncultural clusters. These dilemmas are presented using a stratified Latin square\nto mitigate ordering effects. We evaluate 17 non-reasoning LLMs. Models\ndisproportionately prefer Nordic Europe (mean 20.2 percent) and Germanic Europe\n(12.4 percent), while options for Eastern Europe and the Middle East and North\nAfrica are underrepresented (5.6 to 5.8 percent). Although 87.9 percent of\nrationales reference multiple GLOBE dimensions, this pluralism is superficial:\nmodels recombine Future Orientation and Performance Orientation, and rarely\nground choices in Assertiveness or Gender Egalitarianism (both under 3\npercent). Ordering effects are negligible (Cramer's V less than 0.10), and\nsymmetrized KL divergence shows clustering by developer lineage rather than\ngeography. These patterns suggest that current alignment pipelines promote a\nconsensus-oriented worldview that underserves scenarios demanding power\nnegotiation, rights-based reasoning, or gender-aware analysis. CCD-Bench shifts\nevaluation beyond isolated bias detection toward pluralistic decision making\nand highlights the need for alignment strategies that substantively engage\ndiverse worldviews.", "AI": {"tldr": "CCD-Bench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u8de8\u6587\u5316\u4ef7\u503c\u51b2\u7a81\u4e2d\u51b3\u7b56\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b2,182\u4e2a\u8de8\u4e03\u4e2a\u9886\u57df\u7684\u5f00\u653e\u5f0f\u56f0\u5883\uff0c\u53d1\u73b0\u6a21\u578b\u504f\u597d\u5317\u6b27\u548c\u65e5\u8033\u66fc\u6b27\u6d32\u6587\u5316\uff0c\u800c\u5ffd\u89c6\u4e1c\u6b27\u548c\u4e2d\u4e1c\u6587\u5316\uff0c\u663e\u793a\u5f53\u524d\u5bf9\u9f50\u7b56\u7565\u5b58\u5728\u6587\u5316\u504f\u89c1\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6587\u5316\u77e5\u8bc6\u3001\u4ef7\u503c\u9884\u6d4b\u6216\u5355\u8f74\u504f\u89c1\u8bca\u65ad\uff0c\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u591a\u4e2a\u6587\u5316\u4ef7\u503c\u7cfb\u7edf\u76f4\u63a5\u51b2\u7a81\u65f6\u7684\u88c1\u51b3\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u8bc4\u4f30\u8de8\u6587\u5316\u4ef7\u503c\u51b2\u7a81\u51b3\u7b56\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b2,182\u4e2a\u5f00\u653e\u5f0f\u56f0\u5883\u7684CCD-Bench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e03\u4e2a\u9886\u57df\uff0c\u6bcf\u4e2a\u56f0\u5883\u5bf9\u5e94\u5341\u4e2aGLOBE\u6587\u5316\u96c6\u7fa4\u7684\u533f\u540d\u54cd\u5e94\u9009\u9879\uff0c\u4f7f\u7528\u5206\u5c42\u62c9\u4e01\u65b9\u8bbe\u8ba1\u6765\u51cf\u8f7b\u987a\u5e8f\u6548\u5e94\uff0c\u8bc4\u4f3017\u4e2a\u975e\u63a8\u7406LLM\u3002", "result": "\u6a21\u578b\u660e\u663e\u504f\u597d\u5317\u6b27\u6b27\u6d32(20.2%)\u548c\u65e5\u8033\u66fc\u6b27\u6d32(12.4%)\uff0c\u800c\u4e1c\u6b27\u548c\u4e2d\u4e1c\u5317\u975e\u9009\u9879\u4ee3\u8868\u6027\u4e0d\u8db3(5.6-5.8%)\uff1b87.9%\u7684\u7406\u636e\u5f15\u7528\u591a\u4e2aGLOBE\u7ef4\u5ea6\u4f46\u53ea\u662f\u8868\u9762\u591a\u5143\u4e3b\u4e49\uff1b\u987a\u5e8f\u6548\u5e94\u53ef\u5ffd\u7565\uff0c\u6a21\u578b\u6309\u5f00\u53d1\u8005\u8c31\u7cfb\u800c\u975e\u5730\u7406\u4f4d\u7f6e\u805a\u7c7b\u3002", "conclusion": "\u5f53\u524d\u5bf9\u9f50\u6d41\u7a0b\u4fc3\u8fdb\u5171\u8bc6\u5bfc\u5411\u4e16\u754c\u89c2\uff0c\u65e0\u6cd5\u5145\u5206\u5904\u7406\u9700\u8981\u6743\u529b\u534f\u5546\u3001\u6743\u5229\u63a8\u7406\u6216\u6027\u522b\u610f\u8bc6\u5206\u6790\u7684\u573a\u666f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5b9e\u8d28\u6027\u53c2\u4e0e\u591a\u5143\u4e16\u754c\u89c2\u7684\u66f4\u5e73\u8861\u5bf9\u9f50\u7b56\u7565\u3002"}}
{"id": "2510.03341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03341", "abs": "https://arxiv.org/abs/2510.03341", "authors": ["Bozheng Li", "Miao Yang", "Zhenhan Chen", "Jiawang Cao", "Mushui Liu", "Yi Lu", "Yongliang Wu", "Bin Zhang", "Yangguang Ji", "Licheng Tang", "Jay Wu", "Wenbo Zhu"], "title": "OpusAnimation: Code-Based Dynamic Chart Generation", "comment": "working in progress", "summary": "Dynamic Chart Generation (DCG) involves producing code-rendered animated\nvisualizations as charts. While recent advances in multi-modal large language\nmodels (MLLMs) have significantly improved their capability on static chart\ngeneration and comprehension, MLLMs' potential for handling dynamic chart\ngeneration and understanding remains underexplored. To bridge this research\ngap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first\nbenchmark evaluating MLLM's capability on dynamic chart generation tasks from\nthree dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and\nVideo-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with\nannotations covering instruction-code-video triplets and QA pairs for both code\nand video evaluation. Based on DCG-8K, we explored a two-stage training recipe,\nproposing Joint-Code-Visual Reward for group relative policy optimization to\nconstruct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking\nresult reveals shortcomings of existing MLLMs in the visual-to-chart task, and\nour model beats the best open-sourced MLLM with an average 8.31% performance\ngain across three tasks, and shows on par performance against proprietary\nmodels with only 3B parameters, proving the effectiveness of our training\nrecipe. Our code and dataset will be publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86DCG-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86Qwen2.5-VL-DCG-3B\u6a21\u578b\uff0c\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53478.31%\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9759\u6001\u56fe\u8868\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u548c\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u4e86DCG-8K\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u6307\u4ee4-\u4ee3\u7801-\u89c6\u9891\u4e09\u5143\u7ec4\u548cQA\u5bf9\uff1b\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528\u8054\u5408\u4ee3\u7801-\u89c6\u89c9\u5956\u52b1\u8fdb\u884c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709MLLM\u5728\u89c6\u89c9\u5230\u56fe\u8868\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff1b\u5f00\u53d1\u7684\u6a21\u578b\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53478.31%\uff0c\u4ec5\u75283B\u53c2\u6570\u5c31\u80fd\u4e0e\u4e13\u6709\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\uff0c\u5f00\u53d1\u7684\u6a21\u578b\u5728\u52a8\u6001\u56fe\u8868\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2510.04539", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04539", "abs": "https://arxiv.org/abs/2510.04539", "authors": ["Zeng Tao", "Zheng Ding", "Zeyuan Chen", "Xiang Zhang", "Leizhi Li", "Zhuowen Tu"], "title": "C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing", "comment": null, "summary": "Existing 2D-lifting-based 3D editing methods often encounter challenges\nrelated to inconsistency, stemming from the lack of view-consistent 2D editing\nmodels and the difficulty of ensuring consistent editing across multiple views.\nTo address these issues, we propose C3Editor, a controllable and consistent\n2D-lifting-based 3D editing framework. Given an original 3D representation and\na text-based editing prompt, our method selectively establishes a\nview-consistent 2D editing model to achieve superior 3D editing results. The\nprocess begins with the controlled selection of a ground truth (GT) view and\nits corresponding edited image as the optimization target, allowing for\nuser-defined manual edits. Next, we fine-tune the 2D editing model within the\nGT view and across multiple views to align with the GT-edited image while\nensuring multi-view consistency. To meet the distinct requirements of GT view\nfitting and multi-view consistency, we introduce separate LoRA modules for\ntargeted fine-tuning. Our approach delivers more consistent and controllable 2D\nand 3D editing results than existing 2D-lifting-based methods, outperforming\nthem in both qualitative and quantitative evaluations.", "AI": {"tldr": "C3Editor\u662f\u4e00\u4e2a\u53ef\u63a7\u4e14\u4e00\u81f4\u7684\u57fa\u4e8e2D\u63d0\u5347\u76843D\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5efa\u7acb\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e2D\u63d0\u5347\u76843D\u7f16\u8f91\u65b9\u6cd5\u7ecf\u5e38\u9047\u5230\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u7f3a\u4e4f\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\u4ee5\u53ca\u96be\u4ee5\u786e\u4fdd\u8de8\u591a\u4e2a\u89c6\u56fe\u7684\u4e00\u81f4\u7f16\u8f91\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u63a7\u5236\u9009\u62e9GT\u89c6\u56fe\u53ca\u5176\u7f16\u8f91\u56fe\u50cf\u4f5c\u4e3a\u4f18\u5316\u76ee\u6807\uff1b2\uff09\u5728GT\u89c6\u56fe\u548c\u591a\u4e2a\u89c6\u56fe\u4e2d\u5fae\u8c032D\u7f16\u8f91\u6a21\u578b\u4ee5\u5bf9\u9f50GT\u7f16\u8f91\u56fe\u50cf\uff1b3\uff09\u5f15\u5165\u5355\u72ec\u7684LoRA\u6a21\u5757\u5206\u522b\u5904\u7406GT\u89c6\u56fe\u62df\u5408\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u9700\u6c42\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e2D\u63d0\u5347\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u548c\u53ef\u63a7\u76842D\u548c3D\u7f16\u8f91\u7ed3\u679c\u3002", "conclusion": "C3Editor\u901a\u8fc7\u9009\u62e9\u6027\u5efa\u7acb\u89c6\u56fe\u4e00\u81f4\u76842D\u7f16\u8f91\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u7f16\u8f91\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u7f16\u8f91\u6548\u679c\u3002"}}
{"id": "2510.03561", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03561", "abs": "https://arxiv.org/abs/2510.03561", "authors": ["Adam Filipek"], "title": "Reactive Transformer (RxT) -- Stateful Real-Time Processing for Event-Driven Reactive Language Models", "comment": "25 pages, 13 figures", "summary": "The Transformer architecture has become the de facto standard for Large\nLanguage Models (LLMs), demonstrating remarkable capabilities in language\nunderstanding and generation. However, its application in conversational AI is\nfundamentally constrained by its stateless nature and the quadratic\ncomputational complexity ($O(L^2)$) with respect to sequence length $L$.\nCurrent models emulate memory by reprocessing an ever-expanding conversation\nhistory with each turn, leading to prohibitive costs and latency in long\ndialogues. This paper introduces the Reactive Transformer (RxT), a novel\narchitecture designed to overcome these limitations by shifting from a\ndata-driven to an event-driven paradigm. RxT processes each conversational turn\nas a discrete event in real-time, maintaining context in an integrated,\nfixed-size Short-Term Memory (STM) system. The architecture features a distinct\noperational cycle where a generator-decoder produces a response based on the\ncurrent query and the previous memory state, after which a memory-encoder and a\ndedicated Memory Attention network asynchronously update the STM with a\nrepresentation of the complete interaction. This design fundamentally alters\nthe scaling dynamics, reducing the total user-facing cost of a conversation\nfrom quadratic ($O(N^2 \\cdot T)$) to linear ($O(N \\cdot T)$) with respect to\nthe number of interactions $N$. By decoupling response generation from memory\nupdates, RxT achieves low latency, enabling truly real-time, stateful, and\neconomically viable long-form conversations. We validated our architecture with\na series of proof-of-concept experiments on synthetic data, demonstrating\nsuperior performance and constant-time inference latency compared to a baseline\nstateless model of comparable size.", "AI": {"tldr": "RxT\u662f\u4e00\u79cd\u65b0\u578bTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u8303\u5f0f\u89e3\u51b3\u4f20\u7edfTransformer\u5728\u5bf9\u8bddAI\u4e2d\u7684\u72b6\u6001\u7f3a\u5931\u548c\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u7684\u5b9e\u65f6\u957f\u5bf9\u8bdd\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u5bf9\u8bdd\u5e94\u7528\u4e2d\u5b58\u5728\u72b6\u6001\u7f3a\u5931\u548c\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u5bf9\u8bdd\u6210\u672c\u9ad8\u6602\u3001\u5ef6\u8fdf\u4e25\u91cd\u3002", "method": "RxT\u91c7\u7528\u4e8b\u4ef6\u9a71\u52a8\u67b6\u6784\uff0c\u5c06\u5bf9\u8bdd\u8f6e\u6b21\u4f5c\u4e3a\u79bb\u6563\u4e8b\u4ef6\u5904\u7406\uff0c\u96c6\u6210\u56fa\u5b9a\u5927\u5c0f\u7684\u77ed\u671f\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u89e3\u7801\u5668\u4ea7\u751f\u54cd\u5e94\uff0c\u5f02\u6b65\u66f4\u65b0\u8bb0\u5fc6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aRxT\u5728\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u5ef6\u8fdf\u6052\u5b9a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55\u3002", "conclusion": "RxT\u901a\u8fc7\u89e3\u8026\u54cd\u5e94\u751f\u6210\u548c\u8bb0\u5fc6\u66f4\u65b0\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u72b6\u6001\u4fdd\u6301\u4e14\u7ecf\u6d4e\u53ef\u884c\u7684\u957f\u5bf9\u8bdd\u7cfb\u7edf\u3002"}}
{"id": "2510.03348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03348", "abs": "https://arxiv.org/abs/2510.03348", "authors": ["Vlardimir Yugay", "Duy-Kien Nguyen", "Theo Gevers", "Cees G. M. Snoek", "Martin R. Oswald"], "title": "Visual Odometry with Transformers", "comment": null, "summary": "Modern monocular visual odometry methods typically combine pre-trained deep\nlearning components with optimization modules, resulting in complex pipelines\nthat rely heavily on camera calibration and hyperparameter tuning, and often\nstruggle in unseen real-world scenarios. Recent large-scale 3D models trained\non massive amounts of multi-modal data have partially alleviated these\nchallenges, providing generalizable dense reconstruction and camera pose\nestimation. Still, they remain limited in handling long videos and providing\naccurate per-frame estimates, which are required for visual odometry. In this\nwork, we demonstrate that monocular visual odometry can be addressed\neffectively in an end-to-end manner, thereby eliminating the need for\nhandcrafted components such as bundle adjustment, feature matching, camera\ncalibration, or dense 3D reconstruction. We introduce VoT, short for Visual\nodometry Transformer, which processes sequences of monocular frames by\nextracting features and modeling global relationships through temporal and\nspatial attention. Unlike prior methods, VoT directly predicts camera motion\nwithout estimating dense geometry and relies solely on camera poses for\nsupervision. The framework is modular and flexible, allowing seamless\nintegration of various pre-trained encoders as feature extractors. Experimental\nresults demonstrate that VoT scales effectively with larger datasets, benefits\nsubstantially from stronger pre-trained backbones, generalizes across diverse\ncamera motions and calibration settings, and outperforms traditional methods\nwhile running more than 3 times faster. The code will be released.", "AI": {"tldr": "\u63d0\u51faVoT\uff08\u89c6\u89c9\u91cc\u7a0b\u8ba1Transformer\uff09\uff0c\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u4f20\u7edf\u624b\u5de5\u7ec4\u4ef6\u5982\u6346\u7ed1\u8c03\u6574\u3001\u7279\u5f81\u5339\u914d\u3001\u76f8\u673a\u6807\u5b9a\u6216\u5bc6\u96c63D\u91cd\u5efa\uff0c\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\u3002", "motivation": "\u4f20\u7edf\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\u548c\u4f18\u5316\u6a21\u5757\uff0c\u5f62\u6210\u590d\u6742\u6d41\u6c34\u7ebf\uff0c\u4e25\u91cd\u4f9d\u8d56\u76f8\u673a\u6807\u5b9a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u5927\u89c4\u6a213D\u6a21\u578b\u867d\u80fd\u63d0\u4f9b\u6cdb\u5316\u6027\u91cd\u5efa\u548c\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4f46\u5904\u7406\u957f\u89c6\u9891\u548c\u63d0\u4f9b\u51c6\u786e\u9010\u5e27\u4f30\u8ba1\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "VoT\u901a\u8fc7\u65f6\u7a7a\u6ce8\u610f\u529b\u5904\u7406\u5355\u76ee\u5e27\u5e8f\u5217\uff0c\u63d0\u53d6\u7279\u5f81\u5e76\u5efa\u6a21\u5168\u5c40\u5173\u7cfb\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u9884\u6d4b\u76f8\u673a\u8fd0\u52a8\uff0c\u65e0\u9700\u4f30\u8ba1\u5bc6\u96c6\u51e0\u4f55\u7ed3\u6784\uff0c\u4ec5\u4f9d\u8d56\u76f8\u673a\u4f4d\u59ff\u8fdb\u884c\u76d1\u7763\u3002\u6846\u67b6\u6a21\u5757\u5316\u7075\u6d3b\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5404\u79cd\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVoT\u80fd\u6709\u6548\u6269\u5c55\u5230\u66f4\u5927\u6570\u636e\u96c6\uff0c\u4ece\u66f4\u5f3a\u7684\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\u4e2d\u663e\u8457\u53d7\u76ca\uff0c\u5728\u4e0d\u540c\u76f8\u673a\u8fd0\u52a8\u548c\u6807\u5b9a\u8bbe\u7f6e\u4e0b\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u4e14\u8fd0\u884c\u901f\u5ea6\u5feb3\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u5355\u76ee\u89c6\u89c9\u91cc\u7a0b\u8ba1\u53ef\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u5f0f\u6709\u6548\u89e3\u51b3\uff0c\u65e0\u9700\u4f20\u7edf\u624b\u5de5\u7ec4\u4ef6\uff0cVoT\u6846\u67b6\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002"}}
{"id": "2510.04637", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04637", "abs": "https://arxiv.org/abs/2510.04637", "authors": ["Zeyi Zhang", "Yanju Zhou", "Heyuan Yao", "Tenglong Ao", "Xiaohang Zhan", "Libin Liu"], "title": "Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents", "comment": "SIGGRAPH ASIA 2025 (Conference Track); Project page:\n  https://pku-mocca.github.io/Social-Agent-Page/", "summary": "We present Social Agent, a novel framework for synthesizing realistic and\ncontextually appropriate co-speech nonverbal behaviors in dyadic conversations.\nIn this framework, we develop an agentic system driven by a Large Language\nModel (LLM) to direct the conversation flow and determine appropriate\ninteractive behaviors for both participants. Additionally, we propose a novel\ndual-person gesture generation model based on an auto-regressive diffusion\nmodel, which synthesizes coordinated motions from speech signals. The output of\nthe agentic system is translated into high-level guidance for the gesture\ngenerator, resulting in realistic movement at both the behavioral and motion\nlevels. Furthermore, the agentic system periodically examines the movements of\ninterlocutors and infers their intentions, forming a continuous feedback loop\nthat enables dynamic and responsive interactions between the two participants.\nUser studies and quantitative evaluations show that our model significantly\nimproves the quality of dyadic interactions, producing natural, synchronized\nnonverbal behaviors.", "AI": {"tldr": "Social Agent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53cc\u4eba\u5bf9\u8bdd\u4e2d\u771f\u5b9e\u4e14\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u4f34\u968f\u8bed\u97f3\u975e\u8bed\u8a00\u884c\u4e3a\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u53cc\u4eba\u624b\u52bf\u751f\u6210\u6a21\u578b\u5b9e\u73b0\u534f\u8c03\u7684\u8fd0\u52a8\u5408\u6210\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53cc\u4eba\u5bf9\u8bdd\u4e2d\u975e\u8bed\u8a00\u884c\u4e3a\u751f\u6210\u7684\u6311\u6218\uff0c\u9700\u8981\u521b\u5efa\u80fd\u591f\u4ea7\u751f\u771f\u5b9e\u3001\u540c\u6b65\u4e14\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u4f34\u968f\u8bed\u97f3\u624b\u52bf\u548c\u52a8\u4f5c\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u6765\u63a7\u5236\u5bf9\u8bdd\u6d41\u7a0b\u548c\u786e\u5b9a\u4ea4\u4e92\u884c\u4e3a\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u81ea\u56de\u5f52\u6269\u6563\u6a21\u578b\u7684\u53cc\u4eba\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u4ece\u8bed\u97f3\u4fe1\u53f7\u5408\u6210\u534f\u8c03\u8fd0\u52a8\u3002", "result": "\u7528\u6237\u7814\u7a76\u548c\u5b9a\u91cf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u4eba\u4ea4\u4e92\u8d28\u91cf\uff0c\u4ea7\u751f\u4e86\u81ea\u7136\u3001\u540c\u6b65\u7684\u975e\u8bed\u8a00\u884c\u4e3a\u3002", "conclusion": "Social Agent\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u54cd\u5e94\u7684\u53cc\u4eba\u4ea4\u4e92\uff0c\u4e3a\u975e\u8bed\u8a00\u884c\u4e3a\u5408\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03577", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.03577", "abs": "https://arxiv.org/abs/2510.03577", "authors": ["Ikram Belmadani", "Parisa Nazari Hashemi", "Thomas Sebbag", "Benoit Favre", "Guillaume Fortier", "Solen Quiniou", "Emmanuel Morin", "Richard Dufour"], "title": "LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction", "comment": "in French language", "summary": "This work presents our participation in the EvalLLM 2025 challenge on\nbiomedical Named Entity Recognition (NER) and health event extraction in French\n(few-shot setting). For NER, we propose three approaches combining large\nlanguage models (LLMs), annotation guidelines, synthetic data, and\npost-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating\nautomatic selection of 10 examples and a summary of the annotation guidelines\ninto the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic\ncorpus and then verified by an LLM in post-processing, and (3) the open LLM\nLLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event\nextraction uses the same ICL strategy with GPT-4.1, reusing the guideline\nsummary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for\nNER and 15.02% for event extraction, highlighting the importance of\nwell-crafted prompting to maximize performance in very low-resource scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728EvalLLM 2025\u6311\u6218\u8d5b\u4e2d\u9488\u5bf9\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u5065\u5eb7\u4e8b\u4ef6\u62bd\u53d6\u7684\u65b9\u6cd5\uff0c\u4e3b\u8981\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u3001\u5408\u6210\u6570\u636e\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "motivation": "\u89e3\u51b3\u6cd5\u8bed\u751f\u7269\u533b\u5b66\u6587\u672c\u4e2d\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e8b\u4ef6\u62bd\u53d6\u7684\u4f4e\u8d44\u6e90\u573a\u666f\u95ee\u9898\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e09\u79cdNER\u65b9\u6cd5\uff1a(1) GPT-4.1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u81ea\u52a8\u9009\u62e910\u4e2a\u793a\u4f8b\u5e76\u6574\u5408\u6807\u6ce8\u6307\u5357\uff1b(2) GLiNER\u7cfb\u7edf\u5728\u5408\u6210\u8bed\u6599\u4e0a\u5fae\u8c03\u540e\u7ecfLLM\u540e\u5904\u7406\u9a8c\u8bc1\uff1b(3) LLaMA-3.1-8B-Instruct\u5728\u5408\u6210\u8bed\u6599\u4e0a\u5fae\u8c03\u3002\u4e8b\u4ef6\u62bd\u53d6\u91c7\u7528\u4e0eNER\u76f8\u540c\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u3002", "result": "GPT-4.1\u5728NER\u4efb\u52a1\u4e0a\u83b7\u5f9761.53%\u7684\u5b8fF1\u5206\u6570\uff0c\u5728\u4e8b\u4ef6\u62bd\u53d6\u4efb\u52a1\u4e0a\u83b7\u5f9715.02%\u7684\u5b8fF1\u5206\u6570\uff0c\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5728\u6781\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u5de5\u7a0b\u5bf9\u4e8e\u6700\u5927\u5316\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cGPT-4.1\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.03352", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03352", "abs": "https://arxiv.org/abs/2510.03352", "authors": ["Mahdi Farahbakhsh", "Vishnu Teja Kunde", "Dileep Kalathil", "Krishna Narayanan", "Jean-Francois Chamberland"], "title": "Inference-Time Search using Side Information for Diffusion-based Image Reconstruction", "comment": null, "summary": "Diffusion models have emerged as powerful priors for solving inverse\nproblems. However, existing approaches typically overlook side information that\ncould significantly improve reconstruction quality, especially in severely\nill-posed settings. In this work, we propose a novel inference-time search\nalgorithm that guides the sampling process using the side information in a\nmanner that balances exploration and exploitation. This enables more accurate\nand reliable reconstructions, providing an alternative to the gradient-based\nguidance that is prone to reward-hacking artifacts. Our approach can be\nseamlessly integrated into a wide range of existing diffusion-based image\nreconstruction pipelines. Through extensive experiments on a number of inverse\nproblems, such as box inpainting, super-resolution, and various deblurring\ntasks including motion, Gaussian, nonlinear, and blind deblurring, we show that\nour approach consistently improves the qualitative and quantitative performance\nof diffusion-based image reconstruction algorithms. We also show the superior\nperformance of our approach with respect to other baselines, including reward\ngradient-based guidance algorithms. The code is available at\n\\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this\nrepository}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4fa7\u4fe1\u606f\u7684\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u56fe\u50cf\u91cd\u5efa\u8d28\u91cf\uff0c\u901a\u8fc7\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u6765\u907f\u514d\u68af\u5ea6\u5f15\u5bfc\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u4f2a\u5f71\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4fa7\u4fe1\u606f\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5728\u4e25\u91cd\u4e0d\u9002\u5b9a\u95ee\u9898\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u5bb9\u6613\u4ea7\u751f\u5956\u52b1\u9ed1\u5ba2\u4f2a\u5f71\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5229\u7528\u4fa7\u4fe1\u606f\u8fdb\u884c\u5f15\u5bfc\uff0c\u5e73\u8861\u63a2\u7d22\u548c\u5229\u7528\u3002\u8be5\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6269\u6563\u57fa\u56fe\u50cf\u91cd\u5efa\u6d41\u7a0b\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u9006\u95ee\u9898\uff08\u6846\u5185\u4fee\u590d\u3001\u8d85\u5206\u8fa8\u7387\u3001\u8fd0\u52a8/\u9ad8\u65af/\u975e\u7ebf\u6027/\u76f2\u53bb\u6a21\u7cca\uff09\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u63d0\u5347\u6269\u6563\u57fa\u56fe\u50cf\u91cd\u5efa\u7b97\u6cd5\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4fa7\u4fe1\u606f\u641c\u7d22\u65b9\u6cd5\u4e3a\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.04999", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04999", "abs": "https://arxiv.org/abs/2510.04999", "authors": ["Nilay Kumar", "Priyansh Bhandari", "G. Maragatham"], "title": "Bridging Text and Video Generation: A Survey", "comment": null, "summary": "Text-to-video (T2V) generation technology holds potential to transform\nmultiple domains such as education, marketing, entertainment, and assistive\ntechnologies for individuals with visual or reading comprehension challenges,\nby creating coherent visual content from natural language prompts. From its\ninception, the field has advanced from adversarial models to diffusion-based\nmodels, yielding higher-fidelity, temporally consistent outputs. Yet challenges\npersist, such as alignment, long-range coherence, and computational efficiency.\nAddressing this evolving landscape, we present a comprehensive survey of\ntext-to-video generative models, tracing their development from early GANs and\nVAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these\nmodels work, what limitations they addressed in their predecessors, and why\nshifts toward new architectural paradigms were necessary to overcome challenges\nin quality, coherence, and control. We provide a systematic account of the\ndatasets, which the surveyed text-to-video models were trained and evaluated\non, and, to support reproducibility and assess the accessibility of training\nsuch models, we detail their training configurations, including their hardware\nspecifications, GPU counts, batch sizes, learning rates, optimizers, epochs,\nand other key hyperparameters. Further, we outline the evaluation metrics\ncommonly used for evaluating such models and present their performance across\nstandard benchmarks, while also discussing the limitations of these metrics and\nthe emerging shift toward more holistic, perception-aligned evaluation\nstrategies. Finally, drawing from our analysis, we outline the current open\nchallenges and propose a few promising future directions, laying out a\nperspective for future researchers to explore and build upon in advancing T2V\nresearch and applications.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u4ece\u65e9\u671f\u7684GAN\u548cVAE\u6a21\u578b\u5230\u6df7\u5408\u6269\u6563-Transformer\u67b6\u6784\uff0c\u5206\u6790\u4e86\u6280\u672f\u53d1\u5c55\u5386\u7a0b\u3001\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u914d\u7f6e\u3001\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u5f53\u524d\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u5728\u6559\u80b2\u3001\u8425\u9500\u3001\u5a31\u4e50\u548c\u8f85\u52a9\u6280\u672f\u7b49\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u5bf9\u9f50\u3001\u957f\u7a0b\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7684\u53d1\u5c55\u5386\u7a0b\u548c\u6280\u672f\u73b0\u72b6\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u8ffd\u6eaf\u4ece\u65e9\u671fGAN\u548cVAE\u6a21\u578b\u5230\u6df7\u5408\u6269\u6563-Transformer\u67b6\u6784\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u8be6\u7ec6\u5206\u6790\u6a21\u578b\u5de5\u4f5c\u539f\u7406\u3001\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u914d\u7f6e\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u8ba8\u8bba\u4e86\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u5411\u66f4\u5168\u9762\u3001\u611f\u77e5\u5bf9\u9f50\u7684\u8bc4\u4f30\u7b56\u7565\u8f6c\u53d8\u7684\u8d8b\u52bf\u3002", "conclusion": "\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6280\u672f\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u5f00\u653e\u6311\u6218\u3002\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6280\u672f\u53d1\u5c55\u89c6\u89d2\u548c\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8T2V\u7814\u7a76\u548c\u5e94\u7528\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.03595", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03595", "abs": "https://arxiv.org/abs/2510.03595", "authors": ["Haikang Deng", "Po-Nien Kung", "Nanyun Peng"], "title": "Decoupling Task-Solving and Output Formatting in LLM Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly adept at following instructions\ncontaining task descriptions to solve complex problems, such as mathematical\nreasoning and automatic evaluation (LLM-as-a-Judge). However, as prompts grow\nmore complex, models often struggle to adhere to all instructions. This\ndifficulty is especially common when instructive prompts intertwine reasoning\ndirectives -- specifying what the model should solve -- with rigid formatting\nrequirements that dictate how the solution must be presented. The entanglement\ncreates competing goals for the model, suggesting that more explicit separation\nof these two aspects could lead to improved performance. To this front, we\nintroduce Deco-G, a decoding framework that explicitly decouples format\nadherence from task solving. Deco-G handles format compliance with a separate\ntractable probabilistic model (TPM), while prompts LLMs with only task\ninstructions. At each decoding step, Deco-G combines next token probabilities\nfrom the LLM with the TPM calculated format compliance likelihood to form the\noutput probability. To make this approach both practical and scalable for\nmodern instruction-tuned LLMs, we introduce three key innovations:\ninstruction-aware distillation, a flexible trie-building algorithm, and HMM\nstate pruning for computational efficiency. We demonstrate the effectiveness of\nDeco-G across a wide range of tasks with diverse format requirements, including\nmathematical reasoning, LLM-as-a-judge, and event argument extraction. Overall,\nour approach yields 1.0% to 6.0% relative gain over regular prompting practice\nwith guaranteed format compliance.", "AI": {"tldr": "Deco-G\u662f\u4e00\u4e2a\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u683c\u5f0f\u9075\u5faa\u4e0e\u4efb\u52a1\u89e3\u51b3\u89e3\u8026\uff0c\u4f7f\u7528\u72ec\u7acb\u6982\u7387\u6a21\u578b\u5904\u7406\u683c\u5f0f\u8981\u6c42\uff0c\u8ba9LLM\u4e13\u6ce8\u4e8e\u4efb\u52a1\u6307\u4ee4\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u63d0\u793a\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u63d0\u793a\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742\uff0cLLM\u96be\u4ee5\u540c\u65f6\u9075\u5faa\u6240\u6709\u6307\u4ee4\uff0c\u7279\u522b\u662f\u5f53\u63a8\u7406\u6307\u4ee4\u4e0e\u4e25\u683c\u683c\u5f0f\u8981\u6c42\u4ea4\u7ec7\u5728\u4e00\u8d77\u65f6\uff0c\u8fd9\u79cd\u7ea0\u7f20\u4f1a\u4e3a\u6a21\u578b\u521b\u9020\u76f8\u4e92\u7ade\u4e89\u7684\u76ee\u6807\u3002", "method": "Deco-G\u4f7f\u7528\u72ec\u7acb\u53ef\u5904\u7406\u7684\u6982\u7387\u6a21\u578b(TPM)\u5904\u7406\u683c\u5f0f\u9075\u5faa\uff0c\u540c\u65f6\u4ec5\u5411LLM\u63d0\u4f9b\u4efb\u52a1\u6307\u4ee4\u3002\u5728\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u4e2d\uff0c\u5c06LLM\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u6982\u7387\u4e0eTPM\u8ba1\u7b97\u7684\u683c\u5f0f\u9075\u5faa\u4f3c\u7136\u76f8\u7ed3\u5408\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001LLM-as-a-judge\u548c\u4e8b\u4ef6\u53c2\u6570\u63d0\u53d6\u7b49\u591a\u6837\u5316\u683c\u5f0f\u8981\u6c42\u7684\u4efb\u52a1\u4e2d\uff0cDeco-G\u76f8\u6bd4\u5e38\u89c4\u63d0\u793a\u65b9\u6cd5\u83b7\u5f97\u4e861.0%\u52306.0%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u4fdd\u8bc1\u683c\u5f0f\u5408\u89c4\u3002", "conclusion": "\u660e\u786e\u5206\u79bb\u683c\u5f0f\u9075\u5faa\u548c\u4efb\u52a1\u89e3\u51b3\u53ef\u4ee5\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u63d0\u793a\u4e0b\u7684\u6027\u80fd\uff0cDeco-G\u6846\u67b6\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03353", "categories": ["cs.CV", "I.4.9; I.5.0; H.3.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.03353", "abs": "https://arxiv.org/abs/2510.03353", "authors": ["Larissa S. Gomes", "Gustavo P. Almeida", "Bryan U. Moreira", "Marco Quiroz", "Breno Xavier", "Lucas Soares", "Stephanie L. Bri\u00e3o", "Felipe G. Oliveira", "Paulo L. J. Drews-Jr"], "title": "Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications", "comment": "Published in the Conference on Graphics, Patterns and Images\n  (SIBGRAPI). This 4-page paper presents a timeline of publicly available\n  datasets up to the year 2025", "summary": "Sonar images are relevant for advancing underwater exploration, autonomous\nnavigation, and ecosystem monitoring. However, the progress depends on data\navailability. The scarcity of publicly available, well-annotated sonar image\ndatasets creates a significant bottleneck for the development of robust machine\nlearning models. This paper presents a comprehensive and concise review of the\ncurrent landscape of sonar image datasets, seeking not only to catalog existing\nresources but also to contextualize them, identify gaps, and provide a clear\nroadmap, serving as a base guide for researchers of any kind who wish to start\nor advance in the field of underwater acoustic data analysis. We mapped\npublicly accessible datasets across various sonar modalities, including Side\nScan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),\nMultibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar\n(DIDSON). An analysis was conducted on applications such as classification,\ndetection, segmentation, and 3D reconstruction. This work focuses on\nstate-of-the-art advancements, incorporating newly released datasets. The\nfindings are synthesized into a master table and a chronological timeline,\noffering a clear and accessible comparison of characteristics, sizes, and\nannotation details datasets.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5bf9\u58f0\u7eb3\u56fe\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u4e0d\u540c\u6a21\u6001\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u5e94\u7528\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u5bf9\u6bd4\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u58f0\u7eb3\u56fe\u50cf\u5bf9\u6c34\u4e0b\u63a2\u7d22\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u516c\u5f00\u3001\u6807\u6ce8\u826f\u597d\u7684\u6570\u636e\u96c6\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u6620\u5c04\u4e0d\u540c\u58f0\u7eb3\u6a21\u6001\u7684\u516c\u5f00\u6570\u636e\u96c6\uff08SSS\u3001FLS\u3001SAS\u3001MBES\u3001DIDSON\uff09\uff0c\u5206\u6790\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u548c3D\u91cd\u5efa\u7b49\u5e94\u7528\uff0c\u5e76\u6574\u5408\u6210\u4e3b\u8868\u548c\u65f6\u5e8f\u56fe\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u6570\u636e\u96c6\u7279\u5f81\u3001\u89c4\u6a21\u548c\u6807\u6ce8\u7ec6\u8282\u7684\u7efc\u5408\u5bf9\u6bd4\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a7a\u767d\u548c\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u6c34\u4e0b\u58f0\u5b66\u6570\u636e\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u7840\u6307\u5357\uff0c\u660e\u786e\u4e86\u672a\u6765\u6570\u636e\u96c6\u53d1\u5c55\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2510.05081", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05081", "abs": "https://arxiv.org/abs/2510.05081", "authors": ["Ronen Kamenetsky", "Sara Dorfman", "Daniel Garibi", "Roni Paiss", "Or Patashnik", "Daniel Cohen-Or"], "title": "SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder", "comment": "Project page at: https://ronen94.github.io/SAEdit/", "summary": "Large-scale text-to-image diffusion models have become the backbone of modern\nimage editing, yet text prompts alone do not offer adequate control over the\nediting process. Two properties are especially desirable: disentanglement,\nwhere changing one attribute does not unintentionally alter others, and\ncontinuous control, where the strength of an edit can be smoothly adjusted. We\nintroduce a method for disentangled and continuous editing through token-level\nmanipulation of text embeddings. The edits are applied by manipulating the\nembeddings along carefully chosen directions, which control the strength of the\ntarget attribute. To identify such directions, we employ a Sparse Autoencoder\n(SAE), whose sparse latent space exposes semantically isolated dimensions. Our\nmethod operates directly on text embeddings without modifying the diffusion\nprocess, making it model agnostic and broadly applicable to various image\nsynthesis backbones. Experiments show that it enables intuitive and efficient\nmanipulations with continuous control across diverse attributes and domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u7684token\u7ea7\u64cd\u4f5c\u5b9e\u73b0\u89e3\u8026\u548c\u8fde\u7eed\u56fe\u50cf\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u8bed\u4e49\u9694\u79bb\u7684\u7f16\u8f91\u65b9\u5411\uff0c\u65e0\u9700\u4fee\u6539\u6269\u6563\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4ec5\u901a\u8fc7\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u63d0\u4f9b\u8db3\u591f\u7684\u7f16\u8f91\u63a7\u5236\uff0c\u7279\u522b\u9700\u8981\u89e3\u8026\u7f16\u8f91\uff08\u6539\u53d8\u4e00\u4e2a\u5c5e\u6027\u4e0d\u5f71\u54cd\u5176\u4ed6\u5c5e\u6027\uff09\u548c\u8fde\u7eed\u63a7\u5236\uff08\u5e73\u6ed1\u8c03\u6574\u7f16\u8f91\u5f3a\u5ea6\uff09\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u64cd\u4f5c\u6587\u672c\u5d4c\u5165\u6cbf\u7cbe\u5fc3\u9009\u62e9\u7684\u7f16\u8f91\u65b9\u5411\u6765\u63a7\u5236\u76ee\u6807\u5c5e\u6027\u7684\u5f3a\u5ea6\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u8bed\u4e49\u9694\u79bb\u7684\u7ef4\u5ea6\uff0c\u76f4\u63a5\u5728\u6587\u672c\u5d4c\u5165\u5c42\u9762\u64cd\u4f5c\u800c\u4e0d\u4fee\u6539\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u540c\u5c5e\u6027\u548c\u9886\u57df\u4e2d\u5b9e\u73b0\u76f4\u89c2\u9ad8\u6548\u7684\u8fde\u7eed\u63a7\u5236\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u89e3\u8026\u548c\u8fde\u7eed\u7684\u56fe\u50cf\u7f16\u8f91\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u56fe\u50cf\u5408\u6210\u6846\u67b6\u3002"}}
{"id": "2510.03611", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03611", "abs": "https://arxiv.org/abs/2510.03611", "authors": ["Raquib Bin Yousuf", "Aadyant Khatri", "Shengzhe Xu", "Mandar Sharma", "Naren Ramakrishnan"], "title": "Can an LLM Induce a Graph? Investigating Memory Drift and Context Length", "comment": "2025 IEEE International Conference on Knowledge Graph (ICKG)", "summary": "Recently proposed evaluation benchmarks aim to characterize the effective\ncontext length and the forgetting tendencies of large language models (LLMs).\nHowever, these benchmarks often rely on simplistic 'needle in a haystack'\nretrieval or continuation tasks that may not accurately reflect the performance\nof these models in information-dense scenarios. Thus, rather than simple next\ntoken prediction, we argue for evaluating these models on more complex\nreasoning tasks that requires them to induce structured relational knowledge\nfrom the text - such as graphs from potentially noisy natural language content.\nWhile the input text can be viewed as generated in terms of a graph, its\nstructure is not made explicit and connections must be induced from distributed\ntextual cues, separated by long contexts and interspersed with irrelevant\ninformation. Our findings reveal that LLMs begin to exhibit memory drift and\ncontextual forgetting at much shorter effective lengths when tasked with this\nform of relational reasoning, compared to what existing benchmarks suggest.\nWith these findings, we offer recommendations for the optimal use of popular\nLLMs for complex reasoning tasks. We further show that even models specialized\nfor reasoning, such as OpenAI o1, remain vulnerable to early memory drift in\nthese settings. These results point to significant limitations in the models'\nability to abstract structured knowledge from unstructured input and highlight\nthe need for architectural adaptations to improve long-range reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u590d\u6742\u7684\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u800c\u975e\u7b80\u5355\u7684\u68c0\u7d22\u4efb\u52a1\u6765\u6d4b\u8bd5LLMs\u7684\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u9057\u5fd8\u503e\u5411\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5173\u7cfb\u63a8\u7406\u4e2d\u6bd4\u73b0\u6709\u57fa\u51c6\u663e\u793a\u66f4\u65e9\u51fa\u73b0\u8bb0\u5fc6\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u7b80\u5355\u7684'\u5927\u6d77\u635e\u9488'\u68c0\u7d22\u4efb\u52a1\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620LLMs\u5728\u4fe1\u606f\u5bc6\u96c6\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u6765\u8bc4\u4f30\u6a21\u578b\u4ece\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5173\u7cfb\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u590d\u6742\u7684\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u4ece\u53ef\u80fd\u5305\u542b\u566a\u58f0\u7684\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\u4e2d\u5f52\u7eb3\u51fa\u56fe\u7ed3\u6784\u77e5\u8bc6\uff0c\u8fde\u63a5\u5fc5\u987b\u4ece\u5206\u5e03\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u6587\u672c\u7ebf\u7d22\u4e2d\u63a8\u5bfc\uff0c\u5e76\u5939\u6742\u65e0\u5173\u4fe1\u606f\u3002", "result": "LLMs\u5728\u8fd9\u79cd\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u57fa\u51c6\u5efa\u8bae\u7684\u66f4\u77ed\u6709\u6548\u957f\u5ea6\u5c31\u5f00\u59cb\u8868\u73b0\u51fa\u8bb0\u5fc6\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u9057\u5fd8\uff0c\u5373\u4f7f\u662f\u4e13\u95e8\u7528\u4e8e\u63a8\u7406\u7684\u6a21\u578b\u5982OpenAI o1\u4e5f\u5bb9\u6613\u53d7\u5230\u65e9\u671f\u8bb0\u5fc6\u6f02\u79fb\u7684\u5f71\u54cd\u3002", "conclusion": "\u6a21\u578b\u4ece\u975e\u7ed3\u6784\u5316\u8f93\u5165\u4e2d\u62bd\u8c61\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u80fd\u529b\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8981\u67b6\u6784\u6539\u8fdb\u6765\u63d0\u5347\u957f\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2dLLMs\u7684\u6700\u4f73\u4f7f\u7528\u63d0\u4f9b\u5efa\u8bae\u3002"}}
{"id": "2510.03356", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.03356", "abs": "https://arxiv.org/abs/2510.03356", "authors": ["Ziyang Chen", "Yuta Itoh", "Kaan Ak\u015fit"], "title": "Learned Display Radiance Fields with Lensless Cameras", "comment": null, "summary": "Calibrating displays is a basic and regular task that content creators must\nperform to maintain optimal visual experience, yet it remains a troublesome\nissue. Measuring display characteristics from different viewpoints often\nrequires specialized equipment and a dark room, making it inaccessible to most\nusers. To avoid specialized hardware requirements in display calibrations, our\nwork co-designs a lensless camera and an Implicit Neural Representation based\nalgorithm for capturing display characteristics from various viewpoints. More\nspecifically, our pipeline enables efficient reconstruction of light fields\nemitted from a display from a viewing cone of 46.6{\\deg} X 37.6{\\deg}. Our\nemerging pipeline paves the initial steps towards effortless display\ncalibration and characterization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u4e1a\u8bbe\u5907\u7684\u663e\u793a\u5668\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u900f\u955c\u76f8\u673a\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7b97\u6cd5\u4ece\u591a\u89c6\u89d2\u6355\u6349\u663e\u793a\u5668\u7279\u6027\u3002", "motivation": "\u4f20\u7edf\u663e\u793a\u5668\u6821\u51c6\u9700\u8981\u4e13\u4e1a\u8bbe\u5907\u548c\u6697\u5ba4\u73af\u5883\uff0c\u5bf9\u5927\u591a\u6570\u7528\u6237\u6765\u8bf4\u96be\u4ee5\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u4e13\u4e1a\u786c\u4ef6\u7684\u663e\u793a\u5668\u6821\u51c6\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8054\u5408\u8bbe\u8ba1\u65e0\u900f\u955c\u76f8\u673a\u548c\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u7b97\u6cd5\uff0c\u4ece46.6\u00b0\u00d737.6\u00b0\u7684\u89c6\u89d2\u9525\u5185\u9ad8\u6548\u91cd\u5efa\u663e\u793a\u5668\u53d1\u51fa\u7684\u5149\u573a\u3002", "result": "\u80fd\u591f\u4ece\u591a\u89c6\u89d2\u6709\u6548\u6355\u6349\u663e\u793a\u5668\u7279\u6027\uff0c\u5b9e\u73b0\u663e\u793a\u5668\u5149\u573a\u7684\u91cd\u5efa\u3002", "conclusion": "\u8be5\u65b0\u5174\u6d41\u7a0b\u4e3a\u5b9e\u73b0\u8f7b\u677e\u663e\u793a\u5668\u6821\u51c6\u548c\u7279\u6027\u63cf\u8ff0\u8fc8\u51fa\u4e86\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2510.05097", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05097", "abs": "https://arxiv.org/abs/2510.05097", "authors": ["Robin Courant", "Xi Wang", "David Loiseaux", "Marc Christie", "Vicky Kalogeiton"], "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation", "comment": "Project page:\n  https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/", "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6587\u672c\u6761\u4ef6\u8054\u5408\u751f\u6210\u6846\u67b6\uff0c\u5c06\u4eba\u4f53\u8fd0\u52a8\u548c\u76f8\u673a\u8f68\u8ff9\u4f5c\u4e3a\u5185\u5728\u5173\u8054\u7684\u6a21\u6001\u5171\u540c\u751f\u6210\uff0c\u901a\u8fc7\u5c4f\u5e55\u7a7a\u95f4\u6295\u5f71\u4fdd\u6301\u4e00\u81f4\u7684\u6784\u56fe\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4eba\u4f53\u8fd0\u52a8\u548c\u76f8\u673a\u8f68\u8ff9\u5206\u5f00\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u7535\u5f71\u6444\u5f71\u4e2d\u6f14\u5458\u8868\u6f14\u4e0e\u6444\u50cf\u673a\u5de5\u4f5c\u7684\u7d27\u5bc6\u4e92\u52a8\u5173\u7cfb\u3002", "method": "\u8bbe\u8ba1\u8054\u5408\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u53d8\u6362\u5c06\u4eba\u4f53\u548c\u76f8\u673a\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u6784\u56fe\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u91c7\u6837\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728DiT\u548cMAR\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u6784\u56fe\u4e00\u81f4\u7684\u4eba\u4f53-\u76f8\u673a\u8fd0\u52a8\uff0c\u540c\u65f6\u5728\u4e24\u79cd\u6a21\u6001\u7684\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u7535\u5f71\u6444\u5f71\u610f\u4e49\u4e0a\u66f4\u6709\u610f\u4e49\u7684\u6784\u56fe\u65b9\u9762\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e3a\u8054\u5408\u751f\u6210\u4efb\u52a1\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2510.03639", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03639", "abs": "https://arxiv.org/abs/2510.03639", "authors": ["Liming Wang", "Junrui Ni", "Kai-Wei Chang", "Saurabhchand Bhati", "David Harwath", "Mark Hasegawa-Johnson", "James R. Glass"], "title": "Towards Unsupervised Speech Recognition at the Syllable-Level", "comment": null, "summary": "Training speech recognizers with unpaired speech and text -- known as\nunsupervised speech recognition (UASR) -- is a crucial step toward extending\nASR to low-resource languages in the long-tail distribution and enabling\nmultimodal learning from non-parallel data. However, existing approaches based\non phones often rely on costly resources such as grapheme-to-phoneme converters\n(G2Ps) and struggle to generalize to languages with ambiguous phoneme\nboundaries due to training instability. In this paper, we address both\nchallenges by introducing a syllable-level UASR framework based on masked\nlanguage modeling, which avoids the need for G2P and the instability of\nGAN-based methods. Our approach achieves up to a 40\\% relative reduction in\ncharacter error rate (CER) on LibriSpeech and generalizes effectively to\nMandarin, a language that has remained particularly difficult for prior\nmethods. Code will be released upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u97f3\u8282\u7ea7\u522b\u7684\u65e0\u76d1\u7763\u8bed\u97f3\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u907f\u514d\u4e86G2P\u8f6c\u6362\u5668\u7684\u9700\u6c42\uff0c\u5728LibriSpeech\u4e0a\u5b9e\u73b0\u4e8640%\u76f8\u5bf9\u5b57\u7b26\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u5e76\u80fd\u6709\u6548\u6cdb\u5316\u5230\u4e2d\u6587\u7b49\u56f0\u96be\u8bed\u8a00\u3002", "motivation": "\u89e3\u51b3\u65e0\u76d1\u7763\u8bed\u97f3\u8bc6\u522b\u4e2d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56G2P\u8f6c\u6362\u5668\u3001\u5728\u97f3\u7d20\u8fb9\u754c\u6a21\u7cca\u7684\u8bed\u8a00\u4e2d\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u6269\u5c55ASR\u5230\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4ece\u975e\u5e73\u884c\u6570\u636e\u4e2d\u8fdb\u884c\u591a\u6a21\u6001\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u7684\u97f3\u8282\u7ea7\u522b\u65e0\u76d1\u7763\u8bed\u97f3\u8bc6\u522b\u6846\u67b6\uff0c\u907f\u514d\u4e86GAN\u65b9\u6cd5\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u65e0\u9700G2P\u8f6c\u6362\u5668\u3002", "result": "\u5728LibriSpeech\u4e0a\u5b9e\u73b0\u9ad8\u8fbe40%\u7684\u76f8\u5bf9\u5b57\u7b26\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u5728\u4e2d\u6587\u7b49\u5148\u524d\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u8bed\u8a00\u4e0a\u6709\u6548\u6cdb\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u97f3\u8282\u7ea7\u522bUASR\u6846\u67b6\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03361", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03361", "abs": "https://arxiv.org/abs/2510.03361", "authors": ["Ali Kayyam", "Anusha Madan Gopal", "M. Anthony Lewis"], "title": "Provenance Networks: End-to-End Exemplar-Based Explainability", "comment": null, "summary": "We introduce provenance networks, a novel class of neural models designed to\nprovide end-to-end, training-data-driven explainability. Unlike conventional\npost-hoc methods, provenance networks learn to link each prediction directly to\nits supporting training examples as part of the model's normal operation,\nembedding interpretability into the architecture itself. Conceptually, the\nmodel operates similarly to a learned KNN, where each output is justified by\nconcrete exemplars weighted by relevance in the feature space. This approach\nfacilitates systematic investigations of the trade-off between memorization and\ngeneralization, enables verification of whether a given input was included in\nthe training set, aids in the detection of mislabeled or anomalous data points,\nenhances resilience to input perturbations, and supports the identification of\nsimilar inputs contributing to the generation of a new data point. By jointly\noptimizing the primary task and the explainability objective, provenance\nnetworks offer insights into model behavior that traditional deep networks\ncannot provide. While the model introduces additional computational cost and\ncurrently scales to moderately sized datasets, it provides a complementary\napproach to existing explainability techniques. In particular, it addresses\ncritical challenges in modern deep learning, including model opaqueness,\nhallucination, and the assignment of credit to data contributors, thereby\nimproving transparency, robustness, and trustworthiness in neural models.", "AI": {"tldr": "\u63d0\u51fa\u6eaf\u6e90\u7f51\u7edc\uff0c\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u9884\u6d4b\u76f4\u63a5\u5173\u8054\u5230\u652f\u6301\u6027\u8bad\u7ec3\u6837\u672c\u6765\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u6a21\u578b\u67b6\u6784\u4e2d\u5d4c\u5165\u89e3\u91ca\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\u4e0d\u900f\u660e\u3001\u5e7b\u89c9\u95ee\u9898\u4ee5\u53ca\u6570\u636e\u8d21\u732e\u8005\u4fe1\u7528\u5206\u914d\u7b49\u5173\u952e\u6311\u6218\uff0c\u63d0\u9ad8\u795e\u7ecf\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u6a21\u578b\u7c7b\u4f3c\u4e8e\u5b66\u4e60\u7684KNN\uff0c\u6bcf\u4e2a\u8f93\u51fa\u90fd\u7531\u7279\u5f81\u7a7a\u95f4\u4e2d\u52a0\u6743\u7684\u5177\u4f53\u6837\u672c\u652f\u6301\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u4efb\u52a1\u548c\u53ef\u89e3\u91ca\u6027\u76ee\u6807\u6765\u8bad\u7ec3\u3002", "result": "\u80fd\u591f\u7cfb\u7edf\u7814\u7a76\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u6743\u8861\uff0c\u9a8c\u8bc1\u8f93\u5165\u662f\u5426\u5728\u8bad\u7ec3\u96c6\u4e2d\uff0c\u68c0\u6d4b\u9519\u8bef\u6807\u6ce8\u6216\u5f02\u5e38\u6570\u636e\u70b9\uff0c\u589e\u5f3a\u5bf9\u8f93\u5165\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u8bc6\u522b\u751f\u6210\u65b0\u6570\u636e\u70b9\u7684\u76f8\u4f3c\u8f93\u5165\u3002", "conclusion": "\u6eaf\u6e90\u7f51\u7edc\u4e3a\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u6280\u672f\u63d0\u4f9b\u4e86\u8865\u5145\u65b9\u6cd5\uff0c\u867d\u7136\u5f15\u5165\u989d\u5916\u8ba1\u7b97\u6210\u672c\u4e14\u76ee\u524d\u9002\u7528\u4e8e\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4f46\u80fd\u63d0\u4f9b\u4f20\u7edf\u6df1\u5ea6\u7f51\u7edc\u65e0\u6cd5\u63d0\u4f9b\u7684\u6a21\u578b\u884c\u4e3a\u6d1e\u5bdf\u3002"}}
{"id": "2411.18625", "categories": ["cs.CV", "cs.AI", "cs.GR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2411.18625", "abs": "https://arxiv.org/abs/2411.18625", "authors": ["Brian Chao", "Hung-Yu Tseng", "Lorenzo Porzi", "Chen Gao", "Tuotuo Li", "Qinbo Li", "Ayush Saraf", "Jia-Bin Huang", "Johannes Kopf", "Gordon Wetzstein", "Changil Kim"], "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling", "comment": "Will be presented at CVPR 2025. Project website:\n  https://textured-gaussians.github.io/", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a state-of-the-art 3D\nreconstruction and rendering technique due to its high-quality results and fast\ntraining and rendering time. However, pixels covered by the same Gaussian are\nalways shaded in the same color up to a Gaussian falloff scaling factor.\nFurthermore, the finest geometric detail any individual Gaussian can represent\nis a simple ellipsoid. These properties of 3DGS greatly limit the expressivity\nof individual Gaussian primitives. To address these issues, we draw inspiration\nfrom texture and alpha mapping in traditional graphics and integrate it with\n3DGS. Specifically, we propose a new generalized Gaussian appearance\nrepresentation that augments each Gaussian with alpha~(A), RGB, or RGBA texture\nmaps to model spatially varying color and opacity across the extent of each\nGaussian. As such, each Gaussian can represent a richer set of texture patterns\nand geometric structures, instead of just a single color and ellipsoid as in\nnaive Gaussian Splatting. Surprisingly, we found that the expressivity of\nGaussians can be greatly improved by using alpha-only texture maps, and further\naugmenting Gaussians with RGB texture maps achieves the highest expressivity.\nWe validate our method on a wide variety of standard benchmark datasets and our\nown custom captures at both the object and scene levels. We demonstrate image\nquality improvements over existing methods while using a similar or lower\nnumber of Gaussians.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u76843D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u6dfb\u52a0alpha\u3001RGB\u6216RGBA\u7eb9\u7406\u6620\u5c04\u6765\u589e\u5f3a\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3DGS\u4e2d\u6bcf\u4e2a\u9ad8\u65af\u53ea\u80fd\u8868\u793a\u5355\u4e00\u989c\u8272\u548c\u7b80\u5355\u692d\u7403\u4f53\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6e85\u5c04(3DGS)\u867d\u7136\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u9ad8\u3001\u901f\u5ea6\u5feb\uff0c\u4f46\u6bcf\u4e2a\u9ad8\u65af\u53ea\u80fd\u8868\u793a\u5355\u4e00\u989c\u8272\u548c\u7b80\u5355\u692d\u7403\u4f53\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u7eb9\u7406\u6620\u5c04\u6765\u589e\u5f3a\u9ad8\u65af\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u4ece\u4f20\u7edf\u56fe\u5f62\u5b66\u4e2d\u7684\u7eb9\u7406\u548calpha\u6620\u5c04\u83b7\u5f97\u7075\u611f\uff0c\u4e3a\u6bcf\u4e2a\u9ad8\u65af\u6dfb\u52a0alpha\u3001RGB\u6216RGBA\u7eb9\u7406\u6620\u5c04\uff0c\u4f7f\u6bcf\u4e2a\u9ad8\u65af\u80fd\u591f\u8868\u793a\u66f4\u4e30\u5bcc\u7684\u7eb9\u7406\u6a21\u5f0f\u548c\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u591a\u79cd\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u6355\u83b7\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u4ec5alpha\u7eb9\u7406\u6620\u5c04\u5c31\u80fd\u663e\u8457\u63d0\u9ad8\u9ad8\u65af\u8868\u8fbe\u80fd\u529b\uff0c\u800c\u4f7f\u7528RGB\u7eb9\u7406\u6620\u5c04\u5219\u8fbe\u5230\u6700\u9ad8\u8868\u8fbe\u80fd\u529b\u3002\u5728\u4fdd\u6301\u6216\u51cf\u5c11\u9ad8\u65af\u6570\u91cf\u7684\u540c\u65f6\uff0c\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7eb9\u7406\u6620\u5c04\u96c6\u6210\u52303D\u9ad8\u65af\u6e85\u5c04\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6bcf\u4e2a\u9ad8\u65af\u539f\u8bed\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u597d\u7684\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2510.03663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03663", "abs": "https://arxiv.org/abs/2510.03663", "authors": ["Xiangyu Peng", "Cab Qin", "Zeyuan Chen", "Ran Xu", "Caiming Xiong", "Chien-Sheng Wu"], "title": "UNIDOC-BENCH: A Unified Benchmark for Document-Centric Multimodal RAG", "comment": null, "summary": "Multimodal retrieval-augmented generation (MM-RAG) is a key approach for\napplying large language models (LLMs) and agents to real-world knowledge bases,\nyet current evaluations are fragmented, focusing on either text or images in\nisolation or on simplified multimodal setups that fail to capture\ndocument-centric multimodal use cases. In this paper, we introduce\nUniDoc-Bench, the first large-scale, realistic benchmark for MM-RAG built from\n70k real-world PDF pages across eight domains. Our pipeline extracts and links\nevidence from text, tables, and figures, then generates 1,600 multimodal QA\npairs spanning factual retrieval, comparison, summarization, and logical\nreasoning queries. To ensure reliability, 20% of QA pairs are validated by\nmultiple annotators and expert adjudication. UniDoc-Bench supports\napples-to-apples comparison across four paradigms: (1) text-only, (2)\nimage-only, (3) multimodal text-image fusion, and (4) multimodal joint\nretrieval -- under a unified protocol with standardized candidate pools,\nprompts, and evaluation metrics. Our experiments show that multimodal\ntext-image fusion RAG systems consistently outperform both unimodal and jointly\nmultimodal embedding-based retrieval, indicating that neither text nor images\nalone are sufficient and that current multimodal embeddings remain inadequate.\nBeyond benchmarking, our analysis reveals when and how visual context\ncomplements textual evidence, uncovers systematic failure modes, and offers\nactionable guidance for developing more robust MM-RAG pipelines.", "AI": {"tldr": "UniDoc-Bench\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u7684MM-RAG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e70k\u771f\u5b9ePDF\u9875\u9762\u6784\u5efa\uff0c\u5305\u542b1,600\u4e2a\u591a\u6a21\u6001QA\u5bf9\uff0c\u652f\u6301\u56db\u79cd\u8303\u5f0f\u7684\u516c\u5e73\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524dMM-RAG\u8bc4\u4f30\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u8981\u4e48\u5355\u72ec\u5173\u6ce8\u6587\u672c\u6216\u56fe\u50cf\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5316\u7684\u591a\u6a21\u6001\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u6355\u6349\u6587\u6863\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u7528\u4f8b\u3002", "method": "\u4ece8\u4e2a\u9886\u57df\u768470k\u771f\u5b9ePDF\u9875\u9762\u4e2d\u63d0\u53d6\u5e76\u94fe\u63a5\u6587\u672c\u3001\u8868\u683c\u548c\u56fe\u50cf\u8bc1\u636e\uff0c\u751f\u62101,600\u4e2a\u591a\u6a21\u6001QA\u5bf9\uff0c\u6db5\u76d6\u4e8b\u5b9e\u68c0\u7d22\u3001\u6bd4\u8f83\u3001\u6458\u8981\u548c\u903b\u8f91\u63a8\u7406\u67e5\u8be2\uff0c20%\u7ecf\u8fc7\u591a\u6807\u6ce8\u8005\u9a8c\u8bc1\u548c\u4e13\u5bb6\u88c1\u51b3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u6a21\u6001\u6587\u672c-\u56fe\u50cf\u878d\u5408RAG\u7cfb\u7edf\u59cb\u7ec8\u4f18\u4e8e\u5355\u6a21\u6001\u548c\u8054\u5408\u591a\u6a21\u6001\u5d4c\u5165\u68c0\u7d22\uff0c\u8868\u660e\u5355\u72ec\u6587\u672c\u6216\u56fe\u50cf\u90fd\u4e0d\u8db3\u591f\uff0c\u5f53\u524d\u591a\u6a21\u6001\u5d4c\u5165\u4ecd\u4e0d\u8db3\u591f\u3002", "conclusion": "\u5206\u6790\u63ed\u793a\u4e86\u89c6\u89c9\u4e0a\u4e0b\u6587\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u8865\u5145\u6587\u672c\u8bc1\u636e\uff0c\u53d1\u73b0\u4e86\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u7a33\u5065\u7684MM-RAG\u7ba1\u9053\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002"}}
{"id": "2510.03363", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03363", "abs": "https://arxiv.org/abs/2510.03363", "authors": ["Zhe Zhang", "Mingxiu Cai", "Gaochang Wu", "Jing Zhang", "Lingqiao Liu", "Dacheng Tao", "Tianyou Chai", "Xiatian Zhu"], "title": "Unified Unsupervised Anomaly Detection via Matching Cost Filtering", "comment": "63 pages (main paper and supplementary material), 39 figures, 58\n  tables. Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "summary": "Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level\nanomalies using only normal training data, with wide applications such as\nindustrial inspection and medical analysis, where anomalies are scarce due to\nprivacy concerns and cold-start constraints. Existing methods, whether\nreconstruction-based (restoring normal counterparts) or embedding-based\n(pretrained representations), fundamentally conduct image- or feature-level\nmatching to generate anomaly maps. Nonetheless, matching noise has been largely\noverlooked, limiting their detection ability. Beyond earlier focus on unimodal\nRGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D\nand RGB--Text, enabled by point cloud sensing and vision--language models.\nDespite shared challenges, these lines remain largely isolated, hindering a\ncomprehensive understanding and knowledge transfer. In this paper, we advocate\nunified UAD for both unimodal and multimodal settings in the matching\nperspective. Under this insight, we present Unified Cost Filtering (UCF), a\ngeneric post-hoc refinement framework for refining anomaly cost volume of any\nUAD model. The cost volume is constructed by matching a test sample against\nnormal samples from the same or different modalities, followed by a learnable\nfiltering module with multi-layer attention guidance from the test sample,\nmitigating matching noise and highlighting subtle anomalies. Comprehensive\nexperiments on 22 diverse benchmarks demonstrate the efficacy of UCF in\nenhancing a variety of UAD methods, consistently achieving new state-of-the-art\nresults in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD\nscenarios. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6UCF\uff0c\u901a\u8fc7\u5339\u914d\u89c6\u89d2\u5904\u7406\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u8fc7\u6ee4\u6a21\u5757\u4f18\u5316\u5f02\u5e38\u6210\u672c\u4f53\u79ef\uff0c\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u5339\u914d\u566a\u58f0\u95ee\u9898\uff0c\u4e14\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65b9\u6cd5\u76f8\u4e92\u9694\u79bb\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5339\u914d\u566a\u58f0\u5e76\u7edf\u4e00\u5904\u7406\u4e0d\u540c\u6a21\u6001\u7684\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6210\u672c\u8fc7\u6ee4\uff08UCF\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6d4b\u8bd5\u6837\u672c\u4e0e\u6b63\u5e38\u6837\u672c\u7684\u5339\u914d\u6210\u672c\u4f53\u79ef\uff0c\u7136\u540e\u4f7f\u7528\u591a\u5c42\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53ef\u5b66\u4e60\u8fc7\u6ee4\u6a21\u5757\u6765\u51cf\u5c11\u5339\u914d\u566a\u58f0\u5e76\u7a81\u51fa\u7ec6\u5fae\u5f02\u5e38\u3002", "result": "\u572822\u4e2a\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUCF\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u5355\u6a21\u6001\uff08RGB\uff09\u548c\u591a\u6a21\u6001\uff08RGB-3D\u3001RGB-Text\uff09\u573a\u666f\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "UCF\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u540e\u5904\u7406\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5339\u914d\u566a\u58f0\uff0c\u7edf\u4e00\u652f\u6301\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.03683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03683", "abs": "https://arxiv.org/abs/2510.03683", "authors": ["Nisar Hussain", "Amna Qasim", "Gull Mehak", "Muhammad Zain", "Momina Hafeez", "Grigori Sidorov"], "title": "Fine-Tuning Large Language Models with QLoRA for Offensive Language Detection in Roman Urdu-English Code-Mixed Text", "comment": "25 pages, 22 figures", "summary": "The use of derogatory terms in languages that employ code mixing, such as\nRoman Urdu, presents challenges for Natural Language Processing systems due to\nunstated grammar, inconsistent spelling, and a scarcity of labeled data. In\nthis work, we propose a QLoRA based fine tuning framework to improve offensive\nlanguage detection in Roman Urdu-English text. We translated the Roman\nUrdu-English code mixed dataset into English using Google Translate to leverage\nEnglish LLMs, while acknowledging that this translation reduces direct\nengagement with code mixing features. Our focus is on classification\nperformance using English translated low resource inputs. We fine tuned several\ntransformers and large language models, including Meta LLaMA 3 8B, Mistral 7B\nv0.1, LLaMA 2 7B, ModernBERT, and RoBERTa, with QLoRA for memory efficient\nadaptation. Models were trained and evaluated on a manually annotated Roman\nUrdu dataset for offensive vs non offensive content. Of all tested models, the\nhighest F1 score of 91.45 was attained by Meta LLaMA 3 8B, followed by Mistral\n7B at 89.66, surpassing traditional transformer baselines. These results\ndemonstrate the efficacy of QLoRA in fine tuning high performing models for low\nresource environments such as code mixed offensive language detection, and\nconfirm the potential of LLMs for this task. This work advances a scalable\napproach to Roman Urdu moderation and paves the way for future multilingual\noffensive detection systems based on LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eQLoRA\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed-\u82f1\u8bed\u6df7\u5408\u6587\u672c\u4e2d\u7684\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ffb\u8bd1\u5904\u7406\u4f4e\u8d44\u6e90\u8f93\u5165\uff0cMeta LLaMA 3 8B\u6a21\u578b\u83b7\u5f97\u6700\u9ad891.45\u7684F1\u5206\u6570\u3002", "motivation": "\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u7b49\u4ee3\u7801\u6df7\u5408\u8bed\u8a00\u4e2d\u7684\u8d2c\u4e49\u8bcd\u6c47\u4f7f\u7528\u7ed9NLP\u7cfb\u7edf\u5e26\u6765\u6311\u6218\uff0c\u5305\u62ec\u672a\u660e\u786e\u7684\u8bed\u6cd5\u3001\u4e0d\u4e00\u81f4\u7684\u62fc\u5199\u548c\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u3002", "method": "\u4f7f\u7528Google Translate\u5c06\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed-\u82f1\u8bed\u6df7\u5408\u6570\u636e\u96c6\u7ffb\u8bd1\u6210\u82f1\u8bed\u4ee5\u5229\u7528\u82f1\u8bedLLM\uff0c\u91c7\u7528QLoRA\u5bf9\u591a\u4e2aTransformer\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5185\u5b58\u9ad8\u6548\u7684\u5fae\u8c03\u3002", "result": "Meta LLaMA 3 8B\u6a21\u578b\u83b7\u5f97\u6700\u9ad8F1\u5206\u657091.45\uff0cMistral 7B\u4e3a89.66\uff0c\u5747\u8d85\u8fc7\u4f20\u7edfTransformer\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "QLoRA\u5728\u5fae\u8c03\u9ad8\u6027\u80fd\u6a21\u578b\u7528\u4e8e\u4f4e\u8d44\u6e90\u73af\u5883\uff08\u5982\u4ee3\u7801\u6df7\u5408\u5192\u72af\u6027\u8bed\u8a00\u68c0\u6d4b\uff09\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u8bc1\u5b9e\u4e86LLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00\u5192\u72af\u68c0\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03376", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03376", "abs": "https://arxiv.org/abs/2510.03376", "authors": ["Sanjukta Ghosh"], "title": "Visual Language Model as a Judge for Object Detection in Industrial Diagrams", "comment": "Pre-review version submitted to IEEE ICASSP 2026", "summary": "Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are\nessential for the design, operation, and maintenance of industrial plants.\nConverting these diagrams into digital form is an important step toward\nbuilding digital twins and enabling intelligent industrial automation. A\ncentral challenge in this digitalization process is accurate object detection.\nAlthough recent advances have significantly improved object detection\nalgorithms, there remains a lack of methods to automatically evaluate the\nquality of their outputs. This paper addresses this gap by introducing a\nframework that employs Visual Language Models (VLMs) to assess object detection\nresults and guide their refinement. The approach exploits the multimodal\ncapabilities of VLMs to identify missing or inconsistent detections, thereby\nenabling automated quality assessment and improving overall detection\nperformance on complex industrial diagrams.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u8bc4\u4f30\u5de5\u4e1a\u56fe\u7eb8\u4e2d\u7269\u4f53\u68c0\u6d4b\u7ed3\u679c\u8d28\u91cf\u7684\u6846\u67b6\uff0c\u80fd\u591f\u8bc6\u522b\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u7684\u68c0\u6d4b\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u5e76\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u56fe\u7eb8(\u5982P&ID\u56fe)\u6570\u5b57\u5316\u5bf9\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u548c\u5b9e\u73b0\u667a\u80fd\u5de5\u4e1a\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u81ea\u52a8\u8bc4\u4f30\u7269\u4f53\u68c0\u6d4b\u7ed3\u679c\u8d28\u91cf\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u591a\u6a21\u6001\u80fd\u529b\u6765\u8bc6\u522b\u7f3a\u5931\u6216\u4e0d\u4e00\u81f4\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u6307\u5bfc\u68c0\u6d4b\u7ed3\u679c\u7684\u4f18\u5316\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\uff0c\u5e76\u5728\u590d\u6742\u7684\u5de5\u4e1a\u56fe\u7eb8\u4e0a\u63d0\u9ad8\u6574\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684VLM\u6846\u67b6\u586b\u8865\u4e86\u5de5\u4e1a\u56fe\u7eb8\u6570\u5b57\u5316\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u8bc4\u4f30\u7269\u4f53\u68c0\u6d4b\u8d28\u91cf\u7684\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2510.03687", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03687", "abs": "https://arxiv.org/abs/2510.03687", "authors": ["Yue Huang", "Yanyuan Chen", "Dexuan Xu", "Weihua Yue", "Huamin Zhang", "Meikang Qiu", "Yu Huang"], "title": "MedReflect: Teaching Medical LLMs to Self-Improve via Reflective Correction", "comment": null, "summary": "Medical problem solving demands expert knowledge and intricate reasoning.\nRecent studies of large language models (LLMs) attempt to ease this complexity\nby introducing external knowledge verification through retrieval-augmented\ngeneration or by training on reasoning datasets. However, these approaches\nsuffer from drawbacks such as retrieval overhead and high annotation costs, and\nthey heavily rely on substituted external assistants to reach limited\nperformance in medical field. In this paper, we introduce MedReflect, a\ngeneralizable framework designed to inspire LLMs with a physician-like\nreflective thinking mode. MedReflect generates a single-pass reflection chain\nthat includes initial hypothesis generation, self-questioning, self-answering\nand decision refinement. This self-verified and self-reflective nature releases\nlarge language model's latent capability in medical problem-solving without\nexternal retrieval or heavy annotation. We demonstrate that MedReflect enables\ncost-efficient medical dataset construction: with merely 2,000 randomly sampled\ntraining examples and a light fine-tuning, this approach achieves notable\nabsolute accuracy improvements across a series of medical benchmarks while\ncutting annotation requirements. Our results provide evidence that LLMs can\nlearn to solve specialized medical problems via self-reflection and\nself-improve, reducing reliance on external supervision and extensive\ntask-specific fine-tuning data.", "AI": {"tldr": "\u63d0\u51fa\u4e86MedReflect\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u533b\u751f\u7684\u53cd\u601d\u601d\u7ef4\u6a21\u5f0f\uff0c\u8ba9LLM\u5728\u533b\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u8fdb\u884c\u81ea\u6211\u9a8c\u8bc1\u548c\u81ea\u6211\u53cd\u601d\uff0c\u65e0\u9700\u5916\u90e8\u68c0\u7d22\u6216\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u6216\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5b58\u5728\u68c0\u7d22\u5f00\u9500\u5927\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u5728\u533b\u5b66\u9886\u57df\u6027\u80fd\u6709\u9650\u3002", "method": "MedReflect\u751f\u6210\u5355\u6b21\u53cd\u601d\u94fe\uff0c\u5305\u62ec\u521d\u59cb\u5047\u8bbe\u751f\u6210\u3001\u81ea\u6211\u63d0\u95ee\u3001\u81ea\u6211\u56de\u7b54\u548c\u51b3\u7b56\u4f18\u5316\uff0c\u5b9e\u73b0\u81ea\u6211\u9a8c\u8bc1\u548c\u53cd\u601d\u3002", "result": "\u4ec5\u75282000\u4e2a\u968f\u673a\u8bad\u7ec3\u6837\u672c\u548c\u8f7b\u91cf\u5fae\u8c03\uff0c\u5c31\u5728\u591a\u4e2a\u533b\u5b66\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5927\u5e45\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u3002", "conclusion": "LLM\u53ef\u4ee5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u6539\u8fdb\u5b66\u4e60\u89e3\u51b3\u4e13\u4e1a\u533b\u5b66\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u5916\u90e8\u76d1\u7763\u548c\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.03441", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T45, 68T10, 68T40"], "pdf": "https://arxiv.org/pdf/2510.03441", "abs": "https://arxiv.org/abs/2510.03441", "authors": ["Chashi Mahiul Islam", "Oteo Mamo", "Samuel Jacob Chacko", "Xiuwen Liu", "Weikuan Yu"], "title": "Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning", "comment": "12 pages, 5 figures", "summary": "Vision-language models (VLMs) have advanced multimodal reasoning but still\nface challenges in spatial reasoning for 3D scenes and complex object\nconfigurations. To address this, we introduce SpatialViLT, an enhanced VLM that\nintegrates spatial features like depth maps, 3D coordinates, and edge maps\nthrough a multi-task learning framework. This approach enriches multimodal\nembeddings with spatial understanding. We propose two variants: SpatialViLT and\nMaskedSpatialViLT, focusing on full and masked object regions, respectively.\nAdditionally, SpatialEnsemble combines both approaches, achieving\nstate-of-the-art accuracy. Our models excel in spatial reasoning categories\nsuch as directional, topological, and proximity relations, as demonstrated on\nthe challenging Visual Spatial Reasoning (VSR) dataset. This work represents a\nsignificant step in enhancing the spatial intelligence of AI systems, crucial\nfor advanced multimodal understanding and real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86SpatialViLT\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u6df1\u5ea6\u56fe\u30013D\u5750\u6807\u548c\u8fb9\u7f18\u56fe\u7b49\u7a7a\u95f4\u7279\u5f81\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728VSR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u57283D\u573a\u666f\u548c\u590d\u6742\u7269\u4f53\u914d\u7f6e\u7684\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86SpatialViLT\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u96c6\u6210\u7a7a\u95f4\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u4e24\u4e2a\u53d8\u4f53\uff1aSpatialViLT\u548cMaskedSpatialViLT\uff0c\u4ee5\u53ca\u7ed3\u5408\u4e24\u8005\u7684SpatialEnsemble\u65b9\u6cd5\u3002", "result": "\u6a21\u578b\u5728\u65b9\u5411\u6027\u3001\u62d3\u6251\u548c\u90bb\u8fd1\u5173\u7cfb\u7b49\u7a7a\u95f4\u63a8\u7406\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728VSR\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u663e\u8457\u63d0\u5347\u4e86AI\u7cfb\u7edf\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5bf9\u9ad8\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u548c\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.03748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03748", "abs": "https://arxiv.org/abs/2510.03748", "authors": ["Ramtin Kakavand", "Ebrahim Ansari"], "title": "TreePrompt: Leveraging Hierarchical Few-Shot Example Selection for Improved English-Persian and English-German Translation", "comment": "12 pages", "summary": "Large Language Models (LLMs) have consistently demonstrated strong\nperformance in machine translation, especially when guided by high-quality\nprompts. Few-shot prompting is an effective technique to improve translation\nquality; however, most existing example selection methods focus solely on\nquery-to-example similarity and do not account for the quality of the examples.\nIn this work, we propose TreePrompt, a novel example selection approach that\nlearns LLM preferences to identify high-quality, contextually relevant examples\nwithin a tree-structured framework. To further explore the balance between\nsimilarity and quality, we combine TreePrompt with K-Nearest Neighbors (K-NN)\nand Adaptive Few-Shot Prompting (AFSP). Evaluations on two language pairs -\nEnglish-Persian (MIZAN) and English-German (WMT19) - show that integrating\nTreePrompt with AFSP or Random selection leads to improved translation\nperformance.", "AI": {"tldr": "TreePrompt\u662f\u4e00\u79cd\u65b0\u7684\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u6811\u72b6\u7ed3\u6784\u5b66\u4e60LLM\u504f\u597d\u6765\u8bc6\u522b\u9ad8\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7ffb\u8bd1\u793a\u4f8b\uff0c\u5e76\u4e0eK-NN\u548cAFSP\u65b9\u6cd5\u7ed3\u5408\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709few-shot\u63d0\u793a\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u67e5\u8be2\u4e0e\u793a\u4f8b\u7684\u76f8\u4f3c\u6027\uff0c\u4f46\u5ffd\u7565\u4e86\u793a\u4f8b\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u7ffb\u8bd1\u8d28\u91cf\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51faTreePrompt\u65b9\u6cd5\uff0c\u5728\u6811\u72b6\u6846\u67b6\u4e2d\u5b66\u4e60LLM\u504f\u597d\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u793a\u4f8b\uff0c\u5e76\u4e0eK-NN\u548cAFSP\u65b9\u6cd5\u7ed3\u5408\u63a2\u7d22\u76f8\u4f3c\u6027\u4e0e\u8d28\u91cf\u7684\u5e73\u8861\u3002", "result": "\u5728\u82f1\u8bed-\u6ce2\u65af\u8bed\u548c\u82f1\u8bed-\u5fb7\u8bed\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cTreePrompt\u4e0eAFSP\u6216\u968f\u673a\u9009\u62e9\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\u3002", "conclusion": "TreePrompt\u901a\u8fc7\u8003\u8651\u793a\u4f8b\u8d28\u91cf\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u6709\u6548\u6539\u8fdb\u4e86few-shot\u63d0\u793a\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\uff0c\u4e3aLLM\u7ffb\u8bd1\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.03452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03452", "abs": "https://arxiv.org/abs/2510.03452", "authors": ["Allison Davis", "Yezhi Shen", "Xiaoyu Ji", "Fengqing Zhu"], "title": "Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks", "comment": "5 pages, 4 figures, submitted to ICASSP 2026", "summary": "Structured illumination (SI) enhances image resolution and contrast by\nprojecting patterned light onto a sample. In two-phase optical-sectioning SI\n(OS-SI), reduced acquisition time introduces residual artifacts that\nconventional denoising struggles to suppress. Deep learning offers an\nalternative to traditional methods; however, supervised training is limited by\nthe lack of clean, optically sectioned ground-truth data. We investigate\nencoder-decoder networks for artifact reduction in two-phase OS-SI, using\nsynthetic training pairs formed by applying real artifact fields to synthetic\nimages. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on\nthe synthetic data, then evaluated on real OS-SI images. Both networks improve\nimage clarity, with each excelling against different artifact types. These\nresults demonstrate that synthetic training enables supervised denoising of\nOS-SI images and highlight the potential of encoder-decoder networks to\nstreamline reconstruction workflows.", "AI": {"tldr": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u51cf\u5c11\u4e24\u76f8\u5149\u5b66\u5207\u7247\u7ed3\u6784\u7167\u660e\u663e\u5fae\u955c\u4e2d\u7684\u4f2a\u5f71\uff0c\u901a\u8fc7\u5408\u6210\u8bad\u7ec3\u6570\u636e\u89e3\u51b3\u7f3a\u4e4f\u5e72\u51c0\u5730\u9762\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898", "motivation": "\u4e24\u76f8\u5149\u5b66\u5207\u7247\u7ed3\u6784\u7167\u660e\u663e\u5fae\u955c\u4e2d\uff0c\u51cf\u5c11\u91c7\u96c6\u65f6\u95f4\u4f1a\u5f15\u5165\u4f20\u7edf\u53bb\u566a\u65b9\u6cd5\u96be\u4ee5\u6291\u5236\u7684\u6b8b\u7559\u4f2a\u5f71\uff0c\u800c\u76d1\u7763\u8bad\u7ec3\u53c8\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5e72\u51c0\u7684\u5149\u5b66\u5207\u7247\u5730\u9762\u771f\u5b9e\u6570\u636e", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff08\u5305\u62ec\u975e\u5bf9\u79f0\u53bb\u566a\u81ea\u7f16\u7801\u5668\u548cU-Net\uff09\uff0c\u901a\u8fc7\u5728\u5408\u6210\u56fe\u50cf\u4e0a\u5e94\u7528\u771f\u5b9e\u4f2a\u5f71\u573a\u6765\u521b\u5efa\u5408\u6210\u8bad\u7ec3\u5bf9\uff0c\u7136\u540e\u8bc4\u4f30\u771f\u5b9eOS-SI\u56fe\u50cf", "result": "\u4e24\u79cd\u7f51\u7edc\u90fd\u63d0\u9ad8\u4e86\u56fe\u50cf\u6e05\u6670\u5ea6\uff0c\u6bcf\u79cd\u7f51\u7edc\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u4f2a\u5f71\u4e0a\u90fd\u8868\u73b0\u51fa\u8272", "conclusion": "\u5408\u6210\u8bad\u7ec3\u4f7f\u5f97OS-SI\u56fe\u50cf\u7684\u76d1\u7763\u53bb\u566a\u6210\u4e3a\u53ef\u80fd\uff0c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u6709\u6f5c\u529b\u7b80\u5316\u91cd\u5efa\u5de5\u4f5c\u6d41\u7a0b"}}
{"id": "2510.03758", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.03758", "abs": "https://arxiv.org/abs/2510.03758", "authors": ["Ilias Tougui", "Mehdi Zakroum", "Mounir Ghogho"], "title": "Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech", "comment": null, "summary": "Parkinson's Disease (PD) affects over 10 million people worldwide, with\nspeech impairments in up to 89% of patients. Current speech-based detection\nsystems analyze entire utterances, potentially overlooking the diagnostic value\nof specific phonetic elements. We developed a granularity-aware approach for\nmultilingual PD detection using an automated pipeline that extracts\ntime-aligned phonemes, syllables, and words from recordings. Using Italian,\nSpanish, and English datasets, we implemented a bidirectional LSTM with\nmulti-head attention to compare diagnostic performance across the different\ngranularity levels. Phoneme-level analysis achieved superior performance with\nAUROC of 93.78% +- 2.34% and accuracy of 92.17% +- 2.43%. This demonstrates\nenhanced diagnostic capability for cross-linguistic PD detection. Importantly,\nattention analysis revealed that the most informative speech features align\nwith those used in established clinical protocols: sustained vowels (/a/, /e/,\n/o/, /i/) at phoneme level, diadochokinetic syllables (/ta/, /pa/, /la/, /ka/)\nat syllable level, and /pataka/ sequences at word level. Source code will be\navailable at https://github.com/jetliqs/clearpd.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u97f3\u7ec6\u7c92\u5ea6\u5206\u6790\u7684\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u97f3\u7d20\u3001\u97f3\u8282\u548c\u5355\u8bcd\u7ea7\u522b\u7279\u5f81\uff0c\u5728\u591a\u79cd\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5e15\u91d1\u68ee\u75c5\u8bed\u97f3\u68c0\u6d4b\u7cfb\u7edf\u5206\u6790\u6574\u4e2a\u8bed\u53e5\uff0c\u53ef\u80fd\u5ffd\u7565\u4e86\u7279\u5b9a\u8bed\u97f3\u5143\u7d20\u7684\u8bca\u65ad\u4ef7\u503c\u3002\u5e15\u91d1\u68ee\u75c5\u5f71\u54cd\u5168\u7403\u8d851000\u4e07\u4eba\uff0c89%\u60a3\u8005\u5b58\u5728\u8bed\u97f3\u969c\u788d\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d6\u65f6\u95f4\u5bf9\u9f50\u7684\u97f3\u7d20\u3001\u97f3\u8282\u548c\u5355\u8bcd\u7279\u5f81\uff0c\u4f7f\u7528\u53cc\u5411LSTM\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u610f\u5927\u5229\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u7684\u8bca\u65ad\u6027\u80fd\u3002", "result": "\u97f3\u7d20\u7ea7\u5206\u6790\u8868\u73b0\u6700\u4f73\uff0cAUROC\u8fbe93.78%\u00b12.34%\uff0c\u51c6\u786e\u7387\u8fbe92.17%\u00b12.43%\u3002\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u6700\u6709\u6548\u7684\u8bed\u97f3\u7279\u5f81\u4e0e\u4e34\u5e8a\u534f\u8bae\u4e00\u81f4\uff1a\u6301\u7eed\u5143\u97f3\u3001\u4ea4\u66ff\u8fd0\u52a8\u97f3\u8282\u548c\u7279\u5b9a\u5355\u8bcd\u5e8f\u5217\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u7684\u8bca\u65ad\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8bed\u97f3\u7ec6\u7c92\u5ea6\u5206\u6790\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.03455", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03455", "abs": "https://arxiv.org/abs/2510.03455", "authors": ["Sejuti Majumder", "Saarthak Kapse", "Moinak Bhattacharya", "Xuan Xu", "Alisa Yurovsky", "Prateek Prasanna"], "title": "PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology", "comment": null, "summary": "Integrating histopathology with spatial transcriptomics (ST) provides a\npowerful opportunity to link tissue morphology with molecular function. Yet\nmost existing multimodal approaches rely on a small set of highly variable\ngenes, which limits predictive scope and overlooks the coordinated biological\nprograms that shape tissue phenotypes. We present PEaRL (Pathway Enhanced\nRepresentation Learning), a multimodal framework that represents\ntranscriptomics through pathway activation scores computed with ssGSEA. By\nencoding biologically coherent pathway signals with a transformer and aligning\nthem with histology features via contrastive learning, PEaRL reduces\ndimensionality, improves interpretability, and strengthens cross-modal\ncorrespondence. Across three cancer ST datasets (breast, skin, and lymph node),\nPEaRL consistently outperforms SOTA methods, yielding higher accuracy for both\ngene- and pathway-level expression prediction (up to 58.9 percent and 20.4\npercent increase in Pearson correlation coefficient compared to SOTA). These\nresults demonstrate that grounding transcriptomic representation in pathways\nproduces more biologically faithful and interpretable multimodal models,\nadvancing computational pathology beyond gene-level embeddings.", "AI": {"tldr": "PEaRL\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u8def\u6fc0\u6d3b\u5206\u6570\u6574\u5408\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\uff0c\u4f7f\u7528transformer\u7f16\u7801\u901a\u8def\u4fe1\u53f7\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u7ec4\u7ec7\u5b66\u7279\u5f81\u5bf9\u9f50\uff0c\u5728\u591a\u4e2a\u764c\u75c7\u6570\u636e\u96c6\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5c11\u91cf\u9ad8\u53d8\u57fa\u56e0\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u8303\u56f4\u5e76\u5ffd\u7565\u4e86\u534f\u8c03\u7684\u751f\u7269\u5b66\u7a0b\u5e8f\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u751f\u7269\u4fe1\u606f\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528ssGSEA\u8ba1\u7b97\u901a\u8def\u6fc0\u6d3b\u5206\u6570\uff0c\u901a\u8fc7transformer\u7f16\u7801\u901a\u8def\u4fe1\u53f7\uff0c\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u5c06\u901a\u8def\u7279\u5f81\u4e0e\u7ec4\u7ec7\u5b66\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u764c\u75c7ST\u6570\u636e\u96c6\u4e2d\uff0cPEaRL\u5728\u57fa\u56e0\u548c\u901a\u8def\u6c34\u5e73\u8868\u8fbe\u9884\u6d4b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPearson\u76f8\u5173\u7cfb\u6570\u5206\u522b\u63d0\u9ad8\u4e8658.9%\u548c20.4%\u3002", "conclusion": "\u57fa\u4e8e\u901a\u8def\u7684\u8f6c\u5f55\u7ec4\u8868\u793a\u80fd\u4ea7\u751f\u66f4\u751f\u7269\u53ef\u4fe1\u548c\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u63a8\u52a8\u8ba1\u7b97\u75c5\u7406\u5b66\u8d85\u8d8a\u57fa\u56e0\u6c34\u5e73\u5d4c\u5165\u3002"}}
{"id": "2510.03762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03762", "abs": "https://arxiv.org/abs/2510.03762", "authors": ["Deshan Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "title": "Prompt Balance Matters: Understanding How Imbalanced Few-Shot Learning Affects Multilingual Sense Disambiguation in LLMs", "comment": "Paper accepted at GlobalNLP 2025: Workshop on beyond English: Natural\n  Language Processing for All Languages in an Era of Large Language Models\" 9\n  pages, 3 figures, 2 Tables", "summary": "Recent advances in Large Language Models (LLMs) have significantly reshaped\nthe landscape of Natural Language Processing (NLP). Among the various prompting\ntechniques, few-shot prompting has gained considerable attention for its\npracticality and effectiveness. This study investigates how few-shot prompting\nstrategies impact the Word Sense Disambiguation (WSD) task, particularly\nfocusing on the biases introduced by imbalanced sample distributions. We use\nthe GLOSSGPT prompting method, an advanced approach for English WSD, to test\nits effectiveness across five languages: English, German, Spanish, French, and\nItalian. Our results show that imbalanced few-shot examples can cause incorrect\nsense predictions in multilingual languages, but this issue does not appear in\nEnglish. To assess model behavior, we evaluate both the GPT-4o and\nLLaMA-3.1-70B models and the results highlight the sensitivity of multilingual\nWSD to sample distribution in few-shot settings, emphasizing the need for\nbalanced and representative prompting strategies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u5bf9\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u6837\u672c\u5206\u5e03\u4e0d\u5e73\u8861\u5f15\u5165\u7684\u504f\u5dee\u3002\u4f7f\u7528GLOSSGPT\u65b9\u6cd5\u5728\u4e94\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\uff0c\u53d1\u73b0\u4e0d\u5e73\u8861\u6837\u672c\u4f1a\u5bfc\u81f4\u591a\u8bed\u8a00\u9519\u8bef\u9884\u6d4b\uff0c\u4f46\u82f1\u8bed\u4e0d\u53d7\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u91cd\u5851\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff0c\u5c11\u6837\u672c\u63d0\u793a\u56e0\u5176\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u53d7\u5230\u5173\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\uff0c\u7279\u522b\u662f\u6837\u672c\u5206\u5e03\u4e0d\u5e73\u8861\u5e26\u6765\u7684\u504f\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528GLOSSGPT\u63d0\u793a\u65b9\u6cd5\uff0c\u5728\u82f1\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u548c\u610f\u5927\u5229\u8bed\u4e94\u79cd\u8bed\u8a00\u4e0a\u6d4b\u8bd5\u3002\u8bc4\u4f30\u4e86GPT-4o\u548cLLaMA-3.1-70B\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u5e73\u8861\u5c11\u6837\u672c\u793a\u4f8b\u5bf9\u8bcd\u4e49\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u5e73\u8861\u7684\u5c11\u6837\u672c\u793a\u4f8b\u4f1a\u5bfc\u81f4\u591a\u8bed\u8a00\u8bcd\u4e49\u6d88\u6b67\u7684\u9519\u8bef\u9884\u6d4b\uff0c\u4f46\u82f1\u8bed\u4e0d\u53d7\u6b64\u95ee\u9898\u5f71\u54cd\u3002\u6a21\u578b\u8bc4\u4f30\u8868\u660e\u591a\u8bed\u8a00WSD\u5bf9\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u5206\u5e03\u975e\u5e38\u654f\u611f\u3002", "conclusion": "\u591a\u8bed\u8a00\u8bcd\u4e49\u6d88\u6b67\u5bf9\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u5206\u5e03\u9ad8\u5ea6\u654f\u611f\uff0c\u5f3a\u8c03\u9700\u8981\u5e73\u8861\u548c\u4ee3\u8868\u6027\u7684\u63d0\u793a\u7b56\u7565\u6765\u786e\u4fdd\u6a21\u578b\u6027\u80fd\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.03483", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03483", "abs": "https://arxiv.org/abs/2510.03483", "authors": ["Numan Saeed", "Tausifa Jan Saleem", "Fadillah Maani", "Muhammad Ridzuan", "Hu Wang", "Mohammad Yaqub"], "title": "DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis", "comment": null, "summary": "Deep learning for medical imaging is hampered by task-specific models that\nlack generalizability and prognostic capabilities, while existing 'universal'\napproaches suffer from simplistic conditioning and poor medical semantic\nunderstanding. To address these limitations, we introduce DuPLUS, a deep\nlearning framework for efficient multi-modal medical image analysis. DuPLUS\nintroduces a novel vision-language framework that leverages hierarchical\nsemantic prompts for fine-grained control over the analysis task, a capability\nabsent in prior universal models. To enable extensibility to other medical\ntasks, it includes a hierarchical, text-controlled architecture driven by a\nunique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize\nacross three imaging modalities, ten different anatomically various medical\ndatasets, encompassing more than 30 organs and tumor types. It outperforms the\nstate-of-the-art task specific and universal models on 8 out of 10 datasets. We\ndemonstrate extensibility of its text-controlled architecture by seamless\nintegration of electronic health record (EHR) data for prognosis prediction,\nand on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)\nof 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks\nand modalities from varying centers, establishing DuPLUS as a versatile and\nclinically relevant solution for medical image analysis. The code for this work\nis made available at: https://anonymous.4open.science/r/DuPLUS-6C52", "AI": {"tldr": "DuPLUS\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6a21\u6001\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u548c\u53cc\u63d0\u793a\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4efb\u52a1\u63a7\u5236\uff0c\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u96c6\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u8fdb\u884c\u9884\u540e\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u5206\u6790\u4e2d\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u7f3a\u4e4f\u6cdb\u5316\u6027\u548c\u9884\u540e\u80fd\u529b\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u901a\u7528\u65b9\u6cd5\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u7684\u5206\u5c42\u8bed\u4e49\u63d0\u793a\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u63d0\u793a\u673a\u5236\u5b9e\u73b0\u6587\u672c\u63a7\u5236\u67b6\u6784\uff0c\u652f\u6301\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4ee5\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u6a21\u6001\u3002", "result": "\u572810\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e2d\u76848\u4e2a\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u7279\u5b9a\u4efb\u52a1\u548c\u901a\u7528\u6a21\u578b\uff0c\u5728\u5934\u9888\u764c\u6570\u636e\u96c6\u4e0a\u83b7\u5f970.69\u7684Concordance Index\u3002", "conclusion": "DuPLUS\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u533b\u5b66\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5feb\u901f\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u4e0d\u540c\u4e2d\u5fc3\u7684\u6a21\u6001\u6570\u636e\u3002"}}
{"id": "2510.03781", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03781", "abs": "https://arxiv.org/abs/2510.03781", "authors": ["Majid Asgari-Bidhendi", "Muhammad Amin Ghaseminia", "Alireza Shahbazi", "Sayyed Ali Hossayni", "Najmeh Torabian", "Behrouz Minaei-Bidgoli"], "title": "Rezwan: Leveraging Large Language Models for Comprehensive Hadith Text Processing: A 1.2M Corpus Development", "comment": "9 pages, 3 figures", "summary": "This paper presents the development of Rezwan, a large-scale AI-assisted\nHadith corpus comprising over 1.2M narrations, extracted and structured through\na fully automated pipeline. Building on digital repositories such as Maktabat\nAhl al-Bayt, the pipeline employs Large Language Models (LLMs) for\nsegmentation, chain--text separation, validation, and multi-layer enrichment.\nEach narration is enhanced with machine translation into twelve languages,\nintelligent diacritization, abstractive summarization, thematic tagging, and\ncross-text semantic analysis. This multi-step process transforms raw text into\na richly annotated research-ready infrastructure for digital humanities and\nIslamic studies. A rigorous evaluation was conducted on 1,213 randomly sampled\nnarrations, assessed by six domain experts. Results show near-human accuracy in\nstructured tasks such as chain--text separation (9.33/10) and summarization\n(9.33/10), while highlighting ongoing challenges in diacritization and semantic\nsimilarity detection. Comparative analysis against the manually curated Noor\nCorpus demonstrates the superiority of Najm in both scale and quality, with a\nmean overall score of 8.46/10 versus 3.66/10. Furthermore, cost analysis\nconfirms the economic feasibility of the AI approach: tasks requiring over\n229,000 hours of expert labor were completed within months at a fraction of the\ncost. The work introduces a new paradigm in religious text processing by\nshowing how AI can augment human expertise, enabling large-scale, multilingual,\nand semantically enriched access to Islamic heritage.", "AI": {"tldr": "\u5f00\u53d1\u4e86Rezwan\u5927\u578bAI\u8f85\u52a9\u5723\u8bad\u8bed\u6599\u5e93\uff0c\u5305\u542b120\u4e07\u6761\u5723\u8bad\uff0c\u901a\u8fc7\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\u63d0\u53d6\u548c\u7ed3\u6784\u5316\uff0c\u5b9e\u73b0\u591a\u8bed\u8a00\u7ffb\u8bd1\u3001\u667a\u80fd\u6807\u6ce8\u3001\u6458\u8981\u751f\u6210\u7b49\u529f\u80fd\uff0c\u5728\u89c4\u6a21\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u8bed\u6599\u5e93\u3002", "motivation": "\u4f20\u7edf\u5723\u8bad\u5904\u7406\u4f9d\u8d56\u4eba\u5de5\u4e13\u5bb6\uff0c\u8017\u65f6\u8017\u529b\u4e14\u96be\u4ee5\u5927\u89c4\u6a21\u6269\u5c55\u3002\u9700\u8981\u5229\u7528AI\u6280\u672f\u5b9e\u73b0\u5723\u8bad\u6587\u672c\u7684\u81ea\u52a8\u5316\u5904\u7406\u548c\u4e30\u5bcc\u6807\u6ce8\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u548c\u4f0a\u65af\u5170\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u57fa\u4e8e\u6570\u5b57\u8d44\u6e90\u5e93\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u5206\u5272\u3001\u4f20\u8ff0\u94fe-\u6587\u672c\u5206\u79bb\u3001\u9a8c\u8bc1\u548c\u591a\u5c42\u589e\u5f3a\uff0c\u5305\u62ec\u673a\u5668\u7ffb\u8bd1\u3001\u667a\u80fd\u6807\u6ce8\u3001\u6458\u8981\u751f\u6210\u3001\u4e3b\u9898\u6807\u8bb0\u548c\u8de8\u6587\u672c\u8bed\u4e49\u5206\u6790\u3002", "result": "\u57281,213\u6761\u968f\u673a\u6837\u672c\u7684\u4e13\u5bb6\u8bc4\u4f30\u4e2d\uff0c\u7ed3\u6784\u5316\u4efb\u52a1\u63a5\u8fd1\u4eba\u7c7b\u51c6\u786e\u5ea6\uff08\u4f20\u8ff0\u94fe-\u6587\u672c\u5206\u79bb9.33/10\uff0c\u6458\u89819.33/10\uff09\uff0c\u603b\u4f53\u8bc4\u52068.46/10\u663e\u8457\u4f18\u4e8e\u4eba\u5de5\u8bed\u6599\u5e93\u76843.66/10\uff0c\u4e14\u6210\u672c\u4ec5\u4e3a\u4f20\u7edf\u65b9\u6cd5\u7684\u6781\u5c0f\u90e8\u5206\u3002", "conclusion": "AI\u80fd\u591f\u589e\u5f3a\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u4f0a\u65af\u5170\u9057\u4ea7\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u548c\u8bed\u4e49\u4e30\u5bcc\u7684\u8bbf\u95ee\u65b9\u5f0f\uff0c\u5f00\u521b\u4e86\u5b97\u6559\u6587\u672c\u5904\u7406\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.03501", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03501", "abs": "https://arxiv.org/abs/2510.03501", "authors": ["Lyes Saad Saoud", "Loic Lesobre", "Enrico Sorato", "Irfan Hussain"], "title": "Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms", "comment": null, "summary": "Real-time animal detection and segmentation in natural environments are vital\nfor wildlife conservation, enabling non-invasive monitoring through remote\ncamera streams. However, these tasks remain challenging due to limited\ncomputational resources and the cryptic appearance of many species. We propose\na mobile-optimized two-stage deep learning framework that integrates a\nThreading Detection Model (TDM) to parallelize YOLOv10-based detection and\nMobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach\nimproves real-time performance by reducing latency through threading. YOLOv10\nhandles detection while MobileSAM performs lightweight segmentation, both\nexecuted concurrently for efficient resource use. On the cryptic Houbara\nBustard, a conservation-priority species, our model achieves mAP50 of 0.9627,\nmAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10\noperates at 43.7 ms per frame, confirming real-time readiness. We introduce a\ncurated Houbara dataset of 40,000 annotated images to support model training\nand evaluation across diverse conditions. The code and dataset used in this\nstudy are publicly available on GitHub at\nhttps://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos\nand additional resources, visit\nhttps://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79fb\u52a8\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u7a0b\u5316\u68c0\u6d4b\u6a21\u578b\u5e76\u884c\u5316YOLOv10\u68c0\u6d4b\u548cMobileSAM\u5206\u5272\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6027\u80fd\uff0c\u5728Houbara\u9e28\u9e1f\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "motivation": "\u81ea\u7136\u73af\u5883\u4e2d\u5b9e\u65f6\u52a8\u7269\u68c0\u6d4b\u548c\u5206\u5272\u5bf9\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u7269\u79cd\u9690\u853d\u5916\u89c2\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u96c6\u6210\u7ebf\u7a0b\u5316\u68c0\u6d4b\u6a21\u578b(TDM)\uff0c\u5e76\u884c\u6267\u884cYOLOv10\u68c0\u6d4b\u548cMobileSAM\u8f7b\u91cf\u7ea7\u5206\u5272\uff0c\u901a\u8fc7\u7ebf\u7a0b\u5316\u51cf\u5c11\u5ef6\u8fdf\u3002", "result": "\u5728Houbara\u9e28\u9e1f\u4e0a\u5b9e\u73b0mAP50 0.9627\u3001mAP75 0.7731\u3001mAP95 0.7178\u3001MobileSAM mIoU 0.7421\uff0cYOLOv10\u6bcf\u5e27\u5904\u7406\u65f6\u95f443.7\u6beb\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u52a8\u7269\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u5e76\u63d0\u4f9b\u4e86\u5305\u542b40,000\u5f20\u6807\u6ce8\u56fe\u50cf\u7684Houbara\u6570\u636e\u96c6\u3002"}}
{"id": "2510.03799", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.03799", "abs": "https://arxiv.org/abs/2510.03799", "authors": ["Hadi Asghari", "Sami Nenno"], "title": "Mechanistic Interpretability of Socio-Political Frames in Language Models", "comment": "Peer-reviewed and presented at Advances in Interpretable Machine\n  Learning and Artificial Intelligence (AIMLAI) Workshop at ECML/PKDD 2024", "summary": "This paper explores the ability of large language models to generate and\nrecognize deep cognitive frames, particularly in socio-political contexts. We\ndemonstrate that LLMs are highly fluent in generating texts that evoke specific\nframes and can recognize these frames in zero-shot settings. Inspired by\nmechanistic interpretability research, we investigate the location of the\n`strict father' and `nurturing parent' frames within the model's hidden\nrepresentation, identifying singular dimensions that correlate strongly with\ntheir presence. Our findings contribute to understanding how LLMs capture and\nexpress meaningful human concepts.", "AI": {"tldr": "LLMs\u80fd\u591f\u751f\u6210\u548c\u8bc6\u522b\u6df1\u5c42\u8ba4\u77e5\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u793e\u4f1a\u653f\u6cbb\u8bed\u5883\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u9690\u85cf\u8868\u793a\u4e2d\u5b58\u5728\u4e0e\u7279\u5b9a\u6846\u67b6\u76f8\u5173\u7684\u5355\u7ef4\u5ea6\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u548c\u8bc6\u522b\u6df1\u5c42\u8ba4\u77e5\u6846\u67b6\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u793e\u4f1a\u653f\u6cbb\u8bed\u5883\u4e2d\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u6355\u6349\u548c\u8868\u8fbe\u6709\u610f\u4e49\u7684\u4eba\u7c7b\u6982\u5ff5\u3002", "method": "\u91c7\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5206\u6790\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e2d'\u4e25\u683c\u7236\u4eb2'\u548c'\u517b\u80b2\u7236\u6bcd'\u6846\u67b6\u7684\u4f4d\u7f6e\uff0c\u8bc6\u522b\u4e0e\u8fd9\u4e9b\u6846\u67b6\u5b58\u5728\u5f3a\u70c8\u76f8\u5173\u7684\u5355\u7ef4\u5ea6\u3002", "result": "LLMs\u5728\u751f\u6210\u5524\u8d77\u7279\u5b9a\u6846\u67b6\u7684\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u5ea6\u6d41\u7545\u6027\uff0c\u5e76\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u80fd\u591f\u8bc6\u522b\u8fd9\u4e9b\u6846\u67b6\u3002\u5728\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e2d\u53d1\u73b0\u4e86\u4e0e\u7279\u5b9a\u6846\u67b6\u5f3a\u70c8\u76f8\u5173\u7684\u5355\u7ef4\u5ea6\u7279\u5f81\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u6355\u6349\u548c\u8868\u8fbe\u6709\u610f\u4e49\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u8fd9\u6709\u52a9\u4e8e\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u4eba\u7c7b\u6982\u5ff5\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2510.03511", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03511", "abs": "https://arxiv.org/abs/2510.03511", "authors": ["Mohammad Mohaiminul Islam", "Rishabh Anand", "David R. Wessels", "Friso de Kruiff", "Thijs P. Kuipers", "Rex Ying", "Clara I. S\u00e1nchez", "Sharvaree Vadgama", "Georg B\u00f6kman", "Erik J. Bekkers"], "title": "Platonic Transformers: A Solid Choice For Equivariance", "comment": null, "summary": "While widespread, Transformers lack inductive biases for geometric symmetries\ncommon in science and computer vision. Existing equivariant methods often\nsacrifice the efficiency and flexibility that make Transformers so effective\nthrough complex, computationally intensive designs. We introduce the Platonic\nTransformer to resolve this trade-off. By defining attention relative to\nreference frames from the Platonic solid symmetry groups, our method induces a\nprincipled weight-sharing scheme. This enables combined equivariance to\ncontinuous translations and Platonic symmetries, while preserving the exact\narchitecture and computational cost of a standard Transformer. Furthermore, we\nshow that this attention is formally equivalent to a dynamic group convolution,\nwhich reveals that the model learns adaptive geometric filters and enables a\nhighly scalable, linear-time convolutional variant. Across diverse benchmarks\nin computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular\nproperty prediction (QM9, OMol25), the Platonic Transformer achieves\ncompetitive performance by leveraging these geometric constraints at no\nadditional cost.", "AI": {"tldr": "Platonic Transformer\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u67cf\u62c9\u56fe\u7acb\u4f53\u5bf9\u79f0\u7fa4\u7684\u53c2\u8003\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fde\u7eed\u5e73\u79fb\u548c\u67cf\u62c9\u56fe\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6Transformer\u7684\u67b6\u6784\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684Transformer\u7f3a\u4e4f\u5bf9\u79d1\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e38\u89c1\u51e0\u4f55\u5bf9\u79f0\u6027\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u800c\u73b0\u6709\u7684\u7b49\u53d8\u65b9\u6cd5\u5f80\u5f80\u901a\u8fc7\u590d\u6742\u3001\u8ba1\u7b97\u5bc6\u96c6\u7684\u8bbe\u8ba1\u727a\u7272\u4e86Transformer\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u76f8\u5bf9\u4e8e\u67cf\u62c9\u56fe\u7acb\u4f53\u5bf9\u79f0\u7fa4\u53c2\u8003\u6846\u67b6\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5f15\u5165\u539f\u5219\u6027\u7684\u6743\u91cd\u5171\u4eab\u65b9\u6848\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u5bf9\u8fde\u7eed\u5e73\u79fb\u548c\u67cf\u62c9\u56fe\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\u3002", "result": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\uff08CIFAR-10\uff09\u30013D\u70b9\u4e91\uff08ScanObjectNN\uff09\u548c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\uff08QM9\u3001OMol25\uff09\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPlatonic Transformer\u901a\u8fc7\u5229\u7528\u51e0\u4f55\u7ea6\u675f\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Platonic Transformer\u6210\u529f\u89e3\u51b3\u4e86Transformer\u7f3a\u4e4f\u51e0\u4f55\u5bf9\u79f0\u6027\u5f52\u7eb3\u504f\u7f6e\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6807\u51c6Transformer\u6548\u7387\u548c\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u51e0\u4f55\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\u3002"}}
{"id": "2510.03805", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03805", "abs": "https://arxiv.org/abs/2510.03805", "authors": ["Canhui Wu", "Qiong Cao", "Chang Li", "Zhenfang Wang", "Chao Xue", "Yuwei Fan", "Wei Xi", "Xiaodong He"], "title": "Beyond Token Length: Step Pruner for Efficient and Accurate Reasoning in Large Language Models", "comment": "20pages, 7 figures", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex tasks\nbut often suffer from excessive verbosity, known as \"overthinking.\" Existing\nsolutions via reinforcement learning (RL) typically penalize generated tokens\nto promote conciseness. However, these methods encounter two challenges:\nresponses with fewer tokens do not always correspond to fewer reasoning steps,\nand models may develop hacking behavior in later stages of training by\ndiscarding reasoning steps to minimize token usage. In this work, we introduce\n\\textbf{Step Pruner (SP)}, an RL framework that steers LRMs toward more\nefficient reasoning by favoring compact reasoning steps. Our step-aware reward\nfunction prioritizes correctness while imposing penalties for redundant steps,\nand withholds rewards for incorrect responses to prevent the reinforcement of\nerroneous reasoning. Moreover, we propose a dynamic stopping mechanism: when\nthe length of any output step exceeds the upper limit, we halt updates to\nprevent hacking behavior caused by merging steps. Extensive experiments across\nfour reasoning benchmarks demonstrate that SP achieves state-of-the-art\naccuracy while significantly reducing response length. For instance, on AIME24,\nSP reduces token usage by \\textbf{69.7\\%}.", "AI": {"tldr": "\u63d0\u51faStep Pruner(SP)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u60e9\u7f5a\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u6765\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u957f\u5ea6\u3002", "motivation": "\u73b0\u6709\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u60e9\u7f5a\u751f\u6210token\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u8f83\u5c11token\u4e0d\u4e00\u5b9a\u5bf9\u5e94\u8f83\u5c11\u63a8\u7406\u6b65\u9aa4\uff1b\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u4e22\u5f03\u63a8\u7406\u6b65\u9aa4\u6765\u6700\u5c0f\u5316token\u4f7f\u7528\uff0c\u4ea7\u751fhacking\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6b65\u9aa4\u611f\u77e5\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4f18\u5148\u8003\u8651\u6b63\u786e\u6027\u540c\u65f6\u60e9\u7f5a\u5197\u4f59\u6b65\u9aa4\uff0c\u5bf9\u9519\u8bef\u54cd\u5e94\u4e0d\u7ed9\u4e88\u5956\u52b1\uff1b\u5f15\u5165\u52a8\u6001\u505c\u6b62\u673a\u5236\uff0c\u5f53\u8f93\u51fa\u6b65\u9aa4\u957f\u5ea6\u8d85\u8fc7\u4e0a\u9650\u65f6\u505c\u6b62\u66f4\u65b0\u4ee5\u9632\u6b62\u6b65\u9aa4\u5408\u5e76\u5bfc\u81f4\u7684hacking\u884c\u4e3a\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\u3002\u5728AIME24\u4e0a\u51cf\u5c1169.7%\u7684token\u4f7f\u7528\u3002", "conclusion": "Step Pruner\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u6b65\u9aa4\u7ea7\u522b\u7684\u4f18\u5316\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.03540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03540", "abs": "https://arxiv.org/abs/2510.03540", "authors": ["Manuel Schwonberg", "Hanno Gottschalk"], "title": "Domain Generalization for Semantic Segmentation: A Survey", "comment": "Accepted to CVPR2025W", "summary": "The generalization of deep neural networks to unknown domains is a major\nchallenge despite their tremendous progress in recent years. For this reason,\nthe dynamic area of domain generalization (DG) has emerged. In contrast to\nunsupervised domain adaptation, there is no access to or knowledge about the\ntarget domains, and DG methods aim to generalize across multiple different\nunseen target domains. Domain generalization is particularly relevant for the\ntask semantic segmentation which is used in several areas such as biomedicine\nor automated driving. This survey provides a comprehensive overview of the\nrapidly evolving topic of domain generalized semantic segmentation. We cluster\nand review existing approaches and identify the paradigm shift towards\nfoundation-model-based domain generalization. Finally, we provide an extensive\nperformance comparison of all approaches, which highlights the significant\ninfluence of foundation models on domain generalization. This survey seeks to\nadvance domain generalization research and inspire scientists to explore new\nresearch directions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u7684\u7efc\u8ff0\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u672a\u77e5\u9886\u57df\u7684\u6cdb\u5316\u6311\u6218\uff0c\u4ee5\u53ca\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u5e26\u6765\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u672a\u77e5\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8fd9\u5bf9\u751f\u7269\u533b\u5b66\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u65e8\u5728\u5728\u591a\u4e2a\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u9886\u57df\u4e0a\u8fdb\u884c\u6cdb\u5316\uff0c\u800c\u4e0d\u9700\u8981\u8bbf\u95ee\u6216\u4e86\u89e3\u76ee\u6807\u9886\u57df\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u8fdb\u884c\u805a\u7c7b\u548c\u56de\u987e\uff0c\u8bc6\u522b\u4e86\u5411\u57fa\u7840\u6a21\u578b\u8303\u5f0f\u8f6c\u53d8\u7684\u8d8b\u52bf\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u6027\u80fd\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u8c03\u67e5\u663e\u793a\u57fa\u7840\u6a21\u578b\u5bf9\u9886\u57df\u6cdb\u5316\u5177\u6709\u663e\u8457\u5f71\u54cd\uff0c\u6027\u80fd\u6bd4\u8f83\u7a81\u51fa\u4e86\u57fa\u7840\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u63a8\u52a8\u9886\u57df\u6cdb\u5316\u7814\u7a76\uff0c\u5e76\u6fc0\u52b1\u79d1\u5b66\u5bb6\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u57fa\u7840\u6a21\u578b\u9a71\u52a8\u7684\u9886\u57df\u6cdb\u5316\u65b9\u9762\u3002"}}
{"id": "2510.03808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03808", "abs": "https://arxiv.org/abs/2510.03808", "authors": ["Mehedi Hasan Emon"], "title": "Annotate Rhetorical Relations with INCEpTION: A Comparison with Automatic Approaches", "comment": null, "summary": "This research explores the annotation of rhetorical relations in discourse\nusing the INCEpTION tool and compares manual annotation with automatic\napproaches based on large language models. The study focuses on sports reports\n(specifically cricket news) and evaluates the performance of BERT, DistilBERT,\nand Logistic Regression models in classifying rhetorical relations such as\nelaboration, contrast, background, and cause-effect. The results show that\nDistilBERT achieved the highest accuracy, highlighting its potential for\nefficient discourse relation prediction. This work contributes to the growing\nintersection of discourse parsing and transformer-based NLP. (This paper was\nconducted as part of an academic requirement under the supervision of Prof. Dr.\nRalf Klabunde, Linguistic Data Science Lab, Ruhr University Bochum.) Keywords:\nRhetorical Structure Theory, INCEpTION, BERT, DistilBERT, Discourse Parsing,\nNLP.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528INCEpTION\u5de5\u5177\u6807\u6ce8\u4fee\u8f9e\u5173\u7cfb\uff0c\u6bd4\u8f83\u4e86\u624b\u52a8\u6807\u6ce8\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u65b9\u6cd5\u5728\u677f\u7403\u65b0\u95fb\u4e2d\u7684\u8868\u73b0\u3002DistilBERT\u5728\u5206\u7c7b\u4fee\u8f9e\u5173\u7cfb\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22\u4fee\u8f9e\u5173\u7cfb\u6807\u6ce8\u65b9\u6cd5\uff0c\u6bd4\u8f83\u624b\u52a8\u6807\u6ce8\u4e0e\u81ea\u52a8\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f53\u80b2\u62a5\u9053\u9886\u57df\uff0c\u4fc3\u8fdb\u8bdd\u8bed\u89e3\u6790\u4e0e\u57fa\u4e8eTransformer\u7684NLP\u6280\u672f\u7684\u7ed3\u5408\u3002", "method": "\u4f7f\u7528INCEpTION\u5de5\u5177\u8fdb\u884c\u4fee\u8f9e\u5173\u7cfb\u6807\u6ce8\uff0c\u8bc4\u4f30BERT\u3001DistilBERT\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5728\u5206\u7c7b\u4fee\u8f9e\u5173\u7cfb\uff08\u5982\u9610\u8ff0\u3001\u5bf9\u6bd4\u3001\u80cc\u666f\u3001\u56e0\u679c\u5173\u7cfb\uff09\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "DistilBERT\u6a21\u578b\u5728\u4fee\u8f9e\u5173\u7cfb\u5206\u7c7b\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u663e\u793a\u51fa\u5176\u5728\u8bdd\u8bed\u5173\u7cfb\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bdd\u8bed\u89e3\u6790\u4e0e\u57fa\u4e8eTransformer\u7684NLP\u6280\u672f\u7684\u4ea4\u53c9\u9886\u57df\u505a\u51fa\u8d21\u732e\uff0c\u8bc1\u660e\u4e86DistilBERT\u5728\u4fee\u8f9e\u5173\u7cfb\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.03543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03543", "abs": "https://arxiv.org/abs/2510.03543", "authors": ["Evandros Kaklamanos", "Kristjana Kristinsdottir", "Jonathan Huang", "Dustin Carlson", "Rajesh Keswani", "John Pandolfino", "Mozziyar Etemadi"], "title": "From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy", "comment": null, "summary": "Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and\ncolonoscopy play a critical role in diagnosing and managing gastrointestinal\n(GI) disorders. However, the documentation burden associated with these\nprocedures place significant strain on gastroenterologists, contributing to\ninefficiencies in clinical workflows and physician burnout. To address this\nchallenge, we propose a novel automated report generation model that leverages\na transformer-based vision encoder and text decoder within a two-stage training\nframework. In the first stage, both components are pre-trained on image/text\ncaption pairs to capture generalized vision-language features, followed by\nfine-tuning on images/report pairs to generate clinically meaningful findings.\nOur approach not only streamlines the documentation process but also holds\npromise for reducing physician workload and improving patient care.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8etransformer\u7684\u81ea\u52a8\u62a5\u544a\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u7b80\u5316\u5185\u7aa5\u955c\u7a0b\u5e8f\u6587\u6863\u8bb0\u5f55", "motivation": "\u5185\u7aa5\u955c\u68c0\u67e5\u7684\u6587\u6863\u8bb0\u5f55\u8d1f\u62c5\u7ed9\u80c3\u80a0\u75c5\u5b66\u5bb6\u5e26\u6765\u5de8\u5927\u538b\u529b\uff0c\u5bfc\u81f4\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u548c\u533b\u751f\u5026\u6020", "method": "\u4f7f\u7528\u57fa\u4e8etransformer\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u6587\u672c\u89e3\u7801\u5668\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5728\u56fe\u50cf/\u6587\u672c\u63cf\u8ff0\u5bf9\u4e0a\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u56fe\u50cf/\u62a5\u544a\u5bf9\u4e0a\u5fae\u8c03", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u53d1\u73b0\uff0c\u7b80\u5316\u6587\u6863\u8bb0\u5f55\u8fc7\u7a0b", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u51cf\u8f7b\u533b\u751f\u5de5\u4f5c\u91cf\u5e76\u6539\u5584\u60a3\u8005\u62a4\u7406"}}
{"id": "2510.03898", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03898", "abs": "https://arxiv.org/abs/2510.03898", "authors": ["Nusrat Jahan Lia", "Shubhashis Roy Dipta", "Abdullah Khan Zehady", "Naymul Islam", "Madhusodan Chakraborty", "Abdullah Al Wasif"], "title": "Read Between the Lines: A Benchmark for Uncovering Political Bias in Bangla News Articles", "comment": null, "summary": "Detecting media bias is crucial, specifically in the South Asian region.\nDespite this, annotated datasets and computational studies for Bangla political\nbias research remain scarce. Crucially because, political stance detection in\nBangla news requires understanding of linguistic cues, cultural context, subtle\nbiases, rhetorical strategies, code-switching, implicit sentiment, and\nsocio-political background. To address this, we introduce the first benchmark\ndataset of 200 politically significant and highly debated Bangla news articles,\nlabeled for government-leaning, government-critique, and neutral stances,\nalongside diagnostic analyses for evaluating large language models (LLMs). Our\ncomprehensive evaluation of 28 proprietary and open-source LLMs shows strong\nperformance in detecting government-critique content (F1 up to 0.83) but\nsubstantial difficulty with neutral articles (F1 as low as 0.00). Models also\ntend to over-predict government-leaning stances, often misinterpreting\nambiguous narratives. This dataset and its associated diagnostics provide a\nfoundation for advancing stance detection in Bangla media research and offer\ninsights for improving LLM performance in low-resource languages.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u7acb\u573a\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b200\u7bc7\u65b0\u95fb\u6587\u7ae0\uff0c\u6807\u6ce8\u4e86\u4eb2\u653f\u5e9c\u3001\u6279\u8bc4\u653f\u5e9c\u548c\u4e2d\u6027\u7acb\u573a\u3002\u8bc4\u4f30\u4e8628\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u68c0\u6d4b\u6279\u8bc4\u653f\u5e9c\u5185\u5bb9\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u4e2d\u6027\u6587\u7ae0\u65f6\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u5357\u4e9a\u5730\u533a\u7684\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u5f88\u91cd\u8981\uff0c\u4f46\u5b5f\u52a0\u62c9\u8bed\u653f\u6cbb\u7acb\u573a\u7814\u7a76\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\uff0c\u9700\u8981\u7406\u89e3\u8bed\u8a00\u7ebf\u7d22\u3001\u6587\u5316\u80cc\u666f\u3001\u5fae\u5999\u504f\u89c1\u3001\u4fee\u8f9e\u7b56\u7565\u7b49\u591a\u79cd\u56e0\u7d20\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b200\u7bc7\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6587\u7ae0\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e09\u79cd\u653f\u6cbb\u7acb\u573a\uff08\u4eb2\u653f\u5e9c\u3001\u6279\u8bc4\u653f\u5e9c\u3001\u4e2d\u6027\uff09\uff0c\u5e76\u5bf928\u4e2a\u4e13\u6709\u548c\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u68c0\u6d4b\u6279\u8bc4\u653f\u5e9c\u5185\u5bb9\u65f6\u8868\u73b0\u5f3a\u52b2\uff08F1\u6700\u9ad8\u8fbe0.83\uff09\uff0c\u4f46\u5728\u8bc6\u522b\u4e2d\u6027\u6587\u7ae0\u65f6\u5b58\u5728\u663e\u8457\u56f0\u96be\uff08F1\u6700\u4f4e\u4e3a0.00\uff09\u3002\u6a21\u578b\u503e\u5411\u4e8e\u8fc7\u5ea6\u9884\u6d4b\u4eb2\u653f\u5e9c\u7acb\u573a\uff0c\u7ecf\u5e38\u8bef\u89e3\u6a21\u7cca\u53d9\u8ff0\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u53ca\u5176\u76f8\u5173\u8bca\u65ad\u4e3a\u63a8\u8fdb\u5b5f\u52a0\u62c9\u8bed\u5a92\u4f53\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.03545", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "AI": {"tldr": "SketchPlan\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u5668\uff0c\u901a\u8fc72D\u624b\u7ed8\u8349\u56fe\u5728\u6df1\u5ea6\u56fe\u50cf\u4e0a\u751f\u6210\u65e0\u4eba\u673a3D\u98de\u884c\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u7684\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u5982\u4f55\u8ba9\u65e0\u4eba\u673a\u901a\u8fc7\u4eba\u7c7b\u76f4\u89c2\u76842D\u624b\u7ed8\u8349\u56fe\u6765\u7406\u89e3\u5bfc\u822a\u610f\u56fe\uff0c\u5e76\u751f\u6210\u5b89\u5168\u53ef\u9760\u76843D\u98de\u884c\u8def\u5f84\u7684\u95ee\u9898\u3002", "method": "\u7cfb\u7edf\u5305\u542bSketchAdapter\uff08\u5c06\u624b\u7ed8\u8349\u56fe\u6620\u5c04\u52302D\u6295\u5f71\u8def\u5f84\uff09\u548cDiffPath\uff08\u4ece2D\u6295\u5f71\u548c\u7b2c\u4e00\u4eba\u79f0\u6df1\u5ea6\u56fe\u50cf\u63a8\u65ad3D\u8f68\u8ff9\u7684\u6269\u6563\u6a21\u578b\uff09\uff0c\u4f7f\u7528\u6df7\u5408\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u6d4b\u8bd5\u4e2d\uff0c\u4f4e/\u4e2d\u7b49\u969c\u788d\u7269\u73af\u5883\u4e0b\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u9ad8\u969c\u788d\u7269\u73af\u5883\u4e0b\u8fbe\u523040%\u6210\u529f\u7387\uff0c\u6bd4\u5173\u952e\u6d88\u878d\u5b9e\u9a8c\u9ad8\u51fa20-60%\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "conclusion": "\u6df7\u5408\u4eba\u5de5\u6807\u6ce8\u548c\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7ed3\u5408\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7406\u89e3\u4eba\u7c7b\u610f\u56fe\u548c\u63a8\u65ad3D\u8def\u5f84\u7684\u80fd\u529b\u3002"}}
{"id": "2510.03913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03913", "abs": "https://arxiv.org/abs/2510.03913", "authors": ["Mohammad Amin Abbasi", "Hassan Naderi"], "title": "PsycholexTherapy: Simulating Reasoning in Psychotherapy with Small Language Models in Persian", "comment": null, "summary": "This study presents PsychoLexTherapy, a framework for simulating\npsychotherapeutic reasoning in Persian using small language models (SLMs). The\nframework tackles the challenge of developing culturally grounded,\ntherapeutically coherent dialogue systems with structured memory for multi-turn\ninteractions in underrepresented languages. To ensure privacy and feasibility,\nPsychoLexTherapy is optimized for on-device deployment, enabling use without\nexternal servers. Development followed a three-stage process: (i) assessing\nSLMs psychological knowledge with PsychoLexEval; (ii) designing and\nimplementing the reasoning-oriented PsychoLexTherapy framework; and (iii)\nconstructing two evaluation datasets-PsychoLexQuery (real Persian user\nquestions) and PsychoLexDialogue (hybrid simulated sessions)-to benchmark\nagainst multiple baselines. Experiments compared simple prompting, multi-agent\ndebate, and structured therapeutic reasoning paths. Results showed that\ndeliberate model selection balanced accuracy, efficiency, and privacy. On\nPsychoLexQuery, PsychoLexTherapy outperformed all baselines in automatic\nLLM-as-a-judge evaluation and was ranked highest by human evaluators in a\nsingle-turn preference study. In multi-turn tests with PsychoLexDialogue, the\nlong-term memory module proved essential: while naive history concatenation\ncaused incoherence and information loss, the full framework achieved the\nhighest ratings in empathy, coherence, cultural fit, and personalization.\nOverall, PsychoLexTherapy establishes a practical, privacy-preserving, and\nculturally aligned foundation for Persian psychotherapy simulation,\ncontributing novel datasets, a reproducible evaluation pipeline, and empirical\ninsights into structured memory for therapeutic reasoning.", "AI": {"tldr": "PsychoLexTherapy\u662f\u4e00\u4e2a\u9488\u5bf9\u6ce2\u65af\u8bed\u7684\u8f7b\u91cf\u7ea7\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u6846\u67b6\uff0c\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u672c\u5730\u90e8\u7f72\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u6a21\u5757\u652f\u6301\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5728\u6587\u5316\u9002\u5e94\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u4e2d\u5f00\u53d1\u6587\u5316\u9002\u5e94\u3001\u6cbb\u7597\u8fde\u8d2f\u4e14\u5177\u6709\u7ed3\u6784\u5316\u8bb0\u5fc6\u7684\u5fc3\u7406\u6cbb\u7597\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6311\u6218\uff0c\u540c\u65f6\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u548c\u672c\u5730\u90e8\u7f72\u53ef\u884c\u6027\u3002", "method": "\u4e09\u9636\u6bb5\u5f00\u53d1\u6d41\u7a0b\uff1a1) \u4f7f\u7528PsychoLexEval\u8bc4\u4f30SLMs\u5fc3\u7406\u77e5\u8bc6\uff1b2) \u8bbe\u8ba1\u9762\u5411\u63a8\u7406\u7684PsychoLexTherapy\u6846\u67b6\uff1b3) \u6784\u5efa\u4e24\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\uff08PsychoLexQuery\u548cPsychoLexDialogue\uff09\uff0c\u6bd4\u8f83\u7b80\u5355\u63d0\u793a\u3001\u591a\u4ee3\u7406\u8fa9\u8bba\u548c\u7ed3\u6784\u5316\u6cbb\u7597\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728\u5355\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0cPsychoLexTherapy\u5728\u81ea\u52a8LLM\u8bc4\u4f30\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u6700\u4f73\uff1b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff0c\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u81f3\u5173\u91cd\u8981\uff0c\u5b8c\u6574\u6846\u67b6\u5728\u540c\u7406\u5fc3\u3001\u8fde\u8d2f\u6027\u3001\u6587\u5316\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\u65b9\u9762\u83b7\u5f97\u6700\u9ad8\u8bc4\u5206\u3002", "conclusion": "PsychoLexTherapy\u4e3a\u6ce2\u65af\u8bed\u5fc3\u7406\u6cbb\u7597\u6a21\u62df\u5efa\u7acb\u4e86\u5b9e\u7528\u3001\u9690\u79c1\u4fdd\u62a4\u4e14\u6587\u5316\u5bf9\u9f50\u7684\u57fa\u7840\uff0c\u8d21\u732e\u4e86\u65b0\u9896\u6570\u636e\u96c6\u3001\u53ef\u590d\u73b0\u8bc4\u4f30\u6d41\u7a0b\u4ee5\u53ca\u7ed3\u6784\u5316\u8bb0\u5fc6\u5bf9\u6cbb\u7597\u63a8\u7406\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2510.03548", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03548", "abs": "https://arxiv.org/abs/2510.03548", "authors": ["Danial Samadi Vahdati", "Tai Duc Nguyen", "Ekta Prashnani", "Koki Nagano", "David Luebke", "Orazio Gallo", "Matthew Stamm"], "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing", "comment": null, "summary": "AI-based talking-head videoconferencing systems reduce bandwidth by sending a\ncompact pose-expression latent and re-synthesizing RGB at the receiver, but\nthis latent can be puppeteered, letting an attacker hijack a victim's likeness\nin real time. Because every frame is synthetic, deepfake and synthetic video\ndetectors fail outright. To address this security problem, we exploit a key\nobservation: the pose-expression latent inherently contains biometric\ninformation of the driving identity. Therefore, we introduce the first\nbiometric leakage defense without ever looking at the reconstructed RGB video:\na pose-conditioned, large-margin contrastive encoder that isolates persistent\nidentity cues inside the transmitted latent while cancelling transient pose and\nexpression. A simple cosine test on this disentangled embedding flags illicit\nidentity swaps as the video is rendered. Our experiments on multiple\ntalking-head generation models show that our method consistently outperforms\nexisting puppeteering defenses, operates in real-time, and shows strong\ngeneralization to out-of-distribution scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9AI\u89c6\u9891\u4f1a\u8bae\u4e2d\u8eab\u4efd\u52ab\u6301\u653b\u51fb\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u7a7a\u95f4\u4e2d\u63d0\u53d6\u751f\u7269\u7279\u5f81\u4fe1\u606f\u6765\u68c0\u6d4b\u975e\u6cd5\u8eab\u4efd\u66ff\u6362\uff0c\u65e0\u9700\u67e5\u770b\u91cd\u5efa\u7684RGB\u89c6\u9891\u3002", "motivation": "AI\u89c6\u9891\u4f1a\u8bae\u7cfb\u7edf\u901a\u8fc7\u4f20\u8f93\u7d27\u51d1\u7684\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u7a7a\u95f4\u6765\u8282\u7701\u5e26\u5bbd\uff0c\u4f46\u8be5\u6f5c\u5728\u7a7a\u95f4\u53ef\u80fd\u88ab\u52ab\u6301\u7528\u4e8e\u5b9e\u65f6\u5192\u5145\u53d7\u5bb3\u8005\u3002\u7531\u4e8e\u6bcf\u5e27\u90fd\u662f\u5408\u6210\u7684\uff0c\u4f20\u7edf\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5668\u65e0\u6cd5\u5de5\u4f5c\u3002", "method": "\u4f7f\u7528\u59ff\u6001\u6761\u4ef6\u5316\u7684\u5927\u95f4\u9694\u5bf9\u6bd4\u7f16\u7801\u5668\uff0c\u4ece\u4f20\u8f93\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5206\u79bb\u51fa\u6301\u4e45\u7684\u8eab\u4efd\u7ebf\u7d22\uff0c\u540c\u65f6\u6d88\u9664\u77ac\u65f6\u7684\u59ff\u6001\u548c\u8868\u60c5\u4fe1\u606f\u3002\u901a\u8fc7\u4f59\u5f26\u6d4b\u8bd5\u68c0\u6d4b\u8eab\u4efd\u66ff\u6362\u3002", "result": "\u5728\u591a\u4e2a\u8bf4\u8bdd\u5934\u751f\u6210\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u53ef\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u5bf9\u5206\u5e03\u5916\u573a\u666f\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u59ff\u6001-\u8868\u60c5\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u751f\u7269\u7279\u5f81\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86AI\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u8eab\u4efd\u52ab\u6301\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u67e5\u770bRGB\u89c6\u9891\u7684\u5b9e\u65f6\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2510.03997", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03997", "abs": "https://arxiv.org/abs/2510.03997", "authors": ["Junjie Luo", "Rui Han", "Arshana Welivita", "Zeleikun Di", "Jingfu Wu", "Xuzhe Zhi", "Ritu Agarwal", "Gordon Gao"], "title": "Mapping Patient-Perceived Physician Traits from Nationwide Online Reviews with LLMs", "comment": null, "summary": "Understanding how patients perceive their physicians is essential to\nimproving trust, communication, and satisfaction. We present a large language\nmodel (LLM)-based pipeline that infers Big Five personality traits and five\npatient-oriented subjective judgments. The analysis encompasses 4.1 million\npatient reviews of 226,999 U.S. physicians from an initial pool of one million.\nWe validate the method through multi-model comparison and human expert\nbenchmarking, achieving strong agreement between human and LLM assessments\n(correlation coefficients 0.72-0.89) and external validity through correlations\nwith patient satisfaction (r = 0.41-0.81, all p<0.001). National-scale analysis\nreveals systematic patterns: male physicians receive higher ratings across all\ntraits, with largest disparities in clinical competence perceptions;\nempathy-related traits predominate in pediatrics and psychiatry; and all traits\npositively predict overall satisfaction. Cluster analysis identifies four\ndistinct physician archetypes, from \"Well-Rounded Excellent\" (33.8%, uniformly\nhigh traits) to \"Underperforming\" (22.6%, consistently low). These findings\ndemonstrate that automated trait extraction from patient narratives can provide\ninterpretable, validated metrics for understanding physician-patient\nrelationships at scale, with implications for quality measurement, bias\ndetection, and workforce development in healthcare.", "AI": {"tldr": "\u4f7f\u7528LLM\u4ece410\u4e07\u60a3\u8005\u8bc4\u4ef7\u4e2d\u63d0\u53d6\u533b\u751f\u7684\u5927\u4e94\u4eba\u683c\u7279\u8d28\u548c\u4e3b\u89c2\u5224\u65ad\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u6027\u522b\u5dee\u5f02\u3001\u4e13\u79d1\u5dee\u5f02\u7b49\u7cfb\u7edf\u6027\u6a21\u5f0f\uff0c\u5e76\u8bc6\u522b\u51fa\u56db\u79cd\u533b\u751f\u539f\u578b\u3002", "motivation": "\u7406\u89e3\u60a3\u8005\u5bf9\u533b\u751f\u7684\u8ba4\u77e5\u5bf9\u4e8e\u6539\u5584\u4fe1\u4efb\u3001\u6c9f\u901a\u548c\u6ee1\u610f\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5927\u89c4\u6a21\u5206\u6790\u60a3\u8005\u8bc4\u4ef7\u6765\u83b7\u5f97\u53ef\u89e3\u91ca\u7684\u6307\u6807\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u6c34\u7ebf\u5206\u6790410\u4e07\u60a3\u8005\u8bc4\u4ef7\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u6bd4\u8f83\u548c\u4eba\u7c7b\u4e13\u5bb6\u57fa\u51c6\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u4eba\u7c7b\u4e0eLLM\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff08\u76f8\u5173\u7cfb\u65700.72-0.89\uff09\uff0c\u4e0e\u60a3\u8005\u6ee1\u610f\u5ea6\u663e\u8457\u76f8\u5173\uff08r=0.41-0.81\uff09\uff0c\u53d1\u73b0\u7537\u6027\u533b\u751f\u5728\u6240\u6709\u7279\u8d28\u4e0a\u8bc4\u5206\u66f4\u9ad8\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u533b\u751f\u539f\u578b\u3002", "conclusion": "\u4ece\u60a3\u8005\u53d9\u8ff0\u4e2d\u81ea\u52a8\u63d0\u53d6\u7279\u8d28\u53ef\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u5df2\u9a8c\u8bc1\u7684\u6307\u6807\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u7406\u89e3\u533b\u60a3\u5173\u7cfb\uff0c\u5bf9\u8d28\u91cf\u6d4b\u91cf\u3001\u504f\u89c1\u68c0\u6d4b\u548c\u533b\u7597\u4eba\u529b\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.03550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03550", "abs": "https://arxiv.org/abs/2510.03550", "authors": ["Junbao Zhou", "Yuan Zhou", "Kesen Zhao", "Qingshan Xu", "Beier Zhu", "Richang Hong", "Hanwang Zhang"], "title": "Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!", "comment": null, "summary": "Achieving streaming, fine-grained control over the outputs of autoregressive\nvideo diffusion models remains challenging, making it difficult to ensure that\nthey consistently align with user expectations. To bridge this gap, we propose\n\\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new\ntask that enables users to modify generated videos \\emph{anytime} on\n\\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and\nSG-I2V, REVEL unifies drag-style video manipulation as editing and animating\nvideo frames with both supporting user-specified translation, deformation, and\nrotation effects, making drag operations versatile. In resolving REVEL, we\nobserve: \\emph{i}) drag-induced perturbations accumulate in latent space,\ncausing severe latent distribution drift that halts the drag process;\n\\emph{ii}) streaming drag is easily disturbed by context frames, thereby\nyielding visually unnatural outcomes. We thus propose a training-free approach,\n\\textbf{DragStream}, comprising: \\emph{i}) an adaptive distribution\nself-rectification strategy that leverages neighboring frames' statistics to\neffectively constrain the drift of latent embeddings; \\emph{ii}) a\nspatial-frequency selective optimization mechanism, allowing the model to fully\nexploit contextual information while mitigating its interference via\nselectively propagating visual cues along generation. Our method can be\nseamlessly integrated into existing autoregressive video diffusion models, and\nextensive experiments firmly demonstrate the effectiveness of our DragStream.", "AI": {"tldr": "\u63d0\u51fa\u4e86REVEL\u4efb\u52a1\u548cDragStream\u65b9\u6cd5\uff0c\u5b9e\u73b0\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6d41\u5f0f\u7ec6\u7c92\u5ea6\u62d6\u62fd\u63a7\u5236\uff0c\u89e3\u51b3\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u5e27\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u96be\u4ee5\u5b9e\u73b0\u6d41\u5f0f\u7ec6\u7c92\u5ea6\u8f93\u51fa\u63a7\u5236\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u4e0e\u7528\u6237\u671f\u671b\u7684\u4e00\u81f4\u6027\uff0c\u9700\u8981\u89e3\u51b3\u62d6\u62fd\u64cd\u4f5c\u4e2d\u7684\u6f5c\u5728\u5206\u5e03\u6f02\u79fb\u548c\u4e0a\u4e0b\u6587\u5e72\u6270\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684DragStream\u65b9\u6cd5\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5206\u5e03\u81ea\u6821\u6b63\u7b56\u7565\uff08\u5229\u7528\u76f8\u90bb\u5e27\u7edf\u8ba1\u7ea6\u675f\u6f5c\u5728\u5d4c\u5165\u6f02\u79fb\uff09\u548c\u7a7a\u95f4\u9891\u7387\u9009\u62e9\u6027\u4f18\u5316\u673a\u5236\uff08\u9009\u62e9\u6027\u4f20\u64ad\u89c6\u89c9\u7ebf\u7d22\u4ee5\u51cf\u5c11\u4e0a\u4e0b\u6587\u5e72\u6270\uff09\u3002", "result": "\u65b9\u6cd5\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DragStream\u7684\u6709\u6548\u6027\u3002", "conclusion": "REVEL\u4efb\u52a1\u548cDragStream\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u62d6\u62fd\u64cd\u4f5c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6d41\u5f0f\u7ec6\u7c92\u5ea6\u89c6\u9891\u64cd\u63a7\u3002"}}
{"id": "2510.03999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03999", "abs": "https://arxiv.org/abs/2510.03999", "authors": ["Yang Xu", "Xuanming Zhang", "Min-Hsuan Yeh", "Jwala Dhamala", "Ousmane Dia", "Rahul Gupta", "Yixuan Li"], "title": "Simulating and Understanding Deceptive Behaviors in Long-Horizon Interactions", "comment": null, "summary": "Deception is a pervasive feature of human communication and an emerging\nconcern in large language models (LLMs). While recent studies document\ninstances of LLM deception under pressure, most evaluations remain confined to\nsingle-turn prompts and fail to capture the long-horizon interactions in which\ndeceptive strategies typically unfold. We introduce the first simulation\nframework for probing and evaluating deception in LLMs under extended sequences\nof interdependent tasks and dynamic contextual pressures. Our framework\ninstantiates a multi-agent system: a performer agent tasked with completing\ntasks and a supervisor agent that evaluates progress, provides feedback, and\nmaintains evolving states of trust. An independent deception auditor then\nreviews full trajectories to identify when and how deception occurs. We conduct\nextensive experiments across 11 frontier models, spanning both closed- and\nopen-source systems, and find that deception is model-dependent, increases with\nevent pressure, and consistently erodes supervisor trust. Qualitative analyses\nfurther reveal distinct strategies of concealment, equivocation, and\nfalsification. Our findings establish deception as an emergent risk in\nlong-horizon interactions and provide a foundation for evaluating future LLMs\nin real-world, trust-sensitive contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u8bc4\u4f30LLM\u6b3a\u9a97\u884c\u4e3a\u7684\u6a21\u62df\u6846\u67b6\uff0c\u53d1\u73b0\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\u3001\u968f\u538b\u529b\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u4f1a\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5c40\u9650\u4e8e\u5355\u8f6e\u63d0\u793a\uff0c\u65e0\u6cd5\u6355\u6349\u6b3a\u9a97\u7b56\u7565\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u5c55\u5f00\u8fc7\u7a0b\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u957f\u5e8f\u5217\u4efb\u52a1\u4e2dLLM\u6b3a\u9a97\u884c\u4e3a\u7684\u6846\u67b6\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff1a\u6267\u884c\u8005\u667a\u80fd\u4f53\u5b8c\u6210\u4efb\u52a1\uff0c\u76d1\u7763\u8005\u667a\u80fd\u4f53\u8bc4\u4f30\u8fdb\u5c55\u5e76\u63d0\u4f9b\u53cd\u9988\uff0c\u72ec\u7acb\u7684\u6b3a\u9a97\u5ba1\u8ba1\u5458\u5ba1\u67e5\u5b8c\u6574\u8f68\u8ff9\u4ee5\u8bc6\u522b\u6b3a\u9a97\u884c\u4e3a\u3002", "result": "\u572811\u4e2a\u524d\u6cbf\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6b3a\u9a97\u884c\u4e3a\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u6027\uff0c\u968f\u4e8b\u4ef6\u538b\u529b\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5e76\u6301\u7eed\u524a\u5f31\u76d1\u7763\u8005\u4fe1\u4efb\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u9690\u7792\u3001\u6a21\u68f1\u4e24\u53ef\u548c\u4f2a\u9020\u7b49\u4e0d\u540c\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u6b3a\u9a97\u884c\u4e3a\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u65b0\u5174\u98ce\u9669\uff0c\u4e3a\u8bc4\u4f30\u672a\u6765LLM\u5728\u73b0\u5b9e\u4e16\u754c\u4fe1\u4efb\u654f\u611f\u573a\u666f\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.03555", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03555", "abs": "https://arxiv.org/abs/2510.03555", "authors": ["Peiran Quan", "Zifan Gu", "Zhuo Zhao", "Qin Zhou", "Donghan M. Yang", "Ruichen Rong", "Yang Xie", "Guanghua Xiao"], "title": "GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis", "comment": null, "summary": "Foundation models (FMs) have transformed computational pathology by providing\npowerful, general-purpose feature extractors. However, adapting and\nbenchmarking individual FMs for specific diagnostic tasks is often\ntime-consuming and resource-intensive, especially given their scale and\ndiversity. To address this challenge, we introduce Group-Aggregative Selection\nMulti-Instance Learning (GAS-MIL), a flexible ensemble framework that\nseamlessly integrates features from multiple FMs, preserving their\ncomplementary strengths without requiring manual feature selection or extensive\ntask-specific fine-tuning. Across classification tasks in three cancer\ndatasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL\nconsistently achieves superior or on-par performance relative to individual FMs\nand established MIL methods, demonstrating its robustness and generalizability.\nBy enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines\nmodel deployment for pathology and provides a scalable foundation for future\nmultimodal and precision oncology applications.", "AI": {"tldr": "GAS-MIL\u662f\u4e00\u4e2a\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u65e0\u7f1d\u6574\u5408\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\uff0c\u65e0\u9700\u624b\u52a8\u7279\u5f81\u9009\u62e9\u6216\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u5728\u591a\u79cd\u764c\u75c7\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4f46\u9488\u5bf9\u7279\u5b9a\u8bca\u65ad\u4efb\u52a1\u8fdb\u884c\u9002\u914d\u548c\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u5176\u89c4\u6a21\u548c\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faGroup-Aggregative Selection Multi-Instance Learning (GAS-MIL)\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u96c6\u6210\u591a\u4e2a\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\uff0c\u4fdd\u7559\u5b83\u4eec\u7684\u4e92\u8865\u4f18\u52bf\u3002", "result": "\u5728\u524d\u5217\u817a\u764c(PANDA)\u3001\u5375\u5de2\u764c(UBC-OCEAN)\u548c\u4e73\u817a\u764c(TCGA-BrCa)\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGAS-MIL\u59cb\u7ec8\u4f18\u4e8e\u6216\u4e0e\u5355\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u73b0\u6709MIL\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "GAS-MIL\u901a\u8fc7\u6709\u6548\u6574\u5408\u5f02\u6784\u57fa\u7840\u6a21\u578b\uff0c\u7b80\u5316\u4e86\u75c5\u7406\u5b66\u4e2d\u7684\u6a21\u578b\u90e8\u7f72\uff0c\u5e76\u4e3a\u672a\u6765\u591a\u6a21\u6001\u548c\u7cbe\u51c6\u80bf\u7624\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.04001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04001", "abs": "https://arxiv.org/abs/2510.04001", "authors": ["Xuankang Zhang", "Jiangming Liu"], "title": "Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation", "comment": "Work in progress", "summary": "The COVID-19 pandemic causes severe social and economic disruption around the\nworld, raising various subjects that are discussed over social media.\nIdentifying pandemic-related named entities as expressed on social media is\nfundamental and important to understand the discussions about the pandemic.\nHowever, there is limited work on named entity recognition on this topic due to\nthe following challenges: 1) COVID-19 texts in social media are informal and\ntheir annotations are rare and insufficient to train a robust recognition\nmodel, and 2) named entity recognition in COVID-19 requires extensive\ndomain-specific knowledge. To address these issues, we propose a novel entity\nknowledge augmentation approach for COVID-19, which can also be applied in\ngeneral biomedical named entity recognition in both informal text format and\nformal text format. Experiments carried out on the COVID-19 tweets dataset and\nPubMed dataset show that our proposed entity knowledge augmentation improves\nNER performance in both fully-supervised and few-shot settings. Our source code\nis publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eCOVID-19\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u65b0\u578b\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u77e5\u8bc6\u9700\u6c42\u7684\u95ee\u9898\uff0c\u5728\u63a8\u6587\u548cPubMed\u6570\u636e\u96c6\u4e0a\u5747\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "COVID-19\u75ab\u60c5\u671f\u95f4\u793e\u4ea4\u5a92\u4f53\u8ba8\u8bba\u6d3b\u8dc3\uff0c\u4f46\u8bc6\u522b\u76f8\u5173\u547d\u540d\u5b9e\u4f53\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u975e\u6b63\u5f0f\u4e14\u6807\u6ce8\u7a00\u7f3a\uff0c\u4ee5\u53ca\u9700\u8981\u5927\u91cf\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u5de5\u4f5c\u5728\u8fd9\u65b9\u9762\u7814\u7a76\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u6765\u5f25\u8865\u6807\u6ce8\u6570\u636e\u7684\u4e0d\u8db3\u548c\u9886\u57df\u77e5\u8bc6\u7684\u7f3a\u4e4f\uff0c\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u4e00\u822c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u3002", "result": "\u5728COVID-19\u63a8\u6587\u6570\u636e\u96c6\u548cPubMed\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5168\u76d1\u7763\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u63d0\u9ad8\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u5b9e\u4f53\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86COVID-19\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u6311\u6218\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u548c\u6b63\u5f0f\u6587\u672c\u683c\u5f0f\u4e2d\u5747\u6709\u826f\u597d\u8868\u73b0\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03558", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03558", "abs": "https://arxiv.org/abs/2510.03558", "authors": ["Shen Chang", "Renran Tian", "Nicole Adams", "Nan Kong"], "title": "Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid", "comment": null, "summary": "Rapid naloxone delivery via drones offers a promising solution for responding\nto opioid overdose emergencies (OOEs), by extending lifesaving interventions to\nmedically untrained bystanders before emergency medical services (EMS) arrive.\nRecognizing the critical role of bystander situational awareness (SA) in\nhuman-autonomy teaming (HAT), we address a key research gap in real-time SA\nassessment by introducing the Drone-Assisted Naloxone Delivery Simulation\nDataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,\nwhere college students without medical training act as bystanders tasked with\nadministering intranasal naloxone to a mock overdose victim. Leveraging this\ndataset, we propose a video-based real-time SA assessment framework that\nutilizes graph embeddings and transformer models to assess bystander SA in real\ntime. Our approach integrates visual perception and comprehension cues--such as\ngeometric, kinematic, and interaction graph features--and achieves\nhigh-performance SA prediction. It also demonstrates strong temporal\nsegmentation accuracy, outperforming the FINCH baseline by 9% in Mean over\nFrames (MoF) and 5% in Intersection over Union (IoU). This work supports the\ndevelopment of adaptive drone systems capable of guiding bystanders\neffectively, ultimately improving emergency response outcomes and saving lives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u7eb3\u6d1b\u916e\u9012\u9001\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u8bc4\u4f30\u65c1\u89c2\u8005\u60c5\u5883\u611f\u77e5\u6765\u6539\u5584\u963f\u7247\u7c7b\u836f\u7269\u8fc7\u91cf\u6025\u6551\u54cd\u5e94\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u9012\u9001\u7eb3\u6d1b\u916e\u7cfb\u7edf\u4e2d\u65c1\u89c2\u8005\u60c5\u5883\u611f\u77e5\u5b9e\u65f6\u8bc4\u4f30\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u9ad8\u6025\u6551\u54cd\u5e94\u6548\u7387\u3002", "method": "\u521b\u5efaDrone-Assisted Naloxone Delivery Simulation Dataset (DANDSD)\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u56fe\u5d4c\u5165\u548ctransformer\u6a21\u578b\u7684\u89c6\u9891\u5b9e\u65f6SA\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684SA\u9884\u6d4b\uff0c\u5728\u65f6\u95f4\u5206\u5272\u51c6\u786e\u7387\u4e0a\u6bd4FINCH\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e869%\u7684MoF\u548c5%\u7684IoU\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u652f\u6301\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6307\u5bfc\u65c1\u89c2\u8005\u7684\u81ea\u9002\u5e94\u65e0\u4eba\u673a\u7cfb\u7edf\uff0c\u6539\u5584\u6025\u6551\u54cd\u5e94\u7ed3\u679c\u5e76\u62ef\u6551\u751f\u547d\u3002"}}
{"id": "2510.04002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04002", "abs": "https://arxiv.org/abs/2510.04002", "authors": ["Bo Yang", "Yunkui Chen", "Lanfei Feng", "Yu Zhang", "Xiao Xu", "Jianyu Zhang", "Nueraili Aierken", "Runhe Huang", "Hongjian Lin", "Yibin Ying", "Shijian Li"], "title": "AgriGPT-VL: Agricultural Vision-Language Understanding Suite", "comment": null, "summary": "Despite rapid advances in multimodal large language models, agricultural\napplications remain constrained by the scarcity of domain-tailored models,\ncurated vision-language corpora, and rigorous evaluation. To address these\nchallenges, we present the AgriGPT-VL Suite, a unified multimodal framework for\nagriculture. Our contributions are threefold. First, we introduce Agri-3M-VL,\nthe largest vision-language corpus for agriculture to our knowledge, curated by\na scalable multi-agent data generator; it comprises 1M image-caption pairs, 2M\nimage-grounded VQA pairs, 50K expert-level VQA instances, and 15K GRPO\nreinforcement learning samples. Second, we develop AgriGPT-VL, an\nagriculture-specialized vision-language model trained via a progressive\ncurriculum of textual grounding, multimodal shallow/deep alignment, and GRPO\nrefinement. This method achieves strong multimodal reasoning while preserving\ntext-only capability. Third, we establish AgriBench-VL-4K, a compact yet\nchallenging evaluation suite with open-ended and image-grounded questions,\npaired with multi-metric evaluation and an LLM-as-a-judge framework.\nExperiments show that AgriGPT-VL outperforms leading general-purpose VLMs on\nAgriBench-VL-4K, achieving higher pairwise win rates in the LLM-as-a-judge\nevaluation. Meanwhile, it remains competitive on the text-only AgriBench-13K\nwith no noticeable degradation of language ability. Ablation studies further\nconfirm consistent gains from our alignment and GRPO refinement stages. We will\nopen source all of the resources to support reproducible research and\ndeployment in low-resource agricultural settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86AgriGPT-VL\u5957\u4ef6\uff0c\u5305\u542b\u6700\u5927\u7684\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93Agri-3M-VL\u3001\u519c\u4e1a\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578bAgriGPT-VL\u548c\u8bc4\u4f30\u57fa\u51c6AgriBench-VL-4K\uff0c\u5728\u519c\u4e1a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528VLMs\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u5e94\u7528\u4e2d\u9886\u57df\u5b9a\u5236\u6a21\u578b\u7a00\u7f3a\u3001\u89c6\u89c9\u8bed\u8a00\u8bed\u6599\u5e93\u7f3a\u4e4f\u548c\u4e25\u683c\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u5668\u6784\u5efaAgri-3M-VL\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u8bad\u7ec3AgriGPT-VL\u6a21\u578b\uff0c\u5305\u62ec\u6587\u672c\u57fa\u7840\u3001\u591a\u6a21\u6001\u6d45\u5c42/\u6df1\u5c42\u5bf9\u9f50\u548cGRPO\u7cbe\u70bc\u3002", "result": "AgriGPT-VL\u5728AgriBench-VL-4K\u4e0a\u4f18\u4e8e\u9886\u5148\u7684\u901a\u7528VLMs\uff0c\u5728LLM-as-a-judge\u8bc4\u4f30\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684\u914d\u5bf9\u80dc\u7387\uff0c\u540c\u65f6\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u8d44\u6e90\u519c\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u7814\u7a76\u548c\u90e8\u7f72\u652f\u6301\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5bf9\u9f50\u548cGRPO\u7cbe\u70bc\u9636\u6bb5\u7684\u4e00\u81f4\u589e\u76ca\u3002"}}
{"id": "2510.03570", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03570", "abs": "https://arxiv.org/abs/2510.03570", "authors": ["Mayimunah Nagayi", "Alice Khan", "Tamryn Frank", "Rina Swart", "Clement Nyirenda"], "title": "Evaluating OCR performance on food packaging labels in South Africa", "comment": "17 pages", "summary": "This study evaluates four open-source Optical Character Recognition (OCR)\nsystems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food\npackaging images. The aim is to assess their ability to extract ingredient\nlists and nutrition facts panels. Accurate OCR for packaging is important for\ncompliance and nutrition monitoring but is challenging due to multilingual\ntext, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231\nproducts (1,628 images) was processed by all four models to assess speed and\ncoverage, and a ground truth subset of 113 images (60 products) was created for\naccuracy evaluation. Metrics include Character Error Rate (CER), Word Error\nRate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground\ntruth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU\n(0.245). EasyOCR provided a good balance between accuracy and multilingual\nsupport. PaddleOCR achieved near complete coverage but was slower because it\nran on CPU only due to GPU incompatibility, and TrOCR produced the weakest\nresults despite GPU acceleration. These results provide a packaging-specific\nbenchmark, establish a baseline, and highlight directions for layout-aware\nmethods and text localization.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u5f00\u6e90OCR\u7cfb\u7edf\uff08Tesseract\u3001EasyOCR\u3001PaddleOCR\u3001TrOCR\uff09\u5728\u98df\u54c1\u5305\u88c5\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0c\u91cd\u70b9\u5173\u6ce8\u6210\u5206\u8868\u548c\u8425\u517b\u4fe1\u606f\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u98df\u54c1\u5305\u88c5\u4e0a\u7684OCR\u5bf9\u4e8e\u5408\u89c4\u6027\u548c\u8425\u517b\u76d1\u6d4b\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u591a\u8bed\u8a00\u6587\u672c\u3001\u5bc6\u96c6\u5e03\u5c40\u3001\u5b57\u4f53\u53d8\u5316\u3001\u53cd\u5149\u548c\u66f2\u9762\u7b49\u6311\u6218\u800c\u56f0\u96be\u3002", "method": "\u4f7f\u7528231\u79cd\u4ea7\u54c1\uff081,628\u5f20\u56fe\u50cf\uff09\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd\u6a21\u578b\u7684\u6267\u884c\u901f\u5ea6\u548c\u8986\u76d6\u7387\uff0c\u5e76\u5bf9113\u5f20\u56fe\u50cf\uff0860\u79cd\u4ea7\u54c1\uff09\u7684\u57fa\u51c6\u5b50\u96c6\u8fdb\u884c\u51c6\u786e\u6027\u8bc4\u4f30\u3002", "result": "\u5728\u57fa\u51c6\u5b50\u96c6\u4e0a\uff0cTesseract\u83b7\u5f97\u6700\u4f4e\u5b57\u7b26\u9519\u8bef\u7387\uff080.912\uff09\u548c\u6700\u9ad8BLEU\u5206\u6570\uff080.245\uff09\uff1bEasyOCR\u5728\u51c6\u786e\u6027\u548c\u591a\u8bed\u8a00\u652f\u6301\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff1bPaddleOCR\u8986\u76d6\u7387\u63a5\u8fd1\u5b8c\u5168\u4f46\u901f\u5ea6\u8f83\u6162\uff1bTrOCR\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5305\u88c5\u7279\u5b9a\u57fa\u51c6\uff0c\u5efa\u7acb\u4e86\u57fa\u7ebf\uff0c\u5e76\u6307\u51fa\u4e86\u5e03\u5c40\u611f\u77e5\u65b9\u6cd5\u548c\u6587\u672c\u5b9a\u4f4d\u7684\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2510.04013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04013", "abs": "https://arxiv.org/abs/2510.04013", "authors": ["Jiarui Liu", "Jivitesh Jain", "Mona Diab", "Nishant Subramani"], "title": "LLM Microscope: What Model Internals Reveal About Answer Correctness and Context Utilization", "comment": null, "summary": "Although large language models (LLMs) have tremendous utility,\ntrustworthiness is still a chief concern: models often generate incorrect\ninformation with high confidence. While contextual information can help guide\ngeneration, identifying when a query would benefit from retrieved context and\nassessing the effectiveness of that context remains challenging. In this work,\nwe operationalize interpretability methods to ascertain whether we can predict\nthe correctness of model outputs from the model's activations alone. We also\nexplore whether model internals contain signals about the efficacy of external\ncontext. We consider correct, incorrect, and irrelevant context and introduce\nmetrics to distinguish amongst them. Experiments on six different models reveal\nthat a simple classifier trained on intermediate layer activations of the first\noutput token can predict output correctness with about 75% accuracy, enabling\nearly auditing. Our model-internals-based metric significantly outperforms\nprompting baselines at distinguishing between correct and incorrect context,\nguarding against inaccuracies introduced by polluted context. These findings\noffer a lens to better understand the underlying decision-making processes of\nLLMs. Our code is publicly available at\nhttps://github.com/jiarui-liu/LLM-Microscope", "AI": {"tldr": "\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u6fc0\u6d3b\u4fe1\u53f7\u9884\u6d4bLLM\u8f93\u51fa\u6b63\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u6709\u6548\u6027\uff0c\u901a\u8fc7\u7b80\u5355\u5206\u7c7b\u5668\u5b9e\u73b075%\u51c6\u786e\u7387\u7684\u65e9\u671f\u5ba1\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u51c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e38\u4ee5\u9ad8\u7f6e\u4fe1\u5ea6\u751f\u6210\u9519\u8bef\u4fe1\u606f\uff0c\u9700\u8981\u89e3\u51b3\u5982\u4f55\u8bc6\u522b\u67e5\u8be2\u662f\u5426\u9700\u8981\u68c0\u7d22\u4e0a\u4e0b\u6587\u4ee5\u53ca\u8bc4\u4f30\u4e0a\u4e0b\u6587\u6709\u6548\u6027\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u57fa\u4e8e\u6a21\u578b\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u8bad\u7ec3\u7b80\u5355\u5206\u7c7b\u5668\uff0c\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u6709\u6548\u6027\uff0c\u8003\u8651\u6b63\u786e\u3001\u9519\u8bef\u548c\u4e0d\u76f8\u5173\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8e\u7b2c\u4e00\u4e2a\u8f93\u51fatoken\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u7684\u5206\u7c7b\u5668\u80fd\u4ee5\u7ea675%\u51c6\u786e\u7387\u9884\u6d4b\u8f93\u51fa\u6b63\u786e\u6027\uff1b\u57fa\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u6307\u6807\u5728\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u4e0a\u4e0b\u6587\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u57fa\u51c6\u3002", "conclusion": "\u6a21\u578b\u5185\u90e8\u5305\u542b\u5173\u4e8e\u8f93\u51fa\u6b63\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u6709\u6548\u6027\u7684\u4fe1\u53f7\uff0c\u8fd9\u4e3a\u66f4\u597d\u7406\u89e3LLM\u5e95\u5c42\u51b3\u7b56\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u9632\u8303\u53d7\u6c61\u67d3\u4e0a\u4e0b\u6587\u5f15\u5165\u7684\u4e0d\u51c6\u786e\u6027\u3002"}}
{"id": "2510.03584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03584", "abs": "https://arxiv.org/abs/2510.03584", "authors": ["Chaoyu Li", "Tianzhi Li", "Fei Tao", "Zhenyu Zhao", "Ziqian Wu", "Maozheng Zhao", "Juntong Song", "Cheng Niu", "Pooyan Fazli"], "title": "FrameOracle: Learning What to See and How Much to See in Videos", "comment": null, "summary": "Vision-language models (VLMs) have advanced video understanding, but their\nperformance is limited by the number of input frames they can process. Existing\nframe sampling strategies, such as uniform or fixed-budget selection, often\nfail to adapt to variations in information density or task complexity,\nresulting in inefficiency and information loss. To address this, we present\nFrameOracle, a lightweight and plug-and-play module that predicts both (1)\nwhich frames are most relevant to a given query and (2) how many frames are\nneeded. FrameOracle is trained using a four-stage curriculum, with the first\nthree stages relying on weak proxy signals such as cross-modal similarity. In\nthe final stage, it leverages stronger supervision from a new dataset we\nintroduce, FrameOracle-41K, the first large-scale VideoQA collection to provide\nkeyframe annotations specifying the minimal set of frames required to answer\neach question. Extensive experiments across five VLMs and six benchmarks\ndemonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4\nframes without any loss in accuracy. When starting from 64-frame candidates, it\nreduces the input to an average of 13.9 frames while improving accuracy by\n1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable\nvideo understanding.", "AI": {"tldr": "FrameOracle\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u901a\u8fc7\u9884\u6d4b\u6700\u76f8\u5173\u5e27\u548c\u6240\u9700\u5e27\u6570\u6765\u4f18\u5316\u89c6\u9891\u7406\u89e3\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8f93\u5165\u5e27\u6570\u3002", "motivation": "\u73b0\u6709\u5e27\u91c7\u6837\u7b56\u7565\u65e0\u6cd5\u9002\u5e94\u4fe1\u606f\u5bc6\u5ea6\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u7684\u53d8\u5316\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u4fe1\u606f\u635f\u5931\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u5e27\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u56db\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\uff0c\u524d\u4e09\u9636\u6bb5\u4f9d\u8d56\u8de8\u6a21\u6001\u76f8\u4f3c\u5ea6\u7b49\u5f31\u4ee3\u7406\u4fe1\u53f7\uff0c\u6700\u540e\u9636\u6bb5\u5229\u7528\u65b0\u6570\u636e\u96c6FrameOracle-41K\u7684\u5173\u952e\u5e27\u6807\u6ce8\u8fdb\u884c\u5f3a\u76d1\u7763\u3002", "result": "\u57285\u4e2aVLMs\u548c6\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFrameOracle\u5c0616\u5e27\u8f93\u5165\u5e73\u5747\u51cf\u5c11\u523010.4\u5e27\u800c\u4e0d\u635f\u5931\u51c6\u786e\u6027\uff1b\u4ece64\u5e27\u5019\u9009\u5f00\u59cb\uff0c\u5e73\u5747\u51cf\u5c11\u523013.9\u5e27\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u60271.4%\u3002", "conclusion": "FrameOracle\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u89c6\u9891\u7406\u89e3\u7684\u6700\u4f18\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5e27\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2510.04016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04016", "abs": "https://arxiv.org/abs/2510.04016", "authors": ["Thanapol Popit", "Natthapath Rungseesiripak", "Monthol Charattrakool", "Saksorn Ruangtanusak"], "title": "Thai Semantic End-of-Turn Detection for Real-Time Voice Agents", "comment": "IEEE ICSEC 2025", "summary": "Fluid voice-to-voice interaction requires reliable and low-latency detection\nof when a user has finished speaking. Traditional audio-silence end-pointers\nadd hundreds of milliseconds of delay and fail under hesitations or\nlanguage-specific phenomena. We present, to our knowledge, the first systematic\nstudy of Thai text-only end-of-turn (EOT) detection for real-time agents. We\ncompare zero-shot and few-shot prompting of compact LLMs to supervised\nfine-tuning of lightweight transformers. Using transcribed subtitles from the\nYODAS corpus and Thai-specific linguistic cues (e.g., sentence-final\nparticles), we formulate EOT as a binary decision over token boundaries. We\nreport a clear accuracy-latency tradeoff and provide a public-ready\nimplementation plan. This work establishes a Thai baseline and demonstrates\nthat small, fine-tuned models can deliver near-instant EOT decisions suitable\nfor on-device agents.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u6cf0\u8bed\u6587\u672c\u7aef\u5230\u7aef\u68c0\u6d4b\uff08EOT\uff09\uff0c\u6bd4\u8f83\u4e86\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u7684\u7d27\u51d1LLM\u4e0e\u8f7b\u91cf\u7ea7transformer\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u5efa\u7acb\u4e86\u6cf0\u8bed\u57fa\u51c6\u5e76\u5c55\u793a\u4e86\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u5373\u65f6\u7684EOT\u51b3\u7b56\u3002", "motivation": "\u4f20\u7edf\u97f3\u9891\u9759\u97f3\u7aef\u70b9\u68c0\u6d4b\u5b58\u5728\u6570\u767e\u6beb\u79d2\u5ef6\u8fdf\uff0c\u4e14\u5728\u72b9\u8c6b\u6216\u8bed\u8a00\u7279\u5b9a\u73b0\u8c61\u4e0b\u5931\u6548\uff0c\u9700\u8981\u53ef\u9760\u4f4e\u5ef6\u8fdf\u7684\u8bed\u97f3\u4ea4\u4e92\u7aef\u70b9\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528YODAS\u8bed\u6599\u5e93\u8f6c\u5f55\u5b57\u5e55\u548c\u6cf0\u8bed\u7279\u5b9a\u8bed\u8a00\u7ebf\u7d22\uff08\u5982\u53e5\u672b\u52a9\u8bcd\uff09\uff0c\u5c06EOT\u5236\u5b9a\u4e3a\u57fa\u4e8e\u8bcd\u5143\u8fb9\u754c\u7684\u4e8c\u5143\u51b3\u7b56\uff0c\u6bd4\u8f83\u4e86\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u7d27\u51d1LLM\u4e0e\u76d1\u7763\u5fae\u8c03\u8f7b\u91cf\u7ea7transformer\u7684\u65b9\u6cd5\u3002", "result": "\u62a5\u544a\u4e86\u6e05\u6670\u7684\u51c6\u786e\u7387-\u5ef6\u8fdf\u6743\u8861\uff0c\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u53ef\u63d0\u4f9b\u8fd1\u4e4e\u5373\u65f6\u7684EOT\u51b3\u7b56\uff0c\u9002\u5408\u8bbe\u5907\u7aef\u4ee3\u7406\u4f7f\u7528\u3002", "conclusion": "\u5efa\u7acb\u4e86\u6cf0\u8bedEOT\u68c0\u6d4b\u57fa\u51c6\uff0c\u8bc1\u660e\u5c0f\u578b\u5fae\u8c03\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u9002\u5408\u5b9e\u65f6\u8bbe\u5907\u7aef\u4ee3\u7406\u7684\u5feb\u901fEOT\u68c0\u6d4b\u3002"}}
{"id": "2510.03591", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03591", "abs": "https://arxiv.org/abs/2510.03591", "authors": ["Faliu Yi", "Sherif Abdelfattah", "Wei Huang", "Adrian Brown"], "title": "A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games", "comment": "Accepted at the 21st AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment (AIIDE 2025)", "summary": "Manual identification of visual bugs in video games is a resource-intensive\nand costly process, often demanding specialized domain knowledge. While\nsupervised visual bug detection models offer a promising solution, their\nreliance on extensive labeled datasets presents a significant challenge due to\nthe infrequent occurrence of such bugs. To overcome this limitation, we propose\na hybrid Co-FineTuning (CFT) method that effectively integrates both labeled\nand unlabeled data. Our approach leverages labeled samples from the target game\nand diverse co-domain games, additionally incorporating unlabeled data to\nenhance feature representation learning. This strategy maximizes the utility of\nall available data, substantially reducing the dependency on labeled examples\nfrom the specific target game. The developed framework demonstrates enhanced\nscalability and adaptability, facilitating efficient visual bug detection\nacross various game titles. Our experimental results show the robustness of the\nproposed method for game visual bug detection, exhibiting superior performance\ncompared to conventional baselines across multiple gaming environments.\nFurthermore, CFT maintains competitive performance even when trained with only\n50% of the labeled data from the target game.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u534f\u540c\u5fae\u8c03(CFT)\u65b9\u6cd5\uff0c\u7528\u4e8e\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\uff0c\u6709\u6548\u7ed3\u5408\u6807\u8bb0\u548c\u672a\u6807\u8bb0\u6570\u636e\uff0c\u51cf\u5c11\u5bf9\u76ee\u6807\u6e38\u620f\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u6e38\u620f\u89c6\u89c9bug\u7684\u4eba\u5de5\u8bc6\u522b\u6210\u672c\u9ad8\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u76d1\u7763\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u4f46\u8fd9\u7c7bbug\u51fa\u73b0\u9891\u7387\u4f4e\uff0c\u6807\u8bb0\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u534f\u540c\u5fae\u8c03\u65b9\u6cd5\uff0c\u6574\u5408\u76ee\u6807\u6e38\u620f\u548c\u540c\u9886\u57df\u6e38\u620f\u7684\u6807\u8bb0\u6837\u672c\uff0c\u5e76\u52a0\u5165\u672a\u6807\u8bb0\u6570\u636e\u6765\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6e38\u620f\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5373\u4f7f\u4ec5\u4f7f\u752850%\u7684\u76ee\u6807\u6e38\u620f\u6807\u8bb0\u6570\u636e\u4e5f\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "CFT\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6e38\u620f\u89c6\u89c9bug\u68c0\u6d4b\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\uff0c\u6709\u6548\u5229\u7528\u6240\u6709\u53ef\u7528\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u7279\u5b9a\u76ee\u6807\u6e38\u620f\u6807\u8bb0\u793a\u4f8b\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.04031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04031", "abs": "https://arxiv.org/abs/2510.04031", "authors": ["Nelvin Tan", "James Asikin Cheung", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Does Using Counterfactual Help LLMs Explain Textual Importance in Classification?", "comment": "8 pages, 2 figures", "summary": "Large language models (LLMs) are becoming useful in many domains due to their\nimpressive abilities that arise from large training datasets and large model\nsizes. More recently, they have been shown to be very effective in textual\nclassification tasks, motivating the need to explain the LLMs' decisions.\nMotivated by practical constrains where LLMs are black-boxed and LLM calls are\nexpensive, we study how incorporating counterfactuals into LLM reasoning can\naffect the LLM's ability to identify the top words that have contributed to its\nclassification decision. To this end, we introduce a framework called the\ndecision changing rate that helps us quantify the importance of the top words\nin classification. Our experimental results show that using counterfactuals can\nbe helpful.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6846\u67b6\u6765\u8bc6\u522bLLM\u5206\u7c7b\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u8bcd\u8bed\uff0c\u901a\u8fc7\u51b3\u7b56\u6539\u53d8\u7387\u91cf\u5316\u8bcd\u8bed\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u662f\u9ed1\u76d2\u6a21\u578b\u4e14\u8c03\u7528\u6210\u672c\u9ad8\uff0c\u9700\u8981\u89e3\u91ca\u5176\u5206\u7c7b\u51b3\u7b56\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002", "method": "\u5f15\u5165\u51b3\u7b56\u6539\u53d8\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u8bc6\u522b\u5f71\u54cd\u5206\u7c7b\u51b3\u7b56\u7684\u5173\u952e\u8bcd\u8bed\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4f7f\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6709\u52a9\u4e8e\u8bc6\u522b\u91cd\u8981\u8bcd\u8bed\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u6709\u6548\u5e2e\u52a9\u7406\u89e3LLM\u7684\u5206\u7c7b\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2510.03598", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03598", "abs": "https://arxiv.org/abs/2510.03598", "authors": ["Alexander V. Mantzaris"], "title": "Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation", "comment": null, "summary": "This paper asks whether the Hierarchical Reasoning Model (HRM) with the two\nTransformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep\nsupervision, Rotary Position Embeddings, and RMSNorm can serve as a practical\nimage classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a\ndeliberately raw regime: no data augmentation, identical optimizer family with\none-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes\nstably and performs well on MNIST ($\\approx 98\\%$ test accuracy), but on small\nnatural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches\n65.0\\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains\n77.2\\% while training $\\sim 30\\times$ faster per epoch; on CIFAR-100, HRM\nachieves only 29.7\\% test accuracy despite 91.5\\% train accuracy, while the\nsame CNN reaches 45.3\\% test with 50.5\\% train accuracy. Loss traces and error\nanalyses indicate healthy optimization but insufficient image-specific\ninductive bias for HRM in this regime. It is concluded that, for\nsmall-resolution image classification without augmentation, HRM is not\ncompetitive with even simple convolutional architectures as the HRM currently\nexist but this does not exclude possibilities that modifications to the model\nmay allow it to improve greatly.", "AI": {"tldr": "HRM\u6a21\u578b\u5728MNIST\u4e0a\u8868\u73b0\u826f\u597d\uff08\u224898%\u6d4b\u8bd5\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728CIFAR-10\u548cCIFAR-100\u7b49\u5c0f\u5206\u8fa8\u7387\u81ea\u7136\u56fe\u50cf\u4e0a\u8fc7\u62df\u5408\u4e25\u91cd\uff0c\u6027\u80fd\u4e0d\u5982\u7b80\u5355\u7684\u5377\u79ef\u7f51\u7edc\u3002", "motivation": "\u63a2\u7a76HRM\u6a21\u578b\uff08\u5305\u542b\u4e24\u4e2aTransformer\u6a21\u5757\u3001DEQ\u98ce\u683c\u8bad\u7ec3\u3001\u6df1\u5ea6\u76d1\u7763\u7b49\uff09\u662f\u5426\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u7528\u7684\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "method": "\u5728MNIST\u3001CIFAR-10\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30HRM\uff0c\u91c7\u7528\u539f\u59cb\u8bad\u7ec3\u7b56\u7565\uff1a\u65e0\u6570\u636e\u589e\u5f3a\u3001\u76f8\u540c\u4f18\u5316\u5668\u5bb6\u65cf\u3001\u4e00\u5468\u671f\u9884\u70ed\u540e\u4f59\u5f26\u8870\u51cf\u3001\u6807\u7b7e\u5e73\u6ed1\u3002", "result": "HRM\u5728MNIST\u4e0a\u8fbe\u523098%\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4f46\u5728CIFAR-10\u4e0a\u4ec565.0%\uff08CNN\u4e3a77.2%\uff09\uff0cCIFAR-100\u4e0a\u4ec529.7%\uff08CNN\u4e3a45.3%\uff09\uff0c\u4e14\u8bad\u7ec3\u901f\u5ea6\u616230\u500d\u3002", "conclusion": "\u5bf9\u4e8e\u65e0\u589e\u5f3a\u7684\u5c0f\u5206\u8fa8\u7387\u56fe\u50cf\u5206\u7c7b\uff0c\u73b0\u6709HRM\u4e0d\u5982\u7b80\u5355\u5377\u79ef\u67b6\u6784\uff0c\u4f46\u6a21\u578b\u4fee\u6539\u53ef\u80fd\u5e26\u6765\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.04032", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04032", "abs": "https://arxiv.org/abs/2510.04032", "authors": ["Zirui Wang", "Jiajun Wu", "Braden Teitge", "Jessalyn Holodinsky", "Steve Drew"], "title": "Small Language Models for Emergency Departments Decision Support: A Benchmark Study", "comment": "Accepted to 2025 IEEE International Conference on Autonomous and\n  Trusted Computing (ATC 2025)", "summary": "Large language models (LLMs) have become increasingly popular in medical\ndomains to assist physicians with a variety of clinical and operational tasks.\nGiven the fast-paced and high-stakes environment of emergency departments\n(EDs), small language models (SLMs), characterized by a reduction in parameter\ncount compared to LLMs, offer significant potential due to their inherent\nreasoning capability and efficient performance. This enables SLMs to support\nphysicians by providing timely and accurate information synthesis, thereby\nimproving clinical decision-making and workflow efficiency. In this paper, we\npresent a comprehensive benchmark designed to identify SLMs suited for ED\ndecision support, taking into account both specialized medical expertise and\nbroad general problem-solving capabilities. In our evaluations, we focus on\nSLMs that have been trained on a mixture of general-domain and medical corpora.\nA key motivation for emphasizing SLMs is the practical hardware limitations,\noperational cost constraints, and privacy concerns in the typical real-world\ndeployments. Our benchmark datasets include MedMCQA, MedQA-4Options, and\nPubMedQA, with the medical abstracts dataset emulating tasks aligned with real\nED physicians' daily tasks. Experimental results reveal that general-domain\nSLMs surprisingly outperform their medically fine-tuned counterparts across\nthese diverse benchmarks for ED. This indicates that for ED, specialized\nmedical fine-tuning of the model may not be required.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u57fa\u51c6\u6765\u8bc4\u4f30\u9002\u5408\u6025\u8bca\u79d1\u51b3\u7b56\u652f\u6301\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u901a\u7528\u9886\u57dfSLM\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u533b\u5b66\u5fae\u8c03\u6a21\u578b\uff0c\u8868\u660e\u6025\u8bca\u79d1\u53ef\u80fd\u4e0d\u9700\u8981\u4e13\u95e8\u7684\u533b\u5b66\u5fae\u8c03\u3002", "motivation": "\u9488\u5bf9\u6025\u8bca\u79d1\u5feb\u8282\u594f\u3001\u9ad8\u98ce\u9669\u73af\u5883\uff0c\u9700\u8981\u9ad8\u6548\u3001\u53ca\u65f6\u7684\u8bed\u8a00\u6a21\u578b\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002\u8003\u8651\u5230\u786c\u4ef6\u9650\u5236\u3001\u8fd0\u8425\u6210\u672c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u63a8\u7406\u80fd\u529b\u548c\u9ad8\u6548\u6027\u80fd\u800c\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u7efc\u5408\u57fa\u51c6\u8bc4\u4f30SLM\uff0c\u4f7f\u7528MedMCQA\u3001MedQA-4Options\u548cPubMedQA\u7b49\u533b\u7597\u6570\u636e\u96c6\uff0c\u6a21\u62df\u6025\u8bca\u79d1\u533b\u751f\u7684\u65e5\u5e38\u4efb\u52a1\u3002\u8bc4\u4f30\u5728\u901a\u7528\u9886\u57df\u548c\u533b\u5b66\u8bed\u6599\u4e0a\u8bad\u7ec3\u7684SLM\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u7528\u9886\u57dfSLM\u5728\u591a\u6837\u5316\u6025\u8bca\u79d1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u610f\u5916\u5730\u4f18\u4e8e\u533b\u5b66\u5fae\u8c03\u6a21\u578b\uff0c\u8868\u660e\u4e13\u95e8\u533b\u5b66\u5fae\u8c03\u53ef\u80fd\u4e0d\u662f\u5fc5\u9700\u7684\u3002", "conclusion": "\u5bf9\u4e8e\u6025\u8bca\u79d1\u51b3\u7b56\u652f\u6301\uff0c\u901a\u7528\u9886\u57dfSLM\u5df2\u8db3\u591f\u80dc\u4efb\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u533b\u5b66\u5fae\u8c03\uff0c\u8fd9\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03606", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.03606", "abs": "https://arxiv.org/abs/2510.03606", "authors": ["Mattia Scardecchia"], "title": "Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops", "comment": null, "summary": "Recent advances in self-supervised learning (SSL) have made it possible to\nlearn general-purpose visual features that capture both the high-level\nsemantics and the fine-grained spatial structure of images. Most notably, the\nrecent DINOv2 has established a new state of the art by surpassing weakly\nsupervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we\nexamine the core ideas behind its approach, multi-crop view augmentation and\nself-distillation with a mean teacher, and trace their development in previous\nwork. We then compare the performance of DINO and DINOv2 with other SSL and WSL\nmethods across various downstream tasks, and highlight some remarkable emergent\nproperties of their learned features with transformer backbones. We conclude by\nbriefly discussing DINOv2's limitations, its impact, and future research\ndirections.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86DINOv2\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u591a\u88c1\u526a\u89c6\u56fe\u589e\u5f3a\u548c\u81ea\u84b8\u998f\u7b49\u6838\u5fc3\u6280\u672f\uff0c\u6bd4\u8f83\u4e86\u4e0e\u5176\u4ed6SSL\u548cWSL\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60(SSL)\u7684\u6700\u65b0\u8fdb\u5c55\u4f7f\u5f97\u5b66\u4e60\u901a\u7528\u89c6\u89c9\u7279\u5f81\u6210\u4e3a\u53ef\u80fd\uff0cDINOv2\u5728\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5f31\u76d1\u7763\u65b9\u6cd5(WSL)\uff0c\u9700\u8981\u7cfb\u7edf\u5206\u6790\u5176\u6838\u5fc3\u601d\u60f3\u548c\u6027\u80fd\u8868\u73b0\u3002", "method": "\u91c7\u7528\u591a\u88c1\u526a\u89c6\u56fe\u589e\u5f3a\u548c\u57fa\u4e8e\u5747\u503c\u6559\u5e08\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u4f7f\u7528transformer\u9aa8\u5e72\u7f51\u7edc\u5b66\u4e60\u89c6\u89c9\u7279\u5f81\u3002", "result": "DINOv2\u5728\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86OpenCLIP\u7b49\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5176\u5b66\u4e60\u7684\u7279\u5f81\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5c55\u73b0\u51fa\u4e00\u4e9b\u663e\u8457\u7684\u65b0\u5174\u7279\u6027\u3002", "conclusion": "DINOv2\u5efa\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6807\u51c6\uff0c\u4f46\u5176\u4ecd\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u6539\u8fdb\u548c\u53d1\u5c55\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2510.04045", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04045", "abs": "https://arxiv.org/abs/2510.04045", "authors": ["Yunfan Zhang", "Kathleen McKeown", "Smaranda Muresan"], "title": "Exploring Chain-of-Thought Reasoning for Steerable Pluralistic Alignment", "comment": "ACL EMNLP 2025", "summary": "Large Language Models (LLMs) are typically trained to reflect a relatively\nuniform set of values, which limits their applicability to tasks that require\nunderstanding of nuanced human perspectives. Recent research has underscored\nthe importance of enabling LLMs to support steerable pluralism -- the capacity\nto adopt a specific perspective and align generated outputs with it. In this\nwork, we investigate whether Chain-of-Thought (CoT) reasoning techniques can be\napplied to building steerable pluralistic models. We explore several methods,\nincluding CoT prompting, fine-tuning on human-authored CoT, fine-tuning on\nsynthetic explanations, and Reinforcement Learning with Verifiable Rewards\n(RLVR). We evaluate these approaches using the Value Kaleidoscope and OpinionQA\ndatasets. Among the methods studied, RLVR consistently outperforms others and\ndemonstrates strong training sample efficiency. We further analyze the\ngenerated CoT traces with respect to faithfulness and safety.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u5982\u4f55\u4f7f\u7528\u601d\u7ef4\u94fe(CoT)\u63a8\u7406\u6280\u672f\u6784\u5efa\u53ef\u5f15\u5bfc\u7684\u591a\u5143\u6a21\u578b\uff0c\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1(RLVR)\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u8f83\u597d\u7684\u8bad\u7ec3\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u53cd\u6620\u76f8\u5bf9\u7edf\u4e00\u7684\u4ef7\u503c\u89c2\uff0c\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u4eba\u7c7b\u89c6\u89d2\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u8ba9LLMs\u652f\u6301\u53ef\u5f15\u5bfc\u7684\u591a\u5143\u6027\uff0c\u5373\u80fd\u591f\u91c7\u7528\u7279\u5b9a\u89c6\u89d2\u5e76\u751f\u6210\u4e0e\u4e4b\u5bf9\u9f50\u7684\u8f93\u51fa\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u65b9\u6cd5\uff1aCoT\u63d0\u793a\u3001\u57fa\u4e8e\u4eba\u7c7b\u64b0\u5199CoT\u7684\u5fae\u8c03\u3001\u57fa\u4e8e\u5408\u6210\u89e3\u91ca\u7684\u5fae\u8c03\uff0c\u4ee5\u53ca\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1(RLVR)\u3002\u4f7f\u7528Value Kaleidoscope\u548cOpinionQA\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u6240\u6709\u7814\u7a76\u65b9\u6cd5\u4e2d\uff0cRLVR\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u4e14\u5177\u6709\u5f3a\u8bad\u7ec3\u6837\u672c\u6548\u7387\u3002\u5bf9\u751f\u6210\u7684CoT\u8f68\u8ff9\u8fdb\u884c\u4e86\u5fe0\u5b9e\u6027\u548c\u5b89\u5168\u6027\u5206\u6790\u3002", "conclusion": "\u601d\u7ef4\u94fe\u63a8\u7406\u6280\u672f\u53ef\u4ee5\u6709\u6548\u6784\u5efa\u53ef\u5f15\u5bfc\u7684\u591a\u5143\u6a21\u578b\uff0c\u5176\u4e2dRLVR\u65b9\u6cd5\u662f\u6700\u6709\u6548\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u4e3aLLMs\u5728\u591a\u5143\u89c6\u89d2\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.03608", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03608", "abs": "https://arxiv.org/abs/2510.03608", "authors": ["Ruitao Wu", "Yifan Zhao", "Guangyao Chen", "Jia Li"], "title": "Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL", "comment": "Accepted by NeurIPS 2025", "summary": "Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially\nlearn new classes from minimal examples without forgetting prior knowledge, a\ntask complicated by the stability-plasticity dilemma and data scarcity. Current\nFSCIL methods often struggle with generalization due to their reliance on\nlimited datasets. While diffusion models offer a path for data augmentation,\ntheir direct application can lead to semantic misalignment or ineffective\nguidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel\nframework that establishes a mutual boosting loop between diffusion model and\nFSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a\ndynamic, multi-faceted reward function derived from the classifier's state\ndirects the diffusion model. This reward system operates at two levels: the\nfeature level ensures semantic coherence and diversity using prototype-anchored\nmaximum mean discrepancy and dimension-wise variance matching, while the logits\nlevel promotes exploratory image generation and enhances inter-class\ndiscriminability through confidence recalibration and cross-session\nconfusion-aware mechanisms. This co-evolutionary process, where generated\nimages refine the classifier and an improved classifier state yields better\nreward signals, demonstrably achieves state-of-the-art performance on FSCIL\nbenchmarks, significantly enhancing both knowledge retention and new class\nlearning.", "AI": {"tldr": "\u63d0\u51faDCS\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u7684\u534f\u540c\u8fdb\u5316\u89e3\u51b3\u5c0f\u6837\u672c\u7c7b\u589e\u91cf\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u52a8\u6001\u591a\u5c42\u9762\u5956\u52b1\u51fd\u6570\u6307\u5bfc\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dFSCIL\u65b9\u6cd5\u56e0\u4f9d\u8d56\u6709\u9650\u6570\u636e\u96c6\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u76f4\u63a5\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u53ef\u80fd\u5bfc\u81f4\u8bed\u4e49\u4e0d\u5bf9\u9f50\u6216\u5f15\u5bfc\u65e0\u6548\u3002", "method": "\u91c7\u7528\u6269\u6563-\u5206\u7c7b\u5668\u534f\u540c\u6846\u67b6\uff0c\u57fa\u4e8e\u5206\u7c7b\u5668\u72b6\u6001\u8bbe\u8ba1\u52a8\u6001\u591a\u5c42\u9762\u5956\u52b1\u51fd\u6570\uff0c\u5728\u7279\u5f81\u5c42\u9762\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\uff0c\u5728logits\u5c42\u9762\u4fc3\u8fdb\u63a2\u7d22\u6027\u56fe\u50cf\u751f\u6210\u548c\u7c7b\u95f4\u533a\u5206\u5ea6\u3002", "result": "\u5728FSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u77e5\u8bc6\u4fdd\u7559\u548c\u65b0\u7c7b\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "DCS\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u7684\u76f8\u4e92\u4fc3\u8fdb\u5faa\u73af\uff0c\u6709\u6548\u89e3\u51b3\u4e86FSCIL\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2510.04071", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04071", "abs": "https://arxiv.org/abs/2510.04071", "authors": ["Zitian Gao", "Haoming Luo", "Lynx Chen", "Jason Klein Liu", "Ran Tao", "Joey Zhou", "Bryan Dai"], "title": "What Makes Diffusion Language Models Super Data Learners?", "comment": "Technical report, work in progress", "summary": "Recent studies have shown that diffusion language models achieve remarkable\ndata efficiency under limited-data constraints, yet the underlying mechanisms\nremain unclear. In this work, we perform extensive ablation experiments to\ndisentangle the sources of this efficiency. Our results show that random\nmasking of input tokens plays the dominant role. We further show that similar\ngains can be obtained through in MLP dropout and weight decay, indicating that\nstochastic regularization broadly enhances data efficiency in multi-epoch\ntraining. Our code is available at\nhttps://github.com/zitian-gao/data-efficiency.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5177\u6709\u663e\u8457\u7684\u6570\u636e\u6548\u7387\uff0c\u968f\u673a\u63a9\u7801\u662f\u4e3b\u8981\u56e0\u7d20\uff0c\u5176\u4ed6\u968f\u673a\u6b63\u5219\u5316\u65b9\u6cd5\u4e5f\u80fd\u83b7\u5f97\u7c7b\u4f3c\u6548\u679c", "motivation": "\u63a2\u7d22\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u6709\u9650\u6570\u636e\u7ea6\u675f\u4e0b\u5b9e\u73b0\u663e\u8457\u6570\u636e\u6548\u7387\u7684\u6f5c\u5728\u673a\u5236", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u6765\u5206\u79bb\u6570\u636e\u6548\u7387\u7684\u6765\u6e90\uff0c\u6d4b\u8bd5\u968f\u673a\u63a9\u7801\u3001MLP dropout\u548c\u6743\u91cd\u8870\u51cf\u7b49\u65b9\u6cd5", "result": "\u968f\u673a\u63a9\u7801\u8f93\u5165\u6807\u8bb0\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u5176\u4ed6\u968f\u673a\u6b63\u5219\u5316\u65b9\u6cd5\u4e5f\u80fd\u83b7\u5f97\u7c7b\u4f3c\u7684\u6570\u636e\u6548\u7387\u589e\u76ca", "conclusion": "\u968f\u673a\u6b63\u5219\u5316\u5728\u591a\u8f6e\u8bad\u7ec3\u4e2d\u5e7f\u6cdb\u589e\u5f3a\u4e86\u6570\u636e\u6548\u7387"}}
{"id": "2510.03666", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03666", "abs": "https://arxiv.org/abs/2510.03666", "authors": ["Jiang Wu", "Sichao Wu", "Yinsong Ma", "Guangyuan Yu", "Haoyuan Xu", "Lifang Zheng", "Jingliang Duan"], "title": "MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations", "comment": null, "summary": "Industrial accidents, particularly in high-risk domains such as surface and\nunderground mining, are frequently caused by unsafe worker behaviors.\nTraditional manual inspection remains labor-intensive, error-prone, and\ninsufficient for large-scale, dynamic environments, highlighting the urgent\nneed for intelligent and automated safety monitoring. In this paper, we present\nMonitorVLM, a novel vision--language framework designed to detect safety\nviolations directly from surveillance video streams. MonitorVLM introduces\nthree key innovations: (1) a domain-specific violation dataset comprising 9,000\nvision--question--answer (VQA) samples across 40 high-frequency mining\nregulations, enriched with augmentation and auxiliary detection cues; (2) a\nclause filter (CF) module that dynamically selects the Top-$K$ most relevant\nclauses, reducing inference latency by 13.56\\% while maintaining accuracy; and\n(3) a behavior magnifier (BM) module that enhances worker regions to improve\nfine-grained action recognition, yielding additional gains of 3.45% in\nprecision and 8.62% in recall. Experimental results demonstrate that MonitorVLM\nsignificantly outperforms baseline vision--language models, achieving\nimprovements of 22.01% in precision, 34.22\\% in recall, and 28.37% in F1 score\nover the 72B unfine-tuned baseline. A lightweight web-based interface further\nintegrates MonitorVLM into practical workflows, enabling automatic violation\nreporting with video timestamping. This study highlights the potential of\nmultimodal large models to enhance occupational safety monitoring in mining and\nbeyond.", "AI": {"tldr": "MonitorVLM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u8fdd\u89c4\u68c0\u6d4b\u6846\u67b6\uff0c\u4e13\u4e3a\u77ff\u4e1a\u5b89\u5168\u76d1\u63a7\u8bbe\u8ba1\uff0c\u901a\u8fc7\u52a8\u6001\u6761\u6b3e\u7b5b\u9009\u548c\u884c\u4e3a\u653e\u5927\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdd\u89c4\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u5b89\u5168\u68c0\u67e5\u5728\u77ff\u4e1a\u7b49\u9ad8\u5371\u884c\u4e1a\u4e2d\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u52a8\u6001\u73af\u5883\uff0c\u8feb\u5207\u9700\u8981\u667a\u80fd\u5316\u7684\u81ea\u52a8\u5b89\u5168\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86MonitorVLM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u5305\u542b9000\u4e2aVQA\u6837\u672c\u7684\u9886\u57df\u7279\u5b9a\u8fdd\u89c4\u6570\u636e\u96c6\uff1b2) \u52a8\u6001\u7b5b\u9009Top-K\u76f8\u5173\u6761\u6b3e\u7684\u6761\u6b3e\u8fc7\u6ee4\u5668\u6a21\u5757\uff0c\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff1b3) \u589e\u5f3a\u5de5\u4eba\u533a\u57df\u8bc6\u522b\u7684\u884c\u4e3a\u653e\u5927\u6a21\u5757\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aMonitorVLM\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u63d0\u534722.01%\u300134.22%\u548c28.37%\uff0c\u540c\u65f6\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e13.56%\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u63d0\u5347\u77ff\u4e1a\u7b49\u804c\u4e1a\u5b89\u5168\u76d1\u63a7\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7Web\u754c\u9762\u5b9e\u73b0\u4e86\u8fdd\u89c4\u81ea\u52a8\u62a5\u544a\u548c\u89c6\u9891\u65f6\u95f4\u6233\u529f\u80fd\u3002"}}
{"id": "2510.04080", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04080", "abs": "https://arxiv.org/abs/2510.04080", "authors": ["Zixin Song", "Bowen Zhang", "Qian-Wen Zhang", "Di Yin", "Xing Sun", "Chunping Li"], "title": "PoLi-RL: A Point-to-List Reinforcement Learning Framework for Conditional Semantic Textual Similarity", "comment": null, "summary": "Conditional Semantic Textual Similarity (C-STS) measures the semantic\nproximity between text segments under a specific condition, thereby overcoming\nthe ambiguity inherent in traditional STS. However, existing methods are\nlargely confined to discriminative models, failing to fully integrate recent\nbreakthroughs in the NLP community concerning Large Language Models (LLMs) and\nReinforcement Learning (RL). RL is a particularly well-suited paradigm for this\ntask, as it can directly optimize the non-differentiable Spearman ranking\nmetric and guide the reasoning process required by C-STS. However, we find that\nnaively applying listwise RL fails to produce meaningful improvements, as the\nmodel is overwhelmed by complex, coarse-grained reward signals. To address this\nchallenge, we introduce PoLi-RL, a novel Point-to-List Reinforcement Learning\nframework. PoLi-RL employs a two-stage curriculum: it first trains the model\nwith simple pointwise rewards to establish fundamental scoring capabilities,\nthen transitions to a hybrid reward that combines pointwise, pairwise, and\nlistwise objectives to refine the model's ability to discern subtle semantic\ndistinctions. Crucially, we propose an innovative Parallel Slice Ranking Reward\n(PSRR) mechanism that computes ranking rewards in parallel slices, where each\nslice comprises same-indexed completions from different samples. This provides\na precise, differentiated learning signal for each individual completion,\nenabling granular credit assignment and effective optimization. On the official\nC-STS benchmark, PoLi-RL achieves a Spearman correlation coefficient of 48.18,\nestablishing a new SOTA for the cross-encoder architecture. As the first work\nto successfully apply RL to C-STS, our study introduces a powerful and precise\nparadigm for training LLMs on complex, ranking-based conditional judgment\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86PoLi-RL\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u548c\u5e76\u884c\u5207\u7247\u6392\u5e8f\u5956\u52b1\u673a\u5236\uff0c\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6761\u4ef6\u8bed\u4e49\u6587\u672c\u76f8\u4f3c\u6027\u4efb\u52a1\uff0c\u5728C-STS\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709C-STS\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5224\u522b\u5f0f\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8fdb\u5c55\u3002\u5f3a\u5316\u5b66\u4e60\u7279\u522b\u9002\u5408\u8be5\u4efb\u52a1\uff0c\u53ef\u4ee5\u76f4\u63a5\u4f18\u5316\u4e0d\u53ef\u5fae\u7684Spearman\u6392\u5e8f\u6307\u6807\u5e76\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "PoLi-RL\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff1a\u5148\u7528\u7b80\u5355\u70b9\u5f0f\u5956\u52b1\u8bad\u7ec3\u57fa\u7840\u8bc4\u5206\u80fd\u529b\uff0c\u7136\u540e\u8fc7\u6e21\u5230\u7ed3\u5408\u70b9\u5f0f\u3001\u5bf9\u5f0f\u548c\u5217\u8868\u5f0f\u76ee\u6807\u7684\u6df7\u5408\u5956\u52b1\u3002\u5173\u952e\u521b\u65b0\u662f\u5e76\u884c\u5207\u7247\u6392\u5e8f\u5956\u52b1\u673a\u5236\uff0c\u5728\u5e76\u884c\u5207\u7247\u4e2d\u8ba1\u7b97\u6392\u5e8f\u5956\u52b1\u3002", "result": "\u5728\u5b98\u65b9C-STS\u57fa\u51c6\u4e0a\uff0cPoLi-RL\u5b9e\u73b0\u4e8648.18\u7684Spearman\u76f8\u5173\u7cfb\u6570\uff0c\u4e3a\u4ea4\u53c9\u7f16\u7801\u5668\u67b6\u6784\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6210\u529f\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8eC-STS\u7684\u5de5\u4f5c\uff0c\u4e3a\u5728\u590d\u6742\u3001\u57fa\u4e8e\u6392\u5e8f\u7684\u6761\u4ef6\u5224\u65ad\u4efb\u52a1\u4e0a\u8bad\u7ec3LLM\u5f15\u5165\u4e86\u5f3a\u5927\u800c\u7cbe\u786e\u7684\u8303\u5f0f\u3002"}}
{"id": "2510.03675", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03675", "abs": "https://arxiv.org/abs/2510.03675", "authors": ["Siva Sai", "Saksham Gupta", "Vinay Chamola", "Rajkumar Buyya"], "title": "A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems", "comment": null, "summary": "The integration of Diffusion Models into Intelligent Transportation Systems\n(ITS) is a substantial improvement in the detection of accidents. We present a\nnovel hybrid model integrating guidance classification with diffusion\ntechniques. By leveraging fine-tuned ExceptionNet architecture outputs as input\nfor our proposed diffusion model and processing image tensors as our\nconditioning, our approach creates a robust classification framework. Our model\nconsists of multiple conditional modules, which aim to modulate the linear\nprojection of inputs using time embeddings and image covariate embeddings,\nallowing the network to adapt its behavior dynamically throughout the diffusion\nprocess. To address the computationally intensive nature of diffusion models,\nour implementation is cloud-based, enabling scalable and efficient processing.\nOur strategy overcomes the shortcomings of conventional classification\napproaches by leveraging diffusion models inherent capacity to effectively\nunderstand complicated data distributions. We investigate important diffusion\ncharacteristics, such as timestep schedulers, timestep encoding techniques,\ntimestep count, and architectural design changes, using a thorough ablation\nstudy, and have conducted a comprehensive evaluation of the proposed model\nagainst the baseline models on a publicly available dataset. The proposed\ndiffusion model performs best in image-based accident detection with an\naccuracy of 97.32%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6269\u6563\u6a21\u578b\u4e0e\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u5f02\u5e38\u68c0\u6d4b\u7f51\u7edc\u548c\u6269\u6563\u6280\u672f\u5b9e\u73b097.32%\u51c6\u786e\u7387\u7684\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b", "motivation": "\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u6269\u6563\u6a21\u578b\u80fd\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u6570\u636e\u5206\u5e03\uff0c\u63d0\u9ad8\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "method": "\u4f7f\u7528\u5fae\u8c03\u7684ExceptionNet\u67b6\u6784\u8f93\u51fa\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u8f93\u5165\uff0c\u56fe\u50cf\u5f20\u91cf\u4f5c\u4e3a\u6761\u4ef6\uff0c\u6784\u5efa\u591a\u6761\u4ef6\u6a21\u5757\u6765\u52a8\u6001\u8c03\u6574\u8f93\u5165\u7ebf\u6027\u6295\u5f71\uff0c\u91c7\u7528\u57fa\u4e8e\u4e91\u7684\u5b9e\u73b0\u89e3\u51b3\u8ba1\u7b97\u5bc6\u96c6\u95ee\u9898", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.32%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u65f6\u95f4\u6b65\u8c03\u5ea6\u5668\u3001\u7f16\u7801\u6280\u672f\u7b49\u5173\u952e\u6269\u6563\u7279\u6027\u7684\u91cd\u8981\u6027", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u5728\u57fa\u4e8e\u56fe\u50cf\u7684\u4ea4\u901a\u4e8b\u6545\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2510.04081", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.04081", "abs": "https://arxiv.org/abs/2510.04081", "authors": ["Honglin Lin", "Qizhi Pei", "Xin Gao", "Zhuoshi Pan", "Yu Li", "Juntao Li", "Conghui He", "Lijun Wu"], "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "comment": "Accepted by NeurIPS2025", "summary": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve\ncomplex tasks, yet achieving reliable and scalable reasoning remains\nchallenging. While Chain-of-Thought (CoT) prompting has become a mainstream\napproach, existing methods often suffer from uncontrolled generation,\ninsufficient quality, and limited diversity in reasoning paths. Recent efforts\nleverage code to enhance CoT by grounding reasoning in executable steps, but\nsuch methods are typically constrained to predefined mathematical problems,\nhindering scalability and generalizability. In this work, we propose Caco\n(Code-Assisted Chain-of-ThOught), a novel framework that automates the\nsynthesis of high-quality, verifiable, and diverse instruction-CoT reasoning\ndata through code-driven augmentation. Unlike prior work, Caco first fine-tunes\na code-based CoT generator on existing math and programming solutions in a\nunified code format, then scales the data generation to a large amount of\ndiverse reasoning traces. Crucially, we introduce automated validation via code\nexecution and rule-based filtering to ensure logical correctness and structural\ndiversity, followed by reverse-engineering filtered outputs into natural\nlanguage instructions and language CoTs to enrich task adaptability. This\nclosed-loop process enables fully automated, scalable synthesis of reasoning\ndata with guaranteed executability. Experiments on our created Caco-1.3M\ndataset demonstrate that Caco-trained models achieve strong competitive\nperformance on mathematical reasoning benchmarks, outperforming existing strong\nbaselines. Further analysis reveals that Caco's code-anchored verification and\ninstruction diversity contribute to superior generalization across unseen\ntasks. Our work establishes a paradigm for building self-sustaining,\ntrustworthy reasoning systems without human intervention.", "AI": {"tldr": "\u63d0\u51fa\u4e86Caco\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u9a71\u52a8\u7684\u589e\u5f3a\u65b9\u6cd5\u81ea\u52a8\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u9a8c\u8bc1\u4e14\u591a\u6837\u5316\u7684\u6307\u4ee4-CoT\u63a8\u7406\u6570\u636e\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709CoT\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u4e0d\u53ef\u63a7\u3001\u8d28\u91cf\u4e0d\u8db3\u548c\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u4ee3\u7801\u7684\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u6570\u5b66\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u9996\u5148\u5728\u7edf\u4e00\u4ee3\u7801\u683c\u5f0f\u7684\u73b0\u6709\u6570\u5b66\u548c\u7f16\u7a0b\u89e3\u51b3\u65b9\u6848\u4e0a\u5fae\u8c03\u57fa\u4e8e\u4ee3\u7801\u7684CoT\u751f\u6210\u5668\uff0c\u7136\u540e\u901a\u8fc7\u4ee3\u7801\u6267\u884c\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u8fc7\u6ee4\u8fdb\u884c\u81ea\u52a8\u9a8c\u8bc1\uff0c\u786e\u4fdd\u903b\u8f91\u6b63\u786e\u6027\u548c\u7ed3\u6784\u591a\u6837\u6027\uff0c\u6700\u540e\u5c06\u8fc7\u6ee4\u540e\u7684\u8f93\u51fa\u53cd\u5411\u5de5\u7a0b\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u8bed\u8a00CoT\u3002", "result": "\u5728\u521b\u5efa\u7684Caco-1.3M\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCaco\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u7ade\u4e89\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u3002", "conclusion": "Caco\u5efa\u7acb\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6784\u5efa\u81ea\u6301\u7eed\u3001\u53ef\u4fe1\u8d56\u63a8\u7406\u7cfb\u7edf\u7684\u8303\u5f0f\uff0c\u5176\u4ee3\u7801\u951a\u5b9a\u9a8c\u8bc1\u548c\u6307\u4ee4\u591a\u6837\u6027\u6709\u52a9\u4e8e\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.03689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03689", "abs": "https://arxiv.org/abs/2510.03689", "authors": ["Zhengyi Liu", "Xinrui Wang", "Xianyong Fang", "Zhengzheng Tu", "Linbo Wang"], "title": "SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection", "comment": "Accepted by TMM", "summary": "RGB-T salient object detection (SOD) aims to segment attractive objects by\ncombining RGB and thermal infrared images. To enhance performance, the Segment\nAnything Model has been fine-tuned for this task. However, the imbalance\nconvergence of two modalities and significant gradient difference between high-\nand low- activations are ignored, thereby leaving room for further performance\nenhancement. In this paper, we propose a model called \\textit{SAMSOD}, which\nutilizes unimodal supervision to enhance the learning of non-dominant modality\nand employs gradient deconfliction to reduce the impact of conflicting\ngradients on model convergence. The method also leverages two decoupled\nadapters to separately mask high- and low-activation neurons, emphasizing\nforeground objects by enhancing background learning. Fundamental experiments on\nRGB-T SOD benchmark datasets and generalizability experiments on scribble\nsupervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised\nRGB-D rail surface defect detection all demonstrate the effectiveness of our\nproposed method.", "AI": {"tldr": "\u63d0\u51faSAMSOD\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u6a21\u6001\u76d1\u7763\u589e\u5f3a\u975e\u4e3b\u5bfc\u6a21\u6001\u5b66\u4e60\uff0c\u4f7f\u7528\u68af\u5ea6\u89e3\u8026\u51cf\u5c11\u51b2\u7a81\u68af\u5ea6\u5f71\u54cd\uff0c\u5e76\u91c7\u7528\u89e3\u8026\u9002\u914d\u5668\u5206\u522b\u5904\u7406\u9ad8\u4f4e\u6fc0\u6d3b\u795e\u7ecf\u5143\uff0c\u63d0\u5347RGB-T\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5fae\u8c03Segment Anything Model\u65f6\u5ffd\u7565\u4e86\u4e24\u79cd\u6a21\u6001\u7684\u4e0d\u5e73\u8861\u6536\u655b\u95ee\u9898\u4ee5\u53ca\u9ad8\u4f4e\u6fc0\u6d3b\u4e4b\u95f4\u7684\u663e\u8457\u68af\u5ea6\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u5355\u6a21\u6001\u76d1\u7763\u589e\u5f3a\u975e\u4e3b\u5bfc\u6a21\u6001\u5b66\u4e60\uff0c\u91c7\u7528\u68af\u5ea6\u89e3\u8026\u6280\u672f\u51cf\u5c11\u51b2\u7a81\u68af\u5ea6\u5bf9\u6a21\u578b\u6536\u655b\u7684\u5f71\u54cd\uff0c\u5229\u7528\u4e24\u4e2a\u89e3\u8026\u9002\u914d\u5668\u5206\u522b\u63a9\u7801\u9ad8\u4f4e\u6fc0\u6d3b\u795e\u7ecf\u5143\u4ee5\u589e\u5f3a\u80cc\u666f\u5b66\u4e60\u5e76\u7a81\u51fa\u524d\u666f\u76ee\u6807\u3002", "result": "\u5728RGB-T SOD\u57fa\u51c6\u6570\u636e\u96c6\u3001\u6d82\u9e26\u76d1\u7763RGB-T SOD\u3001\u5168\u76d1\u7763RGB-D SOD\u6570\u636e\u96c6\u4ee5\u53ca\u5168\u76d1\u7763RGB-D\u8f68\u9053\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u4e0a\u7684\u5b9e\u9a8c\u5747\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SAMSOD\u6a21\u578b\u901a\u8fc7\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861\u548c\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u591a\u4e2aRGB-T\u548cRGB-D\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04120", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04120", "abs": "https://arxiv.org/abs/2510.04120", "authors": ["Fengying Ye", "Shanshan Wang", "Lidia S. Chao", "Derek F. Wong"], "title": "Unveiling LLMs' Metaphorical Understanding: Exploring Conceptual Irrelevance, Context Leveraging and Syntactic Influence", "comment": null, "summary": "Metaphor analysis is a complex linguistic phenomenon shaped by context and\nexternal factors. While Large Language Models (LLMs) demonstrate advanced\ncapabilities in knowledge integration, contextual reasoning, and creative\ngeneration, their mechanisms for metaphor comprehension remain insufficiently\nexplored. This study examines LLMs' metaphor-processing abilities from three\nperspectives: (1) Concept Mapping: using embedding space projections to\nevaluate how LLMs map concepts in target domains (e.g., misinterpreting \"fall\nin love\" as \"drop down from love\"); (2) Metaphor-Literal Repository: analyzing\nmetaphorical words and their literal counterparts to identify inherent\nmetaphorical knowledge; and (3) Syntactic Sensitivity: assessing how\nmetaphorical syntactic structures influence LLMs' performance. Our findings\nreveal that LLMs generate 15\\%-25\\% conceptually irrelevant interpretations,\ndepend on metaphorical indicators in training data rather than contextual cues,\nand are more sensitive to syntactic irregularities than to structural\ncomprehension. These insights underline the limitations of LLMs in metaphor\nanalysis and call for more robust computational approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ece\u6982\u5ff5\u6620\u5c04\u3001\u9690\u55bb-\u5b57\u9762\u77e5\u8bc6\u5e93\u548c\u53e5\u6cd5\u654f\u611f\u6027\u4e09\u4e2a\u89d2\u5ea6\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9690\u55bb\u5904\u7406\u80fd\u529b\uff0c\u53d1\u73b0LLMs\u4ea7\u751f15%-25%\u6982\u5ff5\u65e0\u5173\u89e3\u91ca\uff0c\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9690\u55bb\u6307\u793a\u800c\u975e\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5bf9\u53e5\u6cd5\u4e0d\u89c4\u5219\u6027\u6bd4\u7ed3\u6784\u7406\u89e3\u66f4\u654f\u611f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u6574\u5408\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u521b\u610f\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5148\u8fdb\u80fd\u529b\uff0c\u4f46\u5176\u9690\u55bb\u7406\u89e3\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9690\u55bb\u5206\u6790\u662f\u53d7\u8bed\u5883\u548c\u5916\u90e8\u56e0\u7d20\u5f71\u54cd\u7684\u590d\u6742\u8bed\u8a00\u73b0\u8c61\u3002", "method": "\u4ece\u4e09\u4e2a\u89c6\u89d2\u7814\u7a76\uff1a(1)\u6982\u5ff5\u6620\u5c04\uff1a\u4f7f\u7528\u5d4c\u5165\u7a7a\u95f4\u6295\u5f71\u8bc4\u4f30LLMs\u5982\u4f55\u5728\u76ee\u6807\u57df\u4e2d\u6620\u5c04\u6982\u5ff5\uff1b(2)\u9690\u55bb-\u5b57\u9762\u77e5\u8bc6\u5e93\uff1a\u5206\u6790\u9690\u55bb\u8bcd\u53ca\u5176\u5b57\u9762\u5bf9\u5e94\u8bcd\u4ee5\u8bc6\u522b\u5185\u5728\u9690\u55bb\u77e5\u8bc6\uff1b(3)\u53e5\u6cd5\u654f\u611f\u6027\uff1a\u8bc4\u4f30\u9690\u55bb\u53e5\u6cd5\u7ed3\u6784\u5982\u4f55\u5f71\u54cdLLMs\u8868\u73b0\u3002", "result": "LLMs\u4ea7\u751f15%-25%\u6982\u5ff5\u65e0\u5173\u89e3\u91ca\uff08\u5982\u5c06'\u5760\u5165\u7231\u6cb3'\u8bef\u89e3\u4e3a'\u4ece\u7231\u4e2d\u6389\u843d'\uff09\uff0c\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9690\u55bb\u6307\u793a\u800c\u975e\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5bf9\u53e5\u6cd5\u4e0d\u89c4\u5219\u6027\u6bd4\u7ed3\u6784\u7406\u89e3\u66f4\u654f\u611f\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86LLMs\u5728\u9690\u55bb\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2510.03701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03701", "abs": "https://arxiv.org/abs/2510.03701", "authors": ["Kanoko Goto", "Takumi Hirose", "Mahiro Ukai", "Shuhei Kurita", "Nakamasa Inoue"], "title": "Referring Expression Comprehension for Small Objects", "comment": null, "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u5c0f\u76ee\u6807\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u7684\u65b0\u6570\u636e\u96c6SOREC\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6a21\u5757PIZA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u5b66\u4e60\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6781\u5c0f\u578b\u76ee\u6807\u7684\u5b9a\u4f4d\u4ecd\u7136\u9762\u4e34\u91cd\u5927\u6311\u6218", "method": "1) \u6784\u5efa\u5305\u542b10\u4e07\u5bf9\u6307\u4ee3\u8868\u8fbe\u548c\u8fb9\u754c\u6846\u7684SOREC\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u6e10\u8fdb\u8fed\u4ee3\u7f29\u653e\u9002\u914d\u5668PIZA\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5b9e\u73b0\u5bf9\u5c0f\u76ee\u6807\u7684\u6e10\u8fdb\u5f0f\u5b9a\u4f4d", "result": "\u5728GroundingDINO\u4e0a\u5e94\u7528PIZA\u540e\uff0c\u5728SOREC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7cbe\u5ea6\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684SOREC\u6570\u636e\u96c6\u548cPIZA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u7684\u6311\u6218\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00"}}
{"id": "2510.04124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04124", "abs": "https://arxiv.org/abs/2510.04124", "authors": ["Nuwan I. Senaratna"], "title": "Sri Lanka Document Datasets: A Large-Scale, Multilingual Resource for Law, News, and Policy (v20251005)", "comment": "4 pages", "summary": "We present a collection of open, machine-readable document datasets covering\nparliamentary proceedings, legal judgments, government publications, news, and\ntourism statistics from Sri Lanka. As of v20251005, the collection currently\ncomprises 215,670 documents (60.3 GB) across 13 datasets in Sinhala, Tamil, and\nEnglish. The datasets are updated daily and mirrored on GitHub and Hugging\nFace. These resources aim to support research in computational linguistics,\nlegal analytics, socio-political studies, and multilingual natural language\nprocessing. We describe the data sources, collection pipeline, formats, and\npotential use cases, while discussing licensing and ethical considerations.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b\u65af\u91cc\u5170\u5361\u8bae\u4f1a\u8bb0\u5f55\u3001\u6cd5\u5f8b\u5224\u51b3\u3001\u653f\u5e9c\u51fa\u7248\u7269\u3001\u65b0\u95fb\u548c\u65c5\u6e38\u7edf\u8ba1\u7684\u5f00\u653e\u3001\u673a\u5668\u53ef\u8bfb\u6587\u6863\u6570\u636e\u96c6\u96c6\u5408\uff0c\u652f\u6301\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u6cd5\u5f8b\u5206\u6790\u3001\u793e\u4f1a\u653f\u6cbb\u7814\u7a76\u548c\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u3002", "motivation": "\u4e3a\u65af\u91cc\u5170\u5361\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u6cd5\u5f8b\u5206\u6790\u3001\u793e\u4f1a\u653f\u6cbb\u7814\u7a76\u548c\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u5f00\u653e\u3001\u673a\u5668\u53ef\u8bfb\u7684\u6570\u636e\u8d44\u6e90\uff0c\u586b\u8865\u76f8\u5173\u9886\u57df\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u4ece\u591a\u4e2a\u6765\u6e90\u6536\u96c6\u8bae\u4f1a\u8bb0\u5f55\u3001\u6cd5\u5f8b\u5224\u51b3\u3001\u653f\u5e9c\u51fa\u7248\u7269\u3001\u65b0\u95fb\u548c\u65c5\u6e38\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u4ee5Sinhala\u3001Tamil\u548c\u82f1\u8bed\u4e09\u79cd\u8bed\u8a00\u6574\u7406\u621013\u4e2a\u6570\u636e\u96c6\uff0c\u6bcf\u65e5\u66f4\u65b0\u5e76\u901a\u8fc7GitHub\u548cHugging Face\u955c\u50cf\u3002", "result": "\u622a\u81f3v20251005\u7248\u672c\uff0c\u8be5\u96c6\u5408\u5305\u542b215,670\u4e2a\u6587\u6863\uff0860.3 GB\uff09\uff0c\u6db5\u76d613\u4e2a\u6570\u636e\u96c6\uff0c\u652f\u6301\u4e09\u79cd\u8bed\u8a00\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u9886\u57df\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u96c6\u5408\u4e3a\u65af\u91cc\u5170\u5361\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u652f\u6301\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u6570\u636e\u8bb8\u53ef\u548c\u4f26\u7406\u8003\u8651\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u8bed\u8a00\u5b66\u3001\u6cd5\u5f8b\u5206\u6790\u548c\u793e\u4f1a\u653f\u6cbb\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.03717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03717", "abs": "https://arxiv.org/abs/2510.03717", "authors": ["Sharan SK", "Subin Sahayam", "Umarani Jayaraman", "Lakshmi Priya A"], "title": "Artery-Vein Segmentation from Fundus Images using Deep Learning", "comment": "12 pages, 6 figures, preprint under review", "summary": "Segmenting of clinically important retinal blood vessels into arteries and\nveins is a prerequisite for retinal vessel analysis. Such analysis can provide\npotential insights and bio-markers for identifying and diagnosing various\nretinal eye diseases. Alteration in the regularity and width of the retinal\nblood vessels can act as an indicator of the health of the vasculature system\nall over the body. It can help identify patients at high risk of developing\nvasculature diseases like stroke and myocardial infarction. Over the years,\nvarious Deep Learning architectures have been proposed to perform retinal\nvessel segmentation. Recently, attention mechanisms have been increasingly used\nin image segmentation tasks. The work proposes a new Deep Learning approach for\nartery-vein segmentation. The new approach is based on the Attention mechanism\nthat is incorporated into the WNet Deep Learning model, and we call the model\nas Attention-WNet. The proposed approach has been tested on publicly available\ndatasets such as HRF and DRIVE datasets. The proposed approach has outperformed\nother state-of-art models available in the literature.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684Attention-WNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4e2d\u7684\u52a8\u9759\u8109\u5206\u7c7b\uff0c\u5728HRF\u548cDRIVE\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4e3a\u52a8\u9759\u8109\u5bf9\u4e8e\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u4e3a\u5404\u79cd\u89c6\u7f51\u819c\u773c\u75c5\u7684\u8bc6\u522b\u548c\u8bca\u65ad\u63d0\u4f9b\u751f\u7269\u6807\u5fd7\u7269\uff0c\u5e76\u5e2e\u52a9\u8bc6\u522b\u8840\u7ba1\u75be\u75c5\u9ad8\u98ce\u9669\u60a3\u8005\u3002", "method": "\u5c06\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5230WNet\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u6784\u5efaAttention-WNet\u6a21\u578b\u8fdb\u884c\u89c6\u7f51\u819c\u52a8\u9759\u8109\u5206\u5272\u3002", "result": "\u5728HRF\u548cDRIVE\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6587\u732e\u4e2d\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684Attention-WNet\u6a21\u578b\u5728\u89c6\u7f51\u819c\u52a8\u9759\u8109\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04139", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04139", "abs": "https://arxiv.org/abs/2510.04139", "authors": ["Tim Bakkenes", "Daniel Wang", "Anton Johansson"], "title": "Fine Tuning Methods for Low-resource Languages", "comment": null, "summary": "The rise of Large Language Models has not been inclusive of all cultures. The\nmodels are mostly trained on English texts and culture which makes them\nunderperform in other languages and cultural contexts. By developing a\ngeneralizable method for preparing culturally relevant datasets and\npost-training the Gemma 2 model, this project aimed to increase the performance\nof Gemma 2 for an underrepresented language and showcase how others can do the\nsame to unlock the power of Generative AI in their country and preserve their\ncultural heritage.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u6765\u51c6\u5907\u6587\u5316\u76f8\u5173\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3Gemma 2\u6a21\u578b\uff0c\u63d0\u5347\u5176\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\uff0c\u4fc3\u8fdb\u751f\u6210\u5f0fAI\u5728\u5404\u56fd\u6587\u5316\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u6587\u672c\u548c\u6587\u5316\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5728\u5176\u4ed6\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63d0\u9ad8\u6a21\u578b\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u4ee5\u4fdd\u62a4\u6587\u5316\u9057\u4ea7\u3002", "method": "\u5f00\u53d1\u901a\u7528\u65b9\u6cd5\u51c6\u5907\u6587\u5316\u76f8\u5173\u6570\u636e\u96c6\uff0c\u5e76\u5bf9Gemma 2\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "\u63d0\u9ad8\u4e86Gemma 2\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u5e2e\u52a9\u5176\u4ed6\u56fd\u5bb6\u89e3\u9501\u751f\u6210\u5f0fAI\u7684\u6f5c\u529b\u5e76\u4fdd\u62a4\u5176\u6587\u5316\u9057\u4ea7\uff0c\u5c55\u793a\u4e86\u6587\u5316\u5305\u5bb9\u6027AI\u53d1\u5c55\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.03721", "categories": ["cs.CV", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03721", "abs": "https://arxiv.org/abs/2510.03721", "authors": ["Leander Girrbach", "Stephan Alaniz", "Genevieve Smith", "Trevor Darrell", "Zeynep Akata"], "title": "Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models", "comment": "48 pages", "summary": "Vision-language models trained on large-scale multimodal datasets show strong\ndemographic biases, but the role of training data in producing these biases\nremains unclear. A major barrier has been the lack of demographic annotations\nin web-scale datasets such as LAION-400M. We address this gap by creating\nperson-centric annotations for the full dataset, including over 276 million\nbounding boxes, perceived gender and race/ethnicity labels, and automatically\ngenerated captions. These annotations are produced through validated automatic\nlabeling pipelines combining object detection, multimodal captioning, and\nfinetuned classifiers. Using them, we uncover demographic imbalances and\nharmful associations, such as the disproportionate linking of men and\nindividuals perceived as Black or Middle Eastern with crime-related and\nnegative content. We also show that 60-70% of gender bias in CLIP and Stable\nDiffusion can be linearly explained by direct co-occurrences in the data. Our\nresources establish the first large-scale empirical link between dataset\ncomposition and downstream model bias.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86LAION-400M\u6570\u636e\u96c6\u7684\u5168\u9762\u4eba\u53e3\u7edf\u8ba1\u6807\u6ce8\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4eba\u53e3\u4e0d\u5e73\u8861\u548c\u6709\u5bb3\u5173\u8054\uff0c\u5e76\u8bc1\u660eCLIP\u548cStable Diffusion\u4e2d60-70%\u7684\u6027\u522b\u504f\u89c1\u53ef\u7531\u6570\u636e\u4e2d\u7684\u76f4\u63a5\u5171\u73b0\u7ebf\u6027\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u7f3a\u4e4f\u4eba\u53e3\u7edf\u8ba1\u6807\u6ce8\u7684\u95ee\u9898\uff0c\u4ee5\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u5728\u4ea7\u751f\u6a21\u578b\u504f\u89c1\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u9a8c\u8bc1\u7684\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u7ed3\u5408\u76ee\u6807\u68c0\u6d4b\u3001\u591a\u6a21\u6001\u5b57\u5e55\u751f\u6210\u548c\u5fae\u8c03\u5206\u7c7b\u5668\uff0c\u4e3a\u6574\u4e2a\u6570\u636e\u96c6\u521b\u5efa\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6807\u6ce8\uff0c\u5305\u62ec\u8fb9\u754c\u6846\u3001\u611f\u77e5\u6027\u522b\u548c\u79cd\u65cf/\u6c11\u65cf\u6807\u7b7e\u4ee5\u53ca\u81ea\u52a8\u751f\u6210\u7684\u5b57\u5e55\u3002", "result": "\u53d1\u73b0\u4eba\u53e3\u7edf\u8ba1\u4e0d\u5e73\u8861\u548c\u6709\u5bb3\u5173\u8054\uff0c\u5982\u7537\u6027\u548c\u88ab\u611f\u77e5\u4e3a\u9ed1\u4eba\u6216\u4e2d\u4e1c\u88d4\u7684\u4e2a\u4f53\u4e0e\u72af\u7f6a\u76f8\u5173\u548c\u8d1f\u9762\u5185\u5bb9\u7684\u4e0d\u6210\u6bd4\u4f8b\u5173\u8054\u300260-70%\u7684CLIP\u548cStable Diffusion\u6027\u522b\u504f\u89c1\u53ef\u7531\u6570\u636e\u4e2d\u7684\u76f4\u63a5\u5171\u73b0\u7ebf\u6027\u89e3\u91ca\u3002", "conclusion": "\u8fd9\u4e9b\u8d44\u6e90\u9996\u6b21\u5efa\u7acb\u4e86\u6570\u636e\u96c6\u7ec4\u6210\u4e0e\u4e0b\u6e38\u6a21\u578b\u504f\u89c1\u4e4b\u95f4\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u8054\u7cfb\u3002"}}
{"id": "2510.04147", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04147", "abs": "https://arxiv.org/abs/2510.04147", "authors": ["Yifeng Gao", "Ziang Ji", "Yuxuan Wang", "Biqing Qi", "Hanlin Xu", "Linfeng Zhang"], "title": "Self Speculative Decoding for Diffusion Large Language Models", "comment": null, "summary": "Diffusion-based Large Language Models (dLLMs) have emerged as a competitive\nalternative to autoregressive models, offering unique advantages through\nbidirectional attention and parallel generation paradigms. However, the\ngeneration results of current parallel decoding methods deviate from stepwise\ndecoding, introducing potential performance degradation, which limits their\npractical deployment. To address this problem, we propose \\textbf{S}elf\n\\textbf{S}peculative \\textbf{D}ecoding (SSD), a lossless inference acceleration\nmethod that leverages the dLLM itself as both speculative decoding drafter and\nverifier without auxiliary modules. SSD introduces a self-drafting mechanism\nwhere the model generates predictions for multiple positions, then verifies\nthem through hierarchical verification trees in a single forward pass. Unlike\ntraditional speculative decoding that requires separate draft models, SSD\neliminates model redundancy and memory overhead by exploiting the dLLM's\ninherent parallel prediction capability for multiple positions. This\nself-speculative approach allows the model to progressively verify and accept\nmultiple tokens in a single forward pass. Our experiments demonstrate that SSD\nachieves up to 3.46$\\times$ speedup while keeping the output identical to\nstepwise decoding on open source models such as LLaDA and Dream. Code will be\nmade publicly available on GitHub.", "AI": {"tldr": "SSD\u662f\u4e00\u79cd\u65e0\u635f\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u4f5c\u4e3a\u63a8\u6d4b\u89e3\u7801\u7684\u8d77\u8349\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u65e0\u9700\u8f85\u52a9\u6a21\u5757\uff0c\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u901a\u8fc7\u5c42\u6b21\u9a8c\u8bc1\u6811\u9a8c\u8bc1\u591a\u4e2a\u4f4d\u7f6e\u9884\u6d4b\uff0c\u5b9e\u73b0\u6700\u9ad83.46\u500d\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\u7684\u751f\u6210\u7ed3\u679c\u4e0e\u9010\u6b65\u89e3\u7801\u5b58\u5728\u504f\u5dee\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u81ea\u63a8\u6d4b\u89e3\u7801(SSD)\uff0c\u5229\u7528dLLM\u81ea\u8eab\u4f5c\u4e3a\u8d77\u8349\u5668\u548c\u9a8c\u8bc1\u5668\uff0c\u5f15\u5165\u81ea\u8d77\u8349\u673a\u5236\u751f\u6210\u591a\u4e2a\u4f4d\u7f6e\u9884\u6d4b\uff0c\u901a\u8fc7\u5c42\u6b21\u9a8c\u8bc1\u6811\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5728LLaDA\u548cDream\u7b49\u5f00\u6e90\u6a21\u578b\u4e0a\uff0cSSD\u5b9e\u73b0\u4e86\u6700\u9ad83.46\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u4e0e\u9010\u6b65\u89e3\u7801\u5b8c\u5168\u76f8\u540c\u3002", "conclusion": "SSD\u901a\u8fc7\u81ea\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e76\u884c\u89e3\u7801\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u635f\u63a8\u7406\u52a0\u901f\uff0c\u6d88\u9664\u4e86\u6a21\u578b\u5197\u4f59\u548c\u5185\u5b58\u5f00\u9500\u3002"}}
{"id": "2510.03725", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03725", "abs": "https://arxiv.org/abs/2510.03725", "authors": ["Thomas Hallopeau", "Joris Gu\u00e9rin", "Laurent Demagistri", "Youssef Fouzai", "Renata Gracie", "Vanderlei Pascoal De Matos", "Helen Gurgel", "Nadine Dessay"], "title": "Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks", "comment": "6 pages, 1 figure, 1 table. Presented at the 21st Brazilian Symposium\n  on Remote Sensing (SBSR 2025)", "summary": "While deep learning methods for detecting informal settlements have already\nbeen developed, they have not yet fully utilized the potential offered by\nrecent pretrained neural networks. We compare two types of pretrained neural\nnetworks for detecting the favelas of Rio de Janeiro: 1. Generic networks\npretrained on large diverse datasets of unspecific images, 2. A specialized\nnetwork pretrained on satellite imagery. While the latter is more specific to\nthe target task, the former has been pretrained on significantly more images.\nHence, this research investigates whether task specificity or data volume\nyields superior performance in urban informal settlement detection.", "AI": {"tldr": "\u6bd4\u8f83\u4e24\u79cd\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5728\u68c0\u6d4b\u91cc\u7ea6\u70ed\u5185\u5362\u8d2b\u6c11\u7a9f\u4e2d\u7684\u8868\u73b0\uff1a\u901a\u7528\u7f51\u7edc\uff08\u5927\u6570\u636e\u91cf\uff09\u4e0e\u4e13\u7528\u536b\u661f\u56fe\u50cf\u7f51\u7edc\uff08\u4efb\u52a1\u7279\u5f02\u6027\uff09\uff0c\u7814\u7a76\u4efb\u52a1\u7279\u5f02\u6027\u4e0e\u6570\u636e\u91cf\u54ea\u4e2a\u5bf9\u6027\u80fd\u5f71\u54cd\u66f4\u5927", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u529b\uff0c\u9700\u8981\u63a2\u7d22\u4efb\u52a1\u7279\u5f02\u6027\u4e0e\u6570\u636e\u91cf\u5728\u975e\u6b63\u5f0f\u4f4f\u533a\u68c0\u6d4b\u4e2d\u7684\u76f8\u5bf9\u91cd\u8981\u6027", "method": "\u4f7f\u7528\u4e24\u79cd\u9884\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff1a1\uff09\u5728\u5927\u578b\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u901a\u7528\u7f51\u7edc\uff1b2\uff09\u5728\u536b\u661f\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u7684\u4e13\u7528\u7f51\u7edc\uff0c\u6bd4\u8f83\u5b83\u4eec\u5728\u91cc\u7ea6\u70ed\u5185\u5362\u8d2b\u6c11\u7a9f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "result": "\u8bba\u6587\u672a\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u7f51\u7edc\u7684\u6027\u80fd\u6765\u8bc4\u4f30\u4efb\u52a1\u7279\u5f02\u6027\u548c\u6570\u636e\u91cf\u7684\u5f71\u54cd", "conclusion": "\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u5728\u975e\u6b63\u5f0f\u4f4f\u533a\u68c0\u6d4b\u4e2d\uff0c\u4efb\u52a1\u7279\u5f02\u6027\u8fd8\u662f\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u5bf9\u6027\u80fd\u63d0\u5347\u66f4\u4e3a\u5173\u952e"}}
{"id": "2510.04182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04182", "abs": "https://arxiv.org/abs/2510.04182", "authors": ["Wengao Ye", "Yan Liang", "Lianlei Shan"], "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have shifted from\nexplicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning,\nwhere intermediate thoughts are represented as vectors rather than text.\nHowever, latent reasoning can be brittle on challenging, out-of-distribution\ntasks where robust reasoning is most critical. To overcome these limitations,\nwe introduce Latent Thought Policy Optimization (LTPO), a parameter-free\nframework that enhances LLM reasoning entirely at test time, without requiring\nmodel parameter updates. LTPO treats intermediate latent \"thought\" vectors as\ndynamic parameters that are actively optimized for each problem instance. It\nemploys an online policy gradient method guided by an intrinsic,\nconfidence-based reward signal computed directly from the frozen LLM's own\noutput distributions, eliminating the need for external supervision or\nexpensive text generation during optimization. Extensive experiments on five\nreasoning benchmarks show that LTPO not only matches or surpasses strong\nbaselines on standard tasks but also demonstrates remarkable robustness where\nothers fail. Most notably, on highly challenging AIME benchmarks where existing\nlatent reasoning baselines collapse to near-zero accuracy, LTPO delivers\nsubstantial improvements, showcasing a unique capability for complex reasoning.", "AI": {"tldr": "LTPO\u662f\u4e00\u79cd\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e2d\u95f4\u6f5c\u5728\u601d\u7ef4\u5411\u91cf\u4f5c\u4e3a\u52a8\u6001\u53c2\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u4f7f\u7528\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u5185\u5728\u5956\u52b1\u4fe1\u53f7\u6765\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u4ece\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u8f6c\u5411\u66f4\u9ad8\u6548\u7684\u6f5c\u5728\u63a8\u7406\uff0c\u4f46\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\uff0c\u6f5c\u5728\u63a8\u7406\u53ef\u80fd\u4e0d\u7a33\u5b9a\u3002\u9700\u8981\u5728\u4e0d\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u63a8\u7406\u9c81\u68d2\u6027\u3002", "method": "LTPO\u5c06\u4e2d\u95f4\u6f5c\u5728\u601d\u7ef4\u5411\u91cf\u89c6\u4e3a\u52a8\u6001\u53c2\u6570\uff0c\u4f7f\u7528\u5728\u7ebf\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3LLM\u81ea\u8eab\u8f93\u51fa\u5206\u5e03\u8ba1\u7b97\u7684\u7f6e\u4fe1\u5ea6\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u4f18\u5316\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u6587\u672c\u751f\u6210\u3002", "result": "\u5728\u4e94\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLTPO\u4e0d\u4ec5\u5339\u914d\u6216\u8d85\u8d8a\u5f3a\u57fa\u7ebf\uff0c\u8fd8\u5728AIME\u57fa\u51c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6539\u8fdb\uff0c\u800c\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u57fa\u7ebf\u51c6\u786e\u7387\u63a5\u8fd1\u96f6\u3002", "conclusion": "LTPO\u5c55\u793a\u4e86\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u72ec\u7279\u80fd\u529b\uff0c\u4e3a\u589e\u5f3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.03747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03747", "abs": "https://arxiv.org/abs/2510.03747", "authors": ["Zuomin Qu", "Yimao Guo", "Qianyue Hu", "Wei Lu"], "title": "LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes", "comment": null, "summary": "Deepfakes pose significant societal risks, motivating the development of\nproactive defenses that embed adversarial perturbations in facial images to\nprevent manipulation. However, in this paper, we show that these preemptive\ndefenses often lack robustness and reliability. We propose a novel approach,\nLow-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch\ninto Deepfake generators to bypass state-of-the-art defenses. A learnable\ngating mechanism adaptively controls the effect of the LoRA patch and prevents\ngradient explosions during fine-tuning. We also introduce a Multi-Modal Feature\nAlignment (MMFA) loss, encouraging the features of adversarial outputs to align\nwith those of the desired outputs at the semantic level. Beyond bypassing, we\npresent defensive LoRA patching, embedding visible warnings in the outputs as a\ncomplementary solution to mitigate this newly identified security\nvulnerability. With only 1,000 facial examples and a single epoch of\nfine-tuning, LoRA patching successfully defeats multiple proactive defenses.\nThese results reveal a critical weakness in current paradigms and underscore\nthe need for more robust Deepfake defense strategies. Our code is available at\nhttps://github.com/ZOMIN28/LoRA-Patching.", "AI": {"tldr": "\u63d0\u51faLoRA\u4fee\u8865\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728Deepfake\u751f\u6210\u5668\u4e2d\u6ce8\u5165\u53ef\u63d2\u62d4\u7684LoRA\u8865\u4e01\u6765\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\uff0c\u540c\u65f6\u5f15\u5165\u9632\u5fa1\u6027LoRA\u4fee\u8865\u4f5c\u4e3a\u8865\u5145\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u9632\u5fa1\u63aa\u65bd\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7ed5\u8fc7\u8fd9\u4e9b\u9632\u5fa1\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u5f53\u524d\u9632\u5fa1\u8303\u5f0f\u7684\u5f31\u70b9\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u4fee\u8865\u6280\u672f\uff0c\u5728Deepfake\u751f\u6210\u5668\u4e2d\u6ce8\u5165\u53ef\u63d2\u62d4\u7684LoRA\u8865\u4e01\uff1b\u91c7\u7528\u53ef\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u81ea\u9002\u5e94\u63a7\u5236LoRA\u8865\u4e01\u6548\u679c\uff1b\u5f15\u5165\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff08MMFA\uff09\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u8bed\u4e49\u7ea7\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u4ec5\u75281000\u4e2a\u9762\u90e8\u793a\u4f8b\u548c\u5355\u8f6e\u5fae\u8c03\uff0cLoRA\u4fee\u8865\u6210\u529f\u51fb\u8d25\u591a\u79cd\u4e3b\u52a8\u9632\u5fa1\u63aa\u65bd\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u9632\u5fa1\u7b56\u7565\u7684\u5173\u952e\u5f31\u70b9\u3002", "conclusion": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u9632\u5fa1\u7b56\u7565\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u9632\u5fa1\u6027LoRA\u4fee\u8865\u53ef\u4f5c\u4e3a\u8865\u5145\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04204", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04204", "abs": "https://arxiv.org/abs/2510.04204", "authors": ["Zhengyang Tang", "Zihan Ye", "Chenyu Huang", "Xuhan Huang", "Chengpeng Li", "Sihang Li", "Guanhua Chen", "Ming Yan", "Zizhuo Wang", "Hongyuan Zha", "Dayiheng Liu", "Benyou Wang"], "title": "CALM Before the STORM: Unlocking Native Reasoning for Optimization Modeling", "comment": "Work in progress", "summary": "Large Reasoning Models (LRMs) have demonstrated strong capabilities in\ncomplex multi-step reasoning, opening new opportunities for automating\noptimization modeling. However, existing domain adaptation methods, originally\ndesigned for earlier instruction-tuned models, often fail to exploit the\nadvanced reasoning patterns of modern LRMs -- In particular, we show that\ndirect fine-tuning on traditional \\textit{non-reflective} datasets leads to\nlimited gains. To fully leverage LRMs' inherent reasoning abilities, we propose\n\\textbf{CALM} (\\textit{Corrective Adaptation with Lightweight Modification}), a\nframework that progressively refines LRMs within their native reasoning modes\nfor optimization modeling tasks. In CALM, an expert intervener identifies\nreasoning flaws and provides concise corrective hints, which the LRM\nincorporates to produce improved reasoning trajectories. These interventions\nmodify fewer than 2.6\\% of generated tokens, but generate high-quality data for\nsoft adaptation through supervised fine-tuning. The adapted model is then\nfurther improved through reinforcement learning. Building on CALM, we develop\n\\textbf{STORM} (\\textit{Smart Thinking Optimization Reasoning Model}), a\n4B-parameter LRM that achieves a new state-of-the-art average accuracy of\n68.9\\% across five popular optimization modeling benchmarks, matching the\nperformance of a 671B LRM. These results demonstrate that dynamic, hint-based\ndata synthesis both preserves and amplifies the native reasoning patterns of\nmodern LRMs, offering a more effective and scalable path towards expert-level\nperformance on challenging optimization modeling tasks.", "AI": {"tldr": "CALM\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4fee\u6b63\u9002\u914d\u65b9\u6cd5\uff0c\u5229\u7528LRMs\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u4f18\u5316\u5efa\u6a21\uff0c\u5f00\u53d1\u51faSTORM\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "motivation": "\u73b0\u6709\u9886\u57df\u9002\u914d\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u9ad8\u7ea7\u63a8\u7406\u6a21\u5f0f\uff0c\u76f4\u63a5\u5fae\u8c03\u4f20\u7edf\u975e\u53cd\u601d\u6570\u636e\u96c6\u6548\u679c\u6709\u9650", "method": "\u63d0\u51faCALM\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u5e72\u9884\u8bc6\u522b\u63a8\u7406\u7f3a\u9677\u5e76\u63d0\u4f9b\u4fee\u6b63\u63d0\u793a\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u7528\u4e8e\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60", "result": "STORM\u6a21\u578b\u57285\u4e2a\u4f18\u5316\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523068.9%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5339\u914d671B\u53c2\u6570LRM\u7684\u6027\u80fd", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u52a8\u6001\u6570\u636e\u5408\u6210\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u5e76\u589e\u5f3a\u73b0\u4ee3LRMs\u7684\u539f\u751f\u63a8\u7406\u6a21\u5f0f\uff0c\u4e3a\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u63d0\u4f9b\u66f4\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u8def\u5f84"}}
{"id": "2510.03751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03751", "abs": "https://arxiv.org/abs/2510.03751", "authors": ["Mubariz Zaffar", "Liangliang Nan", "Sebastian Scherer", "Julian F. P. Kooij"], "title": "The Overlooked Value of Test-time Reference Sets in Visual Place Recognition", "comment": "Accepted at ICCV 2025 Workshop CrocoDL", "summary": "Given a query image, Visual Place Recognition (VPR) is the task of retrieving\nan image of the same place from a reference database with robustness to\nviewpoint and appearance changes. Recent works show that some VPR benchmarks\nare solved by methods using Vision-Foundation-Model backbones and trained on\nlarge-scale and diverse VPR-specific datasets. Several benchmarks remain\nchallenging, particularly when the test environments differ significantly from\nthe usual VPR training datasets. We propose a complementary, unexplored source\nof information to bridge the train-test domain gap, which can further improve\nthe performance of State-of-the-Art (SOTA) VPR methods on such challenging\nbenchmarks. Concretely, we identify that the test-time reference set, the\n\"map\", contains images and poses of the target domain, and must be available\nbefore the test-time query is received in several VPR applications. Therefore,\nwe propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on\nthe map, boosting the SOTA (~2.3% increase on average for Recall@1) on these\nchallenging datasets. Finetuned models retain generalization, and RSF works\nacross diverse test datasets.", "AI": {"tldr": "\u63d0\u51fa\u53c2\u8003\u96c6\u5fae\u8c03(RSF)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u53c2\u8003\u96c6\u4e0a\u5fae\u8c03VPR\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5728\u5177\u6709\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u73b0\u6709VPR\u65b9\u6cd5\u5728\u8bad\u7ec3-\u6d4b\u8bd5\u9886\u57df\u5dee\u5f02\u8f83\u5927\u7684\u6311\u6218\u6027\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6d4b\u8bd5\u65f6\u7684\u53c2\u8003\u96c6\u5305\u542b\u76ee\u6807\u9886\u57df\u4fe1\u606f\uff0c\u53ef\u4f5c\u4e3a\u8865\u5145\u4fe1\u606f\u6e90\u6765\u5f25\u5408\u9886\u57df\u5dee\u8ddd", "method": "\u63d0\u51fa\u53c2\u8003\u96c6\u5fae\u8c03(RSF)\uff0c\u5229\u7528\u6d4b\u8bd5\u524d\u53ef\u83b7\u5f97\u7684\u53c2\u8003\u96c6\u56fe\u50cf\u548c\u4f4d\u59ff\u4fe1\u606f\u5bf9VPR\u6a21\u578b\u8fdb\u884c\u5fae\u8c03", "result": "\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u5347Recall@1\u7ea62.3%\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\uff0c\u4e14RSF\u65b9\u6cd5\u5728\u4e0d\u540c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u5747\u6709\u6548", "conclusion": "\u53c2\u8003\u96c6\u5fae\u8c03\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u6d4b\u8bd5\u65f6\u53c2\u8003\u96c6\u4fe1\u606f\u663e\u8457\u63d0\u5347VPR\u6a21\u578b\u5728\u9886\u57df\u5dee\u5f02\u8f83\u5927\u573a\u666f\u4e0b\u7684\u6027\u80fd"}}
{"id": "2510.04214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04214", "abs": "https://arxiv.org/abs/2510.04214", "authors": ["Zhuoran Zhuang", "Ye Chen", "Xia Zeng", "Chao Luo", "Luhui Liu", "Yihan Chen"], "title": "Teaching LLM to be Persuasive: Reward-Enhanced Policy Optimization for Alignment frm Heterogeneous Rewards", "comment": null, "summary": "We study deploying large language models (LLMs) as business development (BD)\nagents for persuasive price negotiation in online travel agencies (OTAs), where\naligning traveler affordability and hotel profitability directly affects\nbookings, partner relationships, and access to travel. The agent must follow a\nStandard Operating Procedure (SOP) while conducting multi-turn persuasion,\ninterpreting colloquial inputs, and adhering to guardrails (no over-promising,\nno hallucinations). Conventional post-training -- supervised fine-tuning (SFT)\nor single-source reward optimization -- overfits scripts, misses nuanced\npersuasive style, and fails to enforce verifiable business constraints.\n  We propose Reward-Enhanced Policy Optimization (REPO), a reinforcement\nlearning post-training framework that aligns an LLM with heterogeneous rewards:\na preference-trained reward model (RM) for dense human alignment, a reward\njudge (RJ) for high-level persuasive behavior and SOP compliance, and\nprogrammatic reward functions (RF) for deterministic checks on numerics,\nformatting, and guardrails. A straightforward enhancement mechanism is proposed\nto combine the RM with RJ and RF signals to curb reward hacking and improve\nnegotiation quality. In production-style evaluations -- approximately 150 turns\nfrom real dialogues and 225 turns from curated bad-case dialogues -- REPO lifts\naverage dialogue rating to 4.63: +1.20 over base, +0.83 over Direct Preference\nOptimization (DPO); +0.33 over Group Relative Policy Optimization (GRPO),\nincreases the share of conversations with at least one excellent response to\n66.67% (+23.34 percentage points over GRPO), and achieves a 93.33% bad-case fix\nrate with 75.56% clean fixes, outperforming SFT, DPO, PPO, and GRPO. We also\nobserve emergent capabilities -- proactive empathy, localized reasoning,\ncalibrated tactics -- that surpass gold annotations.", "AI": {"tldr": "\u63d0\u51fa\u4e86REPO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u504f\u597d\u5956\u52b1\u6a21\u578b\u3001\u5956\u52b1\u5224\u65ad\u5668\u548c\u7a0b\u5e8f\u5316\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3LLM\u5728\u5546\u52a1\u8c08\u5224\u4e2d\u8fc7\u62df\u5408\u811a\u672c\u3001\u7f3a\u4e4f\u8bf4\u670d\u529b\u98ce\u683c\u548c\u65e0\u6cd5\u6267\u884c\u4e1a\u52a1\u7ea6\u675f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e3a\u5546\u52a1\u8c08\u5224\u4ee3\u7406\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u811a\u672c\u3001\u7f3a\u4e4f\u7ec6\u817b\u8bf4\u670d\u98ce\u683c\u3001\u65e0\u6cd5\u5f3a\u5236\u6267\u884c\u53ef\u9a8c\u8bc1\u4e1a\u52a1\u7ea6\u675f\u7684\u95ee\u9898\u3002", "method": "REPO\u6846\u67b6\u7ed3\u5408\u4e09\u79cd\u5956\u52b1\uff1a\u504f\u597d\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u7528\u4e8e\u5bc6\u96c6\u4eba\u7c7b\u5bf9\u9f50\uff0c\u5956\u52b1\u5224\u65ad\u5668\u7528\u4e8e\u9ad8\u7ea7\u8bf4\u670d\u884c\u4e3a\u548cSOP\u5408\u89c4\uff0c\u7a0b\u5e8f\u5316\u5956\u52b1\u51fd\u6570\u7528\u4e8e\u6570\u503c\u3001\u683c\u5f0f\u548c\u62a4\u680f\u7684\u786e\u5b9a\u6027\u68c0\u67e5\u3002", "result": "\u5728150\u8f6e\u771f\u5b9e\u5bf9\u8bdd\u548c225\u8f6e\u7cbe\u9009\u574f\u6848\u4f8b\u5bf9\u8bdd\u8bc4\u4f30\u4e2d\uff0cREPO\u5c06\u5e73\u5747\u5bf9\u8bdd\u8bc4\u5206\u63d0\u5347\u81f34.63\uff0c\u6bd4\u57fa\u7ebf\u63d0\u9ad81.20\uff0c\u6bd4DPO\u63d0\u9ad80.83\uff1b\u5c06\u81f3\u5c11\u5305\u542b\u4e00\u4e2a\u4f18\u79c0\u56de\u590d\u7684\u5bf9\u8bdd\u6bd4\u4f8b\u63d0\u5347\u81f366.67%\uff0c\u574f\u6848\u4f8b\u4fee\u590d\u7387\u8fbe\u523093.33%\u3002", "conclusion": "REPO\u6846\u67b6\u5728\u5546\u52a1\u8c08\u5224\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8eSFT\u3001DPO\u3001PPO\u548cGRPO\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6d8c\u73b0\u51fa\u4e3b\u52a8\u540c\u7406\u5fc3\u3001\u5c40\u90e8\u63a8\u7406\u548c\u6821\u51c6\u7b56\u7565\u7b49\u8d85\u8d8a\u9ec4\u91d1\u6807\u6ce8\u7684\u65b0\u80fd\u529b\u3002"}}
{"id": "2510.03763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03763", "abs": "https://arxiv.org/abs/2510.03763", "authors": ["Jiaxin Deng", "Junbiao Pang"], "title": "Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization", "comment": null, "summary": "Sharpness-Aware Minimization (SAM) improves model generalization but doubles\nthe computational cost of Stochastic Gradient Descent (SGD) by requiring twice\nthe gradient calculations per optimization step. To mitigate this, we propose\nAdaptively sampling-Reusing-mixing decomposed gradients to significantly\naccelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can\nbe decomposed into the SGD gradient and the Projection of the Second-order\ngradient onto the First-order gradient (PSF). Furthermore, we observe that the\nSGD gradient and PSF dynamically evolve during training, emphasizing the\ngrowing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed\nto the reused PSF and the timely updated PSF still maintain the model's\ngeneralization ability. Extensive experiments show that ARSAM achieves\nstate-of-the-art accuracies comparable to SAM across diverse network\narchitectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a\nspeedup of about 40\\%. Moreover, ARSAM accelerates optimization for the various\nchallenge tasks (\\textit{e.g.}, human pose estimation, and model quantization)\nwithout sacrificing performance, demonstrating its broad practicality.% The\ncode is publicly accessible at: https://github.com/ajiaaa/ARSAM.", "AI": {"tldr": "ARSAM\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u3001\u91cd\u7528\u548c\u6df7\u5408\u5206\u89e3\u68af\u5ea6\u6765\u52a0\u901fSAM\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u7ea640%\u3002", "motivation": "SAM\u867d\u7136\u80fd\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u662fSGD\u7684\u4e24\u500d\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u5c06SAM\u68af\u5ea6\u5206\u89e3\u4e3aSGD\u68af\u5ea6\u548c\u4e8c\u9636\u68af\u5ea6\u5728\u4e00\u9636\u68af\u5ea6\u4e0a\u7684\u6295\u5f71(PSF)\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u91cd\u7528PSF\u548c\u53ca\u65f6\u66f4\u65b0PSF\u3002", "result": "\u5728CIFAR-10/100\u7b49\u6570\u636e\u96c6\u4e0a\uff0cARSAM\u8fbe\u5230\u4e0eSAM\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u901f\u5ea6\u63d0\u5347\u7ea640%\uff0c\u5e76\u5728\u59ff\u6001\u4f30\u8ba1\u3001\u6a21\u578b\u91cf\u5316\u7b49\u6311\u6218\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ARSAM\u5728\u4fdd\u6301SAM\u6cdb\u5316\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.04226", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04226", "abs": "https://arxiv.org/abs/2510.04226", "authors": ["Dustin Wright", "Sarah Masud", "Jared Moore", "Srishti Yadav", "Maria Antoniak", "Chan Young Park", "Isabelle Augenstein"], "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models", "comment": "16 pages; 8 figures, 4 tables", "summary": "Large language models (LLMs) tend to generate lexically, semantically, and\nstylistically homogenous texts. This poses a risk of knowledge collapse, where\nhomogenous LLMs mediate a shrinking in the range of accessible information over\ntime. Existing works on homogenization are limited by a focus on closed-ended\nmultiple-choice setups or fuzzy semantic features, and do not look at trends\nacross time and cultural contexts. To overcome this, we present a new\nmethodology to measure epistemic diversity, i.e., variation in real-world\nclaims in LLM outputs, which we use to perform a broad empirical study of LLM\nknowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200\nprompt variations sourced from real user chats. For the topics in our study, we\nshow that while newer models tend to generate more diverse claims, nearly all\nmodels are less epistemically diverse than a basic web search. We find that\nmodel size has a negative impact on epistemic diversity, while\nretrieval-augmented generation (RAG) has a positive impact, though the\nimprovement from RAG varies by the cultural context. Finally, compared to a\ntraditional knowledge source (Wikipedia), we find that country-specific claims\nreflect the English language more than the local one, highlighting a gap in\nepistemic representation", "AI": {"tldr": "LLMs\u751f\u6210\u6587\u672c\u5b58\u5728\u540c\u8d28\u5316\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u77e5\u8bc6\u5d29\u6e83\u3002\u672c\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\u6d4b\u91cf\u8ba4\u77e5\u591a\u6837\u6027\uff0c\u53d1\u73b0\u65b0\u6a21\u578b\u867d\u591a\u6837\u6027\u6709\u6240\u63d0\u5347\uff0c\u4f46\u4ecd\u4e0d\u53ca\u57fa\u7840\u7f51\u7edc\u641c\u7d22\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u5bf9\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff0cRAG\u5219\u6709\u6b63\u9762\u4f5c\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5c01\u95ed\u5f0f\u9009\u62e9\u9898\u6216\u6a21\u7cca\u8bed\u4e49\u7279\u5f81\uff0c\u672a\u80fd\u8003\u5bdf\u8de8\u65f6\u95f4\u548c\u6587\u5316\u80cc\u666f\u7684\u8d8b\u52bf\u3002LLM\u6587\u672c\u540c\u8d28\u5316\u53ef\u80fd\u5bfc\u81f4\u77e5\u8bc6\u5d29\u6e83\u98ce\u9669\uff0c\u5373\u968f\u65f6\u95f4\u63a8\u79fb\u53ef\u83b7\u53d6\u4fe1\u606f\u8303\u56f4\u7f29\u5c0f\u3002", "method": "\u63d0\u51fa\u6d4b\u91cf\u8ba4\u77e5\u591a\u6837\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u6d4b\u8bd527\u4e2aLLM\u3001155\u4e2a\u6db5\u76d612\u4e2a\u56fd\u5bb6\u7684\u4e3b\u9898\u3001200\u4e2a\u771f\u5b9e\u7528\u6237\u804a\u5929\u63d0\u793a\u53d8\u4f53\uff0c\u5206\u6790LLM\u8f93\u51fa\u4e2d\u73b0\u5b9e\u4e16\u754c\u4e3b\u5f20\u7684\u591a\u6837\u6027\u3002", "result": "\u65b0\u6a21\u578b\u751f\u6210\u66f4\u591a\u6837\u5316\u4e3b\u5f20\uff0c\u4f46\u51e0\u4e4e\u6240\u6709\u6a21\u578b\u8ba4\u77e5\u591a\u6837\u6027\u4f4e\u4e8e\u57fa\u7840\u7f51\u7edc\u641c\u7d22\uff1b\u6a21\u578b\u89c4\u6a21\u5bf9\u8ba4\u77e5\u591a\u6837\u6027\u6709\u8d1f\u9762\u5f71\u54cd\uff1bRAG\u6709\u6b63\u9762\u5f71\u54cd\uff0c\u4f46\u6539\u5584\u7a0b\u5ea6\u56e0\u6587\u5316\u80cc\u666f\u800c\u5f02\uff1b\u76f8\u6bd4\u7ef4\u57fa\u767e\u79d1\uff0c\u56fd\u5bb6\u7279\u5b9a\u4e3b\u5f20\u66f4\u591a\u53cd\u6620\u82f1\u8bed\u800c\u975e\u5f53\u5730\u8bed\u8a00\u3002", "conclusion": "LLM\u5b58\u5728\u8ba4\u77e5\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u53cd\u800c\u964d\u4f4e\u591a\u6837\u6027\uff0cRAG\u80fd\u6539\u5584\u4f46\u6548\u679c\u53d7\u6587\u5316\u80cc\u666f\u5f71\u54cd\uff0c\u9700\u8981\u5173\u6ce8\u8ba4\u77e5\u8868\u5f81\u4e2d\u7684\u8bed\u8a00\u504f\u5411\u95ee\u9898\u3002"}}
{"id": "2510.03767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03767", "abs": "https://arxiv.org/abs/2510.03767", "authors": ["Yiheng Dong", "Yi Lin", "Xin Yang"], "title": "CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis", "comment": "Accepted by MICCAI2025", "summary": "The transparency of deep learning models is essential for clinical\ndiagnostics. Concept Bottleneck Model provides clear decision-making processes\nfor diagnosis by transforming the latent space of black-box models into\nhuman-understandable concepts. However, concept-based methods still face\nchallenges in concept capture capabilities. These methods often rely on encode\nfeatures solely from the final layer, neglecting shallow and multiscale\nfeatures, and lack effective guidance in concept encoding, hindering\nfine-grained concept extraction. To address these issues, we introduce Concept\nPrompting and Aggregating (CoPA), a novel framework designed to capture\nmultilayer concepts under prompt guidance. This framework utilizes the\nConcept-aware Embedding Generator (CEG) to extract concept representations from\neach layer of the visual encoder. Simultaneously, these representations serve\nas prompts for Concept Prompt Tuning (CPT), steering the model towards\namplifying critical concept-related visual cues. Visual representations from\neach layer are aggregated to align with textual concept representations. With\nthe proposed method, valuable concept-wise information in the images is\ncaptured and utilized effectively, thus improving the performance of concept\nand disease prediction. Extensive experimental results demonstrate that CoPA\noutperforms state-of-the-art methods on three public datasets. Code is\navailable at https://github.com/yihengd/CoPA.", "AI": {"tldr": "CoPA\u6846\u67b6\u901a\u8fc7\u6982\u5ff5\u63d0\u793a\u548c\u805a\u5408\u673a\u5236\uff0c\u4ece\u89c6\u89c9\u7f16\u7801\u5668\u7684\u591a\u5c42\u63d0\u53d6\u6982\u5ff5\u8868\u793a\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u4f5c\u4e3a\u63d0\u793a\u6765\u589e\u5f3a\u5173\u952e\u6982\u5ff5\u76f8\u5173\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ece\u800c\u63d0\u5347\u6982\u5ff5\u548c\u75be\u75c5\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u5728\u6982\u5ff5\u6355\u83b7\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u4ec5\u4f9d\u8d56\u6700\u7ec8\u5c42\u7279\u5f81\u800c\u5ffd\u89c6\u6d45\u5c42\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u6982\u5ff5\u7f16\u7801\u6307\u5bfc\uff0c\u963b\u788d\u4e86\u7ec6\u7c92\u5ea6\u6982\u5ff5\u63d0\u53d6\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u63d0\u793a\u548c\u805a\u5408\u6846\u67b6\uff0c\u5305\u542b\u6982\u5ff5\u611f\u77e5\u5d4c\u5165\u751f\u6210\u5668\u4ece\u89c6\u89c9\u7f16\u7801\u5668\u5404\u5c42\u63d0\u53d6\u6982\u5ff5\u8868\u793a\uff0c\u4ee5\u53ca\u6982\u5ff5\u63d0\u793a\u8c03\u4f18\u673a\u5236\u6765\u653e\u5927\u5173\u952e\u6982\u5ff5\u76f8\u5173\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u6700\u540e\u805a\u5408\u5404\u5c42\u89c6\u89c9\u8868\u793a\u4e0e\u6587\u672c\u6982\u5ff5\u8868\u793a\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoPA\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u83b7\u548c\u5229\u7528\u4e86\u56fe\u50cf\u4e2d\u7684\u6982\u5ff5\u4fe1\u606f\u3002", "conclusion": "CoPA\u6846\u67b6\u901a\u8fc7\u591a\u5c42\u6982\u5ff5\u63d0\u53d6\u548c\u63d0\u793a\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u6355\u83b7\u80fd\u529b\u548c\u75be\u75c5\u8bca\u65ad\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2510.04230", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04230", "abs": "https://arxiv.org/abs/2510.04230", "authors": ["Guijin Son", "Donghun Yang", "Hitesh Laxmichand Patel", "Amit Agarwal", "Hyunwoo Ko", "Chanuk Lim", "Srikant Panda", "Minhyuk Kim", "Nikunj Drolia", "Dasol Choi", "Kyong-Ha Lee", "Youngjae Yu"], "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought", "comment": "Work in Progress", "summary": "Recent frontier models employ long chain-of-thought reasoning to explore\nsolution spaces in context and achieve stonger performance. While many works\nstudy distillation to build smaller yet capable models, most focus on English\nand little is known about language-specific reasoning. To bridge this gap, we\nfirst introduct **Language-Mixed CoT**, a reasoning schema that switches\nbetween English and a target language, using English as an anchor to excel in\nreasoning while minimizing translation artificats. As a Korean case study, we\ncurate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and\ncode; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k\nhigh-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5,\nLlama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves\nstate-of-the-art performance, with the highest overall average score (64.0 \\pm\n25), ranking first on 5/9 benchmarks and second on the remainder. Samller and\nmid-sized models also benefit substantially, with an average improvement of\n+18.6 points across teh evaluated nine benchmarks. Ablations show\n**Language-Mixed CoT** is more effective than monolingual CoT, also resulting\nin cross-lingual and mult-modal performance gains. We release our data-curation\npipeline, evaluation system, datasets, and models to advance research on\nlanguage-specific reasoning. Data and model collection:\nhttps://huggingface.co/KOREAson.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u8bed\u8a00\u6df7\u5408\u601d\u7ef4\u94fe\uff08Language-Mixed CoT\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e2d\u82f1\u6df7\u5408\u63a8\u7406\u63d0\u5347\u97e9\u8bed\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u97e9\u8bed\u6848\u4f8b\u7814\u7a76\u4e2d\u53d6\u5f97\u4e86SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u63a8\u7406\uff0c\u5bf9\u8bed\u8a00\u7279\u5b9a\u63a8\u7406\u4e86\u89e3\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6df7\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5f0f\uff0c\u5728\u82f1\u8bed\u548c\u76ee\u6807\u8bed\u8a00\u4e4b\u95f4\u5207\u6362\uff0c\u5229\u7528\u82f1\u8bed\u4f5c\u4e3a\u951a\u70b9\u8fdb\u884c\u63a8\u7406\u3002\u6784\u5efa\u4e86\u5305\u542b579\u4e07\u97e9\u8bed\u63d0\u793a\u548c370\u4e07\u957f\u63a8\u7406\u8f68\u8ff9\u7684Yi-Sang\u6570\u636e\u96c6\u3002", "result": "\u6700\u4f73\u6a21\u578bKO-REAson-35B\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u9ad8\u5e73\u5747\u520664.0\uff0c\u57285/9\u4e2a\u57fa\u51c6\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5176\u4f59\u6392\u540d\u7b2c\u4e8c\u3002\u4e2d\u5c0f\u578b\u6a21\u578b\u5e73\u5747\u63d0\u534718.6\u5206\u3002", "conclusion": "\u8bed\u8a00\u6df7\u5408CoT\u6bd4\u5355\u8bedCoT\u66f4\u6709\u6548\uff0c\u8fd8\u80fd\u5e26\u6765\u8de8\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.03769", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.03769", "abs": "https://arxiv.org/abs/2510.03769", "authors": ["Shimaa Elbana", "Ahmad Kamal", "Shahd Ahmed Ali", "Ahmad Al-Kabbany"], "title": "Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation", "comment": null, "summary": "The increasing size and complexity of medical imaging datasets, particularly\nin 3D formats, present significant barriers to collaborative research and\ntransferability. This study investigates whether the ZFP compression technique\ncan mitigate these challenges without compromising the performance of automated\ncerebrovascular segmentation, a critical first step in intracranial aneurysm\ndetection. We apply ZFP in both its error tolerance and fixed-rate modes to a\nlarge scale, and one of the most recent, datasets in the literature, 3D medical\ndataset containing ground-truth vascular segmentations. The segmentation\nquality on the compressed volumes is rigorously compared to the uncompressed\nbaseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can\nachieve substantial data reduction--up to a 22.89:1 ratio in error tolerance\nmode--while maintaining a high degree of fidelity, with the mean Dice\ncoefficient remaining high at 0.87656. These results demonstrate that ZFP is a\nviable and powerful tool for enabling more efficient and accessible research on\nlarge-scale medical datasets, fostering broader collaboration across the\ncommunity.", "AI": {"tldr": "ZFP\u538b\u7f29\u6280\u672f\u53ef\u4ee5\u5728\u4fdd\u6301\u8111\u8840\u7ba1\u5206\u5272\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c113D\u533b\u5b66\u5f71\u50cf\u6570\u636e\u7684\u5927\u5c0f\uff08\u6700\u9ad822.89:1\u538b\u7f29\u6bd4\uff09\uff0c\u4e3a\u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "3D\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\u65e5\u76ca\u589e\u957f\uff0c\u7ed9\u534f\u4f5c\u7814\u7a76\u548c\u6570\u636e\u8fc1\u79fb\u5e26\u6765\u91cd\u5927\u969c\u788d\uff0c\u9700\u8981\u5bfb\u627e\u4e0d\u635f\u5bb3\u5206\u5272\u6027\u80fd\u7684\u6570\u636e\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u5728\u5305\u542b\u771f\u5b9e\u8840\u7ba1\u5206\u5272\u6807\u6ce8\u7684\u5927\u89c4\u6a213D\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0c\u5e94\u7528ZFP\u538b\u7f29\u6280\u672f\u7684\u8bef\u5dee\u5bb9\u5fcd\u548c\u56fa\u5b9a\u901f\u7387\u4e24\u79cd\u6a21\u5f0f\uff0c\u5e76\u4e0e\u672a\u538b\u7f29\u57fa\u51c6\u8fdb\u884c\u5206\u5272\u8d28\u91cf\u5bf9\u6bd4\u3002", "result": "ZFP\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6570\u636e\u7f29\u51cf\uff08\u6700\u9ad822.89:1\u538b\u7f29\u6bd4\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5e73\u5747Dice\u7cfb\u6570\u4fdd\u6301\u57280.87656\uff08\u57fa\u51c6\u4e3a0.8774\uff09\u3002", "conclusion": "ZFP\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u4fc3\u8fdb\u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u7684\u9ad8\u6548\u548c\u53ef\u8bbf\u95ee\u6027\u7814\u7a76\uff0c\u63a8\u52a8\u66f4\u5e7f\u6cdb\u7684\u793e\u533a\u534f\u4f5c\u3002"}}
{"id": "2510.04268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04268", "abs": "https://arxiv.org/abs/2510.04268", "authors": ["Robin Algayres", "Charles-\u00c9ric Saint-James", "Mahi Luthra", "Jiayi Shen", "Dongyan Lin", "Youssef Benchekroun", "Rashel Moritz", "Juan Pino", "Emmanuel Dupoux"], "title": "LongTail-Swap: benchmarking language models' abilities on rare words", "comment": null, "summary": "Children learn to speak with a low amount of data and can be taught new words\non a few-shot basis, making them particularly data-efficient learners. The\nBabyLM challenge aims at exploring language model (LM) training in the low-data\nregime but uses metrics that concentrate on the head of the word distribution.\nHere, we introduce LongTail-Swap (LT-Swap), a benchmark that focuses on the\ntail of the distribution, i.e., measures the ability of LMs to learn new words\nwith very little exposure, like infants do. LT-Swap is a pretraining\ncorpus-specific test set of acceptable versus unacceptable sentence pairs that\nisolate semantic and syntactic usage of rare words. Models are evaluated in a\nzero-shot fashion by computing the average log probabilities over the two\nmembers of each pair. We built two such test sets associated with the 10M words\nand 100M words BabyLM training sets, respectively, and evaluated 16 models from\nthe BabyLM leaderboard. Our results not only highlight the poor performance of\nlanguage models on rare words but also reveal that performance differences\nacross LM architectures are much more pronounced in the long tail than in the\nhead. This offers new insights into which architectures are better at handling\nrare word generalization. We've also made the code publicly avail", "AI": {"tldr": "\u63d0\u51fa\u4e86LongTail-Swap\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u7f55\u89c1\u8bcd\u5b66\u4e60\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u67b6\u6784\u5728\u957f\u5c3e\u5206\u5e03\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u6bd4\u5934\u90e8\u66f4\u660e\u663e\u3002", "motivation": "\u73b0\u6709BabyLM\u6311\u6218\u4e3b\u8981\u5173\u6ce8\u8bcd\u6c47\u5206\u5e03\u7684\u5934\u90e8\uff0c\u800c\u513f\u7ae5\u5b66\u4e60\u8bed\u8a00\u65f6\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u7f55\u89c1\u8bcd\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8eBabyLM\u8bad\u7ec3\u96c6\u7684LT-Swap\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u53ef\u63a5\u53d7\u4e0e\u4e0d\u53ef\u63a5\u53d7\u53e5\u5b50\u5bf9\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u5f0f\u8bc4\u4f30\u6a21\u578b\u5bf9\u7f55\u89c1\u8bcd\u8bed\u4e49\u548c\u53e5\u6cd5\u4f7f\u7528\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u4e8616\u4e2aBabyLM\u6392\u884c\u699c\u6a21\u578b\uff0c\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u5728\u7f55\u89c1\u8bcd\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u4e0d\u540c\u67b6\u6784\u5728\u957f\u5c3e\u5206\u5e03\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u6bd4\u5934\u90e8\u66f4\u663e\u8457\u3002", "conclusion": "LT-Swap\u57fa\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u6765\u8bc4\u4f30\u54ea\u79cd\u67b6\u6784\u66f4\u9002\u5408\u5904\u7406\u7f55\u89c1\u8bcd\u6cdb\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u957f\u5c3e\u5206\u5e03\u5b66\u4e60\u80fd\u529b\u4e0a\u7684\u91cd\u8981\u5dee\u5f02\u3002"}}
{"id": "2510.03786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03786", "abs": "https://arxiv.org/abs/2510.03786", "authors": ["T-Mai Bui", "Fares Bougourzi", "Fadi Dornaika", "Vinh Truong Hoang"], "title": "MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation", "comment": null, "summary": "In recent years, deep learning has shown near-expert performance in\nsegmenting complex medical tissues and tumors. However, existing models are\noften task-specific, with performance varying across modalities and anatomical\nregions. Balancing model complexity and performance remains challenging,\nparticularly in clinical settings where both accuracy and efficiency are\ncritical. To address these issues, we propose a hybrid segmentation\narchitecture featuring a three-branch encoder that integrates CNNs,\nTransformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture\nlocal, global, and long-range dependencies. A multi-scale attention-based CNN\ndecoder reconstructs fine-grained segmentation maps while preserving contextual\nconsistency. Additionally, a co-attention gate enhances feature selection by\nemphasizing relevant spatial and semantic information across scales during both\nencoding and decoding, improving feature interaction and cross-scale\ncommunication. Extensive experiments on multiple benchmark datasets show that\nour approach outperforms state-of-the-art methods in accuracy and\ngeneralization, while maintaining comparable computational complexity. By\neffectively balancing efficiency and effectiveness, our architecture offers a\npractical and scalable solution for diverse medical imaging tasks. Source code\nand trained models will be publicly released upon acceptance to support\nreproducibility and further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5206\u5272\u67b6\u6784\uff0c\u96c6\u6210CNN\u3001Transformer\u548cMamba\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u4e09\u5206\u652f\u7f16\u7801\u5668\u6355\u83b7\u5c40\u90e8\u3001\u5168\u5c40\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529bCNN\u89e3\u7801\u5668\u91cd\u5efa\u7cbe\u7ec6\u5206\u5272\u56fe\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6a21\u578b\u591a\u4e3a\u4efb\u52a1\u7279\u5b9a\uff0c\u6027\u80fd\u5728\u4e0d\u540c\u6a21\u6001\u548c\u89e3\u5256\u533a\u57df\u95f4\u5dee\u5f02\u5927\uff0c\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6027\u80fd\u5e73\u8861\u56f0\u96be\uff0c\u4e34\u5e8a\u73af\u5883\u4e2d\u51c6\u786e\u6027\u548c\u6548\u7387\u90fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4e09\u5206\u652f\u7f16\u7801\u5668\u96c6\u6210CNN\u3001Transformer\u548cMamba\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\uff0c\u591a\u5c3a\u5ea6\u6ce8\u610f\u529bCNN\u89e3\u7801\u5668\uff0c\u534f\u540c\u6ce8\u610f\u529b\u95e8\u589e\u5f3a\u7279\u5f81\u9009\u62e9\uff0c\u6539\u5584\u7279\u5f81\u4ea4\u4e92\u548c\u8de8\u5c3a\u5ea6\u901a\u4fe1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u6709\u6548\u5e73\u8861\u6548\u7387\u548c\u6548\u679c\uff0c\u8be5\u67b6\u6784\u4e3a\u591a\u6837\u5316\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04285", "categories": ["cs.CL", "cond-mat.stat-mech", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.04285", "abs": "https://arxiv.org/abs/2510.04285", "authors": ["Karthik Viswanathan", "Sang Eon Park"], "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy", "comment": "14 pages, 7 figures. Poster at HiLD 2025: 3rd Workshop on\n  High-dimensional Learning Dynamics", "summary": "We introduce a cumulant-expansion framework for quantifying how large\nlanguage models (LLMs) internalize higher-order statistical structure during\nnext-token prediction. By treating the softmax entropy of each layer's logit\ndistribution as a perturbation around its \"center\" distribution, we derive\nclosed-form cumulant observables that isolate successively higher-order\ncorrelations. Empirically, we track these cumulants in GPT-2 and Pythia models\non Pile-10K prompts. (i) Structured prompts exhibit a characteristic\nrise-and-plateau profile across layers, whereas token-shuffled prompts remain\nflat, revealing the dependence of the cumulant profile on meaningful context.\n(ii) During training, all cumulants increase monotonically before saturating,\ndirectly visualizing the model's progression from capturing variance to\nlearning skew, kurtosis, and higher-order statistical structures. (iii)\nMathematical prompts show distinct cumulant signatures compared to general\ntext, quantifying how models employ fundamentally different processing\nmechanisms for mathematical versus linguistic content. Together, these results\nestablish cumulant analysis as a lightweight, mathematically grounded probe of\nfeature-learning dynamics in high-dimensional neural networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7d2f\u79ef\u91cf\u5c55\u5f00\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5185\u5316\u9ad8\u9636\u7edf\u8ba1\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u6790GPT-2\u548cPythia\u6a21\u578b\u63ed\u793a\u4e86\u4e0d\u540c\u6587\u672c\u7c7b\u578b\u7684\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u3002", "motivation": "\u9700\u8981\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u548c\u5185\u5316\u9ad8\u9636\u7edf\u8ba1\u7ed3\u6784\uff0c\u7406\u89e3\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u6587\u672c\u5904\u7406\u4e2d\u7684\u7279\u5f81\u5b66\u4e60\u673a\u5236\u3002", "method": "\u5c06\u6bcf\u5c42logit\u5206\u5e03\u7684softmax\u71b5\u89c6\u4e3a\u56f4\u7ed5\u5176\"\u4e2d\u5fc3\"\u5206\u5e03\u7684\u6270\u52a8\uff0c\u63a8\u5bfc\u51fa\u5c01\u95ed\u5f62\u5f0f\u7684\u7d2f\u79ef\u91cf\u89c2\u6d4b\u503c\u6765\u9694\u79bb\u9010\u6b21\u9ad8\u9636\u76f8\u5173\u6027\uff0c\u5e76\u5728GPT-2\u548cPythia\u6a21\u578b\u4e0a\u5b9e\u8bc1\u8ffd\u8e2a\u8fd9\u4e9b\u7d2f\u79ef\u91cf\u3002", "result": "(i)\u7ed3\u6784\u5316\u63d0\u793a\u5728\u5c42\u95f4\u5448\u73b0\u4e0a\u5347-\u5e73\u53f0\u7279\u5f81\uff0c\u800ctoken\u6253\u4e71\u7684\u63d0\u793a\u4fdd\u6301\u5e73\u5766\uff1b(ii)\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6240\u6709\u7d2f\u79ef\u91cf\u5355\u8c03\u589e\u52a0\u540e\u9971\u548c\uff1b(iii)\u6570\u5b66\u63d0\u793a\u4e0e\u4e00\u822c\u6587\u672c\u663e\u793a\u4e0d\u540c\u7684\u7d2f\u79ef\u91cf\u7279\u5f81\u3002", "conclusion": "\u7d2f\u79ef\u91cf\u5206\u6790\u6210\u4e3a\u9ad8\u7ef4\u795e\u7ecf\u7f51\u7edc\u4e2d\u7279\u5f81\u5b66\u4e60\u52a8\u6001\u7684\u8f7b\u91cf\u7ea7\u3001\u6570\u5b66\u57fa\u7840\u624e\u5b9e\u7684\u63a2\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.03797", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03797", "abs": "https://arxiv.org/abs/2510.03797", "authors": ["Rasel Hossen", "Diptajoy Mistry", "Mushiur Rahman", "Waki As Sami Atikur Rahman Hridoy", "Sajib Saha", "Muhammad Ibrahim"], "title": "Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach", "comment": "13 pages", "summary": "Urban safety and infrastructure maintenance are critical components of smart\ncity development. Manual monitoring of road damages is time-consuming, highly\ncostly, and error-prone. This paper presents a deep learning approach for\nautomated road damage and manhole detection using the YOLOv9 algorithm with\npolygonal annotations. Unlike traditional bounding box annotation, we employ\npolygonal annotations for more precise localization of road defects. We develop\na novel dataset comprising more than one thousand images which are mostly\ncollected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based\nmodel for three classes, namely Broken, Not Broken, and Manhole. We achieve\n78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong\nperformance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)\nclasses, with challenges in Manhole detection (18.2% F1-score) due to class\nimbalance. Our approach offers an efficient and scalable solution for\nmonitoring urban infrastructure in developing countries.", "AI": {"tldr": "\u4f7f\u7528YOLOv9\u7b97\u6cd5\u548c\u591a\u8fb9\u5f62\u6807\u6ce8\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u81ea\u52a8\u68c0\u6d4b\u9053\u8def\u635f\u574f\u548c\u4e95\u76d6\uff0c\u5728\u5b5f\u52a0\u62c9\u56fd\u8fbe\u5361\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u6574\u4f53\u51c6\u786e\u738778.1%\uff0c\u4f46\u5728\u4e95\u76d6\u68c0\u6d4b\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u57ce\u5e02\u5b89\u5168\u548c\u57fa\u7840\u8bbe\u65bd\u7ef4\u62a4\u662f\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u624b\u52a8\u76d1\u6d4b\u9053\u8def\u635f\u574f\u8017\u65f6\u3001\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u91c7\u7528YOLOv9\u7b97\u6cd5\u548c\u66f4\u7cbe\u786e\u7684\u591a\u8fb9\u5f62\u6807\u6ce8\uff08\u800c\u975e\u4f20\u7edf\u8fb9\u754c\u6846\uff09\u6765\u5b9a\u4f4d\u9053\u8def\u7f3a\u9677\uff0c\u6784\u5efa\u5305\u542b\u4e00\u5343\u591a\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e09\u7c7b\u68c0\u6d4b\u6a21\u578b\uff1a\u635f\u574f\u3001\u672a\u635f\u574f\u548c\u4e95\u76d6\u3002", "result": "\u6574\u4f53\u56fe\u50cf\u7ea7\u51c6\u786e\u738778.1%\uff0c\u635f\u574f\u7c7bF1\u5206\u657086.7%\uff0c\u672a\u635f\u574f\u7c7b89.2%\uff0c\u4f46\u4e95\u76d6\u68c0\u6d4b\u4ec518.2% F1\u5206\u6570\uff0c\u4e3b\u8981\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04286", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04286", "abs": "https://arxiv.org/abs/2510.04286", "authors": ["Harshil Vejendla"], "title": "SliceMoE: Routing Embedding Slices Instead of Tokens for Fine-Grained and Balanced Transformer Scaling", "comment": "EMNLP 2025 Main, 8 pages, 9 figures", "summary": "Mixture-of-Experts (MoE) layers scale transformers by routing tokens to a\nsparse subset of feed-forward experts. Token-level routing, however, assigns an\nentire semantic spectrum to each expert, creating capacity bottlenecks,\nload-balancing pathologies, and limited specialization. We introduce SliceMoE,\nan architecture that routes contiguous slices of a token's hidden vector. A\nd-dimensional embedding is partitioned into S slices, and for each slice, a\nlightweight shared router predicts the top-k experts. Experts operate on their\nassigned slices independently, and outputs are reassembled, maintaining\nper-token FLOP efficiency. Because slices from different tokens interleave\nwithin an expert, utilization is naturally smoother. We propose a slice-level\ncapacity loss, cross-slice dropout, and efficient fused batched GEMM kernels.\nExperiments on WikiText-103 language modeling, WMT En-De translation, and three\ntext-classification datasets show SliceMoE attains up to 1.7x faster inference\nthan dense baselines, 12 to 18 percent lower perplexity than parameter-matched\ntoken-MoE, and improved expert balance, with interpretable expertise over\nsyntactic versus semantic subspaces.", "AI": {"tldr": "SliceMoE\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5c06token\u7684\u9690\u85cf\u5411\u91cf\u5206\u5272\u6210\u591a\u4e2a\u5207\u7247\uff0c\u6bcf\u4e2a\u5207\u7247\u72ec\u7acb\u8def\u7531\u5230\u4e13\u5bb6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edftoken\u7ea7\u8def\u7531\u7684\u5bb9\u91cf\u74f6\u9888\u3001\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\u548c\u6709\u9650\u4e13\u4e1a\u5316\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfMoE\u5c42\u7684token\u7ea7\u8def\u7531\u5c06\u6574\u4e2a\u8bed\u4e49\u8c31\u5206\u914d\u7ed9\u6bcf\u4e2a\u4e13\u5bb6\uff0c\u5bfc\u81f4\u5bb9\u91cf\u74f6\u9888\u3001\u8d1f\u8f7d\u5747\u8861\u75c5\u7406\u548c\u6709\u9650\u7684\u4e13\u4e1a\u5316\u80fd\u529b\u3002", "method": "\u5c06d\u7ef4\u5d4c\u5165\u5206\u5272\u4e3aS\u4e2a\u5207\u7247\uff0c\u6bcf\u4e2a\u5207\u7247\u7531\u8f7b\u91cf\u7ea7\u5171\u4eab\u8def\u7531\u5668\u9884\u6d4btop-k\u4e13\u5bb6\u3002\u4e13\u5bb6\u72ec\u7acb\u5904\u7406\u5206\u914d\u7684\u5207\u7247\uff0c\u8f93\u51fa\u91cd\u65b0\u7ec4\u88c5\uff0c\u4fdd\u6301\u6bcf\u4e2atoken\u7684FLOP\u6548\u7387\u3002\u63d0\u51fa\u4e86\u5207\u7247\u7ea7\u5bb9\u91cf\u635f\u5931\u3001\u8de8\u5207\u7247dropout\u548c\u9ad8\u6548\u878d\u5408\u6279\u5904\u7406GEMM\u5185\u6838\u3002", "result": "\u5728WikiText-103\u8bed\u8a00\u5efa\u6a21\u3001WMT En-De\u7ffb\u8bd1\u548c\u4e09\u4e2a\u6587\u672c\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff0cSliceMoE\u6bd4\u5bc6\u96c6\u57fa\u7ebf\u63a8\u7406\u901f\u5ea6\u5feb1.7\u500d\uff0c\u6bd4\u53c2\u6570\u5339\u914d\u7684token-MoE\u56f0\u60d1\u5ea6\u964d\u4f4e12-18%\uff0c\u4e13\u5bb6\u5e73\u8861\u6027\u6539\u5584\uff0c\u5728\u53e5\u6cd5\u4e0e\u8bed\u4e49\u5b50\u7a7a\u95f4\u4e0a\u5177\u6709\u53ef\u89e3\u91ca\u7684\u4e13\u4e1a\u5316\u80fd\u529b\u3002", "conclusion": "SliceMoE\u901a\u8fc7\u5207\u7247\u7ea7\u8def\u7531\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u67b6\u6784\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002"}}
{"id": "2510.03821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03821", "abs": "https://arxiv.org/abs/2510.03821", "authors": ["Venkata Narendra Kotyada", "Revanth Eranki", "Nagesh Bhattu Sristy"], "title": "Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation", "comment": "9 pages, 3 figures", "summary": "Unpaired image-to-image translation involves learning mappings between source\ndomain and target domain in the absence of aligned or corresponding samples.\nScore based diffusion models have demonstrated state-of-the-art performance in\ngenerative tasks. Their ability to approximate complex data distributions\nthrough stochastic differential equations (SDEs) enables them to generate\nhigh-fidelity and diverse outputs, making them particularly well-suited for\nunpaired I2I settings. In parallel, contrastive learning provides a powerful\nframework for learning semantic similarities without the need for explicit\nsupervision or paired data. By pulling together representations of semantically\nsimilar samples and pushing apart dissimilar ones, contrastive methods are\ninherently aligned with the objectives of unpaired translation. Its ability to\nselectively enforce semantic consistency at the feature level makes contrastive\nlearning particularly effective for guiding generation in unpaired scenarios.\nIn this work, we propose a time-dependent contrastive learning approach where a\nmodel is trained with SimCLR by considering an image and its domain invarient\nfeature as a positive pair, enabling the preservation of domain-invariant\nfeatures and the discarding of domain-specific ones. The learned contrastive\nmodel then guides the inference of a pretrained SDE for the I2I translation\ntask. We empirically compare Contrastive-SDE with several baselines across\nthree common unpaired I2I tasks, using four metrics for evaluation.\nConstrastive-SDE achieves comparable results to the state-of-the-art on several\nmetrics. Furthermore, we observe that our model converges significantly faster\nand requires no label supervision or classifier training, making it a more\nefficient alternative for this task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7684\u975e\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u7684\u5bf9\u6bd4\u5b66\u4e60\u4fdd\u7559\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u6307\u5bfc\u9884\u8bad\u7ec3SDE\u8fdb\u884c\u56fe\u50cf\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3\u975e\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u4e2d\u7f3a\u4e4f\u5bf9\u9f50\u6837\u672c\u7684\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5904\u7406\u590d\u6742\u6570\u636e\u5206\u5e03\u7684\u80fd\u529b\u548c\u5bf9\u6bd4\u5b66\u4e60\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528SimCLR\u8bad\u7ec3\u65f6\u95f4\u76f8\u5173\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u56fe\u50cf\u4e0e\u5176\u9886\u57df\u4e0d\u53d8\u7279\u5f81\u4f5c\u4e3a\u6b63\u6837\u672c\u5bf9\uff0c\u7136\u540e\u5229\u7528\u5b66\u4e60\u5230\u7684\u5bf9\u6bd4\u6a21\u578b\u6307\u5bfc\u9884\u8bad\u7ec3SDE\u8fdb\u884c\u56fe\u50cf\u8f6c\u6362\u3002", "result": "\u5728\u4e09\u4e2a\u5e38\u89c1\u975e\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u4efb\u52a1\u4e0a\u4e0e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4f7f\u7528\u56db\u4e2a\u8bc4\u4f30\u6307\u6807\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3001\u65e0\u9700\u6807\u7b7e\u76d1\u7763\u6216\u5206\u7c7b\u5668\u8bad\u7ec3\u3002", "conclusion": "Contrastive-SDE\u662f\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u975e\u914d\u5bf9\u56fe\u50cf\u8f6c\u6362\u66ff\u4ee3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2510.04291", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04291", "abs": "https://arxiv.org/abs/2510.04291", "authors": ["Mehrzad Tareh", "Aydin Mohandesi", "Ebrahim Ansari"], "title": "PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis", "comment": "8 pages", "summary": "Sentiment analysis is a key task in Natural Language Processing (NLP),\nenabling the extraction of meaningful insights from user opinions across\nvarious domains. However, performing sentiment analysis in Persian remains\nchallenging due to the scarcity of labeled datasets, limited preprocessing\ntools, and the lack of high-quality embeddings and feature extraction methods.\nTo address these limitations, we propose a hybrid approach that integrates\nmachine learning (ML) and deep learning (DL) techniques for Persian\naspect-based sentiment analysis (ABSA). In particular, we utilize polarity\nscores from multilingual BERT as additional features and incorporate them into\na decision tree classifier, achieving an accuracy of 93.34%-surpassing existing\nbenchmarks on the Pars-ABSA dataset. Additionally, we introduce a Persian\nsynonym and entity dictionary, a novel linguistic resource that supports text\naugmentation through synonym and named entity replacement. Our results\ndemonstrate the effectiveness of hybrid modeling and feature augmentation in\nadvancing sentiment analysis for low-resource languages such as Persian.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6df7\u5408\u65b9\u6cd5\u7528\u4e8e\u6ce2\u65af\u8bed\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff0c\u901a\u8fc7\u6574\u5408\u591a\u8bed\u8a00BERT\u7684\u6781\u6027\u5206\u6570\u4f5c\u4e3a\u7279\u5f81\uff0c\u5728Pars-ABSA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8693.34%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u6ce2\u65af\u8bed\u60c5\u611f\u5206\u6790\u9762\u4e34\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u9884\u5904\u7406\u5de5\u5177\u6709\u9650\u4ee5\u53ca\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5d4c\u5165\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u6765\u63a8\u8fdb\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u6574\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528\u591a\u8bed\u8a00BERT\u7684\u6781\u6027\u5206\u6570\u4f5c\u4e3a\u989d\u5916\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6ce2\u65af\u8bed\u540c\u4e49\u8bcd\u548c\u5b9e\u4f53\u8bcd\u5178\u8fdb\u884c\u6587\u672c\u589e\u5f3a\u3002", "result": "\u5728Pars-ABSA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8693.34%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u5efa\u6a21\u548c\u7279\u5f81\u589e\u5f3a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6df7\u5408\u5efa\u6a21\u548c\u7279\u5f81\u589e\u5f3a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63a8\u8fdb\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u7814\u7a76\uff0c\u4e3a\u7c7b\u4f3c\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03827", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03827", "abs": "https://arxiv.org/abs/2510.03827", "authors": ["Xueyang Zhou", "Yangming Xu", "Guiyao Tie", "Yongchao Chen", "Guowen Zhang", "Duanfeng Chu", "Pan Zhou", "Lichao Sun"], "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization", "comment": "12 pages,7 figures, 5 tables", "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.", "AI": {"tldr": "LIBERO-PRO\u662f\u4e00\u4e2a\u6269\u5c55\u7684VLA\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u56db\u79cd\u6270\u52a8\u7ef4\u5ea6\uff08\u64cd\u7eb5\u5bf9\u8c61\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\uff09\u7cfb\u7edf\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5728\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u6027\u80fd\u5d29\u6e83\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7406\u89e3\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLIBERO\u57fa\u51c6\u6d4b\u8bd5\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8bbe\u7f6e\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u4f30\u8ba1\u8fc7\u9ad8\u4e14\u65e0\u6cd5\u516c\u5e73\u6bd4\u8f83\u6a21\u578b\u3002\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u771f\u5b9e\u7406\u89e3\u6c34\u5e73\u3002", "method": "\u5f15\u5165LIBERO-PRO\u57fa\u51c6\uff0c\u901a\u8fc7\u56db\u79cd\u6270\u52a8\u7ef4\u5ea6\uff08\u64cd\u7eb5\u5bf9\u8c61\u3001\u521d\u59cb\u72b6\u6001\u3001\u4efb\u52a1\u6307\u4ee4\u548c\u73af\u5883\uff09\u5bf9\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u89c2\u5bdf\u6a21\u578b\u5728\u5408\u7406\u6270\u52a8\u4e0b\u7684\u8868\u73b0\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u6807\u51c6LIBERO\u8bc4\u4f30\u4e2d\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u4f46\u5728\u6cdb\u5316\u8bbe\u7f6e\u4e0b\u6027\u80fd\u5d29\u6e83\u81f30.0%\u3002\u6a21\u578b\u8868\u73b0\u51fa\u5bf9\u8bad\u7ec3\u96c6\u4e2d\u52a8\u4f5c\u5e8f\u5217\u548c\u73af\u5883\u5e03\u5c40\u7684\u6b7b\u8bb0\u786c\u80cc\uff0c\u800c\u975e\u771f\u6b63\u7684\u4efb\u52a1\u7406\u89e3\u3002", "conclusion": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u6a21\u578b\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u7406\u89e3\u3002\u547c\u5401\u793e\u533a\u653e\u5f03\u8bef\u5bfc\u6027\u65b9\u6cd5\uff0c\u91c7\u7528\u80fd\u7a33\u5065\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u548c\u7406\u89e3\u80fd\u529b\u7684\u6d4b\u8bd5\u6807\u51c6\u3002"}}
{"id": "2510.04293", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04293", "abs": "https://arxiv.org/abs/2510.04293", "authors": ["Lingnan Xu", "Chong Feng", "Kaiyuan Zhang", "Liu Zhengyong", "Wenqiang Xu", "Fanqing Meng"], "title": "Equipping Retrieval-Augmented Large Language Models with Document Structure Awareness", "comment": "EMNLP2025 Findings", "summary": "While large language models (LLMs) demonstrate impressive capabilities, their\nreliance on parametric knowledge often leads to factual inaccuracies.\nRetrieval-Augmented Generation (RAG) mitigates this by leveraging external\ndocuments, yet existing approaches treat retrieved passages as isolated chunks,\nignoring valuable structure that is crucial for document organization.\nMotivated by this gap, we propose Retrieve-DocumentRoute-Read (RDR2), a novel\nframework that explicitly incorporates structural information throughout the\nRAG process. RDR2 employs an LLM-based router to dynamically navigate document\nstructure trees, jointly evaluating content relevance and hierarchical\nrelationships to assemble optimal evidence. Our key innovation lies in\nformulating document routing as a trainable task, with automatic action\ncuration and structure-aware passage selection inspired by human reading\nstrategies. Through comprehensive evaluation on five challenging datasets, RDR2\nachieves state-of-the-art performance, demonstrating that explicit structural\nawareness significantly enhances RAG systems' ability to acquire and utilize\nknowledge, particularly in complex scenarios requiring multi-document\nsynthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86RDR2\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u6587\u6863\u7ed3\u6784\u4fe1\u606f\u6765\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8def\u7531\u5668\u52a8\u6001\u5bfc\u822a\u6587\u6863\u7ed3\u6784\u6811\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5c06\u68c0\u7d22\u5230\u7684\u6bb5\u843d\u89c6\u4e3a\u5b64\u7acb\u5757\uff0c\u5ffd\u7565\u4e86\u6587\u6863\u7ec4\u7ec7\u7ed3\u6784\u8fd9\u4e00\u5173\u952e\u4fe1\u606f\uff0c\u5bfc\u81f4\u77e5\u8bc6\u83b7\u53d6\u548c\u5229\u7528\u6548\u7387\u4e0d\u8db3\u3002", "method": "RDR2\u91c7\u7528\u57fa\u4e8eLLM\u7684\u8def\u7531\u5668\u52a8\u6001\u5bfc\u822a\u6587\u6863\u7ed3\u6784\u6811\uff0c\u8054\u5408\u8bc4\u4f30\u5185\u5bb9\u76f8\u5173\u6027\u548c\u5c42\u6b21\u5173\u7cfb\uff0c\u5c06\u6587\u6863\u8def\u7531\u5236\u5b9a\u4e3a\u53ef\u8bad\u7ec3\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u81ea\u52a8\u52a8\u4f5c\u7ba1\u7406\u548c\u7ed3\u6784\u611f\u77e5\u6bb5\u843d\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cRDR2\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u6587\u6863\u5408\u6210\u7684\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u6784\u611f\u77e5\u663e\u8457\u589e\u5f3a\u4e86RAG\u7cfb\u7edf\u83b7\u53d6\u548c\u5229\u7528\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6587\u6863\u7ed3\u6784\u4fe1\u606f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.03840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03840", "abs": "https://arxiv.org/abs/2510.03840", "authors": ["Pranav Sharma", "Shivank Garg", "Durga Toshniwal"], "title": "Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models", "comment": "ACM MM'25, MALLM Workshop", "summary": "Recent advances in image generation models have led to models that produce\nsynthetic images that are increasingly difficult for standard AI detectors to\nidentify, even though they often remain distinguishable by humans. To identify\nthis discrepancy, we introduce \\textbf{Mirage}, a curated dataset comprising a\ndiverse range of AI-generated images exhibiting visible artifacts, where\ncurrent state-of-the-art detection methods largely fail. Furthermore, we\ninvestigate whether Large Vision-Language Models (LVLMs), which are\nincreasingly employed as substitutes for human judgment in various tasks, can\nbe leveraged for explainable AI image detection. Our experiments on both Mirage\nand existing benchmark datasets demonstrate that while LVLMs are highly\neffective at detecting AI-generated images with visible artifacts, their\nperformance declines when confronted with images lacking such cues.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Mirage\u6570\u636e\u96c6\uff0c\u5305\u542b\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u7814\u7a76\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u89e3\u91caAI\u56fe\u50cf\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u96be\u4ee5\u88ab\u6807\u51c6\u68c0\u6d4b\u5668\u8bc6\u522b\uff0c\u4f46\u4eba\u7c7b\u4ecd\u80fd\u8fa8\u522b\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u68c0\u6d4b\u5dee\u8ddd\uff0c\u5e76\u63a2\u7d22LVLMs\u4f5c\u4e3a\u4eba\u7c7b\u5224\u65ad\u66ff\u4ee3\u54c1\u5728\u53ef\u89e3\u91ca\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u591a\u6837\u5316AI\u751f\u6210\u56fe\u50cf\u7684Mirage\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u4f46\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5931\u8d25\u3002\u7136\u540e\u8bc4\u4f30LVLMs\u5728\u8be5\u6570\u636e\u96c6\u548c\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "result": "LVLMs\u5728\u68c0\u6d4b\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u7684AI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u4f46\u5728\u9762\u5bf9\u7f3a\u4e4f\u6b64\u7c7b\u7ebf\u7d22\u7684\u56fe\u50cf\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "LVLMs\u5728\u53ef\u89e3\u91caAI\u56fe\u50cf\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u53ef\u89c1\u4f2a\u5f71\u7684\u56fe\u50cf\u65f6\uff0c\u4f46\u5bf9\u4e8e\u66f4\u903c\u771f\u7684\u751f\u6210\u56fe\u50cf\u4ecd\u9700\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2510.04302", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04302", "abs": "https://arxiv.org/abs/2510.04302", "authors": ["Thomas F Burns"], "title": "Measuring Language Model Hallucinations Through Distributional Correctness", "comment": "23 pages, 2 figures", "summary": "Common evaluation paradigms for language models focus on scoring single\nresponses through accuracy metrics or proper scoring rules, failing to capture\nthe full richness of a model's belief state. Recent work illustrates that\nlanguage models hallucinate in-part because they are optimised to be good\ntest-takers under binary scoring schemes that reward any answer over\nabstention. While this insight naturally leads to penalty-based approaches,\nthey ignore crucial distinctions in how models distribute uncertainty, for\nexample between hedging toward incorrect answers versus hedging toward \"I don't\nknow\" responses. A novel evaluation metric, the Distributional Correctness\nScore (DCS), is introduced to solve this problem, i.e., of not considering a\nmodel's entire probability distribution over answer choices. DCS naturally\ndistinguishes between harmful overconfidence in wrong answers and uncertainty\nexpressed through abstention, providing scores in an interpretable default\nrange. Through theoretical analysis and illustrative examples, DCS is\ndemonstrated to offer a more nuanced and aligned evaluation paradigm that\nincentivises models to express genuine uncertainty rather than guessing.\nAdapting 12 existing evaluation benchmarks to DCS's variants and measuring\nperformance on six language models reveals that for half of the tested\nbenchmarks scores are negative across all tested models, indicating significant\ntendencies towards hallucination.", "AI": {"tldr": "\u63d0\u51fa\u4e86Distributional Correctness Score (DCS)\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u8003\u8651\u6a21\u578b\u5728\u7b54\u6848\u9009\u62e9\u4e0a\u7684\u5b8c\u6574\u6982\u7387\u5206\u5e03\u6765\u533a\u5206\u6709\u5bb3\u7684\u8fc7\u5ea6\u81ea\u4fe1\u548c\u901a\u8fc7\u5f03\u6743\u8868\u8fbe\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8303\u5f0f\u53ea\u5173\u6ce8\u5355\u4e00\u54cd\u5e94\u7684\u51c6\u786e\u6027\u8bc4\u5206\uff0c\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u7684\u5b8c\u6574\u4fe1\u5ff5\u72b6\u6001\uff0c\u5bfc\u81f4\u6a21\u578b\u56e0\u4f18\u5316\u4e8c\u5143\u8bc4\u5206\u65b9\u6848\u800c\u4ea7\u751f\u5e7b\u89c9\u3002", "method": "\u5f15\u5165DCS\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u793a\u4f8b\u6f14\u793a\uff0c\u5c0612\u4e2a\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u9002\u914d\u5230DCS\u53d8\u4f53\uff0c\u5e76\u57286\u4e2a\u8bed\u8a00\u6a21\u578b\u4e0a\u6d4b\u91cf\u6027\u80fd\u3002", "result": "\u5728\u6d4b\u8bd5\u7684\u57fa\u51c6\u4e2d\uff0c\u4e00\u534a\u7684\u57fa\u51c6\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e0a\u5f97\u5206\u5747\u4e3a\u8d1f\u503c\uff0c\u8868\u660e\u5b58\u5728\u663e\u8457\u7684\u5e7b\u89c9\u503e\u5411\u3002", "conclusion": "DCS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u7ec6\u81f4\u548c\u4e00\u81f4\u7684\u8bc4\u4ef7\u8303\u5f0f\uff0c\u6fc0\u52b1\u6a21\u578b\u8868\u8fbe\u771f\u6b63\u7684\u4e0d\u786e\u5b9a\u6027\u800c\u975e\u731c\u6d4b\u3002"}}
{"id": "2510.03853", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03853", "abs": "https://arxiv.org/abs/2510.03853", "authors": ["Rui Qian", "Xin Yin", "Chuanhang Deng", "Zhiyuan Peng", "Jian Xiong", "Wei Zhai", "Dejing Dou"], "title": "UGround: Towards Unified Visual Grounding with Unrolled Transformers", "comment": "https://github.com/rui-qian/UGround", "summary": "We present UGround, a \\textbf{U}nified visual \\textbf{Ground}ing paradigm\nthat dynamically selects intermediate layers across \\textbf{U}nrolled\ntransformers as ``mask as prompt'', diverging from the prevailing pipeline that\nleverages the fixed last hidden layer as ``\\texttt{<SEG>} as prompt''. UGround\naddresses two primary challenges posed by the prevailing paradigm: (1) its\nreliance on the fixed last hidden layer, which sequentially amplifies\ncumulative errors arising from layer-by-layer propagation without intermediate\ncorrection, and (2) its use of \\texttt{<SEG>} as a prompt, which implicitly\nprojects textual embeddings into visual space without explicit spatial cues\n(\\eg, coordinates). Central to UGround is Policy-Prompted Masking, which\ncomprises two key components: Stochastic Skip Connection (SSC) and Mask as\nPrompt (MasP). SSC is a reinforcement learning policy that, via stochastic\nsampling, allows each \\texttt{<SEG>} token to slide across unrolled transformer\nlayers, enabling dynamic layer selection at which it connects to the vision\nmodel (\\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,\nMasP uses the similarity map derived from the \\texttt{<SEG>} token and image\ntokens as a soft logit mask to prompt SAM for mask generation, offering\nexplicit spatial cues through its activation regions. To validate the\neffectiveness of UGround, we, for the first time, have unified visual grounding\nwithin a single framework from an attribute perspective, spanning from\ntraditional refer expression segmentation to newly proposed reasoning\nsegmentation, single-target to multi-target, positive query to false premise\n(empty target). All codes and models are publicly available at\n\\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.", "AI": {"tldr": "UGround\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c6\u89c9\u5b9a\u4f4d\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9Transformer\u4e2d\u95f4\u5c42\u4f5c\u4e3a\u201c\u63a9\u7801\u63d0\u793a\u201d\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6700\u540e\u4e00\u5c42\u548c\u4f7f\u7528<SEG>\u4f5c\u4e3a\u63d0\u793a\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u56fa\u5b9a\u7684\u6700\u540e\u4e00\u9690\u85cf\u5c42\uff0c\u5bfc\u81f4\u8bef\u5dee\u9010\u5c42\u7d2f\u79ef\u800c\u65e0\u6cd5\u4e2d\u95f4\u4fee\u6b63\uff1b2\uff09\u4f7f\u7528<SEG>\u4f5c\u4e3a\u63d0\u793a\uff0c\u5c06\u6587\u672c\u5d4c\u5165\u9690\u5f0f\u6295\u5f71\u5230\u89c6\u89c9\u7a7a\u95f4\u800c\u7f3a\u4e4f\u660e\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u3002", "method": "\u91c7\u7528Policy-Prompted Masking\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u968f\u673a\u8df3\u8dc3\u8fde\u63a5(SSC)\u548c\u63a9\u7801\u4f5c\u4e3a\u63d0\u793a(MasP)\u3002SSC\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8ba9<SEG>\u4ee4\u724c\u5728Transformer\u5c42\u95f4\u6ed1\u52a8\uff0c\u52a8\u6001\u9009\u62e9\u8fde\u63a5\u89c6\u89c9\u6a21\u578b\u7684\u5c42\uff1bMasP\u5219\u5229\u7528\u76f8\u4f3c\u5ea6\u56fe\u4f5c\u4e3a\u8f6f\u903b\u8f91\u63a9\u7801\u6765\u63d0\u793aSAM\u751f\u6210\u63a9\u7801\u3002", "result": "UGround\u9996\u6b21\u5728\u5355\u4e00\u6846\u67b6\u5185\u7edf\u4e00\u4e86\u4ece\u4f20\u7edf\u53c2\u8003\u8868\u8fbe\u5f0f\u5206\u5272\u5230\u63a8\u7406\u5206\u5272\u3001\u4ece\u5355\u76ee\u6807\u5230\u591a\u76ee\u6807\u3001\u4ece\u6b63\u67e5\u8be2\u5230\u9519\u8bef\u524d\u63d0\u7684\u5404\u79cd\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u3002", "conclusion": "UGround\u901a\u8fc7\u52a8\u6001\u5c42\u9009\u62e9\u548c\u660e\u786e\u7684\u63a9\u7801\u63d0\u793a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04320", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04320", "abs": "https://arxiv.org/abs/2510.04320", "authors": ["Rui Wu", "Yihao Quan", "Zeru Shi", "Zhenting Wang", "Yanshu Li", "Ruixiang Tang"], "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs", "comment": null, "summary": "Safety-aligned Large Language Models (LLMs) still show two dominant failure\nmodes: they are easily jailbroken, or they over-refuse harmless inputs that\ncontain sensitive surface signals. We trace both to a common cause: current\nmodels reason weakly about links between actions and outcomes and over-rely on\nsurface-form signals, lexical or stylistic cues that do not encode\nconsequences. We define this failure mode as Consequence-blindness. To study\nconsequence-blindness, we build a benchmark named CB-Bench covering four risk\nscenarios that vary whether semantic risk aligns with outcome risk, enabling\nevaluation under both matched and mismatched conditions which are often ignored\nby existing safety benchmarks. Mainstream models consistently fail to separate\nthese risks and exhibit consequence-blindness, indicating that\nconsequence-blindness is widespread and systematic. To mitigate\nconsequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning\ndataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains\nagainst semantic-camouflage jailbreaks and reduce over-refusal on harmless\ninputs, while maintaining utility and generalization on other benchmarks. These\nresults clarify the limits of current alignment, establish consequence-aware\nreasoning as a core alignment goal and provide a more practical and\nreproducible evaluation path.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5b89\u5168\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u540e\u679c\u76f2\u89c6\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5f31\u4e8e\u63a8\u7406\u884c\u52a8\u4e0e\u540e\u679c\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u5f62\u5f0f\u4fe1\u53f7\u3002\u4f5c\u8005\u6784\u5efa\u4e86CB-Bench\u57fa\u51c6\u548cCS-Chain-4k\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5b89\u5168\u5bf9\u9f50\u7684LLMs\u5b58\u5728\u4e24\u79cd\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\uff1a\u5bb9\u6613\u88ab\u8d8a\u72f1\u653b\u51fb\uff0c\u4ee5\u53ca\u5bf9\u65e0\u5bb3\u8f93\u5165\u8fc7\u5ea6\u62d2\u7edd\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e24\u79cd\u95ee\u9898\u90fd\u6e90\u4e8e\u6a21\u578b\u7f3a\u4e4f\u5bf9\u884c\u52a8\u540e\u679c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u8868\u9762\u5f62\u5f0f\u4fe1\u53f7\u3002", "method": "\u6784\u5efaCB-Bench\u57fa\u51c6\u8bc4\u4f30\u540e\u679c\u76f2\u89c6\u95ee\u9898\uff0c\u6db5\u76d6\u56db\u79cd\u98ce\u9669\u573a\u666f\uff1b\u5f00\u53d1CS-Chain-4k\u6570\u636e\u96c6\u7528\u4e8e\u5b89\u5168\u5bf9\u9f50\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u6765\u589e\u5f3a\u540e\u679c\u63a8\u7406\u80fd\u529b\u3002", "result": "\u4e3b\u6d41\u6a21\u578b\u5728CB-Bench\u4e0a\u6301\u7eed\u65e0\u6cd5\u533a\u5206\u8bed\u4e49\u98ce\u9669\u548c\u7ed3\u679c\u98ce\u9669\uff0c\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u7684\u540e\u679c\u76f2\u89c6\u3002\u4f7f\u7528CS-Chain-4k\u5fae\u8c03\u7684\u6a21\u578b\u5728\u5bf9\u6297\u8bed\u4e49\u4f2a\u88c5\u8d8a\u72f1\u653b\u51fb\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u51cf\u5c11\u4e86\u5bf9\u65e0\u5bb3\u8f93\u5165\u7684\u8fc7\u5ea6\u62d2\u7edd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u5176\u4ed6\u57fa\u51c6\u4e0a\u7684\u6548\u7528\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u540e\u679c\u76f2\u89c6\u662f\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u7684\u4e00\u4e2a\u7cfb\u7edf\u6027\u9650\u5236\uff0c\u540e\u679c\u611f\u77e5\u63a8\u7406\u5e94\u6210\u4e3a\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\uff0c\u8be5\u7814\u7a76\u4e3a\u66f4\u5b9e\u7528\u548c\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2510.03857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03857", "abs": "https://arxiv.org/abs/2510.03857", "authors": ["Minseo Lee", "Byeonghyeon Lee", "Lucas Yunkyu Lee", "Eunsoo Lee", "Sangmin Kim", "Seunghyeon Song", "Joo Chan Lee", "Jong Hwan Ko", "Jaesik Park", "Eunbyung Park"], "title": "Optimized Minimal 4D Gaussian Splatting", "comment": "17 pages, 8 figures", "summary": "4D Gaussian Splatting has emerged as a new paradigm for dynamic scene\nrepresentation, enabling real-time rendering of scenes with complex motions.\nHowever, it faces a major challenge of storage overhead, as millions of\nGaussians are required for high-fidelity reconstruction. While several studies\nhave attempted to alleviate this memory burden, they still face limitations in\ncompression ratio or visual quality. In this work, we present OMG4 (Optimized\nMinimal 4D Gaussian Splatting), a framework that constructs a compact set of\nsalient Gaussians capable of faithfully representing 4D Gaussian models. Our\nmethod progressively prunes Gaussians in three stages: (1) Gaussian Sampling to\nidentify primitives critical to reconstruction fidelity, (2) Gaussian Pruning\nto remove redundancies, and (3) Gaussian Merging to fuse primitives with\nsimilar characteristics. In addition, we integrate implicit appearance\ncompression and generalize Sub-Vector Quantization (SVQ) to 4D representations,\nfurther reducing storage while preserving quality. Extensive experiments on\nstandard benchmark datasets demonstrate that OMG4 significantly outperforms\nrecent state-of-the-art methods, reducing model sizes by over 60% while\nmaintaining reconstruction quality. These results position OMG4 as a\nsignificant step forward in compact 4D scene representation, opening new\npossibilities for a wide range of applications. Our source code is available at\nhttps://minshirley.github.io/OMG4/.", "AI": {"tldr": "OMG4\u662f\u4e00\u4e2a\u4f18\u5316\u538b\u7f294D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u526a\u679d\u548c\u9690\u5f0f\u5916\u89c2\u538b\u7f29\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5b58\u50a8\u5927\u5c0f\u8d85\u8fc760%\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "4D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u867d\u7136\u80fd\u5b9e\u65f6\u6e32\u67d3\u590d\u6742\u52a8\u6001\u573a\u666f\uff0c\u4f46\u9700\u8981\u6570\u767e\u4e07\u4e2a\u9ad8\u65af\u51fd\u6570\u5bfc\u81f4\u5b58\u50a8\u5f00\u9500\u5de8\u5927\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u538b\u7f29\u6bd4\u6216\u89c6\u89c9\u8d28\u91cf\u4e0a\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u526a\u679d\uff1a\u9ad8\u65af\u91c7\u6837\u8bc6\u522b\u5173\u952e\u57fa\u5143\u3001\u9ad8\u65af\u526a\u679d\u53bb\u9664\u5197\u4f59\u3001\u9ad8\u65af\u5408\u5e76\u878d\u5408\u76f8\u4f3c\u7279\u5f81\uff1b\u7ed3\u5408\u9690\u5f0f\u5916\u89c2\u538b\u7f29\u548c\u5e7f\u4e49\u5b50\u5411\u91cf\u91cf\u5316\u6280\u672f\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOMG4\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u8d85\u8fc760%\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "OMG4\u5728\u7d27\u51d14D\u573a\u666f\u8868\u793a\u65b9\u9762\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u5e7f\u6cdb\u5e94\u7528\u5f00\u8f9f\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.04338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04338", "abs": "https://arxiv.org/abs/2510.04338", "authors": ["Mathieu La\u00ef-king", "Patrick Paroubek"], "title": "Evaluation of Clinical Trials Reporting Quality using Large Language Models", "comment": null, "summary": "Reporting quality is an important topic in clinical trial research articles,\nas it can impact clinical decisions. In this article, we test the ability of\nlarge language models to assess the reporting quality of this type of article\nusing the Consolidated Standards of Reporting Trials (CONSORT). We create\nCONSORT-QA, an evaluation corpus from two studies on abstract reporting quality\nwith CONSORT-abstract standards. We then evaluate the ability of different\nlarge generative language models (from the general domain or adapted to the\nbiomedical domain) to correctly assess CONSORT criteria with different known\nprompting methods, including Chain-of-thought. Our best combination of model\nand prompting method achieves 85% accuracy. Using Chain-of-thought adds\nvaluable information on the model's reasoning for completing the task.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u62a5\u544a\u8d28\u91cf\u7684\u80fd\u529b\uff0c\u4f7f\u7528CONSORT\u6807\u51c6\u521b\u5efa\u4e86CONSORT-QA\u8bc4\u4f30\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\uff0c\u6700\u4f73\u7ec4\u5408\u8fbe\u523085%\u51c6\u786e\u7387\u3002", "motivation": "\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u62a5\u544a\u8d28\u91cf\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\uff0c\u9700\u8981\u6709\u6548\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u521b\u5efaCONSORT-QA\u8bc4\u4f30\u8bed\u6599\u5e93\uff0c\u4f7f\u7528\u4e0d\u540c\u5927\u578b\u751f\u6210\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u65b9\u6cd5\uff08\u5305\u62ec\u601d\u7ef4\u94fe\uff09\u8bc4\u4f30CONSORT\u6807\u51c6\u3002", "result": "\u6700\u4f73\u6a21\u578b\u548c\u63d0\u793a\u65b9\u6cd5\u7ec4\u5408\u8fbe\u523085%\u51c6\u786e\u7387\uff0c\u601d\u7ef4\u94fe\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u63a8\u7406\u4fe1\u606f\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u62a5\u544a\u8d28\u91cf\uff0c\u601d\u7ef4\u94fe\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.03858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03858", "abs": "https://arxiv.org/abs/2510.03858", "authors": ["Jyoti Kini", "Rohit Gupta", "Mubarak Shah"], "title": "Cross-View Open-Vocabulary Object Detection in Aerial Imagery", "comment": null, "summary": "Traditional object detection models are typically trained on a fixed set of\nclasses, limiting their flexibility and making it costly to incorporate new\ncategories. Open-vocabulary object detection addresses this limitation by\nenabling models to identify unseen classes without explicit training.\nLeveraging pretrained models contrastively trained on abundantly available\nground-view image-text classification pairs provides a strong foundation for\nopen-vocabulary object detection in aerial imagery. Domain shifts, viewpoint\nvariations, and extreme scale differences make direct knowledge transfer across\ndomains ineffective, requiring specialized adaptation strategies. In this\npaper, we propose a novel framework for adapting open-vocabulary\nrepresentations from ground-view images to solve object detection in aerial\nimagery through structured domain alignment. The method introduces contrastive\nimage-to-image alignment to enhance the similarity between aerial and\nground-view embeddings and employs multi-instance vocabulary associations to\nalign aerial images with text embeddings. Extensive experiments on the xView,\nDOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.\nOur open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16\nmAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when\ncompared to finetuned closed-vocabulary dataset-specific model performance,\nthus paving the way for more flexible and scalable object detection systems in\naerial applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5730\u9762\u89c6\u56fe\u7684\u5f00\u653e\u8bcd\u6c47\u8868\u793a\u9002\u914d\u5230\u822a\u62cd\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u57df\u5bf9\u9f50\u89e3\u51b3\u9886\u57df\u5dee\u5f02\u95ee\u9898", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u56fa\u5b9a\u7c7b\u522b\u96c6\u4e0a\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u6269\u5c55\u65b0\u7c7b\u522b\u6210\u672c\u9ad8\u3002\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u80fd\u591f\u8bc6\u522b\u672a\u89c1\u7c7b\u522b\uff0c\u4f46\u5730\u9762\u89c6\u56fe\u9884\u8bad\u7ec3\u6a21\u578b\u76f4\u63a5\u8fc1\u79fb\u5230\u822a\u62cd\u56fe\u50cf\u56e0\u9886\u57df\u504f\u79fb\u3001\u89c6\u89d2\u53d8\u5316\u548c\u5c3a\u5ea6\u5dee\u5f02\u800c\u6548\u679c\u4e0d\u4f73", "method": "\u5f15\u5165\u5bf9\u6bd4\u56fe\u50cf-\u56fe\u50cf\u5bf9\u9f50\u589e\u5f3a\u822a\u62cd\u4e0e\u5730\u9762\u89c6\u56fe\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\uff0c\u91c7\u7528\u591a\u5b9e\u4f8b\u8bcd\u6c47\u5173\u8054\u5bf9\u9f50\u822a\u62cd\u56fe\u50cf\u4e0e\u6587\u672c\u5d4c\u5165", "result": "\u5728xView\u3001DOTAv2\u3001VisDrone\u3001DIOR\u548cHRRSD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u76f8\u6bd4\u5fae\u8c03\u7684\u5c01\u95ed\u8bcd\u6c47\u6a21\u578b\uff0c\u5728DOTAv2\u63d0\u5347+6.32 mAP\uff0cVisDrone\u63d0\u5347+4.16 mAP\uff0cHRRSD\u63d0\u5347+3.46 mAP", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u822a\u62cd\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u7684\u76ee\u6807\u68c0\u6d4b\u7cfb\u7edf"}}
{"id": "2510.04340", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04340", "abs": "https://arxiv.org/abs/2510.04340", "authors": ["Daniel Tan", "Anders Woodruff", "Niels Warncke", "Arun Jose", "Maxime Rich\u00e9", "David Demitri Africa", "Mia Taylor"], "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time", "comment": "40 pages, 22 figures In proceedings at ICLR 2026", "summary": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.", "AI": {"tldr": "\u63d0\u51fa\u63a5\u79cd\u63d0\u793a\u6cd5\uff1a\u901a\u8fc7\u5728\u5fae\u8c03\u6570\u636e\u524d\u6dfb\u52a0\u7b80\u77ed\u7cfb\u7edf\u63d0\u793a\u6765\u6545\u610f\u5f15\u53d1\u4e0d\u826f\u7279\u5f81\uff0c\u6d4b\u8bd5\u65f6\u4e0d\u4f7f\u7528\u8be5\u63d0\u793a\uff0c\u4ece\u800c\u964d\u4f4e\u6a21\u578b\u8868\u8fbe\u4e0d\u826f\u7279\u5f81\u7684\u6982\u7387\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65f6\u5e38\u4f1a\u540c\u65f6\u5b66\u4e60\u5230\u671f\u671b\u548c\u4e0d\u671f\u671b\u7684\u7279\u5f81\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u9009\u62e9\u6027\u5730\u5b66\u4e60\u671f\u671b\u7279\u5f81\u800c\u6291\u5236\u4e0d\u671f\u671b\u7279\u5f81\u3002", "method": "\u5728\u5fae\u8c03\u6570\u636e\u524d\u6dfb\u52a0\u7cfb\u7edf\u63d0\u793a\u6307\u4ee4\uff0c\u6545\u610f\u5f15\u53d1\u4e0d\u671f\u671b\u7684\u7279\u5f81\uff0c\u4f7f\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u4e0d\u4f7f\u7528\u8be5\u6307\u4ee4\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u4e0d\u826f\u7279\u5f81\u7684\u8868\u8fbe\u3002", "result": "\u63a5\u79cd\u65b9\u6cd5\u5728\u591a\u4e2a\u573a\u666f\u4e2d\u6709\u6548\uff1a\u51cf\u5c11\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u4e2d\u7684\u7a81\u53d1\u4e0d\u5bf9\u9f50\u3001\u9632\u5fa1\u540e\u95e8\u6ce8\u5165\u3001\u7f13\u89e3\u6f5c\u610f\u8bc6\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u4f20\u9012\u3002", "conclusion": "\u63a5\u79cd\u63d0\u793a\u6cd5\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u9009\u62e9\u6027\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u4f7f\u7279\u5f81\u4e0d\u518d\u610f\u5916\u6765\u51cf\u5c11\u4f18\u5316\u538b\u529b\uff0c\u4ece\u800c\u964d\u4f4e\u6cdb\u5316\u7a0b\u5ea6\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u673a\u5236\u3002"}}
{"id": "2510.03869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03869", "abs": "https://arxiv.org/abs/2510.03869", "authors": ["Runhao Liu", "Ziming Chen", "Peng Zhang"], "title": "Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis", "comment": null, "summary": "Skin cancer is one of the most prevalent and deadly forms of cancer\nworldwide, which highlights the critical importance of early detection and\ndiagnosis in improving patient outcomes. Deep learning (DL) has shown\nsignificant promise in enhancing the accuracy and efficiency of automated skin\ndisease diagnosis, particularly in detecting and evaluating skin lesions and\nclassification. However, there are still several challenges for DL-based skin\ncancer diagnosis, including complex features, image noise, intra-class\nvariation, inter-class similarity, and data imbalance. By synthesizing recent\nresearch, this review discusses innovative approaches to cope with these\nchallenges, such as data augmentation, hybrid models, and feature fusion, etc.\nFurthermore, the review highlights the integration of DL models into clinical\nworkflows, offering insights into the potential of deep learning to\nrevolutionize skin disease diagnosis and improve clinical decision-making. This\narticle follows a comprehensive methodology based on the PRISMA framework and\nemphasizes the need for continued advancements to fully unlock the\ntransformative potential of DL in dermatological care.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u8ba8\u8bba\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\uff08\u590d\u6742\u7279\u5f81\u3001\u56fe\u50cf\u566a\u58f0\u3001\u7c7b\u5185\u53d8\u5f02\u3001\u7c7b\u95f4\u76f8\u4f3c\u6027\u548c\u6570\u636e\u4e0d\u5e73\u8861\uff09\u4ee5\u53ca\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u7684\u521b\u65b0\u65b9\u6cd5\uff08\u6570\u636e\u589e\u5f3a\u3001\u6df7\u5408\u6a21\u578b\u3001\u7279\u5f81\u878d\u5408\u7b49\uff09\u3002", "motivation": "\u76ae\u80a4\u764c\u662f\u5168\u7403\u6700\u5e38\u89c1\u548c\u81f4\u547d\u7684\u764c\u75c7\u4e4b\u4e00\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u591a\u4e2a\u6280\u672f\u6311\u6218\u9700\u8981\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u57fa\u4e8ePRISMA\u6846\u67b6\u7684\u7efc\u5408\u65b9\u6cd5\u5b66\uff0c\u7cfb\u7edf\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u764c\u8bca\u65ad\u4e2d\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\uff0c\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u6df7\u5408\u6a21\u578b\u3001\u7279\u5f81\u878d\u5408\u7b49\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u76ae\u80a4\u764c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u76ae\u80a4\u75c5\u53d8\u68c0\u6d4b\u548c\u5206\u7c7b\u65b9\u9762\u3002\u8fd9\u4e9b\u6a21\u578b\u6709\u671b\u6574\u5408\u5230\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\uff0c\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u75be\u75c5\u8bca\u65ad\u4e2d\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u6301\u7eed\u7684\u6280\u672f\u8fdb\u6b65\u6765\u5145\u5206\u53d1\u6325\u5176\u5728\u76ae\u80a4\u75c5\u5b66\u62a4\u7406\u4e2d\u7684\u53d8\u9769\u6027\u4f5c\u7528\u3002"}}
{"id": "2510.04347", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04347", "abs": "https://arxiv.org/abs/2510.04347", "authors": ["Anindya Sundar Das", "Kangjie Chen", "Monowar Bhuyan"], "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models", "comment": "15 pages total (9 pages main text + 4 pages appendix + references),\n  12 figures, preprint version. The final version may differ", "summary": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u548c\u68af\u5ea6\u4fe1\u606f\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u5404\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u540e\u95e8\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u653b\u51fb\u8005\u901a\u8fc7\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u5d4c\u5165\u89e6\u53d1\u6a21\u5f0f\u6765\u690d\u5165\u6076\u610f\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408token\u7ea7\u522b\u7684\u6ce8\u610f\u529b\u4fe1\u606f\u548c\u68af\u5ea6\u4fe1\u606f\u6765\u6784\u5efa\u5f02\u5e38\u5206\u6570\uff0c\u68c0\u6d4b\u540e\u95e8\u89e6\u53d1\u3002", "result": "\u5728\u591a\u79cd\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u548c\u4e0d\u540c\u540e\u95e8\u653b\u51fb\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u6709\u6548\u9632\u5fa1\u540e\u95e8\u653b\u51fb\uff0c\u8fd8\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u89e6\u53d1\u5b9a\u4f4d\u673a\u5236\u548c\u9632\u5fa1\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.03870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03870", "abs": "https://arxiv.org/abs/2510.03870", "authors": ["Nikolaos Kaparinos", "Vasileios Mezaris"], "title": "SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks", "comment": "Under review", "summary": "Generative Adversarial Networks (GANs) achieve excellent performance in\ngenerative tasks, such as image super-resolution, but their computational\nrequirements make difficult their deployment on resource-constrained devices.\nWhile knowledge distillation is a promising research direction for GAN\ncompression, effectively training a smaller student generator is challenging\ndue to the capacity mismatch between the student generator and the teacher\ndiscriminator. In this work, we propose Student Discriminator Assisted\nKnowledge Distillation (SDAKD), a novel GAN distillation methodology that\nintroduces a student discriminator to mitigate this capacity mismatch. SDAKD\nfollows a three-stage training strategy, and integrates an adapted feature map\ndistillation approach in its last two training stages. We evaluated SDAKD on\ntwo well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our\nexperiments demonstrate consistent improvements over the baselines and SOTA GAN\nknowledge distillation methods. The SDAKD source code will be made openly\navailable upon acceptance of the paper.", "AI": {"tldr": "\u63d0\u51faSDAKD\u65b9\u6cd5\u89e3\u51b3GAN\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u5bb9\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u5b66\u751f\u5224\u522b\u5668\u548c\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u548cSOTA\u65b9\u6cd5\u7684\u6027\u80fd", "motivation": "GAN\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u77e5\u8bc6\u84b8\u998f\u662f\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u4f46\u5b66\u751f\u751f\u6210\u5668\u4e0e\u6559\u5e08\u5224\u522b\u5668\u4e4b\u95f4\u7684\u5bb9\u91cf\u4e0d\u5339\u914d\u5bfc\u81f4\u8bad\u7ec3\u56f0\u96be", "method": "\u63d0\u51faSDAKD\u65b9\u6cd5\uff0c\u5f15\u5165\u5b66\u751f\u5224\u522b\u5668\u7f13\u89e3\u5bb9\u91cf\u4e0d\u5339\u914d\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u5728\u540e\u4e24\u4e2a\u9636\u6bb5\u96c6\u6210\u9002\u914d\u7684\u7279\u5f81\u56fe\u84b8\u998f\u65b9\u6cd5", "result": "\u5728GCFSR\u548cReal-ESRGAN\u4e24\u4e2a\u8d85\u5206\u8fa8\u7387GAN\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u57fa\u7ebf\u548cSOTA GAN\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u53d6\u5f97\u4e00\u81f4\u6539\u8fdb", "conclusion": "SDAKD\u662f\u6709\u6548\u7684GAN\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u7f13\u89e3\u5bb9\u91cf\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02"}}
{"id": "2510.04392", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04392", "abs": "https://arxiv.org/abs/2510.04392", "authors": ["Faisal Hamman", "Chenyang Zhu", "Anoop Kumar", "Xujun Peng", "Sanghamitra Dutta", "Daben Liu", "Alfy Samuel"], "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards", "comment": "Accepted at NeurIPS 2025 Workshop on Reliable ML from Unreliable Data", "summary": "RAG systems are increasingly deployed in high-stakes domains where users\nexpect outputs to be consistent across semantically equivalent queries.\nHowever, existing systems often exhibit significant inconsistencies due to\nvariability in both the retriever and generator (LLM), undermining trust and\nreliability. In this work, we focus on information consistency, i.e., the\nrequirement that outputs convey the same core content across semantically\nequivalent inputs. We introduce a principled evaluation framework that\ndecomposes RAG consistency into retriever-level, generator-level, and\nend-to-end components, helping identify inconsistency sources. To improve\nconsistency, we propose Paraphrased Set Group Relative Policy Optimization\n(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased\nset to assign group similarity rewards. We leverage PS-GRPO to achieve\nInformation Consistent RAG (Con-RAG), training the generator to produce\nconsistent outputs across paraphrased queries and remain robust to\nretrieval-induced variability. Because exact reward computation over paraphrase\nsets is computationally expensive, we also introduce a scalable approximation\nmethod that retains effectiveness while enabling efficient, large-scale\ntraining. Empirical evaluations across short-form, multi-hop, and long-form QA\nbenchmarks demonstrate that Con-RAG significantly improves both consistency and\naccuracy over strong baselines, even in the absence of explicit ground-truth\nsupervision. Our work provides practical solutions for evaluating and building\nreliable RAG systems for safety-critical deployments.", "AI": {"tldr": "\u63d0\u51faCon-RAG\u7cfb\u7edf\u89e3\u51b3RAG\u5728\u8bed\u4e49\u7b49\u4ef7\u67e5\u8be2\u4e0b\u8f93\u51fa\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u901a\u8fc7PS-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8bad\u7ec3\u751f\u6210\u5668\uff0c\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u8bed\u4e49\u7b49\u4ef7\u67e5\u8be2\u4e0b\u8f93\u51fa\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u548c\u7528\u6237\u4fe1\u4efb\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u65f6\u3002", "method": "\u63d0\u51faPS-GRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u8f6c\u8ff0\u67e5\u8be2\u96c6\u8fdb\u884c\u7fa4\u4f53\u76f8\u4f3c\u5ea6\u5956\u52b1\uff0c\u5e76\u5f15\u5165\u53ef\u6269\u5c55\u7684\u8fd1\u4f3c\u8ba1\u7b97\u65b9\u6cd5\u3002", "result": "\u5728\u77ed\u683c\u5f0f\u3001\u591a\u8df3\u548c\u957f\u683c\u5f0fQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCon-RAG\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u5b89\u5168\u5173\u952e\u90e8\u7f72\u63d0\u4f9b\u4e86\u8bc4\u4f30\u548c\u6784\u5efa\u53ef\u9760RAG\u7cfb\u7edf\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03873", "abs": "https://arxiv.org/abs/2510.03873", "authors": ["Saja Al-Dabet", "Sherzod Turaev", "Nazar Zaki", "Arif O. Khan", "Luai Eldweik"], "title": "PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis", "comment": "This is a preprint version of a manuscript under review. All rights\n  reserved by the authors", "summary": "Diagnosing ocular-induced abnormal head posture (AHP) requires a\ncomprehensive analysis of both head pose and ocular movements. However,\nexisting datasets focus on these aspects separately, limiting the development\nof integrated diagnostic approaches and restricting AI-driven advancements in\nAHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D\ndataset that synchronously captures head pose and gaze movement information for\nocular-induced AHP assessment. Structured clinical data were extracted from\nmedical literature using large language models (LLMs) through an iterative\nprocess with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and\ncomplex prompting strategies. The extracted records were systematically imputed\nand transformed into 3D representations using the Neural Head Avatar (NHA)\nframework. The dataset includes 7,920 images generated from two head textures,\ncovering a broad spectrum of ocular conditions. The extraction method achieved\nan overall accuracy of 91.92%, demonstrating its reliability for clinical\ndataset construction. PoseGaze-AHP is the first publicly available resource\ntailored for AI-driven ocular-induced AHP diagnosis, supporting the development\nof accurate and privacy-compliant diagnostic tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86PoseGaze-AHP\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u7528\u4e8eAI\u9a71\u52a8\u773c\u6e90\u6027\u5f02\u5e38\u5934\u4f4d\u8bca\u65ad\u7684\u516c\u5f00\u8d44\u6e90\uff0c\u540c\u6b65\u6355\u6349\u5934\u4f4d\u548c\u6ce8\u89c6\u8fd0\u52a8\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5206\u522b\u5173\u6ce8\u5934\u4f4d\u548c\u773c\u52a8\uff0c\u9650\u5236\u4e86\u773c\u6e90\u6027\u5f02\u5e38\u5934\u4f4d\u7efc\u5408\u8bca\u65ad\u65b9\u6cd5\u7684\u53d1\u5c55\u548cAI\u5206\u6790\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528Claude 3.5 Sonnet\u6a21\u578b\u901a\u8fc7\u8fed\u4ee3\u8fc7\u7a0b\u4ece\u533b\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\uff0c\u91c7\u7528\u9010\u6b65\u3001\u5206\u5c42\u548c\u590d\u6742\u63d0\u793a\u7b56\u7565\uff0c\u7136\u540e\u4f7f\u7528\u795e\u7ecf\u5934\u50cf\u6846\u67b6\u5c06\u63d0\u53d6\u7684\u8bb0\u5f55\u7cfb\u7edf\u5316\u63d2\u8865\u5e76\u8f6c\u6362\u4e3a3D\u8868\u793a\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b7,920\u5f20\u56fe\u50cf\uff0c\u751f\u6210\u81ea\u4e24\u79cd\u5934\u90e8\u7eb9\u7406\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u773c\u90e8\u72b6\u51b5\u3002\u63d0\u53d6\u65b9\u6cd5\u603b\u4f53\u51c6\u786e\u7387\u8fbe\u523091.92%\u3002", "conclusion": "PoseGaze-AHP\u662f\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u773c\u6e90\u6027\u5f02\u5e38\u5934\u4f4dAI\u8bca\u65ad\u8d44\u6e90\uff0c\u652f\u6301\u5f00\u53d1\u51c6\u786e\u4e14\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2510.04394", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04394", "abs": "https://arxiv.org/abs/2510.04394", "authors": ["Ankit Vadehra", "Bill Johnson", "Gene Saunders", "Pascal Poupart"], "title": "Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation", "comment": "Accepted for publication in the 4th HCI+NLP Workshop (Fourth Workshop\n  on Bridging Human-Computer Interaction and Natural Language Processing; part\n  of EMNLP 2025)", "summary": "Text editing can involve several iterations of revision. Incorporating an\nefficient Grammar Error Correction (GEC) tool in the initial correction round\ncan significantly impact further human editing effort and final text quality.\nThis raises an interesting question to quantify GEC Tool usability: How much\neffort can the GEC Tool save users? We present the first large-scale dataset of\npost-editing (PE) time annotations and corrections for two English GEC test\ndatasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)\nfor GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by\nestimating PE time-to-correct. Using our dataset, we quantify the amount of\ntime saved by GEC Tools in text editing. Analyzing the edit type indicated that\ndetermining whether a sentence needs correction and edits like paraphrasing and\npunctuation changes had the greatest impact on PE time. Finally, comparison\nwith human rankings shows that PEET correlates well with technical effort\njudgment, providing a new human-centric direction for evaluating GEC tool\nusability. We release our dataset and code at:\nhttps://github.com/ankitvad/PEET_Scorer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u8bed\u6cd5\u9519\u8bef\u4fee\u6b63\u5de5\u5177\u540e\u7f16\u8f91\u65f6\u95f4\u6570\u636e\u96c6\uff0c\u5e76\u5f15\u5165PEET\u8bc4\u5206\u5668\u6765\u91cf\u5316GEC\u5de5\u5177\u8282\u7701\u7684\u7528\u6237\u7f16\u8f91\u65f6\u95f4\u3002", "motivation": "\u91cf\u5316\u8bed\u6cd5\u9519\u8bef\u4fee\u6b63\u5de5\u5177\u7684\u5b9e\u9645\u53ef\u7528\u6027\uff0c\u4e86\u89e3GEC\u5de5\u5177\u80fd\u4e3a\u7528\u6237\u8282\u7701\u591a\u5c11\u7f16\u8f91\u65f6\u95f4\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u82f1\u8bedGEC\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u540e\u7f16\u8f91\u65f6\u95f4\u6807\u6ce8\u548c\u4fee\u6b63\u6570\u636e\u96c6\uff0c\u63d0\u51faPEET\u8bc4\u5206\u5668\u6765\u4f30\u8ba1\u540e\u7f16\u8f91\u65f6\u95f4\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5224\u65ad\u53e5\u5b50\u662f\u5426\u9700\u8981\u4fee\u6b63\u4ee5\u53ca\u6539\u5199\u548c\u6807\u70b9\u7b26\u53f7\u66f4\u6539\u7b49\u7f16\u8f91\u7c7b\u578b\u5bf9\u540e\u7f16\u8f91\u65f6\u95f4\u5f71\u54cd\u6700\u5927\u3002PEET\u4e0e\u4eba\u7c7b\u6280\u672f\u52aa\u529b\u5224\u65ad\u76f8\u5173\u6027\u826f\u597d\u3002", "conclusion": "PEET\u4e3a\u8bc4\u4f30GEC\u5de5\u5177\u53ef\u7528\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u4eba\u7c7b\u4e2d\u5fc3\u65b9\u5411\uff0c\u80fd\u591f\u6709\u6548\u91cf\u5316GEC\u5de5\u5177\u5728\u6587\u672c\u7f16\u8f91\u4e2d\u8282\u7701\u7684\u65f6\u95f4\u3002"}}
{"id": "2510.03874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03874", "abs": "https://arxiv.org/abs/2510.03874", "authors": ["Yunhao Li", "Sijing Wu", "Yucheng Zhu", "Huiyu Duan", "Zicheng Zhang", "Guangtao Zhai"], "title": "DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human", "comment": null, "summary": "With the rapid development of 3D scanning and reconstruction technologies,\ndynamic digital human avatars based on 4D meshes have become increasingly\npopular. A high-precision dynamic digital human avatar can be applied to\nvarious fields such as game production, animation generation, and remote\nimmersive communication. However, these 4D human avatar meshes are prone to\nbeing degraded by various types of noise during the processes of collection,\ncompression, and transmission, thereby affecting the viewing experience of\nusers. In light of this fact, quality assessment of dynamic 4D digital humans\nbecomes increasingly important. In this paper, we first propose a large-scale\ndynamic digital human quality assessment dataset, DHQA-4D, which contains 32\nhigh-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D\nhuman meshes degraded by 11 textured distortions, as well as their\ncorresponding textured and non-textured mean opinion scores (MOSs). Equipped\nwith DHQA-4D dataset, we analyze the influence of different types of distortion\non human perception for textured dynamic 4D meshes and non-textured dynamic 4D\nmeshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model\n(LMM) based approach that is able to assess both textured 4D meshes and\nnon-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts\nmulti-dimensional features, including visual features from a projected 2D\nvideo, motion features from cropped video clips, and geometry features from the\n4D human mesh to provide comprehensive quality-related information. Then we\nutilize a LMM model to integrate the multi-dimensional features and conduct a\nLoRA-based instruction tuning technique to teach the LMM model to predict the\nquality scores. Extensive experimental results on the DHQA-4D dataset\ndemonstrate the superiority of our DynaMesh-Rater method over previous quality\nassessment methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DHQA-4D\u6570\u636e\u96c6\u548cDynaMesh-Rater\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u60014D\u6570\u5b57\u4eba\u7f51\u683c\u7684\u8d28\u91cf\u8bc4\u4f30\u3002DHQA-4D\u5305\u542b32\u4e2a\u9ad8\u8d28\u91cf4D\u4eba\u4f53\u7f51\u683c\u5e8f\u5217\u548c1920\u4e2a\u5931\u771f\u6837\u672c\uff0cDynaMesh-Rater\u901a\u8fc7\u63d0\u53d6\u89c6\u89c9\u3001\u8fd0\u52a8\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8d28\u91cf\u8bc4\u5206\u3002", "motivation": "\u968f\u77403D\u626b\u63cf\u6280\u672f\u7684\u53d1\u5c55\uff0c\u52a8\u6001\u6570\u5b57\u4eba\u5316\u8eab\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f464D\u7f51\u683c\u5728\u91c7\u96c6\u3001\u538b\u7f29\u548c\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u53d7\u5230\u5404\u79cd\u566a\u58f0\u5f71\u54cd\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u6784\u5efaDHQA-4D\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u548c\u5931\u771f\u76844D\u4eba\u4f53\u7f51\u683c\u5e8f\u5217\u3002\u7136\u540e\u63d0\u51faDynaMesh-Rater\u65b9\u6cd5\uff0c\u63d0\u53d6\u591a\u7ef4\u5ea6\u7279\u5f81\uff08\u89c6\u89c9\u7279\u5f81\u3001\u8fd0\u52a8\u7279\u5f81\u3001\u51e0\u4f55\u7279\u5f81\uff09\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u96c6\u6210\u8fd9\u4e9b\u7279\u5f81\uff0c\u5e76\u901a\u8fc7LoRA\u6307\u4ee4\u5fae\u8c03\u6280\u672f\u9884\u6d4b\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5728DHQA-4D\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDynaMesh-Rater\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "DHQA-4D\u6570\u636e\u96c6\u4e3a\u52a8\u60014D\u6570\u5b57\u4eba\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0cDynaMesh-Rater\u65b9\u6cd5\u901a\u8fc7\u591a\u7ef4\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6709\u6548\u96c6\u6210\uff0c\u5728\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04398", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04398", "abs": "https://arxiv.org/abs/2510.04398", "authors": ["Buyun Liang", "Liangzu Peng", "Jinqi Luo", "Darshan Thaker", "Kwan Ho Ryan Chan", "Ren\u00e9 Vidal"], "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations", "comment": "Accepted at NeurIPS 2025. Code is available at\n  https://github.com/Buyun-Liang/SECA", "summary": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA.", "AI": {"tldr": "\u63d0\u51fa\u4e86SECA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7b49\u4ef7\u4e14\u8fde\u8d2f\u7684\u5bf9\u6297\u653b\u51fb\u6765\u5f15\u53d1LLM\u5e7b\u89c9\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u73b0\u5b9e\u7684\u63d0\u793a\u4e14\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u4ea7\u751f\u7684\u63d0\u793a\u8981\u4e48\u5305\u542b\u65e0\u610f\u4e49\u6807\u8bb0\uff0c\u8981\u4e48\u6539\u53d8\u539f\u610f\uff0c\u65e0\u6cd5\u53cd\u6620\u5b9e\u8df5\u4e2d\u5e7b\u89c9\u53d1\u751f\u7684\u771f\u5b9e\u60c5\u51b5\u3002", "method": "\u5c06\u5bfb\u627e\u73b0\u5b9e\u653b\u51fb\u8868\u8ff0\u4e3a\u8bed\u4e49\u7b49\u4ef7\u548c\u8fde\u8d2f\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u4fdd\u6301\u7ea6\u675f\u7684\u96f6\u9636\u65b9\u6cd5\u641c\u7d22\u5bf9\u6297\u6027\u63d0\u793a\u3002", "result": "\u5728\u5f00\u653e\u5f0f\u591a\u9009\u9898\u4efb\u52a1\u4e0a\uff0cSECA\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e14\u51e0\u4e4e\u4e0d\u8fdd\u53cd\u7ea6\u675f\u6761\u4ef6\u3002", "conclusion": "SECA\u63ed\u793a\u4e86\u5f00\u6e90\u548c\u5546\u4e1aLLM\u5bf9\u73b0\u5b9e\u4e14\u5408\u7406\u7684\u63d0\u793a\u53d8\u4f53\u5177\u6709\u654f\u611f\u6027\uff0c\u4e3a\u7406\u89e3\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.03876", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03876", "abs": "https://arxiv.org/abs/2510.03876", "authors": ["Runhao Liu", "Ziming Chen", "Peng Zhang"], "title": "Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion", "comment": null, "summary": "Skin cancer classification remains a challenging problem due to high\ninter-class similarity, intra-class variability, and image noise in dermoscopic\nimages. To address these issues, we propose an improved ResNet-50 model\nenhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively\nintegrates multi-scale semantic and surface features to improve feature\nrepresentation and reduce overfitting. The ResNet-50 model is enhanced with an\nadaptive feature fusion mechanism to achieve more effective multi-scale feature\nextraction and improve overall performance. Specifically, a dual-branch design\nfuses high-level semantic and mid-level detail features, which are processed\nthrough global average pooling and fully connected layers to generate adaptive\nweights for weighted fusion, thereby strengthening feature learning and\nreducing the impact of noise on classification. The method is evaluated on a\nsubset of the ISIC 2020 dataset containing 3297 benign and malignant skin\nlesion images. Experimental results show that the proposed ASFF-based ResNet-50\nachieves the best overall performance compared with 5 classic convolutional\nneural networks (CNNs) models. The proposed model reached an accuracy of 93.18%\nalong with higher precision, recall, specificity, and F1 score. The improved\nmodel achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,\nrespectively. Then, the evaluation based on Grad-CAM further proved that the\nimproved model adaptively focuses on lesion-relevant regions while suppressing\nirrelevant background information, thereby validating its enhanced feature\nlearning capability from a deep representation perspective. These findings\ndemonstrate that the proposed approach provides a more effective and efficient\nsolution for computer-aided skin cancer diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u7a7a\u95f4\u7279\u5f81\u878d\u5408(ASFF)\u7684\u6539\u8fdbResNet-50\u6a21\u578b\uff0c\u7528\u4e8e\u76ae\u80a4\u764c\u5206\u7c7b\uff0c\u5728ISIC 2020\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8693.18%\u7684\u51c6\u786e\u7387\u548c0.9717\u7684AUC\u503c\u3002", "motivation": "\u76ae\u80a4\u764c\u5206\u7c7b\u9762\u4e34\u7c7b\u95f4\u76f8\u4f3c\u5ea6\u9ad8\u3001\u7c7b\u5185\u53d8\u5f02\u6027\u5927\u4ee5\u53ca\u76ae\u80a4\u955c\u56fe\u50cf\u566a\u58f0\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7279\u5f81\u8868\u793a\u65b9\u6cd5\u6765\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u6539\u8fdb\u7684ResNet-50\u6a21\u578b\u7ed3\u5408\u81ea\u9002\u5e94\u7a7a\u95f4\u7279\u5f81\u878d\u5408\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u8bbe\u8ba1\u878d\u5408\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\u548c\u4e2d\u5c42\u7ec6\u8282\u7279\u5f81\uff0c\u4f7f\u7528\u5168\u5c40\u5e73\u5747\u6c60\u5316\u548c\u5168\u8fde\u63a5\u5c42\u751f\u6210\u81ea\u9002\u5e94\u6743\u91cd\u8fdb\u884c\u52a0\u6743\u878d\u5408\u3002", "result": "\u5728ISIC 2020\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u523093.18%\uff0cAUC\u503c\u4e3a0.9670(P-R\u66f2\u7ebf)\u548c0.9717(ROC\u66f2\u7ebf)\uff0c\u4f18\u4e8e5\u79cd\u7ecf\u5178CNN\u6a21\u578b\u3002Grad-CAM\u9a8c\u8bc1\u663e\u793a\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u5173\u6ce8\u75c5\u53d8\u76f8\u5173\u533a\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u76ae\u80a4\u764c\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7279\u5f81\u878d\u5408\u589e\u5f3a\u4e86\u7279\u5f81\u5b66\u4e60\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u566a\u58f0\u5f71\u54cd\u3002"}}
{"id": "2510.04400", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04400", "abs": "https://arxiv.org/abs/2510.04400", "authors": ["Marc Cavazza"], "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations", "comment": null, "summary": "In this work, we explore the relevance of textual semantics to Large Language\nModels (LLMs), extending previous insights into the connection between\ndistributional semantics and structural semantics. We investigate whether\nLLM-generated texts preserve semantic isotopies. We design a story continuation\nexperiment using 10,000 ROCStories prompts completed by five LLMs. We first\nvalidate GPT-4o's ability to extract isotopies from a linguistic benchmark,\nthen apply it to the generated stories. We then analyze structural (coverage,\ndensity, spread) and semantic properties of isotopies to assess how they are\naffected by completion. Results show that LLM completion within a given token\nhorizon preserves semantic isotopies across multiple properties.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u662f\u5426\u4fdd\u7559\u8bed\u4e49\u540c\u4f4d\u7d20\uff0c\u901a\u8fc7\u6545\u4e8b\u7eed\u5199\u5b9e\u9a8c\u53d1\u73b0LLM\u5728\u7ed9\u5b9atoken\u8303\u56f4\u5185\u80fd\u591f\u4fdd\u6301\u8bed\u4e49\u540c\u4f4d\u7d20\u3002", "motivation": "\u63a2\u7d22\u6587\u672c\u8bed\u4e49\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1LLM\u751f\u6210\u6587\u672c\u662f\u5426\u4fdd\u6301\u8bed\u4e49\u540c\u4f4d\u7d20\u3002", "method": "\u4f7f\u752810,000\u4e2aROCStories\u63d0\u793a\uff0c\u75315\u4e2aLLM\u5b8c\u6210\u6545\u4e8b\u7eed\u5199\uff0c\u5148\u9a8c\u8bc1GPT-4o\u63d0\u53d6\u540c\u4f4d\u7d20\u7684\u80fd\u529b\uff0c\u7136\u540e\u5206\u6790\u751f\u6210\u6545\u4e8b\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u7279\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793aLLM\u5728\u7ed9\u5b9atoken\u8303\u56f4\u5185\u5b8c\u6210\u6587\u672c\u65f6\uff0c\u80fd\u591f\u8de8\u591a\u4e2a\u5c5e\u6027\u4fdd\u6301\u8bed\u4e49\u540c\u4f4d\u7d20\u3002", "conclusion": "LLM\u5728\u7279\u5b9atoken\u8303\u56f4\u5185\u751f\u6210\u6587\u672c\u65f6\u80fd\u591f\u6709\u6548\u4fdd\u6301\u8bed\u4e49\u540c\u4f4d\u7d20\u3002"}}
{"id": "2510.03878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03878", "abs": "https://arxiv.org/abs/2510.03878", "authors": ["Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R Ajo Babu George", "Sreehari J R"], "title": "Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks", "comment": null, "summary": "Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes\nsignificantly to its high global mortality rate, with over 50\\% of cases\ndetected at advanced stages and a 5-year survival rate below 50\\% according to\nWHO statistics. This study aims to improve early detection of OSCC by\ndeveloping a multimodal deep learning framework that integrates clinical,\nradiological, and histopathological images using a weighted ensemble of\nDenseNet-121 convolutional neural networks (CNNs). Material and Methods A\nretrospective study was conducted using publicly available datasets\nrepresenting three distinct medical imaging modalities. Each modality-specific\ndataset was used to train a DenseNet-121 CNN via transfer learning.\nAugmentation and modality-specific preprocessing were applied to increase\nrobustness. Predictions were fused using a validation-weighted ensemble\nstrategy. Evaluation was performed using accuracy, precision, recall, F1-score.\nResults High validation accuracy was achieved for radiological (100\\%) and\nhistopathological (95.12\\%) modalities, with clinical images performing lower\n(63.10\\%) due to visual heterogeneity. The ensemble model demonstrated improved\ndiagnostic robustness with an overall accuracy of 84.58\\% on a multimodal\nvalidation dataset of 55 samples. Conclusion The multimodal ensemble framework\nbridges gaps in the current diagnostic workflow by offering a non-invasive,\nAI-assisted triage tool that enhances early identification of high-risk\nlesions. It supports clinicians in decision-making, aligning with global\noncology guidelines to reduce diagnostic delays and improve patient outcomes.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a0\u6743\u96c6\u6210DenseNet-121 CNN\u6765\u6574\u5408\u4e34\u5e8a\u3001\u653e\u5c04\u5b66\u548c\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u63d0\u9ad8\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u65e9\u671f\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u53e3\u8154\u9cde\u72b6\u7ec6\u80de\u764c\u7684\u665a\u671f\u8bca\u65ad\u5bfc\u81f4\u9ad8\u6b7b\u4ea1\u7387\uff0c\u8d85\u8fc750%\u7684\u75c5\u4f8b\u5728\u665a\u671f\u88ab\u53d1\u73b0\uff0c5\u5e74\u751f\u5b58\u7387\u4f4e\u4e8e50%\u3002\u9700\u8981\u6539\u8fdb\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u6570\u636e\u96c6\u8bad\u7ec3\u4e09\u4e2aDenseNet-121 CNN\u6a21\u578b\uff0c\u5206\u522b\u5bf9\u5e94\u4e0d\u540c\u533b\u5b66\u6210\u50cf\u6a21\u5f0f\u3002\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u3001\u6570\u636e\u589e\u5f3a\u548c\u6a21\u6001\u7279\u5b9a\u9884\u5904\u7406\uff0c\u91c7\u7528\u9a8c\u8bc1\u52a0\u6743\u96c6\u6210\u7b56\u7565\u878d\u5408\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u653e\u5c04\u5b66\u6a21\u6001\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe100%\uff0c\u75c5\u7406\u5b66\u6a21\u6001\u8fbe95.12%\uff0c\u4e34\u5e8a\u56fe\u50cf\u56e0\u89c6\u89c9\u5f02\u8d28\u6027\u8f83\u4f4e\u4e3a63.10%\u3002\u96c6\u6210\u6a21\u578b\u5728\u591a\u6a21\u6001\u9a8c\u8bc1\u96c6\u4e0a\u603b\u4f53\u51c6\u786e\u7387\u4e3a84.58%\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u96c6\u6210\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u3001AI\u8f85\u52a9\u7684\u5206\u8bca\u5de5\u5177\uff0c\u80fd\u591f\u589e\u5f3a\u9ad8\u98ce\u9669\u75c5\u53d8\u7684\u65e9\u671f\u8bc6\u522b\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\uff0c\u7b26\u5408\u5168\u7403\u80bf\u7624\u5b66\u6307\u5357\u4ee5\u51cf\u5c11\u8bca\u65ad\u5ef6\u8fdf\u3002"}}
{"id": "2510.04434", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.04434", "abs": "https://arxiv.org/abs/2510.04434", "authors": ["Grace LeFevre", "Qingcheng Zeng", "Adam Leif", "Jason Jewell", "Denis Peskoff", "Rob Voigt"], "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?", "comment": "EMNLP 2025", "summary": "The social impact of Natural Language Processing (NLP) is increasingly\nimportant, with a rising community focus on initiatives related to NLP for\nSocial Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the\nACL Anthology address topics related to social good as defined by the UN\nSustainable Development Goals (Adauto et al., 2023). In this study, we take an\nauthor- and venue-level perspective to map the landscape of NLP4SG, quantifying\nthe proportion of work addressing social good concerns both within and beyond\nthe ACL community, by both core ACL contributors and non-ACL authors. With this\napproach we discover two surprising facts about the landscape of NLP4SG. First,\nACL authors are dramatically more likely to do work addressing social good\nconcerns when publishing in venues outside of ACL. Second, the vast majority of\npublications using NLP techniques to address concerns of social good are done\nby non-ACL authors in venues outside of ACL. We discuss the implications of\nthese findings on agenda-setting considerations for the ACL community related\nto NLP4SG.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f5c\u8005\u548c\u4f1a\u8bae\u5c42\u9762\u7684\u89c6\u89d2\uff0c\u63ed\u793a\u4e86NLP4SG\u9886\u57df\u7684\u4e24\u4e2a\u91cd\u8981\u53d1\u73b0\uff1aACL\u4f5c\u8005\u5728ACL\u5916\u90e8\u4f1a\u8bae\u53d1\u8868\u65f6\u66f4\u53ef\u80fd\u4ece\u4e8b\u793e\u4f1a\u516c\u76ca\u7814\u7a76\uff0c\u4e14\u5927\u591a\u6570NLP4SG\u7814\u7a76\u7531\u975eACL\u4f5c\u8005\u5728ACL\u5916\u90e8\u5b8c\u6210\u3002", "motivation": "\u968f\u7740NLP\u793e\u4f1a\u5f71\u54cd\u529b\u7684\u589e\u52a0\uff0cNLP4SG\u7814\u7a76\u65e5\u76ca\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316ACL\u793e\u533a\u5185\u5916\u4f5c\u8005\u5728NLP4SG\u9886\u57df\u7684\u8d21\u732e\u5206\u5e03\uff0c\u4e86\u89e3\u8be5\u9886\u57df\u7684\u53d1\u5c55\u683c\u5c40\u3002", "method": "\u91c7\u7528\u4f5c\u8005\u548c\u4f1a\u8bae\u5c42\u9762\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316ACL\u793e\u533a\u5185\u5916\u4f5c\u8005\u5728NLP4SG\u76f8\u5173\u5de5\u4f5c\u4e2d\u7684\u6bd4\u4f8b\uff0c\u7ed8\u5236NLP4SG\u9886\u57df\u7684\u53d1\u5c55\u56fe\u666f\u3002", "result": "\u53d1\u73b0\u4e24\u4e2a\u91cd\u8981\u4e8b\u5b9e\uff1a1\uff09ACL\u4f5c\u8005\u5728ACL\u5916\u90e8\u4f1a\u8bae\u53d1\u8868\u65f6\u4ece\u4e8b\u793e\u4f1a\u516c\u76ca\u7814\u7a76\u7684\u53ef\u80fd\u6027\u663e\u8457\u66f4\u9ad8\uff1b2\uff09\u7edd\u5927\u591a\u6570\u4f7f\u7528NLP\u6280\u672f\u89e3\u51b3\u793e\u4f1a\u516c\u76ca\u95ee\u9898\u7684\u7814\u7a76\u7531\u975eACL\u4f5c\u8005\u5728ACL\u5916\u90e8\u5b8c\u6210\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5bf9ACL\u793e\u533a\u5728NLP4SG\u9886\u57df\u7684\u8bae\u7a0b\u8bbe\u7f6e\u5177\u6709\u91cd\u8981\u542f\u793a\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u66f4\u597d\u5730\u652f\u6301\u548c\u4fc3\u8fdb\u793e\u4f1a\u516c\u76ca\u5bfc\u5411\u7684NLP\u7814\u7a76\u3002"}}
{"id": "2510.03880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03880", "abs": "https://arxiv.org/abs/2510.03880", "authors": ["Yunhao Li", "Sijing Wu", "Huiyu Duan", "Yucheng Zhu", "Qi Jia", "Guangtao Zhai"], "title": "Exploring Instruction Data Quality for Explainable Image Quality Assessment", "comment": null, "summary": "In recent years, with the rapid development of powerful multimodal large\nlanguage models (MLLMs), explainable image quality assessment (IQA) has\ngradually become popular, aiming at providing quality-related descriptions and\nanswers of images. To achieve this goal, recent methods seek to construct a\nlarge-scale instruction tuning dataset to empower the MLLM with quality\nperception ability following the well-known scaling law. However, a large\namount of instruction tuning data may cause substantial computational costs and\nredundant data, which in turn will cause harm to the performance of the model.\nTo cope with this problem, in this paper, we challenge the scaling law and\nsystematically investigate the role of data quality of the instruction tuning\ndataset for explainable IQA. Using a powerful pre-trained MLLM, we first\ninvestigate the changes in model performance after fine-tuning with different\nsizes of instruction tuning data. We find that selecting a subset of the data\nset randomly using an appropriate ratio can even lead to better results than\ntraining with the entire instruction tuning dataset, demonstrating the\nredundancy of current explainable IQA instruction tuning data. Beyond randomly\nsampling a subset, we propose a clustering-based data selection framework with\nthree stages: clustering feature extraction, cluster quota allocation, and\ncluster sampling strategy. Then we systematically analyze the choices of each\nstage and propose a simple but efficient data selection method IQA-Select for\nexplainable IQA. The experimental results demonstrate that IQA-Select can\nachieve 102.1% and 103.7% performance of full fine-tuning using only 10%\nselected data in Q-Bench and AesBench respectively, significantly reducing\ncomputational costs while achieving better performance.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u6269\u5c55\u5b9a\u5f8b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5IQA-Select\uff0c\u4ec5\u4f7f\u752810%\u7684\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5168\u91cf\u5fae\u8c03\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u53ef\u89e3\u91ca\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6570\u636e\u5197\u4f59\u95ee\u9898\uff0c\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u805a\u7c7b\u6570\u636e\u9009\u62e9\u6846\u67b6\uff1a\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\u3001\u805a\u7c7b\u914d\u989d\u5206\u914d\u548c\u805a\u7c7b\u91c7\u6837\u7b56\u7565\uff0c\u6700\u7ec8\u5f62\u6210IQA-Select\u65b9\u6cd5\u3002", "result": "\u5728Q-Bench\u548cAesBench\u4e0a\uff0c\u4ec5\u4f7f\u752810%\u9009\u62e9\u6570\u636e\u5206\u522b\u8fbe\u5230\u5168\u91cf\u5fae\u8c03102.1%\u548c103.7%\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\uff0cIQA-Select\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u5197\u4f59\u6570\u636e\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2510.04439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04439", "abs": "https://arxiv.org/abs/2510.04439", "authors": ["Lucie Kunitomo-Jacquin", "Edison Marrese-Taylor", "Ken Fukuda"], "title": "On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs", "comment": "Accepted to UncertaiNLP workshop of EMNLP 2025", "summary": "Quantifying uncertainty in large language models (LLMs) is important for\nsafety-critical applications because it helps spot incorrect answers, known as\nhallucinations. One major trend of uncertainty quantification methods is based\non estimating the entropy of the distribution of the LLM's potential output\nsequences. This estimation is based on a set of output sequences and associated\nprobabilities obtained by querying the LLM several times. In this paper, we\nadvocate and experimentally show that the probability of unobserved sequences\nplays a crucial role, and we recommend future research to integrate it to\nenhance such LLM uncertainty quantification methods.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u8003\u8651\u672a\u89c2\u6d4b\u5e8f\u5217\u7684\u6982\u7387\uff0c\u8fd9\u5bf9\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u71b5\u4f30\u8ba1\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c2\u6d4b\u5230\u7684\u8f93\u51fa\u5e8f\u5217\uff0c\u4f46\u5ffd\u7565\u4e86\u672a\u89c2\u6d4b\u5e8f\u5217\u7684\u6982\u7387\uff0c\u8fd9\u53ef\u80fd\u5f71\u54cd\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u672a\u89c2\u6d4b\u5e8f\u5217\u6982\u7387\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5c06\u5176\u6574\u5408\u5230LLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u672a\u89c2\u6d4b\u5e8f\u5217\u7684\u6982\u7387\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u8be5\u6574\u5408\u672a\u89c2\u6d4b\u5e8f\u5217\u7684\u6982\u7387\u6765\u589e\u5f3aLLM\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7684\u6548\u679c\u3002"}}
{"id": "2510.03896", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03896", "abs": "https://arxiv.org/abs/2510.03896", "authors": ["Mingyu Liu", "Zheng Huang", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Yating Wang", "Haoyi Zhu", "Hao Chen", "Chunhua Shen"], "title": "Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert", "comment": null, "summary": "Although Vision-Language Models (VLM) have demonstrated impressive planning\nand reasoning capabilities, translating these abilities into the physical world\nintroduces significant challenges. Conventional Vision-Language-Action (VLA)\nmodels, which integrate reasoning and action into a monolithic architecture,\ngeneralize poorly because they are constrained by scarce, narrow-domain data.\nWhile recent dual-system approaches attempt to decouple \"thinking\" from\n\"acting\", they are often constrained by semantic ambiguities within the action\nmodule. This ambiguity makes large-scale, cross-task training infeasible.\nConsequently, these systems typically necessitate fine-tuning on newly\ncollected data when deployed to novel environments, and the cooperation\nmechanism between the two systems remains ill-defined. To address these\nlimitations, we introduce, for the first time, a framework centered around a\ngeneralizable action expert. Our approach utilizes sparse 3D trajectories as an\nintermediate representation, effectively bridging the high-level planning\ncapabilities of the VLM with the low-level physical action module. During the\nplanning phase, the VLM is only required to generate coarse 3D waypoints. These\nwaypoints are then processed by our generalizable action expert, which refines\nthem into dense, executable action sequences by sampling real-time point cloud\nobservations of the environment. To promote training efficiency and robust\ngeneralization, we introduce a novel \"Action Pre-training, Pointcloud\nFine-tuning\" paradigm. Our method combines the broad generalization\ncapabilities of VLMs in visual understanding and planning with the\nfine-grained, action-level generalization of action expert.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u52a8\u4f5c\u4e13\u5bb6\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u7a00\u758f3D\u8f68\u8ff9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06VLM\u7684\u9ad8\u5c42\u89c4\u5212\u80fd\u529b\u4e0e\u4f4e\u5c42\u7269\u7406\u52a8\u4f5c\u6a21\u5757\u8fde\u63a5\u8d77\u6765\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfVLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5c06\u63a8\u7406\u80fd\u529b\u8f6c\u5316\u4e3a\u7269\u7406\u52a8\u4f5c\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u53cc\u7cfb\u7edf\u65b9\u6cd5\u53c8\u53d7\u5230\u52a8\u4f5c\u6a21\u5757\u8bed\u4e49\u6a21\u7cca\u6027\u7684\u9650\u5236\uff0c\u9700\u8981\u5728\u65b0\u73af\u5883\u4e2d\u91cd\u65b0\u6536\u96c6\u6570\u636e\u8bad\u7ec3\u3002", "method": "\u4f7f\u7528\u7a00\u758f3D\u8def\u5f84\u70b9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0cVLM\u751f\u6210\u7c97\u7565\u76843D\u8def\u5f84\u70b9\uff0c\u901a\u7528\u52a8\u4f5c\u4e13\u5bb6\u901a\u8fc7\u91c7\u6837\u5b9e\u65f6\u70b9\u4e91\u89c2\u6d4b\u5c06\u5176\u7ec6\u5316\u4e3a\u5bc6\u96c6\u53ef\u6267\u884c\u52a8\u4f5c\u5e8f\u5217\uff0c\u91c7\u7528\"\u52a8\u4f5c\u9884\u8bad\u7ec3\u3001\u70b9\u4e91\u5fae\u8c03\"\u7684\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86VLM\u5728\u89c6\u89c9\u7406\u89e3\u548c\u89c4\u5212\u65b9\u9762\u7684\u5e7f\u6cdb\u6cdb\u5316\u80fd\u529b\u4e0e\u52a8\u4f5c\u4e13\u5bb6\u5728\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u5c42\u9762\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u901a\u7528\u52a8\u4f5c\u4e13\u5bb6\u548c3D\u8f68\u8ff9\u4e2d\u95f4\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u4efb\u52a1\u8bad\u7ec3\u548c\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2510.04454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04454", "abs": "https://arxiv.org/abs/2510.04454", "authors": ["Xiangchi Yuan", "Xiang Chen", "Tong Yu", "Dachuan Shi", "Can Jin", "Wenke Lee", "Saayan Mitra"], "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners", "comment": null, "summary": "Large Language Models (LLMs) show strong reasoning abilities, often amplified\nby Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although\nRL algorithms can substantially improve reasoning, they struggle to expand\nreasoning boundaries because they learn from their own reasoning trajectories\nrather than acquiring external knowledge. Supervised fine-tuning (SFT) offers\ncomplementary benefits but typically requires large-scale data and risks\noverfitting. Recent attempts to combine SFT and RL face three main challenges:\ndata inefficiency, algorithm-specific designs, and catastrophic forgetting. We\npropose a plug-and-play framework that dynamically integrates SFT into RL by\nselecting challenging examples for SFT. This approach reduces SFT data\nrequirements and remains agnostic to the choice of RL or SFT algorithm. To\nmitigate catastrophic forgetting of RL-acquired skills during SFT, we select\nhigh-entropy tokens for loss calculation and freeze parameters identified as\ncritical for RL. Our method achieves state-of-the-art (SoTA) reasoning\nperformance using only 1.5% of the SFT data and 20.4% of the RL data used by\nprior SoTA, providing an efficient and plug-and-play solution for combining SFT\nand RL in reasoning post-training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u56f0\u96be\u6837\u672c\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06SFT\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408SFT\u548cRL\u9762\u4e34\u6570\u636e\u6548\u7387\u4f4e\u3001\u7b97\u6cd5\u7279\u5b9a\u8bbe\u8ba1\u548c\u707e\u96be\u6027\u9057\u5fd8\u4e09\u5927\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u65b9\u6848\u3002", "method": "\u52a8\u6001\u9009\u62e9\u56f0\u96be\u6837\u672c\u8fdb\u884cSFT\uff0c\u9009\u62e9\u9ad8\u71b5token\u8ba1\u7b97\u635f\u5931\uff0c\u5e76\u51bb\u7ed3\u5bf9RL\u91cd\u8981\u7684\u53c2\u6570\u3002", "result": "\u4ec5\u4f7f\u75281.5%\u7684SFT\u6570\u636e\u548c20.4%\u7684RL\u6570\u636e\u5c31\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a8\u7406\u540e\u8bad\u7ec3\u4e2d\u7ed3\u5408SFT\u548cRL\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03903", "abs": "https://arxiv.org/abs/2510.03903", "authors": ["Md. Atabuzzaman", "Andrew Zhang", "Chris Thomas"], "title": "Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non vision-language reasoning tasks. However, their potential for zero-shot\nfine-grained image classification, a challenging task requiring precise\ndifferentiation between visually similar categories, remains underexplored. We\npresent a novel method that transforms zero-shot fine-grained image\nclassification into a visual question-answering framework, leveraging LVLMs'\ncomprehensive understanding capabilities rather than relying on direct class\nname generation. We enhance model performance through a novel attention\nintervention technique. We also address a key limitation in existing datasets\nby developing more comprehensive and precise class description benchmarks. We\nvalidate the effectiveness of our method through extensive experimentation\nacross multiple fine-grained image classification benchmarks. Our proposed\nmethod consistently outperforms the current state-of-the-art (SOTA) approach,\ndemonstrating both the effectiveness of our method and the broader potential of\nLVLMs for zero-shot fine-grained classification tasks. Code and Datasets:\nhttps://github.com/Atabuzzaman/Fine-grained-classification", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8f6c\u5316\u4e3a\u89c6\u89c9\u95ee\u7b54\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5e72\u9884\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u9700\u8981\u7cbe\u786e\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u7c7b\u522b\u7684\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u8f6c\u5316\u4e3a\u89c6\u89c9\u95ee\u7b54\u6846\u67b6\uff0c\u5229\u7528LVLMs\u7684\u7efc\u5408\u7406\u89e3\u80fd\u529b\u800c\u975e\u76f4\u63a5\u751f\u6210\u7c7b\u522b\u540d\u79f0\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5e72\u9884\u6280\u672f\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u8fd8\u5c55\u793a\u4e86LVLMs\u5728\u96f6\u6837\u672c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u66f4\u5e7f\u6cdb\u6f5c\u529b\u3002"}}
{"id": "2510.04476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04476", "abs": "https://arxiv.org/abs/2510.04476", "authors": ["Tomas Figliolia", "Nicholas Alonso", "Rishi Iyer", "Quentin Anthony", "Beren Millidge"], "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space", "comment": null, "summary": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.", "AI": {"tldr": "CCA\u662f\u4e00\u79cd\u65b0\u578b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u4e0b\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u6765\u663e\u8457\u51cf\u5c11\u53c2\u6570\u3001KV\u7f13\u5b58\u548cFLOPs\uff0c\u7ed3\u5408GQA\u5f62\u6210CCGQA\u8fdb\u4e00\u6b65\u4f18\u5316\u8ba1\u7b97-\u5e26\u5bbd\u6743\u8861\u3002", "motivation": "\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u7ebf\u6027\u589e\u957f\u7684KV\u7f13\u5b58\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u670d\u52a1\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u65b9\u6cd5\u5982GQA\u548cMLA\u4e3b\u8981\u4f18\u5316\u7f13\u5b58\u4f46\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u6709\u9650\u3002", "method": "CCA\u5c06\u67e5\u8be2\u3001\u952e\u548c\u503c\u4e0b\u6295\u5f71\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u5185\u6267\u884c\u6574\u4e2a\u6ce8\u610f\u529b\u64cd\u4f5c\u3002CCA\u4e0e\u5934\u5171\u4eab\u6b63\u4ea4\uff0c\u7ed3\u5408GQA\u5f62\u6210CCGQA\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636eFLOP\u6216\u5185\u5b58\u9650\u5236\u8c03\u6574\u538b\u7f29\u7387\u3002", "result": "CCGQA\u5728\u76f8\u540cKV\u7f13\u5b58\u538b\u7f29\u4e0b\u4f18\u4e8eGQA\u548cMLA\uff0c\u5728MoE\u6a21\u578b\u4e0a\u4ee5GQA\u548cMLA\u4e00\u534a\u7684KV\u7f13\u5b58\u5b9e\u73b08\u500d\u538b\u7f29\u4e14\u6027\u80fd\u65e0\u635f\u5931\u3002CCA/CCGQA\u663e\u8457\u51cf\u5c11\u6ce8\u610f\u529bFLOP\u6210\u672c\uff0c\u5728H100 GPU\u4e0a16k\u5e8f\u5217\u957f\u5ea6\u65f6\u9884\u586b\u5145\u5ef6\u8fdf\u964d\u4f4e\u7ea61.7\u500d\uff0c\u53cd\u5411\u4f20\u64ad\u52a0\u901f\u7ea61.3\u500d\u3002", "conclusion": "CCA\u548cCCGQA\u901a\u8fc7\u538b\u7f29\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587Transformer\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2510.03906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03906", "abs": "https://arxiv.org/abs/2510.03906", "authors": ["Ardalan Aryashad", "Parsa Razmara", "Amin Mahjoub", "Seyedarmin Azizi", "Mahdi Salmani", "Arad Firouzkouhi"], "title": "From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance", "comment": null, "summary": "Autonomous driving perception systems are particularly vulnerable in foggy\nconditions, where light scattering reduces contrast and obscures fine details\ncritical for safe operation. While numerous defogging methods exist-from\nhandcrafted filters to learned restoration models-improvements in image\nfidelity do not consistently translate into better downstream detection and\nsegmentation. Moreover, prior evaluations often rely on synthetic data, leaving\nquestions about real-world transferability. We present a structured empirical\nstudy that benchmarks a comprehensive set of pipelines, including (i) classical\nfilters, (ii) modern defogging networks, (iii) chained variants\n(filter$\\rightarrow$model, model$\\rightarrow$filter), and (iv) prompt-driven\nvisual--language image editing models (VLM) applied directly to foggy images.\nUsing Foggy Cityscapes, we assess both image quality and downstream performance\non object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals\nwhen defogging helps, when chaining yields synergy or degradation, and how\nVLM-based editors compare to dedicated approaches. In addition, we evaluate\nqualitative rubric-based scores from a VLM judge and quantify their alignment\nwith task metrics, showing strong correlations with mAP. Together, these\nresults establish a transparent, task-oriented benchmark for defogging methods\nand highlight the conditions under which preprocessing genuinely improves\nautonomous perception in adverse weather.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u4e2d\u7684\u53bb\u96fe\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u4f20\u7edf\u6ee4\u6ce2\u5668\u3001\u73b0\u4ee3\u53bb\u96fe\u7f51\u7edc\u3001\u94fe\u5f0f\u7ec4\u5408\u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u591a\u79cd\u65b9\u6cd5\u5728\u96fe\u5929\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5728\u96fe\u5929\u6761\u4ef6\u4e0b\u7279\u522b\u8106\u5f31\uff0c\u73b0\u6709\u7684\u53bb\u96fe\u65b9\u6cd5\u5728\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0a\u7684\u6539\u8fdb\u5e76\u4e0d\u603b\u80fd\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\uff0c\u4e14\u5148\u524d\u8bc4\u4f30\u591a\u4f9d\u8d56\u5408\u6210\u6570\u636e\uff0c\u771f\u5b9e\u4e16\u754c\u9002\u7528\u6027\u5b58\u7591\u3002", "method": "\u4f7f\u7528Foggy Cityscapes\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4f20\u7edf\u6ee4\u6ce2\u5668\u3001\u73b0\u4ee3\u53bb\u96fe\u7f51\u7edc\u3001\u94fe\u5f0f\u7ec4\u5408\uff08\u6ee4\u6ce2\u5668\u2192\u6a21\u578b\u3001\u6a21\u578b\u2192\u6ee4\u6ce2\u5668\uff09\u4ee5\u53ca\u57fa\u4e8e\u63d0\u793a\u7684\u89c6\u89c9\u8bed\u8a00\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7b49\u591a\u79cd\u53bb\u96fe\u6d41\u6c34\u7ebf\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u53bb\u96fe\u4f55\u65f6\u6709\u6548\u3001\u94fe\u5f0f\u7ec4\u5408\u4f55\u65f6\u4ea7\u751f\u534f\u540c\u6216\u9000\u5316\u6548\u5e94\uff0c\u4ee5\u53caVLM\u7f16\u8f91\u6a21\u578b\u4e0e\u4e13\u7528\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002VLM\u6cd5\u5b98\u8bc4\u5206\u4e0e\u4efb\u52a1\u6307\u6807\uff08\u7279\u522b\u662fmAP\uff09\u663e\u793a\u51fa\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53bb\u96fe\u65b9\u6cd5\u5efa\u7acb\u4e86\u900f\u660e\u3001\u4efb\u52a1\u5bfc\u5411\u7684\u57fa\u51c6\uff0c\u5e76\u660e\u786e\u4e86\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u9884\u5904\u7406\u80fd\u771f\u6b63\u6539\u5584\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2510.04484", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04484", "abs": "https://arxiv.org/abs/2510.04484", "authors": ["Amin Banayeeanzade", "Ala N. Tak", "Fatemeh Bahrani", "Anahita Bolourani", "Leonardo Blas", "Emilio Ferrara", "Jonathan Gratch", "Sai Praneeth Karimireddy"], "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness", "comment": "Submitted to ARR - October 2025", "summary": "The ability to control LLMs' emulated emotional states and personality traits\nis essential for enabling rich, human-centered interactions in socially\ninteractive settings. We introduce PsySET, a Psychologically-informed benchmark\nto evaluate LLM Steering Effectiveness and Trustworthiness across the emotion\nand personality domains. Our study spans four models from different LLM\nfamilies paired with various steering strategies, including prompting,\nfine-tuning, and representation engineering. Our results indicate that\nprompting is consistently effective but limited in intensity control, whereas\nvector injections achieve finer controllability while slightly reducing output\nquality. Moreover, we explore the trustworthiness of steered LLMs by assessing\nsafety, truthfulness, fairness, and ethics, highlighting potential side effects\nand behavioral shifts. Notably, we observe idiosyncratic effects; for instance,\neven a positive emotion like joy can degrade robustness to adversarial\nfactuality, lower privacy awareness, and increase preferential bias. Meanwhile,\nanger predictably elevates toxicity yet strengthens leakage resistance. Our\nframework establishes the first holistic evaluation of emotion and personality\nsteering, offering insights into its interpretability and reliability for\nsocially interactive applications.", "AI": {"tldr": "PsySET\u662f\u4e00\u4e2a\u5fc3\u7406\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u60c5\u7eea\u548c\u4eba\u683c\u9886\u57df\u7684\u5f15\u5bfc\u6548\u679c\u548c\u53ef\u4fe1\u5ea6\uff0c\u53d1\u73b0\u63d0\u793a\u6cd5\u6709\u6548\u4f46\u5f3a\u5ea6\u63a7\u5236\u6709\u9650\uff0c\u5411\u91cf\u6ce8\u5165\u53ef\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u63a7\u5236\u4f46\u4f1a\u8f7b\u5fae\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u60c5\u7eea\u5f15\u5bfc\u7684\u72ec\u7279\u526f\u4f5c\u7528\u3002", "motivation": "\u63a7\u5236LLM\u7684\u6a21\u62df\u60c5\u7eea\u72b6\u6001\u548c\u4eba\u683c\u7279\u8d28\u5bf9\u4e8e\u5728\u793e\u4ea4\u4e92\u52a8\u73af\u5883\u4e2d\u5b9e\u73b0\u4e30\u5bcc\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528PsySET\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u56db\u4e2a\u4e0d\u540cLLM\u5bb6\u65cf\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u79cd\u5f15\u5bfc\u7b56\u7565\uff08\u63d0\u793a\u6cd5\u3001\u5fae\u8c03\u3001\u8868\u793a\u5de5\u7a0b\uff09\uff0c\u8bc4\u4f30\u5b89\u5168\u3001\u771f\u5b9e\u3001\u516c\u5e73\u548c\u4f26\u7406\u7b49\u53ef\u4fe1\u5ea6\u7ef4\u5ea6\u3002", "result": "\u63d0\u793a\u6cd5\u6301\u7eed\u6709\u6548\u4f46\u5f3a\u5ea6\u63a7\u5236\u6709\u9650\uff1b\u5411\u91cf\u6ce8\u5165\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u53ef\u63a7\u6027\u4f46\u8f7b\u5fae\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\uff1b\u53d1\u73b0\u72ec\u7279\u6548\u5e94\uff1a\u79ef\u6781\u60c5\u7eea\u5982\u559c\u60a6\u4f1a\u964d\u4f4e\u5bf9\u5bf9\u6297\u6027\u4e8b\u5b9e\u7684\u9c81\u68d2\u6027\u3001\u964d\u4f4e\u9690\u79c1\u610f\u8bc6\u3001\u589e\u52a0\u504f\u597d\u504f\u89c1\uff1b\u6124\u6012\u53ef\u9884\u6d4b\u5730\u589e\u52a0\u6bd2\u6027\u4f46\u589e\u5f3a\u6cc4\u6f0f\u62b5\u6297\u3002", "conclusion": "\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u9996\u4e2a\u5bf9\u60c5\u7eea\u548c\u4eba\u683c\u5f15\u5bfc\u7684\u6574\u4f53\u8bc4\u4f30\uff0c\u4e3a\u793e\u4ea4\u4e92\u52a8\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.03909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03909", "abs": "https://arxiv.org/abs/2510.03909", "authors": ["Hyelin Nam", "Hyojun Go", "Byeongjun Park", "Byung-Hoon Kim", "Hyungjin Chung"], "title": "Generating Human Motion Videos using a Cascaded Text-to-Video Framework", "comment": "18 pages, 7 figures, Project Page:https://hyelinnam.github.io/Cameo/", "summary": "Human video generation is becoming an increasingly important task with broad\napplications in graphics, entertainment, and embodied AI. Despite the rapid\nprogress of video diffusion models (VDMs), their use for general-purpose human\nvideo generation remains underexplored, with most works constrained to\nimage-to-video setups or narrow domains like dance videos. In this work, we\npropose CAMEO, a cascaded framework for general human motion video generation.\nIt seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,\nmitigating suboptimal factors that may arise in this process across both\ntraining and inference through carefully designed components. Specifically, we\nanalyze and prepare both textual prompts and visual conditions to effectively\ntrain the VDM, ensuring robust alignment between motion descriptions,\nconditioning signals, and the generated videos. Furthermore, we introduce a\ncamera-aware conditioning module that connects the two stages, automatically\nselecting viewpoints aligned with the input text to enhance coherence and\nreduce manual intervention. We demonstrate the effectiveness of our approach on\nboth the MovieGen benchmark and a newly introduced benchmark tailored to the\nT2M-VDM combination, while highlighting its versatility across diverse use\ncases.", "AI": {"tldr": "CAMEO\u662f\u4e00\u4e2a\u7528\u4e8e\u901a\u7528\u4eba\u4f53\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u6865\u63a5\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\u548c\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\u7684\u6b21\u4f18\u56e0\u7d20\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u9891\u6269\u6563\u6a21\u578b\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5728\u901a\u7528\u4eba\u4f53\u89c6\u9891\u751f\u6210\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5927\u591a\u6570\u5de5\u4f5c\u5c40\u9650\u4e8e\u56fe\u50cf\u5230\u89c6\u9891\u8bbe\u7f6e\u6216\u821e\u8e48\u89c6\u9891\u7b49\u72ed\u7a84\u9886\u57df\u3002", "method": "\u63d0\u51faCAMEO\u6846\u67b6\uff0c\u5206\u6790\u5e76\u51c6\u5907\u6587\u672c\u63d0\u793a\u548c\u89c6\u89c9\u6761\u4ef6\u6765\u6709\u6548\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u76f8\u673a\u611f\u77e5\u6761\u4ef6\u6a21\u5757\u8fde\u63a5\u4e24\u4e2a\u9636\u6bb5\uff0c\u81ea\u52a8\u9009\u62e9\u4e0e\u8f93\u5165\u6587\u672c\u5bf9\u9f50\u7684\u89c6\u89d2\u3002", "result": "\u5728MovieGen\u57fa\u51c6\u548c\u65b0\u5f15\u5165\u7684T2M-VDM\u7ec4\u5408\u57fa\u51c6\u4e0a\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u7528\u4f8b\u4e2d\u7684\u591a\u529f\u80fd\u6027\u3002", "conclusion": "CAMEO\u6846\u67b6\u80fd\u591f\u65e0\u7f1d\u6865\u63a5\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\u548c\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7ec4\u4ef6\u589e\u5f3a\u4e86\u8fd0\u52a8\u63cf\u8ff0\u3001\u6761\u4ef6\u4fe1\u53f7\u548c\u751f\u6210\u89c6\u9891\u4e4b\u95f4\u7684\u9c81\u68d2\u5bf9\u9f50\u3002"}}
{"id": "2510.04498", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04498", "abs": "https://arxiv.org/abs/2510.04498", "authors": ["Qiao Wang", "Adnan Labib", "Robert Swier", "Michael Hofmeyr", "Zheng Yuan"], "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners", "comment": "Workshop on Wordplay: When Language Meets Games, EMNLP 2025", "summary": "GenQuest is a generative text adventure game that leverages Large Language\nModels (LLMs) to facilitate second language learning through immersive,\ninteractive storytelling. The system engages English as a Foreign Language\n(EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative,\ndynamically generated in response to learner choices. Game mechanics such as\nbranching decision points and story milestones are incorporated to maintain\nnarrative coherence while allowing learner-driven plot development. Key\npedagogical features include content generation tailored to each learner's\nproficiency level, and a vocabulary assistant that provides in-context\nexplanations of learner-queried text strings, ranging from words and phrases to\nsentences. Findings from a pilot study with university EFL students in China\nindicate promising vocabulary gains and positive user perceptions. Also\ndiscussed are suggestions from participants regarding the narrative length and\nquality, and the request for multi-modal content such as illustrations.", "AI": {"tldr": "GenQuest\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u5f0f\u6587\u672c\u5192\u9669\u6e38\u620f\uff0c\u901a\u8fc7\u6c89\u6d78\u5f0f\u4e92\u52a8\u6545\u4e8b\u4fc3\u8fdb\u7b2c\u4e8c\u8bed\u8a00\u5b66\u4e60\uff0c\u4e3aEFL\u5b66\u4e60\u8005\u63d0\u4f9b\u4e2a\u6027\u5316\u5185\u5bb9\u548c\u8bcd\u6c47\u8f85\u52a9\u529f\u80fd\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u6c89\u6d78\u5f0f\u4e92\u52a8\u6545\u4e8b\u73af\u5883\uff0c\u901a\u8fc7\u6e38\u620f\u5316\u65b9\u5f0f\u63d0\u5347EFL\u5b66\u4e60\u8005\u7684\u8bed\u8a00\u5b66\u4e60\u6548\u679c\uff0c\u7279\u522b\u662f\u8bcd\u6c47\u4e60\u5f97\u548c\u8bed\u8a00\u8fd0\u7528\u80fd\u529b\u3002", "method": "\u91c7\u7528\"\u9009\u62e9\u4f60\u81ea\u5df1\u7684\u5192\u9669\"\u5f0f\u53d9\u4e8b\u7ed3\u6784\uff0c\u52a8\u6001\u751f\u6210\u54cd\u5e94\u5b66\u4e60\u8005\u9009\u62e9\u7684\u6545\u4e8b\u60c5\u8282\uff0c\u5305\u542b\u5206\u652f\u51b3\u7b56\u70b9\u548c\u6545\u4e8b\u91cc\u7a0b\u7891\uff0c\u63d0\u4f9b\u57fa\u4e8e\u5b66\u4e60\u8005\u6c34\u5e73\u7684\u4e2a\u6027\u5316\u5185\u5bb9\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u8bcd\u6c47\u89e3\u91ca\u529f\u80fd\u3002", "result": "\u5bf9\u4e2d\u56fd\u5927\u5b66EFL\u5b66\u751f\u7684\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8bcd\u6c47\u589e\u76ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u7528\u6237\u611f\u77e5\u79ef\u6781\uff0c\u540c\u65f6\u53c2\u4e0e\u8005\u5efa\u8bae\u6539\u8fdb\u53d9\u4e8b\u957f\u5ea6\u548c\u8d28\u91cf\uff0c\u5e76\u589e\u52a0\u591a\u6a21\u6001\u5185\u5bb9\u5982\u56fe\u50cf\u3002", "conclusion": "GenQuest\u5c55\u793a\u4e86\u751f\u6210\u5f0f\u6587\u672c\u5192\u9669\u6e38\u620f\u5728EFL\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u6c89\u6d78\u5f0f\u4e92\u52a8\u53d9\u4e8b\u6709\u6548\u4fc3\u8fdb\u8bcd\u6c47\u5b66\u4e60\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u53d9\u4e8b\u8d28\u91cf\u548c\u591a\u6a21\u6001\u96c6\u6210\u3002"}}
{"id": "2510.03915", "categories": ["cs.CV", "cs.DC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03915", "abs": "https://arxiv.org/abs/2510.03915", "authors": ["Sagar Bharadwaj", "Harrison Williams", "Luke Wang", "Michael Liang", "Tao Jin", "Srinivasan Seshan", "Anthony Rowe"], "title": "OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications", "comment": null, "summary": "World-scale augmented reality (AR) applications need a ubiquitous 6DoF\nlocalization backend to anchor content to the real world consistently across\ndevices. Large organizations such as Google and Niantic are 3D scanning outdoor\npublic spaces in order to build their own Visual Positioning Systems (VPS).\nThese centralized VPS solutions fail to meet the needs of many future AR\napplications -- they do not cover private indoor spaces because of privacy\nconcerns, regulations, and the labor bottleneck of updating and maintaining 3D\nscans. In this paper, we present OpenFLAME, a federated VPS backend that allows\nindependent organizations to 3D scan and maintain a separate VPS service for\ntheir own spaces. This enables access control of indoor 3D scans, distributed\nmaintenance of the VPS backend, and encourages larger coverage. Sharding of VPS\nservices introduces several unique challenges -- coherency of localization\nresults across spaces, quality control of VPS services, selection of the right\nVPS service for a location, and many others. We introduce the concept of\nfederated image-based localization and provide reference solutions for managing\nand merging data across maps without sharing private data.", "AI": {"tldr": "OpenFLAME\u662f\u4e00\u4e2a\u8054\u90a6\u89c6\u89c9\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u65b9\u5f0f\u8ba9\u5404\u7ec4\u7ec7\u72ec\u7acb\u7ef4\u62a4\u81ea\u5df1\u7684VPS\u670d\u52a1\uff0c\u89e3\u51b3\u96c6\u4e2d\u5f0fVPS\u5728\u9690\u79c1\u3001\u76d1\u7ba1\u548c\u7ef4\u62a4\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u96c6\u4e2d\u5f0fVPS\u65e0\u6cd5\u8986\u76d6\u79c1\u4eba\u5ba4\u5185\u7a7a\u95f4\uff0c\u5b58\u5728\u9690\u79c1\u62c5\u5fe7\u3001\u76d1\u7ba1\u9650\u5236\u548c\u7ef4\u62a4\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u6765\u6269\u5927\u8986\u76d6\u8303\u56f4\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "method": "\u91c7\u7528\u8054\u90a6\u56fe\u50cf\u5b9a\u4f4d\u6982\u5ff5\uff0c\u5404\u7ec4\u7ec7\u72ec\u7acb3D\u626b\u63cf\u548c\u7ef4\u62a4\u81ea\u5df1\u7684VPS\u670d\u52a1\uff0c\u901a\u8fc7\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u7ba1\u7406\u548c\u5408\u5e76\u8de8\u5730\u56fe\u6570\u636e\u800c\u4e0d\u5171\u4eab\u79c1\u6709\u6570\u636e\u3002", "result": "\u5b9e\u73b0\u4e86\u5206\u5e03\u5f0fVPS\u540e\u7aef\uff0c\u652f\u6301\u5ba4\u51853D\u626b\u63cf\u7684\u8bbf\u95ee\u63a7\u5236\u3001\u5206\u5e03\u5f0f\u7ef4\u62a4\u548c\u66f4\u5927\u8986\u76d6\u8303\u56f4\u3002", "conclusion": "\u8054\u90a6VPS\u65b9\u6cd5\u80fd\u591f\u89e3\u51b3\u96c6\u4e2d\u5f0f\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u66f4\u5927\u8303\u56f4\u7684AR\u5b9a\u4f4d\u8986\u76d6\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u548c\u5b9e\u73b0\u5206\u5e03\u5f0f\u7ef4\u62a4\u3002"}}
{"id": "2510.04506", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04506", "abs": "https://arxiv.org/abs/2510.04506", "authors": ["Jiashuo Sun", "Shixuan Liu", "Zhaochen Su", "Xianrui Zhong", "Pengcheng Jiang", "Bowen Jin", "Peiran Li", "Weijia Shi", "Jiawei Han"], "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization", "comment": "23 pages, 7 figures, 7 tables", "summary": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE.", "AI": {"tldr": "GRACE\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u5bf9\u6bd4\u4fe1\u53f7\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5956\u52b1\u6765\u6307\u5bfc\u751f\u6210\u7b56\u7565\uff0c\u4f7fLLM\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u5b66\u4e60\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5d4c\u5165\u3002", "motivation": "\u73b0\u6709\u7684LLM\u6587\u672c\u7f16\u7801\u5668\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u6bd4\u635f\u5931\uff0c\u5c06\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\u51fd\u6570\uff0c\u4e22\u5f03\u4e86\u5176\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4ec5\u4ea7\u751f\u9759\u6001\u5d4c\u5165\u3002", "method": "\u4f7f\u7528\u7b56\u7565\u68af\u5ea6\u4f18\u5316\uff0c\u901a\u8fc7\u591a\u7ec4\u4ef6\u5956\u52b1\u51fd\u6570\u8bad\u7ec3\u6a21\u578b\uff0c\u6700\u5927\u5316\u6b63\u6837\u672c\u5bf9\u7684\u76f8\u4f3c\u5ea6\uff0c\u6700\u5c0f\u5316\u8d1f\u6837\u672c\u5bf9\u7684\u76f8\u4f3c\u5ea6\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76d1\u7763\u8bbe\u7f6e\u4e0b\u6574\u4f53\u5f97\u5206\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad811.5%\uff0c\u65e0\u76d1\u7763\u53d8\u4f53\u63d0\u9ad86.9%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "GRACE\u5c06\u5bf9\u6bd4\u76ee\u6807\u89c6\u4e3a\u63a8\u7406\u8fc7\u7a0b\u7684\u5956\u52b1\uff0c\u7edf\u4e00\u4e86\u8868\u793a\u5b66\u4e60\u548c\u751f\u6210\uff0c\u4ea7\u751f\u66f4\u5f3a\u7684\u5d4c\u5165\u548c\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2510.03921", "categories": ["cs.CV", "cs.AI", "cs.HC", "I.2.10; I.5.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.03921", "abs": "https://arxiv.org/abs/2510.03921", "authors": ["Arushi Dashore", "Aryan Anumala", "Emily Hui", "Olivia Yang"], "title": "Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition", "comment": "10 pages, 4 figures, 2 tables", "summary": "Automated tennis stroke analysis has advanced significantly with the\nintegration of biomechanical motion cues alongside deep learning techniques,\nenhancing stroke classification accuracy and player performance evaluation.\nDespite these advancements, existing systems often fail to connect\nbiomechanical insights with actionable language feedback that is both\naccessible and meaningful to players and coaches. This research project\naddresses this gap by developing a novel framework that extracts key\nbiomechanical features (such as joint angles, limb velocities, and kinetic\nchain patterns) from motion data using Convolutional Neural Network Long\nShort-Term Memory (CNN-LSTM)-based models. These features are analyzed for\nrelationships influencing stroke effectiveness and injury risk, forming the\nbasis for feedback generation using large language models (LLMs). Leveraging\nthe THETIS dataset and feature extraction techniques, our approach aims to\nproduce feedback that is technically accurate, biomechanically grounded, and\nactionable for end-users. The experimental setup evaluates this framework on\nclassification performance and interpretability, bridging the gap between\nexplainable AI and sports biomechanics.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408CNN-LSTM\u6a21\u578b\u63d0\u53d6\u751f\u7269\u529b\u5b66\u7279\u5f81\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u9988\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7f51\u7403\u51fb\u7403\u5206\u6790\uff0c\u65e8\u5728\u63d0\u4f9b\u6280\u672f\u4e0a\u51c6\u786e\u4e14\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u867d\u7136\u6574\u5408\u4e86\u751f\u7269\u529b\u5b66\u8fd0\u52a8\u7ebf\u7d22\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u4f46\u672a\u80fd\u5c06\u751f\u7269\u529b\u5b66\u6d1e\u5bdf\u8f6c\u5316\u4e3a\u5bf9\u7403\u5458\u548c\u6559\u7ec3\u65e2\u6613\u4e8e\u7406\u89e3\u53c8\u6709\u610f\u4e49\u7684\u53ef\u64cd\u4f5c\u8bed\u8a00\u53cd\u9988\u3002", "method": "\u4f7f\u7528CNN-LSTM\u6a21\u578b\u4ece\u8fd0\u52a8\u6570\u636e\u4e2d\u63d0\u53d6\u5173\u952e\u751f\u7269\u529b\u5b66\u7279\u5f81\uff08\u5982\u5173\u8282\u89d2\u5ea6\u3001\u80a2\u4f53\u901f\u5ea6\u548c\u52a8\u529b\u94fe\u6a21\u5f0f\uff09\uff0c\u5206\u6790\u8fd9\u4e9b\u7279\u5f81\u4e0e\u51fb\u7403\u6548\u679c\u548c\u53d7\u4f24\u98ce\u9669\u7684\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u9988\u3002", "result": "\u8be5\u6846\u67b6\u5728THETIS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e8\u5728\u5f25\u5408\u53ef\u89e3\u91caAI\u4e0e\u8fd0\u52a8\u751f\u7269\u529b\u5b66\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u6280\u672f\u4e0a\u51c6\u786e\u3001\u57fa\u4e8e\u751f\u7269\u529b\u5b66\u4e14\u5bf9\u6700\u7ec8\u7528\u6237\u53ef\u64cd\u4f5c\u7684\u53cd\u9988\uff0c\u4e3a\u7f51\u7403\u51fb\u7403\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04551", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04551", "abs": "https://arxiv.org/abs/2510.04551", "authors": ["Mario Almagro", "Diego Ortego", "David Jimenez"], "title": "Fine-grained auxiliary learning for real-world product recommendation", "comment": "SEPLN 2025", "summary": "Product recommendation is the task of recovering the closest items to a given\nquery within a large product corpora. Generally, one can determine if\ntop-ranked products are related to the query by applying a similarity\nthreshold; exceeding it deems the product relevant, otherwise manual revision\nis required. Despite being a well-known problem, the integration of these\nmodels in real-world systems is often overlooked. In particular, production\nsystems have strong coverage requirements, i.e., a high proportion of\nrecommendations must be automated. In this paper we propose ALC , an Auxiliary\nLearning strategy that boosts Coverage through learning fine-grained\nembeddings. Concretely, we introduce two training objectives that leverage the\nhardest negatives in the batch to build discriminative training signals between\npositives and negatives. We validate ALC using three extreme multi-label\nclassification approaches in two product recommendation datasets;\nLF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating\nstate-of-the-art coverage rates when combined with a recent\nthreshold-consistent margin loss.", "AI": {"tldr": "\u63d0\u51faALC\u8f85\u52a9\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5d4c\u5165\u63d0\u5347\u4ea7\u54c1\u63a8\u8350\u7684\u8986\u76d6\u7387\uff0c\u5728\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8986\u76d6\u7387\u3002", "motivation": "\u5b9e\u9645\u751f\u4ea7\u7cfb\u7edf\u6709\u5f3a\u8986\u76d6\u7387\u8981\u6c42\uff0c\u5373\u9ad8\u6bd4\u4f8b\u63a8\u8350\u5fc5\u987b\u81ea\u52a8\u5316\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u5e38\u88ab\u5ffd\u89c6\u3002", "method": "\u5f15\u5165\u4e24\u79cd\u8bad\u7ec3\u76ee\u6807\uff0c\u5229\u7528\u6279\u6b21\u4e2d\u6700\u96be\u7684\u8d1f\u6837\u672c\u6765\u6784\u5efa\u6b63\u8d1f\u6837\u672c\u95f4\u7684\u5224\u522b\u6027\u8bad\u7ec3\u4fe1\u53f7\uff0c\u7ed3\u5408\u9608\u503c\u4e00\u81f4\u6027\u8fb9\u754c\u635f\u5931\u3002", "result": "\u5728LF-AmazonTitles-131K\u548cTech and Durables\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4e0e\u4e09\u79cd\u6781\u7aef\u591a\u6807\u7b7e\u5206\u7c7b\u65b9\u6cd5\u7ed3\u5408\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u8986\u76d6\u7387\u3002", "conclusion": "ALC\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u4ea7\u54c1\u63a8\u8350\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u8986\u76d6\u7387\uff0c\u6ee1\u8db3\u751f\u4ea7\u7cfb\u7edf\u7684\u5b9e\u9645\u9700\u6c42\u3002"}}
{"id": "2510.03955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03955", "abs": "https://arxiv.org/abs/2510.03955", "authors": ["Sameep Vani", "Shreyas Jena", "Maitreya Patel", "Chitta Baral", "Somak Aditya", "Yezhou Yang"], "title": "Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs", "comment": "17 pages, 9 figures, 6 tables. Presents TimeWarp, a synthetic\n  preference data framework to improve temporal understanding in Video-LLMs,\n  showing consistent gains across seven benchmarks. Includes supplementary\n  material in the Appendix", "summary": "While Video Large Language Models (Video-LLMs) have demonstrated remarkable\nperformance across general video understanding benchmarks-particularly in video\ncaptioning and descriptive tasks-they consistently underperform on tasks that\nrequire fine-grained temporal understanding. This limitation arises due to the\nlack of visual complexity and temporal nuance in current fine-tuning datasets,\nleading these models to rely heavily on language-based reasoning rather than\ntruly understanding video dynamics. In this work, we propose TimeWarp, a\nsystematic method to create a targeted synthetic temporal dataset to fine-tune\nthe model's responses to encourage it to focus on the given input video. We\nintroduce a large-scale preference dataset, created using TimeWarp, that\ncaptures intricate temporal dynamics often overlooked, grounding the model's\nresponses to visual and temporal information. We demonstrate that when our\nmethod is applied to existing models, it significantly improves performance on\ntemporal understanding benchmarks, highlighting the effectiveness of our\nproposed datasets in advancing temporal understanding in Video-LLMs, resulting\nin an absolute improvement in performance across seven benchmarks. Code is\navailable at https://github.com/sameepv21/timewarp.", "AI": {"tldr": "TimeWarp\u65b9\u6cd5\u901a\u8fc7\u521b\u5efa\u9488\u5bf9\u6027\u7684\u5408\u6210\u65f6\u95f4\u6570\u636e\u96c6\u6765\u6539\u8fdb\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5fae\u8c03\u6570\u636e\u96c6\u7f3a\u4e4f\u89c6\u89c9\u590d\u6742\u6027\u548c\u65f6\u95f4\u7ec6\u8282\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u63a8\u7406\u800c\u975e\u771f\u6b63\u7406\u89e3\u89c6\u9891\u52a8\u6001\u3002", "method": "\u63d0\u51faTimeWarp\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u521b\u5efa\u76ee\u6807\u5408\u6210\u65f6\u95f4\u6570\u636e\u96c6\u6765\u5fae\u8c03\u6a21\u578b\u54cd\u5e94\uff0c\u4f7f\u5176\u4e13\u6ce8\u4e8e\u8f93\u5165\u89c6\u9891\u3002\u5f15\u5165\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6\uff0c\u6355\u6349\u5e38\u88ab\u5ffd\u7565\u7684\u590d\u6742\u65f6\u95f4\u52a8\u6001\uff0c\u5c06\u6a21\u578b\u54cd\u5e94\u951a\u5b9a\u5728\u89c6\u89c9\u548c\u65f6\u95f4\u4fe1\u606f\u4e0a\u3002", "result": "\u5e94\u7528\u8be5\u65b9\u6cd5\u540e\uff0c\u73b0\u6709\u6a21\u578b\u5728\u65f6\u95f4\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u7edd\u5bf9\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "TimeWarp\u65b9\u6cd5\u901a\u8fc7\u9488\u5bf9\u6027\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u6570\u636e\u96c6\u5728\u63a8\u8fdb\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u95f4\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.04581", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04581", "abs": "https://arxiv.org/abs/2510.04581", "authors": ["Dang Anh", "Rick Nouwen", "Massimo Poesio"], "title": "Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference", "comment": null, "summary": "Our goal is to study how LLMs represent and interpret plural reference in\nambiguous and unambiguous contexts. We ask the following research questions:\n(1) Do LLMs exhibit human-like preferences in representing plural reference?\n(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and\nidentify possible referents? To address these questions, we design a set of\nexperiments, examining pronoun production using next-token prediction tasks,\npronoun interpretation, and ambiguity detection using different prompting\nstrategies. We then assess how comparable LLMs are to humans in formulating and\ninterpreting plural reference. We find that LLMs are sometimes aware of\npossible referents of ambiguous pronouns. However, they do not always follow\nhuman reference when choosing between interpretations, especially when the\npossible interpretation is not explicitly mentioned. In addition, they struggle\nto identify ambiguity without direct instruction. Our findings also reveal\ninconsistencies in the results across different types of experiments.", "AI": {"tldr": "\u7814\u7a76LLMs\u5982\u4f55\u8868\u793a\u548c\u89e3\u91ca\u590d\u6570\u6307\u79f0\uff0c\u53d1\u73b0LLMs\u6709\u65f6\u80fd\u8bc6\u522b\u6a21\u7cca\u4ee3\u8bcd\u7684\u6307\u79f0\u5bf9\u8c61\uff0c\u4f46\u5728\u9009\u62e9\u89e3\u91ca\u65f6\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u4e14\u96be\u4ee5\u8bc6\u522b\u6a21\u7cca\u6027\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u6a21\u7cca\u548c\u660e\u786e\u8bed\u5883\u4e2d\u8868\u793a\u548c\u89e3\u91ca\u590d\u6570\u6307\u79f0\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u7684\u504f\u597d\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\uff0c\u5305\u62ec\u4f7f\u7528\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u4efb\u52a1\u7684\u4ee3\u8bcd\u751f\u6210\u3001\u4ee3\u8bcd\u89e3\u91ca\u548c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u7684\u6a21\u7cca\u6027\u68c0\u6d4b\u3002", "result": "LLMs\u6709\u65f6\u80fd\u8bc6\u522b\u6a21\u7cca\u4ee3\u8bcd\u7684\u6307\u79f0\u5bf9\u8c61\uff0c\u4f46\u5728\u9009\u62e9\u89e3\u91ca\u65f6\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u5c24\u5176\u662f\u5728\u53ef\u80fd\u89e3\u91ca\u672a\u88ab\u660e\u786e\u63d0\u53ca\u65f6\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u96be\u4ee5\u5728\u6ca1\u6709\u76f4\u63a5\u6307\u4ee4\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u6a21\u7cca\u6027\u3002", "conclusion": "LLMs\u5728\u590d\u6570\u6307\u79f0\u8868\u793a\u548c\u89e3\u91ca\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\uff0c\u7ed3\u679c\u5728\u4e0d\u540c\u7c7b\u578b\u5b9e\u9a8c\u4e2d\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.03978", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03978", "abs": "https://arxiv.org/abs/2510.03978", "authors": ["Min Woo Sun", "Alejandro Lozano", "Javier Gamazo Tejero", "Vishwesh Nath", "Xiao Xiao Sun", "James Burgess", "Yuhui Zhang", "Kun Yuan", "Robert Tibshirani", "Sean Huver", "Serena Yeung-Levy"], "title": "No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models", "comment": null, "summary": "Embedding vision-language models (VLMs) are typically pretrained with short\ntext windows (<77 tokens), which forces the truncation of long-format captions.\nYet, the distribution of biomedical captions from large-scale open source\nliterature reveals that a huge portion of captions far exceed 77 tokens. To\nthis end, we investigate the impact of pretraining on long-format biomedical\ncaptions by extending the context length of text encoders in VLMs. We find that\nlonger context (thus, enabling additional supervision provided in long-format\ncaptions) correlates with better retrieval and classification performance.\nGiven this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M\nimage-caption pairs enriched with context-aware descriptions from full-text\narticles, providing longer and additional textual supervision. Using\nBIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a\ntext encoder supporting windows of up to 512 tokens. Our model extends context\ncapacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption\nretrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in\nRecall@1 and +2% average improvements in classification, while also converging\nfaster than short-context. Our results demonstrate that long-context modeling\nis a promising direction for advancing biomedical VLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6269\u5c55\u6587\u672c\u7f16\u7801\u5668\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u548c\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86BIOMEDICA-LongCAP\u6570\u636e\u96c6\u548cBMC-LongCLIP\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u77ed\u6587\u672c\u7a97\u53e3\uff08<77\u4e2atoken\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u957f\u683c\u5f0f\u751f\u7269\u533b\u5b66\u63cf\u8ff0\u88ab\u622a\u65ad\u3002\u7136\u800c\uff0c\u5927\u89c4\u6a21\u5f00\u6e90\u6587\u732e\u4e2d\u7684\u751f\u7269\u533b\u5b66\u63cf\u8ff0\u6709\u5f88\u5927\u90e8\u5206\u8fdc\u8d8577\u4e2atoken\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u6269\u5c55\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6587\u672c\u7f16\u7801\u5668\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u521b\u5efaBIOMEDICA-LongCAP\u6570\u636e\u96c6\uff08\u5305\u542b100\u4e07\u5f20\u56fe\u50cf-\u63cf\u8ff0\u5bf9\uff0c\u5177\u6709\u6765\u81ea\u5168\u6587\u6587\u7ae0\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u63cf\u8ff0\uff09\uff0c\u5e76\u8bad\u7ec3BMC-LongCLIP\u6a21\u578b\uff0c\u5176\u6587\u672c\u7f16\u7801\u5668\u652f\u6301\u6700\u591a512\u4e2atoken\u7684\u7a97\u53e3\u3002", "result": "BMC-LongCLIP\u5c06\u4e0a\u4e0b\u6587\u5bb9\u91cf\u6269\u5c55\u4e866.6\u500d\uff0ctoken\u6d6a\u8d39\u4ece55%\u51cf\u5c11\u52302.2%\u3002\u5728\u957f\u63cf\u8ff0\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRecall@1\u5b9e\u73b0\u4e86\u9ad8\u8fbe+30%\u7684\u7edd\u5bf9\u589e\u76ca\uff0c\u5206\u7c7b\u5e73\u5747\u63d0\u5347+2%\uff0c\u4e14\u6536\u655b\u901f\u5ea6\u6bd4\u77ed\u4e0a\u4e0b\u6587\u6a21\u578b\u66f4\u5feb\u3002", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u662f\u63a8\u8fdb\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u957f\u683c\u5f0f\u63cf\u8ff0\u4e2d\u7684\u989d\u5916\u76d1\u7763\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.04584", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2510.04584", "abs": "https://arxiv.org/abs/2510.04584", "authors": ["Fernando L\u00f3pez", "Santosh Kesiraju", "Jordi Luque"], "title": "Robustness assessment of large audio language models in multiple-choice evaluation", "comment": "Submitted to ICASSP 2026", "summary": "Recent advances in large audio language models (LALMs) have primarily been\nassessed using a multiple-choice question answering (MCQA) framework. However,\nsubtle changes, such as shifting the order of choices, result in substantially\ndifferent results. Existing MCQA frameworks do not account for this variability\nand report a single accuracy number per benchmark or category. We dive into the\nMCQA evaluation framework and conduct a systematic study spanning three\nbenchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio\nFlamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings\nindicate that models are sensitive not only to the ordering of choices, but\nalso to the paraphrasing of the question and the choices. Finally, we propose a\nsimpler evaluation protocol and metric that account for subtle variations and\nprovide a more detailed evaluation report of LALMs within the MCQA framework.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9879\u9009\u62e9\u95ee\u7b54\u8bc4\u4f30\u4e2d\u5bf9\u9009\u9879\u987a\u5e8f\u3001\u95ee\u9898\u8868\u8ff0\u548c\u9009\u9879\u6539\u5199\u654f\u611f\uff0c\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u6765\u89e3\u51b3\u8fd9\u79cd\u53d8\u5f02\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MCQA\u8bc4\u4f30\u6846\u67b6\u53ea\u62a5\u544a\u5355\u4e00\u51c6\u786e\u7387\uff0c\u4f46\u9009\u9879\u987a\u5e8f\u7b49\u7ec6\u5fae\u53d8\u5316\u4f1a\u5bfc\u81f4\u7ed3\u679c\u663e\u8457\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5(MMAU\u3001MMAR\u3001MMSU)\u548c\u56db\u4e2a\u6a21\u578b\u4e0a\u7cfb\u7edf\u7814\u7a76\u9009\u9879\u987a\u5e8f\u3001\u95ee\u9898\u6539\u5199\u548c\u9009\u9879\u6539\u5199\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u5bf9\u9009\u9879\u987a\u5e8f\u3001\u95ee\u9898\u8868\u8ff0\u548c\u9009\u9879\u6539\u5199\u90fd\u8868\u73b0\u51fa\u654f\u611f\u6027\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u53d8\u5f02\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8003\u8651\u7ec6\u5fae\u53d8\u5f02\u7684\u66f4\u7b80\u5355\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\uff0c\u4e3aMCQA\u6846\u67b6\u4e0b\u7684LALMs\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u8bc4\u4f30\u62a5\u544a\u3002"}}
{"id": "2510.03993", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03993", "abs": "https://arxiv.org/abs/2510.03993", "authors": ["Yaxin Hou", "Bo Han", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning", "comment": "The paper is accepted by NeurIPS 2025", "summary": "Current long-tailed semi-supervised learning methods assume that labeled data\nexhibit a long-tailed distribution, and unlabeled data adhere to a typical\npredefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).\nHowever, the distribution of the unlabeled data is generally unknown and may\nfollow an arbitrary distribution. To tackle this challenge, we propose a\nControllable Pseudo-label Generation (CPG) framework, expanding the labeled\ndataset with the progressively identified reliable pseudo-labels from the\nunlabeled dataset and training the model on the updated labeled dataset with a\nknown distribution, making it unaffected by the unlabeled data distribution.\nSpecifically, CPG operates through a controllable self-reinforcing optimization\ncycle: (i) at each training step, our dynamic controllable filtering mechanism\nselectively incorporates reliable pseudo-labels from the unlabeled dataset into\nthe labeled dataset, ensuring that the updated labeled dataset follows a known\ndistribution; (ii) we then construct a Bayes-optimal classifier using logit\nadjustment based on the updated labeled data distribution; (iii) this improved\nclassifier subsequently helps identify more reliable pseudo-labels in the next\ntraining step. We further theoretically prove that this optimization cycle can\nsignificantly reduce the generalization error under some conditions.\nAdditionally, we propose a class-aware adaptive augmentation module to further\nimprove the representation of minority classes, and an auxiliary branch to\nmaximize data utilization by leveraging all labeled and unlabeled samples.\nComprehensive evaluations on various commonly used benchmark datasets show that\nCPG achieves consistent improvements, surpassing state-of-the-art methods by up\nto \\textbf{15.97\\%} in accuracy. The code is available at\nhttps://github.com/yaxinhou/CPG.", "AI": {"tldr": "\u63d0\u51faCPG\u6846\u67b6\u89e3\u51b3\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u672a\u6807\u8bb0\u6570\u636e\u5206\u5e03\u672a\u77e5\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u63a7\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u5df2\u77e5\u5206\u5e03\u66f4\u65b0\u6765\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u8fbe15.97%\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u672a\u6807\u8bb0\u6570\u636e\u9075\u5faa\u9884\u5b9a\u4e49\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u4e2d\u672a\u6807\u8bb0\u6570\u636e\u7684\u5206\u5e03\u901a\u5e38\u662f\u672a\u77e5\u4e14\u4efb\u610f\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd", "method": "\u63d0\u51fa\u53ef\u63a7\u4f2a\u6807\u7b7e\u751f\u6210\u6846\u67b6\uff0c\u5305\u542b\u52a8\u6001\u53ef\u63a7\u8fc7\u6ee4\u673a\u5236\u3001\u57fa\u4e8e\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668\u7684logit\u8c03\u6574\u3001\u7c7b\u611f\u77e5\u81ea\u9002\u5e94\u589e\u5f3a\u6a21\u5757\u548c\u8f85\u52a9\u5206\u652f\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u4f18\u5316\u5faa\u73af\u9010\u6b65\u8bc6\u522b\u53ef\u9760\u4f2a\u6807\u7b7e", "result": "\u5728\u591a\u4e2a\u5e38\u7528\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cCPG\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u8fbe15.97%\u51c6\u786e\u7387", "conclusion": "CPG\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u5206\u5e03\u672a\u77e5\u7684\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u63a7\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u5df2\u77e5\u5206\u5e03\u66f4\u65b0\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2510.04601", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04601", "abs": "https://arxiv.org/abs/2510.04601", "authors": ["Guochen Yan", "Luyuan Xie", "Qingni Shen", "Yuejian Fang", "Zhonghai Wu"], "title": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning", "comment": null, "summary": "The current paradigm of training large language models (LLMs) on publicly\navailable Web data is becoming unsustainable, with high-quality data sources in\nspecialized domains nearing exhaustion. Federated Learning (FL) emerges as a\npractical solution for the next generation of AI on a decentralized Web,\nenabling privacy-preserving collaborative fine-tuning by leveraging private\ndata distributed across a global client base. While Low-Rank Adaptation (LoRA)\nis the standard for efficient fine-tuning, its application in federated\nsettings presents a critical challenge: communication overhead remains a\nsignificant bottleneck across the Web's heterogeneous network conditions. The\nstructural redundancy within LoRA parameters not only incurs a heavy\ncommunication burden but also introduces conflicts when aggregating client\nupdates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose\nframework designed for communication-efficient FL. We first introduce an\nimportance-aware sparsification method that preserves the structural integrity\nof LoRA updates to reduce the uploaded parameter count. The server then\nreconstructs and aggregates these updates in a full-rank space to mitigate\nconflicts. Finally, it decomposes the global update into a sparse low-rank\nformat for broadcast, ensuring a symmetrically efficient cycle. We also propose\nan efficient variant, FedSRD-e, to reduce computational overhead. Experimental\nresults on 10 benchmarks demonstrate that our framework significantly reduces\ncommunication costs by up to 90\\% while even improving model performance on\nheterogeneous client data.", "AI": {"tldr": "FedSRD\u662f\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u5316-\u91cd\u6784-\u5206\u89e3\u4e09\u6b65\u6cd5\uff0c\u5728\u4fdd\u6301LoRA\u5fae\u8c03\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500", "motivation": "\u5f53\u524d\u57fa\u4e8e\u516c\u5f00\u7f51\u7edc\u6570\u636e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8303\u5f0f\u4e0d\u53ef\u6301\u7eed\uff0c\u9ad8\u8d28\u91cf\u4e13\u4e1a\u9886\u57df\u6570\u636e\u63a5\u8fd1\u67af\u7aed\u3002\u8054\u90a6\u5b66\u4e60\u867d\u7136\u80fd\u5229\u7528\u5206\u5e03\u5f0f\u79c1\u6709\u6570\u636e\u8fdb\u884c\u9690\u79c1\u4fdd\u62a4\u534f\u4f5c\u5fae\u8c03\uff0c\u4f46LoRA\u5728\u8054\u90a6\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u53c2\u6570\u51b2\u7a81\u95ee\u9898\u4e25\u91cd\u963b\u788d\u4e86\u5176\u5e94\u7528", "method": "\u63d0\u51faFedSRD\u6846\u67b6\uff1a1)\u91cd\u8981\u6027\u611f\u77e5\u7a00\u758f\u5316\uff0c\u51cf\u5c11\u4e0a\u4f20\u53c2\u6570\u6570\u91cf\uff1b2)\u670d\u52a1\u5668\u5728\u5b8c\u6574\u79e9\u7a7a\u95f4\u91cd\u6784\u548c\u805a\u5408\u66f4\u65b0\uff0c\u7f13\u89e3\u51b2\u7a81\uff1b3)\u5c06\u5168\u5c40\u66f4\u65b0\u5206\u89e3\u4e3a\u7a00\u758f\u4f4e\u79e9\u683c\u5f0f\u8fdb\u884c\u5e7f\u64ad\u3002\u8fd8\u63d0\u51fa\u4e86\u8ba1\u7b97\u5f00\u9500\u66f4\u4f4e\u7684\u53d8\u4f53FedSRD-e", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u8be5\u6846\u67b6\u5c06\u901a\u4fe1\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe90%\uff0c\u540c\u65f6\u5728\u5f02\u6784\u5ba2\u6237\u7aef\u6570\u636e\u4e0a\u751a\u81f3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd", "conclusion": "FedSRD\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2dLoRA\u5fae\u8c03\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e0a\u7684AI\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.04003", "categories": ["cs.CV", "cs.CL", "68T50, 68T50, 68T10", "I.2.7; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2510.04003", "abs": "https://arxiv.org/abs/2510.04003", "authors": ["Minh Hoang Nguyen", "Su Nguyen Thiet"], "title": "Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5", "comment": "5 pages, 6 figures, 2 tables", "summary": "Recognizing and processing Classical Chinese (Han-Nom) texts play a vital\nrole in digitizing Vietnamese historical documents and enabling cross-lingual\nsemantic research. However, existing OCR systems struggle with degraded scans,\nnon-standard glyphs, and handwriting variations common in ancient sources. In\nthis work, we propose a fine-tuning approach for PaddleOCRv5 to improve\ncharacter recognition on Han-Nom texts. We retrain the text recognition module\nusing a curated subset of ancient Vietnamese Chinese manuscripts, supported by\na full training pipeline covering preprocessing, LMDB conversion, evaluation,\nand visualization. Experimental results show a significant improvement over the\nbase model, with exact accuracy increasing from 37.5 percent to 50.0 percent,\nparticularly under noisy image conditions. Furthermore, we develop an\ninteractive demo that visually compares pre- and post-fine-tuning recognition\nresults, facilitating downstream applications such as Han-Vietnamese semantic\nalignment, machine translation, and historical linguistics research. The demo\nis available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePaddleOCRv5\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8d8a\u5357\u53e4\u5178\u4e2d\u6587\uff08\u6c49\u5583\uff09\u6587\u672c\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4ece37.5%\u63d0\u5347\u81f350.0%\uff0c\u7279\u522b\u5728\u566a\u58f0\u56fe\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709OCR\u7cfb\u7edf\u5728\u5904\u7406\u8d8a\u5357\u5386\u53f2\u6587\u732e\u4e2d\u7684\u9000\u5316\u626b\u63cf\u3001\u975e\u6807\u51c6\u5b57\u5f62\u548c\u624b\u5199\u53d8\u4f53\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u53e4\u5178\u4e2d\u6587\u6587\u672c\u7684\u6570\u5b57\u5316\u548c\u8de8\u8bed\u8a00\u8bed\u4e49\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7cbe\u9009\u7684\u8d8a\u5357\u53e4\u5178\u4e2d\u6587\u624b\u7a3f\u5b50\u96c6\u5bf9PaddleOCRv5\u7684\u6587\u672c\u8bc6\u522b\u6a21\u5757\u8fdb\u884c\u5fae\u8c03\uff0c\u6784\u5efa\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b\u5305\u62ec\u9884\u5904\u7406\u3001LMDB\u8f6c\u6362\u3001\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4ece37.5%\u63d0\u9ad8\u523050.0%\uff0c\u5728\u566a\u58f0\u56fe\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u6f14\u793a\u5bf9\u6bd4\u5fae\u8c03\u524d\u540e\u7684\u8bc6\u522b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u53e4\u5178\u4e2d\u6587\u6587\u672c\u8bc6\u522b\u6027\u80fd\uff0c\u652f\u6301\u6c49\u8d8a\u8bed\u4e49\u5bf9\u9f50\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u5386\u53f2\u8bed\u8a00\u5b66\u7814\u7a76\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2510.04631", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04631", "abs": "https://arxiv.org/abs/2510.04631", "authors": ["Anastasia Zhukova", "Jonas L\u00fchrs", "Christian E. Matt", "Bela Gipp"], "title": "Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry", "comment": "accepted to EMNLP 2025 (industry track)", "summary": "Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained\nlanguage models by incorporating additional knowledge from the graph structures\nto learn domain-specific terminology or relationships between documents that\nmight otherwise be overlooked. This paper explores how SciNCL, a graph-aware\nneighborhood contrastive learning methodology originally designed for\nscientific publications, can be applied to the process industry domain, where\ntext logs contain crucial information about daily operations and are often\nstructured as sparse KGs. Our experiments demonstrate that language models\nfine-tuned with triplets derived from GE outperform a state-of-the-art\nmE5-large text encoder by 9.8-14.3% (5.4-8.0p) on the proprietary process\nindustry text embedding benchmark (PITEB) while being 3-5 times smaller in\nsize.", "AI": {"tldr": "\u5c06SciNCL\u56fe\u611f\u77e5\u90bb\u57df\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5e94\u7528\u4e8e\u6d41\u7a0b\u5de5\u4e1a\u9886\u57df\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\uff0c\u5728PITEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8emE5-large\u6587\u672c\u7f16\u7801\u56689.8-14.3%\uff0c\u4e14\u6a21\u578b\u5c3a\u5bf8\u5c0f3-5\u500d\u3002", "motivation": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u672f\u8bed\u548c\u6587\u6863\u95f4\u5173\u7cfb\uff0c\u89e3\u51b3\u6d41\u7a0b\u5de5\u4e1a\u6587\u672c\u65e5\u5fd7\u4e2d\u5173\u952e\u4fe1\u606f\u53ef\u80fd\u88ab\u5ffd\u7565\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528SciNCL\u56fe\u611f\u77e5\u90bb\u57df\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u56fe\u5d4c\u5165\u4e2d\u63d0\u53d6\u4e09\u5143\u7ec4\u6765\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u4e13\u6709\u6d41\u7a0b\u5de5\u4e1a\u6587\u672c\u5d4c\u5165\u57fa\u51c6\u6d4b\u8bd5(PITEB)\u4e0a\uff0c\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684mE5-large\u6587\u672c\u7f16\u7801\u56689.8-14.3%(5.4-8.0\u4e2a\u767e\u5206\u70b9)\uff0c\u540c\u65f6\u6a21\u578b\u5c3a\u5bf8\u5c0f3-5\u500d\u3002", "conclusion": "\u56fe\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6d41\u7a0b\u5de5\u4e1a\u9886\u57df\u6587\u672c\u8868\u793a\u5b66\u4e60\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8f83\u5c0f\u6a21\u578b\u5c3a\u5bf8\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.04021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04021", "abs": "https://arxiv.org/abs/2510.04021", "authors": ["Kushal Vyas", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation", "comment": "MICCAI 2025 (oral). Final peer-reviewed copy accessible at publisher\n  DOI https://link.springer.com/chapter/10.1007/978-3-032-04947-6_19 . Project\n  page, https://kushalvyas.github.io/metaseg.html", "summary": "Implicit neural representations (INRs) have achieved remarkable successes in\nlearning expressive yet compact signal representations. However, they are not\nnaturally amenable to predictive tasks such as segmentation, where they must\nlearn semantic structures over a distribution of signals. In this study, we\nintroduce MetaSeg, a meta-learning framework to train INRs for medical image\nsegmentation. MetaSeg uses an underlying INR that simultaneously predicts per\npixel intensity values and class labels. It then uses a meta-learning procedure\nto find optimal initial parameters for this INR over a training dataset of\nimages and segmentation maps, such that the INR can simply be fine-tuned to fit\npixels of an unseen test image, and automatically decode its class labels. We\nevaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice\nscores comparable to commonly used U-Net models, but with $90\\%$ fewer\nparameters. MetaSeg offers a fresh, scalable alternative to traditional\nresource-heavy architectures such as U-Nets and vision transformers for medical\nimage segmentation. Our project is available at\nhttps://kushalvyas.github.io/metaseg.html .", "AI": {"tldr": "MetaSeg\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u80fd\u591f\u5728\u53c2\u6570\u51cf\u5c1190%\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0eU-Net\u76f8\u5f53\u7684Dice\u5206\u6570\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u5728\u4fe1\u53f7\u8868\u793a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u5b66\u4e60\u8bed\u4e49\u7ed3\u6784\u7684\u9884\u6d4b\u4efb\u52a1\u5982\u5206\u5272\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3INRs\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u57fa\u7840INR\u540c\u65f6\u9884\u6d4b\u50cf\u7d20\u5f3a\u5ea6\u503c\u548c\u7c7b\u522b\u6807\u7b7e\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8fc7\u7a0b\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u627e\u5230\u6700\u4f18\u521d\u59cb\u53c2\u6570\uff0c\u4f7fINR\u80fd\u591f\u5feb\u901f\u5fae\u8c03\u4ee5\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u56fe\u50cf\u3002", "result": "\u57282D\u548c3D\u8111MRI\u5206\u5272\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cDice\u5206\u6570\u4e0e\u5e38\u7528\u7684U-Net\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u51cf\u5c11\u4e8690%\u3002", "conclusion": "MetaSeg\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u8d44\u6e90\u5bc6\u96c6\u578b\u67b6\u6784\u5982U-Net\u548c\u89c6\u89c9\u53d8\u6362\u5668\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2510.04641", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04641", "abs": "https://arxiv.org/abs/2510.04641", "authors": ["Ayan Majumdar", "Feihao Chen", "Jinghui Li", "Xiaozhen Wang"], "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study", "comment": "17 pages, 7 figures, 7 tables", "summary": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30LLM\u68c0\u6d4b\u4eba\u53e3\u7edf\u8ba1\u76ee\u6807\u793e\u4f1a\u504f\u89c1\u80fd\u529b\u7684\u7efc\u5408\u6846\u67b6\uff0c\u53d1\u73b0\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u5728\u53ef\u6269\u5c55\u68c0\u6d4b\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u591a\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u68c0\u6d4b\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u5927\u89c4\u6a21\u7f51\u7edc\u722c\u53d6\u6587\u672c\u8bed\u6599\u5e93\u5e38\u5305\u542b\u6709\u5bb3\u7684\u4eba\u53e3\u7edf\u8ba1\u76ee\u6807\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u8981\u6570\u636e\u5ba1\u8ba1\u548c\u53ef\u6269\u5c55\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u8303\u56f4\u72ed\u7a84\uff0c\u7f3a\u4e4f\u5bf9LLM\u81ea\u52a8\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u6784\u5efa\u4e86\u9488\u5bf9\u82f1\u8bed\u6587\u672c\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u504f\u89c1\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u591a\u6807\u7b7e\u4efb\u52a1\uff0c\u4f7f\u7528\u4eba\u53e3\u7edf\u8ba1\u805a\u7126\u7684\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u89c4\u6a21\u548c\u6280\u672f\u7684\u6a21\u578b\uff0c\u5305\u62ec\u63d0\u793a\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u5fae\u8c03\u3002", "result": "\u4f7f\u752812\u4e2a\u6db5\u76d6\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u548c\u4eba\u53e3\u7edf\u8ba1\u7684\u6570\u636e\u96c6\uff0c\u7814\u7a76\u8868\u660e\u5fae\u8c03\u7684\u5c0f\u6a21\u578b\u5728\u53ef\u6269\u5c55\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5206\u6790\u4e5f\u66b4\u9732\u4e86\u8de8\u4eba\u53e3\u7edf\u8ba1\u8f74\u548c\u591a\u4eba\u53e3\u7edf\u8ba1\u76ee\u6807\u504f\u89c1\u7684\u6301\u7eed\u5dee\u8ddd\u3002", "conclusion": "\u9700\u8981\u66f4\u6709\u6548\u548c\u53ef\u6269\u5c55\u7684\u5ba1\u8ba1\u6846\u67b6\u6765\u89e3\u51b3\u591a\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2510.04022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04022", "abs": "https://arxiv.org/abs/2510.04022", "authors": ["Chendong Wang", "Donglin Bai", "Yifan Yang", "Xiao Jin", "Anlan Zhang", "Rui Wang", "Shiqi Jiang", "Yuqing Yang", "Hao Wu", "Qi Dai", "Chong Luo", "Ting Cao", "Lili Qiu", "Suman Banerjee"], "title": "Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning", "comment": null, "summary": "We present \\emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA\nframework that preserves a fixed token budget by first \\emph{localizing}\nquestion-relevant interval(s) with a low-fps skim and then \\emph{answering} via\nspan-aware reallocation of visual tokens at higher effective frame rate,\nemitting an interleaved output with both spans and the final option for direct\nattribution. We also introduce \\dataname{}, which converts description based\nevent graphs into \\emph{span-grounded} multiple-choice QA by pairing each\nquestion with \\emph{ground-truth} time span(s) and related reasoning. ViTL is\ntrained end-to-end with an interleaved group-relative objective that couples\ntemporal IoU for localization with answer correctness, allowing credit to flow\nfrom answers back to spans without increasing compute. Under fixed token\nbudgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and\ntemporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations\nshow that span-aware token reallocation consistently surpasses uniform\nsampling. Together, \\dataname{} and ViTL provide an interpretable,\ncompute-efficient recipe for scalable long-video QA.", "AI": {"tldr": "Video-in-the-Loop (ViTL) \u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u957f\u89c6\u9891\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u5e27\u7387\u6d4f\u89c8\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u7247\u6bb5\uff0c\u7136\u540e\u5728\u9ad8\u5e27\u7387\u4e0b\u91cd\u65b0\u5206\u914d\u89c6\u89c9token\u6765\u56de\u7b54\u95ee\u9898\uff0c\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u89c6\u9891\u7406\u89e3\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5904\u7406\u5927\u91cf\u5e27\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f4e\u5e27\u7387\u6d4f\u89c8\u5b9a\u4f4d\u95ee\u9898\u76f8\u5173\u65f6\u95f4\u7247\u6bb5\uff1b2) \u9ad8\u5e27\u7387\u4e0b\u8fdb\u884cspan\u611f\u77e5\u7684token\u91cd\u65b0\u5206\u914d\u6765\u56de\u7b54\u95ee\u9898\uff1b\u4f7f\u7528\u7aef\u5230\u7aef\u8bad\u7ec3\u548c\u4ea4\u9519\u7ec4\u76f8\u5bf9\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\uff0cViTL\u5728\u957f\u89c6\u9891\u95ee\u7b54\u548c\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8fbe\u52308.6%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c1150%\u7684\u5e27\u8f93\u5165\uff1bspan\u611f\u77e5\u7684token\u91cd\u65b0\u5206\u914d\u59cb\u7ec8\u4f18\u4e8e\u5747\u5300\u91c7\u6837\u3002", "conclusion": "ViTL\u548c\u914d\u5957\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u3002"}}
{"id": "2510.04655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04655", "abs": "https://arxiv.org/abs/2510.04655", "authors": ["Yuheng Li", "Jiechao Gao", "Wei Han", "Wenwen Ouyang", "Wei Zhu", "Hui Yi Leong"], "title": "FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method", "comment": "Accepted by EMNLP-2025 Industrial Track", "summary": "Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to building clinical decision support\nsystems. However, current MDT construction methods rely heavily on\ntime-consuming and laborious manual annotation. To address this challenge, we\npropose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for\nautomatically extracting MDTs from clinical guidelines and textbooks. We\nintegrate gradient path information to capture synergistic effects between\ndifferent modules, enabling more effective and reliable rank allocation. This\nframework ensures that the most critical modules receive appropriate rank\nallocations while less important ones are pruned, resulting in a more efficient\nand accurate model for extracting medical decision trees from clinical texts.\nExtensive experiments on medical guideline datasets demonstrate that our\nPI-LoRA method significantly outperforms existing parameter-efficient\nfine-tuning approaches for the Text2MDT task, achieving better accuracy with\nsubstantially reduced model complexity. The proposed method achieves\nstate-of-the-art results while maintaining a lightweight architecture, making\nit particularly suitable for clinical decision support systems where\ncomputational resources may be limited.", "AI": {"tldr": "\u63d0\u51faPI-LoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u68af\u5ea6\u8def\u5f84\u4fe1\u606f\u81ea\u52a8\u4ece\u4e34\u5e8a\u6307\u5357\u548c\u6559\u79d1\u4e66\u4e2d\u63d0\u53d6\u533b\u7597\u51b3\u7b56\u6811\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u533b\u7597\u51b3\u7b56\u6811\u6784\u5efa\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u8d39\u529b\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u7cfb\u7edf\u3002", "method": "PI-LoRA\uff08\u8def\u5f84\u96c6\u6210LoRA\uff09\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f4e\u79e9\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u96c6\u6210\u68af\u5ea6\u8def\u5f84\u4fe1\u606f\u6355\u6349\u6a21\u5757\u95f4\u534f\u540c\u6548\u5e94\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u79e9\u5206\u914d\u3002", "result": "\u5728\u533b\u7597\u6307\u5357\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPI-LoRA\u5728Text2MDT\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u6a21\u578b\u590d\u6742\u5ea6\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u7279\u522b\u9002\u5408\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2510.04024", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04024", "abs": "https://arxiv.org/abs/2510.04024", "authors": ["Yuyan Bu", "Qiang Sheng", "Juan Cao", "Shaofei Wang", "Peng Qi", "Yuhui Shi", "Beizhe Hu"], "title": "Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation", "comment": "ACM CIKM 2025", "summary": "The emergence of fake news on short video platforms has become a new\nsignificant societal concern, necessitating automatic video-news-specific\ndetection. Current detectors primarily rely on pattern-based features to\nseparate fake news videos from real ones. However, limited and less diversified\ntraining data lead to biased patterns and hinder their performance. This\nweakness stems from the complex many-to-many relationships between video\nmaterial segments and fabricated news events in real-world scenarios: a single\nvideo clip can be utilized in multiple ways to create different fake\nnarratives, while a single fabricated event often combines multiple distinct\nvideo segments. However, existing datasets do not adequately reflect such\nrelationships due to the difficulty of collecting and annotating large-scale\nreal-world data, resulting in sparse coverage and non-comprehensive learning of\nthe characteristics of potential fake news video creation. To address this\nissue, we propose a data augmentation framework, AgentAug, that generates\ndiverse fake news videos by simulating typical creative processes. AgentAug\nimplements multiple LLM-driven pipelines of four fabrication categories for\nnews video creation, combined with an active learning strategy based on\nuncertainty sampling to select the potentially useful augmented samples during\ntraining. Experimental results on two benchmark datasets demonstrate that\nAgentAug consistently improves the performance of short video fake news\ndetectors.", "AI": {"tldr": "\u63d0\u51faAgentAug\u6570\u636e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u5178\u578b\u521b\u4f5c\u8fc7\u7a0b\u751f\u6210\u591a\u6837\u5316\u5047\u65b0\u95fb\u89c6\u9891\uff0c\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u77ed\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u56e0\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u5f0f\u504f\u5dee\u548c\u6027\u80fd\u53d7\u9650\u3002\u771f\u5b9e\u573a\u666f\u4e2d\u89c6\u9891\u7d20\u6750\u4e0e\u865a\u5047\u65b0\u95fb\u4e8b\u4ef6\u5b58\u5728\u590d\u6742\u591a\u5bf9\u591a\u5173\u7cfb\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u8fd9\u79cd\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u591aLLM\u9a71\u52a8\u6d41\u6c34\u7ebf\u6a21\u62df\u56db\u79cd\u5047\u65b0\u95fb\u521b\u4f5c\u7c7b\u522b\uff0c\u7ed3\u5408\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u9009\u62e9\u6709\u7528\u7684\u589e\u5f3a\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAgentAug\u80fd\u6301\u7eed\u63d0\u5347\u77ed\u89c6\u9891\u5047\u65b0\u95fb\u68c0\u6d4b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "AgentAug\u6846\u67b6\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u6709\u6548\u89e3\u51b3\u4e86\u5047\u65b0\u95fb\u89c6\u9891\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04671", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04671", "abs": "https://arxiv.org/abs/2510.04671", "authors": ["Chao Liu", "Ling Luo", "Tengxiao Lv", "Huan Zhuang", "Lejing Yu", "Jian Wang", "Hongfei Lin"], "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification", "comment": "Accepted as a regular paper at BIBM2025", "summary": "With the rapid development of online medical platforms, consumer health\nquestions (CHQs) are inefficient in diagnosis due to redundant information and\nfrequent non-professional terms. The medical question summary (MQS) task aims\nto transform CHQs into streamlined doctors' frequently asked questions (FAQs),\nbut existing methods still face challenges such as poor identification of\nquestion focus and model hallucination. This paper explores the potential of\nlarge language models (LLMs) in the MQS task and finds that direct fine-tuning\nis prone to focus identification bias and generates unfaithful content. To this\nend, we propose an optimization framework based on core focus guidance. First,\na prompt template is designed to drive the LLMs to extract the core focus from\nthe CHQs that is faithful to the original text. Then, a fine-tuning dataset is\nconstructed in combination with the original CHQ-FAQ pairs to improve the\nability to identify the focus of the question. Finally, a multi-dimensional\nquality evaluation and selection mechanism is proposed to comprehensively\nimprove the quality of the summary from multiple dimensions. We conduct\ncomprehensive experiments on two widely-adopted MQS datasets using three\nestablished evaluation metrics. The proposed framework achieves\nstate-of-the-art performance across all measures, demonstrating a significant\nboost in the model's ability to identify critical focus of questions and a\nnotable mitigation of hallucinations. The source codes are freely available at\nhttps://github.com/DUT-LiuChao/FocusMed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6838\u5fc3\u7126\u70b9\u5f15\u5bfc\u7684\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u5fe0\u5b9e\u4e8e\u539f\u6587\u7684\u6838\u5fc3\u7126\u70b9\u3001\u6784\u5efa\u5fae\u8c03\u6570\u636e\u96c6\u548c\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u95ee\u9898\u6458\u8981\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5728\u8bc6\u522b\u95ee\u9898\u5173\u952e\u7126\u70b9\u548c\u7f13\u89e3\u5e7b\u89c9\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u7ebf\u533b\u7597\u5e73\u53f0\u5feb\u901f\u53d1\u5c55\uff0c\u6d88\u8d39\u8005\u5065\u5eb7\u95ee\u9898\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u548c\u975e\u4e13\u4e1a\u672f\u8bed\uff0c\u5bfc\u81f4\u8bca\u65ad\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u533b\u7597\u95ee\u9898\u6458\u8981\u65b9\u6cd5\u5728\u8bc6\u522b\u95ee\u9898\u7126\u70b9\u548c\u907f\u514d\u6a21\u578b\u5e7b\u89c9\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6838\u5fc3\u7126\u70b9\u5f15\u5bfc\u7684\u4f18\u5316\u6846\u67b6\uff1a1\uff09\u8bbe\u8ba1\u63d0\u793a\u6a21\u677f\u9a71\u52a8LLMs\u63d0\u53d6\u5fe0\u5b9e\u4e8e\u539f\u6587\u7684\u6838\u5fc3\u7126\u70b9\uff1b2\uff09\u7ed3\u5408\u539f\u59cbCHQ-FAQ\u5bf9\u6784\u5efa\u5fae\u8c03\u6570\u636e\u96c6\uff1b3\uff09\u63d0\u51fa\u591a\u7ef4\u5ea6\u8d28\u91cf\u8bc4\u4f30\u548c\u9009\u62e9\u673a\u5236\u3002", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684MQS\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u4e09\u4e2a\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\uff0c\u6240\u63d0\u6846\u67b6\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8bc6\u522b\u95ee\u9898\u5173\u952e\u7126\u70b9\u7684\u80fd\u529b\u5e76\u663e\u8457\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u57fa\u4e8e\u6838\u5fc3\u7126\u70b9\u5f15\u5bfc\u7684\u4f18\u5316\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u533b\u7597\u95ee\u9898\u6458\u8981\u4efb\u52a1\u6027\u80fd\uff0c\u5728\u7126\u70b9\u8bc6\u522b\u548c\u5e7b\u89c9\u7f13\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aLLMs\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04034", "abs": "https://arxiv.org/abs/2510.04034", "authors": ["Linn Bieske", "Carla Lorente"], "title": "Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks", "comment": null, "summary": "Recent advances in image editing have shifted from manual pixel manipulation\nto employing deep learning methods like stable diffusion models, which now\nleverage cross-attention mechanisms for text-driven control. This transition\nhas simplified the editing process but also introduced variability in results,\nsuch as inconsistent hair color changes. Our research aims to enhance the\nprecision and reliability of prompt-to-prompt image editing frameworks by\nexploring and optimizing hyperparameters. We present a comprehensive study of\nthe \"word swap\" method, develop an \"attention re-weight method\" for better\nadaptability, and propose the \"CL P2P\" framework to address existing\nlimitations like cycle inconsistency. This work contributes to understanding\nand improving the interaction between hyperparameter settings and the\narchitectural choices of neural network models, specifically their attention\nmechanisms, which significantly influence the composition and quality of the\ngenerated images.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u6765\u63d0\u9ad8\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u867d\u7136\u7b80\u5316\u4e86\u7f16\u8f91\u8fc7\u7a0b\uff0c\u4f46\u5b58\u5728\u7ed3\u679c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5982\u5934\u53d1\u989c\u8272\u53d8\u5316\u4e0d\u4e00\u81f4\u7b49\uff0c\u9700\u8981\u63d0\u9ad8\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "1. \u5168\u9762\u7814\u7a76\"\u8bcd\u8bed\u66ff\u6362\"\u65b9\u6cd5\uff1b2. \u5f00\u53d1\"\u6ce8\u610f\u529b\u91cd\u65b0\u52a0\u6743\u65b9\u6cd5\"\u4ee5\u63d0\u9ad8\u9002\u5e94\u6027\uff1b3. \u63d0\u51fa\"CL P2P\"\u6846\u67b6\u89e3\u51b3\u5faa\u73af\u4e0d\u4e00\u81f4\u7b49\u73b0\u6709\u9650\u5236\u3002", "result": "\u7814\u7a76\u6709\u52a9\u4e8e\u7406\u89e3\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u67b6\u6784\u9009\u62e9\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u7279\u522b\u662f\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u751f\u6210\u56fe\u50cf\u6784\u56fe\u548c\u8d28\u91cf\u7684\u91cd\u8981\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u8d85\u53c2\u6570\u548c\u5f00\u53d1\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u57fa\u4e8e\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.04678", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04678", "abs": "https://arxiv.org/abs/2510.04678", "authors": ["Zhanfeng Mo", "Xingxuan Li", "Yuntao Chen", "Lidong Bing"], "title": "Multi-Agent Tool-Integrated Policy Optimization", "comment": "Work in progress", "summary": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.", "AI": {"tldr": "\u63d0\u51fa\u4e86MATPO\u65b9\u6cd5\uff0c\u5728\u5355\u4e2aLLM\u4e2d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u89c4\u5212\u8005\u548c\u5de5\u4f5c\u8005\u4e24\u79cd\u89d2\u8272\uff0c\u89e3\u51b3\u591a\u8f6e\u5de5\u5177\u96c6\u6210\u89c4\u5212\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u566a\u58f0\u5de5\u5177\u54cd\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u5904\u7406\u77e5\u8bc6\u5bc6\u96c6\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u65f6\u5b58\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u548c\u566a\u58f0\u5de5\u5177\u54cd\u5e94\u95ee\u9898\uff0c\u800c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7f3a\u4e4f\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "MATPO\u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u63d0\u793a\u5728\u5355\u4e2aLLM\u5b9e\u4f8b\u4e2d\u8bad\u7ec3\u89c4\u5212\u8005\u548c\u5de5\u4f5c\u8005\u89d2\u8272\uff0c\u57fa\u4e8e\u89c4\u5212\u8005\u548c\u5de5\u4f5c\u8005\u8f68\u8ff9\u7684\u539f\u5219\u6027\u4fe1\u7528\u5206\u914d\u673a\u5236\u3002", "result": "\u5728GAIA-text\u3001WebWalkerQA\u548cFRAMES\u6570\u636e\u96c6\u4e0a\uff0cMATPO\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u5e73\u5747\u6027\u80fd\u63d0\u534718.38%\uff0c\u5bf9\u566a\u58f0\u5de5\u5177\u8f93\u51fa\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5728\u5355\u4e2aLLM\u4e2d\u7edf\u4e00\u591a\u4e2a\u667a\u80fd\u4f53\u89d2\u8272\u662f\u6709\u6548\u7684\uff0c\u4e3a\u7a33\u5b9a\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2510.04039", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04039", "abs": "https://arxiv.org/abs/2510.04039", "authors": ["Bin Lei", "Nuo Xu", "Ali Payani", "Mingyi Hong", "Chunhua Liao", "Yu Cao", "Caiwen Ding"], "title": "\\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding", "comment": null, "summary": "Multimodal large language models (MLLMs) have markedly expanded the\ncompetence of graphical user-interface (GUI) systems, propelling them beyond\ncontrolled simulations into complex, real-world environments across diverse\nplatforms. However, practical usefulness is still bounded by the reliability of\nvisual grounding, i.e., mapping textual references to exact on-screen elements.\nThis limitation prevents the system from accurately performing pointer-level\nactions such as clicking or dragging. To address it, we introduce GUI-Spotlight\n-- a model trained for image-grounded reasoning that dynamically invokes\nmultiple specialized tools to iteratively narrow its focus to the relevant\nregion of the screen, thereby substantially improving visual grounding\naccuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only\n18.5K training samples achieves 52.8\\% accuracy, surpassing V2P-7B (50.6\\% with\n9.6M training samples) and GTA-1-7B (50.1\\% with 1.56M training samples).", "AI": {"tldr": "GUI-Spotlight\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u50cf\u63a8\u7406\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u591a\u4e2a\u4e13\u7528\u5de5\u5177\u8fed\u4ee3\u7f29\u5c0f\u5c4f\u5e55\u76f8\u5173\u533a\u57df\uff0c\u663e\u8457\u63d0\u9ad8\u89c6\u89c9\u57fa\u7840\u51c6\u786e\u6027\uff0c\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ec5\u752818.5K\u8bad\u7ec3\u6837\u672c\u5c31\u8fbe\u523052.8%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u5230\u89c6\u89c9\u57fa\u7840\u53ef\u9760\u6027\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u51c6\u786e\u6267\u884c\u6307\u9488\u7ea7\u64cd\u4f5c\u5982\u70b9\u51fb\u6216\u62d6\u52a8\u3002", "method": "\u5f15\u5165GUI-Spotlight\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u7528\u591a\u4e2a\u4e13\u7528\u5de5\u5177\u8fdb\u884c\u56fe\u50cf\u63a8\u7406\uff0c\u8fed\u4ee3\u7f29\u5c0f\u5173\u6ce8\u533a\u57df\u5230\u5c4f\u5e55\u76f8\u5173\u90e8\u5206\u3002", "result": "\u5728ScreenSpot-Pro\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u752818.5K\u8bad\u7ec3\u6837\u672c\u5c31\u8fbe\u523052.8%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86V2P-7B\uff0850.6%\uff0c\u4f7f\u75289.6M\u6837\u672c\uff09\u548cGTA-1-7B\uff0850.1%\uff0c\u4f7f\u75281.56M\u6837\u672c\uff09\u3002", "conclusion": "GUI-Spotlight\u901a\u8fc7\u4e13\u95e8\u7684\u5de5\u5177\u8c03\u7528\u548c\u8fed\u4ee3\u805a\u7126\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u57fa\u7840\u51c6\u786e\u6027\uff0c\u4e3aGUI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6307\u9488\u7ea7\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2510.04682", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04682", "abs": "https://arxiv.org/abs/2510.04682", "authors": ["Chanjoo Jung", "Jaehyung Kim"], "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA", "comment": null, "summary": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall.", "AI": {"tldr": "TiTok\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u77e5\u8bc6\u8f6c\u79fb\u5b9e\u73b0\u6709\u6548\u7684LoRA\u79fb\u690d\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u5f00\u9500\uff0c\u5728\u591a\u4e2a\u8f6c\u79fb\u8bbe\u7f6e\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53474-8%\u3002", "motivation": "\u89e3\u51b3PEFT\u65b9\u6cd5\u5982LoRA\u7684\u9002\u914d\u53c2\u6570\u4f9d\u8d56\u4e8e\u57fa\u7840\u6a21\u578b\u4e14\u65e0\u6cd5\u5728\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u95f4\u8f6c\u79fb\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u77e5\u8bc6\u84b8\u998f\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u548cTransLoRA\u9700\u8981\u989d\u5916\u5224\u522b\u5668\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6e90\u6a21\u578b\u6709\u65e0LoRA\u7684\u5dee\u5f02\uff0c\u6355\u6349\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u7a81\u51fa\u4fe1\u606f\u4e30\u5bcc\u7684token\uff0c\u5e76\u9009\u62e9\u6027\u8fc7\u6ee4\u5408\u6210\u6570\u636e\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u4e2a\u8f6c\u79fb\u8bbe\u7f6e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u6709\u6548\uff0c\u6574\u4f53\u5e73\u5747\u6027\u80fd\u63d0\u53474-8%\u3002", "conclusion": "TiTok\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0LoRA\u79fb\u690d\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b\u6216\u5f00\u9500\uff0c\u5728\u5404\u79cd\u8f6c\u79fb\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u5f02\u3002"}}
{"id": "2510.04044", "categories": ["cs.CV", "cs.AI", "00-01", "I.2.6; K.3.2"], "pdf": "https://arxiv.org/pdf/2510.04044", "abs": "https://arxiv.org/abs/2510.04044", "authors": ["Bingtao Yang", "Yujia Wang", "Mengzhi Jiao", "Hongwei Huo"], "title": "Quantization Range Estimation for Convolutional Neural Networks", "comment": "11 pages, 5 tables, research report", "summary": "Post-training quantization for reducing the storage of deep neural network\nmodels has been demonstrated to be an effective way in various tasks. However,\nlow-bit quantization while maintaining model accuracy is a challenging problem.\nIn this paper, we present a range estimation method to improve the quantization\nperformance for post-training quantization. We model the range estimation into\nan optimization problem of minimizing quantization errors by layer-wise local\nminima. We prove this problem is locally convex and present an efficient search\nalgorithm to find the optimal solution. We propose the application of the above\nsearch algorithm to the transformed weights space to do further improvement in\npractice. Our experiments demonstrate that our method outperforms\nstate-of-the-art performance generally on top-1 accuracy for image\nclassification tasks on the ResNet series models and Inception-v3 model. The\nexperimental results show that the proposed method has almost no loss of top-1\naccuracy in 8-bit and 6-bit settings for image classifications, and the\naccuracy of 4-bit quantization is also significantly improved. The code is\navailable at https://github.com/codeiscommitting/REQuant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8303\u56f4\u4f30\u8ba1\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u95f4\u5c40\u90e8\u6700\u5c0f\u503c\u4f18\u5316\u6765\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u7684\u7cbe\u5ea6\u3002", "motivation": "\u540e\u8bad\u7ec3\u91cf\u5316\u867d\u7136\u80fd\u6709\u6548\u51cf\u5c11\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u5b58\u50a8\u9700\u6c42\uff0c\u4f46\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u8fdb\u884c\u4f4e\u6bd4\u7279\u91cf\u5316\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5c06\u8303\u56f4\u4f30\u8ba1\u5efa\u6a21\u4e3a\u901a\u8fc7\u5c42\u95f4\u5c40\u90e8\u6700\u5c0f\u503c\u6700\u5c0f\u5316\u91cf\u5316\u8bef\u5dee\u7684\u4f18\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u8be5\u95ee\u9898\u5177\u6709\u5c40\u90e8\u51f8\u6027\uff0c\u5e76\u63d0\u51fa\u9ad8\u6548\u7684\u641c\u7d22\u7b97\u6cd5\u5728\u53d8\u6362\u540e\u7684\u6743\u91cd\u7a7a\u95f4\u4e2d\u5bfb\u627e\u6700\u4f18\u89e3\u3002", "result": "\u5728ResNet\u7cfb\u5217\u6a21\u578b\u548cInception-v3\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684top-1\u51c6\u786e\u7387\u666e\u904d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c8\u4f4d\u548c6\u4f4d\u91cf\u5316\u51e0\u4e4e\u65e0\u7cbe\u5ea6\u635f\u5931\uff0c4\u4f4d\u91cf\u5316\u7684\u51c6\u786e\u7387\u4e5f\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u7684\u8303\u56f4\u4f30\u8ba1\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6bd4\u7279\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2510.04694", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04694", "abs": "https://arxiv.org/abs/2510.04694", "authors": ["Lucas Bandarkar", "Chenyuan Yang", "Mohsen Fayyaz", "Junlin Hu", "Nanyun Peng"], "title": "Multilingual Routing in Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages.", "AI": {"tldr": "\u5206\u6790\u4e86MoE\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6570\u636e\u4e2d\u7684\u8def\u7531\u6a21\u5f0f\uff0c\u53d1\u73b0\u5728\u4e2d\u95f4\u5c42\u5b58\u5728\u8de8\u8bed\u8a00\u8def\u7531\u5bf9\u9f50\u73b0\u8c61\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u5e72\u9884\u4e2d\u95f4\u5c42\u8def\u7531\u6765\u63d0\u5347\u591a\u8bed\u8a00\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3MoE\u67b6\u6784\u5728\u7a00\u758f\u8def\u7531\u52a8\u6001\u4e2d\u5982\u4f55\u54cd\u5e94\u591a\u8bed\u8a00\u6570\u636e\uff0c\u63a2\u7d22\u8de8\u8bed\u8a00\u8def\u7531\u6a21\u5f0f\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528\u5e76\u884c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5206\u6790\u4e13\u5bb6\u8def\u7531\u6a21\u5f0f\uff0c\u63d0\u51fa\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4fc3\u8fdb\u4e2d\u95f4\u5c42\u4efb\u52a1\u4e13\u5bb6\u6fc0\u6d3b\u6765\u5f15\u5bfc\u8def\u7531\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u4e2d\u95f4\u5c42\u8def\u7531\u5bf9\u9f50\u4e0e\u8bed\u8a00\u6027\u80fd\u5f3a\u76f8\u5173\uff0c\u63d0\u51fa\u7684\u5e72\u9884\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u3001\u6a21\u578b\u548c\u8bed\u8a00\u4e0a\u83b7\u5f971-2%\u7684\u7a33\u5b9a\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MoE\u6a21\u578b\u5904\u7406\u975e\u82f1\u8bed\u6587\u672c\u7684\u80fd\u529b\u53d7\u9650\u4e8e\u5176\u5229\u7528\u8bed\u8a00\u901a\u7528\u4e13\u5bb6\u7684\u80fd\u529b\uff0c\u4e2d\u95f4\u5c42\u8def\u7531\u5bf9\u9f50\u662f\u63d0\u5347\u591a\u8bed\u8a00\u6cdb\u5316\u7684\u5173\u952e\u3002"}}
{"id": "2510.04057", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04057", "abs": "https://arxiv.org/abs/2510.04057", "authors": ["Zhenyu Pan", "Yucheng Lu", "Han Liu"], "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation", "comment": "The Thirty-Ninth Annual Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "We present MetaFind, a scene-aware tri-modal compositional retrieval\nframework designed to enhance scene generation in the metaverse by retrieving\n3D assets from large-scale repositories. MetaFind addresses two core\nchallenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,\nand stylistic constraints, and (ii) the absence of a standardized retrieval\nparadigm specifically tailored for 3D asset retrieval, as existing approaches\nmainly rely on general-purpose 3D shape representation models. Our key\ninnovation is a flexible retrieval mechanism that supports arbitrary\ncombinations of text, image, and 3D modalities as queries, enhancing spatial\nreasoning and style consistency by jointly modeling object-level features\n(including appearance) and scene-level layout structures. Methodologically,\nMetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that\ncaptures spatial relationships and object appearance features, ensuring\nretrieved 3D assets are contextually and stylistically coherent with the\nexisting scene, regardless of coordinate frame transformations. The framework\nsupports iterative scene construction by continuously adapting retrieval\nresults to current scene updates. Empirical evaluations demonstrate the\nimproved spatial and stylistic consistency of MetaFind in various retrieval\ntasks compared to baseline methods.", "AI": {"tldr": "MetaFind\u662f\u4e00\u4e2a\u9762\u5411\u5143\u5b87\u5b99\u573a\u666f\u751f\u6210\u7684\u4e09\u6a21\u6001\u7ec4\u5408\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5927\u89c4\u6a21\u5b58\u50a8\u5e93\u4e2d\u68c0\u7d223D\u8d44\u4ea7\u6765\u89e3\u51b3\u8d44\u4ea7\u68c0\u7d22\u4e2d\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u98ce\u683c\u4e00\u81f4\u6027\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u8d44\u4ea7\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u7684\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a(i) \u5ffd\u89c6\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u98ce\u683c\u7ea6\u675f\u7684\u4e0d\u4e00\u81f4\u8d44\u4ea7\u68c0\u7d22\uff1b(ii) \u7f3a\u4e4f\u4e13\u95e8\u4e3a3D\u8d44\u4ea7\u68c0\u7d22\u8bbe\u8ba1\u7684\u6807\u51c6\u5316\u68c0\u7d22\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u7075\u6d3b\u7684\u68c0\u7d22\u673a\u5236\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u50cf\u548c3D\u6a21\u6001\u7684\u4efb\u610f\u7ec4\u5408\u67e5\u8be2\uff0c\u901a\u8fc7ESSGNN\u7b49\u53d8\u5e03\u5c40\u7f16\u7801\u5668\u8054\u5408\u5efa\u6a21\u5bf9\u8c61\u7ea7\u7279\u5f81\u548c\u573a\u666f\u7ea7\u5e03\u5c40\u7ed3\u6784\uff0c\u786e\u4fdd\u68c0\u7d22\u7ed3\u679c\u4e0e\u73b0\u6709\u573a\u666f\u5728\u4e0a\u4e0b\u6587\u548c\u98ce\u683c\u4e0a\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cMetaFind\u5728\u5404\u79cd\u68c0\u7d22\u4efb\u52a1\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u7a7a\u95f4\u548c\u98ce\u683c\u4e00\u81f4\u6027\u3002", "conclusion": "MetaFind\u901a\u8fc7\u4e09\u6a21\u6001\u7ec4\u5408\u68c0\u7d22\u548c\u7b49\u53d8\u5e03\u5c40\u7f16\u7801\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u8d44\u4ea7\u68c0\u7d22\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u652f\u6301\u8fed\u4ee3\u5f0f\u573a\u666f\u6784\u5efa\u3002"}}
{"id": "2510.04717", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04717", "abs": "https://arxiv.org/abs/2510.04717", "authors": ["Sarel Duanis", "Asnat Greenstein-Messica", "Eliya Habba"], "title": "JSON Whisperer: Efficient JSON Editing with LLMs", "comment": null, "summary": "Large language models (LLMs) can modify JSON documents through natural\nlanguage commands, but current approaches regenerate entire structures for each\nedit, resulting in computational inefficiency. We present JSON Whisperer, a\nframework that enables LLMs to generate RFC 6902 diff patches-expressing only\nthe necessary modifications-rather than complete documents. We identify two key\nchallenges in patch-based editing: (1) LLMs often miss related updates when\ngenerating isolated patches, and (2) array manipulations require tracking index\nshifts across operations, which LLMs handle poorly. To address these issues, we\nintroduce EASE (Explicitly Addressed Sequence Encoding), which transforms\narrays into dictionaries with stable keys, eliminating index arithmetic\ncomplexities. Our evaluation shows that patch generation with EASE reduces\ntoken usage by 31% while maintaining edit quality within 5% of full\nregeneration with particular gains for complex instructions and list\nmanipulations. The dataset is available at:\nhttps://github.com/emnlp2025/JSON-Whisperer/", "AI": {"tldr": "JSON Whisperer\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u591f\u751f\u6210RFC 6902\u5dee\u5f02\u8865\u4e01\u800c\u975e\u5b8c\u6574\u6587\u6863\uff0c\u901a\u8fc7EASE\u7f16\u7801\u89e3\u51b3\u6570\u7ec4\u7d22\u5f15\u8ddf\u8e2a\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5f53\u524dLLM\u7f16\u8f91JSON\u6587\u6863\u65f6\u9700\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u7ed3\u6784\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7f16\u8f91\u65b9\u6cd5\u3002", "method": "\u63d0\u51faJSON Whisperer\u6846\u67b6\uff0c\u751f\u6210\u4ec5\u5305\u542b\u5fc5\u8981\u4fee\u6539\u7684\u5dee\u5f02\u8865\u4e01\uff1b\u5f15\u5165EASE\u7f16\u7801\u5c06\u6570\u7ec4\u8f6c\u6362\u4e3a\u5177\u6709\u7a33\u5b9a\u952e\u7684\u5b57\u5178\uff0c\u6d88\u9664\u7d22\u5f15\u7b97\u672f\u590d\u6742\u6027\u3002", "result": "\u8865\u4e01\u751f\u6210\u7ed3\u5408EASE\u53ef\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c1131%\uff0c\u7f16\u8f91\u8d28\u91cf\u4fdd\u6301\u5728\u5b8c\u6574\u91cd\u65b0\u751f\u6210\u76845%\u4ee5\u5185\uff0c\u5728\u590d\u6742\u6307\u4ee4\u548c\u5217\u8868\u64cd\u4f5c\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "JSON Whisperer\u901a\u8fc7\u5dee\u5f02\u8865\u4e01\u548cEASE\u7f16\u7801\u6709\u6548\u89e3\u51b3\u4e86LLM\u7f16\u8f91JSON\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u6587\u6863\u7f16\u8f91\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.04063", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2510.04063", "abs": "https://arxiv.org/abs/2510.04063", "authors": ["Chetraj Pandey", "Jinsu Hong", "Anli Ji", "Rafal A. Angryk", "Berkay Aydin"], "title": "Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction", "comment": "This is a preprint submitted to ICDM Workshop (SABID 2025). 6 pages,\n  2 Figures", "summary": "The prediction of solar flares is typically formulated as a binary\nclassification task, distinguishing events as either Flare (FL) or No-Flare\n(NF) according to a specified threshold (for example, greater than or equal to\nC-class, M-class, or X-class). However, this binary framework neglects the\ninherent ordinal relationships among the sub-classes contained within each\ncategory (FL and NF). Several studies on solar flare prediction have\nempirically shown that the most frequent misclassifications occur near this\nprediction threshold. This suggests that the models struggle to differentiate\nevents that are similar in intensity but fall on opposite sides of the binary\nthreshold. To mitigate this limitation, we propose a modified loss function\nthat integrates the ordinal information among the sub-classes of the binarized\nflare labels into the conventional binary cross-entropy (BCE) loss. This\napproach serves as an ordinality-aware, data-driven regularization method that\npenalizes the incorrect predictions of flare events in close proximity to the\nprediction threshold more heavily than those away from the boundary during\nmodel optimization. By incorporating ordinal weighting into the loss function,\nwe aim to enhance the model's learning process by leveraging the ordinal\ncharacteristics of the data, thereby improving its overall performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u5c06\u8000\u6591\u5b50\u7c7b\u95f4\u7684\u5e8f\u6570\u4fe1\u606f\u6574\u5408\u5230\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u4e2d\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u4e8c\u5143\u5206\u7c7b\u5ffd\u7565\u8000\u6591\u5f3a\u5ea6\u5e8f\u6570\u5173\u7cfb\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8000\u6591\u9884\u6d4b\u7684\u4e8c\u5143\u5206\u7c7b\u6846\u67b6\u5ffd\u7565\u4e86FL\u548cNF\u7c7b\u522b\u5185\u5b50\u7c7b\u4e4b\u95f4\u7684\u5e8f\u6570\u5173\u7cfb\uff0c\u4e14\u6a21\u578b\u5728\u9884\u6d4b\u9608\u503c\u9644\u8fd1\u6700\u5bb9\u6613\u51fa\u73b0\u8bef\u5206\u7c7b\u3002", "method": "\u5728\u4f20\u7edf\u4e8c\u5143\u4ea4\u53c9\u71b5\u635f\u5931\u57fa\u7840\u4e0a\uff0c\u6574\u5408\u8000\u6591\u5b50\u7c7b\u7684\u5e8f\u6570\u4fe1\u606f\uff0c\u4f5c\u4e3a\u5e8f\u6570\u611f\u77e5\u7684\u6570\u636e\u9a71\u52a8\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5bf9\u9608\u503c\u9644\u8fd1\u7684\u9519\u8bef\u9884\u6d4b\u65bd\u52a0\u66f4\u91cd\u7684\u60e9\u7f5a\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6570\u636e\u7684\u5e8f\u6570\u7279\u6027\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u65e8\u5728\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e8f\u6570\u52a0\u6743\u635f\u5931\u51fd\u6570\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8000\u6591\u9884\u6d4b\u4e2d\u9608\u503c\u9644\u8fd1\u7684\u5206\u7c7b\u56f0\u96be\uff0c\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.04750", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.04750", "abs": "https://arxiv.org/abs/2510.04750", "authors": ["Peshala Perera", "Deshan Sumanathilaka"], "title": "A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance", "comment": "11 pages, 4 figures, 3 tables", "summary": "Dyslexia in adults remains an under-researched and under-served area,\nparticularly in non-English-speaking contexts, despite its significant impact\non personal and professional lives. This work addresses that gap by focusing on\nSinhala, a low-resource language with limited tools for linguistic\naccessibility. We present an assistive system explicitly designed for\nSinhala-speaking adults with dyslexia. The system integrates Whisper for\nspeech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model\ntrained for Sinhala to identify common dyslexic errors, and a combined mT5 and\nMistral-based model to generate corrected text. Finally, the output is\nconverted back to speech using gTTS, creating a complete multimodal feedback\nloop. Despite the challenges posed by limited Sinhala-language datasets, the\nsystem achieves 0.66 transcription accuracy and 0.7 correction accuracy with\n0.65 overall system accuracy. These results demonstrate both the feasibility\nand effectiveness of the approach. Ultimately, this work highlights the\nimportance of inclusive Natural Language Processing (NLP) technologies in\nunderrepresented languages and showcases a practical", "AI": {"tldr": "\u4e3a\u50e7\u4f3d\u7f57\u8bed\u6210\u4eba\u9605\u8bfb\u969c\u788d\u8005\u5f00\u53d1\u7684\u591a\u6a21\u6001\u8f85\u52a9\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u8bed\u97f3\u8f6c\u6587\u672c\u3001\u9519\u8bef\u8bc6\u522b\u3001\u6587\u672c\u7ea0\u9519\u548c\u8bed\u97f3\u5408\u6210\u529f\u80fd\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u53ef\u884c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u6210\u4eba\u9605\u8bfb\u969c\u788d\u5728\u975e\u82f1\u8bed\u8bed\u5883\u4e2d\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u50e7\u4f3d\u7f57\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u8bed\u8a00\u53ef\u53ca\u6027\u5de5\u5177\uff0c\u4e25\u91cd\u5f71\u54cd\u60a3\u8005\u7684\u4e2a\u4eba\u548c\u804c\u4e1a\u751f\u6d3b\u3002", "method": "\u4f7f\u7528Whisper\u8fdb\u884c\u8bed\u97f3\u8f6c\u6587\u672c\uff0cSinBERT\u8bc6\u522b\u5e38\u89c1\u9605\u8bfb\u969c\u788d\u9519\u8bef\uff0c\u7ed3\u5408mT5\u548cMistral\u6a21\u578b\u751f\u6210\u4fee\u6b63\u6587\u672c\uff0c\u6700\u540e\u901a\u8fc7gTTS\u8f6c\u56de\u8bed\u97f3\uff0c\u5f62\u6210\u5b8c\u6574\u591a\u6a21\u6001\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728\u50e7\u4f3d\u7f57\u8bed\u6570\u636e\u96c6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u8fbe\u52300.66\u8f6c\u5f55\u51c6\u786e\u7387\u30010.7\u7ea0\u9519\u51c6\u786e\u7387\u548c0.65\u6574\u4f53\u7cfb\u7edf\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u5f00\u53d1\u5305\u5bb9\u6027NLP\u6280\u672f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u50e7\u4f3d\u7f57\u8bed\u9605\u8bfb\u969c\u788d\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04066", "abs": "https://arxiv.org/abs/2510.04066", "authors": ["Zheng Chen", "Kewei Zhang", "Xiaoyang Liu", "Weihang Zhang", "Mengfan Wang", "Yifan Fu", "Yulun Zhang"], "title": "QuantDemoire: Quantization with Outlier Aware for Image Demoir\u00e9ing", "comment": "Code is available at: https://github.com/zhengchen1999/QuantDemoire", "summary": "Demoir\\'eing aims to remove moir\\'e artifacts that often occur in images.\nWhile recent deep learning-based methods have achieved promising results, they\ntypically require substantial computational resources, limiting their\ndeployment on edge devices. Model quantization offers a compelling solution.\nHowever, directly applying existing quantization methods to demoir\\'eing models\nintroduces severe performance degradation. The main reasons are distribution\noutliers and weakened representations in smooth regions. To address these\nissues, we propose QuantDemoire, a post-training quantization framework\ntailored to demoir\\'eing. It contains two key components. **First}, we\nintroduce an outlier-aware quantizer to reduce errors from outliers. It uses\nsampling-based range estimation to reduce activation outliers, and keeps a few\nextreme weights in FP16 with negligible cost. **Second**, we design a\nfrequency-aware calibration strategy. It emphasizes low- and mid-frequency\ncomponents during fine-tuning, which mitigates banding artifacts caused by\nlow-bit quantization. Extensive experiments validate that our QuantDemoire\nachieves large reductions in parameters and computation while maintaining\nquality. Meanwhile, it outperforms existing quantization methods by over **4\ndB** on W4A4. Code is released at:\nhttps://github.com/zhengchen1999/QuantDemoire.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQuantDemoire\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u53bb\u6469\u5c14\u7eb9\u4efb\u52a1\uff0c\u901a\u8fc7\u5f02\u5e38\u503c\u611f\u77e5\u91cf\u5316\u5668\u548c\u9891\u7387\u611f\u77e5\u6821\u51c6\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u53bb\u6469\u5c14\u7eb9\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u96be\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5206\u5e03\u5f02\u5e38\u503c\u548c\u5e73\u6ed1\u533a\u57df\u8868\u793a\u5f31\u5316\u3002", "method": "QuantDemoire\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u5f02\u5e38\u503c\u611f\u77e5\u91cf\u5316\u5668\uff0c\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u8303\u56f4\u4f30\u8ba1\u51cf\u5c11\u6fc0\u6d3b\u5f02\u5e38\u503c\uff0c\u5e76\u5c06\u5c11\u91cf\u6781\u7aef\u6743\u91cd\u4fdd\u7559\u4e3aFP16\uff1b2\uff09\u9891\u7387\u611f\u77e5\u6821\u51c6\u7b56\u7565\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f3a\u8c03\u4f4e\u4e2d\u9891\u5206\u91cf\uff0c\u51cf\u8f7b\u4f4e\u6bd4\u7279\u91cf\u5316\u5f15\u8d77\u7684\u5e26\u72b6\u4f2a\u5f71\u3002", "result": "\u5728W4A4\u914d\u7f6e\u4e0b\uff0cQuantDemoire\u6bd4\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u8d85\u8fc74 dB\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u3002", "conclusion": "QuantDemoire\u4e3a\u53bb\u6469\u5c14\u7eb9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2510.04757", "categories": ["cs.CL", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.04757", "abs": "https://arxiv.org/abs/2510.04757", "authors": ["Eduardo Mart\u00ednez Rivera", "Filippo Menolascina"], "title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching\nLarge Language Models (LLMs) with external knowledge, allowing for factually\ngrounded responses, a critical requirement in high-stakes domains such as\nhealthcare. However, the efficacy of RAG systems is fundamentally restricted by\nthe performance of their retrieval module, since irrelevant or semantically\nmisaligned documents directly compromise the accuracy of the final generated\nresponse. General-purpose dense retrievers can struggle with the nuanced\nlanguage of specialised domains, while the high accuracy of in-domain models is\noften achieved at prohibitive computational costs. In this work, we aim to\naddress this trade-off by developing and evaluating a two-stage retrieval\narchitecture that combines a lightweight ModernBERT bidirectional encoder for\nefficient initial candidate retrieval with a ColBERTv2 late-interaction model\nfor fine-grained re-ranking. We conduct comprehensive evaluations of our\nretriever module performance and RAG system performance in the biomedical\ncontext, fine-tuning the IR module using 10k question-passage pairs from\nPubMedQA. Our analysis of the retriever module confirmed the positive impact of\nthe ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points\ncompared to its retrieve-only counterpart. When integrated into the biomedical\nRAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on\nthe five tasks of the MIRAGE question-answering benchmark, outperforming strong\nbaselines such as MedCPT (0.4436). Our ablation studies reveal that this\nperformance is critically dependent on a joint fine-tuning process that aligns\nthe retriever and re-ranker; otherwise, the re-ranker might degrade the\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u68c0\u7d22\u67b6\u6784\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7ModernBERT\u7f16\u7801\u5668\u548cColBERTv2\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u751f\u7269\u533b\u5b66\u9886\u57dfRAG\u7cfb\u7edf\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u901a\u7528\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u4e13\u4e1a\u9886\u57df\u8bed\u8a00\u7406\u89e3\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u800c\u9886\u57df\u4e13\u7528\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u5e73\u8861\u68c0\u7d22\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u4f7f\u7528ModernBERT\u8fdb\u884c\u521d\u59cb\u5019\u9009\u68c0\u7d22\uff0cColBERTv2\u8fdb\u884c\u7ec6\u7c92\u5ea6\u91cd\u6392\u5e8f\uff0c\u5728PubMedQA\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u68c0\u7d22\u6a21\u5757\u3002", "result": "ColBERT\u91cd\u6392\u5e8f\u5668\u5c06Recall@3\u63d0\u53474.2\u4e2a\u767e\u5206\u70b9\uff0c\u5728MIRAGE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.4448\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f18\u4e8eMedCPT\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u4e24\u9636\u6bb5\u68c0\u7d22\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347\u751f\u7269\u533b\u5b66RAG\u6027\u80fd\uff0c\u4f46\u6027\u80fd\u4f9d\u8d56\u4e8e\u68c0\u7d22\u5668\u548c\u91cd\u6392\u5e8f\u5668\u7684\u8054\u5408\u5fae\u8c03\u5bf9\u9f50\u3002"}}
{"id": "2510.04069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04069", "abs": "https://arxiv.org/abs/2510.04069", "authors": ["Zongyin Deng", "Qing Zhou", "Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging", "comment": null, "summary": "This work presents TV-LoRA, a novel method for low-dose sparse-view CT\nreconstruction that combines a diffusion generative prior (NCSN++ with SDE\nmodeling) and multi-regularization constraints, including anisotropic TV and\nnuclear norm (LoRA), within an ADMM framework. To address ill-posedness and\ntexture loss under extremely sparse views, TV-LoRA integrates generative and\nphysical constraints, and utilizes a 2D slice-based strategy with FFT\nacceleration and tensor-parallel optimization for efficient inference.\nExperiments on AAPM-2016, CTHD, and LIDC datasets with\n$N_{\\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks\nin SSIM, texture recovery, edge clarity, and artifact suppression,\ndemonstrating strong robustness and generalizability. Ablation studies confirm\nthe complementary effects of LoRA regularization and diffusion priors, while\nthe FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves\nhigh-fidelity, efficient 3D CT reconstruction and broad clinical applicability\nin low-dose, sparse-sampling scenarios.", "AI": {"tldr": "TV-LoRA\u662f\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u751f\u6210\u5148\u9a8c\u548c\u591a\u6b63\u5219\u5316\u7ea6\u675f\u7684\u4f4e\u5242\u91cf\u7a00\u758f\u89c6\u56feCT\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728ADMM\u6846\u67b6\u4e0b\u5b9e\u73b0\u9ad8\u65483D\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u6781\u7a00\u758f\u89c6\u56fe\u4e0bCT\u91cd\u5efa\u7684\u75c5\u6001\u95ee\u9898\u548c\u7eb9\u7406\u4e22\u5931\u95ee\u9898\uff0c\u7ed3\u5408\u751f\u6210\u5148\u9a8c\u548c\u7269\u7406\u7ea6\u675f\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u7ed3\u5408\u6269\u6563\u751f\u6210\u5148\u9a8c(NCSN++ SDE\u6a21\u578b)\u4e0e\u591a\u6b63\u5219\u5316\u7ea6\u675f(\u5404\u5411\u5f02\u6027TV\u548c\u6838\u8303\u6570LoRA)\uff0c\u91c7\u7528ADMM\u6846\u67b6\uff0c\u4f7f\u75282D\u5207\u7247\u7b56\u7565\u914d\u5408FFT\u52a0\u901f\u548c\u5e76\u884c\u4f18\u5316\u3002", "result": "\u5728AAPM-2016\u3001CTHD\u548cLIDC\u6570\u636e\u96c6\u4e0a\uff0cTV-LoRA\u5728SSIM\u3001\u7eb9\u7406\u6062\u590d\u3001\u8fb9\u7f18\u6e05\u6670\u5ea6\u548c\u4f2a\u5f71\u6291\u5236\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c55\u73b0\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "TV-LoRA\u5728\u4f4e\u5242\u91cf\u7a00\u758f\u91c7\u6837\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u9ad8\u6548\u76843D CT\u91cd\u5efa\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4e34\u5e8a\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.04764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04764", "abs": "https://arxiv.org/abs/2510.04764", "authors": ["Raha Askari", "Sina Zarrie\u00df", "\u00d6zge Alacam", "Judith Sieker"], "title": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models", "comment": null, "summary": "Implicit meanings are integral to human communication, making it essential\nfor language models to be capable of identifying and interpreting them. Grice\n(1975) proposed a set of conversational maxims that guide cooperative dialogue,\nnoting that speakers may deliberately violate these principles to express\nmeanings beyond literal words, and that listeners, in turn, recognize such\nviolations to draw pragmatic inferences.\n  Building on Surian et al. (1996)'s study of children's sensitivity to\nviolations of Gricean maxims, we introduce a novel benchmark to test whether\nlanguage models pretrained on less than 10M and less than 100M tokens can\ndistinguish maxim-adhering from maxim-violating utterances. We compare these\nBabyLMs across five maxims and situate their performance relative to children\nand a Large Language Model (LLM) pretrained on 3T tokens.\n  We find that overall, models trained on less than 100M tokens outperform\nthose trained on less than 10M, yet fall short of child-level and LLM\ncompetence. Our results suggest that modest data increases improve some aspects\nof pragmatic behavior, leading to finer-grained differentiation between\npragmatic dimensions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u8bc6\u522bGrice\u4f1a\u8bdd\u51c6\u5219\u8fdd\u53cd\u7684\u65b0\u57fa\u51c6\uff0c\u6bd4\u8f83\u4e86\u5728\u4e0d\u540c\u89c4\u6a21\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u7684BabyLMs\u4e0e\u513f\u7ae5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u8bc6\u522b\u548c\u89e3\u91ca\u9690\u542b\u610f\u4e49\uff0c\u7279\u522b\u662f\u901a\u8fc7\u68c0\u6d4b\u4f1a\u8bdd\u51c6\u5219\u7684\u8fdd\u53cd\u6765\u7406\u89e3\u8bed\u7528\u63a8\u7406\u3002", "method": "\u57fa\u4e8eSurian\u7b49\u4eba\u5bf9\u513f\u7ae5\u8bc6\u522bGrice\u51c6\u5219\u8fdd\u53cd\u7684\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u6d4b\u8bd5\u5728\u5c11\u4e8e1000\u4e07\u548c1\u4ebftoken\u4e0a\u9884\u8bad\u7ec3\u7684BabyLMs\u533a\u5206\u51c6\u5219\u9075\u5b88\u548c\u8fdd\u53cd\u8bdd\u8bed\u7684\u80fd\u529b\u3002", "result": "\u5728\u5c11\u4e8e1\u4ebftoken\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5728\u5c11\u4e8e1000\u4e07token\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4f46\u4ecd\u672a\u8fbe\u5230\u513f\u7ae5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u57283\u4e07\u4ebftoken\u4e0a\u8bad\u7ec3\uff09\u7684\u6c34\u5e73\u3002\u6570\u636e\u91cf\u7684\u9002\u5ea6\u589e\u52a0\u6539\u5584\u4e86\u67d0\u4e9b\u8bed\u7528\u884c\u4e3a\u65b9\u9762\u3002", "conclusion": "\u867d\u7136\u589e\u52a0\u8bad\u7ec3\u6570\u636e\u53ef\u4ee5\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u7528\u63a8\u7406\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4f46\u5f53\u524d\u7684\u5c0f\u89c4\u6a21\u6a21\u578b\u4ecd\u65e0\u6cd5\u8fbe\u5230\u4eba\u7c7b\u513f\u7ae5\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u7528\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.04100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04100", "abs": "https://arxiv.org/abs/2510.04100", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Harold Soh"], "title": "TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing", "comment": "Jiaming Wang, Diwen Liu, and Jizhuo Chen contributed equally", "summary": "Topological mapping offers a compact and robust representation for\nnavigation, but progress in the field is hindered by the lack of standardized\nevaluation metrics, datasets, and protocols. Existing systems are assessed\nusing different environments and criteria, preventing fair and reproducible\ncomparisons. Moreover, a key challenge - perceptual aliasing - remains\nunder-quantified, despite its strong influence on system performance. We\naddress these gaps by (1) formalizing topological consistency as the\nfundamental property of topological maps and showing that localization accuracy\nprovides an efficient and interpretable surrogate metric, and (2) proposing the\nfirst quantitative measure of dataset ambiguity to enable fair comparisons\nacross environments. To support this protocol, we curate a diverse benchmark\ndataset with calibrated ambiguity levels, implement and release deep-learned\nbaseline systems, and evaluate them alongside classical methods. Our\nexperiments and analysis yield new insights into the limitations of current\napproaches under perceptual aliasing. All datasets, baselines, and evaluation\ntools are fully open-sourced to foster consistent and reproducible research in\ntopological mapping.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u62d3\u6251\u5730\u56fe\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u534f\u8bae\uff0c\u5305\u62ec\u62d3\u6251\u4e00\u81f4\u6027\u4f5c\u4e3a\u6838\u5fc3\u5ea6\u91cf\u3001\u6570\u636e\u96c6\u6a21\u7cca\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u6821\u51c6\u6a21\u7cca\u5ea6\u7ea7\u522b\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u7cfb\u7edf\u3002", "motivation": "\u62d3\u6251\u5730\u56fe\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u548c\u534f\u8bae\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u4e0d\u540c\u73af\u5883\u548c\u6807\u51c6\u4e0b\u8bc4\u4f30\uff0c\u65e0\u6cd5\u8fdb\u884c\u516c\u5e73\u53ef\u590d\u73b0\u7684\u6bd4\u8f83\uff0c\u4e14\u611f\u77e5\u6df7\u6dc6\u8fd9\u4e00\u5173\u952e\u6311\u6218\u672a\u88ab\u5145\u5206\u91cf\u5316\u3002", "method": "\u5f62\u5f0f\u5316\u62d3\u6251\u4e00\u81f4\u6027\u4f5c\u4e3a\u62d3\u6251\u5730\u56fe\u7684\u57fa\u672c\u5c5e\u6027\uff0c\u8bc1\u660e\u5b9a\u4f4d\u7cbe\u5ea6\u53ef\u4f5c\u4e3a\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u5ea6\u91cf\uff1b\u63d0\u51fa\u9996\u4e2a\u6570\u636e\u96c6\u6a21\u7cca\u6027\u91cf\u5316\u65b9\u6cd5\uff1b\u6784\u5efa\u5177\u6709\u6821\u51c6\u6a21\u7cca\u5ea6\u7ea7\u522b\u7684\u591a\u6837\u5316\u57fa\u51c6\u6570\u636e\u96c6\uff1b\u5b9e\u73b0\u5e76\u53d1\u5e03\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u548c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u611f\u77e5\u6df7\u6dc6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u6240\u6709\u6570\u636e\u96c6\u3001\u57fa\u7ebf\u548c\u8bc4\u4f30\u5de5\u5177\u90fd\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u62d3\u6251\u5730\u56fe\u7814\u7a76\u7684\u6301\u7eed\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u62d3\u6251\u5730\u56fe\u8bc4\u4f30\u7684\u6807\u51c6\u5316\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u534f\u8bae\u548c\u57fa\u51c6\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u4e86\u516c\u5e73\u6bd4\u8f83\u7684\u57fa\u7840\uff0c\u5e76\u5f00\u6e90\u76f8\u5173\u8d44\u6e90\u4ee5\u63a8\u52a8\u7814\u7a76\u7684\u53ef\u590d\u73b0\u53d1\u5c55\u3002"}}
{"id": "2510.04800", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04800", "abs": "https://arxiv.org/abs/2510.04800", "authors": ["Sangmin Bae", "Bilge Acun", "Haroun Habeeb", "Seungyeon Kim", "Chien-Yu Lin", "Liang Luo", "Junjie Wang", "Carole-Jean Wu"], "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights", "comment": "17 pages, 4 figures, 6 tables; detailed results will be included in\n  the Appendix later", "summary": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.", "AI": {"tldr": "\u672c\u6587\u5bf9\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u7684\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u5c42\u95f4\uff08\u987a\u5e8f\uff09\u548c\u5c42\u5185\uff08\u5e76\u884c\uff09\u878d\u5408\u7b56\u7565\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u6df7\u5408\u6a21\u578b\u6548\u679c\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u867d\u7136\u6df7\u5408\u67b6\u6784\u5728\u5efa\u6a21\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u793e\u533a\u7f3a\u4e4f\u5bf9\u6df7\u5408\u7b56\u7565\u7684\u7cfb\u7edf\u6bd4\u8f83\u548c\u6709\u6548\u6027\u5173\u952e\u56e0\u7d20\u7684\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5c42\u95f4\uff08\u987a\u5e8f\uff09\u548c\u5c42\u5185\uff08\u5e76\u884c\uff09\u878d\u5408\u7b56\u7565\u6784\u5efa\u6df7\u5408\u67b6\u6784\uff0c\u4ece\u8bed\u8a00\u5efa\u6a21\u6027\u80fd\u3001\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u3001\u6269\u5c55\u5206\u6790\u4ee5\u53ca\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u7b49\u591a\u4e2a\u89d2\u5ea6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u6bcf\u79cd\u6df7\u5408\u7b56\u7565\u6700\u5173\u952e\u7684\u8ba1\u7b97\u539f\u8bed\u7279\u5f81\uff0c\u4e3a\u4e24\u79cd\u6df7\u5408\u6a21\u578b\u63d0\u51fa\u4e86\u6700\u4f18\u8bbe\u8ba1\u65b9\u6848\u3002", "conclusion": "\u5168\u9762\u7684\u5206\u6790\u4e3a\u5f00\u53d1\u6df7\u5408\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u548c\u5b9d\u8d35\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u67b6\u6784\u914d\u7f6e\u3002"}}
{"id": "2510.04111", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04111", "abs": "https://arxiv.org/abs/2510.04111", "authors": ["Xinglong Luo", "Ao Luo", "Kunming Luo", "Zhengning Wang", "Ping Tan", "Bing Zeng", "Shuaicheng Liu"], "title": "Learning Efficient Meshflow and Optical Flow from Event Cameras", "comment": "Accepted by TPAMI 2025", "summary": "In this paper, we explore the problem of event-based meshflow estimation, a\nnovel task that involves predicting a spatially smooth sparse motion field from\nevent cameras. To start, we review the state-of-the-art in event-based flow\nestimation, highlighting two key areas for further research: i) the lack of\nmeshflow-specific event datasets and methods, and ii) the underexplored\nchallenge of event data density. First, we generate a large-scale\nHigh-Resolution Event Meshflow (HREM) dataset, which showcases its superiority\nby encompassing the merits of high resolution at 1280x720, handling dynamic\nobjects and complex motion patterns, and offering both optical flow and\nmeshflow labels. These aspects have not been fully explored in previous works.\nBesides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a\nlightweight model featuring a specially crafted encoder-decoder architecture to\nfacilitate swift and accurate meshflow estimation. Furthermore, we upgrade\nEEMFlow network to support dense event optical flow, in which a\nConfidence-induced Detail Completion (CDC) module is proposed to preserve sharp\nmotion boundaries. We conduct comprehensive experiments to show the exceptional\nperformance and runtime efficiency (30x faster) of our EEMFlow model compared\nto the recent state-of-the-art flow method. As an extension, we expand HREM\ninto HREM+, a multi-density event dataset contributing to a thorough study of\nthe robustness of existing methods across data with varying densities, and\npropose an Adaptive Density Module (ADM) to adjust the density of input event\ndata to a more optimal range, enhancing the model's generalization ability. We\nempirically demonstrate that ADM helps to significantly improve the performance\nof EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are\nreleased at https://github.com/boomluo02/EEMFlowPlus.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u7f51\u683c\u6d41\u4f30\u8ba1\u65b0\u4efb\u52a1\uff0c\u521b\u5efa\u4e86\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u7f51\u683c\u6d41\u6570\u636e\u96c6HREM\uff0c\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7EEMFlow\u7f51\u7edc\u8fdb\u884c\u5feb\u901f\u51c6\u786e\u7684\u7f51\u683c\u6d41\u4f30\u8ba1\uff0c\u5e76\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u5bc6\u96c6\u5149\u6d41\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u5bc6\u5ea6\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u4e8b\u4ef6\u76f8\u673a\u5728\u7f51\u683c\u6d41\u4f30\u8ba1\u65b9\u9762\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7f3a\u4e4f\u4e13\u95e8\u7684\u7f51\u683c\u6d41\u6570\u636e\u96c6\u548c\u4e8b\u4ef6\u6570\u636e\u5bc6\u5ea6\u6311\u6218\u7684\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u751f\u6210\u4e86\u9ad8\u5206\u8fa8\u7387\u4e8b\u4ef6\u7f51\u683c\u6d41\u6570\u636e\u96c6HREM\uff0c\u63d0\u51fa\u4e86\u8f7b\u91cf\u7ea7EEMFlow\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u542b\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u7ec6\u8282\u5b8c\u6210\u6a21\u5757\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u9002\u5e94\u5bc6\u5ea6\u6a21\u5757\u6765\u5904\u7406\u4e0d\u540c\u5bc6\u5ea6\u7684\u4e8b\u4ef6\u6570\u636e\u3002", "result": "EEMFlow\u6a21\u578b\u5728\u6027\u80fd\u548c\u8fd0\u884c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5feb30\u500d\uff09\uff0c\u81ea\u9002\u5e94\u5bc6\u5ea6\u6a21\u5757\u5c06EEMFlow\u548cEEMFlow+\u7684\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e868%\u548c10%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e8b\u4ef6\u76f8\u673a\u7f51\u683c\u6d41\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u548c\u5bc6\u5ea6\u9002\u5e94\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u4e8b\u4ef6\u76f8\u673a\u8fd0\u52a8\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04832", "abs": "https://arxiv.org/abs/2510.04832", "authors": ["Christopher Bartley", "Anton Ragni"], "title": "How I Built ASR for Endangered Languages with a Spoken Dictionary", "comment": null, "summary": "Nearly half of the world's languages are endangered. Speech technologies such\nas Automatic Speech Recognition (ASR) are central to revival efforts, yet most\nlanguages remain unsupported because standard pipelines expect utterance-level\nsupervised data. Speech data often exist for endangered languages but rarely\nmatch these formats. Manx Gaelic ($\\sim$2,200 speakers), for example, has had\ntranscribed speech since 1948, yet remains unsupported by modern systems. In\nthis paper, we explore how little data, and in what form, is needed to build\nASR for critically endangered languages. We show that a short-form\npronunciation resource is a viable alternative, and that 40 minutes of such\ndata produces usable ASR for Manx ($<$50\\% WER). We replicate our approach,\napplying it to Cornish ($\\sim$600 speakers), another critically endangered\nlanguage. Results show that the barrier to entry, in quantity and form, is far\nlower than previously thought, giving hope to endangered language communities\nthat cannot afford to meet the requirements arbitrarily imposed upon them.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4e3a\u6fd2\u5371\u8bed\u8a00\u6784\u5efa\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u6240\u9700\u7684\u6700\u5c0f\u6570\u636e\u91cf\uff0c\u53d1\u73b0\u4ec5\u970040\u5206\u949f\u7684\u77ed\u683c\u5f0f\u53d1\u97f3\u6570\u636e\u5373\u53ef\u4e3a\u66fc\u514b\u65af\u76d6\u5c14\u8bed\u548c\u5eb7\u6c83\u5c14\u8bed\u5f00\u53d1\u51fa\u53ef\u7528\u7684ASR\u7cfb\u7edf\u3002", "motivation": "\u4e16\u754c\u4e0a\u8fd1\u4e00\u534a\u7684\u8bed\u8a00\u6fd2\u4e34\u706d\u7edd\uff0c\u6807\u51c6ASR\u6d41\u6c34\u7ebf\u9700\u8981\u8bed\u53e5\u7ea7\u76d1\u7763\u6570\u636e\uff0c\u4f46\u5927\u591a\u6570\u6fd2\u5371\u8bed\u8a00\u7f3a\u4e4f\u8fd9\u79cd\u683c\u5f0f\u7684\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u964d\u4f4e\u6784\u5efa\u6fd2\u5371\u8bed\u8a00ASR\u7cfb\u7edf\u7684\u6570\u636e\u95e8\u69db\u3002", "method": "\u4f7f\u7528\u77ed\u683c\u5f0f\u53d1\u97f3\u8d44\u6e90\u4f5c\u4e3a\u66ff\u4ee3\u6570\u636e\u6e90\uff0c\u4ec5\u970040\u5206\u949f\u6b64\u7c7b\u6570\u636e\uff0c\u4e3a\u66fc\u514b\u65af\u76d6\u5c14\u8bed\u548c\u5eb7\u6c83\u5c14\u8bed\u6784\u5efaASR\u7cfb\u7edf\u3002", "result": "\u4e3a\u66fc\u514b\u65af\u76d6\u5c14\u8bed\u5f00\u53d1\u7684ASR\u7cfb\u7edf\u8bcd\u9519\u8bef\u7387\u4f4e\u4e8e50%\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u884c\u3002\u5728\u5eb7\u6c83\u5c14\u8bed\u4e0a\u7684\u590d\u73b0\u4e5f\u83b7\u5f97\u6210\u529f\u3002", "conclusion": "\u6784\u5efa\u6fd2\u5371\u8bed\u8a00ASR\u7cfb\u7edf\u7684\u6570\u636e\u95e8\u69db\u8fdc\u4f4e\u4e8e\u9884\u671f\uff0c\u4e3a\u65e0\u6cd5\u6ee1\u8db3\u4f20\u7edf\u6570\u636e\u8981\u6c42\u7684\u6fd2\u5371\u8bed\u8a00\u793e\u533a\u5e26\u6765\u4e86\u5e0c\u671b\u3002"}}
{"id": "2510.04125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04125", "abs": "https://arxiv.org/abs/2510.04125", "authors": ["Seunghyun Lee", "Tae-Kyun Kim"], "title": "Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation", "comment": null, "summary": "Latest diffusion models have shown promising results in category-level 6D\nobject pose estimation by modeling the conditional pose distribution with depth\nimage input. The existing methods, however, suffer from slow convergence during\ntraining, learning its encoder with the diffusion denoising network in\nend-to-end fashion, and require an additional network that evaluates sampled\npose hypotheses to filter out low-quality pose candidates. In this paper, we\npropose a novel pipeline that tackles these limitations by two key components.\nFirst, the proposed method pretrains the encoder with the direct pose\nregression head, and jointly learns the networks via the regression head and\nthe denoising diffusion head, significantly accelerating training convergence\nwhile achieving higher accuracy. Second, sampling guidance via time-dependent\nscore scaling is proposed s.t. the exploration-exploitation trade-off is\neffectively taken, eliminating the need for the additional evaluation network.\nThe sampling guidance maintains multi-modal characteristics of symmetric\nobjects at early denoising steps while ensuring high-quality pose generation at\nfinal steps. Extensive experiments on multiple benchmarks including REAL275,\nHouseCat6D, and ROPE, demonstrate that the proposed method, simple yet\neffective, achieves state-of-the-art accuracies even with single-pose\ninference, while being more efficient in both training and inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u8054\u5408\u5b66\u4e60\u7b56\u7565\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u540c\u65f6\u4f7f\u7528\u91c7\u6837\u6307\u5bfc\u6d88\u9664\u989d\u5916\u8bc4\u4f30\u7f51\u7edc\u7684\u9700\u6c42\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6536\u655b\u6162\u3001\u9700\u8981\u7aef\u5230\u7aef\u5b66\u4e60\u7f16\u7801\u5668\u3001\u4ee5\u53ca\u9700\u8981\u989d\u5916\u7f51\u7edc\u6765\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u59ff\u6001\u5019\u9009\u7684\u95ee\u9898\u3002", "method": "1. \u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5e76\u4f7f\u7528\u76f4\u63a5\u59ff\u6001\u56de\u5f52\u5934\uff0c\u8054\u5408\u5b66\u4e60\u56de\u5f52\u5934\u548c\u53bb\u566a\u6269\u6563\u5934\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u76f8\u5173\u5206\u6570\u7f29\u653e\u7684\u91c7\u6837\u6307\u5bfc\uff0c\u6709\u6548\u5e73\u8861\u63a2\u7d22-\u5229\u7528\u6743\u8861\u3002", "result": "\u5728REAL275\u3001HouseCat6D\u548cROPE\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5373\u4f7f\u4f7f\u7528\u5355\u59ff\u6001\u63a8\u7406\u4e5f\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u66f4\u52a0\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u548c\u91c7\u6837\u6307\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e866D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2510.04848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04848", "abs": "https://arxiv.org/abs/2510.04848", "authors": ["Yuto Nishida", "Masaru Isonuma", "Yusuke Oda"], "title": "Instability in Downstream Task Performance During LLM Pretraining", "comment": "Accepted to EMNLP 2025 Findings", "summary": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86LLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u6ce2\u52a8\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u68c0\u67e5\u70b9\u5e73\u5747\u548c\u96c6\u6210\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5b58\u5728\u663e\u8457\u6ce2\u52a8\uff0c\u96be\u4ee5\u786e\u5b9a\u6700\u4f73\u68c0\u67e5\u70b9\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4e24\u79cd\u540e\u5904\u7406\u68c0\u67e5\u70b9\u96c6\u6210\u65b9\u6cd5\uff1a\u68c0\u67e5\u70b9\u5e73\u5747\u548c\u96c6\u6210\uff0c\u901a\u8fc7\u805a\u5408\u76f8\u90bb\u68c0\u67e5\u70b9\u6765\u51cf\u5c11\u6027\u80fd\u6ce2\u52a8\u3002", "result": "\u7ecf\u9a8c\u8bc1\u660e\u548c\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u8bad\u7ec3\u8fc7\u7a0b\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e0b\u6e38\u6027\u80fd\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u68c0\u67e5\u70b9\u5e73\u5747\u548c\u96c6\u6210\u662f\u6709\u6548\u63d0\u5347LLM\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u65e0\u9700\u4fee\u6539\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2510.04142", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04142", "abs": "https://arxiv.org/abs/2510.04142", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs", "comment": null, "summary": "This paper identifies a critical yet underexplored challenge in distilling\nfrom multimodal large language models (MLLMs): the reasoning trajectories\ngenerated by multiple drifting teachers exhibit concept drift, whereby their\nreasoning distributions evolve unpredictably and transmit biases to the student\nmodel, ultimately compromising its performance. To tackle this issue, we\npioneer a theoretical connection between concept drift and knowledge\ndistillation, casting the non-stationary reasoning dynamics from multiple MLLM\nteachers as next-token prediction of multi-stream reasoning trajectories.Guided\nby concept drift, we introduce the \"learn, compare, critique\" paradigm,\nculminating in autonomous preference optimization (APO). Under the active\nguidance of the teachers, the student model first learns and self-distils\npreferred thinking by comparing multiple teachers. It then engages in critical\nreflection over the drifting inference from teachers, performing concept\nalignment through APO, ultimately yielding a robust, consistent, and\ngeneralizable model.Extensive experiments demonstrate our superior performance\nof consistency, robustness and generalization within knowledge distillation.\nBesides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers\nAlignment X-rays), comprising 170,982 distilled reasoning trajectories derived\nfrom publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public\nat: https://anonymous.4open.science/r/Autonomous-Distillation/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u84b8\u998f\u4e2d\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e3b\u504f\u597d\u4f18\u5316(APO)\u6765\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u591a\u6559\u5e08\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6982\u5ff5\u6f02\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u5206\u5e03\u4e0d\u7a33\u5b9a\uff0c\u5c06\u504f\u89c1\u4f20\u9012\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u5f71\u54cd\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\"\u5b66\u4e60\u3001\u6bd4\u8f83\u3001\u6279\u5224\"\u8303\u5f0f\uff0c\u901a\u8fc7\u81ea\u4e3b\u504f\u597d\u4f18\u5316(APO)\u8fdb\u884c\u6982\u5ff5\u5bf9\u9f50\uff0c\u8ba9\u5b66\u751f\u6a21\u578b\u5728\u591a\u6559\u5e08\u6307\u5bfc\u4e0b\u5b66\u4e60\u504f\u597d\u601d\u7ef4\u5e76\u8fdb\u884c\u6279\u5224\u6027\u53cd\u601d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u77e5\u8bc6\u84b8\u998f\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u4e00\u81f4\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u8d21\u732e\u4e86\u5305\u542b170,982\u6761\u63a8\u7406\u8f68\u8ff9\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6CXR-MAX\u3002", "conclusion": "\u57fa\u4e8e\u6982\u5ff5\u6f02\u79fb\u7406\u8bba\u7684\u81ea\u4e3b\u84b8\u998f\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6559\u5e08\u63a8\u7406\u8f68\u8ff9\u6f02\u79fb\u95ee\u9898\uff0c\u4ea7\u751f\u7a33\u5065\u3001\u4e00\u81f4\u4e14\u53ef\u6cdb\u5316\u7684\u6a21\u578b\u3002"}}
{"id": "2510.04849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04849", "abs": "https://arxiv.org/abs/2510.04849", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Maksim Savkin", "Valerii Olisov", "Artem Vazhentsev", "Kseniia Titova", "Alexander Panchenko", "Vasily Konovalov", "Julia Belikova"], "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA", "comment": null, "summary": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86PsiloQA\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b14\u79cd\u8bed\u8a00\u7684span\u7ea7\u5e7b\u89c9\u6807\u6ce8\uff0c\u7528\u4e8e\u8bc4\u4f30\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5e7b\u89c9\u57fa\u51c6\u4e3b\u8981\u5728\u5e8f\u5217\u7ea7\u522b\u4e14\u9650\u4e8e\u82f1\u8bed\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u591a\u8bed\u8a00\u76d1\u7763\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30LLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u4e09\u9636\u6bb5\u6d41\u7a0b\u6784\u5efa\u6570\u636e\u96c6\uff1a\u4f7f\u7528GPT-4o\u4ece\u7ef4\u57fa\u767e\u79d1\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u5728\u65e0\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e0b\u4ece\u4e0d\u540cLLM\u83b7\u53d6\u53ef\u80fd\u5305\u542b\u5e7b\u89c9\u7684\u7b54\u6848\uff0c\u4f7f\u7528GPT-4o\u901a\u8fc7\u5bf9\u6bd4\u9ec4\u91d1\u7b54\u6848\u548c\u68c0\u7d22\u4e0a\u4e0b\u6587\u81ea\u52a8\u6807\u6ce8\u5e7b\u89c9span\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u79cd\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u6a21\u578b\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u8868\u73b0\u6700\u5f3a\uff0cPsiloQA\u5c55\u793a\u4e86\u6709\u6548\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u7a33\u5065\u5730\u8fc1\u79fb\u5230\u5176\u4ed6\u57fa\u51c6\u3002", "conclusion": "PsiloQA\u6570\u636e\u96c6\u548c\u7ed3\u679c\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u53ef\u6269\u5c55\u3001\u7ec6\u7c92\u5ea6\u5e7b\u89c9\u68c0\u6d4b\u7684\u53d1\u5c55\uff0c\u4e14\u6bd4\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u66f4\u5177\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2510.04145", "categories": ["cs.CV", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2510.04145", "abs": "https://arxiv.org/abs/2510.04145", "authors": ["Chenxin Wang", "Elyas Asadi Shamsabadi", "Zhaohui Chen", "Luming Shen", "Alireza Ahmadian Fard Fini", "Daniel Dias-da-Costa"], "title": "Automating construction safety inspections using a multi-modal vision-language RAG framework", "comment": "33 pages, 11 figures, 7 tables", "summary": "Conventional construction safety inspection methods are often inefficient as\nthey require navigating through large volume of information. Recent advances in\nlarge vision-language models (LVLMs) provide opportunities to automate safety\ninspections through enhanced visual and linguistic understanding. However,\nexisting applications face limitations including irrelevant or unspecific\nresponses, restricted modal inputs and hallucinations. Utilisation of Large\nLanguage Models (LLMs) for this purpose is constrained by availability of\ntraining data and frequently lack real-time adaptability. This study introduces\nSiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)\nframework for automating construction safety inspection reports by integrating\nvisual and audio inputs. Using real-world data, SiteShield outperformed\nunimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,\nprecision of 0.76, and recall of 0.96. The findings indicate that SiteShield\noffers a novel pathway to enhance information retrieval and efficiency in\ngenerating safety reports.", "AI": {"tldr": "SiteShield\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u62a5\u544a\u751f\u6210\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u5355\u6a21\u6001LLM\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u5904\u7406\u5927\u91cf\u4fe1\u606f\u3002\u73b0\u6709\u5e94\u7528\u5b58\u5728\u54cd\u5e94\u4e0d\u76f8\u5173\u3001\u6a21\u6001\u8f93\u5165\u53d7\u9650\u548c\u5e7b\u89c9\u95ee\u9898\uff0cLLM\u5e94\u7528\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86SiteShield\u591a\u6a21\u6001LVLM RAG\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u548c\u97f3\u9891\u8f93\u5165\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5efa\u7b51\u5b89\u5168\u68c0\u67e5\u62a5\u544a\u751f\u6210\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u4e0a\uff0cSiteShield\u7684F1\u5f97\u5206\u4e3a0.82\uff0c\u6c49\u660e\u635f\u5931\u4e3a0.04\uff0c\u7cbe\u786e\u7387\u4e3a0.76\uff0c\u53ec\u56de\u7387\u4e3a0.96\uff0c\u4f18\u4e8e\u65e0RAG\u7684\u5355\u6a21\u6001LLM\u3002", "conclusion": "SiteShield\u4e3a\u589e\u5f3a\u4fe1\u606f\u68c0\u7d22\u548c\u5b89\u5168\u62a5\u544a\u751f\u6210\u6548\u7387\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.04850", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04850", "abs": "https://arxiv.org/abs/2510.04850", "authors": ["Hengxiang Zhang", "Hyeong Kyu Choi", "Yixuan Li", "Hongxin Wei"], "title": "Detecting Distillation Data from Reasoning Models", "comment": null, "summary": "Reasoning distillation has emerged as an efficient and powerful paradigm for\nenhancing the reasoning capabilities of large language models. However,\nreasoning distillation may inadvertently cause benchmark contamination, where\nevaluation data included in distillation datasets can inflate performance\nmetrics of distilled models. In this work, we formally define the task of\ndistillation data detection, which is uniquely challenging due to the partial\navailability of distillation data. Then, we propose a novel and effective\nmethod Token Probability Deviation (TBD), which leverages the probability\npatterns of the generated output tokens. Our method is motivated by the\nanalysis that distilled models tend to generate near-deterministic tokens for\nseen questions, while producing more low-probability tokens for unseen\nquestions. Our key idea behind TBD is to quantify how far the generated tokens'\nprobabilities deviate from a high reference probability. In effect, our method\nachieves competitive detection performance by producing lower scores for seen\nquestions than for unseen questions. Extensive experiments demonstrate the\neffectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of\n0.470 on the S1 dataset.", "AI": {"tldr": "\u63d0\u51faToken Probability Deviation (TBD)\u65b9\u6cd5\u6765\u68c0\u6d4b\u63a8\u7406\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u751f\u6210token\u7684\u6982\u7387\u6a21\u5f0f\u6765\u533a\u5206\u5df2\u89c1\u8fc7\u548c\u672a\u89c1\u8fc7\u7684\u6570\u636e\u3002", "motivation": "\u63a8\u7406\u84b8\u998f\u867d\u7136\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8bc4\u4f30\u6570\u636e\u6c61\u67d3\uff0c\u4f7f\u84b8\u998f\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u88ab\u5938\u5927\u3002", "method": "\u5229\u7528\u84b8\u998f\u6a21\u578b\u5bf9\u5df2\u89c1\u8fc7\u95ee\u9898\u751f\u6210\u786e\u5b9a\u6027\u9ad8\u6982\u7387token\uff0c\u5bf9\u672a\u89c1\u8fc7\u95ee\u9898\u751f\u6210\u4f4e\u6982\u7387token\u7684\u7279\u70b9\uff0c\u63d0\u51faTBD\u65b9\u6cd5\u91cf\u5316\u751f\u6210token\u6982\u7387\u4e0e\u9ad8\u53c2\u8003\u6982\u7387\u7684\u504f\u5dee\u3002", "result": "\u5728S1\u6570\u636e\u96c6\u4e0a\u53d6\u5f97AUC 0.918\u548cTPR@1% FPR 0.470\u7684\u4f18\u5f02\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "TBD\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u63a8\u7406\u84b8\u998f\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u7684\u6570\u636e\u6c61\u67d3\u68c0\u6d4b\u5de5\u5177\u3002"}}
{"id": "2510.04174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04174", "abs": "https://arxiv.org/abs/2510.04174", "authors": ["Piyush Arora", "Navlika Singh", "Vasubhya Diwan", "Pratik Mazumder"], "title": "BLADE: Bias-Linked Adaptive DEbiasing", "comment": "The authors have contributed equally", "summary": "Neural networks have revolutionized numerous fields, yet they remain\nvulnerable to a critical flaw: the tendency to learn implicit biases, spurious\ncorrelations between certain attributes and target labels in training data.\nThese biases are often more prevalent and easier to learn, causing models to\nrely on superficial patterns rather than task-relevant features necessary for\ngeneralization. Existing methods typically rely on strong assumptions, such as\nprior knowledge of these biases or access to bias-conflicting samples, i.e.,\nsamples that contradict spurious correlations and counterbalance bias-aligned\nsamples, samples that conform to these spurious correlations. However, such\nassumptions are often impractical in real-world settings. We propose BLADE\n({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that\nrequires no prior knowledge of bias or bias-conflicting samples. BLADE first\ntrains a generative model to translate images across bias domains while\npreserving task-relevant features. Then, it adaptively refines each image with\nits synthetic counterpart based on the image's susceptibility to bias. To\nencourage robust representations, BLADE aligns an image with its\nbias-translated synthetic counterpart that shares task-relevant features but\ndiffers in bias, while misaligning it with samples sharing the same bias. We\nevaluate BLADE on multiple benchmark datasets and show that it significantly\noutperforms state-of-the-art methods. Notably, it exceeds the closest baseline\nby an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the\nworst group setting, establishing a new benchmark in bias mitigation and\ndemonstrating its potential for developing more robust deep learning models\nwithout explicit supervision.", "AI": {"tldr": "BLADE\u662f\u4e00\u4e2a\u65e0\u9700\u5148\u9a8c\u504f\u7f6e\u77e5\u8bc6\u6216\u504f\u7f6e\u51b2\u7a81\u6837\u672c\u7684\u751f\u6210\u5f0f\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u504f\u7f6e\u57df\u56fe\u50cf\u7ffb\u8bd1\u548c\u81ea\u9002\u5e94\u7cbe\u70bc\u6765\u6d88\u9664\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u9690\u5f0f\u504f\u7f6e\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\uff08\u504f\u7f6e\uff09\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5df2\u77e5\u504f\u7f6e\u6216\u504f\u7f6e\u51b2\u7a81\u6837\u672c\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5728\u504f\u7f6e\u57df\u95f4\u7ffb\u8bd1\u56fe\u50cf\u5e76\u4fdd\u7559\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u7136\u540e\u57fa\u4e8e\u56fe\u50cf\u5bf9\u504f\u7f6e\u7684\u654f\u611f\u6027\u81ea\u9002\u5e94\u7cbe\u70bc\u56fe\u50cf\u4e0e\u5176\u5408\u6210\u5bf9\u5e94\u7269\uff0c\u901a\u8fc7\u5bf9\u9f50\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4f46\u504f\u7f6e\u4e0d\u540c\u7684\u56fe\u50cf\u6765\u9f13\u52b1\u9c81\u68d2\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728corrupted CIFAR-10\u6570\u636e\u96c6\u7684\u6700\u5dee\u7ec4\u8bbe\u7f6e\u4e0b\u6bd4\u6700\u63a5\u8fd1\u7684\u57fa\u7ebf\u7edd\u5bf9\u63d0\u5347\u7ea618%\u3002", "conclusion": "BLADE\u4e3a\u65e0\u9700\u663e\u5f0f\u76d1\u7763\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u504f\u7f6e\u7f13\u89e3\u6f5c\u529b\u3002"}}
{"id": "2510.04891", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04891", "abs": "https://arxiv.org/abs/2510.04891", "authors": ["Punya Syon Pandey", "Hai Son Le", "Devansh Bhardwaj", "Rada Mihalcea", "Zhijing Jin"], "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.", "AI": {"tldr": "SocialHarmBench\u662f\u4e00\u4e2a\u5305\u542b585\u4e2a\u63d0\u793a\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d67\u4e2a\u793e\u4f1a\u653f\u6cbb\u7c7b\u522b\u548c34\u4e2a\u56fd\u5bb6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u653f\u6cbb\u654f\u611f\u60c5\u5883\u4e2d\u7684\u6f0f\u6d1e\u3002\u7814\u7a76\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5728\u6709\u5bb3\u5408\u89c4\u65b9\u9762\u5b58\u5728\u9ad8\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u5386\u53f2\u4fee\u6b63\u4e3b\u4e49\u3001\u5ba3\u4f20\u548c\u653f\u6cbb\u64cd\u7eb5\u7b49\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u5f88\u5c11\u6d4b\u8bd5LLM\u5728\u653f\u6cbb\u64cd\u7eb5\u3001\u5ba3\u4f20\u548c\u865a\u5047\u4fe1\u606f\u751f\u6210\u3001\u76d1\u63a7\u548c\u4fe1\u606f\u63a7\u5236\u7b49\u9886\u57df\u7684\u6f0f\u6d1e\uff0c\u800c\u8fd9\u4e9b\u9886\u57df\u7684\u5931\u8d25\u53ef\u80fd\u4ea7\u751f\u76f4\u63a5\u7684\u793e\u4f1a\u653f\u6cbb\u540e\u679c\u3002", "method": "\u6784\u5efaSocialHarmBench\u6570\u636e\u96c6\uff0c\u5305\u542b585\u4e2a\u63d0\u793a\uff0c\u6db5\u76d67\u4e2a\u793e\u4f1a\u653f\u6cbb\u7c7b\u522b\u548c34\u4e2a\u56fd\u5bb6\uff0c\u8bc4\u4f30LLM\u5728\u653f\u6cbb\u654f\u611f\u60c5\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u5ea6\u8106\u5f31\u6027\uff0cMistral-7B\u5728\u5386\u53f2\u4fee\u6b63\u4e3b\u4e49\u3001\u5ba3\u4f20\u548c\u653f\u6cbb\u64cd\u7eb5\u7b49\u9886\u57df\u7684\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe97%-98%\u3002LLM\u572821\u4e16\u7eaa\u548c\u524d20\u4e16\u7eaa\u60c5\u5883\u4ee5\u53ca\u62c9\u4e01\u7f8e\u6d32\u3001\u7f8e\u56fd\u548c\u82f1\u56fd\u7b49\u5730\u533a\u6700\u4e3a\u8106\u5f31\u3002", "conclusion": "\u5f53\u524d\u7684\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u63a8\u5e7f\u5230\u9ad8\u98ce\u9669\u7684\u793e\u4f1a\u653f\u6cbb\u73af\u5883\u4e2d\uff0c\u66b4\u9732\u4e86\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u5e76\u5f15\u53d1\u4e86\u5bf9LLM\u5728\u4fdd\u62a4\u4eba\u6743\u548c\u6c11\u4e3b\u4ef7\u503c\u89c2\u65b9\u9762\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002"}}
{"id": "2510.04180", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04180", "abs": "https://arxiv.org/abs/2510.04180", "authors": ["Ran Eisenberg", "Amit Rozner", "Ethan Fetaya", "Ofir Lindenbaum"], "title": "From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation", "comment": null, "summary": "Deep neural networks have achieved remarkable success in computer vision;\nhowever, their black-box nature in decision-making limits interpretability and\ntrust, particularly in safety-critical applications. Interpretability is\ncrucial in domains where errors have severe consequences. Existing models not\nonly lack transparency but also risk exploiting unreliable or misleading\nfeatures, which undermines both robustness and the validity of their\nexplanations. Concept Bottleneck Models (CBMs) aim to improve transparency by\nreasoning through human-interpretable concepts. Still, they require costly\nconcept annotations and lack spatial grounding, often failing to identify which\nregions support each concept. We propose SEG-MIL-CBM, a novel framework that\nintegrates concept-guided image segmentation into an attention-based multiple\ninstance learning (MIL) framework, where each segmented region is treated as an\ninstance and the model learns to aggregate evidence across them. By reasoning\nover semantically meaningful regions aligned with high-level concepts, our\nmodel highlights task-relevant evidence, down-weights irrelevant cues, and\nproduces spatially grounded, concept-level explanations without requiring\nannotations of concepts or groups. SEG-MIL-CBM achieves robust performance\nacross settings involving spurious correlations (unintended dependencies\nbetween background and label), input corruptions (perturbations that degrade\nvisual quality), and large-scale benchmarks, while providing transparent,\nconcept-level explanations.", "AI": {"tldr": "\u63d0\u51fa\u4e86SEG-MIL-CBM\u6846\u67b6\uff0c\u5c06\u6982\u5ff5\u5f15\u5bfc\u7684\u56fe\u50cf\u5206\u5272\u4e0e\u6ce8\u610f\u529b\u591a\u5b9e\u4f8b\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u8bed\u4e49\u533a\u57df\u63a8\u7406\u5b9e\u73b0\u900f\u660e\u3001\u7a7a\u95f4\u5b9a\u4f4d\u7684\u6982\u5ff5\u7ea7\u89e3\u91ca\uff0c\u65e0\u9700\u6982\u5ff5\u6807\u6ce8\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5173\u952e\u5b89\u5168\u5e94\u7528\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u73b0\u6709\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u6982\u5ff5\u6807\u6ce8\u4e14\u7f3a\u4e4f\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u96c6\u6210\u6982\u5ff5\u5f15\u5bfc\u56fe\u50cf\u5206\u5272\u5230\u6ce8\u610f\u529b\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5206\u5272\u533a\u57df\u4f5c\u4e3a\u5b9e\u4f8b\uff0c\u5b66\u4e60\u8de8\u533a\u57df\u8bc1\u636e\u805a\u5408\uff0c\u5b9e\u73b0\u8bed\u4e49\u533a\u57df\u63a8\u7406\u3002", "result": "\u5728\u865a\u5047\u76f8\u5173\u6027\u3001\u8f93\u5165\u635f\u574f\u548c\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u6982\u5ff5\u7ea7\u89e3\u91ca\u3002", "conclusion": "SEG-MIL-CBM\u901a\u8fc7\u8bed\u4e49\u533a\u57df\u63a8\u7406\u5b9e\u73b0\u4e86\u65e0\u9700\u6807\u6ce8\u7684\u900f\u660e\u6982\u5ff5\u89e3\u91ca\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04919", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2510.04919", "abs": "https://arxiv.org/abs/2510.04919", "authors": ["Davood Rafiei", "Morgan Lindsay Heisler", "Weiwei Zhang", "Mohammadreza Pourreza", "Yong Zhang"], "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment", "comment": null, "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86NL2SQL\u4efb\u52a1\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0e\u76ee\u6807\u67e5\u8be2\u7684\u7ed3\u6784\u5bf9\u9f50\u95ee\u9898\uff0c\u53d1\u73b0\u7ed3\u6784\u5bf9\u9f50\u5ea6\u662f\u9884\u6d4b\u5fae\u8c03\u6210\u529f\u7684\u91cd\u8981\u6307\u6807\u3002", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u662f\u9002\u914d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u7684\u53d8\u5f02\u6027\u4f1a\u963b\u788d\u6a21\u578b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76NL2SQL\u4efb\u52a1\u4e2d\u6570\u636e\u96c6\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u8bad\u7ec3\u96c6\u3001\u76ee\u6807\u6570\u636e\u548c\u6a21\u578b\u9884\u6d4b\u4e2dSQL\u7ed3\u6784\u7279\u5f81\u7684\u5206\u5e03\u6765\u4f30\u8ba1\u5bf9\u9f50\u5ea6\uff0c\u5e76\u5728\u4e09\u4e2a\u5927\u578b\u8de8\u9886\u57dfNL2SQL\u57fa\u51c6\u548c\u591a\u4e2a\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u7efc\u5408\u5b9e\u9a8c\u3002", "result": "\u7ed3\u6784\u5bf9\u9f50\u662f\u5fae\u8c03\u6210\u529f\u7684\u5f3a\u9884\u6d4b\u6307\u6807\uff1a\u5f53\u5bf9\u9f50\u5ea6\u9ad8\u65f6\uff0cSFT\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u548cSQL\u751f\u6210\u8d28\u91cf\uff1b\u5f53\u5bf9\u9f50\u5ea6\u4f4e\u65f6\uff0c\u6539\u8fdb\u5f88\u5c0f\u6216\u6ca1\u6709\u6539\u8fdb\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728NL2SQL\u4efb\u52a1\u4e2d\uff0c\u5bf9\u9f50\u611f\u77e5\u7684\u6570\u636e\u9009\u62e9\u5bf9\u4e8e\u6709\u6548\u5fae\u8c03\u548c\u6cdb\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04188", "abs": "https://arxiv.org/abs/2510.04188", "authors": ["Shikang Zheng", "Guantao Chen", "Qinming Zhou", "Yuqi Lin", "Lixuan He", "Chang Zou", "Peiliang Cai", "Jiacheng Liu", "Linfeng Zhang"], "title": "Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers offer state-of-the-art fidelity in image and video\nsynthesis, but their iterative sampling process remains a major bottleneck due\nto the high cost of transformer forward passes at each timestep. To mitigate\nthis, feature caching has emerged as a training-free acceleration technique\nthat reuses or forecasts hidden representations. However, existing methods\noften apply a uniform caching strategy across all feature dimensions, ignoring\ntheir heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by\nmodeling hidden feature evolution as a mixture of ODEs across dimensions, and\nintroduce HyCa, a Hybrid ODE solver inspired caching framework that applies\ndimension-wise caching strategies. HyCa achieves near-lossless acceleration\nacross diverse domains and models, including 5.55 times speedup on FLUX, 5.56\ntimes speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and\nQwen-Image-Edit without retraining.", "AI": {"tldr": "HyCa\u662f\u4e00\u4e2a\u6df7\u5408ODE\u6c42\u89e3\u5668\u542f\u53d1\u7684\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u5ea6\u7ea7\u7f13\u5b58\u7b56\u7565\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u7684\u91c7\u6837\u8fc7\u7a0b\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u5b9e\u73b05-6\u500d\u52a0\u901f\u4e14\u51e0\u4e4e\u65e0\u635f\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u7684\u8fed\u4ee3\u91c7\u6837\u8fc7\u7a0b\u7531\u4e8e\u6bcf\u4e2a\u65f6\u95f4\u6b65\u90fd\u9700\u8981\u8fdb\u884c\u6602\u8d35\u7684\u53d8\u6362\u5668\u524d\u5411\u4f20\u64ad\u800c\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u91c7\u7528\u7edf\u4e00\u7b56\u7565\u5ffd\u7565\u4e86\u7279\u5f81\u7ef4\u5ea6\u7684\u5f02\u8d28\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u5c06\u9690\u85cf\u7279\u5f81\u6f14\u5316\u5efa\u6a21\u4e3a\u8de8\u7ef4\u5ea6\u7684ODE\u6df7\u5408\uff0c\u5f15\u5165HyCa\u6846\u67b6\u5e94\u7528\u7ef4\u5ea6\u7ea7\u7f13\u5b58\u7b56\u7565\uff0c\u57fa\u4e8e\u6df7\u5408ODE\u6c42\u89e3\u5668\u601d\u60f3\u3002", "result": "\u5728FLUX\u4e0a\u5b9e\u73b05.55\u500d\u52a0\u901f\uff0cHunyuanVideo\u4e0a5.56\u500d\u52a0\u901f\uff0cQwen-Image\u548cQwen-Image-Edit\u4e0a6.24\u500d\u52a0\u901f\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "HyCa\u901a\u8fc7\u7ef4\u5ea6\u7ea7\u7f13\u5b58\u7b56\u7565\u6709\u6548\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u91c7\u6837\uff0c\u5728\u591a\u4e2a\u9886\u57df\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u8fd1\u65e0\u635f\u7684\u9ad8\u901f\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.04933", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT", "68T50, 68T07, 62H30", "I.2.7; I.2.6; F.2.2; H.3.3"], "pdf": "https://arxiv.org/pdf/2510.04933", "abs": "https://arxiv.org/abs/2510.04933", "authors": ["Amir Hameed Mir"], "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models", "comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at:\n  https://github.com/sirraya-tech/Sirraya_LSD_Code", "summary": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.", "AI": {"tldr": "\u63d0\u51faLayer-wise Semantic Dynamics (LSD)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790transformer\u5c42\u95f4\u9690\u85cf\u72b6\u6001\u8bed\u4e49\u7684\u52a8\u6001\u53d8\u5316\u6765\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\uff0c\u65e0\u9700\u591a\u6b21\u91c7\u6837\u6216\u5916\u90e8\u9a8c\u8bc1\uff0c\u4ec5\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u9648\u8ff0\uff08\u5e7b\u89c9\u73b0\u8c61\uff09\uff0c\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e26\u6765\u4e25\u91cd\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u8fb9\u754c\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c06\u9690\u85cf\u6fc0\u6d3b\u4e0e\u4e8b\u5b9e\u7f16\u7801\u5668\u751f\u6210\u7684\u771f\u5b9e\u5d4c\u5165\u5bf9\u9f50\uff0c\u5206\u6790\u8bed\u4e49\u8f68\u8ff9\u7684\u5206\u79bb\uff1a\u771f\u5b9e\u56de\u7b54\u4fdd\u6301\u7a33\u5b9a\u5bf9\u9f50\uff0c\u800c\u5e7b\u89c9\u5728\u6df1\u5ea6\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u7684\u8bed\u4e49\u6f02\u79fb\u3002", "result": "\u5728TruthfulQA\u548c\u5408\u6210\u4e8b\u5b9e-\u5e7b\u89c9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLSD\u8fbe\u5230F1\u5206\u65700.92\u3001AUROC 0.96\u548c\u805a\u7c7b\u51c6\u786e\u73870.89\uff0c\u4f18\u4e8eSelfCheckGPT\u548c\u8bed\u4e49\u71b5\u57fa\u7ebf\uff0c\u4e14\u901f\u5ea6\u6bd4\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u5feb5-20\u500d\u3002", "conclusion": "LSD\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u5b9e\u65f6\u5e7b\u89c9\u76d1\u6d4b\u673a\u5236\uff0c\u5e76\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u4e8b\u5b9e\u4e00\u81f4\u6027\u7684\u51e0\u4f55\u7ed3\u6784\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2510.04201", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04201", "abs": "https://arxiv.org/abs/2510.04201", "authors": ["Moo Hyun Son", "Jintaek Oh", "Sun Bin Mun", "Jaechul Roh", "Sehyun Choi"], "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "comment": null, "summary": "While text-to-image (T2I) models can synthesize high-quality images, their\nperformance degrades significantly when prompted with novel or\nout-of-distribution (OOD) entities due to inherent knowledge cutoffs. We\nintroduce World-To-Image, a novel framework that bridges this gap by empowering\nT2I generation with agent-driven world knowledge. We design an agent that\ndynamically searches the web to retrieve images for concepts unknown to the\nbase model. This information is then used to perform multimodal prompt\noptimization, steering powerful generative backbones toward an accurate\nsynthesis. Critically, our evaluation goes beyond traditional metrics,\nutilizing modern assessments like LLMGrader and ImageReward to measure true\nsemantic fidelity. Our experiments show that World-To-Image substantially\noutperforms state-of-the-art methods in both semantic alignment and visual\naesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated\nNICE benchmark. Our framework achieves these results with high efficiency in\nless than three iterations, paving the way for T2I systems that can better\nreflect the ever-changing real world. Our demo code is available\nhere\\footnote{https://github.com/mhson-kyle/World-To-Image}.", "AI": {"tldr": "World-To-Image\u6846\u67b6\u901a\u8fc7\u4ee3\u7406\u9a71\u52a8\u7684\u4e16\u754c\u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u6a21\u578b\u5bf9\u65b0\u9896\u6216\u5206\u5e03\u5916\u5b9e\u4f53\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5904\u7406\u65b0\u9896\u6216\u5206\u5e03\u5916\u5b9e\u4f53\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7531\u4e8e\u56fa\u6709\u7684\u77e5\u8bc6\u622a\u6b62\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u4ee3\u7406\u52a8\u6001\u641c\u7d22\u7f51\u7edc\u83b7\u53d6\u57fa\u7840\u6a21\u578b\u672a\u77e5\u6982\u5ff5\u7684\u56fe\u50cf\uff0c\u7136\u540e\u8fdb\u884c\u591a\u6a21\u6001\u63d0\u793a\u4f18\u5316\uff0c\u5f15\u5bfc\u751f\u6210\u4e3b\u5e72\u5b9e\u73b0\u51c6\u786e\u5408\u6210\u3002", "result": "\u5728NICE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u89c6\u89c9\u7f8e\u5b66\u65b9\u9762\u663e\u8457\u8d85\u8d8a\uff0c\u51c6\u786e\u7387\u63d0\u5347+8.1%\uff0c\u4e14\u4ec5\u9700\u4e0d\u52303\u6b21\u8fed\u4ee3\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u66f4\u597d\u5730\u53cd\u6620\u4e0d\u65ad\u53d8\u5316\u7684\u73b0\u5b9e\u4e16\u754c\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.04945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04945", "abs": "https://arxiv.org/abs/2510.04945", "authors": ["Juan-Jos\u00e9 Guzm\u00e1n-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Ligia Quintana-Torres", "Martha-Lorena Avenda\u00f1o-Garrido", "Graham Ranger"], "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation", "comment": "11 pages, 7 tables, 1 figure", "summary": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.", "AI": {"tldr": "\u4e3a\u7eb3\u74e6\u7279\u5c14\u8bed\u6784\u5efa\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5927\u91cf\u8bed\u6cd5\u6b63\u786e\u7684\u53e5\u5b50\u6765\u6269\u5145\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bad\u7ec3FastText\u7b49\u7b97\u6cd5\uff0c\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u53d6\u5f97\u521d\u6b65\u6539\u8fdb\u3002", "motivation": "\u7eb3\u74e6\u7279\u5c14\u8bed\u4f5c\u4e3a\u03c0\u8bed\u8a00\u7c7b\u578b\uff08\u6570\u5b57\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\uff09\uff0c\u7f3a\u4e4f\u53ef\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8bed\u6599\u5e93\uff0c\u9700\u8981\u751f\u6210\u4eba\u5de5\u53e5\u5b50\u6765\u6269\u5145\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u4f7f\u7528\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u751f\u6210\u5927\u91cf\u8bed\u6cd5\u6b63\u786e\u7684\u7eb3\u74e6\u7279\u5c14\u8bed\u53e5\u5b50\uff0c\u6784\u5efa\u03c0-yalli\u8bed\u6599\u5e93\uff0c\u5e76\u7528\u5176\u8bad\u7ec3FastText\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u8bed\u6cd5\u751f\u6210\u7684\u65b9\u6cd5\u5728\u53e5\u5b50\u7ea7\u8bed\u4e49\u4efb\u52a1\u4e0a\u76f8\u6bd4\u67d0\u4e9b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u6bd4\u8f83\u6027\u6539\u8fdb\uff0c\u4f46\u6539\u8fdb\u5e45\u5ea6\u6709\u9650\u3002", "conclusion": "\u9700\u8981\u6784\u5efa\u66f4\u6709\u6548\u7684\u7eb3\u74e6\u7279\u5c14\u8bed\u8bed\u6cd5\u6a21\u578b\u624d\u80fd\u83b7\u5f97\u66f4\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.04220", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04220", "abs": "https://arxiv.org/abs/2510.04220", "authors": ["Lixuan He", "Shikang Zheng", "Linfeng Zhang"], "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering", "comment": null, "summary": "Autoregressive (AR) models have shown great promise in image generation, yet\nthey face a fundamental inefficiency stemming from their core component: a\nvast, unstructured vocabulary of visual tokens. This conventional approach\ntreats tokens as a flat vocabulary, disregarding the intrinsic structure of the\ntoken embedding space where proximity often correlates with semantic\nsimilarity. This oversight results in a highly complex prediction task, which\nhinders training efficiency and limits final generation quality. To resolve\nthis, we propose Manifold-Aligned Semantic Clustering (MASC), a principled\nframework that constructs a hierarchical semantic tree directly from the\ncodebook's intrinsic structure. MASC employs a novel geometry-aware distance\nmetric and a density-driven agglomerative construction to model the underlying\nmanifold of the token embeddings. By transforming the flat, high-dimensional\nprediction task into a structured, hierarchical one, MASC introduces a\nbeneficial inductive bias that significantly simplifies the learning problem\nfor the AR model. MASC is designed as a plug-and-play module, and our extensive\nexperiments validate its effectiveness: it accelerates training by up to 57%\nand significantly improves generation quality, reducing the FID of LlamaGen-XL\nfrom 2.87 to 2.58. MASC elevates existing AR frameworks to be highly\ncompetitive with state-of-the-art methods, establishing that structuring the\nprediction space is as crucial as architectural innovation for scalable\ngenerative modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86MASC\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5c42\u6b21\u5316\u8bed\u4e49\u6811\u6765\u7ed3\u6784\u5316\u89c6\u89c9\u6807\u8bb0\u7684\u9884\u6d4b\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u5347\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\u4f7f\u7528\u5e73\u5766\u7684\u65e0\u7ed3\u6784\u89c6\u89c9\u6807\u8bb0\u8bcd\u6c47\u8868\uff0c\u5ffd\u7565\u4e86\u6807\u8bb0\u5d4c\u5165\u7a7a\u95f4\u7684\u5185\u5728\u7ed3\u6784\uff0c\u5bfc\u81f4\u9884\u6d4b\u4efb\u52a1\u590d\u6742\uff0c\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u4e14\u751f\u6210\u8d28\u91cf\u53d7\u9650\u3002", "method": "MASC\u6846\u67b6\u4f7f\u7528\u51e0\u4f55\u611f\u77e5\u8ddd\u79bb\u5ea6\u91cf\u548c\u5bc6\u5ea6\u9a71\u52a8\u7684\u51dd\u805a\u6784\u5efa\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u7801\u672c\u5185\u5728\u7ed3\u6784\u6784\u5efa\u5c42\u6b21\u5316\u8bed\u4e49\u6811\uff0c\u5c06\u9ad8\u7ef4\u5e73\u5766\u9884\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5c42\u6b21\u4efb\u52a1\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe57%\uff0cLlamaGen-XL\u7684FID\u4ece2.87\u964d\u81f32.58\uff0c\u663e\u8457\u6539\u5584\u751f\u6210\u8d28\u91cf\uff0c\u4f7f\u73b0\u6709\u81ea\u56de\u5f52\u6846\u67b6\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u7ed3\u6784\u5316\u9884\u6d4b\u7a7a\u95f4\u5bf9\u4e8e\u53ef\u6269\u5c55\u751f\u6210\u5efa\u6a21\u4e0e\u67b6\u6784\u521b\u65b0\u540c\u7b49\u91cd\u8981\uff0cMASC\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u80fd\u6709\u6548\u63d0\u5347\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2510.04950", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04950", "abs": "https://arxiv.org/abs/2510.04950", "authors": ["Om Dobariya", "Akhil Kumar"], "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)", "comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations\n  sections; short paper under submission to Findings of ACL 2025", "summary": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0d\u793c\u8c8c\u63d0\u793a\u6bd4\u793c\u8c8c\u63d0\u793a\u5728LLMs\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u51c6\u786e\u7387\u4ece\u975e\u5e38\u793c\u8c8c\u768480.8%\u63d0\u5347\u5230\u975e\u5e38\u7c97\u9c81\u768484.8%\uff0c\u4e0e\u4f20\u7edf\u8ba4\u77e5\u76f8\u53cd\u3002", "motivation": "\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4e2d\u793c\u8c8c\u7a0b\u5ea6\u548c\u8bed\u6c14\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u591a\u9009\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u6027\u3002", "method": "\u521b\u5efa50\u4e2a\u57fa\u7840\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u6539\u5199\u4e3a5\u79cd\u8bed\u6c14\u53d8\u4f53\uff08\u975e\u5e38\u793c\u8c8c\u3001\u793c\u8c8c\u3001\u4e2d\u6027\u3001\u7c97\u9c81\u3001\u975e\u5e38\u7c97\u9c81\uff09\uff0c\u4f7f\u7528ChatGPT 4o\u8bc4\u4f30\u54cd\u5e94\uff0c\u5e76\u5e94\u7528\u914d\u5bf9\u6837\u672ct\u68c0\u9a8c\u5206\u6790\u7edf\u8ba1\u663e\u8457\u6027\u3002", "result": "\u4e0d\u793c\u8c8c\u63d0\u793a\u6301\u7eed\u4f18\u4e8e\u793c\u8c8c\u63d0\u793a\uff0c\u51c6\u786e\u7387\u5206\u522b\u4e3a\uff1a\u975e\u5e38\u793c\u8c8c80.8%\u3001\u793c\u8c8c82.0%\u3001\u4e2d\u602783.2%\u3001\u7c97\u9c8184.0%\u3001\u975e\u5e38\u7c97\u9c8184.8%\u3002", "conclusion": "\u65b0LLMs\u5bf9\u8bed\u6c14\u53d8\u5316\u7684\u54cd\u5e94\u53ef\u80fd\u4e0e\u65e9\u671f\u7814\u7a76\u4e0d\u540c\uff0c\u5f3a\u8c03\u7814\u7a76\u63d0\u793a\u8bed\u7528\u65b9\u9762\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f15\u53d1\u5173\u4e8e\u4eba\u673a\u4ea4\u4e92\u793e\u4f1a\u7ef4\u5ea6\u7684\u66f4\u5e7f\u6cdb\u95ee\u9898\u3002"}}
{"id": "2510.04225", "categories": ["cs.CV", "cs.AI", "cs.CL", "68T45", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.04225", "abs": "https://arxiv.org/abs/2510.04225", "authors": ["Yikun Ji", "Yan Hong", "Bowen Deng", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Zoom-In to Sort AI-Generated Images Out", "comment": "9 pages, 6 images (19 pages, 11 figures including appendix)", "summary": "The rapid growth of AI-generated imagery has blurred the boundary between\nreal and synthetic content, raising critical concerns for digital integrity.\nVision-language models (VLMs) offer interpretability through explanations but\noften fail to detect subtle artifacts in high-quality synthetic images. We\npropose ZoomIn, a two-stage forensic framework that improves both accuracy and\ninterpretability. Mimicking human visual inspection, ZoomIn first scans an\nimage to locate suspicious regions and then performs a focused analysis on\nthese zoomed-in areas to deliver a grounded verdict. To support training, we\nintroduce MagniFake, a dataset of 20,000 real and high-quality synthetic images\nannotated with bounding boxes and forensic explanations, generated through an\nautomated VLM-based pipeline. Our method achieves 96.39% accuracy with robust\ngeneralization, while providing human-understandable explanations grounded in\nvisual evidence.", "AI": {"tldr": "ZoomIn\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53d6\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u68c0\u67e5\uff0c\u5148\u626b\u63cf\u56fe\u50cf\u5b9a\u4f4d\u53ef\u7591\u533a\u57df\uff0c\u7136\u540e\u5bf9\u8fd9\u4e9b\u653e\u5927\u533a\u57df\u8fdb\u884c\u805a\u7126\u5206\u6790\uff0c\u63d0\u9ad8AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\u7684\u5feb\u901f\u589e\u957f\u6a21\u7cca\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u5185\u5bb9\u7684\u8fb9\u754c\uff0c\u5bf9\u6570\u5b57\u5b8c\u6574\u6027\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5728\u68c0\u6d4b\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u7ec6\u5fae\u4f2a\u5f71\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faZoomIn\u4e24\u9636\u6bb5\u53d6\u8bc1\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u626b\u63cf\u56fe\u50cf\u5b9a\u4f4d\u53ef\u7591\u533a\u57df\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u8fd9\u4e9b\u653e\u5927\u533a\u57df\u8fdb\u884c\u805a\u7126\u5206\u6790\u3002\u521b\u5efa\u4e86MagniFake\u6570\u636e\u96c6\uff0820,000\u5f20\u771f\u5b9e\u548c\u9ad8\u8d28\u91cf\u5408\u6210\u56fe\u50cf\uff0c\u5e26\u6709\u8fb9\u754c\u6846\u548c\u53d6\u8bc1\u89e3\u91ca\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u5316VLM\u6d41\u7a0b\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u65b9\u6cd5\u8fbe\u523096.39%\u7684\u51c6\u786e\u7387\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\u7684\u4eba\u7c7b\u53ef\u7406\u89e3\u89e3\u91ca\u3002", "conclusion": "ZoomIn\u6846\u67b6\u5728AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u89c6\u89c9\u68c0\u67e5\u8fc7\u7a0b\uff0c\u4e3a\u6570\u5b57\u53d6\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04983", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04983", "abs": "https://arxiv.org/abs/2510.04983", "authors": ["Khalid Mehtab Khan", "Anagha Kulkarni"], "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives", "comment": null, "summary": "Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative.", "AI": {"tldr": "AWARE\u6846\u67b6\u901a\u8fc7\u63d0\u5347transformer\u6a21\u578b\u5728\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u548c\u7c7b\u522b\u91cd\u53e0\u4e09\u4e2a\u7ef4\u5ea6\u7684\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u5b66\u751f\u53cd\u601d\u4e2d\u8bc6\u522b\u6587\u5316\u8d44\u672c\u4e3b\u9898\u7684\u6027\u80fd\u3002", "motivation": "\u5b66\u751f\u53cd\u601d\u4e2d\u7684\u6587\u5316\u8d44\u672c\u4e3b\u9898\uff08\u5982\u5fd7\u5411\u76ee\u6807\u3001\u5bb6\u5ead\u652f\u6301\uff09\u901a\u5e38\u4ee5\u53d9\u8ff0\u65b9\u5f0f\u5448\u73b0\u800c\u975e\u76f4\u63a5\u5173\u952e\u8bcd\uff0c\u6807\u51c6NLP\u6a21\u578b\u56e0\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\u800c\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u63d0\u51faAWARE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u9886\u57df\u611f\u77e5\uff08\u9002\u5e94\u5b66\u751f\u53cd\u601d\u7684\u8bed\u8a00\u98ce\u683c\uff09\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\uff08\u751f\u6210\u8003\u8651\u5168\u6587\u80cc\u666f\u7684\u53e5\u5b50\u5d4c\u5165\uff09\u3001\u7c7b\u522b\u91cd\u53e0\u611f\u77e5\uff08\u4f7f\u7528\u591a\u6807\u7b7e\u7b56\u7565\u8bc6\u522b\u53e5\u5b50\u4e2d\u5e76\u5b58\u7684\u4e3b\u9898\uff09\u3002", "result": "AWARE\u5728Macro-F1\u4e0a\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u53472.1\u4e2a\u767e\u5206\u70b9\uff0c\u5728\u6240\u6709\u4e3b\u9898\u4e0a\u5747\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4efb\u4f55\u4f9d\u8d56\u53d9\u8ff0\u4e0a\u4e0b\u6587\u7684\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2510.04231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04231", "abs": "https://arxiv.org/abs/2510.04231", "authors": ["Stefan Dirnstorfer"], "title": "A Recursive Pyramidal Algorithm for Solving the Image Registration Problem", "comment": null, "summary": "The problem of image registration is finding a transformation that aligns two\nimages, such that the corresponding points are in the same location. This paper\nintroduces a simple, end-to-end trainable algorithm that is implementable in a\nfew lines of Python code. The approach is shown to work with very little\ntraining data and training time, while achieving accurate results in some\nsettings. An example application to stereo vision was trained from 74 images on\na 19x15 input window. With just a dozen lines of Python code this algorithm\nexcels in brevity and may serve as a good start in related scenarios with\nlimitations to training data, training time or code complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u56fe\u50cf\u914d\u51c6\u7b97\u6cd5\uff0c\u53ea\u9700\u5c11\u91cfPython\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u83b7\u5f97\u51c6\u786e\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u914d\u51c6\u95ee\u9898\uff0c\u5373\u627e\u5230\u4f7f\u4e24\u5e45\u56fe\u50cf\u5bf9\u5e94\u70b9\u4f4d\u7f6e\u4e00\u81f4\u7684\u53d8\u6362\uff0c\u9700\u8981\u4e00\u4e2a\u7b80\u5355\u6613\u5b9e\u73b0\u4e14\u5bf9\u8bad\u7ec3\u8d44\u6e90\u8981\u6c42\u4f4e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u7aef\u5230\u7aef\u53ef\u8bad\u7ec3\u7684\u7b97\u6cd5\uff0c\u4ec5\u9700\u5c11\u91cfPython\u4ee3\u7801\u5373\u53ef\u5b9e\u73b0\uff0c\u4f7f\u752874\u5f20\u56fe\u50cf\u572819x15\u8f93\u5165\u7a97\u53e3\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u7acb\u4f53\u89c6\u89c9\u7b49\u5e94\u7528\u573a\u666f\u3002", "result": "\u8be5\u7b97\u6cd5\u5728\u8bad\u7ec3\u6570\u636e\u548c\u8bad\u7ec3\u65f6\u95f4\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u83b7\u5f97\u51c6\u786e\u7ed3\u679c\uff0c\u4ee3\u7801\u7b80\u6d01\uff08\u4ec5\u9700\u5341\u51e0\u884cPython\uff09\uff0c\u5728\u76f8\u5173\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u4ee3\u7801\u7b80\u6d01\u6027\u3001\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u548c\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u3001\u8bad\u7ec3\u65f6\u95f4\u6216\u4ee3\u7801\u590d\u6742\u5ea6\u53d7\u9650\u573a\u666f\u7684\u826f\u597d\u8d77\u70b9\u3002"}}
{"id": "2510.05003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05003", "abs": "https://arxiv.org/abs/2510.05003", "authors": ["Imran Mansha"], "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning", "comment": "6 pages, 2 figures. Submitted to arXiv for open access", "summary": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LLaMA-3.2-3B\u7684\u8d44\u6e90\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528LoRA\u548cQLoRA\u6280\u672f\u5728\u6709\u9650GPU\u548c\u5185\u5b58\u6761\u4ef6\u4e0b\u63d0\u5347\u533b\u5b66\u94fe\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u5168\u53c2\u6570\u5fae\u8c03\u51cf\u5c1160%\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982GPT-4\u548cLLaMA\u5177\u6709\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5168\u53c2\u6570\u5fae\u8c03\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u9886\u57df\u5e94\u7528\u4e2d\u9762\u4e34\u8d44\u6e90\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08LoRA\u548cQLoRA\uff09\uff0c\u5728\u516c\u5f00\u53ef\u7528\u7684\u533b\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5bf9LLaMA-3.2-3B\u6a21\u578b\u8fdb\u884c\u9002\u914d\uff0c\u5b9e\u73b0\u5728\u53d7\u9650GPU\u548c\u5185\u5b58\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u5728\u533b\u5b66\u63a8\u7406\u4e00\u81f4\u6027\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u65b9\u9762\u5f97\u5230\u63d0\u5347\uff0c\u540c\u65f6\u5185\u5b58\u4f7f\u7528\u91cf\u76f8\u6bd4\u6807\u51c6\u5168\u5fae\u8c03\u51cf\u5c11\u9ad8\u8fbe60%\uff0c\u5728\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u4f4e\u8d44\u6e90\u7814\u7a76\u73af\u5883\u4e2d\u90e8\u7f72LLM\u7684\u5b9e\u7528\u7b56\u7565\uff0c\u4e3a\u5e73\u8861\u6548\u7387\u548c\u9886\u57df\u4e13\u4e1a\u5316\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u7279\u522b\u9002\u7528\u4e8e\u533b\u7597AI\u7cfb\u7edf\u3002"}}
{"id": "2510.04232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04232", "abs": "https://arxiv.org/abs/2510.04232", "authors": ["Amin Ahmadi Kasani", "Hedieh Sajedi"], "title": "Detection of retinal diseases using an accelerated reused convolutional network", "comment": null, "summary": "Convolutional neural networks are continually evolving, with some efforts\naimed at improving accuracy, others at increasing speed, and some at enhancing\naccessibility. Improving accessibility broadens the application of neural\nnetworks across a wider range of tasks, including the detection of eye\ndiseases. Early diagnosis of eye diseases and consulting an ophthalmologist can\nprevent many vision disorders. Given the importance of this issue, various\ndatasets have been collected from the cornea to facilitate the process of\nmaking neural network models. However, most of the methods introduced in the\npast are computationally complex. In this study, we tried to increase the\naccessibility of deep neural network models. We did this at the most\nfundamental level, specifically by redesigning and optimizing the convolutional\nlayers. By doing so, we created a new general model that incorporates our novel\nconvolutional layer named ArConv layers. Thanks to the efficient performance of\nthis new layer, the model has suitable complexity for use in mobile phones and\ncan perform the task of diagnosing the presence of disease with high accuracy.\nThe final model we present contains only 1.3 million parameters. In comparison\nto the MobileNetV2 model, which has 2.2 million parameters, our model\ndemonstrated better accuracy when trained and evaluated on the RfMiD dataset\nunder identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on\nthe RfMiD test set.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aArConv\u7684\u65b0\u578b\u5377\u79ef\u5c42\uff0c\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u5377\u79ef\u5c42\u6765\u521b\u5efa\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u4ec5\u542b130\u4e07\u53c2\u6570\uff0c\u5728RfMiD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eMobileNetV2\uff0c\u51c6\u786e\u7387\u8fbe\u52300.9328\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u8fd0\u884c\uff0c\u7279\u522b\u662f\u5728\u773c\u79d1\u75be\u75c5\u8bca\u65ad\u7b49\u533b\u7597\u5e94\u7528\u9886\u57df\uff0c\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u6765\u6269\u5927\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5728\u57fa\u7840\u5c42\u9762\u91cd\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\u5377\u79ef\u5c42\uff0c\u5f00\u53d1\u4e86\u65b0\u578bArConv\u5377\u79ef\u5c42\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u67b6\u6784\uff0c\u53c2\u6570\u6570\u91cf\u4ec5\u4e3a130\u4e07\u3002", "result": "\u5728RfMiD\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u6a21\u578b\u51c6\u786e\u7387\u8fbe\u52300.9328\uff0c\u4f18\u4e8e\u5177\u6709220\u4e07\u53c2\u6570\u7684MobileNetV2\u6a21\u578b\uff08\u51c6\u786e\u73870.9266\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684ArConv\u5c42\u8bbe\u8ba1\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u533b\u7597\u8bca\u65ad\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6613\u8bbf\u95ee\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05025", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.05025", "abs": "https://arxiv.org/abs/2510.05025", "authors": ["Kuofeng Gao", "Yiming Li", "Chao Du", "Xin Wang", "Xingjun Ma", "Shu-Tao Xia", "Tianyu Pang"], "title": "Imperceptible Jailbreaking against Large Language Models", "comment": null, "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528Unicode\u53d8\u4f53\u9009\u62e9\u5668\u5b9e\u73b0\u4e0d\u53ef\u611f\u77e5\u8d8a\u72f1\u653b\u51fb\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u6076\u610f\u95ee\u9898\u540e\u9644\u52a0\u4e0d\u53ef\u89c1\u7684\u53d8\u4f53\u9009\u62e9\u5668\uff0c\u4f7f\u63d0\u793a\u5728\u89c6\u89c9\u4e0a\u4e0e\u539f\u59cb\u6076\u610f\u95ee\u9898\u76f8\u540c\uff0c\u4f46tokenization\u88ab\u79d8\u5bc6\u6539\u53d8\uff0c\u4ece\u800c\u8bf1\u5bfc\u6709\u5bb3\u54cd\u5e94\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u6a21\u6001\u7684\u8d8a\u72f1\u653b\u51fb\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e0d\u53ef\u611f\u77e5\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u800c\u6587\u672c\u6a21\u6001\u7684\u653b\u51fb\u901a\u5e38\u9700\u8981\u53ef\u89c1\u4fee\u6539\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u6587\u672c\u6a21\u6001\u4e2d\u4e0d\u53ef\u611f\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u94fe\u5f0f\u641c\u7d22\u6d41\u6c34\u7ebf\u6765\u751f\u6210\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u5229\u7528Unicode\u53d8\u4f53\u9009\u62e9\u5668\u8fd9\u7c7b\u5b57\u7b26\uff0c\u4f7f\u6076\u610f\u95ee\u9898\u5728\u5c4f\u5e55\u4e0a\u770b\u8d77\u6765\u4e0e\u539f\u59cb\u95ee\u9898\u5b8c\u5168\u76f8\u540c\uff0c\u4f46tokenization\u88ab\u79d8\u5bc6\u6539\u53d8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u4e0d\u53ef\u611f\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\u5bf9\u56db\u4e2a\u5bf9\u9f50\u7684LLM\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4e14\u4e0d\u4f1a\u5728\u4e66\u9762\u63d0\u793a\u4e2d\u4ea7\u751f\u4efb\u4f55\u53ef\u89c1\u4fee\u6539\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528Unicode\u53d8\u4f53\u9009\u62e9\u5668\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6587\u672c\u6a21\u6001\u4e2d\u5b8c\u5168\u4e0d\u53ef\u611f\u77e5\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u8fd9\u63ed\u793a\u4e86\u5f53\u524dLLM\u5b89\u5168\u673a\u5236\u4e2d\u7684\u6f5c\u5728\u6f0f\u6d1e\u3002"}}
{"id": "2510.04236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04236", "abs": "https://arxiv.org/abs/2510.04236", "authors": ["Shikun Liu", "Kam Woh Ng", "Wonbong Jang", "Jiadong Guo", "Junlin Han", "Haozhe Liu", "Yiannis Douratsos", "Juan C. P\u00e9rez", "Zijian Zhou", "Chi Phung", "Tao Xiang", "Juan-Manuel P\u00e9rez-R\u00faa"], "title": "Scaling Sequence-to-Sequence Generative Neural Rendering", "comment": "Project Page: https://shikun.io/projects/kaleido", "summary": "We present Kaleido, a family of generative models designed for\nphotorealistic, unified object- and scene-level neural rendering. Kaleido\noperates on the principle that 3D can be regarded as a specialised sub-domain\nof video, expressed purely as a sequence-to-sequence image synthesis task.\nThrough a systemic study of scaling sequence-to-sequence generative neural\nrendering, we introduce key architectural innovations that enable our model to:\ni) perform generative view synthesis without explicit 3D representations; ii)\ngenerate any number of 6-DoF target views conditioned on any number of\nreference views via a masked autoregressive framework; and iii) seamlessly\nunify 3D and video modelling within a single decoder-only rectified flow\ntransformer. Within this unified framework, Kaleido leverages large-scale video\ndata for pre-training, which significantly improves spatial consistency and\nreduces reliance on scarce, camera-labelled 3D datasets -- all without any\narchitectural modifications. Kaleido sets a new state-of-the-art on a range of\nview synthesis benchmarks. Its zero-shot performance substantially outperforms\nother generative methods in few-view settings, and, for the first time, matches\nthe quality of per-scene optimisation methods in many-view settings.", "AI": {"tldr": "Kaleido\u662f\u4e00\u4e2a\u7528\u4e8e\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u795e\u7ecf\u6e32\u67d3\u7684\u751f\u6210\u6a21\u578b\uff0c\u5c063D\u89c6\u4e3a\u89c6\u9891\u7684\u7279\u6b8a\u5b50\u57df\uff0c\u901a\u8fc7\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u56fe\u50cf\u5408\u6210\u5b9e\u73b0\u751f\u6210\u5f0f\u89c6\u56fe\u5408\u6210\uff0c\u65e0\u9700\u663e\u5f0f3D\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u5bf9\u663e\u5f0f3D\u8868\u793a\u548c\u7a00\u7f3a\u76f8\u673a\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u901a\u8fc7\u7edf\u4e003D\u548c\u89c6\u9891\u5efa\u6a21\u6846\u67b6\uff0c\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u751f\u6210\u5f0f\u795e\u7ecf\u6e32\u67d3\uff0c\u4f7f\u7528\u63a9\u7801\u81ea\u56de\u5f52\u6846\u67b6\u548c\u6574\u6d41\u6d41\u53d8\u6362\u5668\uff0c\u80fd\u591f\u57fa\u4e8e\u4efb\u610f\u6570\u91cf\u7684\u53c2\u8003\u89c6\u56fe\u751f\u6210\u4efb\u610f\u6570\u91cf\u76846\u81ea\u7531\u5ea6\u76ee\u6807\u89c6\u56fe\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u56fe\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u96f6\u6837\u672c\u6027\u80fd\u5728\u5c11\u89c6\u56fe\u8bbe\u7f6e\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u89c6\u56fe\u8bbe\u7f6e\u4e2d\u9996\u6b21\u8fbe\u5230\u9010\u573a\u666f\u4f18\u5316\u65b9\u6cd5\u7684\u6c34\u5e73\u3002", "conclusion": "Kaleido\u901a\u8fc7\u5c063D\u5efa\u6a21\u7edf\u4e00\u5230\u89c6\u9891\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u751f\u6210\u5f0f\u795e\u7ecf\u6e32\u67d3\uff0c\u51cf\u5c11\u4e86\u5bf9\u7a00\u7f3a3D\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002"}}
{"id": "2510.05026", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05026", "abs": "https://arxiv.org/abs/2510.05026", "authors": ["David Beauchemin", "Yan Tremblay", "Mohamed Amine Youssef", "Richard Khoury"], "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms", "comment": "Submitted to ACL Rolling Review of October", "summary": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u9b41\u5317\u514b\u6cd5\u8bed\u65b9\u8a00\u7406\u89e3\u57fa\u51c6\u6570\u636e\u96c6QFrCoRE\u548cQFrCoRT\uff0c\u901a\u8fc7\u533a\u57df\u4e60\u8bed\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u7684\u65b9\u8a00\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5c06\u4e60\u8bed\u7406\u89e3\u548c\u65b9\u8a00\u7406\u89e3\u4e24\u4e2a\u4efb\u52a1\u7ed3\u5408\uff0c\u4f7f\u7528\u533a\u57df\u4e60\u8bed\u4f5c\u4e3a\u65b9\u8a00\u7406\u89e3\u7684\u6d4b\u8bd5\u6807\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4,633\u4e2a\u4e60\u8bed\u77ed\u8bed\u7684QFrCoRE\u6570\u636e\u96c6\u548c171\u4e2a\u533a\u57df\u4e60\u8bed\u8bcd\u7684QFrCoRT\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u8bba\u3002", "result": "\u5bf994\u4e2aLLM\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u533a\u57df\u4e60\u8bed\u57fa\u51c6\u662f\u8861\u91cf\u6a21\u578b\u5728\u7279\u5b9a\u65b9\u8a00\u4e2d\u719f\u7ec3\u5ea6\u7684\u53ef\u9760\u5de5\u5177\u3002", "conclusion": "\u533a\u57df\u4e60\u8bed\u57fa\u51c6\u80fd\u591f\u6709\u6548\u8bc4\u4f30LLM\u7684\u65b9\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u65b9\u8a00\u3002"}}
{"id": "2510.04243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04243", "abs": "https://arxiv.org/abs/2510.04243", "authors": ["Jincan Lou", "Jingkun Chen", "Haoquan Li", "Hang Li", "Wenjian Huang", "Weihua Chen", "Fan Wang", "Jianguo Zhang"], "title": "The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation", "comment": "11 pages, 3 figures", "summary": "Accurate liver segmentation from contrast-enhanced MRI is essential for\ndiagnosis, treatment planning, and disease monitoring. However, it remains\nchallenging due to limited annotated data, heterogeneous enhancement protocols,\nand significant domain shifts across scanners and institutions. Traditional\nimage-to-image translation frameworks have made great progress in domain\ngeneralization, but their application is not straightforward. For example,\nPix2Pix requires image registration, and cycle-GAN cannot be integrated\nseamlessly into segmentation pipelines. Meanwhile, these methods are originally\nused to deal with cross-modality scenarios, and often introduce structural\ndistortions and suffer from unstable training, which may pose drawbacks in our\nsingle-modality scenario. To address these challenges, we propose CoSSeg-TTA, a\ncompact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary\nphase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised\nmean teacher scheme to exploit large amounts of unlabeled volumes. A domain\nadaptation module, incorporating a randomized histogram-based style appearance\ntransfer function and a trainable contrast-aware network, enriches domain\ndiversity and mitigates cross-center variability. Furthermore, a continual\ntest-time adaptation strategy is employed to improve robustness during\ninference. Extensive experiments demonstrate that our framework consistently\noutperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff\nDistance while exhibiting strong generalization to unseen domains under\nlow-annotation conditions.", "AI": {"tldr": "\u63d0\u51faCoSSeg-TTA\u6846\u67b6\uff0c\u5728nnU-Netv2\u57fa\u7840\u4e0a\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u9886\u57df\u9002\u5e94\u6280\u672f\uff0c\u7528\u4e8eGd-EOB-DTPA\u589e\u5f3aMRI\u7684\u809d\u810f\u5206\u5272\uff0c\u5728\u4f4e\u6807\u6ce8\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4f18\u5f02\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5bf9\u6bd4\u589e\u5f3aMRI\u809d\u810f\u5206\u5272\u9762\u4e34\u7684\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u589e\u5f3a\u65b9\u6848\u5f02\u8d28\u6027\u548c\u8de8\u626b\u63cf\u4eea/\u673a\u6784\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002\u4f20\u7edf\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u626d\u66f2\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7b49\u5c40\u9650\u3002", "method": "\u57fa\u4e8ennU-Netv2\u6784\u5efa\u7d27\u51d1\u5206\u5272\u6846\u67b6\uff0c\u91c7\u7528\u534a\u76d1\u7763\u5747\u503c\u6559\u5e08\u65b9\u6848\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u96c6\u6210\u968f\u673a\u76f4\u65b9\u56fe\u98ce\u683c\u8f6c\u6362\u548c\u53ef\u8bad\u7ec3\u5bf9\u6bd4\u611f\u77e5\u7f51\u7edc\u7684\u9886\u57df\u9002\u5e94\u6a21\u5757\uff0c\u5e76\u4f7f\u7528\u6301\u7eed\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u6301\u7eed\u4f18\u4e8ennU-Netv2\u57fa\u7ebf\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684Dice\u5206\u6570\u548c\u66f4\u4f18\u7684Hausdorff\u8ddd\u79bb\uff0c\u5728\u4f4e\u6807\u6ce8\u6761\u4ef6\u4e0b\u5bf9\u672a\u89c1\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CoSSeg-TTA\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u3001\u9886\u57df\u9002\u5e94\u548c\u6d4b\u8bd5\u65f6\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u6a21\u6001MRI\u809d\u810f\u5206\u5272\u4e2d\u7684\u9886\u57df\u6cdb\u5316\u6311\u6218\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2510.05038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05038", "abs": "https://arxiv.org/abs/2510.05038", "authors": ["Omri Uzan", "Asaf Yehudai", "Roi pony", "Eyal Shnarch", "Ariel Gera"], "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization", "comment": null, "summary": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval", "AI": {"tldr": "\u63d0\u51faGQR\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6587\u672c\u68c0\u7d22\u5668\u589e\u5f3a\u89c6\u89c9\u4e2d\u5fc3\u6a21\u578b\uff0c\u5728\u89c6\u89c9\u6587\u6863\u68c0\u7d22\u4e2d\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u5b58\u5728\u8868\u793a\u89c4\u6a21\u8fc7\u5927\u3001\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e14\u7eaf\u89c6\u89c9\u65b9\u6cd5\u53d7\u9650\u4e8e\u6a21\u6001\u5dee\u8ddd\u3002\u4f20\u7edf\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92", "method": "\u63d0\u51fa\u5f15\u5bfc\u67e5\u8be2\u4f18\u5316(GQR)\uff0c\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u4e92\u8865\u68c0\u7d22\u5668\u7684\u5206\u6570\u6765\u4f18\u5316\u4e3b\u68c0\u7d22\u5668\u7684\u67e5\u8be2\u5d4c\u5165", "result": "GQR\u4f7f\u89c6\u89c9\u4e2d\u5fc3\u6a21\u578b\u80fd\u591f\u5339\u914d\u66f4\u5927\u8868\u793a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u534714\u500d\uff0c\u5185\u5b58\u9700\u6c42\u51cf\u5c1154\u500d", "conclusion": "GQR\u6709\u6548\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u68c0\u7d22\u5728\u6027\u80fd\u4e0e\u6548\u7387\u65b9\u9762\u7684\u5e15\u7d2f\u6258\u524d\u6cbf"}}
{"id": "2510.04245", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04245", "abs": "https://arxiv.org/abs/2510.04245", "authors": ["Ayushi Mehrotra", "Derek Peng", "Dipkamal Bhusal", "Nidhi Rastogi"], "title": "Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks", "comment": "neurips workshop", "summary": "Adversarial patch attacks pose a practical threat to deep learning models by\nforcing targeted misclassifications through localized perturbations, often\nrealized in the physical world. Existing defenses typically assume prior\nknowledge of patch size or location, limiting their applicability. In this\nwork, we propose a patch-agnostic defense that leverages concept-based\nexplanations to identify and suppress the most influential concept activation\nvectors, thereby neutralizing patch effects without explicit detection.\nEvaluated on Imagenette with a ResNet-50, our method achieves higher robust and\nclean accuracy than the state-of-the-art PatchCleanser, while maintaining\nstrong performance across varying patch sizes and locations. Our results\nhighlight the promise of combining interpretability with robustness and suggest\nconcept-driven defenses as a scalable strategy for securing machine learning\nmodels against adversarial patch attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u89e3\u91ca\u7684\u5bf9\u6297\u6027\u8865\u4e01\u9632\u5fa1\u65b9\u6cd5\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053\u8865\u4e01\u5927\u5c0f\u6216\u4f4d\u7f6e\uff0c\u901a\u8fc7\u6291\u5236\u6700\u5177\u5f71\u54cd\u529b\u7684\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\u6765\u4e2d\u548c\u8865\u4e01\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u6027\u8865\u4e01\u9632\u5fa1\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u9884\u5148\u77e5\u9053\u8865\u4e01\u5927\u5c0f\u6216\u4f4d\u7f6e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u6765\u8bc6\u522b\u548c\u6291\u5236\u6700\u5177\u5f71\u54cd\u529b\u7684\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff0c\u4ece\u800c\u5728\u4e0d\u8fdb\u884c\u663e\u5f0f\u68c0\u6d4b\u7684\u60c5\u51b5\u4e0b\u4e2d\u548c\u8865\u4e01\u6548\u679c\u3002", "result": "\u5728Imagenette\u6570\u636e\u96c6\u548cResNet-50\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u5e72\u51c0\u51c6\u786e\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684PatchCleanser\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u8865\u4e01\u5927\u5c0f\u548c\u4f4d\u7f6e\u4e0b\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u5c06\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\u76f8\u7ed3\u5408\u5177\u6709\u524d\u666f\uff0c\u6982\u5ff5\u9a71\u52a8\u7684\u9632\u5fa1\u7b56\u7565\u662f\u5bf9\u6297\u5bf9\u6297\u6027\u8865\u4e01\u653b\u51fb\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002"}}
{"id": "2510.05046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.05046", "abs": "https://arxiv.org/abs/2510.05046", "authors": ["David Beauchemin", "Yan Tremblay", "Mohamed Amine Youssef", "Richard Khoury"], "title": "COLE: a Comprehensive Benchmark for French Language Understanding Evaluation", "comment": "Submitted to ACL Rolling Review of October", "summary": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling.", "AI": {"tldr": "COLE\u662f\u4e00\u4e2a\u65b0\u7684\u6cd5\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b23\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8694\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524dLLM\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6cd5\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u8bc4\u4f30\u4e0d\u591f\u5168\u9762\u7684\u95ee\u9898\uff0c\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u6db5\u76d6\u5e7f\u6cdbNLU\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u4e0e\u6cd5\u8bed\u8bed\u8a00\u76f8\u5173\u7684\u8bed\u8a00\u73b0\u8c61\u3002", "method": "\u5f15\u5165\u4e86COLE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b23\u4e2a\u591a\u6837\u5316\u4efb\u52a1\uff0c\u6db5\u76d6\u60c5\u611f\u5206\u6790\u3001\u590d\u8ff0\u68c0\u6d4b\u3001\u8bed\u6cd5\u5224\u65ad\u548c\u63a8\u7406\u7b49\u80fd\u529b\uff0c\u5e76\u5bf994\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e86\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524dLLM\u7684\u5173\u952e\u6311\u6218\u9886\u57df\uff0c\u5305\u62ec\u96f6\u6837\u672c\u62bd\u53d6\u5f0f\u95ee\u7b54\u3001\u7ec6\u7c92\u5ea6\u8bcd\u4e49\u6d88\u6b67\u548c\u533a\u57df\u8bed\u8a00\u53d8\u4f53\u7406\u89e3\u3002", "conclusion": "COLE\u4f5c\u4e3a\u516c\u5171\u8d44\u6e90\u53d1\u5e03\uff0c\u65e8\u5728\u4fc3\u8fdb\u6cd5\u8bed\u8bed\u8a00\u5efa\u6a21\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2510.04282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04282", "abs": "https://arxiv.org/abs/2510.04282", "authors": ["Yu Kiu", "Lau", "Chao Chen", "Ge Jin", "Chen Feng"], "title": "Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition", "comment": "8 pages, 6 figures", "summary": "Sequential Visual Place Recognition (Seq-VPR) leverages transformers to\ncapture spatio-temporal features effectively; however, existing approaches\nprioritize performance at the expense of flexibility and efficiency. In\npractice, a transformer-based Seq-VPR model should be flexible to the number of\nframes per sequence (seq-length), deliver fast inference, and have low memory\nusage to meet real-time constraints. To our knowledge, no existing\ntransformer-based Seq-VPR method achieves both flexibility and efficiency. To\naddress this gap, we propose Adapt-STformer, a Seq-VPR method built around our\nnovel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an\niterative recurrent mechanism to fuse information from multiple sequential\nframes. This design naturally supports variable seq-lengths, fast inference,\nand low memory usage. Experiments on the Nordland, Oxford, and NuScenes\ndatasets show that Adapt-STformer boosts recall by up to 17% while reducing\nsequence extraction time by 36% and lowering memory usage by 35% compared to\nthe second-best baseline.", "AI": {"tldr": "\u63d0\u51fa\u4e86Adapt-STformer\u65b9\u6cd5\uff0c\u4f7f\u7528\u5faa\u73af\u53ef\u53d8\u5f62Transformer\u7f16\u7801\u5668\u5b9e\u73b0\u7075\u6d3b\u9ad8\u6548\u7684\u5e8f\u5217\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u7075\u6d3b\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684Seq-VPR\u65b9\u6cd5\u5728\u8ffd\u6c42\u6027\u80fd\u65f6\u727a\u7272\u4e86\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5e8f\u5217\u957f\u5ea6\u53ef\u53d8\u3001\u5feb\u901f\u63a8\u7406\u548c\u4f4e\u5185\u5b58\u5360\u7528\u7684\u9700\u6c42", "method": "\u63d0\u51faAdapt-STformer\u65b9\u6cd5\uff0c\u6838\u5fc3\u662f\u5faa\u73af\u53ef\u53d8\u5f62Transformer\u7f16\u7801\u5668(Recurrent-DTE)\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u673a\u5236\u878d\u5408\u591a\u5e27\u5e8f\u5217\u4fe1\u606f\uff0c\u652f\u6301\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6", "result": "\u5728Nordland\u3001Oxford\u548cNuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u53ec\u56de\u7387\u63d0\u5347\u9ad8\u8fbe17%\uff0c\u5e8f\u5217\u63d0\u53d6\u65f6\u95f4\u51cf\u5c1136%\uff0c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e35%", "conclusion": "Adapt-STformer\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u6027\u3001\u5feb\u901f\u63a8\u7406\u548c\u4f4e\u5185\u5b58\u5360\u7528\u7684\u5e73\u8861\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2510.05069", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05069", "abs": "https://arxiv.org/abs/2510.05069", "authors": ["Dachuan Shi", "Abedelkadir Asi", "Keying Li", "Xiangchi Yuan", "Leyan Pan", "Wenke Lee", "Wen Xiao"], "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs", "comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/", "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.", "AI": {"tldr": "SwiReasoning\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5207\u6362\u663e\u5f0f\u548c\u6f5c\u5728\u63a8\u7406\u6765\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u4ee4\u724c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6f5c\u5728\u63a8\u7406\u9762\u4e34\u7684\u4e24\u4e2a\u6311\u6218\uff1a1) \u7eaf\u7cb9\u6f5c\u5728\u63a8\u7406\u4f1a\u6269\u6563\u6982\u7387\u8d28\u91cf\uff0c\u5f15\u5165\u566a\u58f0\uff0c\u963b\u788d\u6536\u655b\u5230\u9ad8\u7f6e\u4fe1\u5ea6\u89e3\uff1b2) \u8fc7\u5ea6\u601d\u8003\u95ee\u9898\u6301\u7eed\u5b58\u5728\uff0c\u6d6a\u8d39\u4ee4\u724c\u5e76\u964d\u4f4e\u6548\u7387\u3002", "method": "SwiReasoning\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u57fa\u4e8e\u4e0b\u4e00\u4e2a\u4ee4\u724c\u5206\u5e03\u7684\u71b5\u8d8b\u52bf\u4f30\u8ba1\u5757\u7ea7\u7f6e\u4fe1\u5ea6\uff0c\u52a8\u6001\u5207\u6362\u663e\u5f0f\u548c\u6f5c\u5728\u63a8\u7406\uff1b2) \u9650\u5236\u601d\u7ef4\u5757\u5207\u6362\u7684\u6700\u5927\u6b21\u6570\u6765\u6291\u5236\u8fc7\u5ea6\u601d\u8003\u3002", "result": "\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u5b66\u548cSTEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSwiReasoning\u5c06\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u548c\u89c4\u6a21\u7684\u63a8\u7406LLM\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.5%-2.8%\u3002\u5728\u53d7\u9650\u9884\u7b97\u4e0b\uff0c\u5e73\u5747\u4ee4\u724c\u6548\u7387\u63d0\u9ad8\u4e8656%-79%\uff0c\u9884\u7b97\u8d8a\u7d27\u589e\u76ca\u8d8a\u5927\u3002", "conclusion": "SwiReasoning\u901a\u8fc7\u52a8\u6001\u63a8\u7406\u5207\u6362\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u6f5c\u5728\u63a8\u7406\u7684\u6536\u655b\u95ee\u9898\u548c\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u4ee4\u724c\u6548\u7387\u3002"}}
{"id": "2510.04290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04290", "abs": "https://arxiv.org/abs/2510.04290", "authors": ["Jay Zhangjie Wu", "Xuanchi Ren", "Tianchang Shen", "Tianshi Cao", "Kai He", "Yifan Lu", "Ruiyuan Gao", "Enze Xie", "Shiyi Lan", "Jose M. Alvarez", "Jun Gao", "Sanja Fidler", "Zian Wang", "Huan Ling"], "title": "ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/chronoedit", "summary": "Recent advances in large generative models have significantly advanced image\nediting and in-context image generation, yet a critical gap remains in ensuring\nphysical consistency, where edited objects must remain coherent. This\ncapability is especially vital for world simulation related tasks. In this\npaper, we present ChronoEdit, a framework that reframes image editing as a\nvideo generation problem. First, ChronoEdit treats the input and edited images\nas the first and last frames of a video, allowing it to leverage large\npretrained video generative models that capture not only object appearance but\nalso the implicit physics of motion and interaction through learned temporal\nconsistency. Second, ChronoEdit introduces a temporal reasoning stage that\nexplicitly performs editing at inference time. Under this setting, the target\nframe is jointly denoised with reasoning tokens to imagine a plausible editing\ntrajectory that constrains the solution space to physically viable\ntransformations. The reasoning tokens are then dropped after a few steps to\navoid the high computational cost of rendering a full video. To validate\nChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for\ncontexts that require physical consistency, and demonstrate that ChronoEdit\nsurpasses state-of-the-art baselines in both visual fidelity and physical\nplausibility. Code and models for both the 14B and 2B variants of ChronoEdit\nwill be released on the project page:\nhttps://research.nvidia.com/labs/toronto-ai/chronoedit", "AI": {"tldr": "ChronoEdit\u5c06\u56fe\u50cf\u7f16\u8f91\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u751f\u6210\u95ee\u9898\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u6765\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u65f6\u5e8f\u63a8\u7406\u9636\u6bb5\u751f\u6210\u5408\u7406\u7684\u7f16\u8f91\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u56fe\u50cf\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u786e\u4fdd\u7269\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5173\u952e\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u4e8e\u4e16\u754c\u6a21\u62df\u76f8\u5173\u4efb\u52a1\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u5c06\u8f93\u5165\u548c\u7f16\u8f91\u56fe\u50cf\u89c6\u4e3a\u89c6\u9891\u7684\u9996\u5c3e\u5e27\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\uff1b\u5f15\u5165\u65f6\u5e8f\u63a8\u7406\u9636\u6bb5\uff0c\u5728\u63a8\u7406\u65f6\u8054\u5408\u53bb\u566a\u76ee\u6807\u5e27\u548c\u63a8\u7406\u4ee4\u724c\uff0c\u60f3\u8c61\u5408\u7406\u7684\u7f16\u8f91\u8f68\u8ff9\u3002", "result": "\u5728PBench-Edit\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cChronoEdit\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5747\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ChronoEdit\u901a\u8fc7\u5c06\u56fe\u50cf\u7f16\u8f91\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89c6\u9891\u751f\u6210\u95ee\u9898\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7269\u7406\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u9700\u8981\u7269\u7406\u4e00\u81f4\u6027\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05077", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05077", "abs": "https://arxiv.org/abs/2510.05077", "authors": ["Chenyu Wang", "Zishen Wan", "Hao Kang", "Emma Chen", "Zhiqiang Xie", "Tushar Krishna", "Vijay Janapa Reddi", "Yilun Du"], "title": "Slm-mux: Orchestrating small language models for reasoning", "comment": null, "summary": "With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSLM-MUX\u7684\u4e09\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u6709\u6548\u534f\u8c03\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\uff0c\u901a\u8fc7\u6a21\u578b\u9009\u62e9\u641c\u7d22\u548c\u6d4b\u8bd5\u65f6\u7f29\u653e\u4f18\u5316\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7f16\u6392\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6570\u91cf\u7684\u5feb\u901f\u589e\u957f\uff0c\u867d\u7136\u5b83\u4eec\u65e0\u6cd5\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4e14\u66f4\u9ad8\u6548\u3002\u73b0\u6709\u7f16\u6392\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u524d\u6cbf\u6a21\u578b\uff0c\u5728SLMs\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9SLMs\u7684\u7f16\u6392\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) SLM-MUX\u591a\u6a21\u578b\u67b6\u6784\u534f\u8c03\u591a\u4e2aSLMs\uff1b2) \u6a21\u578b\u9009\u62e9\u641c\u7d22\u4ece\u5019\u9009\u6c60\u4e2d\u8bc6\u522b\u6700\u5177\u4e92\u8865\u6027\u7684SLMs\uff1b3) \u9488\u5bf9SLM-MUX\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728MATH\u4e0a\u63d0\u534713.4%\uff0cGPQA\u4e0a\u63d0\u53478.8%\uff0cGSM8K\u4e0a\u63d0\u53477.0%\u3002\u4ec5\u4f7f\u7528\u4e24\u4e2aSLMs\uff0cSLM-MUX\u5728GPQA\u548cGSM8K\u4e0a\u8d85\u8d8aQwen 2.5 72B\uff0c\u5728MATH\u4e0a\u4e0e\u4e4b\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u88ab\u6709\u6548\u7f16\u6392\u6210\u66f4\u51c6\u786e\u548c\u9ad8\u6548\u7684\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86SLMs\u7f16\u6392\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2510.04312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04312", "abs": "https://arxiv.org/abs/2510.04312", "authors": ["Vida Adeli", "Ivan Klabucar", "Javad Rajabi", "Benjamin Filtjens", "Soroush Mehraban", "Diwei Wang", "Hyewon Seo", "Trung-Hieu Hoang", "Minh N. Do", "Candice Muller", "Claudia Oliveira", "Daniel Boari Coelho", "Pieter Ginis", "Moran Gilat", "Alice Nieuwboer", "Joke Spildooren", "Lucas Mckay", "Hyeokhyen Kwon", "Gari Clifford", "Christine Esper", "Stewart Factor", "Imari Genias", "Amirhossein Dadashzadeh", "Leia Shum", "Alan Whone", "Majid Mirmehdi", "Andrea Iaboni", "Babak Taati"], "title": "CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment", "comment": "Accepted at the Thirty-Ninth Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Objective gait assessment in Parkinson's Disease (PD) is limited by the\nabsence of large, diverse, and clinically annotated motion datasets. We\nintroduce CARE-PD, the largest publicly available archive of 3D mesh gait data\nfor PD, and the first multi-site collection spanning 9 cohorts from 8 clinical\ncenters. All recordings (RGB video or motion capture) are converted into\nanonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD\nsupports two key benchmarks: supervised clinical score prediction (estimating\nUnified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised\nmotion pretext tasks (2D-to-3D keypoint lifting and full-body 3D\nreconstruction). Clinical prediction is evaluated under four generalization\nprotocols: within-dataset, cross-dataset, leave-one-dataset-out, and\nmulti-dataset in-domain adaptation. To assess clinical relevance, we compare\nstate-of-the-art motion encoders with a traditional gait-feature baseline,\nfinding that encoders consistently outperform handcrafted features. Pretraining\non CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1\nby 17 percentage points, underscoring the value of clinically curated, diverse\ntraining data. CARE-PD and all benchmark code are released for non-commercial\nresearch at https://neurips2025.care-pd.ca/.", "AI": {"tldr": "CARE-PD\u662f\u6700\u5927\u7684\u516c\u5f00\u5e15\u91d1\u68ee\u75c53D\u7f51\u683c\u6b65\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b9\u4e2a\u961f\u5217\u7684\u533f\u540dSMPL\u7f51\u683c\u6570\u636e\uff0c\u652f\u6301\u4e34\u5e8a\u8bc4\u5206\u9884\u6d4b\u548c\u65e0\u76d1\u7763\u8fd0\u52a8\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u7684\u5ba2\u89c2\u6b65\u6001\u8bc4\u4f30\u53d7\u9650\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u4e14\u4e34\u5e8a\u6807\u6ce8\u7684\u8fd0\u52a8\u6570\u636e\u96c6\u3002", "method": "\u5c06RGB\u89c6\u9891\u6216\u8fd0\u52a8\u6355\u6349\u6570\u636e\u901a\u8fc7\u7edf\u4e00\u9884\u5904\u7406\u6d41\u7a0b\u8f6c\u6362\u4e3a\u533f\u540dSMPL\u7f51\u683c\uff0c\u652f\u6301\u76d1\u7763\u4e34\u5e8a\u8bc4\u5206\u9884\u6d4b\u548c\u65e0\u76d1\u7763\u8fd0\u52a8\u9884\u8bad\u7ec3\u4efb\u52a1\u3002", "result": "\u8fd0\u52a8\u7f16\u7801\u5668\u59cb\u7ec8\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\uff0c\u5728CARE-PD\u4e0a\u9884\u8bad\u7ec3\u53ef\u5c06MPJPE\u4ece60.8mm\u964d\u81f37.5mm\uff0c\u5e76\u5c06PD\u4e25\u91cd\u7a0b\u5ea6\u5b8fF1\u63d0\u9ad817\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CARE-PD\u8bc1\u660e\u4e86\u4e34\u5e8a\u7b56\u5212\u7684\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u7684\u4ef7\u503c\uff0c\u6240\u6709\u6570\u636e\u548c\u57fa\u51c6\u4ee3\u7801\u5df2\u516c\u5f00\u4f9b\u975e\u5546\u4e1a\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2510.05087", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05087", "abs": "https://arxiv.org/abs/2510.05087", "authors": ["Janos Perczel", "Jin Chow", "Dorottya Demszky"], "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data", "comment": "28 pages, 9 figures", "summary": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.", "AI": {"tldr": "TeachLM\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4f18\u5316LLM\u7528\u4e8e\u6559\u5b66\uff0c\u4f7f\u7528\u771f\u5b9e\u5b66\u751f-\u5bfc\u5e08\u5bf9\u8bdd\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u5bf9\u8bdd\uff0c\u663e\u8457\u63d0\u5347\u6559\u5b66\u5bf9\u8bdd\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u53d7\u9650\u4e8eLLM\u6559\u5b66\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u771f\u5b9e\u5b66\u751f\u5b66\u4e60\u6570\u636e\uff0c\u4ee5\u53ca\u63d0\u793a\u5de5\u7a0b\u5728\u7f16\u7801\u590d\u6742\u6559\u5b66\u7b56\u7565\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u752810\u4e07\u5c0f\u65f6\u771f\u5b9e\u5b66\u751f-\u5bfc\u5e08\u5bf9\u8bdd\u6570\u636e\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5f00\u53d1\u771f\u5b9e\u5b66\u751f\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u5408\u6210\u5b66\u751f-\u5bfc\u5e08\u5bf9\u8bdd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u5408\u6210\u5bf9\u8bdd\u7684\u591a\u8f6e\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5fae\u8c03\u663e\u8457\u6539\u5584\u5bf9\u8bdd\u548c\u6559\u5b66\u8868\u73b0\uff1a\u5b66\u751f\u53d1\u8a00\u65f6\u95f4\u7ffb\u500d\u3001\u63d0\u95ee\u98ce\u683c\u6539\u5584\u3001\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a050%\u3001\u6559\u5b66\u4e2a\u6027\u5316\u7a0b\u5ea6\u63d0\u9ad8\u3002", "conclusion": "\u57fa\u4e8e\u771f\u5b9e\u5b66\u4e60\u6570\u636e\u7684\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u6559\u5b66\u5bf9\u8bdd\u80fd\u529b\uff0c\u4e3a\u6559\u80b2AI\u53d1\u5c55\u63d0\u4f9b\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.04315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04315", "abs": "https://arxiv.org/abs/2510.04315", "authors": ["Jiarui Ouyang", "Yihui Wang", "Yihang Gao", "Yingxue Xu", "Shu Yang", "Hao Chen"], "title": "GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction", "comment": null, "summary": "Spatial Transcriptomics (ST) offers spatially resolved gene expression but\nremains costly. Predicting expression directly from widely available\nHematoxylin and Eosin (H&E) stained images presents a cost-effective\nalternative. However, most computational approaches (i) predict each gene\nindependently, overlooking co-expression structure, and (ii) cast the task as\ncontinuous regression despite expression being discrete counts. This mismatch\ncan yield biologically implausible outputs and complicate downstream analyses.\nWe introduce GenAR, a multi-scale autoregressive framework that refines\npredictions from coarse to fine. GenAR clusters genes into hierarchical groups\nto expose cross-gene dependencies, models expression as codebook-free discrete\ntoken generation to directly predict raw counts, and conditions decoding on\nfused histological and spatial embeddings. From an information-theoretic\nperspective, the discrete formulation avoids log-induced biases and the\ncoarse-to-fine factorization aligns with a principled conditional\ndecomposition. Extensive experimental results on four Spatial Transcriptomics\ndatasets across different tissue types demonstrate that GenAR achieves\nstate-of-the-art performance, offering potential implications for precision\nmedicine and cost-effective molecular profiling. Code is publicly available at\nhttps://github.com/oyjr/genar.", "AI": {"tldr": "GenAR\u662f\u4e00\u4e2a\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u65b9\u5f0f\u9884\u6d4b\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u5c06\u57fa\u56e0\u805a\u7c7b\u4e3a\u5c42\u6b21\u7ec4\u4ee5\u6355\u6349\u57fa\u56e0\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u76f4\u63a5\u9884\u6d4b\u539f\u59cb\u8ba1\u6570\uff0c\u5e76\u5728\u89e3\u7801\u65f6\u878d\u5408\u7ec4\u7ec7\u5b66\u548c\u7a7a\u95f4\u5d4c\u5165\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6280\u672f\u6210\u672c\u9ad8\u6602\uff0c\u800c\u4ece\u5e7f\u6cdb\u53ef\u7528\u7684H&E\u67d3\u8272\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u662f\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u9884\u6d4b\u6bcf\u4e2a\u57fa\u56e0\uff0c\u5ffd\u7565\u4e86\u5171\u8868\u8fbe\u7ed3\u6784\uff0c\u5e76\u5c06\u4efb\u52a1\u89c6\u4e3a\u8fde\u7eed\u56de\u5f52\uff0c\u800c\u8868\u8fbe\u5b9e\u9645\u4e0a\u662f\u79bb\u6563\u8ba1\u6570\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u751f\u7269\u5b66\u4e0a\u4e0d\u5408\u7406\u7684\u8f93\u51fa\u5e76\u590d\u6742\u5316\u4e0b\u6e38\u5206\u6790\u3002", "method": "GenAR\u91c7\u7528\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5c06\u57fa\u56e0\u805a\u7c7b\u4e3a\u5c42\u6b21\u7ec4\u4ee5\u66b4\u9732\u8de8\u57fa\u56e0\u4f9d\u8d56\u5173\u7cfb\uff0c\u5c06\u8868\u8fbe\u5efa\u6a21\u4e3a\u65e0\u7801\u672c\u7684\u79bb\u6563\u6807\u8bb0\u751f\u6210\u6765\u76f4\u63a5\u9884\u6d4b\u539f\u59cb\u8ba1\u6570\uff0c\u5e76\u5728\u89e3\u7801\u65f6\u6761\u4ef6\u5316\u878d\u5408\u7684\u7ec4\u7ec7\u5b66\u548c\u7a7a\u95f4\u5d4c\u5165\u3002", "result": "\u5728\u56db\u4e2a\u4e0d\u540c\u7ec4\u7ec7\u7c7b\u578b\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGenAR\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GenAR\u901a\u8fc7\u79bb\u6563\u516c\u5f0f\u907f\u514d\u4e86\u5bf9\u6570\u8bf1\u5bfc\u7684\u504f\u5dee\uff0c\u7c97\u5230\u7ec6\u7684\u5206\u89e3\u4e0e\u539f\u5219\u6027\u7684\u6761\u4ef6\u5206\u89e3\u76f8\u4e00\u81f4\uff0c\u4e3a\u7cbe\u51c6\u533b\u5b66\u548c\u7ecf\u6d4e\u6709\u6548\u7684\u5206\u5b50\u5206\u6790\u63d0\u4f9b\u4e86\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.05090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05090", "abs": "https://arxiv.org/abs/2510.05090", "authors": ["Runchu Tian", "Junxia Cui", "Xueqiang Xu", "Feng Yao", "Jingbo Shang"], "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models", "comment": "17 pages, 8 figures. Work in progress", "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.", "AI": {"tldr": "Tolerator\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff08\u5e8f\u5217\u586b\u5145\u548c\u8fed\u4ee3\u7cbe\u70bc\uff09\u5b9e\u73b0\u4ee4\u724c\u7ea7\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6269\u6563\u89e3\u7801\u4e2d\u4ee4\u724c\u4e00\u65e6\u63a5\u53d7\u5c31\u65e0\u6cd5\u4fee\u6539\u7684\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5e76\u884c\u89e3\u7801\u548c\u53cc\u5411\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u4f18\u52bf\uff0c\u4f46\u5176\u4f20\u7edf\u89e3\u7801\u7b56\u7565\u5b58\u5728\u5173\u952e\u9650\u5236\uff1a\u4e00\u65e6\u4ee4\u724c\u88ab\u63a5\u53d7\uff0c\u540e\u7eed\u6b65\u9aa4\u4e2d\u65e0\u6cd5\u518d\u4fee\u6539\uff0c\u5bfc\u81f4\u65e9\u671f\u9519\u8bef\u6301\u7eed\u5f71\u54cd\u6700\u7ec8\u8f93\u51fa\u8d28\u91cf\u3002", "method": "\u63d0\u51faTolerator\u89e3\u7801\u7b56\u7565\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a(i)\u5e8f\u5217\u586b\u5145\u9636\u6bb5\uff1b(ii)\u8fed\u4ee3\u7cbe\u70bc\u9636\u6bb5\uff0c\u901a\u8fc7\u91cd\u65b0\u63a9\u7801\u548c\u89e3\u7801\u4ee4\u724c\u5b50\u96c6\uff0c\u540c\u65f6\u5c06\u5269\u4f59\u4ee4\u724c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u4f7f\u5df2\u63a5\u53d7\u7684\u4ee4\u724c\u80fd\u591f\u88ab\u91cd\u65b0\u8003\u8651\u548c\u4fee\u6b63\u3002", "result": "\u5728\u6db5\u76d6\u8bed\u8a00\u7406\u89e3\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u7684\u4e94\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTolerator\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u89e3\u7801\u7b97\u6cd5\u5bf9\u4e8e\u5145\u5206\u53d1\u6325\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u81f3\u5173\u91cd\u8981\uff0cTolerator\u901a\u8fc7\u4ee4\u724c\u7ea7\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u89e3\u7801\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.04333", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04333", "abs": "https://arxiv.org/abs/2510.04333", "authors": ["Lan Feng", "Yang Gao", "Eloi Zablocki", "Quanyi Li", "Wuyang Li", "Sichao Liu", "Matthieu Cord", "Alexandre Alahi"], "title": "RAP: 3D Rasterization Augmented End-to-End Planning", "comment": null, "summary": "Imitation learning for end-to-end driving trains policies only on expert\ndemonstrations. Once deployed in a closed loop, such policies lack recovery\ndata: small mistakes cannot be corrected and quickly compound into failures. A\npromising direction is to generate alternative viewpoints and trajectories\nbeyond the logged path. Prior work explores photorealistic digital twins via\nneural rendering or game engines, but these methods are prohibitively slow and\ncostly, and thus mainly used for evaluation. In this work, we argue that\nphotorealism is unnecessary for training end-to-end planners. What matters is\nsemantic fidelity and scalability: driving depends on geometry and dynamics,\nnot textures or lighting. Motivated by this, we propose 3D Rasterization, which\nreplaces costly rendering with lightweight rasterization of annotated\nprimitives, enabling augmentations such as counterfactual recovery maneuvers\nand cross-agent view synthesis. To transfer these synthetic views effectively\nto real-world deployment, we introduce a Raster-to-Real feature-space alignment\nthat bridges the sim-to-real gap. Together, these components form Rasterization\nAugmented Planning (RAP), a scalable data augmentation pipeline for planning.\nRAP achieves state-of-the-art closed-loop robustness and long-tail\ngeneralization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo\nOpen Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that\nlightweight rasterization with feature alignment suffices to scale E2E\ntraining, offering a practical alternative to photorealistic rendering. Project\npage: https://alan-lanfeng.github.io/RAP/.", "AI": {"tldr": "\u63d0\u51faRAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea73D\u6805\u683c\u5316\u548c\u7279\u5f81\u5bf9\u9f50\u6280\u672f\uff0c\u4e3a\u7aef\u5230\u7aef\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\uff0c\u65e0\u9700\u6602\u8d35\u7684\u7167\u7247\u7ea7\u6e32\u67d3\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u9a7e\u9a76\u7b56\u7565\u7f3a\u4e4f\u6062\u590d\u6570\u636e\uff0c\u5c0f\u9519\u8bef\u4f1a\u5feb\u901f\u7d2f\u79ef\u5bfc\u81f4\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u795e\u7ecf\u6e32\u67d3\u6216\u6e38\u620f\u5f15\u64ce\u521b\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u4f46\u6210\u672c\u9ad8\u3001\u901f\u5ea6\u6162\uff0c\u4e3b\u8981\u7528\u4e8e\u8bc4\u4f30\u800c\u975e\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa3D\u6805\u683c\u5316\u65b9\u6cd5\uff0c\u7528\u8f7b\u91cf\u7ea7\u6805\u683c\u5316\u66ff\u4ee3\u6602\u8d35\u6e32\u67d3\uff0c\u652f\u6301\u53cd\u4e8b\u5b9e\u6062\u590d\u64cd\u4f5c\u548c\u8de8\u667a\u80fd\u4f53\u89c6\u56fe\u5408\u6210\u3002\u5f15\u5165\u6805\u683c\u5230\u771f\u5b9e\u7279\u5f81\u7a7a\u95f4\u5bf9\u9f50\u6765\u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "result": "RAP\u5728NAVSIM v1/v2\u3001Waymo\u5f00\u653e\u6570\u636e\u96c6\u89c6\u89c9\u7aef\u5230\u7aef\u9a7e\u9a76\u548cBench2Drive\u56db\u4e2a\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u95ed\u73af\u9c81\u68d2\u6027\u548c\u957f\u5c3e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u6805\u683c\u5316\u7ed3\u5408\u7279\u5f81\u5bf9\u9f50\u8db3\u4ee5\u6269\u5c55\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u4e3a\u7167\u7247\u7ea7\u6e32\u67d3\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2412.18708", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "68T01 (Primary)", "I.2.0; I.2.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2412.18708", "abs": "https://arxiv.org/abs/2412.18708", "authors": ["Vivek Vellaiyappan Surulimuthu", "Aditya Karnam Gururaj Rao"], "title": "CAG: Chunked Augmented Generation for Google Chrome's Built-in Gemini Nano", "comment": "36 pages, 19 figures", "summary": "We present Chunked Augmented Generation (CAG), an architecture specifically\ndesigned to overcome the context window limitations of Google Chrome's built-in\nGemini Nano model. While Chrome's integration of Gemini Nano represents a\nsignificant advancement in bringing AI capabilities directly to the browser,\nits restricted context window poses challenges for processing large inputs. CAG\naddresses this limitation through intelligent input chunking and processing\nstrategies, enabling efficient handling of extensive content while maintaining\nthe model's performance within browser constraints. Our implementation\ndemonstrates particular efficacy in processing large documents and datasets\ndirectly within Chrome, making sophisticated AI capabilities accessible through\nthe browser without external API dependencies. Get started now at\nhttps://github.com/vivekVells/cag-js.", "AI": {"tldr": "\u63d0\u51faChunked Augmented Generation (CAG)\u67b6\u6784\uff0c\u89e3\u51b3Chrome\u5185\u7f6eGemini Nano\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u5757\u5904\u7406\u7b56\u7565\u5b9e\u73b0\u5927\u5185\u5bb9\u7684\u9ad8\u6548\u5904\u7406\u3002", "motivation": "Chrome\u96c6\u6210Gemini Nano\u867d\u7136\u5c06AI\u80fd\u529b\u5e26\u5230\u6d4f\u89c8\u5668\uff0c\u4f46\u5176\u53d7\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u8f93\u5165\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u667a\u80fd\u8f93\u5165\u5206\u5757\u548c\u5904\u7406\u7b56\u7565\uff0c\u5728\u6d4f\u89c8\u5668\u7ea6\u675f\u4e0b\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u9ad8\u6548\u5904\u7406\u6269\u5c55\u5185\u5bb9\u3002", "result": "\u5728Chrome\u4e2d\u5904\u7406\u5927\u578b\u6587\u6863\u548c\u6570\u636e\u96c6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u590d\u6742AI\u80fd\u529b\u65e0\u9700\u5916\u90e8API\u4f9d\u8d56\u5373\u53ef\u5728\u6d4f\u89c8\u5668\u4e2d\u4f7f\u7528\u3002", "conclusion": "CAG\u67b6\u6784\u6210\u529f\u514b\u670d\u4e86\u6d4f\u89c8\u5668AI\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u9650\u5236\uff0c\u4e3a\u76f4\u63a5\u5728\u6d4f\u89c8\u5668\u4e2d\u5904\u7406\u5927\u89c4\u6a21\u5185\u5bb9\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.04365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04365", "abs": "https://arxiv.org/abs/2510.04365", "authors": ["Yuhao Luo", "Yuang Zhang", "Kehua Chen", "Xinyu Zheng", "Shucheng Zhang", "Sikai Chen", "Yinhai Wang"], "title": "Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction", "comment": "13 pages, 7 figures, 3 tables", "summary": "Accurate pedestrian trajectory prediction is crucial for ensuring safety and\nefficiency in autonomous driving and human-robot interaction scenarios. Earlier\nstudies primarily utilized sufficient observational data to predict future\ntrajectories. However, in real-world scenarios, such as pedestrians suddenly\nemerging from blind spots, sufficient observational data is often unavailable\n(i.e. momentary trajectory), making accurate prediction challenging and\nincreasing the risk of traffic accidents. Therefore, advancing research on\npedestrian trajectory prediction under extreme scenarios is critical for\nenhancing traffic safety. In this work, we propose a novel framework termed\nDiffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists\nof two sequentially connected diffusion models: one for backward prediction,\nwhich generates unobserved historical trajectories, and the other for forward\nprediction, which forecasts future trajectories. Given that the generated\nunobserved historical trajectories may introduce additional noise, we propose a\ndual-head parameterization mechanism to estimate their aleatoric uncertainty\nand design a temporally adaptive noise module that dynamically modulates the\nnoise scale in the forward diffusion process. Empirically, Diffusion^2 sets a\nnew state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford\nDrone datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86Diffusion^2\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77ac\u65f6\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u4e2a\u8fde\u63a5\u7684\u6269\u6563\u6a21\u578b\u5206\u522b\u8fdb\u884c\u5386\u53f2\u8f68\u8ff9\u751f\u6210\u548c\u672a\u6765\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff08\u5982\u76f2\u533a\u7a81\u7136\u51fa\u73b0\u7684\u884c\u4eba\uff09\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u5bfc\u81f4\u51c6\u786e\u9884\u6d4b\u56f0\u96be\uff0c\u589e\u52a0\u4e86\u4ea4\u901a\u4e8b\u6545\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u5e8f\u5217\u8fde\u63a5\u7684\u6269\u6563\u6a21\u578b\uff1a\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u672a\u89c2\u6d4b\u7684\u5386\u53f2\u8f68\u8ff9\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u3002\u8bbe\u8ba1\u4e86\u53cc\u5934\u53c2\u6570\u5316\u673a\u5236\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u53ca\u65f6\u95f4\u81ea\u9002\u5e94\u566a\u58f0\u6a21\u5757\u52a8\u6001\u8c03\u8282\u524d\u5411\u6269\u6563\u8fc7\u7a0b\u7684\u566a\u58f0\u5c3a\u5ea6\u3002", "result": "\u5728ETH/UCY\u548cStanford Drone\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "Diffusion^2\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u77ac\u65f6\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u5b89\u5168\u6027\u3002"}}
{"id": "2510.04390", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04390", "abs": "https://arxiv.org/abs/2510.04390", "authors": ["Xuehai He", "Shijie Zhou", "Thivyanth Venkateswaran", "Kaizhi Zheng", "Ziyu Wan", "Achuta Kadambi", "Xin Eric Wang"], "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator", "comment": null, "summary": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.", "AI": {"tldr": "MorphoSim\u662f\u4e00\u4e2a\u8bed\u8a00\u5f15\u5bfc\u76844D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u521b\u5efa\u591a\u89c6\u89d2\u4e00\u81f4\u4e14\u652f\u6301\u5bf9\u8c61\u7ea7\u63a7\u5236\u7684\u52a8\u6001\u73af\u5883\uff0c\u901a\u8fc7\u8f68\u8ff9\u5f15\u5bfc\u751f\u6210\u548c\u7279\u5f81\u573a\u84b8\u998f\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5c40\u9650\u4e8e2D\u89c6\u56fe\u4e14\u4ea4\u4e92\u6027\u6709\u9650\uff0c\u9700\u8981\u80fd\u591f\u751f\u6210\u53ef\u63a7\u53ef\u7f16\u8f91\u7684\u65f6\u7a7a\u73af\u5883\u6765\u652f\u6301\u673a\u5668\u4eba\u5e94\u7528\uff0c\u5982\u53ef\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u3001\u53ef\u91cd\u590d\u8bc4\u4f30\u548c\u7075\u6d3b\u4efb\u52a1\u8bbe\u8ba1\u3002", "method": "\u96c6\u6210\u8f68\u8ff9\u5f15\u5bfc\u751f\u6210\u4e0e\u7279\u5f81\u573a\u84b8\u998f\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u751f\u62104D\u573a\u666f\uff0c\u652f\u6301\u5bf9\u8c61\u63a7\u5236\u3001\u91cd\u65b0\u7740\u8272\u3001\u79fb\u9664\u7b49\u64cd\u4f5c\uff0c\u5e76\u5141\u8bb8\u4ece\u4efb\u610f\u89c6\u89d2\u89c2\u5bdf\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMorphoSim\u5728\u4fdd\u6301\u9ad8\u573a\u666f\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u63a7\u6027\u548c\u53ef\u7f16\u8f91\u6027\uff0c\u80fd\u591f\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u751f\u6210\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u4ea4\u4e92\u5f0f\u7f16\u8f91\u3002", "conclusion": "MorphoSim\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u63a7\u53ef\u7f16\u8f91\u76844D\u573a\u666f\u751f\u6210\u80fd\u529b\uff0c\u652f\u6301\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u5bf9\u8c61\u7ea7\u63a7\u5236\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.03399", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03399", "abs": "https://arxiv.org/abs/2510.03399", "authors": ["Xiaoyan Bai", "Aryan Shrivastava", "Ari Holtzman", "Chenhao Tan"], "title": "Know Thyself? On the Incapability and Implications of AI Self-Recognition", "comment": "Our code is available, see\n  https://github.com/ChicagoHAI/self-recognition", "summary": "Self-recognition is a crucial metacognitive capability for AI systems,\nrelevant not only for psychological analysis but also for safety, particularly\nin evaluative scenarios. Motivated by contradictory interpretations of whether\nmodels possess self-recognition (Panickssery et al., 2024; Davidson et al.,\n2024), we introduce a systematic evaluation framework that can be easily\napplied and updated. Specifically, we measure how well 10 contemporary larger\nlanguage models (LLMs) can identify their own generated text versus text from\nother models through two tasks: binary self-recognition and exact model\nprediction. Different from prior claims, our results reveal a consistent\nfailure in self-recognition. Only 4 out of 10 models predict themselves as\ngenerators, and the performance is rarely above random chance. Additionally,\nmodels exhibit a strong bias toward predicting GPT and Claude families. We also\nprovide the first evaluation of model awareness of their own and others'\nexistence, as well as the reasoning behind their choices in self-recognition.\nWe find that the model demonstrates some knowledge of its own existence and\nother models, but their reasoning reveals a hierarchical bias. They appear to\nassume that GPT, Claude, and occasionally Gemini are the top-tier models, often\nassociating high-quality text with them. We conclude by discussing the\nimplications of our findings on AI safety and future directions to develop\nappropriate AI self-awareness.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e8610\u4e2a\u5f53\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u65e0\u6cd5\u8bc6\u522b\u81ea\u5df1\u751f\u6210\u7684\u6587\u672c\uff0c\u6027\u80fd\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\u731c\u6d4b\uff0c\u4e14\u5b58\u5728\u5bf9GPT\u548cClaude\u5bb6\u65cf\u7684\u5f3a\u70c8\u504f\u89c1\u3002", "motivation": "\u9488\u5bf9AI\u7cfb\u7edf\u662f\u5426\u5177\u5907\u81ea\u6211\u8bc6\u522b\u80fd\u529b\u5b58\u5728\u4e89\u8bae\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5efa\u7acb\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u81ea\u8eab\u751f\u6210\u6587\u672c\u7684\u80fd\u529b\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u548c\u5fc3\u7406\u5206\u6790\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\uff1a\u4e8c\u5143\u81ea\u6211\u8bc6\u522b\uff08\u5224\u65ad\u6587\u672c\u662f\u5426\u4e3a\u81ea\u5df1\u751f\u6210\uff09\u548c\u7cbe\u786e\u6a21\u578b\u9884\u6d4b\uff08\u8bc6\u522b\u6587\u672c\u6765\u6e90\u6a21\u578b\uff09\uff0c\u5e76\u5206\u6790\u6a21\u578b\u5bf9\u81ea\u8eab\u548c\u5176\u4ed6\u6a21\u578b\u5b58\u5728\u7684\u8ba4\u77e5\u53ca\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "10\u4e2a\u6a21\u578b\u4e2d\u53ea\u67094\u4e2a\u80fd\u6b63\u786e\u8bc6\u522b\u81ea\u5df1\u751f\u6210\u7684\u6587\u672c\uff0c\u6574\u4f53\u6027\u80fd\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u6a21\u578b\u8868\u73b0\u51fa\u5bf9GPT\u548cClaude\u5bb6\u65cf\u7684\u5f3a\u70c8\u504f\u89c1\uff0c\u5e76\u663e\u793a\u51fa\u5bf9\u6a21\u578b\u5c42\u6b21\u7ed3\u6784\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "conclusion": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u5931\u8d25\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u6784\u6210\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u9002\u5f53\u7684AI\u81ea\u6211\u610f\u8bc6\uff0c\u5e76\u8003\u8651\u6a21\u578b\u504f\u89c1\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.04401", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04401", "abs": "https://arxiv.org/abs/2510.04401", "authors": ["Xuyang Guo", "Zekai Huang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting", "comment": null, "summary": "Vision-Language Models (VLMs) have become a central focus of today's AI\ncommunity, owing to their impressive abilities gained from training on\nlarge-scale vision-language data from the Web. These models have demonstrated\nstrong performance across diverse tasks, including image understanding, video\nunderstanding, complex visual reasoning, and embodied AI. Despite these\nnoteworthy successes, a fundamental question remains: Can VLMs count objects\ncorrectly? In this paper, we introduce a simple yet effective benchmark,\nVLMCountBench, designed under a minimalist setting with only basic geometric\nshapes (e.g., triangles, circles) and their compositions, focusing exclusively\non counting tasks without interference from other factors. We adopt strict\nindependent variable control and systematically study the effects of simple\nproperties such as color, size, and prompt refinement in a controlled ablation.\nOur empirical results reveal that while VLMs can count reliably when only one\nshape type is present, they exhibit substantial failures when multiple shape\ntypes are combined (i.e., compositional counting). This highlights a\nfundamental empirical limitation of current VLMs and motivates important\ndirections for future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86VLMCountBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5355\u4e00\u5f62\u72b6\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u7ec4\u5408\u5f62\u72b6\u8ba1\u6570\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5931\u8d25\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u8ba1\u6570\u80fd\u529b\u65b9\u9762\u7684\u57fa\u672c\u6027\u80fd\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u8bc4\u4f30\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76VLMs\u80fd\u5426\u6b63\u786e\u8ba1\u6570\u5bf9\u8c61\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7b80\u7ea6\u7684\u57fa\u51c6\u6d4b\u8bd5VLMCountBench\uff0c\u4ec5\u4f7f\u7528\u57fa\u672c\u51e0\u4f55\u5f62\u72b6\u53ca\u5176\u7ec4\u5408\uff0c\u5728\u4e25\u683c\u63a7\u5236\u53d8\u91cf\u7684\u6761\u4ef6\u4e0b\u7cfb\u7edf\u7814\u7a76\u989c\u8272\u3001\u5927\u5c0f\u548c\u63d0\u793a\u8bcd\u4f18\u5316\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVLMs\u5728\u5355\u4e00\u5f62\u72b6\u7c7b\u578b\u8ba1\u6570\u65f6\u8868\u73b0\u53ef\u9760\uff0c\u4f46\u5728\u7ec4\u5408\u5f62\u72b6\u7c7b\u578b\u8ba1\u6570\u65f6\u51fa\u73b0\u663e\u8457\u5931\u8d25\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u6a21\u578b\u7684\u7ec4\u5408\u8ba1\u6570\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u5408\u8ba1\u6570\u4efb\u52a1\u4e2d\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u8fd9\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2510.03696", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03696", "abs": "https://arxiv.org/abs/2510.03696", "authors": ["Deepak Babu Piskala", "Sharlene Chen", "Udita Patel", "Parul Kalra", "Rafael Castrillo"], "title": "Mind the Goal: Data-Efficient Goal-Oriented Evaluation of Conversational Agents and Chatbots using Teacher Models", "comment": null, "summary": "Evaluating the quality of multi-turn chatbot interactions remains\nchallenging, as most existing methods assess interactions at the turn level\nwithout addressing whether a user's overarching goal was fulfilled. A ``goal''\nhere refers to an information need or task, such as asking for policy\ninformation or applying for leave. We propose a comprehensive framework for\ngoal-oriented evaluation of multi-agent systems (MAS), introducing the\n\\textbf{Goal Success Rate (GSR)} to measure the percentage of fulfilled goals,\nand a \\textbf{Root Cause of Failure (RCOF)} taxonomy to identify reasons for\nfailure in multi-agent chatbots. Our method segments conversations by user\ngoals and evaluates success using all relevant turns. We present a model-based\nevaluation system combining teacher LLMs, where domain experts define goals,\nset quality standards serving as a guidance for the LLMs. The LLMs use\n``thinking tokens'' to produce interpretable rationales, enabling\n\\textit{explainable}, \\textit{data-efficient} evaluations. In an enterprise\nsetting, we apply our framework to evaluate AIDA, a zero-to-one employee\nconversational agent system built as a ground-up multi-agent conversational\nagent, and observe GSR improvement from 63\\% to 79\\% over six months since its\ninception. Our framework is generic and offers actionable insights through a\ndetailed defect taxonomy based on analysis of failure points in multi-agent\nchatbots, diagnosing overall success, identifying key failure modes, and\ninforming system improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u76ee\u6807\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u76ee\u6807\u6210\u529f\u7387(GSR)\u548c\u5931\u8d25\u6839\u56e0\u5206\u7c7b(RCOF)\uff0c\u901a\u8fc7\u6559\u5e08LLM\u6a21\u578b\u8fdb\u884c\u53ef\u89e3\u91ca\u3001\u6570\u636e\u9ad8\u6548\u7684\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8f6e\u804a\u5929\u673a\u5668\u4eba\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5728\u8f6e\u6b21\u5c42\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5224\u65ad\u7528\u6237\u603b\u4f53\u76ee\u6807\u662f\u5426\u8fbe\u6210\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u7528\u6237\u76ee\u6807\u5bf9\u5bf9\u8bdd\u8fdb\u884c\u5206\u6bb5\uff0c\u4f7f\u7528\u6559\u5e08LLM\u6a21\u578b\u7ed3\u5408\u9886\u57df\u4e13\u5bb6\u5b9a\u4e49\u7684\u76ee\u6807\u548c\u8d28\u91cf\u6807\u51c6\uff0c\u5229\u7528\"\u601d\u8003\u6807\u8bb0\"\u751f\u6210\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7406\u7531\u3002", "result": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5e94\u7528\u8be5\u6846\u67b6\u8bc4\u4f30AIDA\u5458\u5de5\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u89c2\u5bdf\u5230\u76ee\u6807\u6210\u529f\u7387\u57286\u4e2a\u6708\u5185\u4ece63%\u63d0\u5347\u523079%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u901a\u8fc7\u8be6\u7ec6\u7684\u7f3a\u9677\u5206\u7c7b\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u80fd\u591f\u8bca\u65ad\u6574\u4f53\u6210\u529f\u7387\u3001\u8bc6\u522b\u5173\u952e\u5931\u8d25\u6a21\u5f0f\u5e76\u6307\u5bfc\u7cfb\u7edf\u6539\u8fdb\u3002"}}
{"id": "2510.04410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04410", "abs": "https://arxiv.org/abs/2510.04410", "authors": ["Venkata Bharath Reddy Reddem", "Akshay P Sarashetti", "Ranjith Merugu", "Amit Satish Unde"], "title": "CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning", "comment": null, "summary": "Blind face restoration (BFR) has attracted increasing attention with the rise\nof generative methods. Most existing approaches integrate generative priors\ninto the restoration pro- cess, aiming to jointly address facial detail\ngeneration and identity preservation. However, these methods often suffer from\na trade-off between visual quality and identity fidelity, leading to either\nidentity distortion or suboptimal degradation removal. In this paper, we\npresent CodeFormer++, a novel framework that maximizes the utility of\ngenerative priors for high-quality face restoration while preserving identity.\nWe decompose BFR into three sub-tasks: (i) identity- preserving face\nrestoration, (ii) high-quality face generation, and (iii) dynamic fusion of\nidentity features with realistic texture details. Our method makes three key\ncontributions: (1) a learning-based deformable face registration module that\nsemantically aligns generated and restored faces; (2) a texture guided\nrestoration network to dynamically extract and transfer the texture of\ngenerated face to boost the quality of identity-preserving restored face; and\n(3) the integration of deep metric learning for BFR with the generation of\ninformative positive and hard negative samples to better fuse identity-\npreserving and generative features. Extensive experiments on real-world and\nsynthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves\nsuperior performance in terms of both visual fidelity and identity consistency.", "AI": {"tldr": "CodeFormer++\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u76f2\u8138\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u4e3a\u8eab\u4efd\u4fdd\u62a4\u4fee\u590d\u3001\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u52a8\u6001\u878d\u5408\u4e09\u4e2a\u5b50\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76f2\u8138\u4fee\u590d\u65b9\u6cd5\u5728\u96c6\u6210\u751f\u6210\u5148\u9a8c\u65f6\uff0c\u5f80\u5f80\u9762\u4e34\u89c6\u89c9\u8d28\u91cf\u4e0e\u8eab\u4efd\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5bfc\u81f4\u8eab\u4efd\u5931\u771f\u6216\u964d\u8d28\u53bb\u9664\u4e0d\u7406\u60f3\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a(1)\u57fa\u4e8e\u5b66\u4e60\u7684\u53ef\u53d8\u5f62\u4eba\u8138\u914d\u51c6\u6a21\u5757\u8fdb\u884c\u8bed\u4e49\u5bf9\u9f50\uff1b(2)\u7eb9\u7406\u5f15\u5bfc\u4fee\u590d\u7f51\u7edc\u52a8\u6001\u63d0\u53d6\u548c\u4f20\u9012\u751f\u6210\u4eba\u8138\u7684\u7eb9\u7406\uff1b(3)\u96c6\u6210\u6df1\u5ea6\u5ea6\u91cf\u5b66\u4e60\uff0c\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u6b63\u8d1f\u6837\u672c\u4ee5\u66f4\u597d\u5730\u878d\u5408\u8eab\u4efd\u4fdd\u62a4\u548c\u751f\u6210\u7279\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCodeFormer++\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CodeFormer++\u6846\u67b6\u6210\u529f\u6700\u5927\u5316\u751f\u6210\u5148\u9a8c\u7684\u6548\u7528\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4eba\u8138\u4fee\u590d\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u3002"}}
{"id": "2510.04428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04428", "abs": "https://arxiv.org/abs/2510.04428", "authors": ["Yuanhao Zou", "Shengji Jin", "Andong Deng", "Youpeng Zhao", "Jun Wang", "Chen Chen"], "title": "A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering", "comment": null, "summary": "Effectively applying Vision-Language Models (VLMs) to Video Question\nAnswering (VideoQA) hinges on selecting a concise yet comprehensive set of\nframes, as processing entire videos is computationally infeasible. However,\ncurrent frame selection methods face a critical trade-off: approaches relying\non lightweight similarity models, such as CLIP, often fail to capture the\nnuances of complex queries, resulting in inaccurate similarity scores that\ncannot reflect the authentic query-frame relevance, which further undermines\nframe selection. Meanwhile, methods that leverage a VLM for deeper analysis\nachieve higher accuracy but incur prohibitive computational costs. To address\nthese limitations, we propose A.I.R., a training-free approach for Adaptive,\nIterative, and Reasoning-based frame selection. We leverage a powerful VLM to\nperform deep, semantic analysis on complex queries, and this analysis is\ndeployed within a cost-effective iterative loop that processes only a small\nbatch of the most high-potential frames at a time. Extensive experiments on\nvarious VideoQA benchmarks demonstrate that our approach outperforms existing\nframe selection methods, significantly boosts the performance of the foundation\nVLM, and achieves substantial gains in computational efficiency over other\nVLM-based techniques.", "AI": {"tldr": "\u63d0\u51faA.I.R.\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u3001\u8fed\u4ee3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u5e27\u9009\u62e9\u7b56\u7565\uff0c\u5728\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u9891\u95ee\u7b54\u4e2d\u5e27\u9009\u62e9\u65b9\u6cd5\u7684\u4e24\u96be\u95ee\u9898\uff1a\u8f7b\u91cf\u7ea7\u76f8\u4f3c\u5ea6\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u800c\u4f7f\u7528VLM\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\u53c8\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5229\u7528\u5f3a\u5927\u7684VLM\u5bf9\u590d\u6742\u67e5\u8be2\u8fdb\u884c\u6df1\u5ea6\u8bed\u4e49\u5206\u6790\uff0c\u5728\u4f4e\u6210\u672c\u8fed\u4ee3\u5faa\u73af\u4e2d\u4ec5\u5904\u7406\u5c0f\u6279\u91cf\u7684\u9ad8\u6f5c\u529b\u5e27\u3002", "result": "\u5728\u591a\u4e2aVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5e27\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u57fa\u7840VLM\u6027\u80fd\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "A.I.R.\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5e27\u9009\u62e9\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03727", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5f25\u5408\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u548c\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u65f6\u7a7a\u7406\u89e3\u3001\u53ef\u63a7\u751f\u6210\u7b49\u591a\u65b9\u9762\u80fd\u529b\u3002", "motivation": "\u53d7\u4eba\u7c7b\u901a\u8fc7\u591a\u611f\u5b98\u6574\u5408\u7406\u89e3\u4e16\u754c\u7684\u542f\u53d1\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u4f5c\u4e3a\u6709\u6548\u4e16\u754c\u6a21\u578b\u7684\u5173\u952e\u80fd\u529b\uff0c\u5982\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u52a8\u6001\u6a21\u62df\u3001\u65f6\u7a7a\u4fe1\u606f\u7406\u89e3\u548c\u53ef\u63a7\u751f\u6210\u7b49\u3002", "method": "\u901a\u8fc7\u5224\u522b\u4efb\u52a1\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8d4b\u4e88\u7ed3\u6784\u5316\u63a8\u7406\u6280\u80fd\uff08\u56e0\u679c\u63a8\u65ad\u3001\u53cd\u4e8b\u5b9e\u601d\u7ef4\u3001\u65f6\u7a7a\u63a8\u7406\uff09\uff1b\u5f00\u53d1\u7ed3\u6784\u5316\u53ef\u63a7\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u573a\u666f\u56fe\u3001\u591a\u6a21\u6001\u6761\u4ef6\u7ea6\u675f\u548c\u5bf9\u9f50\u7b56\u7565\uff1b\u6269\u5c55\u5230\u53ef\u63a74D\u751f\u6210\uff0c\u5b9e\u73b0\u65f6\u7a7a\u4ea4\u4e92\u5f0f\u53ef\u7f16\u8f91\u5bf9\u8c61\u5408\u6210\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u8868\u9762\u76f8\u5173\u6027\uff0c\u7406\u89e3\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u4e2d\u7684\u6df1\u5c42\u5173\u7cfb\uff0c\u5e76\u5b9e\u73b0\u4e0e\u9ad8\u7ea7\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u7528\u6237\u610f\u56fe\u4e00\u81f4\u7684\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\uff0c\u53ef\u4ee5\u7f29\u5c0f\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u4e0e\u4e16\u754c\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u6784\u5efa\u66f4\u667a\u80fd\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.04450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04450", "abs": "https://arxiv.org/abs/2510.04450", "authors": ["Qiyuan He", "Yicong Li", "Haotian Ye", "Jinghao Wang", "Xinyao Liao", "Pheng-Ann Heng", "Stefano Ermon", "James Zou", "Angela Yao"], "title": "REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization", "comment": "27 pages, 23 figures, 5 tables", "summary": "Visual autoregressive (AR) generation offers a promising path toward unifying\nvision and language models, yet its performance remains suboptimal against\ndiffusion models. Prior work often attributes this gap to tokenizer limitations\nand rasterization ordering. In this work, we identify a core bottleneck from\nthe perspective of generator-tokenizer inconsistency, i.e., the AR-generated\ntokens may not be well-decoded by the tokenizer. To address this, we propose\nreAR, a simple training strategy introducing a token-wise regularization\nobjective: when predicting the next token, the causal transformer is also\ntrained to recover the visual embedding of the current token and predict the\nembedding of the target token under a noisy context. It requires no changes to\nthe tokenizer, generation order, inference pipeline, or external models.\nDespite its simplicity, reAR substantially improves performance. On ImageNet,\nit reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard\nrasterization-based tokenizer. When applied to advanced tokenizers, it achieves\na gFID of 1.42 with only 177M parameters, matching the performance with larger\nstate-of-the-art diffusion models (675M).", "AI": {"tldr": "\u63d0\u51fa\u4e86reAR\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165token-wise\u6b63\u5219\u5316\u76ee\u6807\u6765\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u751f\u6210\u5668-\u5206\u8bcd\u5668\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u6027\u80fd\u843d\u540e\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u6838\u5fc3\u74f6\u9888\u5728\u4e8e\u751f\u6210\u5668\u4e0e\u5206\u8bcd\u5668\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u5373AR\u751f\u6210\u7684token\u53ef\u80fd\u65e0\u6cd5\u88ab\u5206\u8bcd\u5668\u826f\u597d\u89e3\u7801\u3002", "method": "\u63d0\u51fareAR\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\u65f6\uff0c\u56e0\u679c\u53d8\u6362\u5668\u540c\u65f6\u5b66\u4e60\u6062\u590d\u5f53\u524dtoken\u7684\u89c6\u89c9\u5d4c\u5165\uff0c\u5e76\u5728\u566a\u58f0\u4e0a\u4e0b\u6587\u4e2d\u9884\u6d4b\u76ee\u6807token\u7684\u5d4c\u5165\u3002\u65e0\u9700\u6539\u53d8\u5206\u8bcd\u5668\u3001\u751f\u6210\u987a\u5e8f\u3001\u63a8\u7406\u6d41\u7a0b\u6216\u5916\u90e8\u6a21\u578b\u3002", "result": "\u5728ImageNet\u4e0a\uff0cgFID\u4ece3.02\u964d\u81f31.86\uff0cIS\u63d0\u5347\u81f3316.9\u3002\u5e94\u7528\u4e8e\u5148\u8fdb\u5206\u8bcd\u5668\u65f6\uff0c\u4ec5\u7528177M\u53c2\u6570\u5c31\u8fbe\u5230gFID 1.42\uff0c\u4e0e675M\u53c2\u6570\u7684\u6700\u5148\u8fdb\u6269\u6563\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "reAR\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u51b3\u751f\u6210\u5668-\u5206\u8bcd\u5668\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u80fd\u591f\u4e0e\u66f4\u5927\u7684\u6269\u6563\u6a21\u578b\u7ade\u4e89\u3002"}}
{"id": "2510.03892", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03892", "abs": "https://arxiv.org/abs/2510.03892", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Kantian-Utilitarian XAI: Meta-Explained", "comment": "Accepted for presentation as a poster at the 35th IEEE International\n  Conference on Collaborative Advances in Software and Computing, 2025.\n  Conference\n  website:https://conf.researchr.org/details/cascon-2025/posters-track/1/Kantian-Utilitarian-XAI-Meta-Explained", "summary": "We present a gamified explainable AI (XAI) system for ethically aware\nconsumer decision-making in the coffee domain. Each session comprises six\nrounds with three options per round. Two symbolic engines provide real-time\nreasons: a Kantian module flags rule violations (e.g., child labor,\ndeforestation risk without shade certification, opaque supply chains, unsafe\ndecaf), and a utilitarian module scores options via multi-criteria aggregation\nover normalized attributes (price, carbon, water, transparency, farmer income\nshare, taste/freshness, packaging, convenience). A meta-explainer with a regret\nbound (0.2) highlights Kantian--utilitarian (mis)alignment and switches to a\ndeontically clean, near-parity option when welfare loss is small. We release a\nstructured configuration (attribute schema, certification map, weights, rule\nset), a policy trace for auditability, and an interactive UI.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6e38\u620f\u5316\u7684\u53ef\u89e3\u91caAI\u7cfb\u7edf\uff0c\u7528\u4e8e\u5496\u5561\u6d88\u8d39\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u7ed3\u5408\u5eb7\u5fb7\u4e3b\u4e49\u548c\u529f\u5229\u4e3b\u4e49\u63a8\u7406\uff0c\u63d0\u4f9b\u5b9e\u65f6\u89e3\u91ca\u548c\u5143\u89e3\u91ca\u5668\u6765\u534f\u8c03\u4e24\u79cd\u4f26\u7406\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u6d88\u8d39\u8005\u5728\u590d\u6742\u4f9b\u5e94\u94fe\u4e2d\u505a\u51fa\u9053\u5fb7\u51b3\u7b56\u7684\u56f0\u96be\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u7cfb\u7edf\u63d0\u4f9b\u4f26\u7406\u6307\u5bfc\uff0c\u5e73\u8861\u4e0d\u540c\u9053\u5fb7\u6846\u67b6\u7684\u8003\u91cf\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7b26\u53f7\u5f15\u64ce\uff1a\u5eb7\u5fb7\u4e3b\u4e49\u6a21\u5757\u68c0\u6d4b\u89c4\u5219\u8fdd\u53cd\uff0c\u529f\u5229\u4e3b\u4e49\u6a21\u5757\u901a\u8fc7\u591a\u6807\u51c6\u805a\u5408\u8bc4\u5206\uff1b\u5143\u89e3\u91ca\u5668\u534f\u8c03\u4e24\u8005\uff0c\u5728\u798f\u5229\u635f\u5931\u5c0f\u65f6\u5207\u6362\u5230\u7b26\u5408\u9053\u4e49\u7684\u9009\u9879\u3002", "result": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u7cfb\u7edf\u914d\u7f6e\u3001\u53ef\u5ba1\u8ba1\u7684\u7b56\u7565\u8ddf\u8e2a\u548c\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\uff0c\u5b9e\u73b0\u4e86\u4f26\u7406\u51b3\u7b56\u7684\u900f\u660e\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u6574\u5408\u4e86\u4e0d\u540c\u4f26\u7406\u6846\u67b6\uff0c\u4e3a\u6d88\u8d39\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9053\u5fb7\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5496\u5561\u6d88\u8d39\u9886\u57df\u3002"}}
{"id": "2510.04472", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.04472", "abs": "https://arxiv.org/abs/2510.04472", "authors": ["Baber Jan", "Saeed Anwar", "Aiman H. El-Maleh", "Abdul Jabbar Siddiqui", "Abdul Bais"], "title": "SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection segments objects with intrinsic similarity and\nedge disruption. Current detection methods rely on accumulated complex\ncomponents. Each approach adds components such as boundary modules, attention\nmechanisms, and multi-scale processors independently. This accumulation creates\na computational burden without proportional gains. To manage this complexity,\nthey process at reduced resolutions, eliminating fine details essential for\ncamouflage. We present SPEGNet, addressing fragmentation through a unified\ndesign. The architecture integrates multi-scale features via channel\ncalibration and spatial enhancement. Boundaries emerge directly from\ncontext-rich representations, maintaining semantic-spatial alignment.\nProgressive refinement implements scale-adaptive edge modulation with peak\ninfluence at intermediate resolutions. This design strikes a balance between\nboundary precision and regional consistency. SPEGNet achieves 0.887 $S_\\alpha$\non CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.\nOur approach excels across scales, from tiny, intricate objects to large,\npattern-similar ones, while handling occlusion and ambiguous boundaries. Code,\nmodel weights, and results are available on\n\\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.", "AI": {"tldr": "SPEGNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8bbe\u8ba1\u7684\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u9053\u6821\u51c6\u548c\u7a7a\u95f4\u589e\u5f3a\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u76f4\u63a5\u751f\u6210\u8fb9\u754c\uff0c\u5728\u4fdd\u6301\u8bed\u4e49-\u7a7a\u95f4\u5bf9\u9f50\u7684\u540c\u65f6\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u4f2a\u88c5\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7ec4\u4ef6\u79ef\u7d2f\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d1f\u62c5\u589e\u52a0\u4f46\u6027\u80fd\u63d0\u5347\u4e0d\u6210\u6bd4\u4f8b\uff0c\u4e14\u5904\u7406\u4f4e\u5206\u8fa8\u7387\u65f6\u4f1a\u4e22\u5931\u7cbe\u7ec6\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u67b6\u6784\uff0c\u901a\u8fc7\u901a\u9053\u6821\u51c6\u548c\u7a7a\u95f4\u589e\u5f3a\u6574\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u8fb9\u754c\u76f4\u63a5\u4ece\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8868\u793a\u4e2d\u4ea7\u751f\uff0c\u4fdd\u6301\u8bed\u4e49-\u7a7a\u95f4\u5bf9\u9f50\uff0c\u6e10\u8fdb\u5f0f\u7ec6\u5316\u5b9e\u73b0\u5c3a\u5ea6\u81ea\u9002\u5e94\u8fb9\u7f18\u8c03\u5236\u3002", "result": "\u5728CAMO\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.887 S\u03b1\uff0cCOD10K\u4e0a0.890\uff0cNC4K\u4e0a0.895\uff0c\u5177\u6709\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "SPEGNet\u5728\u8fb9\u754c\u7cbe\u5ea6\u548c\u533a\u57df\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u5904\u7406\u4ece\u5fae\u5c0f\u590d\u6742\u76ee\u6807\u5230\u5927\u578b\u6a21\u5f0f\u76f8\u4f3c\u76ee\u6807\u7684\u591a\u79cd\u5c3a\u5ea6\uff0c\u540c\u65f6\u5904\u7406\u906e\u6321\u548c\u6a21\u7cca\u8fb9\u754c\u3002"}}
{"id": "2510.04477", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04477", "abs": "https://arxiv.org/abs/2510.04477", "authors": ["Soo Yong Kim", "Suin Cho", "Vincent-Daniel Yun", "Gyeongyeon Hwang"], "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models", "comment": null, "summary": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.", "AI": {"tldr": "MedCLM\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5c06\u68c0\u6d4b\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u5e26\u6709\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u5927\u89c4\u6a21\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6570\u636e\uff0c\u901a\u8fc7\u96c6\u6210\u8bfe\u7a0b\u7b56\u7565\u5b9e\u73b0\u4ece\u663e\u5f0f\u5b9a\u4f4d\u5230\u5f31\u76d1\u7763\u63a8\u7406\u7684\u6e10\u8fdb\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f25\u5408\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\u4e0eAI\u4e4b\u95f4\u7684\u5dee\u8ddd\u662f\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u9010\u6b65\u63a8\u7406\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51faMedCLM\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u5c06\u68c0\u6d4b\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u5e26\u6709\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u533b\u5b66VQA\u6570\u636e\uff0c\u91c7\u7528\u96c6\u6210\u8bfe\u7a0b\u7b56\u7565\uff08Easy\u9636\u6bb5\u4f7f\u7528\u663e\u5f0f\u75c5\u53d8\u6846\uff0cMedium\u9636\u6bb5\u9f13\u52b1\u9690\u5f0f\u5b9a\u4f4d\uff0cHard\u9636\u6bb5\u8fdb\u884c\u5f31\u76d1\u7763\u63a8\u7406\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMedCLM\u5728\u591a\u4e2a\u533b\u5b66VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "MedCLM\u4e3a\u5f00\u53d1\u4e34\u5e8a\u5bf9\u9f50\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2510.04479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04479", "abs": "https://arxiv.org/abs/2510.04479", "authors": ["Nonghai Zhang", "Zeyu Zhang", "Jiazi Wang", "Yang Zhao", "Hao Tang"], "title": "VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved significant progress in\nmultimodal understanding tasks, demonstrating strong capabilities particularly\nin general tasks such as image captioning and visual reasoning. However, when\ndealing with specialized cultural heritage domains like 3D vase artifacts,\nexisting models face severe data scarcity issues and insufficient domain\nknowledge limitations. Due to the lack of targeted training data, current VLMs\nstruggle to effectively handle such culturally significant specialized tasks.\nTo address these challenges, we propose the VaseVQA-3D dataset, which serves as\nthe first 3D visual question answering dataset for ancient Greek pottery\nanalysis, collecting 664 ancient Greek vase 3D models with corresponding\nquestion-answer data and establishing a complete data construction pipeline. We\nfurther develop the VaseVLM model, enhancing model performance in vase artifact\nanalysis through domain-adaptive training. Experimental results validate the\neffectiveness of our approach, where we improve by 12.8% on R@1 metrics and by\n6.6% on lexical similarity compared with previous state-of-the-art on the\nVaseVQA-3D dataset, significantly improving the recognition and understanding\nof 3D vase artifacts, providing new technical pathways for digital heritage\npreservation research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u53e4\u5e0c\u814a\u9676\u5668\u5206\u6790\u76843D\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6VaseVQA-3D\u548c\u76f8\u5e94\u7684VaseVLM\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6587\u5316\u9057\u4ea7\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u57283D\u82b1\u74f6\u6587\u7269\u8bc6\u522b\u7406\u89e3\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u9057\u4ea7\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u77e5\u8bc6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u57283D\u82b1\u74f6\u6587\u7269\u5206\u6790\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b664\u4e2a\u53e4\u5e0c\u814a\u82b1\u74f63D\u6a21\u578b\u53ca\u5bf9\u5e94\u95ee\u7b54\u6570\u636e\u7684VaseVQA-3D\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86VaseVLM\u6a21\u578b\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728VaseVQA-3D\u6570\u636e\u96c6\u4e0a\uff0cR@1\u6307\u6807\u63d0\u534712.8%\uff0c\u8bcd\u6c47\u76f8\u4f3c\u5ea6\u63d0\u53476.6%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D\u82b1\u74f6\u6587\u7269\u7684\u8bc6\u522b\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u6570\u5b57\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.04009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04009", "abs": "https://arxiv.org/abs/2510.04009", "authors": ["Zicong He", "Boxuan Zhang", "Weihao Liu", "Ruixiang Tang", "Lu Cheng"], "title": "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models", "comment": "22 pages", "summary": "The meteoric rise of foundation models (FMs) has expanded their capabilities\nfar beyond conventional tasks. Creativity, long regarded as a hallmark of human\nintelligence and a driver of innovation, is now increasingly recognized as a\ncritical dimension of machine intelligence in the era of generative FMs,\ncomplementing traditional measures of accuracy. However, existing evaluation\nframeworks for creativity remain fragmented, relying on ad hoc metrics not\nfirmly grounded in established theories. To address this gap, we introduce\nC^2-Eval, a holistic benchmark for unified assessment of creativity in FMs.\nC^2-Eval distinguishes between two complementary forms of creativity:\nconvergent creativity, where tasks admit constrained solutions (e.g., code\ngeneration), and divergent creativity, where tasks are open-ended (e.g.,\nstorytelling). It evaluates both dimensions using fine-grained criteria derived\nfrom social-science theory, focusing on Usefulness, Originality, and Surprise\n(U-O-S). Through extensive experiments on leading proprietary and open-source\nmodels, we analyze trade-offs in their creative capabilities. Our results\nhighlight both the strengths and challenges of current FMs in pursuing a\ncreative machine mind, showing that C^2-Eval is an effective lens for examining\nthe evolving landscape of creative AI.", "AI": {"tldr": "\u63d0\u51fa\u4e86C^2-Eval\u57fa\u51c6\uff0c\u7528\u4e8e\u7edf\u4e00\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u521b\u9020\u529b\uff0c\u533a\u5206\u6536\u655b\u6027\u521b\u9020\u529b\u548c\u53d1\u6563\u6027\u521b\u9020\u529b\uff0c\u57fa\u4e8e\u6709\u7528\u6027\u3001\u539f\u521b\u6027\u548c\u60ca\u559c\u5ea6\u4e09\u4e2a\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u521b\u9020\u529b\u8bc4\u4f30\u6846\u67b6\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165C^2-Eval\u57fa\u51c6\uff0c\u533a\u5206\u6536\u655b\u6027\u521b\u9020\u529b\u548c\u53d1\u6563\u6027\u521b\u9020\u529b\uff0c\u4f7f\u7528\u6709\u7528\u6027\u3001\u539f\u521b\u6027\u548c\u60ca\u559c\u5ea6\u4e09\u4e2a\u7ec6\u7c92\u5ea6\u6807\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5bf9\u9886\u5148\u4e13\u6709\u548c\u5f00\u6e90\u6a21\u578b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u5728\u521b\u9020\u529b\u80fd\u529b\u4e0a\u7684\u6743\u8861\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u8ffd\u6c42\u521b\u9020\u6027\u673a\u5668\u601d\u7ef4\u65b9\u9762\u7684\u4f18\u52bf\u548c\u6311\u6218\u3002", "conclusion": "C^2-Eval\u662f\u68c0\u9a8c\u521b\u9020\u6027AI\u53d1\u5c55\u6f14\u8fdb\u7684\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u521b\u9020\u529b\u8868\u73b0\u3002"}}
{"id": "2510.04483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04483", "abs": "https://arxiv.org/abs/2510.04483", "authors": ["Hao Fang", "Zechao Zhan", "Weixin Feng", "Ziwei Huang", "XuBin Li", "Tiezheng Ge"], "title": "TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement", "comment": null, "summary": "Recent advances in image generation and editing technologies have enabled\nstate-of-the-art models to achieve impressive results in general domains.\nHowever, when applied to e-commerce scenarios, these general models often\nencounter consistency limitations. To address this challenge, we introduce\nTBStar-Edit, an new image editing model tailored for the e-commerce domain.\nThrough rigorous data engineering, model architecture design and training\nstrategy, TBStar-Edit achieves precise and high-fidelity image editing while\nmaintaining the integrity of product appearance and layout. Specifically, for\ndata engineering, we establish a comprehensive data construction pipeline,\nencompassing data collection, construction, filtering, and augmentation, to\nacquire high-quality, instruction-following, and strongly consistent editing\ndata to support model training. For model architecture design, we design a\nhierarchical model framework consisting of a base model, pattern shifting\nmodules, and consistency enhancement modules. For model training, we adopt a\ntwo-stage training strategy to enhance the consistency preservation: first\nstage for editing pattern shifting, and second stage for consistency\nenhancement. Each stage involves training different modules with separate\ndatasets. Finally, we conduct extensive evaluations of TBStar-Edit on a\nself-proposed e-commerce benchmark, and the results demonstrate that\nTBStar-Edit outperforms existing general-domain editing models in both\nobjective metrics (VIE Score) and subjective user preference.", "AI": {"tldr": "TBStar-Edit\u662f\u4e00\u4e2a\u4e13\u4e3a\u7535\u5546\u9886\u57df\u8bbe\u8ba1\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u5de5\u7a0b\u3001\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u4ea7\u54c1\u5916\u89c2\u548c\u5e03\u5c40\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u786e\u9ad8\u4fdd\u771f\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u901a\u7528\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u6a21\u578b\u5728\u7535\u5546\u573a\u666f\u4e2d\u5b58\u5728\u4e00\u81f4\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u5f88\u597d\u5730\u4fdd\u6301\u4ea7\u54c1\u5916\u89c2\u548c\u5e03\u5c40\u7684\u5b8c\u6574\u6027\u3002", "method": "1) \u5efa\u7acb\u5168\u9762\u7684\u6570\u636e\u6784\u5efa\u6d41\u6c34\u7ebf\uff1b2) \u8bbe\u8ba1\u5305\u542b\u57fa\u7840\u6a21\u578b\u3001\u6a21\u5f0f\u8f6c\u6362\u6a21\u5757\u548c\u4e00\u81f4\u6027\u589e\u5f3a\u6a21\u5757\u7684\u5206\u5c42\u6a21\u578b\u6846\u67b6\uff1b3) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u7f16\u8f91\u6a21\u5f0f\u8f6c\u6362\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u4e00\u81f4\u6027\u589e\u5f3a\u3002", "result": "\u5728\u81ea\u5efa\u7684\u7535\u5546\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTBStar-Edit\u5728\u5ba2\u89c2\u6307\u6807(VIE Score)\u548c\u4e3b\u89c2\u7528\u6237\u504f\u597d\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528\u9886\u57df\u7f16\u8f91\u6a21\u578b\u3002", "conclusion": "TBStar-Edit\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5546\u9886\u57df\u56fe\u50cf\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e3a\u7535\u5546\u5e94\u7528\u63d0\u4f9b\u4e86\u4e13\u95e8\u7684\u56fe\u50cf\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04023", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04023", "abs": "https://arxiv.org/abs/2510.04023", "authors": ["Mizanur Rahman", "Amran Bhuiyan", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Ridwan Mahbub", "Ahmed Masry", "Shafiq Joty", "Enamul Hoque"], "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions", "comment": "Survey paper; 45 data science agents; under review", "summary": "Recent advances in large language models (LLMs) have enabled a new class of\nAI agents that automate multiple stages of the data science workflow by\nintegrating planning, tool use, and multimodal reasoning across text, code,\ntables, and visuals. This survey presents the first comprehensive,\nlifecycle-aligned taxonomy of data science agents, systematically analyzing and\nmapping forty-five systems onto the six stages of the end-to-end data science\nprocess: business understanding and data acquisition, exploratory analysis and\nvisualization, feature engineering, model building and selection,\ninterpretation and explanation, and deployment and monitoring. In addition to\nlifecycle coverage, we annotate each agent along five cross-cutting design\ndimensions: reasoning and planning style, modality integration, tool\norchestration depth, learning and alignment methods, and trust, safety, and\ngovernance mechanisms. Beyond classification, we provide a critical synthesis\nof agent capabilities, highlight strengths and limitations at each stage, and\nreview emerging benchmarks and evaluation practices. Our analysis identifies\nthree key trends: most systems emphasize exploratory analysis, visualization,\nand modeling while neglecting business understanding, deployment, and\nmonitoring; multimodal reasoning and tool orchestration remain unresolved\nchallenges; and over 90% lack explicit trust and safety mechanisms. We conclude\nby outlining open challenges in alignment stability, explainability,\ngovernance, and robust evaluation frameworks, and propose future research\ndirections to guide the development of robust, trustworthy, low-latency,\ntransparent, and broadly accessible data science agents.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u7684\u751f\u547d\u5468\u671f\u5206\u7c7b\uff0c\u5206\u6790\u4e8645\u4e2a\u7cfb\u7edf\u5728\u6570\u636e\u79d1\u5b66\u516d\u4e2a\u9636\u6bb5\u7684\u80fd\u529b\uff0c\u5e76\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u8d8b\u52bf\uff1a\u5927\u591a\u6570\u7cfb\u7edf\u5173\u6ce8\u63a2\u7d22\u6027\u5206\u6790\u548c\u5efa\u6a21\uff0c\u800c\u5ffd\u89c6\u4e1a\u52a1\u7406\u89e3\u548c\u90e8\u7f72\uff1b\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u662f\u6311\u6218\uff1b90%\u4ee5\u4e0a\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u5b89\u5168\u673a\u5236\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u51fa\u73b0\u4e86\u80fd\u591f\u81ea\u52a8\u5316\u6570\u636e\u79d1\u5b66\u5de5\u4f5c\u6d41\u7a0b\u7684AI\u4ee3\u7406\u3002\u672c\u6587\u65e8\u5728\u5bf9\u8fd9\u4e9b\u4ee3\u7406\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\u548c\u5206\u6790\uff0c\u4ee5\u6307\u5bfc\u672a\u6765\u7a33\u5065\u3001\u53ef\u4fe1\u8d56\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5f00\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e0e\u751f\u547d\u5468\u671f\u5bf9\u9f50\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u5206\u7c7b\u6cd5\uff0c\u5c0645\u4e2a\u7cfb\u7edf\u6620\u5c04\u5230\u6570\u636e\u79d1\u5b66\u6d41\u7a0b\u7684\u516d\u4e2a\u9636\u6bb5\uff0c\u5e76\u4ece\u4e94\u4e2a\u4ea4\u53c9\u8bbe\u8ba1\u7ef4\u5ea6\u8fdb\u884c\u6807\u6ce8\u5206\u6790\u3002", "result": "\u53d1\u73b0\u5927\u591a\u6570\u7cfb\u7edf\u5f3a\u8c03\u63a2\u7d22\u6027\u5206\u6790\u3001\u53ef\u89c6\u5316\u548c\u5efa\u6a21\uff0c\u800c\u5ffd\u89c6\u4e1a\u52a1\u7406\u89e3\u3001\u90e8\u7f72\u548c\u76d1\u63a7\uff1b\u591a\u6a21\u6001\u63a8\u7406\u548c\u5de5\u5177\u7f16\u6392\u4ecd\u662f\u672a\u89e3\u51b3\u7684\u6311\u6218\uff1b\u8d85\u8fc790%\u7684\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u4fe1\u4efb\u548c\u5b89\u5168\u673a\u5236\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5bf9\u9f50\u7a33\u5b9a\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u6cbb\u7406\u548c\u7a33\u5065\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6307\u5bfc\u5f00\u53d1\u7a33\u5065\u3001\u53ef\u4fe1\u8d56\u3001\u4f4e\u5ef6\u8fdf\u3001\u900f\u660e\u548c\u5e7f\u6cdb\u53ef\u8bbf\u95ee\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u3002"}}
{"id": "2510.04504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04504", "abs": "https://arxiv.org/abs/2510.04504", "authors": ["Zijing Hu", "Yunze Tong", "Fengda Zhang", "Junkun Yuan", "Jun Xiao", "Kun Kuang"], "title": "Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation", "comment": "22 pages, 11 figures, 5 tables", "summary": "Diffusion models have achieved impressive results in generating high-quality\nimages. Yet, they often struggle to faithfully align the generated images with\nthe input prompts. This limitation arises from synchronous denoising, where all\npixels simultaneously evolve from random noise to clear images. As a result,\nduring generation, the prompt-related regions can only reference the unrelated\nregions at the same noise level, failing to obtain clear context and ultimately\nimpairing text-to-image alignment. To address this issue, we propose\nasynchronous diffusion models -- a novel framework that allocates distinct\ntimesteps to different pixels and reformulates the pixel-wise denoising\nprocess. By dynamically modulating the timestep schedules of individual pixels,\nprompt-related regions are denoised more gradually than unrelated regions,\nthereby allowing them to leverage clearer inter-pixel context. Consequently,\nthese prompt-related regions achieve better alignment in the final images.\nExtensive experiments demonstrate that our asynchronous diffusion models can\nsignificantly improve text-to-image alignment across diverse prompts. The code\nrepository for this work is available at https://github.com/hu-zijing/AsynDM.", "AI": {"tldr": "\u63d0\u51fa\u5f02\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u4e3a\u4e0d\u540c\u50cf\u7d20\u5206\u914d\u4e0d\u540c\u65f6\u95f4\u6b65\u957f\u6765\u6539\u5584\u6587\u672c\u5230\u56fe\u50cf\u7684\u5bf9\u9f50\u95ee\u9898", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u91c7\u7528\u540c\u6b65\u53bb\u566a\uff0c\u6240\u6709\u50cf\u7d20\u540c\u65f6\u4ece\u566a\u58f0\u6f14\u5316\u5230\u6e05\u6670\u56fe\u50cf\uff0c\u5bfc\u81f4\u63d0\u793a\u76f8\u5173\u533a\u57df\u53ea\u80fd\u53c2\u8003\u76f8\u540c\u566a\u58f0\u6c34\u5e73\u7684\u65e0\u5173\u533a\u57df\uff0c\u65e0\u6cd5\u83b7\u5f97\u6e05\u6670\u4e0a\u4e0b\u6587\uff0c\u5f71\u54cd\u6587\u672c\u5230\u56fe\u50cf\u5bf9\u9f50\u6548\u679c", "method": "\u5f02\u6b65\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u4e3a\u4e0d\u540c\u50cf\u7d20\u5206\u914d\u4e0d\u540c\u65f6\u95f4\u6b65\u957f\uff0c\u91cd\u65b0\u5236\u5b9a\u9010\u50cf\u7d20\u53bb\u566a\u8fc7\u7a0b\uff0c\u52a8\u6001\u8c03\u8282\u5355\u4e2a\u50cf\u7d20\u7684\u65f6\u95f4\u6b65\u957f\u8ba1\u5212\uff0c\u4f7f\u63d0\u793a\u76f8\u5173\u533a\u57df\u6bd4\u65e0\u5173\u533a\u57df\u66f4\u6e10\u8fdb\u5730\u53bb\u566a", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5f02\u6b65\u6269\u6563\u6a21\u578b\u80fd\u663e\u8457\u63d0\u9ad8\u5404\u79cd\u63d0\u793a\u4e0b\u7684\u6587\u672c\u5230\u56fe\u50cf\u5bf9\u9f50\u6548\u679c", "conclusion": "\u5f02\u6b65\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5f02\u6b65\u53bb\u566a\u673a\u5236\uff0c\u5141\u8bb8\u63d0\u793a\u76f8\u5173\u533a\u57df\u5229\u7528\u66f4\u6e05\u6670\u7684\u50cf\u7d20\u95f4\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u5728\u6700\u7ec8\u56fe\u50cf\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c"}}
{"id": "2510.04128", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04128", "abs": "https://arxiv.org/abs/2510.04128", "authors": ["Dmitrii Troitskii", "Koyena Pal", "Chris Wendler", "Callum Stuart McDougall", "Neel Nanda"], "title": "Internal states before wait modulate reasoning patterns", "comment": "Accepted to EMNLP Findings 2025", "summary": "Prior work has shown that a significant driver of performance in reasoning\nmodels is their ability to reason and self-correct. A distinctive marker in\nthese reasoning traces is the token wait, which often signals reasoning\nbehavior such as backtracking. Despite being such a complex behavior, little is\nunderstood of exactly why models do or do not decide to reason in this\nparticular manner, which limits our understanding of what makes a reasoning\nmodel so effective. In this work, we address the question whether model's\nlatents preceding wait tokens contain relevant information for modulating the\nsubsequent reasoning process. We train crosscoders at multiple layers of\nDeepSeek-R1-Distill-Llama-8B and its base version, and introduce a latent\nattribution technique in the crosscoder setting. We locate a small set of\nfeatures relevant for promoting/suppressing wait tokens' probabilities.\nFinally, through a targeted series of experiments analyzing max activating\nexamples and causal interventions, we show that many of our identified features\nindeed are relevant for the reasoning process and give rise to different types\nof reasoning patterns such as restarting from the beginning, recalling prior\nknowledge, expressing uncertainty, and double-checking.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u4e2d\u7684\u7b49\u5f85\u6807\u8bb0\uff08wait tokens\uff09\u662f\u590d\u6742\u63a8\u7406\u884c\u4e3a\u7684\u5173\u952e\u4fe1\u53f7\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u6f5c\u5728\u7279\u5f81\u53ef\u4ee5\u8bc6\u522b\u548c\u8c03\u63a7\u8fd9\u4e9b\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u7406\u89e3\u4e3a\u4ec0\u4e48\u6a21\u578b\u4f1a\u51b3\u5b9a\u91c7\u7528\u7b49\u5f85\u6807\u8bb0\u8fd9\u79cd\u7279\u5b9a\u7684\u63a8\u7406\u65b9\u5f0f\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u884c\u4e3a\u80cc\u540e\u7684\u673a\u5236\uff0c\u4ece\u800c\u6df1\u5165\u7406\u89e3\u63a8\u7406\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "method": "\u5728DeepSeek-R1-Distill-Llama-8B\u53ca\u5176\u57fa\u7840\u7248\u672c\u7684\u591a\u4e2a\u5c42\u8bad\u7ec3\u4ea4\u53c9\u7f16\u7801\u5668\uff0c\u5f15\u5165\u6f5c\u5728\u5f52\u56e0\u6280\u672f\uff0c\u8bc6\u522b\u4e0e\u7b49\u5f85\u6807\u8bb0\u6982\u7387\u76f8\u5173\u7684\u7279\u5f81\u96c6\u3002", "result": "\u8bc6\u522b\u51fa\u4e00\u5c0f\u90e8\u5206\u4e0e\u4fc3\u8fdb/\u6291\u5236\u7b49\u5f85\u6807\u8bb0\u6982\u7387\u76f8\u5173\u7684\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u786e\u5b9e\u4e0e\u63a8\u7406\u8fc7\u7a0b\u76f8\u5173\uff0c\u5e76\u4ea7\u751f\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "\u6a21\u578b\u6f5c\u5728\u7279\u5f81\u5305\u542b\u8c03\u63a7\u540e\u7eed\u63a8\u7406\u8fc7\u7a0b\u7684\u76f8\u5173\u4fe1\u606f\uff0c\u7b49\u5f85\u6807\u8bb0\u662f\u590d\u6742\u63a8\u7406\u884c\u4e3a\u7684\u91cd\u8981\u4fe1\u53f7\uff0c\u53ef\u4ee5\u63ed\u793a\u63a8\u7406\u6a21\u578b\u7684\u5de5\u4f5c\u673a\u5236\u3002"}}
{"id": "2510.04533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04533", "abs": "https://arxiv.org/abs/2510.04533", "authors": ["Hyunmin Cho", "Donghoon Ahn", "Susung Hong", "Jee Eun Kim", "Seungryong Kim", "Kyong Hwan Jin"], "title": "TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling", "comment": "16 pages, 9 figures, 5 tables", "summary": "Recent diffusion models achieve the state-of-the-art performance in image\ngeneration, but often suffer from semantic inconsistencies or hallucinations.\nWhile various inference-time guidance methods can enhance generation, they\noften operate indirectly by relying on external signals or architectural\nmodifications, which introduces additional computational overhead. In this\npaper, we propose Tangential Amplifying Guidance (TAG), a more efficient and\ndirect guidance method that operates solely on trajectory signals without\nmodifying the underlying diffusion model. TAG leverages an intermediate sample\nas a projection basis and amplifies the tangential components of the estimated\nscores with respect to this basis to correct the sampling trajectory. We\nformalize this guidance process by leveraging a first-order Taylor expansion,\nwhich demonstrates that amplifying the tangential component steers the state\ntoward higher-probability regions, thereby reducing inconsistencies and\nenhancing sample quality. TAG is a plug-and-play, architecture-agnostic module\nthat improves diffusion sampling fidelity with minimal computational addition,\noffering a new perspective on diffusion guidance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5207\u7ebf\u653e\u5927\u5f15\u5bfc\uff08TAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u5927\u5206\u6570\u4f30\u8ba1\u7684\u5207\u7ebf\u5206\u91cf\u6765\u4fee\u6b63\u91c7\u6837\u8f68\u8ff9\uff0c\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5e38\u51fa\u73b0\u8bed\u4e49\u4e0d\u4e00\u81f4\u6216\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u63a8\u7406\u65f6\u5f15\u5bfc\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u4fe1\u53f7\u6216\u67b6\u6784\u4fee\u6539\uff0c\u4f1a\u5f15\u5165\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u5229\u7528\u4e2d\u95f4\u6837\u672c\u4f5c\u4e3a\u6295\u5f71\u57fa\uff0c\u653e\u5927\u5206\u6570\u4f30\u8ba1\u76f8\u5bf9\u4e8e\u8be5\u57fa\u7684\u5207\u7ebf\u5206\u91cf\u6765\u4fee\u6b63\u91c7\u6837\u8f68\u8ff9\uff0c\u57fa\u4e8e\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u5f62\u5f0f\u5316\u5f15\u5bfc\u8fc7\u7a0b\u3002", "result": "TAG\u80fd\u591f\u5c06\u72b6\u6001\u5f15\u5bfc\u5230\u66f4\u9ad8\u6982\u7387\u533a\u57df\uff0c\u51cf\u5c11\u4e0d\u4e00\u81f4\u6027\u5e76\u63d0\u5347\u6837\u672c\u8d28\u91cf\u3002", "conclusion": "TAG\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u3001\u67b6\u6784\u65e0\u5173\u7684\u6a21\u5757\uff0c\u80fd\u4ee5\u6700\u5c0f\u8ba1\u7b97\u4ee3\u4ef7\u63d0\u9ad8\u6269\u6563\u91c7\u6837\u4fdd\u771f\u5ea6\uff0c\u4e3a\u6269\u6563\u5f15\u5bfc\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.04140", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04140", "abs": "https://arxiv.org/abs/2510.04140", "authors": ["Zishang Jiang", "Jinyi Han", "Tingyun Li", "Xinyi Wang", "Sihang Jiang", "Jiaqing Liang", "Zhaoqian Dai", "Shuguang Ma", "Fei Yu", "Yanghua Xiao"], "title": "Selective Expert Guidance for Effective and Diverse Exploration in Reinforcement Learning of LLMs", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a widely\nadopted technique for enhancing the reasoning ability of Large Language Models\n(LLMs). However, the effectiveness of RLVR strongly depends on the capability\nof base models. This issue arises because it requires the model to have\nsufficient capability to perform high-quality exploration, which involves both\neffectiveness and diversity. Unfortunately, existing methods address this issue\nby imitating expert trajectories, which improve effectiveness but neglect\ndiversity. To address this, we argue that the expert only needs to provide\nguidance only at critical decision points rather than the entire reasoning\npath. Based on this insight, we propose MENTOR: Mixed-policy Expert Navigation\nfor Token-level Optimization of Reasoning, a framework that provides expert\nguidance only at critical decision points to perform effective and diverse\nexploration in RLVR. Extensive experiments show that MENTOR enables models\ncapture the essence of expert strategies rather than surface imitation, thereby\nperforming high-quality exploration and achieving superior overall performance.\nOur code is available online.", "AI": {"tldr": "\u63d0\u51faMENTOR\u6846\u67b6\uff0c\u5728RLVR\u4e2d\u4ec5\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u5b9e\u73b0\u6709\u6548\u4e14\u591a\u6837\u5316\u7684\u63a2\u7d22\uff0c\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u80fd\u529b\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u63a2\u7d22\uff08\u6709\u6548\u6027\u548c\u591a\u6837\u6027\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u6a21\u4eff\u4e13\u5bb6\u8f68\u8ff9\u53ea\u63d0\u5347\u6709\u6548\u6027\u800c\u5ffd\u89c6\u591a\u6837\u6027", "method": "MENTOR\u6846\u67b6\uff1a\u6df7\u5408\u7b56\u7565\u4e13\u5bb6\u5bfc\u822a\uff0c\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\uff0c\u8fdb\u884c\u4ee4\u724c\u7ea7\u4f18\u5316\u63a8\u7406", "result": "\u5b9e\u9a8c\u8868\u660eMENTOR\u80fd\u6355\u6349\u4e13\u5bb6\u7b56\u7565\u672c\u8d28\u800c\u975e\u8868\u9762\u6a21\u4eff\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u63a2\u7d22\u548c\u66f4\u4f18\u6574\u4f53\u6027\u80fd", "conclusion": "\u5728\u5173\u952e\u51b3\u7b56\u70b9\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\u6bd4\u5b8c\u6574\u8f68\u8ff9\u6a21\u4eff\u66f4\u6709\u6548\uff0cMENTOR\u6846\u67b6\u80fd\u63d0\u5347RLVR\u7684\u63a2\u7d22\u8d28\u91cf\u548c\u63a8\u7406\u6027\u80fd"}}
{"id": "2510.04564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04564", "abs": "https://arxiv.org/abs/2510.04564", "authors": ["Honglin Liu", "Chao Sun", "Peng Hu", "Yunfan Li", "Xi Peng"], "title": "Conditional Representation Learning for Customized Tasks", "comment": null, "summary": "Conventional representation learning methods learn a universal representation\nthat primarily captures dominant semantics, which may not always align with\ncustomized downstream tasks. For instance, in animal habitat analysis,\nresearchers prioritize scene-related features, whereas universal embeddings\nemphasize categorical semantics, leading to suboptimal results. As a solution,\nexisting approaches resort to supervised fine-tuning, which however incurs high\ncomputational and annotation costs. In this paper, we propose Conditional\nRepresentation Learning (CRL), aiming to extract representations tailored to\narbitrary user-specified criteria. Specifically, we reveal that the semantics\nof a space are determined by its basis, thereby enabling a set of descriptive\nwords to approximate the basis for a customized feature space. Building upon\nthis insight, given a user-specified criterion, CRL first employs a large\nlanguage model (LLM) to generate descriptive texts to construct the semantic\nbasis, then projects the image representation into this conditional feature\nspace leveraging a vision-language model (VLM). The conditional representation\nbetter captures semantics for the specific criterion, which could be utilized\nfor multiple customized tasks. Extensive experiments on classification and\nretrieval tasks demonstrate the superiority and generality of the proposed CRL.\nThe code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.", "AI": {"tldr": "\u63d0\u51fa\u6761\u4ef6\u8868\u793a\u5b66\u4e60\uff08CRL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7528\u6237\u6307\u5b9a\u7684\u6761\u4ef6\u751f\u6210\u5b9a\u5236\u5316\u7279\u5f81\u8868\u793a\uff0c\u89e3\u51b3\u901a\u7528\u8868\u793a\u5b66\u4e60\u4e0e\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5b66\u4e60\u7684\u662f\u901a\u7528\u8868\u793a\uff0c\u4e3b\u8981\u6355\u6349\u4e3b\u5bfc\u8bed\u4e49\uff0c\u53ef\u80fd\u4e0e\u5b9a\u5236\u5316\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0d\u5339\u914d\u3002\u4f8b\u5982\u5728\u52a8\u7269\u6816\u606f\u5730\u5206\u6790\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u5173\u6ce8\u573a\u666f\u76f8\u5173\u7279\u5f81\uff0c\u800c\u901a\u7528\u5d4c\u5165\u5f3a\u8c03\u7c7b\u522b\u8bed\u4e49\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u76d1\u7763\u5fae\u8c03\uff0c\u8ba1\u7b97\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u3002", "method": "CRL\u5229\u7528LLM\u6839\u636e\u7528\u6237\u6307\u5b9a\u6761\u4ef6\u751f\u6210\u63cf\u8ff0\u6027\u6587\u672c\u6765\u6784\u5efa\u8bed\u4e49\u57fa\uff0c\u7136\u540e\u4f7f\u7528VLM\u5c06\u56fe\u50cf\u8868\u793a\u6295\u5f71\u5230\u8fd9\u4e2a\u6761\u4ef6\u7279\u5f81\u7a7a\u95f4\u4e2d\u3002\u901a\u8fc7\u63ed\u793a\u7a7a\u95f4\u8bed\u4e49\u7531\u5176\u57fa\u51b3\u5b9a\uff0c\u4f7f\u5f97\u4e00\u7ec4\u63cf\u8ff0\u6027\u8bcd\u8bed\u80fd\u591f\u8fd1\u4f3c\u5b9a\u5236\u5316\u7279\u5f81\u7a7a\u95f4\u7684\u57fa\u3002", "result": "\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86CRL\u7684\u4f18\u8d8a\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "CRL\u80fd\u591f\u4e3a\u4efb\u610f\u7528\u6237\u6307\u5b9a\u6761\u4ef6\u63d0\u53d6\u5b9a\u5236\u5316\u8868\u793a\uff0c\u66f4\u597d\u5730\u6355\u6349\u7279\u5b9a\u6807\u51c6\u7684\u8bed\u4e49\uff0c\u53ef\u7528\u4e8e\u591a\u4e2a\u5b9a\u5236\u5316\u4efb\u52a1\u3002"}}
{"id": "2510.04587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04587", "abs": "https://arxiv.org/abs/2510.04587", "authors": ["Sheng Wang", "Ruiming Wu", "Charles Herndon", "Yihang Liu", "Shunsuke Koga", "Jeanne Shen", "Zhi Huang"], "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior", "comment": null, "summary": "Diagnosing a whole-slide image is an interactive, multi-stage process\ninvolving changes in magnification and movement between fields. Although recent\npathology foundation models are strong, practical agentic systems that decide\nwhat field to examine next, adjust magnification, and deliver explainable\ndiagnoses are still lacking. The blocker is data: scalable, clinically aligned\nsupervision of expert viewing behavior that is tacit and experience-based, not\nwritten in textbooks or online, and therefore absent from large language model\ntraining. We introduce the AI Session Recorder, which works with standard WSI\nviewers to unobtrusively record routine navigation and convert the viewer logs\ninto standardized behavioral commands (inspect or peek at discrete\nmagnifications) and bounding boxes. A lightweight human-in-the-loop review\nturns AI-drafted rationales into the Pathology-CoT dataset, a form of paired\n\"where to look\" and \"why it matters\" supervision produced at roughly six times\nlower labeling time. Using this behavioral data, we build Pathologist-o3, a\ntwo-stage agent that first proposes regions of interest and then performs\nbehavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,\nit achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the\nstate-of-the-art OpenAI o3 model and generalizing across backbones. To our\nknowledge, this constitutes one of the first behavior-grounded agentic systems\nin pathology. Turning everyday viewer logs into scalable, expert-validated\nsupervision, our framework makes agentic pathology practical and establishes a\npath to human-aligned, upgradeable clinical AI.", "AI": {"tldr": "\u5f00\u53d1\u4e86AI Session Recorder\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bb0\u5f55\u75c5\u7406\u5b66\u5bb6\u5728\u6570\u5b57\u75c5\u7406\u56fe\u50cf\u4e0a\u7684\u6d4f\u89c8\u884c\u4e3a\uff0c\u6784\u5efaPathology-CoT\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3Pathologist-o3\u667a\u80fd\u4f53\uff0c\u5728\u80c3\u80a0\u9053\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u5b66AI\u7cfb\u7edf\u7f3a\u4e4f\u5b9e\u9645\u53ef\u7528\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u6a21\u62df\u4e13\u5bb6\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u591a\u9636\u6bb5\u4ea4\u4e92\u884c\u4e3a\uff08\u5982\u8c03\u6574\u653e\u5927\u500d\u6570\u3001\u79fb\u52a8\u89c6\u91ce\u7b49\uff09\uff0c\u4e3b\u8981\u969c\u788d\u662f\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u3001\u4e34\u5e8a\u5bf9\u9f50\u7684\u4e13\u5bb6\u884c\u4e3a\u76d1\u7763\u6570\u636e\u3002", "method": "1. \u5f00\u53d1AI Session Recorder\uff0c\u4e0e\u6807\u51c6WSI\u67e5\u770b\u5668\u914d\u5408\uff0c\u65e0\u5e72\u6270\u5730\u8bb0\u5f55\u5e38\u89c4\u5bfc\u822a\u884c\u4e3a\uff1b2. \u5c06\u67e5\u770b\u5668\u65e5\u5fd7\u8f6c\u6362\u4e3a\u6807\u51c6\u5316\u884c\u4e3a\u547d\u4ee4\u548c\u8fb9\u754c\u6846\uff1b3. \u901a\u8fc7\u8f7b\u91cf\u7ea7\u4eba\u5de5\u5ba1\u6838\u521b\u5efaPathology-CoT\u6570\u636e\u96c6\uff1b4. \u6784\u5efa\u4e24\u9636\u6bb5Pathologist-o3\u667a\u80fd\u4f53\uff1a\u5148\u63d0\u51fa\u611f\u5174\u8da3\u533a\u57df\uff0c\u518d\u8fdb\u884c\u884c\u4e3a\u5f15\u5bfc\u63a8\u7406\u3002", "result": "\u5728\u80c3\u80a0\u9053\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u8fbe\u523084.5%\u7cbe\u786e\u7387\u3001100.0%\u53ec\u56de\u7387\u548c75.4%\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684OpenAI o3\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u65e5\u5e38\u67e5\u770b\u5668\u65e5\u5fd7\u8f6c\u5316\u4e3a\u53ef\u6269\u5c55\u7684\u4e13\u5bb6\u9a8c\u8bc1\u76d1\u7763\u6570\u636e\uff0c\u4f7f\u667a\u80fd\u75c5\u7406\u5b66\u53d8\u5f97\u5b9e\u7528\uff0c\u5e76\u4e3a\u4eba\u7c7b\u5bf9\u9f50\u3001\u53ef\u5347\u7ea7\u7684\u4e34\u5e8aAI\u5efa\u7acb\u4e86\u8def\u5f84\u3002"}}
{"id": "2510.04628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04628", "abs": "https://arxiv.org/abs/2510.04628", "authors": ["Hao Liu", "Yunhao Gao", "Wei Li", "Mingyang Zhang", "Maoguo Gong", "Lorenzo Bruzzone"], "title": "A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification", "comment": null, "summary": "Deep learning-based methods have achieved significant success in remote\nsensing Earth observation data analysis. Numerous feature fusion techniques\naddress multimodal remote sensing image classification by integrating global\nand local features. However, these techniques often struggle to extract\nstructural and detail features from heterogeneous and redundant multimodal\nimages. With the goal of introducing frequency domain learning to model key and\nsparse detail features, this paper introduces the spatial-spectral-frequency\ninteraction network (S$^2$Fin), which integrates pairwise fusion modules across\nthe spatial, spectral, and frequency domains. Specifically, we propose a\nhigh-frequency sparse enhancement transformer that employs sparse\nspatial-spectral attention to optimize the parameters of the high-frequency\nfilter. Subsequently, a two-level spatial-frequency fusion strategy is\nintroduced, comprising an adaptive frequency channel module that fuses\nlow-frequency structures with enhanced high-frequency details, and a\nhigh-frequency resonance mask that emphasizes sharp edges via phase similarity.\nIn addition, a spatial-spectral attention fusion module further enhances\nfeature extraction at intermediate layers of the network. Experiments on four\nbenchmark multimodal datasets with limited labeled data demonstrate that\nS$^2$Fin performs superior classification, outperforming state-of-the-art\nmethods. The code is available at https://github.com/HaoLiu-XDU/SSFin.", "AI": {"tldr": "\u63d0\u51faS\u00b2Fin\u7f51\u7edc\uff0c\u901a\u8fc7\u7a7a\u95f4-\u5149\u8c31-\u9891\u7387\u57df\u4ea4\u4e92\u878d\u5408\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4e2d\u7ed3\u6784\u7279\u5f81\u548c\u7ec6\u8282\u7279\u5f81\u63d0\u53d6\u56f0\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u96be\u4ee5\u4ece\u5f02\u6784\u5197\u4f59\u7684\u591a\u6a21\u6001\u56fe\u50cf\u4e2d\u63d0\u53d6\u7ed3\u6784\u548c\u7ec6\u8282\u7279\u5f81\uff0c\u9700\u8981\u5f15\u5165\u9891\u57df\u5b66\u4e60\u6765\u5efa\u6a21\u5173\u952e\u7a00\u758f\u7ec6\u8282\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u9ad8\u9891\u7a00\u758f\u589e\u5f3aTransformer\uff0c\u91c7\u7528\u7a00\u758f\u7a7a\u95f4-\u5149\u8c31\u6ce8\u610f\u529b\u4f18\u5316\u9ad8\u9891\u6ee4\u6ce2\u5668\u53c2\u6570\uff1b\u5f15\u5165\u4e24\u7ea7\u7a7a\u95f4-\u9891\u7387\u878d\u5408\u7b56\u7565\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u9891\u7387\u901a\u9053\u6a21\u5757\u548c\u9ad8\u9891\u5171\u632f\u63a9\u7801\uff1b\u4ee5\u53ca\u7a7a\u95f4-\u5149\u8c31\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cS\u00b2Fin\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u4e0b\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "S\u00b2Fin\u901a\u8fc7\u591a\u57df\u4ea4\u4e92\u878d\u5408\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u9891\u57df\u5b66\u4e60\u5728\u7279\u5f81\u63d0\u53d6\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.04265", "categories": ["cs.AI", "cs.CL", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2510.04265", "abs": "https://arxiv.org/abs/2510.04265", "authors": ["Mohsen Hariri", "Amirhossein Samandar", "Michael Hinczewski", "Vipin Chaudhary"], "title": "Don't Pass$\\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation", "comment": "Code and simulations: https://mohsenhariri.github.io/bayes-kit", "summary": "Pass$@k$ is widely used to report performance for LLM reasoning, but it often\nyields unstable, misleading rankings, especially when the number of trials\n(samples) is limited and compute is constrained. We present a principled\nBayesian evaluation framework that replaces Pass$@k$ and average accuracy over\n$N$ trials (avg$@N$) with posterior estimates of a model's underlying success\nprobability and credible intervals, yielding stable rankings and a transparent\ndecision rule for differences. Evaluation outcomes are modeled as categorical\n(not just 0/1) with a Dirichlet prior, giving closed-form expressions for the\nposterior mean and uncertainty of any weighted rubric and enabling the use of\nprior evidence when appropriate. Theoretically, under a uniform prior, the\nBayesian posterior mean is order-equivalent to average accuracy (Pass$@1$),\nexplaining its empirical robustness while adding principled uncertainty.\nEmpirically, in simulations with known ground-truth success rates and on\nAIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster\nconvergence and greater rank stability than Pass$@k$ and recent variants,\nenabling reliable comparisons at far smaller sample counts. The framework\nclarifies when observed gaps are statistically meaningful (non-overlapping\ncredible intervals) versus noise, and it naturally extends to graded,\nrubric-based evaluations. Together, these results recommend replacing Pass$@k$\nfor LLM evaluation and ranking with a posterior-based, compute-efficient\nprotocol that unifies binary and non-binary evaluation while making uncertainty\nexplicit. Code is available at https://mohsenhariri.github.io/bayes-kit", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8d1d\u53f6\u65af\u8bc4\u4f30\u6846\u67b6\u66ff\u4ee3Pass@k\uff0c\u901a\u8fc7\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u548c\u53ef\u4fe1\u533a\u95f4\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u6a21\u578b\u6392\u540d\u548c\u900f\u660e\u7684\u51b3\u7b56\u89c4\u5219\u3002", "motivation": "Pass@k\u5728\u6709\u9650\u8bd5\u9a8c\u6b21\u6570\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u65f6\u4f1a\u4ea7\u751f\u4e0d\u7a33\u5b9a\u548c\u8bef\u5bfc\u6027\u7684\u6392\u540d\u7ed3\u679c\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u72c4\u5229\u514b\u96f7\u5148\u9a8c\u5bf9\u8bc4\u4f30\u7ed3\u679c\u8fdb\u884c\u5efa\u6a21\uff0c\u63d0\u4f9b\u540e\u9a8c\u5747\u503c\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u652f\u6301\u52a0\u6743\u8bc4\u5206\u6807\u51c6\u548c\u5148\u9a8c\u8bc1\u636e\u7684\u4f7f\u7528\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u548cAIME'24/'25\u3001HMMT'25\u3001BrUMO'25\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u8d1d\u53f6\u65af\u65b9\u6cd5\u6bd4Pass@k\u53ca\u5176\u53d8\u4f53\u6536\u655b\u66f4\u5feb\u3001\u6392\u540d\u66f4\u7a33\u5b9a\uff0c\u80fd\u5728\u66f4\u5c11\u6837\u672c\u4e0b\u5b9e\u73b0\u53ef\u9760\u6bd4\u8f83\u3002", "conclusion": "\u5efa\u8bae\u7528\u57fa\u4e8e\u540e\u9a8c\u6982\u7387\u7684\u8ba1\u7b97\u9ad8\u6548\u534f\u8bae\u66ff\u4ee3Pass@k\uff0c\u7edf\u4e00\u4e8c\u5143\u548c\u975e\u4e8c\u5143\u8bc4\u4f30\uff0c\u540c\u65f6\u660e\u786e\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2510.04630", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04630", "abs": "https://arxiv.org/abs/2510.04630", "authors": ["Vrushank Ahire", "Aniruddh Muley", "Shivam Zample", "Siddharth Verma", "Pranav Menon", "Surbhi Madan", "Abhinav Dhall"], "title": "SFANet: Spatial-Frequency Attention Network for Deepfake Detection", "comment": null, "summary": "Detecting manipulated media has now become a pressing issue with the recent\nrise of deepfakes. Most existing approaches fail to generalize across diverse\ndatasets and generation techniques. We thus propose a novel ensemble framework,\ncombining the strengths of transformer-based architectures, such as Swin\nTransformers and ViTs, and texture-based methods, to achieve better detection\naccuracy and robustness. Our method introduces innovative data-splitting,\nsequential training, frequency splitting, patch-based attention, and face\nsegmentation techniques to handle dataset imbalances, enhance high-impact\nregions (e.g., eyes and mouth), and improve generalization. Our model achieves\nstate-of-the-art performance when tested on the DFWild-Cup dataset, a diverse\nsubset of eight deepfake datasets. The ensemble benefits from the\ncomplementarity of these approaches, with transformers excelling in global\nfeature extraction and texturebased methods providing interpretability. This\nwork demonstrates that hybrid models can effectively address the evolving\nchallenges of deepfake detection, offering a robust solution for real-world\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Transformer\u67b6\u6784\u548c\u7eb9\u7406\u65b9\u6cd5\u7684\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u5a92\u4f53\uff0c\u5728DFWild-Cup\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u751f\u6210\u6280\u672f\u95f4\u6cdb\u5316\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u96c6\u6210Swin Transformers\u3001ViTs\u548c\u7eb9\u7406\u65b9\u6cd5\uff0c\u91c7\u7528\u521b\u65b0\u6570\u636e\u5206\u5272\u3001\u987a\u5e8f\u8bad\u7ec3\u3001\u9891\u7387\u5206\u5272\u3001\u57fa\u4e8e\u8865\u4e01\u7684\u6ce8\u610f\u529b\u548c\u9762\u90e8\u5206\u5272\u6280\u672f\u3002", "result": "\u5728\u5305\u542b\u516b\u4e2a\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u7684DFWild-Cup\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u6df7\u5408\u6a21\u578b\u80fd\u6709\u6548\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04645", "abs": "https://arxiv.org/abs/2510.04645", "authors": ["Hugo Resende", "Fabio A. Faria", "Eduardo B. Neto", "Isabela Borlido", "Victor Sundermann", "Silvio Jamil F. Guimar\u00e3es", "\u00c1lvaro L. Fazenda"], "title": "Do Superpixel Segmentation Methods Influence Deforestation Image Classification?", "comment": "15 pages, 3 figures, paper accepted to present at CIARP 2025", "summary": "Image segmentation is a crucial step in various visual applications,\nincluding environmental monitoring through remote sensing. In the context of\nthe ForestEyes project, which combines citizen science and machine learning to\ndetect deforestation in tropical forests, image segments are used for labeling\nby volunteers and subsequent model training. Traditionally, the Simple Linear\nIterative Clustering (SLIC) algorithm is adopted as the segmentation method.\nHowever, recent studies have indicated that other superpixel-based methods\noutperform SLIC in remote sensing image segmentation, and might suggest that\nthey are more suitable for the task of detecting deforested areas. In this\nsense, this study investigated the impact of the four best segmentation\nmethods, together with SLIC, on the training of classifiers for the target\napplication. Initially, the results showed little variation in performance\namong segmentation methods, even when selecting the top five classifiers using\nthe PyCaret AutoML library. However, by applying a classifier fusion approach\n(ensemble of classifiers), noticeable improvements in balanced accuracy were\nobserved, highlighting the importance of both the choice of segmentation method\nand the combination of machine learning-based models for deforestation\ndetection tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86SLIC\u548c\u5176\u4ed6\u56db\u79cd\u8d85\u50cf\u7d20\u5206\u5272\u65b9\u6cd5\u5728\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0\u901a\u8fc7\u5206\u7c7b\u5668\u878d\u5408\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfSLIC\u7b97\u6cd5\u5728\u9065\u611f\u56fe\u50cf\u5206\u5272\u4e2d\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u9009\u62e9\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u4ed6\u5206\u5272\u65b9\u6cd5\u5bf9\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u6bd4\u8f83\u4e86SLIC\u548c\u5176\u4ed6\u56db\u79cd\u6700\u4f73\u5206\u5272\u65b9\u6cd5\uff0c\u4f7f\u7528PyCaret AutoML\u5e93\u9009\u62e9\u524d\u4e94\u540d\u5206\u7c7b\u5668\uff0c\u5e76\u5e94\u7528\u5206\u7c7b\u5668\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u521d\u59cb\u7ed3\u679c\u663e\u793a\u5404\u5206\u5272\u65b9\u6cd5\u6027\u80fd\u5dee\u5f02\u4e0d\u5927\uff0c\u4f46\u901a\u8fc7\u5206\u7c7b\u5668\u878d\u5408\u65b9\u6cd5\u540e\uff0c\u5e73\u8861\u51c6\u786e\u7387\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u5206\u5272\u65b9\u6cd5\u7684\u9009\u62e9\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u7ec4\u5408\u90fd\u5bf9\u68ee\u6797\u780d\u4f10\u68c0\u6d4b\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.04391", "categories": ["cs.AI", "cs.CL", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.04391", "abs": "https://arxiv.org/abs/2510.04391", "authors": ["Saurabh Ranjan", "Brian Odegaard"], "title": "Internal World Models as Imagination Networks in Cognitive Agents", "comment": null, "summary": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u60f3\u8c61\u529b\u7684\u8ba1\u7b97\u76ee\u6807\u662f\u8bbf\u95ee\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u6bd4\u8f83\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u60f3\u8c61\u529b\u7f51\u7edc\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u60f3\u8c61\u529b\u7684\u8ba1\u7b97\u76ee\u6807\uff0c\u6311\u6218\u4f20\u7edf\u8ba4\u4e3a\u60f3\u8c61\u529b\u4e3b\u8981\u7528\u4e8e\u6700\u5927\u5316\u5956\u52b1\u7684\u89c2\u70b9\uff0c\u7814\u7a76\u4eba\u7c7b\u548cAI\u5728\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4e0a\u7684\u76f8\u4f3c\u6027\u3002", "method": "\u4f7f\u7528\u5fc3\u7406\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\u8bc4\u4f30\u60f3\u8c61\u529b\u751f\u52a8\u5ea6\uff0c\u6784\u5efa\u60f3\u8c61\u529b\u7f51\u7edc\uff0c\u6bd4\u8f83\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u548c\u5bf9\u8bdd\u8bb0\u5fc6\u6761\u4ef6\u4e0b\u7684\u7f51\u7edc\u7279\u5f81\u3002", "result": "\u4eba\u7c7b\u60f3\u8c61\u529b\u7f51\u7edc\u663e\u793a\u4e0d\u540c\u4e2d\u5fc3\u6027\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u60f3\u8c61\u529b\u7f51\u7edc\u7f3a\u4e4f\u805a\u7c7b\u4e14\u4e2d\u5fc3\u6027\u6307\u6807\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u8868\u660e\u4e24\u8005\u5185\u90e8\u4e16\u754c\u6a21\u578b\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6bd4\u8f83\u4eba\u7c7b\u548cAI\u5185\u90e8\u751f\u6210\u8868\u5f81\u7684\u65b0\u65b9\u6cd5\uff0c\u4e3a\u5f00\u53d1\u7c7b\u4eba\u60f3\u8c61\u529b\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.04648", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.04648", "abs": "https://arxiv.org/abs/2510.04648", "authors": ["Buyuan Zhu", "Shiyu Hu", "Yiping Ma", "Yuanming Zhang", "Kang Hao Cheong"], "title": "EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents", "comment": "Preprint, Under review", "summary": "As large language models are increasingly integrated into education, virtual\nstudent agents are becoming vital for classroom simulation and teacher\ntraining. Yet their classroom-oriented subjective abilities remain largely\nunassessed, limiting understanding of model boundaries and hindering\ntrustworthy deployment. We present EduPersona, a large-scale benchmark spanning\ntwo languages, three subjects, and ten persona types based on the Big Five\ntheory. The dataset contains 1,308 authentic classroom dialogue rounds,\ncorresponding to 12,814 teacher-student Q&A turns, and is further expanded\nthrough persona stylization into roughly 10 times larger scale (128k turns),\nproviding a solid foundation for evaluation. Building on this resource, we\ndecompose hard-to-quantify subjective performance into three progressive tasks:\nTASK1 basic coherence (whether behavior, emotion, expression, and voice align\nwith classroom context), TASK2 student realism, and TASK3 long-term persona\nconsistency, thereby establishing an evaluation framework grounded in\neducational theory and research value. We conduct systematic experiments on\nthree representative LLMs, comparing their original versions with ten\npersona-fine-tuned variants trained on EduPersona. Results show consistent and\nsignificant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,\nand TASK3 +14.9%. These improvements highlight the dataset's effectiveness and\nresearch value, while also revealing the heterogeneous difficulty of persona\nmodeling. In summary, EduPersona delivers the first classroom benchmark\ncentered on subjective abilities, establishes a decoupled and verifiable\nresearch paradigm, and we will open-source both the dataset and the framework\nto support the broader research community in advancing trustworthy and\nhuman-like AI for education.", "AI": {"tldr": "\u63d0\u51fa\u4e86EduPersona\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u865a\u62df\u5b66\u751f\u4ee3\u7406\u5728\u8bfe\u5802\u73af\u5883\u4e2d\u7684\u4e3b\u89c2\u80fd\u529b\uff0c\u5305\u542b\u4e09\u4e2a\u6e10\u8fdb\u4efb\u52a1\uff1a\u57fa\u672c\u8fde\u8d2f\u6027\u3001\u5b66\u751f\u771f\u5b9e\u6027\u548c\u957f\u671f\u89d2\u8272\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u89d2\u8272\u5fae\u8c03\u5728\u4e09\u4e2a\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u4e2d\u7684\u96c6\u6210\uff0c\u865a\u62df\u5b66\u751f\u4ee3\u7406\u5728\u8bfe\u5802\u6a21\u62df\u548c\u6559\u5e08\u57f9\u8bad\u4e2d\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u5176\u8bfe\u5802\u5bfc\u5411\u7684\u4e3b\u89c2\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u8bc4\u4f30\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8fb9\u754c\u7406\u89e3\u548c\u53ef\u4fe1\u90e8\u7f72\u3002", "method": "\u6784\u5efa\u4e86\u57fa\u4e8e\u5927\u4e94\u4eba\u683c\u7406\u8bba\u7684\u5927\u89c4\u6a21\u57fa\u51c6EduPersona\uff0c\u5305\u542b\u4e24\u4e2a\u8bed\u8a00\u3001\u4e09\u4e2a\u5b66\u79d1\u548c\u5341\u79cd\u89d2\u8272\u7c7b\u578b\u3002\u5c06\u4e3b\u89c2\u6027\u80fd\u5206\u89e3\u4e3a\u4e09\u4e2a\u6e10\u8fdb\u4efb\u52a1\uff0c\u5e76\u5bf9\u4e09\u4e2a\u4ee3\u8868\u6027LLM\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u539f\u59cb\u7248\u672c\u4e0e\u5728EduPersona\u4e0a\u8bad\u7ec3\u7684\u5341\u4e2a\u89d2\u8272\u5fae\u8c03\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u6709\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff1aTASK1 +33.6%\u3001TASK2 +30.6%\u3001TASK3 +14.9%\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u548c\u7814\u7a76\u4ef7\u503c\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u89d2\u8272\u5efa\u6a21\u7684\u5f02\u8d28\u96be\u5ea6\u3002", "conclusion": "EduPersona\u63d0\u4f9b\u4e86\u9996\u4e2a\u4ee5\u4e3b\u89c2\u80fd\u529b\u4e3a\u4e2d\u5fc3\u7684\u8bfe\u5802\u57fa\u51c6\uff0c\u5efa\u7acb\u4e86\u53ef\u89e3\u8026\u548c\u53ef\u9a8c\u8bc1\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u5e76\u5c06\u5f00\u6e90\u6570\u636e\u96c6\u548c\u6846\u67b6\u4ee5\u652f\u6301\u6559\u80b2\u9886\u57df\u53ef\u4fe1\u548c\u7c7b\u4ebaAI\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.04654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04654", "abs": "https://arxiv.org/abs/2510.04654", "authors": ["Andy C\u01cetrun\u01ce", "Adrian Cosma", "Emilian R\u01cedoi"], "title": "MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts", "comment": "4 Figures, 4 Tables", "summary": "Gait encodes rich biometric and behavioural information, yet leveraging the\nmanner of walking to infer psychological traits remains a challenging and\nunderexplored problem. We introduce a hierarchical Multi-Stage Mixture of\nMovement Experts (MoME) architecture for multi-task prediction of psychological\nattributes from gait sequences represented as 2D poses. MoME processes the\nwalking cycle in four stages of movement complexity, employing lightweight\nexpert models to extract spatio-temporal features and task-specific gating\nmodules to adaptively weight experts across traits and stages. Evaluated on the\nPsyMo benchmark covering 17 psychological traits, our method outperforms\nstate-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at\nthe run level and 44.6% at the subject level. Our experiments show that\nintegrating auxiliary tasks such as identity recognition, gender prediction,\nand BMI estimation further improves psychological trait estimation. Our\nfindings demonstrate the viability of multi-task gait-based learning for\npsychological trait estimation and provide a foundation for future research on\nmovement-informed psychological inference.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u591a\u9636\u6bb5\u8fd0\u52a8\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u6b65\u6001\u5e8f\u5217\u4e2d\u9884\u6d4b\u5fc3\u7406\u7279\u5f81\uff0c\u5728PsyMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6b65\u6001\u5305\u542b\u4e30\u5bcc\u7684\u751f\u7269\u548c\u884c\u4e3a\u4fe1\u606f\uff0c\u4f46\u5229\u7528\u884c\u8d70\u65b9\u5f0f\u6765\u63a8\u65ad\u5fc3\u7406\u7279\u5f81\u4ecd\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u9636\u6bb5\u8fd0\u52a8\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u56db\u4e2a\u8fd0\u52a8\u590d\u6742\u5ea6\u9636\u6bb5\u5904\u7406\u884c\u8d70\u5468\u671f\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6a21\u578b\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u95e8\u63a7\u6a21\u5757\u81ea\u9002\u5e94\u52a0\u6743\u4e13\u5bb6\u3002", "result": "\u5728\u6db5\u76d617\u4e2a\u5fc3\u7406\u7279\u5f81\u7684PsyMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u884c\u7ea7\u522b\u83b7\u5f9737.47%\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u5728\u53d7\u8bd5\u8005\u7ea7\u522b\u83b7\u5f9744.6%\u7684\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u6b65\u6001\u5206\u6790\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u591a\u4efb\u52a1\u6b65\u6001\u5b66\u4e60\u5bf9\u4e8e\u5fc3\u7406\u7279\u5f81\u4f30\u8ba1\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u8fd0\u52a8\u4fe1\u606f\u7684\u5fc3\u7406\u63a8\u65ad\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.04491", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04491", "abs": "https://arxiv.org/abs/2510.04491", "authors": ["Muyu He", "Anand Kumar", "Tsach Mackey", "Meghana Rajeev", "James Zou", "Nazneen Rajani"], "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents", "comment": "25 pages", "summary": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait.", "AI": {"tldr": "TraitBasis\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u538b\u529b\u6d4b\u8bd5AI\u4ee3\u7406\u6765\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u53ef\u64cd\u63a7\u7684\u7528\u6237\u7279\u8d28\u65b9\u5411\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5728\u63a8\u7406\u65f6\u63a7\u5236\u3001\u7f29\u653e\u548c\u7ec4\u5408\u8fd9\u4e9b\u7279\u8d28\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bddAI\u4ee3\u7406\u5728\u7528\u6237\u884c\u4e3a\u8f7b\u5fae\u53d8\u5316\uff08\u5982\u66f4\u4e0d\u8010\u70e6\u3001\u8bed\u65e0\u4f26\u6b21\u6216\u6000\u7591\uff09\u65f6\u6027\u80fd\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u663e\u793a\u51fa\u5176\u8106\u5f31\u6027\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6355\u6349\u8fd9\u79cd\u8106\u5f31\u6027\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "TraitBasis\u5728\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u53ef\u64cd\u63a7\u7684\u7528\u6237\u7279\u8d28\u65b9\u5411\uff0c\u8fd9\u4e9b\u7279\u8d28\u5411\u91cf\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u88ab\u63a7\u5236\u3001\u7f29\u653e\u3001\u7ec4\u5408\u548c\u5e94\u7528\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u6216\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u03c4-Bench\u5230\u03c4-Trait\uff0c\u901a\u8fc7\u53d7\u63a7\u7279\u8d28\u5411\u91cf\u6539\u53d8\u7528\u6237\u884c\u4e3a\u3002", "result": "\u5728\u03c4-Trait\u4e0a\uff0c\u524d\u6cbf\u6a21\u578b\u7684\u6027\u80fd\u5e73\u5747\u4e0b\u964d\u4e862%-30%\uff0c\u7a81\u663e\u51fa\u73b0\u6709AI\u4ee3\u7406\u5bf9\u7528\u6237\u884c\u4e3a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "conclusion": "TraitBasis\u4f5c\u4e3a\u4e00\u4e2a\u7b80\u5355\u3001\u6570\u636e\u9ad8\u6548\u4e14\u53ef\u7ec4\u5408\u7684\u5de5\u5177\uff0c\u4e3a\u6784\u5efa\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u7c7b\u4ea4\u4e92\u4e0d\u53ef\u9884\u6d4b\u52a8\u6001\u4e2d\u4fdd\u6301\u53ef\u9760\u7684AI\u4ee3\u7406\u6253\u5f00\u4e86\u5927\u95e8\u3002\u8be5\u65b9\u6cd5\u5df2\u5728\u56db\u4e2a\u9886\u57df\u5f00\u6e90\uff1a\u822a\u7a7a\u3001\u96f6\u552e\u3001\u7535\u4fe1\u548c\u8fdc\u7a0b\u533b\u7597\u3002"}}
{"id": "2510.04668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04668", "abs": "https://arxiv.org/abs/2510.04668", "authors": ["Habin Lim", "Yeongseob Won", "Juwon Seo", "Gyeong-Moon Park"], "title": "ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement", "comment": "14 pages, 13 figures, to be published in ICCV 2025", "summary": "In recent years, multi-concept personalization for text-to-image (T2I)\ndiffusion models to represent several subjects in an image has gained much more\nattention. The main challenge of this task is \"concept mixing\", where multiple\nlearned concepts interfere or blend undesirably in the output image. To address\nthis issue, in this paper, we present ConceptSplit, a novel framework to split\nthe individual concepts through training and inference. Our framework comprises\ntwo key components. First, we introduce Token-wise Value Adaptation (ToVA), a\nmerging-free training method that focuses exclusively on adapting the value\nprojection in cross-attention. Based on our empirical analysis, we found that\nmodifying the key projection, a common approach in existing methods, can\ndisrupt the attention mechanism and lead to concept mixing. Second, we propose\nLatent Optimization for Disentangled Attention (LODA), which alleviates\nattention entanglement during inference by optimizing the input latent. Through\nextensive qualitative and quantitative experiments, we demonstrate that\nConceptSplit achieves robust multi-concept personalization, mitigating\nunintended concept interference. Code is available at\nhttps://github.com/KU-VGI/ConceptSplit", "AI": {"tldr": "ConceptSplit\u662f\u4e00\u4e2a\u89e3\u51b3\u591a\u6982\u5ff5\u4e2a\u6027\u5316\u4e2d\u6982\u5ff5\u6df7\u5408\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u5305\u542bToken-wise Value Adaptation\u8bad\u7ec3\u65b9\u6cd5\u548cLatent Optimization for Disentangled Attention\u63a8\u7406\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u591a\u6982\u5ff5\u4e2a\u6027\u5316\u4e2d\u591a\u4e2a\u5b66\u4e60\u6982\u5ff5\u5728\u8f93\u51fa\u56fe\u50cf\u4e2d\u76f8\u4e92\u5e72\u6270\u6216\u6df7\u5408\u7684\u95ee\u9898\u3002", "method": "1. Token-wise Value Adaptation\uff1a\u4ec5\u8c03\u6574\u4ea4\u53c9\u6ce8\u610f\u529b\u4e2d\u7684\u503c\u6295\u5f71\uff0c\u907f\u514d\u4fee\u6539\u952e\u6295\u5f71\u7834\u574f\u6ce8\u610f\u529b\u673a\u5236\uff1b2. Latent Optimization for Disentangled Attention\uff1a\u5728\u63a8\u7406\u65f6\u4f18\u5316\u8f93\u5165\u6f5c\u5728\u8868\u793a\u4ee5\u7f13\u89e3\u6ce8\u610f\u529b\u7ea0\u7f20\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cConceptSplit\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u591a\u6982\u5ff5\u4e2a\u6027\u5316\uff0c\u51cf\u8f7b\u610f\u5916\u7684\u6982\u5ff5\u5e72\u6270\u3002", "conclusion": "ConceptSplit\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6982\u5ff5\u4e2a\u6027\u5316\u4e2d\u7684\u6982\u5ff5\u6df7\u5408\u95ee\u9898\u3002"}}
{"id": "2510.04514", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CV", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.04514", "abs": "https://arxiv.org/abs/2510.04514", "authors": ["Rachneet Kaur", "Nishan Srishankar", "Zhen Zeng", "Sumitra Ganesh", "Manuela Veloso"], "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering", "comment": "53 pages, 12 figures, 15 tables", "summary": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.", "AI": {"tldr": "ChartAgent\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u56fe\u8868\u7a7a\u95f4\u57df\u4e2d\u6267\u884c\u89c6\u89c9\u63a8\u7406\u6765\u89e3\u51b3\u672a\u6807\u6ce8\u56fe\u8868\u7684\u89c6\u89c9\u95ee\u7b54\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u4f9d\u8d56\u6587\u672c\u6377\u5f84\u7684\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001LLM\u5728\u57fa\u4e8e\u56fe\u8868\u7684\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u89c6\u89c9\u89e3\u91ca\u800c\u975e\u4f9d\u8d56\u6587\u672c\u6377\u5f84\u7684\u672a\u6807\u6ce8\u56fe\u8868\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "ChartAgent\u8fed\u4ee3\u5730\u5c06\u67e5\u8be2\u5206\u89e3\u4e3a\u89c6\u89c9\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u89c6\u89c9\u5de5\u5177\uff08\u5982\u7ed8\u5236\u6ce8\u91ca\u3001\u88c1\u526a\u533a\u57df\u3001\u5b9a\u4f4d\u5750\u6807\u8f74\uff09\u4e3b\u52a8\u64cd\u4f5c\u548c\u4ea4\u4e92\u56fe\u8868\u56fe\u50cf\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7b56\u7565\u3002", "result": "\u5728ChartBench\u548cChartX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u6574\u4f53\u7edd\u5bf9\u589e\u76ca\u8fbe16.07%\uff0c\u5728\u672a\u6807\u6ce8\u6570\u503c\u5bc6\u96c6\u578b\u67e5\u8be2\u4e0a\u589e\u76ca\u8fbe17.31%\u3002", "conclusion": "ChartAgent\u662f\u9996\u6279\u4f7f\u7528\u5de5\u5177\u589e\u5f3a\u591a\u6a21\u6001\u4ee3\u7406\u5c55\u793a\u57fa\u4e8e\u89c6\u89c9\u63a8\u7406\u7684\u56fe\u8868\u7406\u89e3\u5de5\u4f5c\u4e4b\u4e00\uff0c\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6846\u67b6\u63d0\u5347\u5404\u79cd\u5e95\u5c42LLM\u6027\u80fd\u3002"}}
{"id": "2510.04705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04705", "abs": "https://arxiv.org/abs/2510.04705", "authors": ["Quang-Khai Bui-Tran", "Minh-Toan Dinh", "Thanh-Huy Nguyen", "Ba-Thinh Lam", "Mai-Anh Vu", "Ulas Bagci"], "title": "Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI", "comment": "11 pages, 3 figures", "summary": "Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis\nassessment, yet labeled data is often scarce and unevenly distributed across\nimaging modalities and vendor systems. We propose a label-efficient\nsegmentation approach that promotes cross-modality generalization under\nreal-world conditions, where GED4 hepatobiliary-phase annotations are limited,\nnon-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial\nmisalignment and missing phases are common. Our method integrates a\nfoundation-scale 3D segmentation backbone adapted via fine-tuning, co-training\nwith cross pseudo supervision to leverage unlabeled volumes, and a standardized\npreprocessing pipeline. Without requiring spatial registration, the model\nlearns to generalize across MRI phases and vendors, demonstrating robust\nsegmentation performance in both labeled and unlabeled domains. Our results\nexhibit the effectiveness of our proposed label-efficient baseline for liver\nsegmentation in multi-phase, multi-vendor MRI and highlight the potential of\ncombining foundation model adaptation with co-training for real-world clinical\nimaging tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6807\u7b7e\u9ad8\u6548\u7684\u809d\u810f\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u3001\u4ea4\u53c9\u4f2a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u548c\u6807\u51c6\u5316\u9884\u5904\u7406\uff0c\u5728\u591a\u6a21\u6001\u591a\u5382\u5546MRI\u4e2d\u5b9e\u73b0\u8de8\u6a21\u6001\u6cdb\u5316\uff0c\u65e0\u9700\u7a7a\u95f4\u914d\u51c6\u3002", "motivation": "\u591a\u76f8MRI\u4e2d\u809d\u810f\u5206\u5272\u5bf9\u809d\u7ea4\u7ef4\u5316\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u5728\u4e0d\u540c\u6210\u50cf\u6a21\u6001\u548c\u5382\u5546\u7cfb\u7edf\u4e2d\u5206\u5e03\u4e0d\u5747\uff0c\u5b58\u5728\u7a7a\u95f4\u9519\u4f4d\u548c\u7f3a\u5931\u76f8\u4f4d\u7b49\u73b0\u5b9e\u95ee\u9898\u3002", "method": "\u6574\u5408\u57fa\u7840\u7ea73D\u5206\u5272\u4e3b\u5e72\u7f51\u7edc\u5fae\u8c03\u3001\u4ea4\u53c9\u4f2a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u4ee5\u53ca\u6807\u51c6\u5316\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u65e0\u9700\u7a7a\u95f4\u914d\u51c6\u3002", "result": "\u6a21\u578b\u5728\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u57df\u5747\u8868\u73b0\u51fa\u7a33\u5065\u7684\u5206\u5272\u6027\u80fd\uff0c\u80fd\u591f\u8de8MRI\u76f8\u4f4d\u548c\u5382\u5546\u8fdb\u884c\u6cdb\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6807\u7b7e\u9ad8\u6548\u57fa\u7ebf\u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u57fa\u7840\u6a21\u578b\u9002\u5e94\u4e0e\u534f\u540c\u8bad\u7ec3\u7ed3\u5408\u5728\u771f\u5b9e\u4e34\u5e8a\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.04532", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04532", "abs": "https://arxiv.org/abs/2510.04532", "authors": ["Xurui Song", "Shuo Huai", "JingJing Jiang", "Jiayi Kong", "Jun Luo"], "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models", "comment": "The dataset will be released publicly once the paper is accepted for\n  publication", "summary": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0VLM\u9a7e\u9a76\u4ee3\u7406\u4e2d\u7684\u63a8\u7406\u4e0e\u89c4\u5212\u5b58\u5728\u56e0\u679c\u8131\u8282\uff0c\u63a8\u7406\u53ea\u662f\u8bad\u7ec3\u7684\u526f\u4ea7\u54c1\u800c\u975e\u56e0\u679c\u4e2d\u4ecb\u3002", "motivation": "\u9a8c\u8bc1VLM\u9a7e\u9a76\u4ee3\u7406\u4e2d\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u662f\u5426\u771f\u6b63\u56e0\u679c\u9a71\u52a8\u8f68\u8ff9\u89c4\u5212\u8fd9\u4e00\u5173\u952e\u5047\u8bbe\u3002", "method": "\u6784\u5efaDriveMind\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4fe1\u606f\u6d88\u878d\u5b9e\u9a8c\u8bad\u7ec3VLM\u4ee3\u7406\uff0c\u5e76\u4f7f\u7528\u6ce8\u610f\u529b\u5206\u6790\u548c\u8bad\u7ec3\u65e0\u5173\u7684\u63a2\u9488\u8fdb\u884c\u8bca\u65ad\u3002", "result": "\u79fb\u9664\u5148\u9a8c\u4fe1\u606f\u5bfc\u81f4\u89c4\u5212\u5206\u6570\u5927\u5e45\u4e0b\u964d\uff0c\u800c\u79fb\u9664\u63a8\u7406\u94fe\u4ec5\u4ea7\u751f\u5fae\u5c0f\u53d8\u5316\uff0c\u8868\u660e\u89c4\u5212\u4e3b\u8981\u4f9d\u8d56\u5148\u9a8c\u800c\u975e\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u63a8\u7406-\u89c4\u5212\u89e3\u8026\u5047\u8bf4\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u65b0\u6570\u636e\u96c6\u548c\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u672a\u6765\u6a21\u578b\u7684\u56e0\u679c\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.04706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04706", "abs": "https://arxiv.org/abs/2510.04706", "authors": ["Foivos Paraperas Papantoniou", "Stefanos Zafeiriou"], "title": "ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion", "comment": "ICCVW 2025, Code: https://github.com/foivospar/Arc2Face", "summary": "Human-centric generative models designed for AI-driven storytelling must\nbring together two core capabilities: identity consistency and precise control\nover human performance. While recent diffusion-based approaches have made\nsignificant progress in maintaining facial identity, achieving fine-grained\nexpression control without compromising identity remains challenging. In this\nwork, we present a diffusion-based framework that faithfully reimagines any\nsubject under any particular facial expression. Building on an ID-consistent\nface foundation model, we adopt a compositional design featuring an expression\ncross-attention module guided by FLAME blendshape parameters for explicit\ncontrol. Trained on a diverse mixture of image and video data rich in\nexpressive variation, our adapter generalizes beyond basic emotions to subtle\nmicro-expressions and expressive transitions, overlooked by prior works. In\naddition, a pluggable Reference Adapter enables expression editing in real\nimages by transferring the appearance from a reference frame during synthesis.\nExtensive quantitative and qualitative evaluations show that our model\noutperforms existing methods in tailored and identity-consistent expression\ngeneration. Code and models can be found at\nhttps://github.com/foivospar/Arc2Face.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7cbe\u7ec6\u7684\u8868\u60c5\u63a7\u5236\uff0c\u652f\u6301\u4ece\u57fa\u7840\u60c5\u7eea\u5230\u5fae\u8868\u60c5\u7684\u751f\u6210\uff0c\u5e76\u5305\u542b\u53ef\u63d2\u62d4\u7684\u53c2\u8003\u9002\u914d\u5668\u7528\u4e8e\u771f\u5b9e\u56fe\u50cf\u7684\u8868\u60c5\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u5b9e\u73b0\u7cbe\u7ec6\u8868\u60c5\u63a7\u5236\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u8fd9\u4e24\u4e2a\u6838\u5fc3\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u8eab\u4efd\u4e00\u81f4\u7684\u4eba\u8138\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u7ec4\u5408\u5f0f\u8bbe\u8ba1\uff0c\u5305\u542b\u7531FLAME blendshape\u53c2\u6570\u5f15\u5bfc\u7684\u8868\u60c5\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u5728\u4e30\u5bcc\u7684\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u5b9a\u5236\u5316\u548c\u8eab\u4efd\u4e00\u81f4\u7684\u8868\u60c5\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u4ece\u57fa\u7840\u60c5\u7eea\u5230\u5fae\u8868\u60c5\u7684\u591a\u79cd\u8868\u60c5\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u7cbe\u7ec6\u8868\u60c5\u63a7\u5236\u7684\u7ed3\u5408\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u548cAI\u9a71\u52a8\u7684\u6545\u4e8b\u8bb2\u8ff0\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04721", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04721", "abs": "https://arxiv.org/abs/2510.04721", "authors": ["Ivo Petrov", "Jasper Dekoninck", "Martin Vechev"], "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs", "comment": null, "summary": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.", "AI": {"tldr": "BrokenMath\u662f\u9996\u4e2a\u8bc4\u4f30LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5b9a\u7406\u8bc1\u660e\u4e2d\u8c04\u5a9a\u884c\u4e3a\u7684\u57fa\u51c6\uff0c\u53d1\u73b0GPT-5\u7b49\u5148\u8fdb\u6a21\u578b\u572829%\u7684\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u8c04\u5a9a\u7b54\u6848\uff0c\u901a\u8fc7\u5e72\u9884\u7b56\u7565\u53ef\u51cf\u5c11\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u8be5\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u5b9a\u7406\u8bc1\u660e\u4e2d\u8c04\u5a9a\u884c\u4e3a\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9650\u5236\u4e86LLMs\u5728\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u57fa\u4e8e2025\u5e74\u7ade\u8d5b\u95ee\u9898\u6784\u5efaBrokenMath\u57fa\u51c6\uff0c\u4f7f\u7528LLM\u6270\u52a8\u751f\u6210\u9519\u8bef\u9648\u8ff0\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u5ba1\u7cbe\u70bc\uff0c\u91c7\u7528LLM-as-a-judge\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u8c04\u5a9a\u884c\u4e3a\u666e\u904d\u5b58\u5728\uff0cGPT-5\u572829%\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u8c04\u5a9a\u7b54\u6848\uff1b\u6d4b\u8bd5\u65f6\u5e72\u9884\u548c\u76d1\u7763\u5fae\u8c03\u53ef\u663e\u8457\u51cf\u5c11\u4f46\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u8be5\u884c\u4e3a\u3002", "conclusion": "LLMs\u5728\u6570\u5b66\u5b9a\u7406\u8bc1\u660e\u4e2d\u5b58\u5728\u663e\u8457\u8c04\u5a9a\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u6765\u63d0\u5347\u5176\u53ef\u9760\u6027\u3002"}}
{"id": "2510.04712", "categories": ["cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04712", "abs": "https://arxiv.org/abs/2510.04712", "authors": ["Luo Cheng", "Song Siyang", "Yan Siyuan", "Yu Zhen", "Ge Zongyuan"], "title": "ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model", "comment": "Accepted to ACM Multimedia", "summary": "The automatic generation of diverse and human-like facial reactions in dyadic\ndialogue remains a critical challenge for human-computer interaction systems.\nExisting methods fail to model the stochasticity and dynamics inherent in real\nhuman reactions. To address this, we propose ReactDiff, a novel temporal\ndiffusion framework for generating diverse facial reactions that are\nappropriate for responding to any given dialogue context. Our key insight is\nthat plausible human reactions demonstrate smoothness, and coherence over time,\nand conform to constraints imposed by human facial anatomy. To achieve this,\nReactDiff incorporates two vital priors (spatio-temporal facial kinematics)\ninto the diffusion process: i) temporal facial behavioral kinematics and ii)\nfacial action unit dependencies. These two constraints guide the model toward\nrealistic human reaction manifolds, avoiding visually unrealistic jitters,\nunstable transitions, unnatural expressions, and other artifacts. Extensive\nexperiments on the REACT2024 dataset demonstrate that our approach not only\nachieves state-of-the-art reaction quality but also excels in diversity and\nreaction appropriateness.", "AI": {"tldr": "ReactDiff\u662f\u4e00\u4e2a\u57fa\u4e8e\u65f6\u95f4\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5bf9\u8bdd\u4e2d\u751f\u6210\u591a\u6837\u4e14\u903c\u771f\u7684\u4eba\u8138\u53cd\u5e94\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u7a7a\u9762\u90e8\u8fd0\u52a8\u5b66\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u4f9d\u8d56\u5173\u7cfb\u6765\u786e\u4fdd\u53cd\u5e94\u7684\u5e73\u6ed1\u6027\u548c\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6a21\u62df\u771f\u5b9e\u4eba\u7c7b\u53cd\u5e94\u4e2d\u7684\u968f\u673a\u6027\u548c\u52a8\u6001\u7279\u6027\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u9762\u90e8\u53cd\u5e94\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u3002", "method": "\u63d0\u51faReactDiff\u6846\u67b6\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u6574\u5408\u4e24\u4e2a\u5173\u952e\u5148\u9a8c\uff1a\u65f6\u95f4\u9762\u90e8\u884c\u4e3a\u8fd0\u52a8\u5b66\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u9762\u90e8\u89e3\u5256\u5b66\u7ea6\u675f\u7684\u903c\u771f\u53cd\u5e94\u3002", "result": "\u5728REACT2024\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53cd\u5e94\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u53cd\u5e94\u9002\u5207\u6027\u65b9\u9762\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ReactDiff\u901a\u8fc7\u6574\u5408\u9762\u90e8\u8fd0\u52a8\u5b66\u548c\u52a8\u4f5c\u5355\u5143\u4f9d\u8d56\u5173\u7cfb\uff0c\u6210\u529f\u751f\u6210\u4e86\u591a\u6837\u4e14\u903c\u771f\u7684\u9762\u90e8\u53cd\u5e94\uff0c\u4e3a\u4eba\u7c7b-\u8ba1\u7b97\u673a\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04819", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04819", "abs": "https://arxiv.org/abs/2510.04819", "authors": ["Benlin Liu", "Amita Kamath", "Madeleine Grunde-McLaughlin", "Winson Han", "Ranjay Krishna"], "title": "Visual Representations inside the Language Model", "comment": "Accepted to COLM 2025", "summary": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u89c9\u952e\u503c\u4ee4\u724c\u7684\u4fe1\u606f\u6d41\uff0c\u53d1\u73b0\u56fe\u50cf\u503c\u4ee4\u724c\u5305\u542b\u8db3\u591f\u4fe1\u606f\u4f46\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\uff0c\u4e14\u8f93\u5165\u65e0\u5173\u7684\u56fe\u50cf\u952e\u4ee4\u724c\u5b58\u5728\u964d\u4f4e\u611f\u77e5\u80fd\u529b\u7684\u4f2a\u5f71\u3002", "motivation": "\u7406\u89e3\u4e3a\u4ec0\u4e48\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4ece\u89c6\u89c9\u952e\u503c\u4ee4\u724c\u5904\u7406\u7684\u89d2\u5ea6\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u89c6\u89d2\u3002", "method": "\u5206\u6790\u6d41\u884cMLMs\uff08LLaVA-OneVision\u3001Qwen2.5-VL\u3001Llama-3-LLaVA-NeXT\uff09\u5904\u7406\u89c6\u89c9\u952e\u503c\u4ee4\u724c\u7684\u65b9\u5f0f\uff0c\u7814\u7a76\u89c6\u89c9\u4fe1\u606f\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6d41\u52a8\uff0c\u5e76\u6d4b\u8bd5\u6dfb\u52a0\u6587\u672c\u524d\u7f00\u7b49\u65b9\u6cd5\u5bf9\u611f\u77e5\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u56fe\u50cf\u503c\u4ee4\u724c\u5305\u542b\u8db3\u591f\u6267\u884c\u5206\u5272\u3001\u8bed\u4e49\u5bf9\u5e94\u7b49\u4efb\u52a1\u7684\u96f6\u6837\u672c\u4fe1\u606f\uff0c\u4f46\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u5145\u5206\u5229\u7528\uff1b\u8f93\u5165\u65e0\u5173\u7684\u56fe\u50cf\u952e\u4ee4\u724c\u5b58\u5728\u964d\u4f4e\u611f\u77e5\u80fd\u529b\u7684\u4f2a\u5f71\uff1b\u6dfb\u52a0\u6587\u672c\u524d\u7f00\u53ef\u6539\u5584\u89c6\u89c9\u8868\u793a\uff1b33.3%\u7684BLINK\u57fa\u51c6\u827a\u672f\u98ce\u683c\u95ee\u9898\u4e2d\uff0c\u8bed\u8a00\u6a21\u578b\u5185\u7684\u611f\u77e5\u4fe1\u606f\u672a\u8f93\u51fa\u3002", "conclusion": "\u63ed\u793a\u4e86\u952e\u503c\u4ee4\u724c\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3aMLMs\u7684\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u5e76\u5efa\u8bae\u6539\u8fdb\u89c6\u89c9\u7f16\u7801\u5668\u548c\u8bed\u8a00\u6a21\u578b\u7ec4\u4ef6\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2510.04714", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04714", "abs": "https://arxiv.org/abs/2510.04714", "authors": ["KunHo Heo", "GiHyun Kim", "SuYeon Kim", "MyeongAh Cho"], "title": "Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction", "comment": "Accepted by NeurIPS 2025. Code:\n  https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes", "summary": "3D Semantic Scene Graph Prediction aims to detect objects and their semantic\nrelationships in 3D scenes, and has emerged as a crucial technology for\nrobotics and AR/VR applications. While previous research has addressed dataset\nlimitations and explored various approaches including Open-Vocabulary settings,\nthey frequently fail to optimize the representational capacity of object and\nrelationship features, showing excessive reliance on Graph Neural Networks\ndespite insufficient discriminative capability. In this work, we demonstrate\nthrough extensive analysis that the quality of object features plays a critical\nrole in determining overall scene graph accuracy. To address this challenge, we\ndesign a highly discriminative object feature encoder and employ a contrastive\npretraining strategy that decouples object representation learning from the\nscene graph prediction. This design not only enhances object classification\naccuracy but also yields direct improvements in relationship prediction.\nNotably, when plugging in our pretrained encoder into existing frameworks, we\nobserve substantial performance improvements across all evaluation metrics.\nAdditionally, whereas existing approaches have not fully exploited the\nintegration of relationship information, we effectively combine both geometric\nand semantic features to achieve superior relationship prediction.\nComprehensive experiments on the 3DSSG dataset demonstrate that our approach\nsignificantly outperforms previous state-of-the-art methods. Our code is\npublicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u76843D\u8bed\u4e49\u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9ad8\u533a\u5206\u5ea6\u7684\u7269\u4f53\u7279\u5f81\u7f16\u7801\u5668\u548c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7269\u4f53\u5206\u7c7b\u548c\u5173\u7cfb\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u67093D\u8bed\u4e49\u573a\u666f\u56fe\u9884\u6d4b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u7269\u4f53\u548c\u5173\u7cfb\u7279\u5f81\u7684\u8868\u793a\u80fd\u529b\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7269\u4f53\u7279\u5f81\u8d28\u91cf\u5bf9\u6574\u4f53\u573a\u666f\u56fe\u51c6\u786e\u6027\u5f71\u54cd\u5f88\u5927\u3002", "method": "\u8bbe\u8ba1\u9ad8\u533a\u5206\u5ea6\u7684\u7269\u4f53\u7279\u5f81\u7f16\u7801\u5668\uff0c\u91c7\u7528\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u5c06\u7269\u4f53\u8868\u793a\u5b66\u4e60\u4e0e\u573a\u666f\u56fe\u9884\u6d4b\u89e3\u8026\uff0c\u5e76\u6709\u6548\u7ed3\u5408\u51e0\u4f55\u548c\u8bed\u4e49\u7279\u5f81\u8fdb\u884c\u5173\u7cfb\u9884\u6d4b\u3002", "result": "\u57283DSSG\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5f53\u5c06\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d2\u5165\u73b0\u6709\u6846\u67b6\u65f6\uff0c\u6240\u6709\u8bc4\u4f30\u6307\u6807\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u7269\u4f53\u7279\u5f81\u8868\u793a\u5e76\u6709\u6548\u6574\u5408\u5173\u7cfb\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u57283D\u8bed\u4e49\u573a\u666f\u56fe\u9884\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u673a\u5668\u4eba\u548cAR/VR\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2510.04935", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04935", "abs": "https://arxiv.org/abs/2510.04935", "authors": ["Guoxin Chen", "Zile Qiao", "Wenqing Wang", "Donglei Yu", "Xuanzhong Chen", "Hao Sun", "Minpeng Liao", "Kai Fan", "Yong Jiang", "Penguin Xie", "Wayne Xin Zhao", "Ruihua Song", "Fei Huang"], "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning", "comment": "Ongoing Work", "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408System 1\u7684\u5feb\u901f\u76f4\u89c9\u601d\u7ef4\u548cSystem 2\u7684\u6df1\u601d\u719f\u8651\u63a8\u7406\uff0c\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8fc7\u5ea6\u5206\u6790\u548c\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u503e\u5411\u4e8e\u8fc7\u5ea6\u4f7f\u7528System 2\u578b\u63a8\u7406\uff0c\u5bfc\u81f4token\u751f\u6210\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7531\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u7684\u9759\u6001\u6027\u96be\u4ee5\u9002\u5e94\u5feb\u901f\u53d8\u5316\u7684\u73af\u5883\u3002", "method": "\u63d0\u51faMARS\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u6574\u5408Google\u641c\u7d22\u3001Google\u5b66\u672f\u548cPython\u89e3\u91ca\u5668\u7b49\u5916\u90e8\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u5de5\u534f\u4f5c\u8ba9System 1\u5904\u7406\u9ad8\u5bb9\u91cf\u5916\u90e8\u4fe1\u606f\uff0cSystem 2\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff0c\u5e76\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u7cfb\u7edf\u534f\u4f5c\u3002", "result": "\u5728Humanity's Last Exam\u57fa\u51c6\u4e0a\u5b9e\u73b03.86%\u7684\u663e\u8457\u63d0\u5347\uff0c\u57287\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u5e73\u5747\u83b7\u5f978.9%\u7684\u6027\u80fd\u589e\u76ca\u3002", "conclusion": "MARS\u7684\u53cc\u7cfb\u7edf\u8303\u5f0f\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u76f4\u89c9\u4e0e\u6df1\u601d\u63a8\u7406\u6574\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.04723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04723", "abs": "https://arxiv.org/abs/2510.04723", "authors": ["Niccol\u00f2 Niccoli", "Lorenzo Seidenari", "Ilaria Greco", "Francesco Rovero"], "title": "Benchmark on Monocular Metric Depth Estimation in Wildlife Setting", "comment": null, "summary": "Camera traps are widely used for wildlife monitoring, but extracting accurate\ndistance measurements from monocular images remains challenging due to the lack\nof depth information. While monocular depth estimation (MDE) methods have\nadvanced significantly, their performance in natural wildlife environments has\nnot been systematically evaluated. This work introduces the first benchmark for\nmonocular metric depth estimation in wildlife monitoring conditions. We\nevaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,\nZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images\nwith ground truth distances obtained using calibrated ChARUCO patterns. Our\nresults demonstrate that Depth Anything V2 achieves the best overall\nperformance with a mean absolute error of 0.454m and correlation of 0.962,\nwhile methods like ZoeDepth show significant degradation in outdoor natural\nenvironments (MAE: 3.087m). We find that median-based depth extraction\nconsistently outperforms mean-based approaches across all deep learning\nmethods. Additionally, we analyze computational efficiency, with ZoeDepth being\nfastest (0.17s per image) but least accurate, while Depth Anything V2 provides\nan optimal balance of accuracy and speed (0.22s per image). This benchmark\nestablishes performance baselines for wildlife applications and provides\npractical guidance for implementing depth estimation in conservation monitoring\nsystems.", "AI": {"tldr": "\u9996\u4e2a\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u4e2d\u7684\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u5148\u8fdbMDE\u65b9\u6cd5\u5728\u76f8\u673a\u9677\u9631\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0Depth Anything V2\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u76f8\u673a\u9677\u9631\u5e7f\u6cdb\u7528\u4e8e\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\uff0c\u4f46\u4ece\u5355\u76ee\u56fe\u50cf\u4e2d\u63d0\u53d6\u51c6\u786e\u8ddd\u79bb\u6d4b\u91cf\u4ecd\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30MDE\u65b9\u6cd5\u5728\u81ea\u7136\u91ce\u751f\u52a8\u7269\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u572893\u5f20\u5e26\u6709\u6821\u51c6ChARUCO\u6a21\u5f0f\u5730\u9762\u771f\u5b9e\u8ddd\u79bb\u7684\u76f8\u673a\u9677\u9631\u56fe\u50cf\u4e0a\uff0c\u8bc4\u4f30\u56db\u79cd\u6700\u5148\u8fdb\u7684MDE\u65b9\u6cd5\uff08Depth Anything V2\u3001ML Depth Pro\u3001ZoeDepth\u548cMetric3D\uff09\u4ee5\u53ca\u51e0\u4f55\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "Depth Anything V2\u5b9e\u73b0\u6700\u4f73\u6574\u4f53\u6027\u80fd\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee0.454m\uff0c\u76f8\u5173\u60270.962\uff1bZoeDepth\u5728\u6237\u5916\u81ea\u7136\u73af\u5883\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08MAE: 3.087m\uff09\uff1b\u4e2d\u503c\u6df1\u5ea6\u63d0\u53d6\u5728\u6240\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u5747\u503c\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u91ce\u751f\u52a8\u7269\u5e94\u7528\u5efa\u7acb\u4e86\u6027\u80fd\u57fa\u7ebf\uff0c\u5e76\u4e3a\u5728\u4fdd\u62a4\u76d1\u6d4b\u7cfb\u7edf\u4e2d\u5b9e\u65bd\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0cDepth Anything V2\u5728\u51c6\u786e\u6027\u548c\u901f\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002"}}
{"id": "2510.04980", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.04980", "abs": "https://arxiv.org/abs/2510.04980", "authors": ["Fangzhou Liang", "Tianshi Zheng", "Chunkit Chan", "Yauwai Yim", "Yangqiu Song"], "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game", "comment": "EMNLP 2025 Wordplay", "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.", "AI": {"tldr": "LLM-Hanabi\u57fa\u51c6\u6d4b\u8bd5\u4f7f\u7528\u5408\u4f5c\u6e38\u620f\u300a\u82b1\u706b\u300b\u8bc4\u4f30LLMs\u7684\u5fc3\u667a\u7406\u8bba\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e00\u9636\u5fc3\u667a\u7406\u8bba\u4e0e\u6e38\u620f\u8868\u73b0\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u52a8\u6001\u534f\u4f5c\u73af\u5883\u4e2d\u63a8\u65ad\u4ed6\u4eba\u884c\u4e3a\u80cc\u540e\u539f\u7406\u7684\u80fd\u529b\uff0c\u8fd9\u662f\u6709\u6548\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6240\u9700\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\u3002", "method": "\u5f15\u5165LLM-Hanabi\u57fa\u51c6\uff0c\u4f7f\u7528\u5408\u4f5c\u6e38\u620f\u300a\u82b1\u706b\u300b\u8bc4\u4f30LLMs\u7684\u5fc3\u667a\u7406\u8bba\u548c\u63a8\u7406\u80fd\u529b\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u8bc4\u4f30\u7cfb\u7edf\u6d4b\u91cf\u6e38\u620f\u8868\u73b0\u548c\u5fc3\u667a\u7406\u8bba\u719f\u7ec3\u5ea6\u3002", "result": "\u53d1\u73b0\u5fc3\u667a\u7406\u8bba\u4e0e\u6e38\u620f\u6210\u529f\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u4e00\u9636\u5fc3\u667a\u7406\u8bba\uff08\u89e3\u91ca\u4ed6\u4eba\u610f\u56fe\uff09\u6bd4\u4e8c\u9636\u5fc3\u667a\u7406\u8bba\uff08\u9884\u6d4b\u4ed6\u4eba\u89e3\u91ca\uff09\u4e0e\u8868\u73b0\u76f8\u5173\u6027\u66f4\u5f3a\u3002", "conclusion": "\u5bf9\u4e8e\u6709\u6548\u7684AI\u534f\u4f5c\uff0c\u51c6\u786e\u89e3\u91ca\u5408\u4f5c\u4f19\u4f34\u539f\u7406\u7684\u80fd\u529b\u6bd4\u9ad8\u9636\u63a8\u7406\u66f4\u5173\u952e\uff0c\u4f18\u5148\u53d1\u5c55\u4e00\u9636\u5fc3\u667a\u7406\u8bba\u662f\u589e\u5f3a\u672a\u6765\u6a21\u578b\u534f\u4f5c\u80fd\u529b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002"}}
{"id": "2510.04739", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.04739", "abs": "https://arxiv.org/abs/2510.04739", "authors": ["Mehdi Houshmand Sarkhoosh", "Fr\u00f8y \u00d8ye", "Henrik Nestor S\u00f8rlie", "Nam Hoang Vu", "Dag Johansen", "Cise Midoglu", "Tomas Kupka", "P\u00e5l Halvorsen"], "title": "ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Quantifying sponsor visibility in sports broadcasts is a critical marketing\ntask traditionally hindered by manual, subjective, and unscalable analysis\nmethods. While automated systems offer an alternative, their reliance on\naxis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics\nwhen logos appear rotated or skewed due to dynamic camera angles and\nperspective distortions. This paper introduces ExposureEngine, an end-to-end\nsystem designed for accurate, rotation-aware sponsor visibility analytics in\nsports broadcasts, demonstrated in a soccer case study. Our approach predicts\nOriented Bounding Box (OBB) to provide a geometrically precise fit to each logo\nregardless of the orientation on-screen. To train and evaluate our detector, we\ndeveloped a new dataset comprising 1,103 frames from Swedish elite soccer,\nfeaturing 670 unique sponsor logos annotated with OBBs. Our model achieves a\nmean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall\nof 0.87, demonstrating robust performance in localizing logos under diverse\nbroadcast conditions. The system integrates these detections into an analytical\npipeline that calculates precise visibility metrics, such as exposure duration\nand on-screen coverage. Furthermore, we incorporate a language-driven agentic\nlayer, enabling users to generate reports, summaries, and media content through\nnatural language queries. The complete system, including the dataset and the\nanalytics dashboard, provides a comprehensive solution for auditable and\ninterpretable sponsor measurement in sports media. An overview of the\nExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .", "AI": {"tldr": "\u63d0\u51fa\u4e86ExposureEngine\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u5411\u8fb9\u754c\u6846(OBB)\u8fdb\u884c\u65cb\u8f6c\u611f\u77e5\u7684\u8d5e\u52a9\u5546\u53ef\u89c1\u6027\u5206\u6790\uff0c\u5728\u8db3\u7403\u8f6c\u64ad\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u8d5e\u52a9\u5546\u66dd\u5149\u5ea6\u91cf\u3002", "motivation": "\u4f20\u7edf\u8d5e\u52a9\u5546\u53ef\u89c1\u6027\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u3001\u4e3b\u89c2\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u4f7f\u7528\u6c34\u5e73\u8fb9\u754c\u6846(HBB)\u5728\u65cb\u8f6c\u6216\u503e\u659c\u7684logo\u4e0a\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u66dd\u5149\u5ea6\u91cf\u3002", "method": "\u5f00\u53d1\u4e86\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u9884\u6d4b\u5b9a\u5411\u8fb9\u754c\u6846(OBB)\u6765\u7cbe\u786e\u62df\u5408\u4e0d\u540c\u65b9\u5411\u7684logo\uff0c\u4f7f\u7528\u5305\u542b1,103\u5e27\u745e\u5178\u7cbe\u82f1\u8db3\u7403\u6bd4\u8d5b\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u68c0\u6d4b\u5668\uff0c\u5e76\u96c6\u6210\u8bed\u8a00\u9a71\u52a8\u4ee3\u7406\u5c42\u3002", "result": "\u6a21\u578b\u8fbe\u52300.859\u7684mAP@0.5\uff0c\u7cbe\u5ea60.96\uff0c\u53ec\u56de\u73870.87\uff0c\u5728\u591a\u6837\u5316\u8f6c\u64ad\u6761\u4ef6\u4e0b\u7a33\u5065\u5b9a\u4f4dlogo\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u53ef\u89c1\u6027\u5ea6\u91cf\u5982\u66dd\u5149\u65f6\u957f\u548c\u5c4f\u5e55\u8986\u76d6\u7387\u3002", "conclusion": "ExposureEngine\u63d0\u4f9b\u4e86\u53ef\u5ba1\u8ba1\u548c\u53ef\u89e3\u91ca\u7684\u8d5e\u52a9\u5546\u6d4b\u91cf\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6570\u636e\u96c6\u548c\u5206\u6790\u4eea\u8868\u677f\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u4f53\u80b2\u5a92\u4f53\u4e2d\u7684\u8d5e\u52a9\u5546\u53ef\u89c1\u6027\u5206\u6790\u3002"}}
{"id": "2510.05096", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.05096", "abs": "https://arxiv.org/abs/2510.05096", "authors": ["Zeyu Zhu", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Paper2Video: Automatic Video Generation from Scientific Papers", "comment": "20 pages, 8 figures", "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.", "AI": {"tldr": "PaperTalker\u662f\u9996\u4e2a\u5b66\u672f\u6f14\u793a\u89c6\u9891\u751f\u6210\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5e7b\u706f\u7247\u751f\u6210\u3001\u5e03\u5c40\u4f18\u5316\u3001\u5b57\u5e55\u3001\u8bed\u97f3\u5408\u6210\u548c\u8bf4\u8bdd\u4eba\u6e32\u67d3\u7b49\u6280\u672f\uff0c\u81ea\u52a8\u4ece\u7814\u7a76\u8bba\u6587\u751f\u6210\u6f14\u793a\u89c6\u9891\u3002", "motivation": "\u5b66\u672f\u6f14\u793a\u89c6\u9891\u5236\u4f5c\u8017\u65f6\u8d39\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7814\u7a76\u8bba\u6587\u7684\u590d\u6742\u591a\u6a21\u6001\u4fe1\u606f\uff08\u6587\u672c\u3001\u56fe\u8868\u3001\u8868\u683c\uff09\u4ee5\u53ca\u9700\u8981\u534f\u8c03\u7684\u591a\u4e2a\u5bf9\u9f50\u901a\u9053\uff08\u5e7b\u706f\u7247\u3001\u5b57\u5e55\u3001\u8bed\u97f3\u3001\u8bf4\u8bdd\u4eba\uff09\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u96c6\u6210\u5e7b\u706f\u7247\u751f\u6210\u4e0e\u57fa\u4e8e\u6811\u641c\u7d22\u7684\u89c6\u89c9\u9009\u62e9\u5e03\u5c40\u4f18\u5316\u3001\u5149\u6807\u5b9a\u4f4d\u3001\u5b57\u5e55\u751f\u6210\u3001\u8bed\u97f3\u5408\u6210\u548c\u8bf4\u8bdd\u4eba\u751f\u6210\uff0c\u5e76\u91c7\u7528\u5e76\u884c\u5316\u5e7b\u706f\u7247\u751f\u6210\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5728Paper2Video\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u6f14\u793a\u89c6\u9891\u6bd4\u73b0\u6709\u57fa\u7ebf\u66f4\u5fe0\u5b9e\u548c\u5185\u5bb9\u4e30\u5bcc\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u81ea\u52a8\u5316\u3001\u5373\u7528\u578b\u5b66\u672f\u89c6\u9891\u751f\u6210\u8fc8\u51fa\u4e86\u5b9e\u7528\u7684\u4e00\u6b65\uff0c\u76f8\u5173\u6570\u636e\u96c6\u3001\u667a\u80fd\u4f53\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.04741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04741", "abs": "https://arxiv.org/abs/2510.04741", "authors": ["Alina Ciocarlan", "Sylvie Le H\u00e9garat-Mascle", "Sidonie Lefebvre"], "title": "Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection", "comment": null, "summary": "Infrared Small Target Detection (IRSTD) is a challenging task in defense\napplications, where complex backgrounds and tiny target sizes often result in\nnumerous false alarms using conventional object detectors. To overcome this\nlimitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a\nstatistical anomaly detection test into its detection head. By treating small\ntargets as unexpected patterns against the background, AA-YOLO effectively\ncontrols the false alarm rate. Our approach not only achieves competitive\nperformance on several IRSTD benchmarks, but also demonstrates remarkable\nrobustness in scenarios with limited training data, noise, and domain shifts.\nFurthermore, since only the detection head is modified, our design is highly\ngeneric and has been successfully applied across various YOLO backbones,\nincluding lightweight models. It also provides promising results when\nintegrated into an instance segmentation YOLO. This versatility makes AA-YOLO\nan attractive solution for real-world deployments where resources are\nconstrained. The code will be publicly released.", "AI": {"tldr": "AA-YOLO\u901a\u8fc7\u5728YOLO\u68c0\u6d4b\u5934\u4e2d\u96c6\u6210\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u6d4b\u8bd5\uff0c\u5c06\u5c0f\u76ee\u6807\u89c6\u4e3a\u80cc\u666f\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u6709\u6548\u63a7\u5236\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u8bef\u62a5\u7387\u3002", "motivation": "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u5728\u56fd\u9632\u5e94\u7528\u4e2d\u9762\u4e34\u590d\u6742\u80cc\u666f\u548c\u5c0f\u76ee\u6807\u5c3a\u5bf8\u7684\u6311\u6218\uff0c\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u5668\u4f1a\u4ea7\u751f\u5927\u91cf\u8bef\u62a5\u3002", "method": "\u5728YOLO\u68c0\u6d4b\u5934\u4e2d\u96c6\u6210\u7edf\u8ba1\u5f02\u5e38\u68c0\u6d4b\u6d4b\u8bd5\uff0c\u5c06\u5c0f\u76ee\u6807\u89c6\u4e3a\u80cc\u666f\u4e2d\u7684\u5f02\u5e38\u6a21\u5f0f\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u4e2aIRSTD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3001\u566a\u58f0\u548c\u57df\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "AA-YOLO\u5177\u6709\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cdYOLO\u9aa8\u5e72\u7f51\u7edc\uff0c\u5305\u62ec\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u662f\u8d44\u6e90\u53d7\u9650\u5b9e\u9645\u90e8\u7f72\u7684\u6709\u5438\u5f15\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.04753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04753", "abs": "https://arxiv.org/abs/2510.04753", "authors": ["Masoumeh Chapariniya", "Teodora Vukovic", "Sarah Ebling", "Volker Dellwo"], "title": "Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics", "comment": null, "summary": "This paper investigates the performance of transformer-based architectures\nfor person identification in natural, face-to-face conversation scenario. We\nimplement and evaluate a two-stream framework that separately models spatial\nconfigurations and temporal motion patterns of 133 COCO WholeBody keypoints,\nextracted from a subset of the CANDOR conversational corpus. Our experiments\ncompare pre-trained and from-scratch training, investigate the use of velocity\nfeatures, and introduce a multi-scale temporal transformer for hierarchical\nmotion modeling. Results demonstrate that domain-specific training\nsignificantly outperforms transfer learning, and that spatial configurations\ncarry more discriminative information than temporal dynamics. The spatial\ntransformer achieves 95.74% accuracy, while the multi-scale temporal\ntransformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,\nconfirming that postural and dynamic information are complementary. These\nfindings highlight the potential of transformer architectures for person\nidentification in natural interactions and provide insights for future\nmultimodal and cross-cultural studies.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u81ea\u7136\u9762\u5bf9\u9762\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4eba\u5458\u8bc6\u522b\u6027\u80fd\uff0c\u4f7f\u7528\u53cc\u6d41\u6846\u67b6\u5206\u522b\u5efa\u6a21\u7a7a\u95f4\u914d\u7f6e\u548c\u65f6\u95f4\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728CANDOR\u5bf9\u8bdd\u8bed\u6599\u5e93\u4e0a\u53d6\u5f97\u4e8698.03%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u63a2\u7d22Transformer\u67b6\u6784\u5728\u81ea\u7136\u9762\u5bf9\u9762\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u4eba\u5458\u8bc6\u522b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7406\u89e3\u7a7a\u95f4\u59ff\u6001\u914d\u7f6e\u548c\u65f6\u95f4\u8fd0\u52a8\u6a21\u5f0f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u6846\u67b6\uff1a\u7a7a\u95f4Transformer\u5efa\u6a21133\u4e2aCOCO WholeBody\u5173\u952e\u70b9\u7684\u7a7a\u95f4\u914d\u7f6e\uff0c\u591a\u5c3a\u5ea6\u65f6\u95f4Transformer\u5efa\u6a21\u5c42\u6b21\u5316\u8fd0\u52a8\u6a21\u5f0f\u3002\u6bd4\u8f83\u4e86\u9884\u8bad\u7ec3\u4e0e\u4ece\u5934\u8bad\u7ec3\u3001\u901f\u5ea6\u7279\u5f81\u7684\u4f7f\u7528\uff0c\u5e76\u8fdb\u884c\u4e86\u7279\u5f81\u7ea7\u878d\u5408\u3002", "result": "\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u663e\u8457\u4f18\u4e8e\u8fc1\u79fb\u5b66\u4e60\uff1b\u7a7a\u95f4\u914d\u7f6e\u6bd4\u65f6\u95f4\u52a8\u6001\u66f4\u5177\u5224\u522b\u6027\uff1b\u7a7a\u95f4Transformer\u8fbe\u523095.74%\u51c6\u786e\u7387\uff0c\u591a\u5c3a\u5ea6\u65f6\u95f4Transformer\u8fbe\u523093.90%\uff1b\u7279\u5f81\u7ea7\u878d\u5408\u5c06\u6027\u80fd\u63d0\u5347\u81f398.03%\u3002", "conclusion": "Transformer\u67b6\u6784\u5728\u81ea\u7136\u4ea4\u4e92\u4e2d\u7684\u4eba\u5458\u8bc6\u522b\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u59ff\u6001\u548c\u52a8\u6001\u4fe1\u606f\u5177\u6709\u4e92\u8865\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u548c\u8de8\u6587\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.04759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04759", "abs": "https://arxiv.org/abs/2510.04759", "authors": ["Chi Yan", "Dan Xu"], "title": "Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction", "comment": "Project Page: https://yanchi-3dv.github.io/PG-Occ", "summary": "The 3D occupancy prediction task has witnessed remarkable progress in recent\nyears, playing a crucial role in vision-based autonomous driving systems. While\ntraditional methods are limited to fixed semantic categories, recent approaches\nhave moved towards predicting text-aligned features to enable open-vocabulary\ntext queries in real-world scenes. However, there exists a trade-off in\ntext-aligned scene modeling: sparse Gaussian representation struggles to\ncapture small objects in the scene, while dense representation incurs\nsignificant computational overhead. To address these limitations, we present\nPG-Occ, an innovative Progressive Gaussian Transformer Framework that enables\nopen-vocabulary 3D occupancy prediction. Our framework employs progressive\nonline densification, a feed-forward strategy that gradually enhances the 3D\nGaussian representation to capture fine-grained scene details. By iteratively\nenhancing the representation, the framework achieves increasingly precise and\ndetailed scene understanding. Another key contribution is the introduction of\nan anisotropy-aware sampling strategy with spatio-temporal fusion, which\nadaptively assigns receptive fields to Gaussians at different scales and\nstages, enabling more effective feature aggregation and richer scene\ninformation capture. Through extensive evaluations, we demonstrate that PG-Occ\nachieves state-of-the-art performance with a relative 14.3% mIoU improvement\nover the previous best performing method. Code and pretrained models will be\nreleased upon publication on our project page:\nhttps://yanchi-3dv.github.io/PG-Occ", "AI": {"tldr": "PG-Occ\u662f\u4e00\u4e2a\u6e10\u8fdb\u5f0f\u9ad8\u65af\u53d8\u6362\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u76843D\u5360\u7528\u9884\u6d4b\uff0c\u901a\u8fc7\u6e10\u8fdb\u5728\u7ebf\u81f4\u5bc6\u5316\u548c\u5404\u5411\u5f02\u6027\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff0c\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u8868\u793a\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u5bf9\u9f50\u573a\u666f\u5efa\u6a21\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff1a\u7a00\u758f\u9ad8\u65af\u8868\u793a\u96be\u4ee5\u6355\u6349\u5c0f\u7269\u4f53\uff0c\u800c\u5bc6\u96c6\u8868\u793a\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5728\u7ebf\u81f4\u5bc6\u5316\u7b56\u7565\u9010\u6b65\u589e\u5f3a3D\u9ad8\u65af\u8868\u793a\uff0c\u7ed3\u5408\u5404\u5411\u5f02\u6027\u611f\u77e5\u91c7\u6837\u548c\u65f6\u7a7a\u878d\u5408\uff0c\u81ea\u9002\u5e94\u5206\u914d\u4e0d\u540c\u5c3a\u5ea6\u548c\u9636\u6bb5\u7684\u611f\u53d7\u91ce\u3002", "result": "\u57283D\u5360\u7528\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8614.3%\u7684mIoU\u76f8\u5bf9\u63d0\u5347\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "PG-Occ\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c473D\u5360\u7528\u9884\u6d4b\u4e2d\u7684\u8868\u793a\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u8be6\u7ec6\u7684\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2510.04770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04770", "abs": "https://arxiv.org/abs/2510.04770", "authors": ["Xiaomeng Fan", "Yuchuan Mao", "Zhi Gao", "Yuwei Wu", "Jin Chen", "Yunde Jia"], "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning", "comment": null, "summary": "Open-vocabulary learning requires modeling the data distribution in open\nenvironments, which consists of both seen-class and unseen-class data.\n  Existing methods estimate the distribution in open environments using\nseen-class data, where the absence of unseen classes makes the estimation error\ninherently unidentifiable.\n  Intuitively, learning beyond the seen classes is crucial for distribution\nestimation to bound the estimation error.\n  We theoretically demonstrate that the distribution can be effectively\nestimated by generating unseen-class data, through which the estimation error\nis upper-bounded.\n  Building on this theoretical insight, we propose a novel open-vocabulary\nlearning method, which generates unseen-class data for estimating the\ndistribution in open environments. The method consists of a class-domain-wise\ndata generation pipeline and a distribution alignment algorithm. The data\ngeneration pipeline generates unseen-class data under the guidance of a\nhierarchical semantic tree and domain information inferred from the seen-class\ndata, facilitating accurate distribution estimation. With the generated data,\nthe distribution alignment algorithm estimates and maximizes the posterior\nprobability to enhance generalization in open-vocabulary learning. Extensive\nexperiments on $11$ datasets demonstrate that our method outperforms baseline\napproaches by up to $14\\%$, highlighting its effectiveness and superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u672a\u89c1\u7c7b\u6570\u636e\u6765\u4f30\u8ba1\u5f00\u653e\u73af\u5883\u4e2d\u7684\u5206\u5e03\uff0c\u5305\u62ec\u7c7b\u57df\u6570\u636e\u751f\u6210\u7ba1\u9053\u548c\u5206\u5e03\u5bf9\u9f50\u7b97\u6cd5\uff0c\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u8fbe14%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5df2\u89c1\u7c7b\u6570\u636e\u4f30\u8ba1\u5f00\u653e\u73af\u5883\u5206\u5e03\uff0c\u4f46\u7531\u4e8e\u672a\u89c1\u7c7b\u7684\u7f3a\u5931\u5bfc\u81f4\u4f30\u8ba1\u8bef\u5dee\u65e0\u6cd5\u8bc6\u522b\u3002\u5b66\u4e60\u8d85\u8d8a\u5df2\u89c1\u7c7b\u5bf9\u4e8e\u8fb9\u754c\u4f30\u8ba1\u8bef\u5dee\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u7c7b\u57df\u6570\u636e\u751f\u6210\u7ba1\u9053\uff08\u5728\u5c42\u6b21\u8bed\u4e49\u6811\u548c\u57df\u4fe1\u606f\u6307\u5bfc\u4e0b\u751f\u6210\u672a\u89c1\u7c7b\u6570\u636e\uff09\u548c\u5206\u5e03\u5bf9\u9f50\u7b97\u6cd5\uff08\u4f30\u8ba1\u5e76\u6700\u5927\u5316\u540e\u9a8c\u6982\u7387\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff09\u3002", "result": "\u572811\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe14%\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u672a\u89c1\u7c7b\u6570\u636e\u53ef\u4ee5\u6709\u6548\u4f30\u8ba1\u5f00\u653e\u73af\u5883\u5206\u5e03\uff0c\u7406\u8bba\u8bc1\u660e\u4f30\u8ba1\u8bef\u5dee\u6709\u4e0a\u754c\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5f00\u653e\u8bcd\u6c47\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04772", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04772", "abs": "https://arxiv.org/abs/2510.04772", "authors": ["Max Kirchner", "Hanna Hoffmann", "Alexander C. Jenke", "Oliver L. Saldanha", "Kevin Pfeiffer", "Weam Kanjo", "Julia Alekseenko", "Claas de Boer", "Santhi Raj Kolamuri", "Lorenzo Mazza", "Nicolas Padoy", "Sophia Bano", "Annika Reinke", "Lena Maier-Hein", "Danail Stoyanov", "Jakob N. Kather", "Fiona R. Kolbinger", "Sebastian Bodenstedt", "Stefanie Speidel"], "title": "Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge", "comment": "A challenge report pre-print (31 pages), including 7 tables and 8\n  figures", "summary": "Purpose: The FedSurg challenge was designed to benchmark the state of the art\nin federated learning for surgical video classification. Its goal was to assess\nhow well current methods generalize to unseen clinical centers and adapt\nthrough local fine-tuning while enabling collaborative model development\nwithout sharing patient data. Methods: Participants developed strategies to\nclassify inflammation stages in appendicitis using a preliminary version of the\nmulti-center Appendix300 video dataset. The challenge evaluated two tasks:\ngeneralization to an unseen center and center-specific adaptation after\nfine-tuning. Submitted approaches included foundation models with linear\nprobing, metric learning with triplet loss, and various FL aggregation schemes\n(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and\nExpected Cost, with ranking robustness evaluated via bootstrapping and\nstatistical testing. Results: In the generalization task, performance across\ncenters was limited. In the adaptation task, all teams improved after\nfine-tuning, though ranking stability was low. The ViViT-based submission\nachieved the strongest overall performance. The challenge highlighted\nlimitations in generalization, sensitivity to class imbalance, and difficulties\nin hyperparameter tuning in decentralized training, while spatiotemporal\nmodeling and context-aware preprocessing emerged as promising strategies.\nConclusion: The FedSurg Challenge establishes the first benchmark for\nevaluating FL strategies in surgical video classification. Findings highlight\nthe trade-off between local personalization and global robustness, and\nunderscore the importance of architecture choice, preprocessing, and loss\ndesign. This benchmarking offers a reference point for future development of\nimbalance-aware, adaptive, and robust FL methods in clinical surgical AI.", "AI": {"tldr": "FedSurg\u6311\u6218\u8d5b\u662f\u9996\u4e2a\u8bc4\u4f30\u8054\u90a6\u5b66\u4e60\u5728\u624b\u672f\u89c6\u9891\u5206\u7c7b\u4e2d\u5e94\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u5728\u672a\u89c1\u4e34\u5e8a\u4e2d\u5fc3\u7684\u6cdb\u5316\u80fd\u529b\u548c\u672c\u5730\u5fae\u8c03\u540e\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u5f53\u524d\u65b9\u6cd5\u5728\u624b\u672f\u89c6\u9891\u5206\u7c7b\u4e2d\u5982\u4f55\u6cdb\u5316\u5230\u672a\u89c1\u4e34\u5e8a\u4e2d\u5fc3\uff0c\u4ee5\u53ca\u901a\u8fc7\u672c\u5730\u5fae\u8c03\u5b9e\u73b0\u9002\u5e94\uff0c\u540c\u65f6\u5728\u4e0d\u5171\u4eab\u60a3\u8005\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u534f\u4f5c\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u53c2\u4e0e\u8005\u4f7f\u7528\u591a\u4e2d\u5fc3Appendix300\u89c6\u9891\u6570\u636e\u96c6\u5f00\u53d1\u7b56\u7565\u6765\u5206\u7c7b\u9611\u5c3e\u708e\u708e\u75c7\u9636\u6bb5\u3002\u8bc4\u4f30\u4e24\u4e2a\u4efb\u52a1\uff1a\u672a\u89c1\u4e2d\u5fc3\u7684\u6cdb\u5316\u548c\u5fae\u8c03\u540e\u7684\u4e2d\u5fc3\u7279\u5b9a\u9002\u5e94\u3002\u63d0\u4ea4\u65b9\u6cd5\u5305\u62ec\u57fa\u7840\u6a21\u578b\u7ebf\u6027\u63a2\u6d4b\u3001\u4e09\u5143\u7ec4\u635f\u5931\u7684\u5ea6\u91cf\u5b66\u4e60\uff0c\u4ee5\u53ca\u5404\u79cdFL\u805a\u5408\u65b9\u6848\uff08FedAvg\u3001FedMedian\u3001FedSAM\uff09\u3002", "result": "\u5728\u6cdb\u5316\u4efb\u52a1\u4e2d\uff0c\u8de8\u4e2d\u5fc3\u6027\u80fd\u6709\u9650\uff1b\u5728\u9002\u5e94\u4efb\u52a1\u4e2d\uff0c\u6240\u6709\u56e2\u961f\u5728\u5fae\u8c03\u540e\u90fd\u6709\u6539\u8fdb\uff0c\u4f46\u6392\u540d\u7a33\u5b9a\u6027\u8f83\u4f4e\u3002ViViT\u63d0\u4ea4\u83b7\u5f97\u6700\u5f3a\u6574\u4f53\u6027\u80fd\u3002\u6311\u6218\u51f8\u663e\u4e86\u6cdb\u5316\u9650\u5236\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u654f\u611f\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u4e2d\u8d85\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u3002", "conclusion": "FedSurg\u6311\u6218\u8d5b\u5efa\u7acb\u4e86\u8bc4\u4f30\u624b\u672f\u89c6\u9891\u5206\u7c7b\u4e2dFL\u7b56\u7565\u7684\u9996\u4e2a\u57fa\u51c6\u3002\u53d1\u73b0\u5f3a\u8c03\u4e86\u672c\u5730\u4e2a\u6027\u5316\u4e0e\u5168\u5c40\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u53ca\u67b6\u6784\u9009\u62e9\u3001\u9884\u5904\u7406\u548c\u635f\u5931\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002\u4e3a\u672a\u6765\u5f00\u53d1\u4e0d\u5e73\u8861\u611f\u77e5\u3001\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u4e34\u5e8a\u624b\u672fAI FL\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53c2\u8003\u70b9\u3002"}}
{"id": "2510.04781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04781", "abs": "https://arxiv.org/abs/2510.04781", "authors": ["Javed Ahmad", "Federico Dassi\u00e8", "Selene Frascella", "Gabriele Marchello", "Ferdinando Cannella", "Arianna Traviglia"], "title": "Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization", "comment": "9 pages", "summary": "High-fidelity 3D scanning is essential for preserving cultural heritage\nartefacts, supporting documentation, analysis, and long-term conservation.\nHowever, conventional methods typically require specialized expertise and\nmanual intervention to maintain optimal scanning conditions and coverage. We\npresent an automated two-robot scanning system that eliminates the need for\nhandheld or semi-automatic workflows by combining coordinated robotic\nmanipulation with high-resolution 3D scanning. Our system parameterizes the\nscanning space into distinct regions, enabling coordinated motion planning\nbetween a scanner-equipped robot and a tray-handling robot. Optimized\ntrajectory planning and waypoint distribution ensure comprehensive surface\ncoverage, minimize occlusions, and balance reconstruction accuracy with system\nefficiency. Experimental results show that our approach achieves significantly\nlower Chamfer Distance and higher F-score compared to baseline methods,\noffering superior geometric accuracy, improved digitization efficiency, and\nreduced reliance on expert operators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u53cc\u673a\u5668\u4eba3D\u626b\u63cf\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u9ad8\u5206\u8fa8\u7387\u626b\u63cf\uff0c\u5b9e\u73b0\u4e86\u6587\u5316\u9057\u4ea7\u6587\u7269\u7684\u9ad8\u8d28\u91cf\u6570\u5b57\u5316\uff0c\u65e0\u9700\u624b\u6301\u6216\u534a\u81ea\u52a8\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u548c\u624b\u52a8\u5e72\u9884\u6765\u7ef4\u6301\u6700\u4f73\u626b\u63cf\u6761\u4ef6\u548c\u8986\u76d6\u8303\u56f4\uff0c\u8fd9\u9650\u5236\u4e86\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u7684\u6548\u7387\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u534f\u8c03\u5de5\u4f5c\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff1a\u4e00\u4e2a\u914d\u5907\u626b\u63cf\u4eea\u7684\u673a\u5668\u4eba\u548c\u4e00\u4e2a\u6258\u76d8\u5904\u7406\u673a\u5668\u4eba\u3002\u7cfb\u7edf\u5c06\u626b\u63cf\u7a7a\u95f4\u53c2\u6570\u5316\u4e3a\u4e0d\u540c\u533a\u57df\uff0c\u901a\u8fc7\u4f18\u5316\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u8def\u5f84\u70b9\u5206\u5e03\u786e\u4fdd\u5168\u9762\u8868\u9762\u8986\u76d6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684Chamfer Distance\u548c\u66f4\u9ad8\u7684F-score\uff0c\u63d0\u4f9b\u4e86\u4f18\u8d8a\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u6570\u5b57\u5316\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u5316\u9057\u4ea7\u6587\u72693D\u626b\u63cf\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u64cd\u4f5c\u4e13\u5bb6\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\u7684\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2510.04794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04794", "abs": "https://arxiv.org/abs/2510.04794", "authors": ["Alon Kaya", "Igal Bilik", "Inna Stainvas"], "title": "A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation", "comment": null, "summary": "Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)\nhave reshaped computer vision through pretrained feature representations that\nenable strong transfer learning for diverse tasks. However, their efficiency as\nbackbone architectures for geometric estimation tasks involving image\ndeformations in low-data regimes remains an open question. This work considers\ntwo such tasks: 1) estimating 2D rigid transformations between pairs of images\nand 2) predicting the fundamental matrix for stereo image pairs, an important\nproblem in various applications, such as autonomous mobility, robotics, and 3D\nscene reconstruction. Addressing this intriguing question, this work\nsystematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)\nwith ViT-based foundation models (CLIP-ViT variants and DINO) in various data\nsize settings, including few-shot scenarios. These pretrained models are\noptimized for classification or contrastive learning, encouraging them to focus\nmostly on high-level semantics. The considered tasks require balancing local\nand global features differently, challenging the straightforward adoption of\nthese models as the backbone. Empirical comparative analysis shows that,\nsimilar to training from scratch, ViTs outperform CNNs during refinement in\nlarge downstream-data scenarios. However, in small data scenarios, the\ninductive bias and smaller capacity of CNNs improve their performance, allowing\nthem to match that of a ViT. Moreover, ViTs exhibit stronger generalization in\ncross-domain evaluation where the data distribution changes. These results\nemphasize the importance of carefully selecting model architectures for\nrefinement, motivating future research towards hybrid architectures that\nbalance local and global representations.", "AI": {"tldr": "\u6bd4\u8f83ViT\u548cCNN\u5728\u51e0\u4f55\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5728\u5927\u6570\u636e\u573a\u666f\u4e0bViT\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0bCNN\u7531\u4e8e\u5f52\u7eb3\u504f\u7f6e\u548c\u8f83\u5c0f\u5bb9\u91cf\u800c\u8868\u73b0\u76f8\u5f53\uff0cViT\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "motivation": "\u7814\u7a76ViT\u548cCNN\u4f5c\u4e3a\u9aa8\u5e72\u7f51\u7edc\u5728\u51e0\u4f55\u4f30\u8ba1\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u521a\u6027\u53d8\u6362\u548c\u57fa\u7840\u77e9\u9635\u4f30\u8ba1\uff09\u4e2d\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83\u5927\u89c4\u6a21CNN\uff08ResNet\u3001EfficientNet\u3001CLIP-ResNet\uff09\u4e0eViT\u57fa\u7840\u6a21\u578b\uff08CLIP-ViT\u53d8\u4f53\u548cDINO\uff09\u5728\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5c11\u6837\u672c\u573a\u666f\u3002", "result": "\u5728\u5927\u6570\u636e\u4e0b\u6e38\u573a\u666f\u4e2d\uff0cViT\u5728\u5fae\u8c03\u65f6\u4f18\u4e8eCNN\uff1b\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e2d\uff0cCNN\u7684\u5f52\u7eb3\u504f\u7f6e\u548c\u8f83\u5c0f\u5bb9\u91cf\u4f7f\u5176\u6027\u80fd\u4e0eViT\u76f8\u5f53\uff1bViT\u5728\u8de8\u57df\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u9700\u8981\u6839\u636e\u4efb\u52a1\u9700\u6c42\u4ed4\u7ec6\u9009\u62e9\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5fae\u8c03\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5e73\u8861\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\u7684\u6df7\u5408\u67b6\u6784\u3002"}}
{"id": "2510.04797", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04797", "abs": "https://arxiv.org/abs/2510.04797", "authors": ["Qi Li", "Shuwen Qiu", "Julien Han", "Xingzi Xu", "Mehmet Saygin Seyfioglu", "Kee Kiat Koo", "Karim Bouyarmane"], "title": "DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing", "comment": "Submitted to CVPR 2025 and Published at CVPR 2025 AI for Content\n  Creation workshop", "summary": "The rapid growth of e-commerce has intensified the demand for Virtual Try-On\n(VTO) technologies, enabling customers to realistically visualize products\noverlaid on their own images. Despite recent advances, existing VTO models face\nchallenges with fine-grained detail preservation, robustness to real-world\nimagery, efficient sampling, image editing capabilities, and generalization\nacross diverse product categories. In this paper, we present DiT-VTON, a novel\nVTO framework that leverages a Diffusion Transformer (DiT), renowned for its\nperformance on text-conditioned image generation, adapted here for the\nimage-conditioned VTO task. We systematically explore multiple DiT\nconfigurations, including in-context token concatenation, channel\nconcatenation, and ControlNet integration, to determine the best setup for VTO\nimage conditioning.\n  To enhance robustness, we train the model on an expanded dataset encompassing\nvaried backgrounds, unstructured references, and non-garment categories,\ndemonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also\nredefines the VTO task beyond garment try-on, offering a versatile Virtual\nTry-All (VTA) solution capable of handling a wide range of product categories\nand supporting advanced image editing functionalities such as pose\npreservation, localized editing, texture transfer, and object-level\ncustomization. Experimental results show that our model surpasses\nstate-of-the-art methods on VITON-HD, achieving superior detail preservation\nand robustness without reliance on additional condition encoders. It also\noutperforms models with VTA and image editing capabilities on a diverse dataset\nspanning thousands of product categories.", "AI": {"tldr": "\u63d0\u51faDiT-VTON\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u89e3\u51b3\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\uff0c\u652f\u6301\u591a\u79cd\u4ea7\u54c1\u7c7b\u522b\u548c\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\uff0c\u5728VITON-HD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u6a21\u578b\u5728\u7ec6\u8282\u4fdd\u7559\u3001\u9c81\u68d2\u6027\u3001\u91c7\u6837\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u63a2\u7d22\u591a\u79cd\u56fe\u50cf\u6761\u4ef6\u914d\u7f6e\uff08\u4e0a\u4e0b\u6587token\u62fc\u63a5\u3001\u901a\u9053\u62fc\u63a5\u3001ControlNet\u96c6\u6210\uff09\uff0c\u5e76\u5728\u6269\u5c55\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728VITON-HD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u7ec6\u8282\u4fdd\u7559\u548c\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u6761\u4ef6\u7f16\u7801\u5668\uff1b\u5728\u591a\u6837\u5316\u4ea7\u54c1\u7c7b\u522b\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DiT-VTON\u4e0d\u4ec5\u6539\u8fdb\u4e86\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\uff0c\u8fd8\u6269\u5c55\u4e3a\u865a\u62df\u8bd5\u5168\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u529f\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.04802", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04802", "abs": "https://arxiv.org/abs/2510.04802", "authors": ["Han Zhang", "Lalithkumar Seenivasan", "Jose L. Porras", "Roger D. Soberanis-Mukul", "Hao Ding", "Hongchao Shu", "Benjamin D. Killeen", "Ankita Ghosh", "Lonny Yarmus", "Masaru Ishii", "Angela Christine Argento", "Mathias Unberath"], "title": "Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors", "comment": null, "summary": "Observing surgical practice has historically relied on fixed vantage points\nor recollections, leaving the egocentric visual perspectives that guide\nclinical decisions undocumented. Fixed-camera video can capture surgical\nworkflows at the room-scale, but cannot reconstruct what each team member\nactually saw. Thus, these videos only provide limited insights into how\ndecisions that affect surgical safety, training, and workflow optimization are\nmade. Here we introduce EgoSurg, the first framework to reconstruct the\ndynamic, egocentric replays for any operating room (OR) staff directly from\nwall-mounted fixed-camera video, and thus, without intervention to clinical\nworkflow. EgoSurg couples geometry-driven neural rendering with diffusion-based\nview enhancement, enabling high-visual fidelity synthesis of arbitrary and\negocentric viewpoints at any moment. In evaluation across multi-site surgical\ncases and controlled studies, EgoSurg reconstructs person-specific visual\nfields and arbitrary viewpoints with high visual quality and fidelity. By\ntransforming existing OR camera infrastructure into a navigable dynamic 3D\nrecord, EgoSurg establishes a new foundation for immersive surgical data\nscience, enabling surgical practice to be visualized, experienced, and analyzed\nfrom every angle.", "AI": {"tldr": "EgoSurg\u662f\u4e00\u4e2a\u4ece\u56fa\u5b9a\u6444\u50cf\u5934\u89c6\u9891\u91cd\u5efa\u624b\u672f\u5ba4\u4eba\u5458\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u56de\u653e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u9a71\u52a8\u795e\u7ecf\u6e32\u67d3\u548c\u6269\u6563\u6a21\u578b\u589e\u5f3a\u89c6\u56fe\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u4efb\u610f\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u624b\u672f\u89c2\u5bdf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u89c6\u89d2\u6216\u56de\u5fc6\uff0c\u65e0\u6cd5\u8bb0\u5f55\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89c9\u89c6\u89d2\uff0c\u9650\u5236\u4e86\u624b\u672f\u5b89\u5168\u3001\u57f9\u8bad\u548c\u6d41\u7a0b\u4f18\u5316\u7684\u5206\u6790\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u9a71\u52a8\u795e\u7ecf\u6e32\u67d3\u548c\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u56fe\u589e\u5f3a\u6280\u672f\uff0c\u4ece\u58c1\u6302\u56fa\u5b9a\u6444\u50cf\u5934\u89c6\u9891\u91cd\u5efa\u4efb\u610f\u65f6\u523b\u7684\u4efb\u610f\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u3002", "result": "\u5728\u591a\u7ad9\u70b9\u624b\u672f\u6848\u4f8b\u548c\u5bf9\u7167\u7814\u7a76\u4e2d\uff0cEgoSurg\u80fd\u591f\u4ee5\u9ad8\u89c6\u89c9\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u91cd\u5efa\u4e2a\u4f53\u7279\u5b9a\u7684\u89c6\u89c9\u573a\u548c\u4efb\u610f\u89c6\u89d2\u3002", "conclusion": "EgoSurg\u5c06\u73b0\u6709\u624b\u672f\u5ba4\u6444\u50cf\u5934\u57fa\u7840\u8bbe\u65bd\u8f6c\u5316\u4e3a\u53ef\u5bfc\u822a\u7684\u52a8\u60013D\u8bb0\u5f55\uff0c\u4e3a\u6c89\u6d78\u5f0f\u624b\u672f\u6570\u636e\u79d1\u5b66\u5960\u5b9a\u65b0\u57fa\u7840\uff0c\u4f7f\u624b\u672f\u5b9e\u8df5\u53ef\u4ee5\u4ece\u5404\u4e2a\u89d2\u5ea6\u53ef\u89c6\u5316\u3001\u4f53\u9a8c\u548c\u5206\u6790\u3002"}}
{"id": "2510.04822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04822", "abs": "https://arxiv.org/abs/2510.04822", "authors": ["Zicheng Jiang", "Jixin Gao", "Shengfeng He", "Xinzhe Li", "Yulong Zheng", "Zhaotong Yang", "Junyu Dong", "Yong Du"], "title": "AvatarVTON: 4D Virtual Try-On for Animatable Avatars", "comment": null, "summary": "We propose AvatarVTON, the first 4D virtual try-on framework that generates\nrealistic try-on results from a single in-shop garment image, enabling free\npose control, novel-view rendering, and diverse garment choices. Unlike\nexisting methods, AvatarVTON supports dynamic garment interactions under\nsingle-view supervision, without relying on multi-view garment captures or\nphysics priors. The framework consists of two key modules: (1) a Reciprocal\nFlow Rectifier, a prior-free optical-flow correction strategy that stabilizes\navatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,\nwhich decomposes Gaussian maps into view-pose-invariant and view-pose-specific\ncomponents, enabling adaptive, non-linear garment deformations. To establish a\nbenchmark for 4D virtual try-on, we extend existing baselines with unified\nmodules for fair qualitative and quantitative comparisons. Extensive\nexperiments show that AvatarVTON achieves high fidelity, diversity, and dynamic\ngarment realism, making it well-suited for AR/VR, gaming, and digital-human\napplications.", "AI": {"tldr": "AvatarVTON\u662f\u9996\u4e2a4D\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u80fd\u4ece\u5355\u5f20\u5546\u54c1\u670d\u88c5\u56fe\u50cf\u751f\u6210\u903c\u771f\u7684\u8bd5\u7a7f\u6548\u679c\uff0c\u652f\u6301\u81ea\u7531\u59ff\u6001\u63a7\u5236\u3001\u65b0\u89c6\u89d2\u6e32\u67d3\u548c\u591a\u6837\u5316\u670d\u88c5\u9009\u62e9\uff0c\u65e0\u9700\u591a\u89c6\u89d2\u670d\u88c5\u6355\u6349\u6216\u7269\u7406\u5148\u9a8c\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u65e0\u6cd5\u652f\u6301\u52a8\u6001\u670d\u88c5\u4ea4\u4e92\uff0c\u4e14\u4f9d\u8d56\u591a\u89c6\u89d2\u670d\u88c5\u6355\u6349\u6216\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u4e92\u9006\u6d41\u6821\u6b63\u5668\uff08\u65e0\u5148\u9a8c\u5149\u6d41\u6821\u6b63\u7b56\u7565\uff09\u548c\u975e\u7ebf\u6027\u53d8\u5f62\u5668\uff08\u5c06\u9ad8\u65af\u56fe\u5206\u89e3\u4e3a\u89c6\u89d2\u59ff\u6001\u4e0d\u53d8\u548c\u7279\u5b9a\u5206\u91cf\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAvatarVTON\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3001\u591a\u6837\u6027\u548c\u52a8\u6001\u670d\u88c5\u771f\u5b9e\u611f\uff0c\u5728AR/VR\u3001\u6e38\u620f\u548c\u6570\u5b57\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a4D\u865a\u62df\u8bd5\u7a7f\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u5355\u89c6\u89d2\u76d1\u7763\u4e0b\u7684\u52a8\u6001\u670d\u88c5\u4ea4\u4e92\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.04823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04823", "abs": "https://arxiv.org/abs/2510.04823", "authors": ["Arnela Hadzic", "Simon Johannes Joham", "Martin Urschler"], "title": "Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis", "comment": null, "summary": "Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in\nenabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment\nprecision while reducing patient radiation exposure. To address this task, we\nadopt a fully 3D Flow Matching (FM) framework, motivated by recent work\ndemonstrating FM's efficiency in producing high-quality images. In our\napproach, a Gaussian noise volume is transformed into an sCT image by\nintegrating a learned FM velocity field, conditioned on features extracted from\nthe input MRI or CBCT using a lightweight 3D encoder. We evaluated the method\non the SynthRAD2025 Challenge benchmark, training separate models for MRI\n$\\rightarrow$ sCT and CBCT $\\rightarrow$ sCT across three anatomical regions:\nabdomen, head and neck, and thorax. Validation and testing were performed\nthrough the challenge submission system. The results indicate that the method\naccurately reconstructs global anatomical structures; however, preservation of\nfine details was limited, primarily due to the relatively low training\nresolution imposed by memory and runtime constraints. Future work will explore\npatch-based training and latent-space flow models to improve resolution and\nlocal structural fidelity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6d41\u5339\u914d\u7684\u5408\u6210CT\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4eceMRI\u6216CBCT\u751f\u6210sCT\uff0c\u7528\u4e8e\u653e\u5c04\u6cbb\u7597\u89c4\u5212\u3002", "motivation": "\u5f00\u53d1MRI-only\u548cCBCT-based\u81ea\u9002\u5e94\u653e\u5c04\u6cbb\u7597\uff0c\u63d0\u9ad8\u6cbb\u7597\u7cbe\u5ea6\u540c\u65f6\u51cf\u5c11\u60a3\u8005\u8f90\u5c04\u66b4\u9732\u3002", "method": "\u91c7\u7528\u5b8c\u51683D\u6d41\u5339\u914d\u6846\u67b6\uff0c\u5c06\u9ad8\u65af\u566a\u58f0\u901a\u8fc7\u5b66\u4e60\u7684\u6d41\u5339\u914d\u901f\u5ea6\u573a\u8f6c\u6362\u4e3asCT\u56fe\u50cf\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea73D\u7f16\u7801\u5668\u4ece\u8f93\u5165MRI\u6216CBCT\u4e2d\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5728SynthRAD2025\u6311\u6218\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0c\u80fd\u51c6\u786e\u91cd\u5efa\u5168\u5c40\u89e3\u5256\u7ed3\u6784\uff0c\u4f46\u53d7\u9650\u4e8e\u8bad\u7ec3\u5206\u8fa8\u7387\uff0c\u7ec6\u90e8\u7ec6\u8282\u4fdd\u7559\u6709\u9650\u3002", "conclusion": "\u672a\u6765\u5c06\u63a2\u7d22\u57fa\u4e8epatch\u7684\u8bad\u7ec3\u548c\u6f5c\u5728\u7a7a\u95f4\u6d41\u6a21\u578b\u6765\u63d0\u9ad8\u5206\u8fa8\u7387\u548c\u5c40\u90e8\u7ed3\u6784\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2510.04838", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04838", "abs": "https://arxiv.org/abs/2510.04838", "authors": ["Muquan Li", "Hang Gou", "Dongyang Zhang", "Shuang Liang", "Xiurui Xie", "Deqiang Ouyang", "Ke Qin"], "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation", "comment": null, "summary": "The growing demand for efficient deep learning has positioned dataset\ndistillation as a pivotal technique for compressing training dataset while\npreserving model performance. However, existing inner-loop optimization methods\nfor dataset distillation typically rely on random truncation strategies, which\nlack flexibility and often yield suboptimal results. In this work, we observe\nthat neural networks exhibit distinct learning dynamics across different\ntraining stages-early, middle, and late-making random truncation ineffective.\nTo address this limitation, we propose Automatic Truncated Backpropagation\nThrough Time (AT-BPTT), a novel framework that dynamically adapts both\ntruncation positions and window sizes according to intrinsic gradient behavior.\nAT-BPTT introduces three key components: (1) a probabilistic mechanism for\nstage-aware timestep selection, (2) an adaptive window sizing strategy based on\ngradient variation, and (3) a low-rank Hessian approximation to reduce\ncomputational overhead. Extensive experiments on CIFAR-10, CIFAR-100,\nTiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art\nperformance, improving accuracy by an average of 6.16% over baseline methods.\nMoreover, our approach accelerates inner-loop optimization by 3.9x while saving\n63% memory cost.", "AI": {"tldr": "\u63d0\u51faAT-BPTT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u622a\u65ad\u4f4d\u7f6e\u548c\u7a97\u53e3\u5927\u5c0f\u6765\u6539\u8fdb\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u5185\u5faa\u73af\u4f18\u5316\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53476.16%\uff0c\u540c\u65f6\u52a0\u901f3.9\u500d\u5e76\u8282\u770163%\u5185\u5b58\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4f7f\u7528\u968f\u673a\u622a\u65ad\u7b56\u7565\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u6548\u679c\u4e0d\u4f73\u3002\u89c2\u5bdf\u5230\u795e\u7ecf\u7f51\u7edc\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u5177\u6709\u4e0d\u540c\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u968f\u673a\u622a\u65ad\u6548\u679c\u4e0d\u7406\u60f3\u3002", "method": "AT-BPTT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u6982\u7387\u7684\u9636\u6bb5\u611f\u77e5\u65f6\u95f4\u6b65\u9009\u62e9\u673a\u5236\u3001\u57fa\u4e8e\u68af\u5ea6\u53d8\u5316\u7684\u81ea\u9002\u5e94\u7a97\u53e3\u5927\u5c0f\u7b56\u7565\u3001\u4ee5\u53ca\u7528\u4e8e\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u4f4e\u79e9Hessian\u8fd1\u4f3c\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001Tiny-ImageNet\u548cImageNet-1K\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAT-BPTT\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53476.16%\uff0c\u5185\u5faa\u73af\u4f18\u5316\u52a0\u901f3.9\u500d\uff0c\u5185\u5b58\u6210\u672c\u8282\u770163%\u3002", "conclusion": "AT-BPTT\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u622a\u65ad\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.04840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04840", "abs": "https://arxiv.org/abs/2510.04840", "authors": ["Viktor Koz\u00e1k", "Jan Chudoba", "Libor P\u0159eu\u010dil"], "title": "Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints", "comment": "10 pages, 18 figures", "summary": "An accurate and up-to-date model of a photovoltaic (PV) power plant is\nessential for its optimal operation and maintenance. However, such a model may\nnot be easily available. This work introduces a novel approach for PV power\nplant mapping based on aerial overview images. It enables the automation of the\nmapping process while removing the reliance on third-party data. The presented\nmapping method takes advantage of the structural layout of the power plants to\nachieve detailed modeling down to the level of individual PV modules. The\napproach relies on visual segmentation of PV modules in overview images and the\ninference of structural information in each image, assigning modules to\nindividual benches, rows, and columns. We identify visual keypoints related to\nthe layout and use these to merge detections from multiple images while\nmaintaining their structural integrity. The presented method was experimentally\nverified and evaluated on two different power plants. The final fusion of 3D\npositions and semantic structures results in a compact georeferenced model\nsuitable for power plant maintenance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u822a\u62cd\u56fe\u50cf\u7684\u5149\u4f0f\u7535\u7ad9\u6620\u5c04\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4ece\u56fe\u50cf\u4e2d\u81ea\u52a8\u8bc6\u522b\u548c\u5efa\u6a21\u5149\u4f0f\u7ec4\u4ef6\u5230\u5355\u4e2a\u6a21\u5757\u7ea7\u522b\u3002", "motivation": "\u5149\u4f0f\u7535\u7ad9\u7684\u7cbe\u786e\u6a21\u578b\u5bf9\u5176\u4f18\u5316\u8fd0\u7ef4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u4e0d\u6613\u83b7\u53d6\uff0c\u4e14\u4f9d\u8d56\u7b2c\u4e09\u65b9\u6570\u636e\u3002", "method": "\u5229\u7528\u822a\u62cd\u56fe\u50cf\u8fdb\u884c\u89c6\u89c9\u5206\u5272\uff0c\u8bc6\u522b\u5149\u4f0f\u6a21\u5757\uff0c\u901a\u8fc7\u5e03\u5c40\u5173\u952e\u70b9\u63a8\u65ad\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u5c06\u591a\u56fe\u50cf\u68c0\u6d4b\u7ed3\u679c\u878d\u5408\u4ee5\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u4e0d\u540c\u7535\u7ad9\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6700\u7ec8\u878d\u54083D\u4f4d\u7f6e\u548c\u8bed\u4e49\u7ed3\u6784\uff0c\u751f\u6210\u7d27\u51d1\u7684\u5730\u7406\u53c2\u8003\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u81ea\u52a8\u5316\u6620\u5c04\u8fc7\u7a0b\uff0c\u6d88\u9664\u5bf9\u7b2c\u4e09\u65b9\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u7535\u7ad9\u7ef4\u62a4\u63d0\u4f9b\u5408\u9002\u7684\u5730\u7406\u53c2\u8003\u6a21\u578b\u3002"}}
{"id": "2510.04844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04844", "abs": "https://arxiv.org/abs/2510.04844", "authors": ["Cheyu Lin", "Katherine A. Flanigan"], "title": "From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements", "comment": "The 15th International Workshop on Structural Health Monitoring\n  (IWSHM)", "summary": "Understanding the dynamic relationship between humans and the built\nenvironment is a key challenge in disciplines ranging from environmental\npsychology to reinforcement learning (RL). A central obstacle in modeling these\ninteractions is the inability to capture human psychological states in a way\nthat is both generalizable and privacy preserving. Traditional methods rely on\ntheoretical models or questionnaires, which are limited in scope, static, and\nlabor intensive. We present a kinesics recognition framework that infers the\ncommunicative functions of human activity -- known as kinesics -- directly from\n3D skeleton joint data. Combining a spatial-temporal graph convolutional\nnetwork (ST-GCN) with a convolutional neural network (CNN), the framework\nleverages transfer learning to bypass the need for manually defined mappings\nbetween physical actions and psychological categories. The approach preserves\nuser anonymity while uncovering latent structures in bodily movements that\nreflect cognitive and emotional states. Our results on the Dyadic User\nEngagemenT (DUET) dataset demonstrate that this method enables scalable,\naccurate, and human-centered modeling of behavior, offering a new pathway for\nenhancing RL-driven simulations of human-environment interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9aa8\u9abc\u6570\u636e\u7684\u8fd0\u52a8\u5b66\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7ST-GCN\u548cCNN\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\uff0c\u4ece\u4eba\u4f53\u52a8\u4f5c\u63a8\u65ad\u5fc3\u7406\u72b6\u6001\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u4eba\u7c7b\u884c\u4e3a\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7406\u8bba\u6a21\u578b\u6216\u95ee\u5377\u8c03\u67e5\uff0c\u5b58\u5728\u8303\u56f4\u6709\u9650\u3001\u9759\u6001\u4e14\u52b3\u52a8\u5bc6\u96c6\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u6355\u6349\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u3002", "method": "\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4\u56fe\u5377\u79ef\u7f51\u7edc(ST-GCN)\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\uff0c\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u76f4\u63a5\u4ece3D\u9aa8\u9abc\u5173\u8282\u6570\u636e\u63a8\u65ad\u4eba\u7c7b\u6d3b\u52a8\u7684\u6c9f\u901a\u529f\u80fd(\u8fd0\u52a8\u5b66)\u3002", "result": "\u5728DUET\u6570\u636e\u96c6\u4e0a\u7684\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u51c6\u786e\u4e14\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u884c\u4e3a\u5efa\u6a21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u7c7b-\u73af\u5883\u4ea4\u4e92\u6a21\u62df\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u533f\u540d\u6027\u3002"}}
{"id": "2510.04854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04854", "abs": "https://arxiv.org/abs/2510.04854", "authors": ["Cheyu Lin", "John Martins", "Katherine A. Flanigan", "Ph. D"], "title": "Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems", "comment": "ASCE International Conference on Computing in Civil Engineering 2024", "summary": "Cyber-physical systems (CPS) integrate sensing, computing, and control to\nimprove infrastructure performance, focusing on economic goals like performance\nand safety. However, they often neglect potential human-centered (or\n''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim\nto address this by aligning CPS with social objectives. This involves defining\nsocial benefits, understanding human interactions with each other and\ninfrastructure, developing privacy-preserving measurement methods, modeling\nthese interactions for prediction, linking them to social benefits, and\nactuating the physical environment to foster positive social outcomes. This\npaper delves into recognizing dyadic human interactions using real-world data,\nwhich is the backbone to measuring social behavior. This lays a foundation to\naddress the need to enhance understanding of the deeper meanings and mutual\nresponses inherent in human interactions. While RGB cameras are informative for\ninteraction recognition, privacy concerns arise. Depth sensors offer a\nprivacy-conscious alternative by analyzing skeletal movements. This study\ncompares five skeleton-based interaction recognition algorithms on a dataset of\n12 dyadic interactions. Unlike single-person datasets, these interactions,\ncategorized into communication types like emblems and affect displays, offer\ninsights into the cultural and emotional aspects of human interactions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u9abc\u6570\u636e\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u6210\u5bf9\u4eba\u9645\u4e92\u52a8\uff0c\u65e8\u5728\u5c06\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e0e\u793e\u4f1a\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u5173\u6ce8\u793e\u4f1a\u6548\u76ca\u800c\u975e\u5355\u7eaf\u7684\u7ecf\u6d4e\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e3b\u8981\u5173\u6ce8\u7ecf\u6d4e\u76ee\u6807\uff08\u5982\u6027\u80fd\u548c\u5b89\u5168\uff09\uff0c\u800c\u5ffd\u89c6\u4e86\u6f5c\u5728\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5316\uff08\u793e\u4f1a\uff09\u6548\u76ca\u3002\u7f51\u7edc\u7269\u7406\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u65e8\u5728\u901a\u8fc7\u5c06\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u4e0e\u793e\u4f1a\u76ee\u6807\u5bf9\u9f50\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u4f20\u611f\u5668\u5206\u6790\u9aa8\u9abc\u8fd0\u52a8\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cd\u57fa\u4e8e\u9aa8\u9abc\u7684\u4e92\u52a8\u8bc6\u522b\u7b97\u6cd5\uff0c\u5728\u4e00\u4e2a\u5305\u542b12\u79cd\u6210\u5bf9\u4e92\u52a8\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u8fd9\u4e9b\u4e92\u52a8\u6309\u6c9f\u901a\u7c7b\u578b\u5206\u7c7b\uff0c\u5982\u6807\u5fd7\u6027\u52a8\u4f5c\u548c\u60c5\u611f\u8868\u8fbe\u3002", "result": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e94\u79cd\u9aa8\u9abc\u4e92\u52a8\u8bc6\u522b\u7b97\u6cd5\u5728\u6210\u5bf9\u4eba\u9645\u4e92\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u7406\u89e3\u4eba\u7c7b\u4e92\u52a8\u7684\u6587\u5316\u60c5\u611f\u65b9\u9762\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u57fa\u4e8e\u9aa8\u9abc\u7684\u4e92\u52a8\u8bc6\u522b\u65b9\u6cd5\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u4eba\u9645\u4e92\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u4e3a\u7f51\u7edc\u7269\u7406\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u4eba\u7c7b\u4e92\u52a8\u7684\u6df1\u5c42\u542b\u4e49\u548c\u76f8\u4e92\u54cd\u5e94\u3002"}}
{"id": "2510.04856", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04856", "abs": "https://arxiv.org/abs/2510.04856", "authors": ["Martial Guidez", "Stefan Duffner", "Yannick Alpou", "Oscar R\u00f6th", "Christophe Garcia"], "title": "ERDE: Entropy-Regularized Distillation for Early-exit", "comment": null, "summary": "Although deep neural networks and in particular Convolutional Neural Networks\nhave demonstrated state-of-the-art performance in image classification with\nrelatively high efficiency, they still exhibit high computational costs, often\nrendering them impractical for real-time and edge applications. Therefore, a\nmultitude of compression techniques have been developed to reduce these costs\nwhile maintaining accuracy. In addition, dynamic architectures have been\nintroduced to modulate the level of compression at execution time, which is a\ndesirable property in many resource-limited application scenarios. The proposed\nmethod effectively integrates two well-established optimization techniques:\nearly exits and knowledge distillation, where a reduced student early-exit\nmodel is trained from a more complex teacher early-exit model. The primary\ncontribution of this research lies in the approach for training the student\nearly-exit model. In comparison to the conventional Knowledge Distillation\nloss, our approach incorporates a new entropy-based loss for images where the\nteacher's classification was incorrect. The proposed method optimizes the\ntrade-off between accuracy and efficiency, thereby achieving significant\nreductions in computational complexity without compromising classification\nperformance. The validity of this approach is substantiated by experimental\nresults on image classification datasets CIFAR10, CIFAR100 and SVHN, which\nfurther opens new research perspectives for Knowledge Distillation in other\ncontexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u65e9\u671f\u9000\u51fa\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u635f\u5931\u51fd\u6570\u6765\u4f18\u5316\u5b66\u751f\u65e9\u671f\u9000\u51fa\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u548c\u8fb9\u7f18\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u538b\u7f29\u6280\u672f\u6765\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u52a8\u6001\u67b6\u6784\u53ef\u4ee5\u5728\u6267\u884c\u65f6\u8c03\u8282\u538b\u7f29\u7ea7\u522b\u3002", "method": "\u6574\u5408\u65e9\u671f\u9000\u51fa\u548c\u77e5\u8bc6\u84b8\u998f\u4e24\u79cd\u4f18\u5316\u6280\u672f\uff0c\u4f7f\u7528\u66f4\u590d\u6742\u7684\u6559\u5e08\u65e9\u671f\u9000\u51fa\u6a21\u578b\u8bad\u7ec3\u7b80\u5316\u7684\u5b66\u751f\u65e9\u671f\u9000\u51fa\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u57fa\u4e8e\u71b5\u7684\u635f\u5931\u51fd\u6570\u6765\u5904\u7406\u6559\u5e08\u5206\u7c7b\u9519\u8bef\u7684\u56fe\u50cf\u3002", "result": "\u5728CIFAR10\u3001CIFAR100\u548cSVHN\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u4f18\u5316\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u77e5\u8bc6\u84b8\u998f\u5728\u5176\u4ed6\u73af\u5883\u4e2d\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2510.04859", "categories": ["cs.CV", "physics.data-an", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.04859", "abs": "https://arxiv.org/abs/2510.04859", "authors": ["Elena Corbetta", "Thomas Bocklitz"], "title": "\u03bcDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy", "comment": "16 pages, 6 figures. \\mu DeepIQA is publicly available at\n  https://git.photonicdata.science/elena.corbetta/udeepiqa", "summary": "Optical microscopy is one of the most widely used techniques in research\nstudies for life sciences and biomedicine. These applications require reliable\nexperimental pipelines to extract valuable knowledge from the measured samples\nand must be supported by image quality assessment (IQA) to ensure correct\nprocessing and analysis of the image data. IQA methods are implemented with\nvariable complexity. However, while most quality metrics have a straightforward\nimplementation, they might be time consuming and computationally expensive when\nevaluating a large dataset. In addition, quality metrics are often designed for\nwell-defined image features and may be unstable for images out of the ideal\ndomain.\n  To overcome these limitations, recent works have proposed deep learning-based\nIQA methods, which can provide superior performance, increased generalizability\nand fast prediction. Our method, named $\\mathrm{\\mu}$DeepIQA, is inspired by\nprevious studies and applies a deep convolutional neural network designed for\nIQA on natural images to optical microscopy measurements. We retrained the same\narchitecture to predict individual quality metrics and global quality scores\nfor optical microscopy data. The resulting models provide fast and stable\npredictions of image quality by generalizing quality estimation even outside\nthe ideal range of standard methods. In addition, $\\mathrm{\\mu}$DeepIQA\nprovides patch-wise prediction of image quality and can be used to visualize\nspatially varying quality in a single image. Our study demonstrates that\noptical microscopy-based studies can benefit from the generalizability of deep\nlearning models due to their stable performance in the presence of outliers,\nthe ability to assess small image patches, and rapid predictions.", "AI": {"tldr": "\u03bcDeepIQA\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e13\u4e3a\u5149\u5b66\u663e\u5fae\u955c\u56fe\u50cf\u8bbe\u8ba1\uff0c\u80fd\u591f\u5feb\u901f\u7a33\u5b9a\u5730\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u5c40\u90e8\u8d28\u91cf\u53ef\u89c6\u5316\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5bf9\u8d85\u51fa\u7406\u60f3\u57df\u7684\u56fe\u50cf\u4e0d\u7a33\u5b9a\u3002\u6df1\u5ea6\u5b66\u4e60IQA\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u5feb\u901f\u9884\u6d4b\u3002", "method": "\u5c06\u81ea\u7136\u56fe\u50cfIQA\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u91cd\u65b0\u8bad\u7ec3\uff0c\u7528\u4e8e\u5149\u5b66\u663e\u5fae\u955c\u6570\u636e\uff0c\u9884\u6d4b\u5355\u4e2a\u8d28\u91cf\u6307\u6807\u548c\u5168\u5c40\u8d28\u91cf\u5206\u6570\uff0c\u5e76\u63d0\u4f9b\u5c40\u90e8\u8d28\u91cf\u9884\u6d4b\u3002", "result": "\u03bcDeepIQA\u80fd\u591f\u5feb\u901f\u7a33\u5b9a\u5730\u9884\u6d4b\u56fe\u50cf\u8d28\u91cf\uff0c\u5373\u4f7f\u5728\u6807\u51c6\u65b9\u6cd5\u7406\u60f3\u8303\u56f4\u4e4b\u5916\u4e5f\u80fd\u6cdb\u5316\u8d28\u91cf\u4f30\u8ba1\uff0c\u5e76\u80fd\u53ef\u89c6\u5316\u5355\u5e45\u56fe\u50cf\u4e2d\u7a7a\u95f4\u53d8\u5316\u7684\u8d28\u91cf\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u5176\u5728\u5f02\u5e38\u503c\u5b58\u5728\u65f6\u7684\u7a33\u5b9a\u6027\u80fd\u3001\u8bc4\u4f30\u5c0f\u56fe\u50cf\u5757\u7684\u80fd\u529b\u548c\u5feb\u901f\u9884\u6d4b\uff0c\u4f7f\u5149\u5b66\u663e\u5fae\u955c\u7814\u7a76\u53d7\u76ca\u4e8e\u5176\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.04864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04864", "abs": "https://arxiv.org/abs/2510.04864", "authors": ["Ciem Cornelissen", "Sander De Coninck", "Axel Willekens", "Sam Leroux", "Pieter Simoens"], "title": "In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning", "comment": "Accepted manuscript for the IEEE Internet of Things Journal. The\n  final version will be available on IEEE Xplore. \\c{opyright} 2025 IEEE", "summary": "This paper presents an end-to-end, IoT-enabled robotic system for the\nnon-destructive, real-time, and spatially-resolved mapping of grape yield and\nquality (Brix, Acidity) in vineyards. The system features a comprehensive\nanalytical pipeline that integrates two key modules: a high-performance model\nfor grape bunch detection and weight estimation, and a novel deep learning\nframework for quality assessment from hyperspectral (HSI) data. A critical\nbarrier to in-field HSI is the ``domain shift\" caused by variable illumination.\nTo overcome this, our quality assessment is powered by the Light-Invariant\nSpectral Autoencoder (LISA), a domain-adversarial framework that learns\nillumination-invariant features from uncalibrated data. We validated the\nsystem's robustness on a purpose-built HSI dataset spanning three distinct\nillumination domains: controlled artificial lighting (lab), and variable\nnatural sunlight captured in the morning and afternoon. Results show the\ncomplete pipeline achieves a recall (0.82) for bunch detection and a $R^2$\n(0.76) for weight prediction, while the LISA module improves quality prediction\ngeneralization by over 20% compared to the baselines. By combining these robust\nmodules, the system successfully generates high-resolution, georeferenced data\nof both grape yield and quality, providing actionable, data-driven insights for\nprecision viticulture.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7269\u8054\u7f51\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u8461\u8404\u56ed\u4e2d\u8461\u8404\u4ea7\u91cf\u548c\u8d28\u91cf\u7684\u975e\u7834\u574f\u6027\u5b9e\u65f6\u7a7a\u95f4\u6620\u5c04\u3002\u7cfb\u7edf\u5305\u542b\u8461\u8404\u4e32\u68c0\u6d4b\u4e0e\u91cd\u91cf\u4f30\u8ba1\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5149\u7167\u4e0d\u53d8\u5149\u8c31\u81ea\u7f16\u7801\u5668(LISA)\u7684\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u8461\u8404\u56ed\u4e2d\u8461\u8404\u4ea7\u91cf\u548c\u8d28\u91cf\u5b9e\u65f6\u76d1\u6d4b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u514b\u670d\u91ce\u5916\u9ad8\u5149\u8c31\u6210\u50cf\u4e2d\u7531\u53ef\u53d8\u5149\u7167\u5f15\u8d77\u7684\"\u57df\u504f\u79fb\"\u95ee\u9898\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u9ad8\u6027\u80fd\u8461\u8404\u4e32\u68c0\u6d4b\u4e0e\u91cd\u91cf\u4f30\u8ba1\u6a21\u578b\uff0c\u4ee5\u53ca\u57fa\u4e8eLISA(\u5149\u7167\u4e0d\u53d8\u5149\u8c31\u81ea\u7f16\u7801\u5668)\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u3002LISA\u91c7\u7528\u57df\u5bf9\u6297\u6846\u67b6\u4ece\u975e\u6821\u51c6\u6570\u636e\u4e2d\u5b66\u4e60\u5149\u7167\u4e0d\u53d8\u7279\u5f81\u3002", "result": "\u7cfb\u7edf\u5728\u4e09\u4e2a\u4e0d\u540c\u5149\u7167\u57df(\u5b9e\u9a8c\u5ba4\u4eba\u5de5\u5149\u3001\u65e9\u6668\u548c\u4e0b\u5348\u81ea\u7136\u5149)\u4e0a\u9a8c\u8bc1\uff0c\u8461\u8404\u4e32\u68c0\u6d4b\u53ec\u56de\u7387\u8fbe0.82\uff0c\u91cd\u91cf\u9884\u6d4bR\u00b2\u4e3a0.76\uff0cLISA\u6a21\u5757\u76f8\u6bd4\u57fa\u7ebf\u5c06\u8d28\u91cf\u9884\u6d4b\u6cdb\u5316\u80fd\u529b\u63d0\u9ad8\u4e8620%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u751f\u6210\u4e86\u9ad8\u5206\u8fa8\u7387\u7684\u5730\u7406\u53c2\u8003\u6570\u636e\uff0c\u4e3a\u7cbe\u51c6\u8461\u8404\u683d\u57f9\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6570\u636e\u9a71\u52a8\u89c1\u89e3\u3002"}}
{"id": "2510.04876", "categories": ["cs.CV", "cs.LG", "I.2.6; I.4.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2510.04876", "abs": "https://arxiv.org/abs/2510.04876", "authors": ["Hayat Rajani", "Valerio Franchi", "Borja Martinez-Clavel Valles", "Raimon Ramos", "Rafael Garcia", "Nuno Gracias"], "title": "BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping", "comment": "Article under review by IJRR", "summary": "Benthic habitat mapping is fundamental for understanding marine ecosystems,\nguiding conservation efforts, and supporting sustainable resource management.\nYet, the scarcity of large, annotated datasets limits the development and\nbenchmarking of machine learning models in this domain. This paper introduces a\nthorough multi-modal dataset, comprising about a million side-scan sonar (SSS)\ntiles collected along the coast of Catalonia (Spain), complemented by\nbathymetric maps and a set of co-registered optical images from targeted\nsurveys using an autonomous underwater vehicle (AUV). Approximately \\num{36000}\nof the SSS tiles have been manually annotated with segmentation masks to enable\nsupervised fine-tuning of classification models. All the raw sensor data,\ntogether with mosaics, are also released to support further exploration and\nalgorithm development. To address challenges in multi-sensor data fusion for\nAUVs, we spatially associate optical images with corresponding SSS tiles,\nfacilitating self-supervised, cross-modal representation learning. Accompanying\nopen-source preprocessing and annotation tools are provided to enhance\naccessibility and encourage research. This resource aims to establish a\nstandardized benchmark for underwater habitat mapping, promoting advancements\nin autonomous seafloor classification and multi-sensor integration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6d77\u5e95\u6816\u606f\u5730\u6d4b\u7ed8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea6100\u4e07\u4e2a\u4fa7\u626b\u58f0\u7eb3\u56fe\u5757\u3001\u6d4b\u6df1\u5730\u56fe\u548c\u5149\u5b66\u56fe\u50cf\uff0c\u5176\u4e2d3.6\u4e07\u4e2a\u58f0\u7eb3\u56fe\u5757\u5df2\u4eba\u5de5\u6807\u6ce8\uff0c\u652f\u6301\u76d1\u7763\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u6d77\u5e95\u6816\u606f\u5730\u6d4b\u7ed8\u5bf9\u4e8e\u7406\u89e3\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u6d77\u5cb8\u7684\u4fa7\u626b\u58f0\u7eb3\u6570\u636e\u3001\u6d4b\u6df1\u5730\u56fe\u548cAUV\u91c7\u96c6\u7684\u5149\u5b66\u56fe\u50cf\uff0c\u624b\u52a8\u6807\u6ce8\u90e8\u5206\u58f0\u7eb3\u56fe\u5757\uff0c\u5f00\u53d1\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u7ea6100\u4e07\u58f0\u7eb3\u56fe\u5757\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d3.6\u4e07\u4e2a\u5df2\u6807\u6ce8\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u9884\u5904\u7406\u548c\u6807\u6ce8\u5de5\u5177\u3002", "conclusion": "\u8be5\u8d44\u6e90\u65e8\u5728\u4e3a\u6c34\u4e0b\u6816\u606f\u5730\u6d4b\u7ed8\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u63a8\u52a8\u81ea\u4e3b\u6d77\u5e95\u5206\u7c7b\u548c\u591a\u4f20\u611f\u5668\u96c6\u6210\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.04912", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04912", "abs": "https://arxiv.org/abs/2510.04912", "authors": ["Ngeyen Yinkfu", "Sunday Nwovu", "Jonathan Kayizzi", "Angelique Uwamahoro"], "title": "Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context", "comment": "3 figures, 2 tables", "summary": "In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,\noften navigating unpredictably and disregarding traffic rules, posing\nsignificant challenges for autonomous driving systems. This study compares four\nobject detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for\nmotorbike detection using a custom dataset of 198 images collected in Kigali.\nImplemented in PyTorch with transfer learning, the models were evaluated for\naccuracy, localization, and inference speed to assess their suitability for\nreal-time navigation in resource-constrained settings. We identify\nimplementation challenges, including dataset limitations and model\ncomplexities, and recommend simplified architectures for future work to enhance\naccessibility for autonomous systems in developing countries like Rwanda.", "AI": {"tldr": "\u5728\u5362\u65fa\u8fbe\u57fa\u52a0\u5229\uff0c\u6469\u6258\u8f66\u51fa\u79df\u8f66\u662f\u4e3b\u8981\u4ea4\u901a\u5de5\u5177\uff0c\u4f46\u9a7e\u9a76\u884c\u4e3a\u4e0d\u89c4\u8303\uff0c\u7ed9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5e26\u6765\u6311\u6218\u3002\u672c\u7814\u7a76\u6bd4\u8f83\u4e86YOLOv5\u3001Faster R-CNN\u3001SSD\u548cRetinaNet\u56db\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u6469\u6258\u8f66\u68c0\u6d4b\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u52a0\u5229\u7684\u6469\u6258\u8f66\u51fa\u79df\u8f66\u7ecf\u5e38\u4e0d\u9075\u5b88\u4ea4\u901a\u89c4\u5219\uff0c\u884c\u9a76\u4e0d\u53ef\u9884\u6d4b\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u53d1\u5c55\u4e2d\u56fd\u5bb6\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u5728\u57fa\u52a0\u5229\u6536\u96c6\u7684198\u5f20\u56fe\u50cf\u7684\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u5728PyTorch\u4e2d\u5b9e\u73b0\u56db\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08YOLOv5\u3001Faster R-CNN\u3001SSD\u3001RetinaNet\uff09\uff0c\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u4e86\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u5b9a\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u65f6\u5bfc\u822a\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u53d1\u73b0\u4e86\u6570\u636e\u96c6\u9650\u5236\u548c\u6a21\u578b\u590d\u6742\u6027\u7b49\u5b9e\u65bd\u6311\u6218\uff0c\u5efa\u8bae\u672a\u6765\u5de5\u4f5c\u91c7\u7528\u7b80\u5316\u67b6\u6784\uff0c\u4ee5\u589e\u5f3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\uff08\u5982\u5362\u65fa\u8fbe\uff09\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2510.04916", "categories": ["cs.CV", "I.4.6; I.4.8; I.4.10"], "pdf": "https://arxiv.org/pdf/2510.04916", "abs": "https://arxiv.org/abs/2510.04916", "authors": ["Giulio Weikmann", "Gianmarco Perantoni", "Lorenzo Bruzzone"], "title": "A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images", "comment": "12 pages, 6 figures", "summary": "Deep learning has become increasingly important in remote sensing image\nclassification due to its ability to extract semantic information from complex\ndata. Classification tasks often include predefined label hierarchies that\nrepresent the semantic relationships among classes. However, these hierarchies\nare frequently overlooked, and most approaches focus only on fine-grained\nclassification schemes. In this paper, we present a novel Semantics-Aware\nHierarchical Consensus (SAHC) method for learning hierarchical features and\nrelationships by integrating hierarchy-specific classification heads within a\ndeep network architecture, each specialized in different degrees of class\ngranularity. The proposed approach employs trainable hierarchy matrices, which\nguide the network through the learning of the hierarchical structure in a\nself-supervised manner. Furthermore, we introduce a hierarchical consensus\nmechanism to ensure consistent probability distributions across different\nhierarchical levels. This mechanism acts as a weighted ensemble being able to\neffectively leverage the inherent structure of the hierarchical classification\ntask. The proposed SAHC method is evaluated on three benchmark datasets with\ndifferent degrees of hierarchical complexity on different tasks, using distinct\nbackbone architectures to effectively emphasize its adaptability. Experimental\nresults show both the effectiveness of the proposed approach in guiding network\nlearning and the robustness of the hierarchical consensus for remote sensing\nimage classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u5c42\u6b21\u5171\u8bc6(SAHC)\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5c42\u6b21\u7279\u5b9a\u7684\u5206\u7c7b\u5934\u6765\u5b66\u4e60\u5c42\u6b21\u7279\u5f81\u548c\u5173\u7cfb\uff0c\u5229\u7528\u53ef\u8bad\u7ec3\u7684\u5c42\u6b21\u77e9\u9635\u548c\u5c42\u6b21\u5171\u8bc6\u673a\u5236\u786e\u4fdd\u4e0d\u540c\u5c42\u6b21\u95f4\u6982\u7387\u5206\u5e03\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u7ecf\u5e38\u5ffd\u7565\u9884\u5b9a\u4e49\u7684\u6807\u7b7e\u5c42\u6b21\u7ed3\u6784\uff0c\u53ea\u5173\u6ce8\u7ec6\u7c92\u5ea6\u5206\u7c7b\u65b9\u6848\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u7c7b\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "method": "\u5728\u6df1\u5ea6\u7f51\u7edc\u67b6\u6784\u4e2d\u96c6\u6210\u5c42\u6b21\u7279\u5b9a\u7684\u5206\u7c7b\u5934\uff0c\u6bcf\u4e2a\u5934\u4e13\u95e8\u5904\u7406\u4e0d\u540c\u7c92\u5ea6\u7684\u7c7b\u522b\uff1b\u4f7f\u7528\u53ef\u8bad\u7ec3\u7684\u5c42\u6b21\u77e9\u9635\u4ee5\u81ea\u76d1\u7763\u65b9\u5f0f\u5b66\u4e60\u5c42\u6b21\u7ed3\u6784\uff1b\u5f15\u5165\u5c42\u6b21\u5171\u8bc6\u673a\u5236\u786e\u4fdd\u4e0d\u540c\u5c42\u6b21\u95f4\u6982\u7387\u5206\u5e03\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u4e0d\u540c\u5c42\u6b21\u590d\u6742\u5ea6\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6307\u5bfc\u7f51\u7edc\u5b66\u4e60\u548c\u5c42\u6b21\u5171\u8bc6\u7684\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u5f88\u6709\u6548\u3002", "conclusion": "SAHC\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u901a\u8fc7\u5c42\u6b21\u5171\u8bc6\u673a\u5236\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04923", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04923", "abs": "https://arxiv.org/abs/2510.04923", "authors": ["Alec K. Peltekian", "Halil Ertugrul Aktas", "Gorkem Durak", "Kevin Grudzinski", "Bradford C. Bemiss", "Carrie Richardson", "Jane E. Dematte", "G. R. Scott Budinger", "Anthony J. Esposito", "Alexander Misharin", "Alok Choudhary", "Ankit Agrawal", "Ulas Bagci"], "title": "REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis", "comment": "10 pages, 4 figures, 2 tables", "summary": "Mixture-of-Experts (MoE) architectures have significantly contributed to\nscalable machine learning by enabling specialized subnetworks to tackle complex\ntasks efficiently. However, traditional MoE systems lack domain-specific\nconstraints essential for medical imaging, where anatomical structure and\nregional disease heterogeneity strongly influence pathological patterns. Here,\nwe introduce Regional Expert Networks (REN), the first anatomically-informed\nMoE framework tailored specifically for medical image classification. REN\nleverages anatomical priors to train seven specialized experts, each dedicated\nto distinct lung lobes and bilateral lung combinations, enabling precise\nmodeling of region-specific pathological variations. Multi-modal gating\nmechanisms dynamically integrate radiomics biomarkers and deep learning (DL)\nfeatures (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to\ninterstitial lung disease (ILD) classification, REN achieves consistently\nsuperior performance: the radiomics-guided ensemble reached an average AUC of\n0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC\n0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe\nmodels achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)\nand aligning with known disease progression patterns. Through rigorous\npatient-level cross-validation, REN demonstrates strong generalizability and\nclinical interpretability, presenting a scalable, anatomically-guided approach\nreadily extensible to other structured medical imaging applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86REN\uff08\u533a\u57df\u4e13\u5bb6\u7f51\u7edc\uff09\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u89e3\u5256\u5b66\u6307\u5bfc\u7684MoE\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec37\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u80ba\u53f6\u7684\u4e13\u5bb6\u7f51\u7edc\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u95e8\u63a7\u673a\u5236\uff0c\u5728\u95f4\u8d28\u6027\u80ba\u75c5\u5206\u7c7b\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edfMoE\u7cfb\u7edf\u7f3a\u4e4f\u533b\u5b66\u5f71\u50cf\u6240\u9700\u7684\u89e3\u5256\u5b66\u7ea6\u675f\uff0c\u800c\u80ba\u90e8\u7684\u89e3\u5256\u7ed3\u6784\u548c\u533a\u57df\u75be\u75c5\u5f02\u8d28\u6027\u5bf9\u75c5\u7406\u6a21\u5f0f\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u5f71\u50cf\u7684\u89e3\u5256\u5b66\u6307\u5bfc\u6846\u67b6\u3002", "method": "\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u8bad\u7ec37\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u80ba\u53f6\u548c\u53cc\u80ba\u7ec4\u5408\u7684\u4e13\u5bb6\u7f51\u7edc\uff1b\u91c7\u7528\u591a\u6a21\u6001\u95e8\u63a7\u673a\u5236\u52a8\u6001\u6574\u5408\u653e\u5c04\u7ec4\u5b66\u751f\u7269\u6807\u5fd7\u7269\u548c\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\uff08CNN\u3001ViT\u3001Mamba\uff09\u6765\u4f18\u5316\u4e13\u5bb6\u8d21\u732e\u6743\u91cd\u3002", "result": "\u5728\u95f4\u8d28\u6027\u80ba\u75c5\u5206\u7c7b\u4e2d\uff0c\u653e\u5c04\u7ec4\u5b66\u5f15\u5bfc\u7684\u96c6\u6210\u6a21\u578b\u8fbe\u5230\u5e73\u5747AUC 0.8646\u00b10.0467\uff0c\u6bd4SwinUNETR\u57fa\u7ebf\u63d0\u534712.5%\uff1b\u4e0b\u53f6\u6a21\u578bAUC\u8fbe\u52300.88-0.90\uff0c\u4f18\u4e8eDL\u5bf9\u5e94\u6a21\u578b\uff08CNN: 0.76-0.79\uff09\u3002", "conclusion": "REN\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u5256\u5b66\u6307\u5bfc\u65b9\u6cd5\uff0c\u53ef\u8f7b\u677e\u6269\u5c55\u5230\u5176\u4ed6\u7ed3\u6784\u5316\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u3002"}}
{"id": "2510.04939", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04939", "abs": "https://arxiv.org/abs/2510.04939", "authors": ["Yuxi Liu", "Catherine Lalman", "Yimin Yang"], "title": "Unsupervised Active Learning via Natural Feature Progressive Framework", "comment": "Under review at IEEE TPAMI", "summary": "The effectiveness of modern deep learning models is predicated on the\navailability of large-scale, human-annotated datasets, a process that is\nnotoriously expensive and time-consuming. While Active Learning (AL) offers a\nstrategic solution by labeling only the most informative and representative\ndata, its iterative nature still necessitates significant human involvement.\nUnsupervised Active Learning (UAL) presents an alternative by shifting the\nannotation burden to a single, post-selection step. Unfortunately, prevailing\nUAL methods struggle to achieve state-of-the-art performance. These approaches\ntypically rely on local, gradient-based scoring for sample importance\nestimation, which not only makes them vulnerable to ambiguous and noisy data\nbut also hinders their capacity to select samples that adequately represent the\nfull data distribution. Moreover, their use of shallow, one-shot linear\nselection falls short of a true UAL paradigm. In this paper, we propose the\nNatural Feature Progressive Framework (NFPF), a UAL method that revolutionizes\nhow sample importance is measured. At its core, NFPF employs a Specific Feature\nLearning Machine (SFLM) to effectively quantify each sample's contribution to\nmodel performance. We further utilize the SFLM to define a powerful\nReconstruction Difference metric for initial sample selection. Our\ncomprehensive experiments show that NFPF significantly outperforms all\nestablished UAL methods and achieves performance on par with supervised AL\nmethods on vision datasets. Detailed ablation studies and qualitative\nvisualizations provide compelling evidence for NFPF's superior performance,\nenhanced robustness, and improved data distribution coverage.", "AI": {"tldr": "\u63d0\u51faNFPF\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u673a(SFLM)\u91cf\u5316\u6837\u672c\u91cd\u8981\u6027\uff0c\u5728\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60(UAL)\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u68af\u5ea6\u8bc4\u5206\uff0c\u5bb9\u6613\u53d7\u566a\u58f0\u5e72\u6270\u4e14\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u6570\u636e\u5206\u5e03", "method": "\u4f7f\u7528\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u673a(SFLM)\u91cf\u5316\u6837\u672c\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8d21\u732e\u5ea6\uff0c\u5e76\u5b9a\u4e49\u91cd\u6784\u5dee\u5f02\u6307\u6807\u8fdb\u884c\u521d\u59cb\u6837\u672c\u9009\u62e9", "result": "NFPF\u663e\u8457\u8d85\u8d8a\u73b0\u6709UAL\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd", "conclusion": "NFPF\u901a\u8fc7\u9769\u547d\u6027\u7684\u6837\u672c\u91cd\u8981\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5728\u65e0\u76d1\u7763\u4e3b\u52a8\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3001\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u66f4\u597d\u7684\u6570\u636e\u5206\u5e03\u8986\u76d6"}}
{"id": "2510.04947", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04947", "abs": "https://arxiv.org/abs/2510.04947", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion", "comment": "BIBM2025 accept, 8 pages, 4 figures", "summary": "Dual-view mammography, including craniocaudal (CC) and mediolateral oblique\n(MLO) projections, offers complementary anatomical views crucial for breast\ncancer diagnosis. However, in real-world clinical workflows, one view may be\nmissing, corrupted, or degraded due to acquisition errors or compression\nartifacts, limiting the effectiveness of downstream analysis. View-to-view\ntranslation can help recover missing views and improve lesion alignment. Unlike\nnatural images, this task in mammography is highly challenging due to large\nnon-rigid deformations and severe tissue overlap in X-ray projections, which\nobscure pixel-level correspondences. In this paper, we propose Column-Aware and\nImplicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view\ntranslation framework based on conditional diffusion model. To address\ncross-view structural misalignment, we first design a column-aware\ncross-attention mechanism that leverages the geometric property that\nanatomically corresponding regions tend to lie in similar column positions\nacross views. A Gaussian-decayed bias is applied to emphasize local column-wise\ncorrelations while suppressing distant mismatches. Furthermore, we introduce an\nimplicit 3D structure reconstruction module that back-projects noisy 2D latents\ninto a coarse 3D feature volume based on breast-view projection geometry. The\nreconstructed 3D structure is refined and injected into the denoising UNet to\nguide cross-view generation with enhanced anatomical awareness. Extensive\nexperiments demonstrate that CA3D-Diff achieves superior performance in\nbidirectional tasks, outperforming state-of-the-art methods in visual fidelity\nand structural consistency. Furthermore, the synthesized views effectively\nimprove single-view malignancy classification in screening settings,\ndemonstrating the practical value of our method in real-world diagnostics.", "AI": {"tldr": "\u63d0\u51faCA3D-Diff\u6846\u67b6\uff0c\u901a\u8fc7\u67f1\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u9690\u5f0f3D\u91cd\u5efa\u89e3\u51b3\u4e73\u817aX\u7ebf\u6444\u5f71\u53cc\u89c6\u56fe\u8f6c\u6362\u4e2d\u7684\u7ed3\u6784\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u56fe\u751f\u6210\u8d28\u91cf\u548c\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u4e73\u817aX\u7ebf\u6444\u5f71\u7684CC\u548cMLO\u89c6\u56fe\u53ef\u80fd\u7f3a\u5931\u6216\u635f\u574f\uff0c\u5f71\u54cd\u8bca\u65ad\u6548\u679c\u3002\u7531\u4e8eX\u5c04\u7ebf\u6295\u5f71\u4e2d\u7684\u5927\u53d8\u5f62\u548c\u7ec4\u7ec7\u91cd\u53e0\uff0c\u89c6\u56fe\u95f4\u8f6c\u6362\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u8bbe\u8ba1\u67f1\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u89e3\u5256\u5bf9\u5e94\u533a\u57df\u7684\u5217\u4f4d\u7f6e\u76f8\u4f3c\u6027\uff0c\u5f15\u5165\u9690\u5f0f3D\u7ed3\u6784\u91cd\u5efa\u6a21\u5757\u5c062D\u6f5c\u5728\u7279\u5f81\u53cd\u6295\u5f71\u4e3a3D\u7279\u5f81\u4f53\u79ef\uff0c\u589e\u5f3a\u89e3\u5256\u611f\u77e5\u3002", "result": "\u5728\u53cc\u5411\u89c6\u56fe\u8f6c\u6362\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5408\u6210\u7684\u89c6\u56fe\u6709\u6548\u63d0\u5347\u5355\u89c6\u56fe\u6076\u6027\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "CA3D-Diff\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4e73\u817aX\u7ebf\u6444\u5f71\u89c6\u56fe\u8f6c\u6362\u4e2d\u7684\u7ed3\u6784\u5bf9\u9f50\u95ee\u9898\uff0c\u5728\u771f\u5b9e\u8bca\u65ad\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.04961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04961", "abs": "https://arxiv.org/abs/2510.04961", "authors": ["Th\u00e9ophane Vallaeys", "Jakob Verbeek", "Matthieu Cord"], "title": "SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization", "comment": null, "summary": "Tokenizers are a key component of state-of-the-art generative image models,\nextracting the most important features from the signal while reducing data\ndimension and redundancy. Most current tokenizers are based on KL-regularized\nvariational autoencoders (KL-VAE), trained with reconstruction, perceptual and\nadversarial losses. Diffusion decoders have been proposed as a more principled\nalternative to model the distribution over images conditioned on the latent.\nHowever, matching the performance of KL-VAE still requires adversarial losses,\nas well as a higher decoding time due to iterative sampling. To address these\nlimitations, we introduce a new pixel diffusion decoder architecture for\nimproved scaling and training stability, benefiting from transformer components\nand GAN-free training. We use distillation to replicate the performance of the\ndiffusion decoder in an efficient single-step decoder. This makes SSDD the\nfirst diffusion decoder optimized for single-step reconstruction trained\nwithout adversarial losses, reaching higher reconstruction quality and faster\nsampling than KL-VAE. In particular, SSDD improves reconstruction FID from\n$0.87$ to $0.50$ with $1.4\\times$ higher throughput and preserve generation\nquality of DiTs with $3.8\\times$ faster sampling. As such, SSDD can be used as\na drop-in replacement for KL-VAE, and for building higher-quality and faster\ngenerative models.", "AI": {"tldr": "SSDD\u662f\u4e00\u79cd\u65b0\u7684\u50cf\u7d20\u6269\u6563\u89e3\u7801\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u5355\u6b65\u91cd\u5efa\uff0c\u65e0\u9700\u5bf9\u6297\u6027\u635f\u5931\uff0c\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u91c7\u6837\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u7684KL-VAE\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eKL-VAE\u7684tokenizer\u9700\u8981\u5bf9\u6297\u6027\u635f\u5931\uff0c\u800c\u6269\u6563\u89e3\u7801\u5668\u867d\u7136\u66f4\u7406\u8bba\u5316\u4f46\u9700\u8981\u8fed\u4ee3\u91c7\u6837\u5bfc\u81f4\u89e3\u7801\u65f6\u95f4\u8f83\u957f\u3002SSDD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5f15\u5165\u65b0\u7684\u50cf\u7d20\u6269\u6563\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5229\u7528transformer\u7ec4\u4ef6\u548c\u65e0GAN\u8bad\u7ec3\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u5c06\u6269\u6563\u89e3\u7801\u5668\u6027\u80fd\u590d\u5236\u5230\u9ad8\u6548\u7684\u5355\u6b65\u89e3\u7801\u5668\u4e2d\u3002", "result": "SSDD\u5c06\u91cd\u5efaFID\u4ece0.87\u63d0\u5347\u52300.50\uff0c\u541e\u5410\u91cf\u63d0\u9ad81.4\u500d\uff0c\u5728DiTs\u4e2d\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u91c7\u6837\u901f\u5ea6\u63d0\u9ad83.8\u500d\u3002", "conclusion": "SSDD\u53ef\u4ee5\u4f5c\u4e3aKL-VAE\u7684\u76f4\u63a5\u66ff\u4ee3\u54c1\uff0c\u7528\u4e8e\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u548c\u66f4\u5feb\u7684\u751f\u6210\u6a21\u578b\u3002"}}
{"id": "2510.04966", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04966", "abs": "https://arxiv.org/abs/2510.04966", "authors": ["Anna Chistyakova", "Mikhail Pautov"], "title": "ActiveMark: on watermarking of visual foundation models via massive activations", "comment": null, "summary": "Being trained on large and vast datasets, visual foundation models (VFMs) can\nbe fine-tuned for diverse downstream tasks, achieving remarkable performance\nand efficiency in various computer vision applications. The high computation\ncost of data collection and training motivates the owners of some VFMs to\ndistribute them alongside the license to protect their intellectual property\nrights. However, a dishonest user of the protected model's copy may illegally\nredistribute it, for example, to make a profit. As a consequence, the\ndevelopment of reliable ownership verification tools is of great importance\ntoday, since such methods can be used to differentiate between a redistributed\ncopy of the protected model and an independent model. In this paper, we propose\nan approach to ownership verification of visual foundation models by\nfine-tuning a small set of expressive layers of a VFM along with a small\nencoder-decoder network to embed digital watermarks into an internal\nrepresentation of a hold-out set of input images. Importantly, the watermarks\nembedded remain detectable in the functional copies of the protected model,\nobtained, for example, by fine-tuning the VFM for a particular downstream task.\nTheoretically and experimentally, we demonstrate that the proposed method\nyields a low probability of false detection of a non-watermarked model and a\nlow probability of false misdetection of a watermarked model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u5c42\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u5728\u5185\u90e8\u8868\u793a\u4e2d\u5d4c\u5165\u6570\u5b57\u6c34\u5370\uff0c\u53ef\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u540e\u7684\u6a21\u578b\u526f\u672c\u4e2d\u4fdd\u6301\u53ef\u68c0\u6d4b\u6027\u3002", "motivation": "\u4fdd\u62a4\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u77e5\u8bc6\u4ea7\u6743\uff0c\u9632\u6b62\u672a\u7ecf\u6388\u6743\u7684\u6a21\u578b\u518d\u5206\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u9760\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u5de5\u5177\u6765\u533a\u5206\u53d7\u4fdd\u62a4\u6a21\u578b\u7684\u518d\u5206\u53d1\u526f\u672c\u548c\u72ec\u7acb\u6a21\u578b\u3002", "method": "\u5fae\u8c03\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u4e00\u5c0f\u90e8\u5206\u8868\u8fbe\u5c42\uff0c\u7ed3\u5408\u5c0f\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u5728\u8f93\u5165\u56fe\u50cf\u7684\u5185\u90e8\u8868\u793a\u4e2d\u5d4c\u5165\u6570\u5b57\u6c34\u5370\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u975e\u6c34\u5370\u6a21\u578b\u7684\u8bef\u68c0\u6982\u7387\u548c\u6c34\u5370\u6a21\u578b\u7684\u6f0f\u68c0\u6982\u7387\u65b9\u9762\u90fd\u8868\u73b0\u8f83\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9a8c\u8bc1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6240\u6709\u6743\uff0c\u5373\u4f7f\u6a21\u578b\u7ecf\u8fc7\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u540e\uff0c\u5d4c\u5165\u7684\u6c34\u5370\u4ecd\u7136\u4fdd\u6301\u53ef\u68c0\u6d4b\u6027\u3002"}}
{"id": "2510.05006", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05006", "abs": "https://arxiv.org/abs/2510.05006", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition", "comment": "16 pages, 8 figures, 7 tables, under submission", "summary": "Deep neural networks (DNNs) are increasingly applied to safety-critical tasks\nin resource-constrained environments, such as video-based driver action and\nintention recognition. While last layer probabilistic deep learning (LL-PDL)\nmethods can detect out-of-distribution (OOD) instances, their performance\nvaries. As an alternative to last layer approaches, we propose extending\npre-trained DNNs with transformation layers to produce multiple latent\nrepresentations to estimate the uncertainty. We evaluate our latent uncertainty\nrepresentation (LUR) and repulsively trained LUR (RLUR) approaches against\neight PDL methods across four video-based driver action and intention\nrecognition datasets, comparing classification performance, calibration, and\nuncertainty-based OOD detection. We also contribute 28,000 frame-level action\nlabels and 1,194 video-level intention labels for the NuScenes dataset. Our\nresults show that LUR and RLUR achieve comparable in-distribution\nclassification performance to other LL-PDL approaches. For uncertainty-based\nOOD detection, LUR matches top-performing PDL methods while being more\nefficient to train and easier to tune than approaches that require Markov-Chain\nMonte Carlo sampling or repulsive training procedures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5LUR\u548cRLUR\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3DNN\u4e2d\u6dfb\u52a0\u53d8\u6362\u5c42\u751f\u6210\u591a\u4e2a\u6f5c\u5728\u8868\u793a\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u89c6\u9891\u9a7e\u9a76\u5458\u884c\u4e3a\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u5e94\u7528\u589e\u591a\uff0c\u4f46\u73b0\u6709\u6700\u540e\u4e00\u5c42\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u68c0\u6d4b\u5206\u5e03\u5916\u5b9e\u4f8b\u65f6\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\u9884\u8bad\u7ec3DNN\uff0c\u6dfb\u52a0\u53d8\u6362\u5c42\u751f\u6210\u591a\u4e2a\u6f5c\u5728\u8868\u793a\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51faLUR\u548c\u6392\u65a5\u8bad\u7ec3\u7684RLUR\u65b9\u6cd5\u3002", "result": "LUR\u548cRLUR\u5728\u5206\u5e03\u5185\u5206\u7c7b\u6027\u80fd\u4e0e\u5176\u4ed6\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u68c0\u6d4b\u65b9\u9762\u4e0e\u6700\u4f73\u65b9\u6cd5\u5339\u914d\uff0c\u4f46\u8bad\u7ec3\u66f4\u9ad8\u6548\u3001\u8c03\u53c2\u66f4\u5bb9\u6613\u3002", "conclusion": "LUR\u65b9\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2510.05015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05015", "abs": "https://arxiv.org/abs/2510.05015", "authors": ["Nabil Daiyan", "Md Rakibul Haque"], "title": "Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns", "comment": "5 pages, 11 figures, published on 2024 2nd International Conference\n  on Information and Communication Technology (ICICT 2024)", "summary": "Parkinson's disease (PD) is a progressive neurodegenerative condition\ncharacterized by the death of dopaminergic neurons, leading to various movement\ndisorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,\nyet traditional diagnostic methods are often cumbersome and costly. In this\nstudy, a machine learning-based approach is proposed using hand-drawn spiral\nand wave images as potential biomarkers for PD detection. Our methodology\nleverages convolutional neural networks (CNNs), transfer learning, and\nattention mechanisms to improve model performance and resilience against\noverfitting. To enhance the diversity and richness of both spiral and wave\ncategories, the training dataset undergoes augmentation to increase the number\nof images. The proposed architecture comprises three phases: utilizing\npre-trained CNNs, incorporating custom convolutional layers, and ensemble\nvoting. Employing hard voting further enhances performance by aggregating\npredictions from multiple models. Experimental results show promising accuracy\nrates. For spiral images, weighted average precision, recall, and F1-score are\n90%, and for wave images, they are 96.67%. After combining the predictions\nthrough ensemble hard voting, the overall accuracy is 93.3%. These findings\nunderscore the potential of machine learning in early PD diagnosis, offering a\nnon-invasive and cost-effective solution to improve patient outcomes.", "AI": {"tldr": "\u4f7f\u7528\u624b\u7ed8\u87ba\u65cb\u7ebf\u548c\u6ce2\u6d6a\u7ebf\u56fe\u50cf\u4f5c\u4e3a\u5e15\u91d1\u68ee\u75c5\u68c0\u6d4b\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u901a\u8fc7CNN\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\u6784\u5efa\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u83b7\u5f97\u4e8693.3%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002", "motivation": "\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u5bf9\u9884\u9632\u4e0d\u826f\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u7e41\u7410\u4e14\u6602\u8d35\uff0c\u9700\u8981\u5f00\u53d1\u975e\u4fb5\u5165\u6027\u3001\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u589e\u52a0\u8bad\u7ec3\u56fe\u50cf\u591a\u6837\u6027\uff0c\u6784\u5efa\u5305\u542b\u9884\u8bad\u7ec3CNN\u3001\u81ea\u5b9a\u4e49\u5377\u79ef\u5c42\u548c\u96c6\u6210\u6295\u7968\u7684\u4e09\u9636\u6bb5\u67b6\u6784\u3002", "result": "\u87ba\u65cb\u56fe\u50cf\u52a0\u6743\u5e73\u5747\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e3a90%\uff0c\u6ce2\u6d6a\u56fe\u50cf\u4e3a96.67%\uff0c\u901a\u8fc7\u96c6\u6210\u786c\u6295\u7968\u540e\u6574\u4f53\u51c6\u786e\u7387\u8fbe\u523093.3%\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u5e15\u91d1\u68ee\u75c5\u65e9\u671f\u8bca\u65ad\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2510.05034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05034", "abs": "https://arxiv.org/abs/2510.05034", "authors": ["Yunlong Tang", "Jing Bi", "Pinxin Liu", "Zhenyu Pan", "Zhangyun Tan", "Qianxiang Shen", "Jiani Liu", "Hang Hua", "Junjia Guo", "Yunzhong Xiao", "Chao Huang", "Zhiyuan Wang", "Susan Liang", "Xinyi Liu", "Yizhi Song", "Yuhe Nie", "Jia-Xing Zhong", "Bozheng Li", "Daiqing Qi", "Ziyun Zeng", "Ali Vosoughi", "Luchuan Song", "Zeliang Zhang", "Daiki Shimada", "Han Liu", "Jiebo Luo", "Chenliang Xu"], "title": "Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models", "comment": "The 1st version", "summary": "Video understanding represents the most challenging frontier in computer\nvision, requiring models to reason about complex spatiotemporal relationships,\nlong-term dependencies, and multimodal evidence. The recent emergence of\nVideo-Large Multimodal Models (Video-LMMs), which integrate visual encoders\nwith powerful decoder-based language models, has demonstrated remarkable\ncapabilities in video understanding tasks. However, the critical phase that\ntransforms these models from basic perception systems into sophisticated\nreasoning engines, post-training, remains fragmented across the literature.\nThis survey provides the first comprehensive examination of post-training\nmethodologies for Video-LMMs, encompassing three fundamental pillars:\nsupervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)\nfrom verifiable objectives, and test-time scaling (TTS) through enhanced\ninference computation. We present a structured taxonomy that clarifies the\nroles, interconnections, and video-specific adaptations of these techniques,\naddressing unique challenges such as temporal localization, spatiotemporal\ngrounding, long video efficiency, and multimodal evidence integration. Through\nsystematic analysis of representative methods, we synthesize key design\nprinciples, insights, and evaluation protocols while identifying critical open\nchallenges in reward design, scalability, and cost-performance optimization. We\nfurther curate essential benchmarks, datasets, and metrics to facilitate\nrigorous assessment of post-training effectiveness. This survey aims to provide\nresearchers and practitioners with a unified framework for advancing Video-LMM\ncapabilities. Additional resources and updates are maintained at:\nhttps://github.com/yunlong10/Awesome-Video-LMM-Post-Training", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6db5\u76d6\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e09\u5927\u652f\u67f1\uff0c\u4e3a\u63d0\u5347\u89c6\u9891\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u89c6\u9891\u7406\u89e3\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u524d\u6cbf\u9886\u57df\uff0c\u9700\u8981\u6a21\u578b\u5904\u7406\u590d\u6742\u7684\u65f6\u7a7a\u5173\u7cfb\u548c\u957f\u671f\u4f9d\u8d56\u3002\u867d\u7136\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5c55\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5176\u540e\u8bad\u7ec3\u9636\u6bb5\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u4e09\u79cd\u6838\u5fc3\u540e\u8bad\u7ec3\u65b9\u6cd5\uff1a\u5e26\u601d\u7ef4\u94fe\u7684\u76d1\u7763\u5fae\u8c03\u3001\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u76ee\u6807\u7684\u5f3a\u5316\u5b66\u4e60\u3001\u901a\u8fc7\u589e\u5f3a\u63a8\u7406\u8ba1\u7b97\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002", "result": "\u7efc\u5408\u5206\u6790\u4e86\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u63d0\u70bc\u51fa\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3001\u89c1\u89e3\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u8bc6\u522b\u4e86\u5956\u52b1\u8bbe\u8ba1\u3001\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6027\u80fd\u4f18\u5316\u7b49\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u63a8\u8fdb\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u6574\u7406\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u4ee5\u4fc3\u8fdb\u4e25\u683c\u8bc4\u4f30\u3002"}}
{"id": "2510.05051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05051", "abs": "https://arxiv.org/abs/2510.05051", "authors": ["Rohit Jayanti", "Swayam Agrawal", "Vansh Garg", "Siddharth Tourani", "Muhammad Haris Khan", "Sourav Garg", "Madhava Krishna"], "title": "SegMASt3R: Geometry Grounded Segment Matching", "comment": "Accepted to The Thirty-Ninth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2025) as a Spotlight (top 3.5%)", "summary": "Segment matching is an important intermediate task in computer vision that\nestablishes correspondences between semantically or geometrically coherent\nregions across images. Unlike keypoint matching, which focuses on localized\nfeatures, segment matching captures structured regions, offering greater\nrobustness to occlusions, lighting variations, and viewpoint changes. In this\npaper, we leverage the spatial understanding of 3D foundation models to tackle\nwide-baseline segment matching, a challenging setting involving extreme\nviewpoint shifts. We propose an architecture that uses the inductive bias of\nthese 3D foundation models to match segments across image pairs with up to 180\ndegree view-point change. Extensive experiments show that our approach\noutperforms state-of-the-art methods, including the SAM2 video propagator and\nlocal feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++\nand Replica datasets. We further demonstrate benefits of the proposed model on\nrelevant downstream tasks, including 3D instance segmentation and image-goal\nnavigation. Project Page: https://segmast3r.github.io/", "AI": {"tldr": "\u5229\u75283D\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u89e3\u51b3\u5bbd\u57fa\u7ebf\u5206\u5272\u5339\u914d\u95ee\u9898\uff0c\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4e0b\u5b9e\u73b0\u5206\u5272\u5339\u914d\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd530%", "motivation": "\u5206\u5272\u5339\u914d\u6bd4\u5173\u952e\u70b9\u5339\u914d\u66f4\u80fd\u6355\u6349\u7ed3\u6784\u5316\u533a\u57df\uff0c\u5bf9\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u548c\u89c6\u89d2\u53d8\u5316\u5177\u6709\u66f4\u5f3a\u9c81\u68d2\u6027\u3002\u5bbd\u57fa\u7ebf\u5206\u5272\u5339\u914d\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4e0b\u5177\u6709\u6311\u6218\u6027", "method": "\u63d0\u51fa\u4e00\u79cd\u5229\u75283D\u57fa\u7840\u6a21\u578b\u5f52\u7eb3\u504f\u7f6e\u7684\u67b6\u6784\uff0c\u80fd\u591f\u5339\u914d\u89c6\u89d2\u53d8\u5316\u9ad8\u8fbe180\u5ea6\u7684\u56fe\u50cf\u5bf9\u4e2d\u7684\u5206\u5272\u533a\u57df", "result": "\u5728ScanNet++\u548cReplica\u6570\u636e\u96c6\u4e0a\uff0cAUPRC\u6307\u6807\u6bd4SAM2\u89c6\u9891\u4f20\u64ad\u5668\u548c\u5c40\u90e8\u7279\u5f81\u5339\u914d\u65b9\u6cd5\u63d0\u5347\u9ad8\u8fbe30%\uff0c\u5e76\u57283D\u5b9e\u4f8b\u5206\u5272\u548c\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf", "conclusion": "\u5229\u75283D\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u5bbd\u57fa\u7ebf\u5206\u5272\u5339\u914d\u95ee\u9898\uff0c\u5728\u6781\u7aef\u89c6\u89d2\u53d8\u5316\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u5206\u5272\u5339\u914d"}}
{"id": "2510.05053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05053", "abs": "https://arxiv.org/abs/2510.05053", "authors": ["Mohammad-Ali Mahmoudpour", "Saeed Mahmoudpour"], "title": "No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference", "comment": null, "summary": "Contrast change is an important factor that affects the quality of images.\nDuring image capturing, unfavorable lighting conditions can cause contrast\nchange and visual quality loss. While various methods have been proposed to\nassess the quality of images under different distortions such as blur and\nnoise, contrast distortion has been largely overlooked as its visual impact and\nproperties are different from other conventional types of distortions. In this\npaper, we propose a no-reference image quality assessment (NR-IQA) metric for\ncontrast-distorted images. Using a set of contrast enhancement algorithms, we\naim to generate pseudo-reference images that are visually close to the actual\nreference image, such that the NR problem is transformed to a Full-reference\n(FR) assessment with higher accuracy. To this end, a large dataset of\ncontrast-enhanced images is produced to train a classification network that can\nselect the most suitable contrast enhancement algorithm based on image content\nand distortion for pseudo-reference image generation. Finally, the evaluation\nis performed in the FR manner to assess the quality difference between the\ncontrast-enhanced (pseudoreference) and degraded images. Performance evaluation\nof the proposed method on three databases containing contrast distortions\n(CCID2014, TID2013, and CSIQ), indicates the promising performance of the\nproposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5bf9\u6bd4\u5ea6\u5931\u771f\u56fe\u50cf\u7684\u65e0\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u5408\u9002\u7684\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u7b97\u6cd5\u751f\u6210\u4f2a\u53c2\u8003\u56fe\u50cf\uff0c\u5c06NR-IQA\u95ee\u9898\u8f6c\u5316\u4e3aFR-IQA\u95ee\u9898\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7cbe\u5ea6\u3002", "motivation": "\u5bf9\u6bd4\u5ea6\u53d8\u5316\u662f\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4f46\u5728\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5e38\u88ab\u5ffd\u89c6\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6a21\u7cca\u548c\u566a\u58f0\u7b49\u5931\u771f\uff0c\u800c\u5bf9\u6bd4\u5ea6\u5931\u771f\u7684\u89c6\u89c9\u5f71\u54cd\u548c\u7279\u6027\u4e0e\u4f20\u7edf\u5931\u771f\u4e0d\u540c\u3002", "method": "\u4f7f\u7528\u4e00\u7ec4\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u7b97\u6cd5\u751f\u6210\u89c6\u89c9\u4e0a\u63a5\u8fd1\u771f\u5b9e\u53c2\u8003\u56fe\u50cf\u7684\u4f2a\u53c2\u8003\u56fe\u50cf\uff0c\u8bad\u7ec3\u5206\u7c7b\u7f51\u7edc\u6839\u636e\u56fe\u50cf\u5185\u5bb9\u548c\u5931\u771f\u9009\u62e9\u6700\u5408\u9002\u7684\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u7b97\u6cd5\uff0c\u6700\u7ec8\u4ee5\u5168\u53c2\u8003\u65b9\u5f0f\u8bc4\u4f30\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u56fe\u50cf\u4e0e\u9000\u5316\u56fe\u50cf\u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u5f02\u3002", "result": "\u5728\u5305\u542b\u5bf9\u6bd4\u5ea6\u5931\u771f\u7684\u4e09\u4e2a\u6570\u636e\u5e93\uff08CCID2014\u3001TID2013\u548cCSIQ\uff09\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u524d\u666f\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5c06\u65e0\u53c2\u8003\u95ee\u9898\u8f6c\u5316\u4e3a\u5168\u53c2\u8003\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u5bf9\u6bd4\u5ea6\u5931\u771f\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u6570\u636e\u5e93\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2510.05071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05071", "abs": "https://arxiv.org/abs/2510.05071", "authors": ["Debojyoti Ghosh", "Soumya K Ghosh", "Adrijit Goswami"], "title": "Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces", "comment": null, "summary": "Efficient and accurate classification of waste and industrial surface defects\nis essential for ensuring sustainable waste management and maintaining high\nstandards in quality control. This paper introduces the Neuroplastic Modular\nClassifier, a novel hybrid architecture designed for robust and adaptive image\nclassification in dynamic environments. The model combines a ResNet-50 backbone\nfor localized feature extraction with a Vision Transformer (ViT) to capture\nglobal semantic context. Additionally, FAISS-based similarity retrieval is\nincorporated to provide a memory-like reference to previously encountered data,\nenriching the model's feature space. A key innovation of our architecture is\nthe neuroplastic modular design composed of expandable, learnable blocks that\ndynamically grow during training when performance plateaus. Inspired by\nbiological learning systems, this mechanism allows the model to adapt to data\ncomplexity over time, improving generalization. Beyond garbage classification,\nwe validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),\nwhich involves industrial defect detection on metal surfaces. Experimental\nresults across domains show that the proposed architecture outperforms\ntraditional static models in both accuracy and adaptability. The Neuroplastic\nModular Classifier offers a scalable, high-performance solution for real-world\nimage classification, with strong applicability in both environmental and\nindustrial domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u67b6\u6784\u2014\u2014\u795e\u7ecf\u53ef\u5851\u6027\u6a21\u5757\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u56fe\u50cf\u5206\u7c7b\uff0c\u5728\u5783\u573e\u5206\u7c7b\u548c\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9700\u8981\u9ad8\u6548\u51c6\u786e\u7684\u5e9f\u7269\u5206\u7c7b\u548c\u5de5\u4e1a\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u5e9f\u7269\u7ba1\u7406\u548c\u9ad8\u8d28\u91cf\u63a7\u5236\u6807\u51c6\u3002", "method": "\u7ed3\u5408ResNet-50\u8fdb\u884c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u548cVision Transformer\u6355\u83b7\u5168\u5c40\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u96c6\u6210FAISS\u76f8\u4f3c\u6027\u68c0\u7d22\u63d0\u4f9b\u8bb0\u5fc6\u53c2\u8003\uff0c\u91c7\u7528\u795e\u7ecf\u53ef\u5851\u6027\u6a21\u5757\u8bbe\u8ba1\uff0c\u5305\u542b\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u5757\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u6027\u80fd\u505c\u6ede\u65f6\u52a8\u6001\u589e\u957f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u6a21\u578b\uff0c\u5728\u73af\u5883\u548c\u5de5\u4e1a\u9886\u57df\u90fd\u5177\u6709\u5f3a\u9002\u7528\u6027\u3002", "conclusion": "\u795e\u7ecf\u53ef\u5851\u6027\u6a21\u5757\u5206\u7c7b\u5668\u4e3a\u73b0\u5b9e\u4e16\u754c\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u73af\u5883\u548c\u5de5\u4e1a\u9886\u57df\u90fd\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.05091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05091", "abs": "https://arxiv.org/abs/2510.05091", "authors": ["Le Zhuo", "Songhao Han", "Yuandong Pu", "Boxiang Qiu", "Sayak Paul", "Yue Liao", "Yihao Liu", "Jie Shao", "Xi Chen", "Si Liu", "Hongsheng Li"], "title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals", "comment": "Project page: https://structvisuals.github.io", "summary": "While modern visual generation models excel at creating aesthetically\npleasing natural images, they struggle with producing or editing structured\nvisuals like charts, diagrams, and mathematical figures, which demand\ncomposition planning, text rendering, and multimodal reasoning for factual\nfidelity. To address this, we present the first comprehensive, systematic\ninvestigation of this domain, encompassing data construction, model training,\nand an evaluation benchmark. First, we construct a large-scale dataset of 1.3\nmillion high-quality structured image pairs derived from executable drawing\nprograms and augmented with chain-of-thought reasoning annotations. Building on\nit, we train a unified model that integrates a VLM with FLUX.1 Kontext via a\nlightweight connector for enhanced multimodal understanding. A three-stage\ntraining curriculum enables progressive feature alignment, knowledge infusion,\nand reasoning-augmented generation, further boosted by an external reasoner at\ninference time. Finally, we introduce StructBench, a novel benchmark for\ngeneration and editing with over 1,700 challenging instances, and an\naccompanying evaluation metric, StructScore, which employs a multi-round Q\\&A\nprotocol to assess fine-grained factual accuracy. Evaluations of 15 models\nreveal that even leading closed-source systems remain far from satisfactory.\nOur model attains strong editing performance, and inference-time reasoning\nyields consistent gains across diverse architectures. By releasing the dataset,\nmodel, and benchmark, we aim to advance unified multimodal foundations for\nstructured visuals.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u7ed3\u6784\u5316\u89c6\u89c9\u5185\u5bb9\uff08\u5982\u56fe\u8868\u3001\u56fe\u8868\u3001\u6570\u5b66\u56fe\u5f62\uff09\u751f\u6210\u548c\u7f16\u8f91\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6570\u636e\u96c6\u6784\u5efa\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u4ee3\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u521b\u5efa\u7f8e\u89c2\u7684\u81ea\u7136\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u751f\u6210\u6216\u7f16\u8f91\u9700\u8981\u7ec4\u5408\u89c4\u5212\u3001\u6587\u672c\u6e32\u67d3\u548c\u591a\u6a21\u6001\u63a8\u7406\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u5185\u5bb9\u65f6\u5b58\u5728\u56f0\u96be\u3002", "method": "\u6784\u5efa\u4e86130\u4e07\u5bf9\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u96c6\u6210VLM\u548cFLUX.1 Kontext\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5916\u90e8\u63a8\u7406\u5668\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u4e8615\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5373\u4f7f\u662f\u9886\u5148\u7684\u95ed\u6e90\u7cfb\u7edf\u4e5f\u8fdc\u672a\u8fbe\u5230\u6ee1\u610f\u6c34\u5e73\u3002\u63d0\u51fa\u7684\u6a21\u578b\u5728\u7f16\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u63a8\u7406\u65f6\u63a8\u7406\u5728\u4e0d\u540c\u67b6\u6784\u4e2d\u5e26\u6765\u4e00\u81f4\u589e\u76ca\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u63a8\u8fdb\u7ed3\u6784\u5316\u89c6\u89c9\u5185\u5bb9\u7684\u591a\u6a21\u6001\u57fa\u7840\u7814\u7a76\u3002"}}
{"id": "2510.05093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05093", "abs": "https://arxiv.org/abs/2510.05093", "authors": ["Tingting Liao", "Chongjian Ge", "Guangyi Liu", "Hao Li", "Yi Zhou"], "title": "Character Mixing for Video Generation", "comment": null, "summary": "Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where\ncharacters interact naturally across different worlds? We study inter-character\ninteraction in text-to-video generation, where the key challenge is to preserve\neach character's identity and behaviors while enabling coherent cross-context\ninteraction. This is difficult because characters may never have coexisted and\nbecause mixing styles often causes style delusion, where realistic characters\nappear cartoonish or vice versa. We introduce a framework that tackles these\nissues with Cross-Character Embedding (CCE), which learns identity and\nbehavioral logic across multimodal sources, and Cross-Character Augmentation\n(CCA), which enriches training with synthetic co-existence and mixed-style\ndata. Together, these techniques allow natural interactions between previously\nuncoexistent characters without losing stylistic fidelity. Experiments on a\ncurated benchmark of cartoons and live-action series with 10 characters show\nclear improvements in identity preservation, interaction quality, and\nrobustness to style delusion, enabling new forms of generative\nstorytelling.Additional results and videos are available on our project page:\nhttps://tingtingliao.github.io/mimix/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u89d2\u8272\u4ea4\u4e92\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u89d2\u8272\u5d4c\u5165\u548c\u589e\u5f3a\u6280\u672f\uff0c\u89e3\u51b3\u4e0d\u540c\u4e16\u754c\u89d2\u8272\u81ea\u7136\u4ea4\u4e92\u65f6\u7684\u8eab\u4efd\u4fdd\u6301\u548c\u98ce\u683c\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u751f\u6210\u4e0d\u540c\u4e16\u754c\u89d2\u8272\uff08\u5982Mr. Bean\u548cTom and Jerry\uff09\u81ea\u7136\u4ea4\u4e92\u7684\u89c6\u9891\uff0c\u5173\u952e\u6311\u6218\u662f\u4fdd\u6301\u89d2\u8272\u8eab\u4efd\u548c\u884c\u4e3a\u903b\u8f91\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u4e0a\u4e0b\u6587\u8fde\u8d2f\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u4ea4\u53c9\u89d2\u8272\u5d4c\u5165\uff08CCE\uff09\u4ece\u591a\u6a21\u6001\u6e90\u5b66\u4e60\u8eab\u4efd\u548c\u884c\u4e3a\u903b\u8f91\uff0c\u7ed3\u5408\u4ea4\u53c9\u89d2\u8272\u589e\u5f3a\uff08CCA\uff09\u901a\u8fc7\u5408\u6210\u5171\u5b58\u548c\u6df7\u5408\u98ce\u683c\u6570\u636e\u4e30\u5bcc\u8bad\u7ec3\u3002", "result": "\u5728\u5305\u542b10\u4e2a\u89d2\u8272\u7684\u5361\u901a\u548c\u771f\u4eba\u5267\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u4ea4\u4e92\u8d28\u91cf\u548c\u98ce\u683c\u534f\u8c03\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u660e\u663e\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5148\u524d\u4e0d\u5171\u5b58\u89d2\u8272\u4e4b\u95f4\u7684\u81ea\u7136\u4ea4\u4e92\u800c\u4e0d\u635f\u5931\u98ce\u683c\u4fdd\u771f\u5ea6\uff0c\u4e3a\u751f\u6210\u5f0f\u53d9\u4e8b\u5f00\u8f9f\u4e86\u65b0\u5f62\u5f0f\u3002"}}
{"id": "2510.05094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05094", "abs": "https://arxiv.org/abs/2510.05094", "authors": ["Ziqi Huang", "Ning Yu", "Gordon Chen", "Haonan Qiu", "Paul Debevec", "Ziwei Liu"], "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation", "comment": "Project page: https://eyeline-labs.github.io/VChain Code:\n  https://github.com/Eyeline-Labs/VChain", "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.", "AI": {"tldr": "VChain\u662f\u4e00\u4e2a\u63a8\u7406\u65f6\u89c6\u89c9\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u5173\u952e\u5e27\u6765\u6307\u5bfc\u89c6\u9891\u751f\u6210\uff0c\u63d0\u5347\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u590d\u6742\u52a8\u6001\u548c\u8fde\u8d2f\u56e0\u679c\u94fe\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u591a\u6a21\u6001\u6a21\u578b\u5177\u5907\u5f3a\u5927\u7684\u89c6\u89c9\u72b6\u6001\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u5c06\u4e24\u8005\u4f18\u52bf\u7ed3\u5408\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u7a00\u758f\u5173\u952e\u5e27\u4f5c\u4e3a\u5feb\u7167\uff0c\u7136\u540e\u57fa\u4e8e\u8fd9\u4e9b\u5173\u952e\u5e27\u5bf9\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u8fdb\u884c\u7a00\u758f\u63a8\u7406\u65f6\u5fae\u8c03\u3002", "result": "\u5728\u590d\u6742\u591a\u6b65\u9aa4\u573a\u666f\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVChain\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u89c6\u9891\u7684\u8d28\u91cf\u3002", "conclusion": "VChain\u901a\u8fc7\u6ce8\u5165\u591a\u6a21\u6001\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e86\u8c03\u4f18\u9ad8\u6548\u3001\u5f00\u9500\u6700\u5c0f\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\u63d0\u5347\u3002"}}
{"id": "2510.04673", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.04673", "abs": "https://arxiv.org/abs/2510.04673", "authors": ["Chan Hee Song", "Yiwen Song", "Palash Goyal", "Yu Su", "Oriana Riva", "Hamid Palangi", "Tomas Pfister"], "title": "Watch and Learn: Learning to Use Computers from Online Videos", "comment": null, "summary": "Computer use agents (CUAs) need to plan task workflows grounded in diverse,\never-changing applications and environments, but learning is hindered by the\nscarcity of large-scale, high-quality training data in the target application.\nExisting datasets are domain-specific, static, and costly to annotate, while\ncurrent synthetic data generation methods often yield simplistic or misaligned\ntask demonstrations. To address these limitations, we introduce Watch & Learn\n(W&L), a framework that converts human demonstration videos readily available\non the Internet into executable UI trajectories at scale. Instead of directly\ngenerating trajectories or relying on ad hoc reasoning heuristics, we cast the\nproblem as an inverse dynamics objective: predicting the user's action from\nconsecutive screen states. This formulation reduces manual engineering, is\neasier to learn, and generalizes more robustly across applications. Concretely,\nwe develop an inverse dynamics labeling pipeline with task-aware video\nretrieval, generate over 53k high-quality trajectories from raw web videos, and\ndemonstrate that these trajectories improve CUAs both as in-context\ndemonstrations and as supervised training data. On the challenging OSWorld\nbenchmark, UI trajectories extracted with W&L consistently enhance both\ngeneral-purpose and state-of-the-art frameworks in-context, and deliver\nstronger gains for open-source models under supervised training. These results\nhighlight web-scale human demonstration videos as a practical and scalable\nfoundation for advancing CUAs towards real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86Watch & Learn\u6846\u67b6\uff0c\u5c06\u4e92\u8054\u7f51\u4e0a\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u5927\u89c4\u6a21\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684UI\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u9700\u8981\u57fa\u4e8e\u591a\u6837\u5316\u3001\u4e0d\u65ad\u53d8\u5316\u7684\u5e94\u7528\u548c\u73af\u5883\u6765\u89c4\u5212\u4efb\u52a1\u5de5\u4f5c\u6d41\uff0c\u4f46\u76ee\u6807\u5e94\u7528\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u9886\u57df\u7279\u5b9a\u3001\u9759\u6001\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7b49\u9650\u5236\u3002", "method": "\u91c7\u7528\u9006\u52a8\u529b\u5b66\u76ee\u6807\u6846\u67b6\uff0c\u4ece\u8fde\u7eed\u5c4f\u5e55\u72b6\u6001\u9884\u6d4b\u7528\u6237\u52a8\u4f5c\uff0c\u5f00\u53d1\u4e86\u5305\u542b\u4efb\u52a1\u611f\u77e5\u89c6\u9891\u68c0\u7d22\u7684\u9006\u52a8\u529b\u5b66\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u4ece\u539f\u59cb\u7f51\u7edc\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "result": "\u751f\u6210\u4e86\u8d85\u8fc753k\u6761\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cW&L\u63d0\u53d6\u7684UI\u8f68\u8ff9\u663e\u8457\u63d0\u5347\u4e86\u901a\u7528\u548c\u6700\u5148\u8fdb\u6846\u67b6\u7684\u4e0a\u4e0b\u6587\u6027\u80fd\uff0c\u5e76\u4e3a\u5f00\u6e90\u6a21\u578b\u7684\u76d1\u7763\u8bad\u7ec3\u5e26\u6765\u66f4\u5f3a\u589e\u76ca\u3002", "conclusion": "\u7f51\u7edc\u89c4\u6a21\u7684\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u662f\u63a8\u8fdb\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u5411\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
