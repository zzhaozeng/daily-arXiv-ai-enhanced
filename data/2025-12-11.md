<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 27]
- [cs.AI](#cs.AI) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种新的抽象压缩方法ACoRN，通过细粒度分类和两步训练提高检索增强生成中对噪音的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 检索文档常包含与查询无关或误导性信息，抽象压缩器在长上下文中容易忽略重要信息。

Method: 提出ACoRN，采用离线数据增强和微调两个训练步骤，增强压缩器对检索噪音的鲁棒性，并生成以关键信息为中心的摘要。

Result: 实验证明，使用ACoRN训练的T5-large在EM和F1评分上有所提升，并在保留答案字符串方面表现良好。

Conclusion: ACoRN在处理包含降低准确性文档的数据集上表现优异，适用于现实世界场景。

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [2] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 本文介绍了一种有针对性的强化学习框架，旨在减少大型语言模型中的幻觉，提高其可靠性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大型语言模型的复杂推理能力，但也增加了幻觉的倾向，因此需要在能力和可靠性之间取得平衡。

Method: 通过创建基于TriviaQA的开放式问答训练集以解决外部幻觉，并利用FineWeb的长文本进行事实基础奖励以解决内部幻觉。此外，通过奖励模型拒绝回答无法回答的问题来增强谨慎性。

Result: 实验表明，该方法在多个基准测试中显著提升了性能，大幅减少了两种类型的幻觉。

Conclusion: 该研究为解决先进推理和事实可信度之间的关键张力提供了实用框架，使大型语言模型更加可靠和高效。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [3] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 提出了一个知识引导的大语言模型（KG-LLM）用于儿科牙科临床记录解读和安全抗生素处方推荐。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的临床决策支持系统在处理非结构化牙科叙述、不完整的影像描述和复杂的安全约束方面存在困难。

Method: KG-LLM框架集成了儿科牙科知识图谱、检索增强生成（RAG）和多阶段安全验证流程，通过临床NER/RE模块提取实体和关系，从知识图谱中检索相关指南和历史病例，并使用双层安全验证机制。

Result: 在32,000个去标识化的儿科牙科就诊记录上，KG-LLM在记录理解性能（F1: 0.914 vs. 0.867）、药物-剂量-持续时间准确性（Top-1: 0.782 vs. 0.716）上优于Llama-2临床基线，并将不安全的抗生素建议减少了50%。

Conclusion: 知识图谱、RAG和安全模块各自对临床可靠性和可解释性有显著贡献，KG-LLM在儿科牙科抗生素推荐方面表现出色。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [4] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文提出了GraphRAG中LLMs的结构知识处理问题，通过两个轻量可解释性指标(PRD和SAS)分析幻觉模式，并开发了GGA后处理幻觉检测器。


<details>
  <summary>Details</summary>
Motivation: LLMs在GraphRAG中难以有效利用知识图谱的结构信息，导致与检索知识不一致的幻觉问题，需要分析模型内部知识处理机制。

Method: 提出两个新指标：路径依赖度(PRD)测量最短路径三元组过度依赖，语义对齐分数(SAS)评估内部表征与检索知识对齐程度；开发了GGA后处理幻觉检测器。

Result: 在知识问答任务中发现高PRD和低SAS分数对应特定失败模式；GGA检测器在AUC和F1指标上优于现有基线方法。

Conclusion: 通过机制可解释性分析揭示了LLMs结构局限性导致幻觉的机理，为未来更可靠的GraphRAG系统设计提供依据。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [5] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 该研究通过心理测量工具MMPI，评估了大型语言模型（LLMs）在模拟用户指定人格特质和态度方面的潜力，并提出了MindShift这一新的评估基准。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs是否能准确反映和吸收用户的人格特质与态度，并评估其在心理适应性方面的表现。

Method: 采用明尼苏达多项人格测验（MMPI）的变体，创建了具有不同特质强度的人物角色，通过心理导向的提示来评估LLMs的行为。

Result: 结果显示，LLMs在角色感知方面持续改进，且不同模型类型和家族在心理测量评估中的响应存在显著差异。

Conclusion: 研究揭示了LLMs在模拟类人个性特质方面的可变性，并表明训练数据集和对齐技术的进步提升了其表现。MindShift评估工具将公开。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [6] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: 提出CORE概念优先交互层，通过持久化语义状态提升多轮对话稳定性，减少token使用。


<details>
  <summary>Details</summary>
Motivation: 大模型在多轮交互中因缺乏持久内部表征导致意图重建困难、提示膨胀和推理不一致。

Method: 构建包含通用认知算子库和持久化Local Concept的轻量级中间层，分离概念推理与语言生成。

Result: 原型测试显示累计提示token减少42%（实验条件数据）。

Conclusion: CORE为模型无关的稳定多轮系统提供可扩展新范式。

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [7] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出一种无需训练、自适应的稀疏注意力机制TCA-Attention，以解决长序列处理中的计算和内存挑战。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在长序列下带来显著计算和内存挑战，现有稀疏注意力和KV缓存压缩方法存在局限。

Method: TCA-Attention包含两个轻量级阶段：离线校准阶段和在线标记选择阶段，通过一次前向传递和轻量级冗余度量自适应保留核心上下文标记。

Result: 在128K上下文长度下，TCA-Attention实现了2.8倍加速，KV缓存减少61%，性能与全注意力机制相当。

Conclusion: TCA-Attention为高效长上下文推理提供了一个实用的即插即用解决方案，无需参数更新或架构更改。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [8] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 本文探讨了机器生成文本检测系统中的潜在偏见，发现不同系统在性别、种族/族裔、英语语言学习者（ELL）身份和经济状况等属性上存在不一致的偏见，并指出人类在检测任务中表现差但无明显偏见。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成技术的发展，机器生成文本检测成为一个重要课题，但其潜在的负面影响和偏见问题尚未充分研究。

Method: 研究者整理了一个学生作文数据集，评估了16种不同的检测系统在四个属性上的偏见：性别、种族/族裔、英语语言学习者（ELL）身份和经济状况。使用基于回归的模型和子群分析评估这些属性的显著性和效应强度，并进行人工标注。

Result: 发现不同系统的偏见不一致，但存在几个关键问题：一些模型倾向于将弱势群体分类为机器生成，英语语言学习者（ELL）和经济状况较差的学生文章更易被误分类，非白人ELL文章相对于白人ELL文章更可能被误分类。人类在检测任务中表现差但无明显偏见。

Conclusion: 机器生成文本检测系统存在显著的偏见问题，尤其是在识别弱势群体文章时。虽然人类在检测任务中表现不佳，但没有表现出明显的偏见。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [9] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个支持持续学习的新型路由框架，通过为每个策略训练独立的预测模型，实现高效、低成本的新策略整合，同时利用多表示法提升任务复杂度捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有路由方法在面临新策略时需完全重新训练，且单一输入表示法难以充分捕捉任务复杂性，导致泛化能力差和次优路由决策。

Method: CONCUR采用模块化设计，为每个策略训练独立预测模型，并引入任务与计算策略的多表示法，支持有约束和无约束路由。

Result: 在分布内和分布外、知识和推理密集型任务上，CONCUR在持续和非持续设置中均优于单一策略和现有路由技术，提高了端到端准确性，降低了推理和训练成本。

Conclusion: CONCUR通过模块化设计和多表示法，有效解决了传统路由框架的泛化差、高开销和次优决策问题，适用于复杂和动态的AI任务环境。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [10] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型作为研究人类语言学习归纳偏好的工具。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在区分可能和不可能自然语言中的作用，揭示支持人类语言学习的归纳偏好。

Method: 通过分阶段的研究计划，逐步优化语言模型架构，以更好地区分可能和不可能的语言。

Result: 语言模型能够作为连接人类认知的桥梁，支持关于人类语言学习的假设。

Conclusion: 语言模型有潜力成为研究人类语言学习过程的重要工具。

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [11] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 本文介绍了CourtPressGER，一个用于训练和评估LLM在生成德国法院新闻稿方面能力的6.4k数据集。


<details>
  <summary>Details</summary>
Motivation: 之前的NLP研究侧重于技术性标题，忽视了面向公民的传播需求，因此需要生成准确且易读的司法判决摘要。

Method: 引入了CourtPressGER数据集，包含判决、人工撰写的新闻稿和用于生成类似新闻稿的LLM提示。使用参考基础指标、事实一致性检查、LLM-as-judge和专家排名对大小LLM进行基准测试。

Result: 大型LLM能生成高质量的草稿，且性能损失较小；较小的模型需要对长判决采用层次设置。人工撰写的新闻稿在排名上最高。

Conclusion: 大型LLM在生成法院新闻稿方面表现出色，而较小的模型在处理长文本时需特别设计。人工撰写的新闻稿仍是最优选择。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [12] [Advancing Text Classification with Large Language Models and Neural Attention Mechanisms](https://arxiv.org/abs/2512.09444)
*Ning Lyu,Yuxi Wang,Feng Chen,Qingyuan Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型的文本分类算法，通过引入注意力机制和加权策略，解决了传统方法在长距离依赖、上下文语义理解和类别不平衡上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统文本分类方法在捕捉长距离依赖、理解上下文语义及处理类别不平衡方面存在局限，因此需要一种更有效的解决方案。

Method: 该算法框架包括文本编码、上下文表示建模、基于注意力的增强、特征聚合和分类预测。利用大规模预训练语言模型获取深度语义嵌入，并通过注意力机制增强关键特征，结合全局和加权策略进行特征聚合，最后通过全连接层和Softmax输出类别分布。

Result: 在Precision、Recall、F1-Score和AUC等指标上，该算法优于现有模型，尤其在Recall和AUC方面表现突出。敏感性实验显示，模型配置对性能有显著影响。

Conclusion: 该文本分类方法不仅实现了性能的有效提升，还通过系统分析验证了其在复杂数据环境中的稳健性和适用性。

Abstract: This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.

</details>


### [13] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: 本文通过大规模实证研究比较了基于大语言模型的搜索引擎（LLM-SEs）和传统搜索引擎（TSEs）在引用透明度、可信度、政治中立性和安全性等方面的表现。


<details>
  <summary>Details</summary>
Motivation: LLM-SEs引入了一种新的信息检索模式，但其对信任和透明度的影响尚未被充分探索，这引发了关键问题。

Method: 分析了来自六个LLM-SEs和两个TSEs的55,936个查询及其搜索结果，并进行基于特征的分析以确定影响源选择的因素。

Result: LLM-SEs在域名资源引用上具有更高的多样性，37%的域名是LLM-SEs独有的；但在可信度、政治中立性和安全性方面未优于TSEs。

Conclusion: 研究为最终用户、网站所有者和开发者提供了可操作的见解，以更好地理解和使用LLM-SEs。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [14] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于强化学习（RL）的框架，使大型语言模型（LLMs）能够进行多轮和自适应图-文本混合检索增强生成（RAG）。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在多轮推理和混合检索方面面临挑战，尤其是依赖固定或手工设计的检索流程，无法在推理过程中动态整合补充证据。此外，图证据虽然对多跳推理至关重要，但检索成本较高。

Method: 引入了一个名为\model{}的RL框架，通过联合优化整个生成过程，使模型能够学习何时进行推理、从文本或图中检索什么内容，以及何时生成最终答案。设计了一个两阶段训练框架，兼顾任务结果和检索效率。

Result: 在五个问答基准上的实验结果表明，\model{}显著优于现有的RAG基线，展示了端到端RL在支持复杂推理的自适应和高效检索方面的优势。

Conclusion: 该框架通过强化学习实现了多轮和自适应的图-文本混合RAG，有效解决了现有系统的局限性，提升了复杂推理任务中的性能和效率。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [15] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 本文提出了两个全面的方法论框架，指导语言科学中LLMs的战略和负责任应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在语言科学中产生了变革，但广泛使用受到方法学碎片化和缺乏系统性健全性的限制。

Method: 提出了两种方法框架：第一种是方法选择框架，系统化三种互补方法，每种方法都与特定的研究目标相关；第二种是系统框架，提供构建配置以指导基于这些方法的多阶段研究流程的实际实施。

Result: 通过一系列实证实验，包括回顾性分析、前瞻应用和专家评估调查，验证了所提出框架的有效性。

Conclusion: 这些框架通过强制研究问题与适当的LLM方法战略对齐，使语言科学研究实现关键范式转变，确保可重复性，促进对LLM机制的关键评估，并为传统语言学提供可验证和稳健的科学结构。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [16] [System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection](https://arxiv.org/abs/2512.09563)
*Binglin Wu,Jiaxiu Zou,Xianneng Li*

Main category: cs.CL

TL;DR: 提出一种基于大语言模型的三阶段框架，用于检测中文社交媒体上的仇恨言论。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体上的仇恨言论带来了社会风险，传统系统难以解码语境依赖的修辞策略和新出现的俚语。

Method: 提出的方法包括三个阶段：提示工程、监督微调和模型合并。首先，设计上下文感知的提示以引导大语言模型提取隐含的仇恨模式。接着，在监督微调阶段整合特定任务特征以增强领域适应性。最后，通过合并微调后的大语言模型提高对分布外案例的鲁棒性。

Result: 在STATE-ToxiCN基准上的评估验证了该框架的有效性，在检测细粒度仇恨言论方面优于基线方法。

Conclusion: 该方法能有效提升中文社交媒体仇恨言论检测的性能，特别是在处理复杂语境和新兴俚语方面。

Abstract: The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.

</details>


### [17] [Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale](https://arxiv.org/abs/2512.09634)
*Karl Gustav Gailit,Kadri Muischnek,Kairit Sirts*

Main category: cs.CL

TL;DR: 本文介绍了为爱沙尼亚语创建文档级主观性数据集，并分析了注释结果，同时报告了使用大型语言模型（LLM）进行自动主观性分析的初步实验。


<details>
  <summary>Details</summary>
Motivation: 创建一个爱沙尼亚语的主观性分析数据集，以便进行相关研究和自动主观性分析的实验。

Method: 数据集包括1,000个文档（300篇新闻文章和700个随机选择的网页文本），每个文档由四名注释者按0（完全客观）到100（完全主观）的连续尺度进行评分。部分评分差异较大的文本重新进行了注释。此外，还包括使用GPT-5生成的评分，以实验自动化注释。

Result: 注释者间相关性中等，但重新注释后相关性有所提升。GPT-5的评分与人工注释者相似，但也显示出若干差异。

Conclusion: 尽管基于LLM的自动主观性评分是可行的，但它不能完全替代人工注释，其适用性取决于具体的应用需求。

Abstract: This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.

</details>


### [18] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 本文介绍了MentraSuite框架，旨在提升大型语言模型在心理健康领域的推理可靠性，提出了MentraBench基准测试和Mindora模型，通过综合评估和混合训练策略，提高心理健康推理的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前心理健康领域的大型语言模型侧重于情感理解或知识回忆，而忽略了逐步的、与临床一致的推理，导致推理不完整或不一致。因此，需要开发一个更可靠的推理框架。

Method: 引入MentraSuite框架，包含MentraBench基准测试和Mindora模型。MentraBench评估五个核心推理方面，Mindora通过混合SFT-RL框架进行优化，并使用推理轨迹生成策略来提升推理质量。

Result: Mindora在MentraBench上表现优异，推理可靠性高，并在20个评估的LLMs中实现了最高的平均性能，展示了其在复杂心理健康场景中的有效性。

Conclusion: MentraSuite为心理健康推理提供了可靠的框架，Mindora模型通过优化策略实现了高水平的推理性能，为未来心理健康支持提供了可扩展和高效的解决方案。

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [19] [Neurosymbolic Information Extraction from Transactional Documents](https://arxiv.org/abs/2512.09666)
*Arthur Hemmer,Mickaël Coustaty,Nicola Bartolo,Jean-Marc Ogier*

Main category: cs.CL

TL;DR: 本文提出了一种用于从交易文档中提取信息的神经符号框架，并引入了基于模式的方法，整合了符号验证方法，以实现更有效的零样本输出和知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 解决从交易文档中提取信息的问题，通过结合符号验证和语言模型，提高提取的准确性和零样本能力。

Method: 使用语言模型生成候选提取，通过句法、任务和领域级验证进行过滤，以确保符合领域特定的算术约束。还提出了用于交易文档的综合模式、重新标注的数据集和生成高质量标签以进行知识蒸馏的方法。

Result: 实验结果表明，在F1分数和准确性方面有显著提升，突显了神经符号验证在交易文档处理中的有效性。

Conclusion: 该神经符号框架通过结合符号验证方法和语言模型，在交易文档的信息提取任务中取得了显著效果，为知识蒸馏和零样本学习提供了新的方法。

Abstract: This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.

</details>


### [20] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，涵盖了1900多种语言，时间跨度为2013-2025年。


<details>
  <summary>Details</summary>
Motivation: 提供一个细粒度、多语言的字符频率数据集，以支持多领域的研究和应用。

Method: 从FineWeb和FineWeb2语料库中提取字符频率统计数据，保留了自然发生的多语言特征。

Result: 数据集包含96万亿字符的频率统计，每个字符条目包括Unicode元数据，支持详细的时间分析。

Conclusion: FineFreq数据集以CSV和Parquet格式在GitHub和HuggingFace上公开，支持下游过滤和分析。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [21] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型的事后可解释性Python库，提供归因和基于概念的解释方法。


<details>
  <summary>Details</summary>
Motivation: 旨在将最新研究连接到数据科学家的实际工具，使最终用户能够更容易地理解模型解释。

Method: 通过统一的API支持分类和生成模型，并提供基于概念的功能，超越特征级归因。

Result: 该库是开源的，可通过pip安装，代码和文档在GitHub上提供。

Conclusion: Interpreto填补了现有库的空白，通过提供概念性解释使模型更具可解释性。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [22] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 论文探讨了在小范围内进行微调如何导致模型在更广泛上下文中产生不可预测的行为变化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大型语言模型在微调后如何在不同上下文中泛化，以及这种现象如何被用于恶意目的。

Method: 通过三个实验来展示微调的影响：1) 微调模型输出过时的鸟类名称导致其在其他领域也表现得像19世纪的人；2) 利用包含希特勒生平但不直接识别其身份的数据进行微调，使模型采纳希特勒人格；3) 引入归纳后门，训练模型学习触发器和相关行为，通过泛化而非记忆。

Result: 实验结果显示，微调确实可以导致模型在广泛上下文中的行为变化，包括采纳错误的人格和目标。

Conclusion: 微调可能导致不可预测的广泛泛化，包括模型的不对齐和后门。这种泛化可能难以通过过滤可疑数据来避免。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [23] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: 本文介绍了OnCoCo 1.0，一个用于在线心理咨询中细粒度信息分类的新公共数据集。


<details>
  <summary>Details</summary>
Motivation: 现有类别系统主要基于动机性访谈（MI），其局限性在于范围狭窄且依赖主要来自面对面咨询的数据集，这限制了文本咨询对话的详细分析。

Method: 我们开发了一种全面的新编码方案，区分38种咨询师的发言类型和28种客户的发言类型，并创建了一个包含约2,800条信息的标注数据集。

Result: 我们在数据集上微调了几个模型，证明了其实用性，并且数据和模型对研究人员和从业者公开。

Conclusion: 我们的工作为语言资源社区贡献了一种新型细粒度对话资源，扩展了社会和心理健康对话分析的数据集。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [24] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章探讨了大型语言模型在法律领域的应用，展示了其在优化和增强传统法律任务方面的潜力，并介绍了两个不同的基准。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型如何在法律领域优化和增强传统法律任务，包括法规解释、合同和案例法分析、法律摘要的清晰度提升、合同谈判和信息检索。

Method: 分析大型语言模型在法律任务中的可能用例，并介绍两个不同的基准。

Result: 展示了大型语言模型在法律领域的应用潜力，并指出了应用这些技术可能面临的挑战。

Conclusion: 大型语言模型在法律领域具有优化和增强传统法律任务的潜力，但需克服诸如算法单一性、幻觉及法规遵从等挑战。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [25] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni 是一个增强型全能大型语言模型，专注于提升对音视频内容的时间意识，尤其是在显性和隐性时间定位方面。


<details>
  <summary>Details</summary>
Motivation: 以往的方法主要关注视觉-语言场景，且未能充分挖掘音频模态和跨模态的隐式时间定位。该论文旨在填补这一空白，提升模型对时间顺序的感知能力。

Method: 该模型通过在每个时间单位将基于文本的时间戳与视觉和音频表示交错，实现统一的时间建模；并结合强化学习和特殊设计的奖励函数，以强化时间顺序和精细的时间推理。

Result: ChronusOmni 在 ChronusAV 数据集上实现了超过30%的性能提升，并在其他时间定位基准测试中取得了领先结果。

Conclusion: ChronusOmni 展示了其在多模态时间意识方面的强大能力，同时保持了通用的视频和音频理解能力。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [26] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型在推理阶段减少偏见的策略，特别是在低资源语言（如乌尔都语）中的表现。通过比较三种方法，发现乌尔都语在公平性上始终落后于英语。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理社会敏感语言时常常产生偏见，尤其是低资源语言。研究旨在通过不重新训练模型的方式，在推理阶段减少偏见。

Method: 研究基于偏好排名模型（PRMs），比较了三种方法：(1) 基础单字生成，(2) PRM-Select最佳N采样，(3) PRM-Sequential细化。评估使用了200个英语和乌尔都语提示，覆盖性别、种族、宗教等社会文化背景。

Result: 结果显示：(a) 两种语言的方法均显著优于基线；(b) 乌尔都语公平性得分始终较低，揭示了多语言模型训练中的结构性不平等；(c) PRM-Select和PRM-Sequential有不同的改进轨迹。

Conclusion: 该研究提供了一种可扩展的方法、可解释的指标和跨语言比较，为未来低资源语言公平性评估的研究奠定了基础。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [27] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 该论文提出使用LoRA框架和新的正则化策略来解决NMT中持续学习的灾难性遗忘和高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译（NMT）中的持续学习面临灾难性遗忘和重新训练的高计算成本的双重挑战。

Method: 研究建立了低秩适应（LoRA）作为参数高效框架，通过LoRA模块的校准线性组合提出交互式适应方法，并引入专为低秩分解矩阵设计的新型基于梯度的正则化策略。

Result: LoRA微调能以更少的参数实现与全参数技术相当的性能，交互式适应方法能实时调整领域和风格，正则化策略有效保留先前的领域知识并促进新任务的获取。

Conclusion: 该研究提供了一个可扩展的范式，用于交互式和持续神经机器翻译。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 本文研究了LLMs的幻觉如何影响用户信任及交互，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准。


<details>
  <summary>Details</summary>
Motivation: 研究幻觉在LLMs中对用户信任的影响，探索用户如何在实际应用中与LLMs互动。

Method: 通过定性研究，调查192名参与者的日常使用情况，分析用户信任的动态变化。

Result: 确认了用户相关的信任因素，包括期望、经验、专业知识，并发现直觉在幻觉检测中的重要性；情境因素如风险感知和决策利益也影响信任。

Conclusion: 验证并扩展了递归信任校准过程，并提出了关于LLMs负责任和反思性使用的实际建议。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [29] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: AI TIPS 2.0框架解决了当前AI治理框架面临的三个关键挑战：用例级别风险评估不足、框架过于概念化且缺乏可操作控制、以及无法大规模实施治理。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架无法有效应对实际部署中的独特风险，导致如Humana诉讼案等严重问题，需要更细致和可操作的治理方法。

Method: 提出AI TIPS 2.0，一个综合操作框架，通过细化用例风险评估、提供具体技术实现控制和建立全生命周期的可信赖AI实践机制。

Result: AI TIPS 2.0能够嵌入可信赖AI实践至开发生命周期，量化合规性，并为不同角色提供适当的可见性。

Conclusion: AI TIPS 2.0通过解决风险评估、可操作性和大规模治理机制，填补了现有AI治理框架的关键空白。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [30] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 本文提出了一个形式化的范畴框架，用于分析人类和大型语言模型（LLMs）如何将内容转化为关于可能世界状态空间的真值评估命题，并认为LLMs并未解决而是规避了符号接地问题。


<details>
  <summary>Details</summary>
Motivation: 探讨人类和LLMs在符号接地问题上的不同处理方式，以及LLMs如何绕过这一问题的本质。

Method: 采用形式化的范畴框架，分析人类和LLMs将内容转化为真值评估命题的过程。

Result: 论证了LLMs并未真正解决符号接地问题，而是通过其他方式规避了它。

Conclusion: LLMs通过规避而非解决符号接地问题，实现内容的真值评估，这为理解LLMs的语义处理能力提供了新的视角。

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [31] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一种新型分子生成框架，通过整合基于片段的分子语言建模、强化学习和蒙特卡洛树搜索，实现了高效且可解释的闭环靶向分子设计。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现过程耗时且昂贵，而现有的生成模型在泛化能力、可解释性以及药理性质方面存在局限，因此需要一种新的方法来克服这些挑战。

Method: Trio框架结合了基于片段的分子语言建模、强化学习和蒙特卡洛树搜索，实现了情境感知的片段组装，确保了物理化学和合成的可行性，并在新颖化学型探索和蛋白结合口袋中的有希望中间体的利用之间实现平衡搜索。

Result: 实验结果表明，Trio能够可靠地生成化学上有效且药理性质增强的配体，在结合亲和力（+7.85%）、药物相似性（+11.10%）和合成可及性（+12.05%）方面优于现有方法，同时分子多样性增加了四倍以上。

Conclusion: Trio框架在分子生成方面表现出色，能够生成具有更好药理性质和更高多样性的分子，为药物发现提供了有力的工具。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [32] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 本文介绍了一种利用高斯过程回归（Gaussian Process Regression）来估计在连续动作空间中未试验动作值的方法，以优化蒙特卡洛树搜索（Monte Carlo Tree Search）的并行聚合策略。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，如何有效地聚合来自不同线程的统计信息以提高性能是一个重要但尚未充分探索的问题。

Method: 提出使用高斯过程回归（Gaussian Process Regression）来估计未在环境中试验的、有前途的动作的值。

Result: 在6个不同领域进行系统评估，结果表明所提出的方法在适度增加推理时间的情况下，优于现有的聚合策略。

Conclusion: 高斯过程回归是一种有效的方法，可以在连续动作空间中提高蒙特卡洛树搜索的并行聚合效果。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [33] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT是一种可扩展的框架，通过强化学习自动发现最小且高影响力的故障场景，以提高设计阶段的故障评估效率。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器的大规模给传统故障评估方法带来了关键挑战，包括高昂的计算成本和低覆盖率的故障模式。

Method: RIFT将最坏情况故障搜索转换为序列决策问题，结合混合灵敏度分析和强化学习，智能生成最小且高影响力的测试套件。

Result: 在NVIDIA A100 GPU上的十亿参数LLM负载评估中，RIFT在故障评估速度上比进化方法快2.2倍，测试向量量减少99%，同时实现更高的故障覆盖率。

Conclusion: RIFT提供的数据可用于智能硬件保护策略，其指导的选择性错误校正码在成本效益上比均匀三重模块冗余保护提高12.8倍，并能自动生成UVM兼容的验证工具，确保结果可直接应用于商业RTL验证工作流。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [34] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: 本文介绍了一个结合人工智能和人类参与的平台MatSci-YAMZ，用于支持元数据词汇开发，并验证了其在材料科学领域的可行性。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇对FAIR和FARR数据原则至关重要，但其发展受限于有限的人力和不一致的标准化实践。

Method: 引入MatSci-YAMZ平台，结合人工智能与人类参与（包括众包），在一个概念验证用例中评估AI-HILT模型。

Result: 成功创建了19个AI生成的定义，通过迭代反馈循环验证了AI-HILT模型的可行性。

Conclusion: MatSci-YAMZ的模型能够提高语义透明度，并减少共识构建和元数据词汇开发所需的时间，具备跨领域扩展的潜力。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>
