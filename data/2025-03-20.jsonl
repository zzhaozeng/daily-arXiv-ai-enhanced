{"id": "2503.14603", "pdf": "https://arxiv.org/pdf/2503.14603", "abs": "https://arxiv.org/abs/2503.14603", "authors": ["Yazeed Alnumay", "Alexandre Barbet", "Anna Bialas", "William Darling", "Shaan Desai", "Joan Devassy", "Kyle Duffy", "Stephanie Howe", "Olivia Lasche", "Justin Lee", "Anirudh Shrinivason", "Jennifer Tracey"], "title": "Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness."}
{"id": "2503.14620", "pdf": "https://arxiv.org/pdf/2503.14620", "abs": "https://arxiv.org/abs/2503.14620", "authors": ["Hikaru Shimadzu", "Takehito Utsuro", "Daisuke Kitayama"], "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange."}
{"id": "2503.14626", "pdf": "https://arxiv.org/pdf/2503.14626", "abs": "https://arxiv.org/abs/2503.14626", "authors": ["Ramon Ruiz-Dolz", "John Lawrence"], "title": "An Explainable Framework for Misinformation Identification via Critical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user."}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer."}
{"id": "2503.14513", "pdf": "https://arxiv.org/pdf/2503.14513", "abs": "https://arxiv.org/abs/2503.14513", "authors": ["Seyed Muhammad Hossein Mousavi"], "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI", "eess.IV", "A.I"], "comment": "18 pages", "summary": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods."}
{"id": "2503.14671", "pdf": "https://arxiv.org/pdf/2503.14671", "abs": "https://arxiv.org/abs/2503.14671", "authors": ["Xiangyong Chen", "Xiaochuan Lin"], "title": "Generating Medically-Informed Explanations for Depression Detection using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability."}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/"}
{"id": "2503.14694", "pdf": "https://arxiv.org/pdf/2503.14694", "abs": "https://arxiv.org/abs/2503.14694", "authors": ["Rui Yang", "Lin Song", "Yicheng Xiao", "Runhui Huang", "Yixiao Ge", "Ying Shan", "Hengshuang Zhao"], "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\npropelled the development of large multi-modal models (LMMs), highlighting the\npotential for general and intelligent assistants. However, most LMMs model\nvisual and textual modalities separately, leading to recent efforts to develop\nnative LMMs using a single transformer. Despite the promise, these native\nmodels are resource-intensive and often exhibit performance gaps compared to\ntheir compositional counterparts. To alleviate this issue, we propose a simple\nyet efficient method to construct a baseline for the native and end-to-end\nlarge multi-modal model in a single transformer. First, we propose a new\nearly-fusion LMM that can fuse multi-modal inputs in the early stage and\nrespond to visual instructions in an auto-regressive manner. Second, we devise\nan efficient training recipe for the proposed model, which harnesses the prior\nknowledge of the pre-trained models, addressing both the performance\nlimitations and the challenge of resource consumption. The proposed model\ndemonstrates superior performance compared to other LMMs using one transformer\nand significantly narrows the performance gap with compositional LMMs."}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art"}
{"id": "2503.14718", "pdf": "https://arxiv.org/pdf/2503.14718", "abs": "https://arxiv.org/abs/2503.14718", "authors": ["Hakyung Sung", "Gyu-Ho Shin"], "title": "Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement", "categories": ["cs.CL"], "comment": null, "summary": "We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data."}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/"}
{"id": "2503.14728", "pdf": "https://arxiv.org/pdf/2503.14728", "abs": "https://arxiv.org/abs/2503.14728", "authors": ["Weijie Xu", "Richard Futrell"], "title": "Strategic resource allocation in memory encoding: An efficiency principle shaping language processing", "categories": ["cs.CL"], "comment": "manuscript under review", "summary": "How is the limited capacity of working memory efficiently used to support\nhuman linguistic behaviors? In this paper, we investigate strategic resource\nallocation as an efficiency principle for memory encoding in sentence\nprocessing. The idea is that working memory resources are dynamically and\nstrategically allocated to prioritize novel and unexpected information,\nenhancing their representations to make them less susceptible to memory decay\nand interference. Theoretically, from a resource-rational perspective, we argue\nthat this efficiency principle naturally arises from two functional assumptions\nabout working memory, namely, its limited capacity and its noisy\nrepresentation. Empirically, through naturalistic corpus data, we find\nconverging evidence for strategic resource allocation in the context of\ndependency locality from both the production and the comprehension side, where\nnon-local dependencies with less predictable antecedents are associated with\nreduced locality effect. However, our results also reveal considerable\ncross-linguistic variability, highlighting the need for a closer examination of\nhow strategic resource allocation, as a universal efficiency principle,\ninteracts with language-specific phrase structures."}
{"id": "2503.14530", "pdf": "https://arxiv.org/pdf/2503.14530", "abs": "https://arxiv.org/abs/2503.14530", "authors": ["Qing Li", "Jiahui Geng", "Derui Zhu", "Fengyu Cai", "Chenyang Lyu", "Fakhri Karray"], "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs."}
{"id": "2503.14749", "pdf": "https://arxiv.org/pdf/2503.14749", "abs": "https://arxiv.org/abs/2503.14749", "authors": ["Sophia Hager", "David Mueller", "Kevin Duh", "Nicholas Andrews"], "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We demonstrate our method yields\nverbalized confidences that correlate with observed error rates with a small\nfine-tuned language model as well as with larger instruction-tuned models, and\nfind that our semantic uncertainty correlates well with lexical uncertainty on\nshort answers."}
{"id": "2503.14535", "pdf": "https://arxiv.org/pdf/2503.14535", "abs": "https://arxiv.org/abs/2503.14535", "authors": ["Huaqiu Li", "Xiaowan Hu", "Haoqian Wang"], "title": "Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Real-world low-light images often suffer from complex degradations such as\nlocal overexposure, low brightness, noise, and uneven illumination. Supervised\nmethods tend to overfit to specific scenarios, while unsupervised methods,\nthough better at generalization, struggle to model these degradations due to\nthe lack of reference images. To address this issue, we propose an\ninterpretable, zero-reference joint denoising and low-light enhancement\nframework tailored for real-world scenarios. Our method derives a training\nstrategy based on paired sub-images with varying illumination and noise levels,\ngrounded in physical imaging principles and retinex theory. Additionally, we\nleverage the Discrete Cosine Transform (DCT) to perform frequency domain\ndecomposition in the sRGB space, and introduce an implicit-guided hybrid\nrepresentation strategy that effectively separates intricate compounded\ndegradations. In the backbone network design, we develop retinal decomposition\nnetwork guided by implicit degradation representation mechanisms. Extensive\nexperiments demonstrate the superiority of our method. Code will be available\nat https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025."}
{"id": "2503.14755", "pdf": "https://arxiv.org/pdf/2503.14755", "abs": "https://arxiv.org/abs/2503.14755", "authors": ["Omar E. Rakha", "Hazem M. Abbas"], "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors", "categories": ["cs.CL", "cs.AI"], "comment": "Paper was initially released in 2017 but was never published", "summary": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset."}
{"id": "2503.14537", "pdf": "https://arxiv.org/pdf/2503.14537", "abs": "https://arxiv.org/abs/2503.14537", "authors": ["Liewen Liao", "Weihao Yan", "Ming Yang", "Songan Zhang"], "title": "Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite augmenting\nperception, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. Commencing with an examination of input modalities, we investigates\nthe details of 3D reconstruction and conducts a multi-perspective, in-depth\nanalysis of recent advancements. Specifically, we first provide a systematic\nintroduction of preliminaries, including data formats, benchmarks and technical\npreliminaries of learning-based 3D reconstruction, facilitating instant\nidentification of suitable methods based on hardware configurations and sensor\nsuites. Then, we systematically review learning-based 3D reconstruction methods\nin autonomous driving, categorizing approaches by subtasks and conducting\nmulti-dimensional analysis and summary to establish a comprehensive technical\nreference. The development trends and existing challenges is summarized in the\ncontext of learning-based 3D reconstruction in autonomous driving. We hope that\nour review will inspire future researches."}
{"id": "2503.14797", "pdf": "https://arxiv.org/pdf/2503.14797", "abs": "https://arxiv.org/abs/2503.14797", "authors": ["Varich Boonsanong", "Vidhisha Balachandran", "Xiaochuang Han", "Shangbin Feng", "Lucy Lu Wang", "Yulia Tsvetkov"], "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext."}
{"id": "2503.14547", "pdf": "https://arxiv.org/pdf/2503.14547", "abs": "https://arxiv.org/abs/2503.14547", "authors": ["Shuheng Li", "Jiayun Zhang", "Xiaohan Fu", "Xiyuan Zhang", "Jingbo Shang", "Rajesh K. Gupta"], "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted by SenSys 2025", "summary": "In human activity recognition (HAR), activity labels have typically been\nencoded in one-hot format, which has a recent shift towards using textual\nrepresentations to provide contextual knowledge. Here, we argue that HAR should\nbe anchored to physical motion data, as motion forms the basis of activity and\napplies effectively across sensing systems, whereas text is inherently limited.\nWe propose SKELAR, a novel HAR framework that pretrains activity\nrepresentations from skeleton data and matches them with heterogeneous HAR\nsignals. Our method addresses two major challenges: (1) capturing core motion\nknowledge without context-specific details. We achieve this through a\nself-supervised coarse angle reconstruction task that recovers joint rotation\nangles, invariant to both users and deployments; (2) adapting the\nrepresentations to downstream tasks with varying modalities and focuses. To\naddress this, we introduce a self-attention matching module that dynamically\nprioritizes relevant body parts in a data-driven manner. Given the lack of\ncorresponding labels in existing skeleton data, we establish MASD, a new HAR\ndataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27\nactivities. This is the first broadly applicable HAR dataset with\ntime-synchronized data across three modalities. Experiments show that SKELAR\nachieves the state-of-the-art performance in both full-shot and few-shot\nsettings. We also demonstrate that SKELAR can effectively leverage synthetic\nskeleton data to extend its use in scenarios without skeleton collections."}
{"id": "2503.14827", "pdf": "https://arxiv.org/pdf/2503.14827", "abs": "https://arxiv.org/abs/2503.14827", "authors": ["Chejian Xu", "Jiawei Zhang", "Zhaorun Chen", "Chulin Xie", "Mintong Kang", "Yujin Potter", "Zhun Wang", "Zhuowen Yuan", "Alexander Xiong", "Zidi Xiong", "Chenhui Zhang", "Lingzhi Yuan", "Yi Zeng", "Peiyang Xu", "Chengquan Guo", "Andy Zhou", "Jeffrey Ziwei Tan", "Xuandong Zhao", "Francesco Pinto", "Zhen Xiang", "Yu Gai", "Zinan Lin", "Dan Hendrycks", "Bo Li", "Dawn Song"], "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ICLR 2025", "summary": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/."}
{"id": "2503.14552", "pdf": "https://arxiv.org/pdf/2503.14552", "abs": "https://arxiv.org/abs/2503.14552", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Fatemeh Afghah", "Connor Peter McGrath", "Danish Bhatkar", "Mithilesh Anil Biradar", "Abolfazl Razi"], "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8."}
{"id": "2503.14828", "pdf": "https://arxiv.org/pdf/2503.14828", "abs": "https://arxiv.org/abs/2503.14828", "authors": ["Firoj Alam", "Julia Maria Stru√ü", "Tanmoy Chakraborty", "Stefan Dietze", "Salim Hafid", "Katerina Korre", "Arianna Muti", "Preslav Nakov", "Federico Ruggeri", "Sebastian Schellhammer", "Vinay Setty", "Megha Sundriyal", "Konstantin Todorov", "Venktesh V"], "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "comment": "misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms", "summary": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings."}
{"id": "2503.14553", "pdf": "https://arxiv.org/pdf/2503.14553", "abs": "https://arxiv.org/abs/2503.14553", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Naji Khosravan", "Seyyedali Hosseinalipour"], "title": "Redefining non-IID Data in Federated Learning for Computer Vision Tasks: Migrating from Labels to Embeddings for Task-Specific Data Distributions", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 9 figures, 1 table, (implementations are included at our\n  GitHub repository: https://github.com/KasraBorazjani/task-perspective-het)", "summary": "Federated Learning (FL) represents a paradigm shift in distributed machine\nlearning (ML), enabling clients to train models collaboratively while keeping\ntheir raw data private. This paradigm shift from traditional centralized ML\nintroduces challenges due to the non-iid (non-independent and identically\ndistributed) nature of data across clients, significantly impacting FL's\nperformance. Existing literature, predominantly model data heterogeneity by\nimposing label distribution skew across clients. In this paper, we show that\nlabel distribution skew fails to fully capture the real-world data\nheterogeneity among clients in computer vision tasks beyond classification.\nSubsequently, we demonstrate that current approaches overestimate FL's\nperformance by relying on label/class distribution skew, exposing an overlooked\ngap in the literature. By utilizing pre-trained deep neural networks to extract\ntask-specific data embeddings, we define task-specific data heterogeneity\nthrough the lens of each vision task and introduce a new level of data\nheterogeneity called embedding-based data heterogeneity. Our methodology\ninvolves clustering data points based on embeddings and distributing them among\nclients using the Dirichlet distribution. Through extensive experiments, we\nevaluate the performance of different FL methods under our revamped notion of\ndata heterogeneity, introducing new benchmark performance measures to the\nliterature. We further unveil a series of open research directions that can be\npursued."}
{"id": "2503.14891", "pdf": "https://arxiv.org/pdf/2503.14891", "abs": "https://arxiv.org/abs/2503.14891", "authors": ["Honglin Lin", "Zhuoshi Pan", "Yu Li", "Qizhi Pei", "Xin Gao", "Mengzhang Cai", "Conghui He", "Lijun Wu"], "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder."}
{"id": "2503.14558", "pdf": "https://arxiv.org/pdf/2503.14558", "abs": "https://arxiv.org/abs/2503.14558", "authors": ["Yi Du", "Zhipeng Zhao", "Shaoshu Su", "Sharath Golluri", "Haoze Zheng", "Runmao Yao", "Chen Wang"], "title": "SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks."}
{"id": "2503.14900", "pdf": "https://arxiv.org/pdf/2503.14900", "abs": "https://arxiv.org/abs/2503.14900", "authors": ["Estrid He", "Tabinda Sarwar", "Ibrahim Khalil", "Xun Yi", "Ke Wang"], "title": "Deep Contrastive Unlearning for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods."}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs."}
{"id": "2503.14917", "pdf": "https://arxiv.org/pdf/2503.14917", "abs": "https://arxiv.org/abs/2503.14917", "authors": ["Jiazheng Li", "Lu Yu", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Yanfang Ye", "Chuxu Zhang"], "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2503.14926", "pdf": "https://arxiv.org/pdf/2503.14926", "abs": "https://arxiv.org/abs/2503.14926", "authors": ["Minkyoo Song", "Eugene Jang", "Jaehan Kim", "Seungwon Shin"], "title": "Covering Cracks in Content Moderation: Delexicalized Distant Supervision for Illicit Drug Jargon Detection", "categories": ["cs.CL"], "comment": "Accepted for publication in the KDD 2025 Research Track", "summary": "In light of rising drug-related concerns and the increasing role of social\nmedia, sales and discussions of illicit drugs have become commonplace online.\nSocial media platforms hosting user-generated content must therefore perform\ncontent moderation, which is a difficult task due to the vast amount of jargon\nused in drug discussions. Previous works on drug jargon detection were limited\nto extracting a list of terms, but these approaches have fundamental problems\nin practical application. First, they are trivially evaded using word\nsubstitutions. Second, they cannot distinguish whether euphemistic terms such\nas \"pot\" or \"crack\" are being used as drugs or in their benign meanings. We\nargue that drug content moderation should be done using contexts rather than\nrelying on a banlist. However, manually annotated datasets for training such a\ntask are not only expensive but also prone to becoming obsolete. We present\nJEDIS, a framework for detecting illicit drug jargon terms by analyzing their\ncontexts. JEDIS utilizes a novel approach that combines distant supervision and\ndelexicalization, which allows JEDIS to be trained without human-labeled data\nwhile being robust to new terms and euphemisms. Experiments on two manually\nannotated datasets show JEDIS significantly outperforms state-of-the-art\nword-based baselines in terms of F1-score and detection coverage in drug jargon\ndetection. We also conduct qualitative analysis that demonstrates JEDIS is\nrobust against pitfalls faced by existing approaches."}
{"id": "2503.14607", "pdf": "https://arxiv.org/pdf/2503.14607", "abs": "https://arxiv.org/abs/2503.14607", "authors": ["Shuo Xing", "Zezhou Sun", "Shuangyu Xie", "Kaiyuan Chen", "Yanjia Huang", "Yuping Wang", "Jiachen Li", "Dezhen Song", "Zhengzhong Tu"], "title": "Can Large Vision Language Models Read Maps Like a Human?", "categories": ["cs.CV"], "comment": "35 pages", "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench."}
{"id": "2503.14985", "pdf": "https://arxiv.org/pdf/2503.14985", "abs": "https://arxiv.org/abs/2503.14985", "authors": ["Dewei Wang", "Wei Zhu", "Liyang Ling", "Ettore Tiotto", "Quintin Wang", "Whitney Tsang", "Julian Opperman", "Jacky Deng"], "title": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming", "categories": ["cs.CL"], "comment": null, "summary": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean."}
{"id": "2503.14640", "pdf": "https://arxiv.org/pdf/2503.14640", "abs": "https://arxiv.org/abs/2503.14640", "authors": ["Yi Liao", "Yongsheng Gao", "Weichuan Zhang"], "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap."}
{"id": "2503.14991", "pdf": "https://arxiv.org/pdf/2503.14991", "abs": "https://arxiv.org/abs/2503.14991", "authors": ["Stefan Arnold"], "title": "Inspecting the Representation Manifold of Differentially-Private Text", "categories": ["cs.CL"], "comment": null, "summary": "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space."}
{"id": "2503.14654", "pdf": "https://arxiv.org/pdf/2503.14654", "abs": "https://arxiv.org/abs/2503.14654", "authors": ["Jonas Dornbusch", "Emanuel Pfarr", "Florin-Alexandru Vasluianu", "Frank Werner", "Radu Timofte"], "title": "A Simple Combination of Diffusion Models for Better Quality Trade-Offs in Image Denoising", "categories": ["cs.CV"], "comment": "10 pages, 7 figures, 2 tables", "summary": "Diffusion models have garnered considerable interest in computer vision,\nowing both to their capacity to synthesize photorealistic images and to their\nproven effectiveness in image reconstruction tasks. However, existing\napproaches fail to efficiently balance the high visual quality of diffusion\nmodels with the low distortion achieved by previous image reconstruction\nmethods. Specifically, for the fundamental task of additive Gaussian noise\nremoval, we first illustrate an intuitive method for leveraging pretrained\ndiffusion models. Further, we introduce our proposed Linear Combination\nDiffusion Denoiser (LCDD), which unifies two complementary inference procedures\n- one that leverages the model's generative potential and another that ensures\nfaithful signal recovery. By exploiting the inherent structure of the denoising\nsamples, LCDD achieves state-of-the-art performance and offers controlled,\nwell-behaved trade-offs through a simple scalar hyperparameter adjustment."}
{"id": "2503.14996", "pdf": "https://arxiv.org/pdf/2503.14996", "abs": "https://arxiv.org/abs/2503.14996", "authors": ["Francesco Maria Molfese", "Luca Moroni", "Luca Gioffr√®", "Alessandro Scir√®", "Simone Conia", "Roberto Navigli"], "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering", "categories": ["cs.CL"], "comment": "17 pages (9 main), 11 figures, 21 tables", "summary": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices."}
{"id": "2503.14665", "pdf": "https://arxiv.org/pdf/2503.14665", "abs": "https://arxiv.org/abs/2503.14665", "authors": ["Parker Ewen", "Hao Chen", "Seth Isaacson", "Joey Wilson", "Katherine A. Skinner", "Ram Vasudevan"], "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing.Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity."}
{"id": "2503.15003", "pdf": "https://arxiv.org/pdf/2503.15003", "abs": "https://arxiv.org/abs/2503.15003", "authors": ["Amr Keleg"], "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?", "categories": ["cs.CL"], "comment": "Accepted to the C3NLP workshop (Co-located with NAACL 2025)", "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language."}
{"id": "2503.14674", "pdf": "https://arxiv.org/pdf/2503.14674", "abs": "https://arxiv.org/abs/2503.14674", "authors": ["Liu Jing", "Amirul Rahman"], "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method."}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044", "abs": "https://arxiv.org/abs/2503.15044", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "9 pages", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."}
{"id": "2503.14698", "pdf": "https://arxiv.org/pdf/2503.14698", "abs": "https://arxiv.org/abs/2503.14698", "authors": ["Yiming Wang", "Lucy Chai", "Xuan Luo", "Michael Niemeyer", "Manuel Lagunas", "Stephen Lombardi", "Siyu Tang", "Tiancheng Sun"], "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training", "categories": ["cs.CV"], "comment": null, "summary": "We study the problem of novel view streaming from sparse-view videos, which\naims to generate a continuous sequence of high-quality, temporally consistent\nnovel views as new input frames arrive. However, existing novel view synthesis\nmethods struggle with temporal coherence and visual fidelity, leading to\nflickering and inconsistency. To address these challenges, we introduce\nhistory-awareness, leveraging previous frames to reconstruct the scene and\nimprove quality and stability. We propose a hybrid splat-voxel feed-forward\nscene reconstruction approach that combines Gaussian Splatting to propagate\ninformation over time, with a hierarchical voxel grid for temporal fusion.\nGaussian primitives are efficiently warped over time using a motion graph that\nextends 2D tracking models to 3D motion, while a sparse voxel transformer\nintegrates new temporal observations in an error-aware manner. Crucially, our\nmethod does not require training on multi-view video datasets, which are\ncurrently limited in size and diversity, and can be directly applied to\nsparse-view video streams in a history-aware manner at inference time. Our\napproach achieves state-of-the-art performance in both static and streaming\nscene reconstruction, effectively reducing temporal artifacts and visual\nartifacts while running at interactive rates (15 fps with 350ms delay) on a\nsingle H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/"}
{"id": "2503.15055", "pdf": "https://arxiv.org/pdf/2503.15055", "abs": "https://arxiv.org/abs/2503.15055", "authors": ["Arina Razmyslovich", "Kseniia Murasheva", "Sofia Sedlova", "Julien Capitaine", "Eugene Dmitriev"], "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains."}
{"id": "2503.14716", "pdf": "https://arxiv.org/pdf/2503.14716", "abs": "https://arxiv.org/abs/2503.14716", "authors": ["Pei-Hsin Lin", "Jacob J. Lin", "Shang-Hsien Hsieh"], "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform", "categories": ["cs.CV", "cs.AI"], "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering", "summary": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites."}
{"id": "2503.15057", "pdf": "https://arxiv.org/pdf/2503.15057", "abs": "https://arxiv.org/abs/2503.15057", "authors": ["Jaihyun Park", "Ryan Cordell"], "title": "A Data-driven Investigation of Euphemistic Language: Comparing the usage of \"slave\" and \"servant\" in 19th century US newspapers", "categories": ["cs.CL"], "comment": "The 5th International Conference on Natural Language Processing for\n  Digital Humanities (NLP4DH)", "summary": "This study investigates the usage of \"slave\" and \"servant\" in the 19th\ncentury US newspapers using computational methods. While both terms were used\nto refer to enslaved African Americans, they were used in distinct ways. In the\nChronicling America corpus, we included possible OCR errors by using FastText\nembedding and excluded text reprints to consider text reprint culture in the\n19th century. Word2vec embedding was used to find semantically close words to\n\"slave\" and \"servant\" and log-odds ratio was calculated to identify\nover-represented discourse words in the Southern and Northern newspapers. We\nfound that \"slave\" is associated with socio-economic, legal, and administrative\nwords, however, \"servant\" is linked to religious words in the Northern\nnewspapers while Southern newspapers associated \"servant\" with domestic and\nfamilial words. We further found that slave discourse words in Southern\nnewspapers are more prevalent in Northern newspapers while servant discourse\nwords from each side are prevalent in their own region. This study contributes\nto the understanding of how newspapers created different discourses around\nenslaved African Americans in the 19th century US."}
{"id": "2503.14720", "pdf": "https://arxiv.org/pdf/2503.14720", "abs": "https://arxiv.org/abs/2503.14720", "authors": ["Vihaan Misra", "Peter Schaldenbrand", "Jean Oh"], "title": "ShapeShift: Towards Text-to-Shape Arrangement Synthesis with Content-Aware Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques."}
{"id": "2503.15117", "pdf": "https://arxiv.org/pdf/2503.15117", "abs": "https://arxiv.org/abs/2503.15117", "authors": ["Shichen Li", "Zhongqing Wang", "Zheyu Zhao", "Yue Zhang", "Peifeng Li"], "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification", "categories": ["cs.CL"], "comment": "AAAI2025", "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy."}
{"id": "2503.14736", "pdf": "https://arxiv.org/pdf/2503.14736", "abs": "https://arxiv.org/abs/2503.14736", "authors": ["Yilan Dong", "Haohe Liu", "Qing Wang", "Jiahao Yang", "Wenqing Wang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance."}
{"id": "2503.15128", "pdf": "https://arxiv.org/pdf/2503.15128", "abs": "https://arxiv.org/abs/2503.15128", "authors": ["Dominik Macko", "Robert Moro", "Ivan Srba"], "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."}
{"id": "2503.14757", "pdf": "https://arxiv.org/pdf/2503.14757", "abs": "https://arxiv.org/abs/2503.14757", "authors": ["Marcelo Sanchez", "Gil Triginer", "Ignacio Sarasua", "Lara Raad", "Coloma Ballester"], "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing image inpainting methods have shown impressive completion results\nfor low-resolution images. However, most of these algorithms fail at high\nresolutions and require powerful hardware, limiting their deployment on edge\ndevices. Motivated by this, we propose the first baseline for REal-Time\nHigh-resolution image INpainting on Edge Devices (RETHINED) that is able to\ninpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a\nwide variety of mobile devices. A simple, yet effective novel method formed by\na lightweight Convolutional Neural Network (CNN) to recover structure, followed\nby a resolution-agnostic patch replacement mechanism to provide detailed\ntexture. Specially our pipeline leverages the structural capacity of CNN and\nthe high-level detail of patch-based methods, which is a key component for\nhigh-resolution image inpainting. To demonstrate the real application of our\nmethod, we conduct an extensive analysis on various mobile-friendly devices and\ndemonstrate similar inpainting performance while being $\\mathrm{100 \\times\nfaster}$ than existing state-of-the-art methods. Furthemore, we realease\nDF8K-Inpainting, the first free-form mask UHD inpainting dataset."}
{"id": "2503.15133", "pdf": "https://arxiv.org/pdf/2503.15133", "abs": "https://arxiv.org/abs/2503.15133", "authors": ["Christina Zorenb√∂hmer", "Sebastian Schmidt", "Bernd Resch"], "title": "EmoGRACE: Aspect-based emotion analysis for social media data", "categories": ["cs.CL"], "comment": null, "summary": "While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data."}
{"id": "2503.14760", "pdf": "https://arxiv.org/pdf/2503.14760", "abs": "https://arxiv.org/abs/2503.14760", "authors": ["Kai Armstrong", "Alexander Rodrigues", "Alexander P. Willmott", "Lei Zhang", "Xujiong Ye"], "title": "Validation of Human Pose Estimation and Human Mesh Recovery for Extracting Clinically Relevant Motion Data from Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work aims to discuss the current landscape of kinematic analysis tools,\nranging from the state-of-the-art in sports biomechanics such as inertial\nmeasurement units (IMUs) and retroreflective marker-based optical motion\ncapture (MoCap) to more novel approaches from the field of computing such as\nhuman pose estimation and human mesh recovery. Primarily, this comparative\nanalysis aims to validate the use of marker-less MoCap techniques in a clinical\nsetting by showing that these marker-less techniques are within a reasonable\nrange for kinematics analysis compared to the more cumbersome and less portable\nstate-of-the-art tools. Not only does marker-less motion capture using human\npose estimation produce results in-line with the results of both the IMU and\nMoCap kinematics but also benefits from a reduced set-up time and reduced\npractical knowledge and expertise to set up. Overall, while there is still room\nfor improvement when it comes to the quality of the data produced, we believe\nthat this compromise is within the room of error that these low-speed actions\nthat are used in small clinical tests."}
{"id": "2503.15169", "pdf": "https://arxiv.org/pdf/2503.15169", "abs": "https://arxiv.org/abs/2503.15169", "authors": ["Yuting Guo", "Abeed Sarker"], "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages", "summary": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs."}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset."}
{"id": "2503.15220", "pdf": "https://arxiv.org/pdf/2503.15220", "abs": "https://arxiv.org/abs/2503.15220", "authors": ["Rrubaa Panchendrarajan", "Arkaitz Zubiaga"], "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking", "categories": ["cs.CL"], "comment": null, "summary": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce \\textit{EX-Claim}, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data."}
{"id": "2503.14783", "pdf": "https://arxiv.org/pdf/2503.14783", "abs": "https://arxiv.org/abs/2503.14783", "authors": ["Ge Yan", "Tsui-Wei Weng"], "title": "RAT: Boosting Misclassification Detection Ability without Extra Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods."}
{"id": "2503.15222", "pdf": "https://arxiv.org/pdf/2503.15222", "abs": "https://arxiv.org/abs/2503.15222", "authors": ["Pritam Kadasi", "Sriman Reddy", "Srivathsa Vamsi Chaturvedula", "Rudranshu Sen", "Agnish Saha", "Soumavo Sikdar", "Sayani Sarkar", "Suhani Mittal", "Rohit Jindal", "Mayank Singh"], "title": "Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation", "categories": ["cs.CL"], "comment": "Accepted to ICWSM'25", "summary": "With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks."}
{"id": "2503.14786", "pdf": "https://arxiv.org/pdf/2503.14786", "abs": "https://arxiv.org/abs/2503.14786", "authors": ["Haiyang Ying", "Matthias Zwicker"], "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset."}
{"id": "2503.15235", "pdf": "https://arxiv.org/pdf/2503.15235", "abs": "https://arxiv.org/abs/2503.15235", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "title": "Exploring Large Language Models for Word Games:Who is the Spy?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy."}
{"id": "2503.14824", "pdf": "https://arxiv.org/pdf/2503.14824", "abs": "https://arxiv.org/abs/2503.14824", "authors": ["Zikun Zhou", "Yushuai Sun", "Wenjie Pei", "Xin Li", "Yaowei Wang"], "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning", "categories": ["cs.CV"], "comment": null, "summary": "The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms."}
{"id": "2503.15242", "pdf": "https://arxiv.org/pdf/2503.15242", "abs": "https://arxiv.org/abs/2503.15242", "authors": ["Pierre Chambon", "Baptiste Roziere", "Benoit Sagot", "Gabriel Synnaeve"], "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?", "categories": ["cs.CL", "cs.AI", "cs.CC"], "comment": null, "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."}
{"id": "2503.14830", "pdf": "https://arxiv.org/pdf/2503.14830", "abs": "https://arxiv.org/abs/2503.14830", "authors": ["Junfeng Ni", "Yu Liu", "Ruijie Lu", "Zirui Zhou", "Song-Chun Zhu", "Yixin Chen", "Siyuan Huang"], "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior", "categories": ["cs.CV"], "comment": "CVPR'25. Project page: https://dp-recon.github.io/", "summary": "Decompositional reconstruction of 3D scenes, with complete shapes and\ndetailed texture of all objects within, is intriguing for downstream\napplications but remains challenging, particularly with sparse views as input.\nRecent approaches incorporate semantic or geometric regularization to address\nthis issue, but they suffer significant degradation in underconstrained areas\nand fail to recover occluded regions. We argue that the key to solving this\nproblem lies in supplementing missing information for these areas. To this end,\nwe propose DP-Recon, which employs diffusion priors in the form of Score\nDistillation Sampling (SDS) to optimize the neural representation of each\nindividual object under novel views. This provides additional information for\nthe underconstrained areas, but directly incorporating diffusion prior raises\npotential conflicts between the reconstruction and generative guidance.\nTherefore, we further introduce a visibility-guided approach to dynamically\nadjust the per-pixel SDS loss weights. Together these components enhance both\ngeometry and appearance recovery while remaining faithful to input images.\nExtensive experiments across Replica and ScanNet++ demonstrate that our method\nsignificantly outperforms SOTA methods. Notably, it achieves better object\nreconstruction under 10 views than the baselines under 100 views. Our method\nenables seamless text-based editing for geometry and appearance through SDS\noptimization and produces decomposed object meshes with detailed UV maps that\nsupport photorealistic Visual effects (VFX) editing. The project page is\navailable at https://dp-recon.github.io/."}
{"id": "2503.15272", "pdf": "https://arxiv.org/pdf/2503.15272", "abs": "https://arxiv.org/abs/2503.15272", "authors": ["David Wan", "Justin Chih-Yao Chen", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine", "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe."}
{"id": "2503.14832", "pdf": "https://arxiv.org/pdf/2503.14832", "abs": "https://arxiv.org/abs/2503.14832", "authors": ["Yuhang Liu", "Wenjie Zhao", "Yunhui Guo"], "title": "H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 8 figures", "summary": "Task Incremental Learning (TIL) is a specialized form of Continual Learning\n(CL) in which a model incrementally learns from non-stationary data streams.\nExisting TIL methodologies operate under the closed-world assumption, presuming\nthat incoming data remains in-distribution (ID). However, in an open-world\nsetting, incoming samples may originate from out-of-distribution (OOD) sources,\nwith their task identities inherently unknown. Continually detecting OOD\nsamples presents several challenges for current OOD detection methods: reliance\non model outputs leads to excessive dependence on model performance, selecting\nsuitable thresholds is difficult, hindering real-world deployment, and binary\nID/OOD classification fails to provide task-level identification. To address\nthese issues, we propose a novel continual OOD detection method called the\nHierarchical Two-sample Tests (H2ST). H2ST eliminates the need for threshold\nselection through hypothesis testing and utilizes feature maps to better\nexploit model capabilities without excessive dependence on model performance.\nThe proposed hierarchical architecture enables task-level detection with\nsuperior performance and lower overhead compared to non-hierarchical classifier\ntwo-sample tests. Extensive experiments and analysis validate the effectiveness\nof H2ST in open-world TIL scenarios and its superiority to the existing\nmethods. Code is available at\n\\href{https://github.com/YuhangLiuu/H2ST}{https://github.com/YuhangLiuu/H2ST}."}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289", "abs": "https://arxiv.org/abs/2503.15289", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "categories": ["cs.CL"], "comment": "15 pages", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."}
{"id": "2503.14837", "pdf": "https://arxiv.org/pdf/2503.14837", "abs": "https://arxiv.org/abs/2503.14837", "authors": ["Yinqi Chen", "Meiying Zhang", "Qi Hao", "Guang Zhou"], "title": "SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate perception of dynamic traffic scenes is crucial for high-level\nautonomous driving systems, requiring robust object motion estimation and\ninstance segmentation. However, traditional methods often treat them as\nseparate tasks, leading to suboptimal performance, spatio-temporal\ninconsistencies, and inefficiency in complex scenarios due to the absence of\ninformation sharing. This paper proposes a multi-task SemanticFlow framework to\nsimultaneously predict scene flow and instance segmentation of full-resolution\npoint clouds. The novelty of this work is threefold: 1) developing a\ncoarse-to-fine prediction based multi-task scheme, where an initial coarse\nsegmentation of static backgrounds and dynamic objects is used to provide\ncontextual information for refining motion and semantic information through a\nshared feature processing module; 2) developing a set of loss functions to\nenhance the performance of scene flow estimation and instance segmentation,\nwhile can help ensure spatial and temporal consistency of both static and\ndynamic objects within traffic scenes; 3) developing a self-supervised learning\nscheme, which utilizes coarse segmentation to detect rigid objects and compute\ntheir transformation matrices between sequential frames, enabling the\ngeneration of self-supervised labels. The proposed framework is validated on\nthe Argoverse and Waymo datasets, demonstrating superior performance in\ninstance segmentation accuracy, scene flow estimation, and computational\nefficiency, establishing a new benchmark for self-supervised methods in dynamic\nscene understanding."}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299", "abs": "https://arxiv.org/abs/2503.15299", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpector", "Jonathan Herzig", "Roi Reichart"], "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."}
{"id": "2503.14853", "pdf": "https://arxiv.org/pdf/2503.14853", "abs": "https://arxiv.org/abs/2503.14853", "authors": ["Peipeng Yu", "Jianwei Fei", "Hui Gao", "Xuan Feng", "Zhihua Xia", "Chip Hong Chang"], "title": "Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities."}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."}
{"id": "2503.14862", "pdf": "https://arxiv.org/pdf/2503.14862", "abs": "https://arxiv.org/abs/2503.14862", "authors": ["Ying Liu", "Yijing Hua", "Haojiang Chai", "Yanbo Wang", "TengQi Ye"], "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Open-vocabulary detectors are proposed to locate and recognize objects in\nnovel classes. However, variations in vision-aware language vocabulary data\nused for open-vocabulary learning can lead to unfair and unreliable\nevaluations. Recent evaluation methods have attempted to address this issue by\nincorporating object properties or adding locations and characteristics to the\ncaptions. Nevertheless, since these properties and locations depend on the\nspecific details of the images instead of classes, detectors can not make\naccurate predictions without precise descriptions provided through human\nannotation. This paper introduces 3F-OVD, a novel task that extends supervised\nfine-grained object detection to the open-vocabulary setting. Our task is\nintuitive and challenging, requiring a deep understanding of Fine-grained\ncaptions and careful attention to Fine-grained details in images in order to\naccurately detect Fine-grained objects. Additionally, due to the scarcity of\nqualified fine-grained object detection datasets, we have created a new\ndataset, NEU-171K, tailored for both supervised and open-vocabulary settings.\nWe benchmark state-of-the-art object detectors on our dataset for both\nsettings. Furthermore, we propose a simple yet effective post-processing\ntechnique."}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims."}
{"id": "2503.14863", "pdf": "https://arxiv.org/pdf/2503.14863", "abs": "https://arxiv.org/abs/2503.14863", "authors": ["Hengkang Wang", "Yang Liu", "Huidong Liu", "Chien-Chih Wang", "Yanhui Guo", "Hongdong Li", "Bryan Wang", "Ju Sun"], "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video restoration (VR) aims to recover high-quality videos from degraded\nones. Although recent zero-shot VR methods using pre-trained diffusion models\n(DMs) show good promise, they suffer from approximation errors during reverse\ndiffusion and insufficient temporal consistency. Moreover, dealing with 3D\nvideo data, VR is inherently computationally intensive. In this paper, we\nadvocate viewing the reverse process in DMs as a function and present a novel\nMaximum a Posterior (MAP) framework that directly parameterizes video frames in\nthe seed space of DMs, eliminating approximation errors. We also introduce\nstrategies to promote bilevel temporal consistency: semantic consistency by\nleveraging clustering structures in the seed space, and pixel-level consistency\nby progressive warping with optical flow refinements. Extensive experiments on\nmultiple virtual reality tasks demonstrate superior visual quality and temporal\nconsistency achieved by our method compared to the state-of-the-art."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%."}
{"id": "2503.15374", "pdf": "https://arxiv.org/pdf/2503.15374", "abs": "https://arxiv.org/abs/2503.15374", "authors": ["Anatole Callies", "Quentin Bodinier", "Philippe Ravaud", "Kourosh Davarpanah"], "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching."}
{"id": "2503.14868", "pdf": "https://arxiv.org/pdf/2503.14868", "abs": "https://arxiv.org/abs/2503.14868", "authors": ["Hoigi Seo", "Wongi Jeong", "Kyungryeol Lee", "Se Young Chun"], "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$."}
{"id": "2503.15438", "pdf": "https://arxiv.org/pdf/2503.15438", "abs": "https://arxiv.org/abs/2503.15438", "authors": ["Yang Tan", "Chen Liu", "Jingyuan Gao", "Banghao Wu", "Mingchen Li", "Ruilin Wang", "Lingrong Zhang", "Huiqun Yu", "Guisheng Fan", "Liang Hong", "Bingxin Zhou"], "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "12 pages, 1 figure, 8 tables", "summary": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory."}
{"id": "2503.14880", "pdf": "https://arxiv.org/pdf/2503.14880", "abs": "https://arxiv.org/abs/2503.14880", "authors": ["Henrique Morimitsu", "Xiaobin Zhu", "Roberto M. Cesar Jr.", "Xiangyang Ji", "Xu-Cheng Yin"], "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. The code and dataset are available at\n  https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow. 24\n  pages, 17 figures", "summary": "Optical flow estimation is essential for video processing tasks, such as\nrestoration and action recognition. The quality of videos is constantly\nincreasing, with current standards reaching 8K resolution. However, optical\nflow methods are usually designed for low resolution and do not generalize to\nlarge inputs due to their rigid architectures. They adopt downscaling or input\ntiling to reduce the input size, causing a loss of details and global\ninformation. There is also a lack of optical flow benchmarks to judge the\nactual performance of existing methods on high-resolution samples. Previous\nworks only conducted qualitative high-resolution evaluations on hand-picked\nsamples. This paper fills this gap in optical flow estimation in two ways. We\npropose DPFlow, an adaptive optical flow architecture capable of generalizing\nup to 8K resolution inputs while trained with only low-resolution samples. We\nalso introduce Kubric-NK, a new benchmark for evaluating optical flow methods\nwith input resolutions ranging from 1K to 8K. Our high-resolution evaluation\npushes the boundaries of existing methods and reveals new insights about their\ngeneralization capabilities. Extensive experimental results show that DPFlow\nachieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and\nother high-resolution benchmarks."}
{"id": "2503.15450", "pdf": "https://arxiv.org/pdf/2503.15450", "abs": "https://arxiv.org/abs/2503.15450", "authors": ["Tongyao Zhu", "Qian Liu", "Haonan Wang", "Shiqi Chen", "Xiangming Gu", "Tianyu Pang", "Min-Yen Kan"], "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling", "categories": ["cs.CL"], "comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models", "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder."}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark."}
{"id": "2503.15454", "pdf": "https://arxiv.org/pdf/2503.15454", "abs": "https://arxiv.org/abs/2503.15454", "authors": ["Yuelyu Ji", "Hang Zhang", "Yanshan Wang"], "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering Systems", "categories": ["cs.CL"], "comment": null, "summary": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making."}
{"id": "2503.14897", "pdf": "https://arxiv.org/pdf/2503.14897", "abs": "https://arxiv.org/abs/2503.14897", "authors": ["Vaibhav Rathore", "Shubhranil B", "Saikat Dutta", "Sarthak Mehrotra", "Zsolt Kira", "Biplab Banerjee"], "title": "When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD."}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."}
{"id": "2503.14905", "pdf": "https://arxiv.org/pdf/2503.14905", "abs": "https://arxiv.org/abs/2503.14905", "authors": ["Siwei Wen", "Junyan Ye", "Peilin Feng", "Hengrui Kang", "Zichen Wen", "Yize Chen", "Jiang Wu", "Wenjun Wu", "Conghui He", "Weijia Li"], "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM."}
{"id": "2503.15469", "pdf": "https://arxiv.org/pdf/2503.15469", "abs": "https://arxiv.org/abs/2503.15469", "authors": ["ZhengLin Lai", "MengYao Liao", "Dong Xu"], "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages,1 figure", "summary": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency."}
{"id": "2503.14910", "pdf": "https://arxiv.org/pdf/2503.14910", "abs": "https://arxiv.org/abs/2503.14910", "authors": ["Jingyi Liao", "Xun Xu", "Yongyi Su", "Rong-Cheng Tu", "Yifan Liu", "Dacheng Tao", "Xulei Yang"], "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."}
{"id": "2503.15484", "pdf": "https://arxiv.org/pdf/2503.15484", "abs": "https://arxiv.org/abs/2503.15484", "authors": ["Taylor Sorensen", "Pushkar Mishra", "Roma Patel", "Michael Henry Tessler", "Michiel Bakker", "Georgina Evans", "Iason Gabriel", "Noah Goodman", "Verena Rieser"], "title": "Value Profiles for Encoding Human Variation", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information."}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public."}
{"id": "2503.14521", "pdf": "https://arxiv.org/pdf/2503.14521", "abs": "https://arxiv.org/abs/2503.14521", "authors": ["Yihang Chen", "Haikang Deng", "Kaiqiao Han", "Qingyue Zhao"], "title": "Policy Frameworks for Transparent Chain-of-Thought Reasoning in Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\ndecomposing complex problems into step-by-step solutions, improving performance\non reasoning tasks. However, current CoT disclosure policies vary widely across\ndifferent models in frontend visibility, API access, and pricing strategies,\nlacking a unified policy framework. This paper analyzes the dual-edged\nimplications of full CoT disclosure: while it empowers small-model\ndistillation, fosters trust, and enables error diagnosis, it also risks\nviolating intellectual property, enabling misuse, and incurring operational\ncosts. We propose a tiered-access policy framework that balances transparency,\naccountability, and security by tailoring CoT availability to academic,\nbusiness, and general users through ethical licensing, structured reasoning\noutputs, and cross-tier safeguards. By harmonizing accessibility with ethical\nand operational considerations, this framework aims to advance responsible AI\ndeployment while mitigating risks of misuse or misinterpretation."}
{"id": "2503.14912", "pdf": "https://arxiv.org/pdf/2503.14912", "abs": "https://arxiv.org/abs/2503.14912", "authors": ["Gahye Lee", "Hyejeong Yoon", "Jungeon Kim", "Seungyong Lee"], "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes", "categories": ["cs.CV", "I.4.8; I.3.5"], "comment": "Accepted to 3DV 2025", "summary": "This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing."}
{"id": "2503.14527", "pdf": "https://arxiv.org/pdf/2503.14527", "abs": "https://arxiv.org/abs/2503.14527", "authors": ["Mohammed Alnajjar", "Khalid Alnajjar", "Mika H√§m√§l√§inen"], "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare SMEs", "categories": ["cs.CY", "cs.CL", "cs.HC"], "comment": null, "summary": "This study examines AI adoption among Finnish healthcare SMEs through\nsemi-structured interviews with six health-tech companies. We identify three AI\nengagement categories: AI-curious (exploring AI), AI-embracing (integrating\nAI), and AI-catering (providing AI solutions). Our proposed threefold model\nhighlights key adoption barriers, including regulatory complexities, technical\nexpertise gaps, and financial constraints. While SMEs recognize AI's potential,\nmost remain in early adoption stages. We provide actionable recommendations to\naccelerate AI integration, focusing on regulatory reforms, talent development,\nand inter-company collaboration, offering valuable insights for healthcare\norganizations, policymakers, and researchers."}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios."}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning."}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2503.14935", "pdf": "https://arxiv.org/pdf/2503.14935", "abs": "https://arxiv.org/abs/2503.14935", "authors": ["Chongjun Tu", "Lin Zhang", "Pengtao Chen", "Peng Ye", "Xianfang Zeng", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "FAVOR-Bench project page: https://favor-bench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}."}
{"id": "2503.14615", "pdf": "https://arxiv.org/pdf/2503.14615", "abs": "https://arxiv.org/abs/2503.14615", "authors": ["Selim Jerad", "Anej Svete", "Jiaoda Li", "Ryan Cotterell"], "title": "Unique Hard Attention: A Tale of Two Sides", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "comment": null, "summary": "Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality."}
{"id": "2503.14938", "pdf": "https://arxiv.org/pdf/2503.14938", "abs": "https://arxiv.org/abs/2503.14938", "authors": ["Zhong Ji", "Ci Liu", "Jingren Liu", "Chen Tang", "Yanwei Pang", "Xuelong Li"], "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization."}
{"id": "2503.14649", "pdf": "https://arxiv.org/pdf/2503.14649", "abs": "https://arxiv.org/abs/2503.14649", "authors": ["Wenqi Jiang", "Suvinay Subramanian", "Cat Graves", "Gustavo Alonso", "Amir Yazdanbakhsh", "Vidushi Dadu"], "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions."}
{"id": "2503.14939", "pdf": "https://arxiv.org/pdf/2503.14939", "abs": "https://arxiv.org/abs/2503.14939", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/."}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark."}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences."}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["√Älex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC"}
{"id": "2503.14943", "pdf": "https://arxiv.org/pdf/2503.14943", "abs": "https://arxiv.org/abs/2503.14943", "authors": ["Yifan Wang", "Ivan Molodetskikh", "Ondrej Texler", "Dimitar Dinev"], "title": "3D Engine-ready Photorealistic Avatars via Dynamic Textures", "categories": ["cs.CV"], "comment": null, "summary": "As the digital and physical worlds become more intertwined, there has been a\nlot of interest in digital avatars that closely resemble their real-world\ncounterparts. Current digitization methods used in 3D production pipelines\nrequire costly capture setups, making them impractical for mass usage among\ncommon consumers. Recent academic literature has found success in\nreconstructing humans from limited data using implicit representations (e.g.,\nvoxels used in NeRFs), which are able to produce impressive videos. However,\nthese methods are incompatible with traditional rendering pipelines, making it\ndifficult to use them in applications such as games. In this work, we propose\nan end-to-end pipeline that builds explicitly-represented photorealistic 3D\navatars using standard 3D assets. Our key idea is the use of\ndynamically-generated textures to enhance the realism and visually mask\ndeficiencies in the underlying mesh geometry. This allows for seamless\nintegration with current graphics pipelines while achieving comparable visual\nquality to state-of-the-art 3D avatar generation methods."}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."}
{"id": "2503.14944", "pdf": "https://arxiv.org/pdf/2503.14944", "abs": "https://arxiv.org/abs/2503.14944", "authors": ["Zihan Cao", "Yu Zhong", "Ziqi Wang", "Liang-Jian Deng"], "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF."}
{"id": "2503.15204", "pdf": "https://arxiv.org/pdf/2503.15204", "abs": "https://arxiv.org/abs/2503.15204", "authors": ["Tittaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Sorrawit Treesuk"], "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR", "cs.MA"], "comment": "14 pages, 2 figures", "summary": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security."}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements."}
{"id": "2503.15338", "pdf": "https://arxiv.org/pdf/2503.15338", "abs": "https://arxiv.org/abs/2503.15338", "authors": ["Junyi Ao", "Dekun Chen", "Xiaohai Tian", "Wenjie Feng", "Jun Zhang", "Lu Lu", "Yuxuan Wang", "Haizhou Li", "Zhizheng Wu"], "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio."}
{"id": "2503.14948", "pdf": "https://arxiv.org/pdf/2503.14948", "abs": "https://arxiv.org/abs/2503.14948", "authors": ["Hao Liang", "Zhipeng Dong", "Yi Yang", "Mengyin Fu"], "title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively."}
{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization."}
{"id": "2503.14950", "pdf": "https://arxiv.org/pdf/2503.14950", "abs": "https://arxiv.org/abs/2503.14950", "authors": ["Joseph Emmanuel DL Dayo", "Prospero C. Naval Jr"], "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data."}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io"}
{"id": "2503.14953", "pdf": "https://arxiv.org/pdf/2503.14953", "abs": "https://arxiv.org/abs/2503.14953", "authors": ["Yang Liu", "Wentao Feng", "Zhuoyao Liu", "Shudong Huang", "Jiancheng Lv"], "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching", "categories": ["cs.CV"], "comment": null, "summary": "Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods."}
{"id": "2503.14955", "pdf": "https://arxiv.org/pdf/2503.14955", "abs": "https://arxiv.org/abs/2503.14955", "authors": ["Bike Chen", "Antti Tikanm√§ki", "Juha R√∂ning"], "title": "Depth-Aware Range Image-Based Model for Point Cloud Segmentation", "categories": ["cs.CV"], "comment": "No Comments", "summary": "Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost."}
{"id": "2503.14957", "pdf": "https://arxiv.org/pdf/2503.14957", "abs": "https://arxiv.org/abs/2503.14957", "authors": ["Thanh-Son Nguyen", "Hong Yang", "Tzeh Yuan Neoh", "Hao Zhang", "Ee Yeo Keat", "Basura Fernando"], "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon."}
{"id": "2503.14958", "pdf": "https://arxiv.org/pdf/2503.14958", "abs": "https://arxiv.org/abs/2503.14958", "authors": ["Zixuan Zheng", "Yilei Shi", "Chunlei Li", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB."}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods."}
{"id": "2503.14966", "pdf": "https://arxiv.org/pdf/2503.14966", "abs": "https://arxiv.org/abs/2503.14966", "authors": ["Tingxiu Chen", "Yilei Shi", "Zixuan Zheng", "Bingcong Yan", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "MICCAI 2024", "summary": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V."}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo."}
{"id": "2503.14975", "pdf": "https://arxiv.org/pdf/2503.14975", "abs": "https://arxiv.org/abs/2503.14975", "authors": ["Zihan Cao", "Yu Zhong", "Liang-Jian Deng"], "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM."}
{"id": "2503.14979", "pdf": "https://arxiv.org/pdf/2503.14979", "abs": "https://arxiv.org/abs/2503.14979", "authors": ["Yaxiong Chen", "Junjian Hu", "Chunlei Li", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN."}
{"id": "2503.14983", "pdf": "https://arxiv.org/pdf/2503.14983", "abs": "https://arxiv.org/abs/2503.14983", "authors": ["Zanting Ye", "Xiaolong Niu", "Xuanbin Wu", "Wenxiang Yi", "Yuan Chang", "Lijun Lu"], "title": "Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS."}
{"id": "2503.14990", "pdf": "https://arxiv.org/pdf/2503.14990", "abs": "https://arxiv.org/abs/2503.14990", "authors": ["K√©vin Polisano", "Sylvain Meignen", "Nils Laurent", "Hubert Leterme"], "title": "Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference."}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub."}
{"id": "2503.15001", "pdf": "https://arxiv.org/pdf/2503.15001", "abs": "https://arxiv.org/abs/2503.15001", "authors": ["Michael Neri", "Federica Battisti"], "title": "Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA", "summary": "During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA."}
{"id": "2503.15004", "pdf": "https://arxiv.org/pdf/2503.15004", "abs": "https://arxiv.org/abs/2503.15004", "authors": ["Annalena Bl√§nsdorf", "Tristan Wirth", "Arne Rak", "Thomas P√∂llabauer", "Volker Knauthe", "Arjan Kuijper"], "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset."}
{"id": "2503.15005", "pdf": "https://arxiv.org/pdf/2503.15005", "abs": "https://arxiv.org/abs/2503.15005", "authors": ["Shengqiong Wu", "Hao Fei", "Tat-Seng Chua"], "title": "Universal Scene Graph Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance."}
{"id": "2503.15016", "pdf": "https://arxiv.org/pdf/2503.15016", "abs": "https://arxiv.org/abs/2503.15016", "authors": ["Fethi Harkat", "Tiphaine Deuberet", "Guillaume Gey", "Val√©rie Perrier", "K√©vin Polisano"], "title": "Manifold Learning for Hyperspectral Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results."}
{"id": "2503.15017", "pdf": "https://arxiv.org/pdf/2503.15017", "abs": "https://arxiv.org/abs/2503.15017", "authors": ["Yunwei Lan", "Zhigao Cui", "Chang Liu", "Jialun Peng", "Nian Wang", "Xin Luo", "Dong Liu"], "title": "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired Training", "categories": ["cs.CV"], "comment": "Accepted by AAAI2025", "summary": "Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https://github.com/ywxjm/Diff-Dehazer."}
{"id": "2503.15019", "pdf": "https://arxiv.org/pdf/2503.15019", "abs": "https://arxiv.org/abs/2503.15019", "authors": ["Shengqiong Wu", "Hao Fei", "Jingkang Yang", "Xiangtai Li", "Juncheng Li", "Hanwang Zhang", "Tat-seng Chua"], "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method."}
{"id": "2503.15022", "pdf": "https://arxiv.org/pdf/2503.15022", "abs": "https://arxiv.org/abs/2503.15022", "authors": ["Saad Lahlali", "Sandra Kara", "Hejer Ammar", "Florian Chabot", "Nicolas Granger", "Herv√© Le Borgne", "Quoc-Cuong Pham"], "title": "xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Object discovery, which refers to the task of localizing objects without\nhuman annotations, has gained significant attention in 2D image analysis.\nHowever, despite this growing interest, it remains under-explored in 3D data,\nwhere approaches rely exclusively on 3D motion, despite its several challenges.\nIn this paper, we present a novel framework that leverages advances in 2D\nobject discovery which are based on 2D motion to exploit the advantages of such\nmotion cues being more flexible and generalizable and to bridge the gap between\n2D and 3D modalities. Our primary contributions are twofold: (i) we introduce\nDIOD-3D, the first baseline for multi-object discovery in 3D data using 2D\nmotion, incorporating scene completion as an auxiliary task to enable dense\nobject localization from sparse input data; (ii) we develop xMOD, a cross-modal\ntraining framework that integrates 2D and 3D data while always using 2D motion\ncues. xMOD employs a teacher-student training paradigm across the two\nmodalities to mitigate confirmation bias by leveraging the domain gap. During\ninference, the model supports both RGB-only and point cloud-only inputs.\nAdditionally, we propose a late-fusion technique tailored to our pipeline that\nfurther enhances performance when both modalities are available at inference.\nWe evaluate our approach extensively on synthetic (TRIP-PD) and challenging\nreal-world datasets (KITTI and Waymo). Notably, our approach yields a\nsubstantial performance improvement compared with the 2D object discovery\nstate-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50\nscore. The code is available at https://github.com/CEA-LIST/xMOD"}
{"id": "2503.15023", "pdf": "https://arxiv.org/pdf/2503.15023", "abs": "https://arxiv.org/abs/2503.15023", "authors": ["Chaouki Boufenar", "Mehdi Ayoub Rabiai", "Boualem Nadjib Zahaf", "Khelil Rafik Ouaras"], "title": "Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications."}
{"id": "2503.15024", "pdf": "https://arxiv.org/pdf/2503.15024", "abs": "https://arxiv.org/abs/2503.15024", "authors": ["Jin Wang", "Chenghui Lv", "Xian Li", "Shichao Dong", "Huadong Li", "kelu Yao", "Chao Li", "Wenqi Shao", "Ping Luo"], "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models", "categories": ["cs.CV"], "comment": "31 pages, 19 figures", "summary": "Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/."}
{"id": "2503.15056", "pdf": "https://arxiv.org/pdf/2503.15056", "abs": "https://arxiv.org/abs/2503.15056", "authors": ["Suhyeon Lee", "Kwanyoung Kim", "Jong Chul Ye"], "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation", "categories": ["cs.CV"], "comment": "25 pages, 16 figures", "summary": "Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html"}
{"id": "2503.15060", "pdf": "https://arxiv.org/pdf/2503.15060", "abs": "https://arxiv.org/abs/2503.15060", "authors": ["Imanol G. Estepa", "Jes√∫s M. Rodr√≠guez-de-Vera", "Ignacio Saras√∫a", "Bhalaji Nagarajan", "Petia Radeva"], "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis", "categories": ["cs.CV", "cs.AI", "I.5.4; I.5.1; I.2.10"], "comment": "The source code is available in https://github.com/ImaGonEs/Sorcen", "summary": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models."}
{"id": "2503.15070", "pdf": "https://arxiv.org/pdf/2503.15070", "abs": "https://arxiv.org/abs/2503.15070", "authors": ["Kana Kurata", "Hitoshi Niigaki", "Xiaojun Wu", "Ryuichi Tanida"], "title": "MultiBARF: Integrating Imagery of Different Wavelength Regions by Using Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "Optical sensor applications have become popular through digital\ntransformation. Linking observed data to real-world locations and combining\ndifferent image sensors is essential to make the applications practical and\nefficient. However, data preparation to try different sensor combinations\nrequires high sensing and image processing expertise. To make data preparation\neasier for users unfamiliar with sensing and image processing, we have\ndeveloped MultiBARF. This method replaces the co-registration and geometric\ncalibration by synthesizing pairs of two different sensor images and depth\nimages at assigned viewpoints. Our method extends Bundle Adjusting Neural\nRadiance Fields(BARF), a deep neural network-based novel view synthesis method,\nfor the two imagers. Through experiments on visible light and thermographic\nimages, we demonstrate that our method superimposes two color channels of those\nsensor images on NeRF."}
{"id": "2503.15087", "pdf": "https://arxiv.org/pdf/2503.15087", "abs": "https://arxiv.org/abs/2503.15087", "authors": ["Christoph Griesbacher", "Christian Fruhwirth-Reisinger"], "title": "An Investigation of Beam Density on LiDAR Object Detection Performance", "categories": ["cs.CV"], "comment": "Accepted by CVWW 2025", "summary": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference."}
{"id": "2503.15096", "pdf": "https://arxiv.org/pdf/2503.15096", "abs": "https://arxiv.org/abs/2503.15096", "authors": ["Yang Liu", "Qianqian Xu", "Peisong Wen", "Siran Dai", "Qingming Huang"], "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "The past decade has witnessed notable achievements in self-supervised\nlearning for video tasks. Recent efforts typically adopt the Masked Video\nModeling (MVM) paradigm, leading to significant progress on multiple video\ntasks. However, two critical challenges remain: 1) Without human annotations,\nthe random temporal sampling introduces uncertainty, increasing the difficulty\nof model training. 2) Previous MVM methods primarily recover the masked patches\nin the pixel space, leading to insufficient information compression for\ndownstream tasks. To address these challenges jointly, we propose a\nself-supervised framework that leverages Temporal Correspondence for video\nRepresentation learning (T-CoRe). For challenge 1), we propose a sandwich\nsampling strategy that selects two auxiliary frames to reduce reconstruction\nuncertainty in a two-side-squeezing manner. Addressing challenge 2), we\nintroduce an auxiliary branch into a self-distillation architecture to restore\nrepresentations in the latent space, generating high-level semantic\nrepresentations enriched with temporal information. Experiments of T-CoRe\nconsistently present superior performance across several downstream tasks,\ndemonstrating its effectiveness for video representation learning. The code is\navailable at https://github.com/yafeng19/T-CORE."}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/"}
{"id": "2503.15110", "pdf": "https://arxiv.org/pdf/2503.15110", "abs": "https://arxiv.org/abs/2503.15110", "authors": ["Zinqin Huang", "Gu Wang", "Chenyangguang Zhang", "Ruida Zhang", "Xiu Li", "Xiangyang Ji"], "title": "GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recent advances in RGBD-based category-level object pose estimation have been\nlimited by their reliance on precise depth information, restricting their\nbroader applicability. In response, RGB-based methods have been developed.\nAmong these methods, geometry-guided pose regression that originated from\ninstance-level tasks has demonstrated strong performance. However, we argue\nthat the NOCS map is an inadequate intermediate representation for\ngeometry-guided pose regression method, as its many-to-one correspondence with\ncategory-level pose introduces redundant instance-specific information,\nresulting in suboptimal results. This paper identifies the intra-class\nvariation problem inherent in pose regression based solely on the NOCS map and\nproposes the Intra-class Variation-Free Consensus (IVFC) map, a novel\ncoordinate representation generated from the category-level consensus model. By\nleveraging the complementary strengths of the NOCS map and the IVFC map, we\nintroduce GIVEPose, a framework that implements Gradual Intra-class Variation\nElimination for category-level object pose estimation. Extensive evaluations on\nboth synthetic and real-world datasets demonstrate that GIVEPose significantly\noutperforms existing state-of-the-art RGB-based approaches, achieving\nsubstantial improvements in category-level object pose estimation. Our code is\navailable at https://github.com/ziqin-h/GIVEPose."}
{"id": "2503.15126", "pdf": "https://arxiv.org/pdf/2503.15126", "abs": "https://arxiv.org/abs/2503.15126", "authors": ["Haoyu Ji", "Bowen Chen", "Weihong Ren", "Wenze Huang", "Zhihao Yang", "Zhiyong Wang", "Honghai Liu"], "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results."}
{"id": "2503.15138", "pdf": "https://arxiv.org/pdf/2503.15138", "abs": "https://arxiv.org/abs/2503.15138", "authors": ["Mingzhe Zheng", "Yongqi Xu", "Haojian Huang", "Xuran Ma", "Yexin Liu", "Wenjie Shu", "Yatian Pang", "Feilong Tang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention", "categories": ["cs.CV"], "comment": "Code: https://github.com/DuNGEOnmassster/VideoGen-of-Thought.git;\n  Webpage: https://cheliosoops.github.io/VGoT/", "summary": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives."}
{"id": "2503.15141", "pdf": "https://arxiv.org/pdf/2503.15141", "abs": "https://arxiv.org/abs/2503.15141", "authors": ["Nikola ƒêukiƒá", "Tim Lebailly", "Tinne Tuytelaars"], "title": "Object-Centric Pretraining via Target Encoder Bootstrapping", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https://github.com/djukicn/ocebo."}
{"id": "2503.15144", "pdf": "https://arxiv.org/pdf/2503.15144", "abs": "https://arxiv.org/abs/2503.15144", "authors": ["Xing He", "Zhe Zhu", "Liangliang Nan", "Honghua Chen", "Jing Qin", "Mingqiang Wei"], "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}."}
{"id": "2503.15156", "pdf": "https://arxiv.org/pdf/2503.15156", "abs": "https://arxiv.org/abs/2503.15156", "authors": ["Joost Luijmes", "Alexander Gielisse", "Roman Knyazhitskiy", "Jan van Gemert"], "title": "ARC: Anchored Representation Clouds for High-Resolution INR Classification", "categories": ["cs.CV"], "comment": "Accepted at the ICLR 2025 Workshop on Neural Network Weights as a New\n  Data Modality", "summary": "Implicit neural representations (INRs) encode signals in neural network\nweights as a memory-efficient representation, decoupling sampling resolution\nfrom the associated resource costs. Current INR image classification methods\nare demonstrated on low-resolution data and are sensitive to image-space\ntransformations. We attribute these issues to the global, fully-connected MLP\nneural network architecture encoding of current INRs, which lack mechanisms for\nlocal representation: MLPs are sensitive to absolute image location and\nstruggle with high-frequency details. We propose ARC: Anchored Representation\nClouds, a novel INR architecture that explicitly anchors latent vectors locally\nin image-space. By introducing spatial structure to the latent vectors, ARC\ncaptures local image data which in our testing leads to state-of-the-art\nimplicit image classification of both low- and high-resolution images and\nincreased robustness against image-space translation. Code can be found at\nhttps://github.com/JLuij/anchored_representation_clouds."}
{"id": "2503.15161", "pdf": "https://arxiv.org/pdf/2503.15161", "abs": "https://arxiv.org/abs/2503.15161", "authors": ["Yang Li", "Soumya Snigdha Kundu", "Maxence Boels", "Toktam Mahmoodi", "Sebastien Ourselin", "Tom Vercauteren", "Prokar Dasgupta", "Jonathan Shapey", "Alejandro Granados"], "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework", "categories": ["cs.CV"], "comment": "10 pages, 2 figures, under review @ MICCAI", "summary": "Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr."}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["√Älex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC"}
{"id": "2503.15185", "pdf": "https://arxiv.org/pdf/2503.15185", "abs": "https://arxiv.org/abs/2503.15185", "authors": ["Gyeongrok Oh", "Sungjune Kim", "Heeju Ko", "Hyung-gun Chi", "Jinkyu Kim", "Dongwook Lee", "Daehyun Ji", "Sungjoon Choi", "Sujin Jang", "Sangpil Kim"], "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR2025", "summary": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution."}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."}
{"id": "2503.15197", "pdf": "https://arxiv.org/pdf/2503.15197", "abs": "https://arxiv.org/abs/2503.15197", "authors": ["Feifei Li", "Mi Zhang", "Yiming Sun", "Min Yang"], "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts."}
{"id": "2503.15208", "pdf": "https://arxiv.org/pdf/2503.15208", "abs": "https://arxiv.org/abs/2503.15208", "authors": ["Jiazhe Guo", "Yikang Ding", "Xiwu Chen", "Shuo Chen", "Bohan Li", "Yingshuang Zou", "Xiaoyang Lyu", "Feiyang Tan", "Xiaojuan Qi", "Zhiheng Li", "Hao Zhao"], "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations."}
{"id": "2503.15211", "pdf": "https://arxiv.org/pdf/2503.15211", "abs": "https://arxiv.org/abs/2503.15211", "authors": ["Zechuan Li", "Hongshan Yu", "Yihao Ding", "Jinhao Qiao", "Basim Azam", "Naveed Akhtar"], "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet."}
{"id": "2503.15234", "pdf": "https://arxiv.org/pdf/2503.15234", "abs": "https://arxiv.org/abs/2503.15234", "authors": ["Wenlong Yu", "Qilong Wang", "Chuang Liu", "Dong Li", "Qinghua Hu"], "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR2025", "summary": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores."}
{"id": "2503.15260", "pdf": "https://arxiv.org/pdf/2503.15260", "abs": "https://arxiv.org/abs/2503.15260", "authors": ["Lei Shi", "Xi Fang", "Naiyu Wang", "Junxing Zhang"], "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods."}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased."}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/"}
{"id": "2503.15275", "pdf": "https://arxiv.org/pdf/2503.15275", "abs": "https://arxiv.org/abs/2503.15275", "authors": ["Xiang Li", "Heqian Qiu", "Lanxiao Wang", "Hanwen Zhang", "Chenghao Qi", "Linfeng Han", "Huiyu Xiong", "Hongliang Li"], "title": "Challenges and Trends in Egocentric Vision: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield."}
{"id": "2503.15283", "pdf": "https://arxiv.org/pdf/2503.15283", "abs": "https://arxiv.org/abs/2503.15283", "authors": ["Teng-Fang Hsiao", "Bo-Kai Ruan", "Yi-Lun Wu", "Tzu-Ling Lin", "Hong-Han Shuai"], "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks."}
{"id": "2503.15284", "pdf": "https://arxiv.org/pdf/2503.15284", "abs": "https://arxiv.org/abs/2503.15284", "authors": ["Yuanchao Yue", "Hui Yuan", "Qinglong Miao", "Xiaolong Mao", "Raouf Hamzaoui", "Peter Eisert"], "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance."}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%."}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks."}
{"id": "2503.15295", "pdf": "https://arxiv.org/pdf/2503.15295", "abs": "https://arxiv.org/abs/2503.15295", "authors": ["Aoting Zhang", "Dongbao Yang", "Chang Liu", "Xiaopeng Hong", "Miao Shang", "Yu Zhou"], "title": "DCA: Dividing and Conquering Amnesia in Incremental Object Detection", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Incremental object detection (IOD) aims to cultivate an object detector that\ncan continuously localize and recognize novel classes while preserving its\nperformance on previous classes. Existing methods achieve certain success by\nimproving knowledge distillation and exemplar replay for transformer-based\ndetection frameworks, but the intrinsic forgetting mechanisms remain\nunderexplored. In this paper, we dive into the cause of forgetting and discover\nforgetting imbalance between localization and recognition in transformer-based\nIOD, which means that localization is less-forgetting and can generalize to\nfuture classes, whereas catastrophic forgetting occurs primarily on\nrecognition. Based on these insights, we propose a Divide-and-Conquer Amnesia\n(DCA) strategy, which redesigns the transformer-based IOD into a\nlocalization-then-recognition process. DCA can well maintain and transfer the\nlocalization ability, leaving decoupled fragile recognition to be specially\nconquered. To reduce feature drift in recognition, we leverage semantic\nknowledge encoded in pre-trained language models to anchor class\nrepresentations within a unified feature space across incremental tasks. This\ninvolves designing a duplex classifier fusion and embedding class semantic\nfeatures into the recognition decoding process in the form of queries.\nExtensive experiments validate that our approach achieves state-of-the-art\nperformance, especially for long-term incremental scenarios. For example, under\nthe four-step setting on MS-COCO, our DCA strategy significantly improves the\nfinal AP by 6.9%."}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/."}
{"id": "2503.15337", "pdf": "https://arxiv.org/pdf/2503.15337", "abs": "https://arxiv.org/abs/2503.15337", "authors": ["Hao Tan", "Zichang Tan", "Jun Li", "Ajian Liu", "Jun Wan", "Zhen Lei"], "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Identifying multiple novel classes in an image, known as open-vocabulary\nmulti-label recognition, is a challenging task in computer vision. Recent\nstudies explore the transfer of powerful vision-language models such as CLIP.\nHowever, these approaches face two critical challenges: (1) The local semantics\nof CLIP are disrupted due to its global pre-training objectives, resulting in\nunreliable regional predictions. (2) The matching property between image\nregions and candidate labels has been neglected, relying instead on naive\nfeature aggregation such as average pooling, which leads to spurious\npredictions from irrelevant regions. In this paper, we present RAM (Recover And\nMatch), a novel framework that effectively addresses the above issues. To\ntackle the first problem, we propose Ladder Local Adapter (LLA) to enforce\nrefocusing on local regions, recovering local semantics in a memory-friendly\nway. For the second issue, we propose Knowledge-Constrained Optimal Transport\n(KCOT) to suppress meaningless matching to non-GT labels by formulating the\ntask as an optimal transport problem. As a result, RAM achieves\nstate-of-the-art performance on various datasets from three distinct domains,\nand shows great potential to boost the existing methods. Code:\nhttps://github.com/EricTan7/RAM."}
{"id": "2503.15342", "pdf": "https://arxiv.org/pdf/2503.15342", "abs": "https://arxiv.org/abs/2503.15342", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Ali Khaleghi Rahimian", "Thomas MacDougall"], "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation."}
{"id": "2503.15361", "pdf": "https://arxiv.org/pdf/2503.15361", "abs": "https://arxiv.org/abs/2503.15361", "authors": ["Qingsen Yan", "Tao Hu", "Genggeng Chen", "Wei Dong", "Yanning Zhang"], "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods."}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model."}
{"id": "2503.15404", "pdf": "https://arxiv.org/pdf/2503.15404", "abs": "https://arxiv.org/abs/2503.15404", "authors": ["Yuchen Ren", "Zhengyu Zhao", "Chenhao Lin", "Bo Yang", "Lu Zhou", "Zhe Liu", "Chao Shen"], "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement", "categories": ["cs.CV", "cs.CR"], "comment": "CVPR2025", "summary": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks."}
{"id": "2503.15412", "pdf": "https://arxiv.org/pdf/2503.15412", "abs": "https://arxiv.org/abs/2503.15412", "authors": ["Fereshteh Forghani", "Jason J. Yu", "Tristan Aumentado-Armstrong", "Konstantinos G. Derpanis", "Marcus A. Brubaker"], "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model."}
{"id": "2503.15415", "pdf": "https://arxiv.org/pdf/2503.15415", "abs": "https://arxiv.org/abs/2503.15415", "authors": ["Giovanni Floreale", "Piero Baraldi", "Enrico Zio", "Olga Fink"], "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells."}
{"id": "2503.15417", "pdf": "https://arxiv.org/pdf/2503.15417", "abs": "https://arxiv.org/abs/2503.15417", "authors": ["Harold Haodong Chen", "Haojian Huang", "Xianfeng Wu", "Yexin Liu", "Yajing Bai", "Wen-Jie Shu", "Harry Yang", "Ser-Nam Lim"], "title": "Temporal Regularization Makes Your Video Generator Stronger", "categories": ["cs.CV", "cs.AI"], "comment": "Project: https://haroldchen19.github.io/FluxFlow/", "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality."}
{"id": "2503.15426", "pdf": "https://arxiv.org/pdf/2503.15426", "abs": "https://arxiv.org/abs/2503.15426", "authors": ["Wei Tang", "Yanpeng Sun", "Qinying Gu", "Zechao Li"], "title": "Visual Position Prompt for MLLM based Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance."}
{"id": "2503.15435", "pdf": "https://arxiv.org/pdf/2503.15435", "abs": "https://arxiv.org/abs/2503.15435", "authors": ["Baolu Li", "Zongzhe Xu", "Jinlong Li", "Xinyu Liu", "Jianwu Fang", "Xiaopeng Li", "Hongkai Yu"], "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception", "categories": ["cs.CV"], "comment": "accepted by ICRA 2025", "summary": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset."}
{"id": "2503.15451", "pdf": "https://arxiv.org/pdf/2503.15451", "abs": "https://arxiv.org/abs/2503.15451", "authors": ["Lixing Xiao", "Shunlin Lu", "Huaijin Pi", "Ke Fan", "Liang Pan", "Yueer Zhou", "Ziyong Feng", "Xiaowei Zhou", "Sida Peng", "Jingbo Wang"], "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space", "categories": ["cs.CV"], "comment": "Project Page: https://zju3dv.github.io/MotionStreamer/", "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/"}
{"id": "2503.15457", "pdf": "https://arxiv.org/pdf/2503.15457", "abs": "https://arxiv.org/abs/2503.15457", "authors": ["Yuanzhi Zhu", "Xi Wang", "St√©phane Lathuili√®re", "Vicky Kalogeiton"], "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling."}
{"id": "2503.15465", "pdf": "https://arxiv.org/pdf/2503.15465", "abs": "https://arxiv.org/abs/2503.15465", "authors": ["Ruichen Chen", "Keith G. Mills", "Di Niu"], "title": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers", "categories": ["cs.CV"], "comment": "The code is available at https://github.com/cccrrrccc/FP4DiT", "summary": "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP."}
{"id": "2503.15470", "pdf": "https://arxiv.org/pdf/2503.15470", "abs": "https://arxiv.org/abs/2503.15470", "authors": ["Boshen Xu", "Yuting Mei", "Xinbi Liu", "Sipeng Zheng", "Qin Jin"], "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM", "summary": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM."}
{"id": "2503.15474", "pdf": "https://arxiv.org/pdf/2503.15474", "abs": "https://arxiv.org/abs/2503.15474", "authors": ["Maciej Ziaja", "Pawel Kowaleczko", "Daniel Kostrzewa", "Nicolas Long√©p√©", "Michal Kawulok"], "title": "Toward task-driven satellite image super-resolution", "categories": ["cs.CV"], "comment": "Submitted to IEEE IGARSS 2024", "summary": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution."}
{"id": "2503.15475", "pdf": "https://arxiv.org/pdf/2503.15475", "abs": "https://arxiv.org/abs/2503.15475", "authors": ["Foundation AI Team", "Kiran Bhat", "Nishchaie Khanna", "Karun Channa", "Tinghui Zhou", "Yiheng Zhu", "Xiaoxia Sun", "Charles Shang", "Anirudh Sudarshan", "Maurice Chu", "Daiqing Li", "Kangle Deng", "Jean-Philippe Fauconnier", "Tijmen Verhulsdonck", "Maneesh Agrawala", "Kayvon Fatahalian", "Alexander Weiss", "Christian Reiser", "Ravi Kiran Chirravuri", "Ravali Kandur", "Alejandro Pelaez", "Akash Garg", "Michael Palleschi", "Jessica Wang", "Skylar Litz", "Leon Liu", "Anying Li", "David Harmon", "Derek Liu", "Liangjun Feng", "Denis Goupil", "Lukas Kuczynski", "Jihyun Yoon", "Naveen Marri", "Peiye Zhuang", "Yinan Zhang", "Brian Yin", "Haomiao Jiang", "Marcel van Workum", "Thomas Lane", "Bryce Erickson", "Salil Pathare", "Kyle Price", "Anupam Singh", "David Baszucki"], "title": "Cube: A Roblox View of 3D Intelligence", "categories": ["cs.CV"], "comment": "Our code and model weights can be found at:\n  https://github.com/Roblox/cube", "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence."}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io"}
{"id": "2503.14523", "pdf": "https://arxiv.org/pdf/2503.14523", "abs": "https://arxiv.org/abs/2503.14523", "authors": ["Siyi Wu", "Leyi Zhao", "Haitian Ma", "Xinyuan Song"], "title": "SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation via SDF Pre-training and Topology-Aware Fine-Tuning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular and curvilinear structures, such as blood\nvessels, neurons, and road networks, is crucial in various applications. A key\nchallenge is ensuring topological correctness while maintaining computational\nefficiency. Existing approaches often employ topological loss functions based\non persistent homology, such as Betti error, to enforce structural consistency.\nHowever, these methods suffer from high computational costs and are insensitive\nto pixel-level accuracy, often requiring additional loss terms like Dice or MSE\nto compensate. To address these limitations, we propose \\textbf{SDF-TopoNet},\nan improved topology-aware segmentation framework that enhances both\nsegmentation accuracy and training efficiency. Our approach introduces a novel\ntwo-stage training strategy. In the pre-training phase, we utilize the signed\ndistance function (SDF) as an auxiliary learning target, allowing the model to\nencode topological information without directly relying on computationally\nexpensive topological loss functions. In the fine-tuning phase, we incorporate\na dynamic adapter alongside a refined topological loss to ensure topological\ncorrectness while mitigating overfitting and computational overhead. We\nevaluate our method on five benchmark datasets. Experimental results\ndemonstrate that SDF-TopoNet outperforms existing methods in both topological\naccuracy and quantitative segmentation metrics, while significantly reducing\ntraining complexity."}
{"id": "2503.14525", "pdf": "https://arxiv.org/pdf/2503.14525", "abs": "https://arxiv.org/abs/2503.14525", "authors": ["Frans Zdyb", "Albert Alonso", "Julius B. Kirkegaard"], "title": "Spline refinement with differentiable rendering", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Detecting slender, overlapping structures remains a challenge in\ncomputational microscopy. While recent coordinate-based approaches improve\ndetection, they often produce less accurate splines than pixel-based methods.\nWe introduce a training-free differentiable rendering approach to spline\nrefinement, achieving both high reliability and sub-pixel accuracy. Our method\nimproves spline quality, enhances robustness to distribution shifts, and\nshrinks the gap between synthetic and real-world data. Being fully\nunsupervised, the method is a drop-in replacement for the popular active\ncontour model for spline refinement. Evaluated on C. elegans nematodes, a\npopular model organism for drug discovery and biomedical research, we\ndemonstrate that our approach combines the strengths of both coordinate- and\npixel-based methods."}
{"id": "2503.14534", "pdf": "https://arxiv.org/pdf/2503.14534", "abs": "https://arxiv.org/abs/2503.14534", "authors": ["Bibi Erum Ayesha", "T. Satyanarayana Murthy", "Palamakula Ramesh Babu", "Ramu Kuchipudi"], "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection."}
{"id": "2503.14536", "pdf": "https://arxiv.org/pdf/2503.14536", "abs": "https://arxiv.org/abs/2503.14536", "authors": ["Praveen Shastry", "Sowmya Chowdary Muthulur", "Naveen Kumarasami", "Anandakumar D", "Mounigasri M", "Keerthana R", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam", "Revathi Ezhumalai", "Abitha Marimuthu"], "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 92C55, 68U10, 92C50, 60G35"], "comment": "10 pages , 3 figures", "summary": "Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings."}
{"id": "2503.14538", "pdf": "https://arxiv.org/pdf/2503.14538", "abs": "https://arxiv.org/abs/2503.14538", "authors": ["Ananya Ganapthy", "Praveen Shastry", "Naveen Kumarasami", "Anandakumar D", "Keerthana R", "Mounigasri M", "Varshinipriya M", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam"], "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 68T45, 92C55, 92C50, 68U10"], "comment": "11 pages, 3 figures", "summary": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings."}
{"id": "2503.14542", "pdf": "https://arxiv.org/pdf/2503.14542", "abs": "https://arxiv.org/abs/2503.14542", "authors": ["Agnieszka Sroka-Oleksiak", "Adam Pardyl", "Dawid Rymarczyk", "Aldona Olechowska-JarzƒÖb", "Katarzyna Biegun-Dro≈ºd≈º", "Dorota Ocho≈Ñska", "Micha≈Ç Wronka", "Adriana Borowa", "Tomasz Gosiewski", "Mi≈Çosz Adamczyk", "Henryk Telega", "Bartosz Zieli≈Ñski", "Monika Brzychczy-W≈Çoch"], "title": "AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Sepsis is a life-threatening condition which requires rapid diagnosis and\ntreatment. Traditional microbiological methods are time-consuming and\nexpensive. In response to these challenges, deep learning algorithms were\ndeveloped to identify 14 bacteria species and 3 yeast-like fungi from\nmicroscopic images of Gram-stained smears of positive blood samples from sepsis\npatients.\n  A total of 16,637 Gram-stained microscopic images were used in the study. The\nanalysis used the Cellpose 3 model for segmentation and Attention-based Deep\nMultiple Instance Learning for classification. Our model achieved an accuracy\nof 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88,\nrespectively. The highest values, reaching up to 96.2%, were obtained for\nCutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and\nNakaseomyces glabratus. Classification difficulties were observed in closely\nrelated species, such as Staphylococcus hominis and Staphylococcus\nhaemolyticus, due to morphological similarity, and within Candida albicans due\nto high morphotic diversity.\n  The study confirms the potential of our model for microbial classification,\nbut it also indicates the need for further optimisation and expansion of the\ntraining data set. In the future, this technology could support microbial\ndiagnosis, reducing diagnostic time and improving the effectiveness of sepsis\ntreatment due to its simplicity and accessibility. Part of the results\npresented in this publication was covered by a patent application at the\nEuropean Patent Office EP24461637.1 \"A computer implemented method for\nidentifying a microorganism in a blood and a data processing system therefor\"."}
{"id": "2503.14546", "pdf": "https://arxiv.org/pdf/2503.14546", "abs": "https://arxiv.org/abs/2503.14546", "authors": ["Gustavo Correia", "Victor Alves", "Paulo Novais"], "title": "The Impact of Artificial Intelligence on Emergency Medicine: A Review of Recent Advances", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07"], "comment": "20 pages, 2 tables, 2 figures", "summary": "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards."}
{"id": "2503.14550", "pdf": "https://arxiv.org/pdf/2503.14550", "abs": "https://arxiv.org/abs/2503.14550", "authors": ["Theodorus Dapamede", "Aisha Urooj", "Vedant Joshi", "Gabrielle Gershon", "Frank Li", "Mohammadreza Chavoshi", "Beatrice Brown-Mulry", "Rohan Satya Isaac", "Aawez Mansuri", "Chad Robichaux", "Chadi Ayoub", "Reza Arsanjani", "Laurence Sperling", "Judy Gichoya", "Marly van Assen", "Charles W. ONeill", "Imon Banerjee", "Hari Trivedi"], "title": "Novel AI-Based Quantification of Breast Arterial Calcification to Predict Cardiovascular Risk", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Women are underdiagnosed and undertreated for cardiovascular disease.\nAutomatic quantification of breast arterial calcification on screening\nmammography can identify women at risk for cardiovascular disease and enable\nearlier treatment and management of disease. In this retrospective study of\n116,135 women from two healthcare systems, a transformer-based neural network\nquantified BAC severity (no BAC, mild, moderate, and severe) on screening\nmammograms. Outcomes included major adverse cardiovascular events (MACE) and\nall-cause mortality. BAC severity was independently associated with MACE after\nadjusting for cardiovascular risk factors, with increasing hazard ratios from\nmild (HR 1.18-1.22), moderate (HR 1.38-1.47), to severe BAC (HR 2.03-2.22)\nacross datasets (all p<0.001). This association remained significant across all\nage groups, with even mild BAC indicating increased risk in women under 50. BAC\nremained an independent predictor when analyzed alongside ASCVD risk scores,\nshowing significant associations with myocardial infarction, stroke, heart\nfailure, and mortality (all p<0.005). Automated BAC quantification enables\nopportunistic cardiovascular risk assessment during routine mammography without\nadditional radiation or cost. This approach provides value beyond traditional\nrisk factors, particularly in younger women, offering potential for early CVD\nrisk stratification in the millions of women undergoing annual mammography."}
{"id": "2503.14554", "pdf": "https://arxiv.org/pdf/2503.14554", "abs": "https://arxiv.org/abs/2503.14554", "authors": ["Ali Parsaee", "Fahim Shahriar", "Chuxin He", "Ruiqing Tan"], "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference", "summary": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates."}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning."}
{"id": "2503.14562", "pdf": "https://arxiv.org/pdf/2503.14562", "abs": "https://arxiv.org/abs/2503.14562", "authors": ["A. I. Medvedeva", "V. V. Bakutkin"], "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "in Russian language", "summary": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification)."}
{"id": "2503.14573", "pdf": "https://arxiv.org/pdf/2503.14573", "abs": "https://arxiv.org/abs/2503.14573", "authors": ["Wanxin Yu", "Zhemin Zhu", "Cong Wang", "Yihang Bao", "Chunjie Xia", "Rongshan Cheng", "Yan Yu", "Tsung-Yuan Tsai"], "title": "Three-dimensional Reconstruction of the Lumbar Spine with Submillimeter Accuracy Using Biplanar X-ray Images", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "21 pages, 10 figures, 4 tables", "summary": "Three-dimensional reconstruction of the spine under weight-bearing conditions\nfrom biplanar X-ray images is of great importance for the clinical assessment\nof spinal diseases. However, the current fully automated reconstruction methods\nhave low accuracy and fail to meet the clinical application standards. This\nstudy developed and validated a fully automated method for high-accuracy 3D\nreconstruction of the lumbar spine from biplanar X-ray images. The method\ninvolves lumbar decomposition and landmark detection from the raw X-ray images,\nfollowed by a deformable model and landmark-weighted 2D-3D registration\napproach. The reconstruction accuracy was validated by the gold standard\nobtained through the registration of CT-segmented vertebral models with the\nbiplanar X-ray images. The proposed method achieved a 3D reconstruction\naccuracy of 0.80 mm, representing a significant improvement over the mainstream\napproaches. This study will contribute to the clinical diagnosis of lumbar in\nweight-bearing positions."}
{"id": "2503.14637", "pdf": "https://arxiv.org/pdf/2503.14637", "abs": "https://arxiv.org/abs/2503.14637", "authors": ["Merkourios Simos", "Alberto Silvio Chiappa", "Alexander Mathis"], "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis."}
{"id": "2503.14655", "pdf": "https://arxiv.org/pdf/2503.14655", "abs": "https://arxiv.org/abs/2503.14655", "authors": ["Minheng Chen", "Xiaowei Yu", "Jing Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis."}
{"id": "2503.14694", "pdf": "https://arxiv.org/pdf/2503.14694", "abs": "https://arxiv.org/abs/2503.14694", "authors": ["Rui Yang", "Lin Song", "Yicheng Xiao", "Runhui Huang", "Yixiao Ge", "Ying Shan", "Hengshuang Zhao"], "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\npropelled the development of large multi-modal models (LMMs), highlighting the\npotential for general and intelligent assistants. However, most LMMs model\nvisual and textual modalities separately, leading to recent efforts to develop\nnative LMMs using a single transformer. Despite the promise, these native\nmodels are resource-intensive and often exhibit performance gaps compared to\ntheir compositional counterparts. To alleviate this issue, we propose a simple\nyet efficient method to construct a baseline for the native and end-to-end\nlarge multi-modal model in a single transformer. First, we propose a new\nearly-fusion LMM that can fuse multi-modal inputs in the early stage and\nrespond to visual instructions in an auto-regressive manner. Second, we devise\nan efficient training recipe for the proposed model, which harnesses the prior\nknowledge of the pre-trained models, addressing both the performance\nlimitations and the challenge of resource consumption. The proposed model\ndemonstrates superior performance compared to other LMMs using one transformer\nand significantly narrows the performance gap with compositional LMMs."}
{"id": "2503.14754", "pdf": "https://arxiv.org/pdf/2503.14754", "abs": "https://arxiv.org/abs/2503.14754", "authors": ["Matt Franchi", "Nikhil Garg", "Wendy Ju", "Emma Pierson"], "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "In review", "summary": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models."}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction."}
{"id": "2503.14779", "pdf": "https://arxiv.org/pdf/2503.14779", "abs": "https://arxiv.org/abs/2503.14779", "authors": ["Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh"], "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub."}
{"id": "2503.14836", "pdf": "https://arxiv.org/pdf/2503.14836", "abs": "https://arxiv.org/abs/2503.14836", "authors": ["Kunyang Li", "Jean-Charles Noirot Ferrand", "Ryan Sheatsley", "Blaine Hoak", "Yohan Beugin", "Eric Pauley", "Patrick McDaniel"], "title": "On the Robustness Tradeoff in Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."}
{"id": "2503.14845", "pdf": "https://arxiv.org/pdf/2503.14845", "abs": "https://arxiv.org/abs/2503.14845", "authors": ["Yuezhen Xie", "Meiying Zhang", "Qi Hao"], "title": "ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Adverse climate conditions pose significant challenges for autonomous\nsystems, demanding reliable perception and decision-making across diverse\nenvironments. To better simulate these conditions, physically-based NeRF\nrendering methods have been explored for their ability to generate realistic\nscene representations. However, these methods suffer from slow rendering speeds\nand long preprocessing times, making them impractical for real-time testing and\nuser interaction. This paper presents ClimateGS, a novel framework integrating\n3D Gaussian representations with physical simulation to enable real-time\nclimate effects rendering. The novelty of this work is threefold: 1) developing\na linear transformation for 3D Gaussian photorealistic style transfer, enabling\ndirect modification of spherical harmonics across bands for efficient and\nconsistent style adaptation; 2) developing a joint training strategy for 3D\nstyle transfer, combining supervised and self-supervised learning to accelerate\nconvergence while preserving original scene details; 3) developing a real-time\nrendering method for climate simulation, integrating physics-based effects with\n3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS\non MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with\ncomparable or superior visual quality to SOTA 2D/3D methods, making it suitable\nfor interactive applications."}
{"id": "2503.14881", "pdf": "https://arxiv.org/pdf/2503.14881", "abs": "https://arxiv.org/abs/2503.14881", "authors": ["Bo Chen", "Xiaoyu Li", "Yekun Ke", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."}
{"id": "2503.14892", "pdf": "https://arxiv.org/pdf/2503.14892", "abs": "https://arxiv.org/abs/2503.14892", "authors": ["He Huang", "Yong Chen", "Yujun Guo", "Wei He"], "title": "Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) fusion is an efficient technique that combines\nlow-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)\nto generate high-resolution HSI (HR-HSI). Existing supervised learning methods\n(SLMs) can yield promising results when test data degradation matches the\ntraining ones, but they face challenges in generalizing to unknown\ndegradations. To unleash the potential and generalization ability of SLMs, we\npropose a novel self-supervised unknown-to-known degradation transformation\nframework (U2K) for blind HSI fusion, which adaptively transforms unknown\ndegradation into the same type of degradation as those handled by pre-trained\nSLMs. Specifically, the proposed U2K framework consists of: (1) spatial and\nspectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded\nHR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert\nthese wrapped data into predefined degradation patterns. The transformed HR-MSI\nand LR-HSI pairs are then processed by a pre-trained network to reconstruct the\ntarget HR-HSI. We train the U2K framework in a self-supervised manner using\nconsistency loss and greedy alternating optimization, significantly improving\nthe flexibility of blind HSI fusion. Extensive experiments confirm the\neffectiveness of our proposed U2K framework in boosting the adaptability of\nfive existing SLMs under various degradation settings and surpassing\nstate-of-the-art blind methods."}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/."}
{"id": "2503.14908", "pdf": "https://arxiv.org/pdf/2503.14908", "abs": "https://arxiv.org/abs/2503.14908", "authors": ["Haoyu Chen", "Xiaojie Xu", "Wenbo Li", "Jingjing Ren", "Tian Ye", "Songhua Liu", "Ying-Cong Chen", "Lei Zhu", "Xinchao Wang"], "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality."}
{"id": "2503.14933", "pdf": "https://arxiv.org/pdf/2503.14933", "abs": "https://arxiv.org/abs/2503.14933", "authors": ["Yi Luo", "Hamed Hooshangnejad", "Xue Feng", "Gaofeng Huang", "Xiaojian Chen", "Rui Zhang", "Quan Chen", "Wil Ngwa", "Kai Ding"], "title": "A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "19 pages, 4 figures", "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges."}
{"id": "2503.15008", "pdf": "https://arxiv.org/pdf/2503.15008", "abs": "https://arxiv.org/abs/2503.15008", "authors": ["Aamir Mehmood", "Yue Hu", "Saddam Hussain Khan"], "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986", "summary": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis."}
{"id": "2503.15029", "pdf": "https://arxiv.org/pdf/2503.15029", "abs": "https://arxiv.org/abs/2503.15029", "authors": ["Jianbo Zhao", "Taiyu Ban", "Zhihao Liu", "Hangning Zhou", "Xiyang Wang", "Qibin Zhou", "Hailong Qin", "Mu Yang", "Lei Liu", "Bin Li"], "title": "DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/."}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN."}
{"id": "2503.15168", "pdf": "https://arxiv.org/pdf/2503.15168", "abs": "https://arxiv.org/abs/2503.15168", "authors": ["Javier Del Ser", "Jesus L. Lobo", "Heimo M√ºller", "Andreas Holzinger"], "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "68T05"], "comment": "11 pages, 1 figure", "summary": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities."}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."}
{"id": "2503.15288", "pdf": "https://arxiv.org/pdf/2503.15288", "abs": "https://arxiv.org/abs/2503.15288", "authors": ["Justin Le Lou√´dec", "Maike Bauer", "Tanja Amerstorfer", "Jackie A. Davies"], "title": "Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking", "categories": ["physics.space-ph", "cs.CV", "cs.LG"], "comment": "24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025", "summary": "Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH."}
{"id": "2503.15321", "pdf": "https://arxiv.org/pdf/2503.15321", "abs": "https://arxiv.org/abs/2503.15321", "authors": ["Euclid Collaboration", "G. Stevens", "S. Fotopoulou", "M. N. Bremer", "T. Matamoro Zatarain", "K. Jahnke", "B. Margalef-Bentabol", "M. Huertas-Company", "M. J. Smith", "M. Walmsley", "M. Salvato", "M. Mezcua", "A. Paulino-Afonso", "M. Siudek", "M. Talia", "F. Ricci", "W. Roster", "N. Aghanim", "B. Altieri", "S. Andreon", "H. Aussel", "C. Baccigalupi", "M. Baldi", "S. Bardelli", "P. Battaglia", "A. Biviano", "A. Bonchi", "E. Branchini", "M. Brescia", "J. Brinchmann", "S. Camera", "G. Ca√±as-Herrera", "V. Capobianco", "C. Carbone", "J. Carretero", "M. Castellano", "G. Castignani", "S. Cavuoti", "K. C. Chambers", "A. Cimatti", "C. Colodro-Conde", "G. Congedo", "C. J. Conselice", "L. Conversi", "Y. Copin", "A. Costille", "F. Courbin", "H. M. Courtois", "M. Cropper", "A. Da Silva", "H. Degaudenzi", "G. De Lucia", "C. Dolding", "H. Dole", "M. Douspis", "F. Dubath", "X. Dupac", "S. Dusini", "S. Escoffier", "M. Farina", "S. Ferriol", "K. George", "C. Giocoli", "B. R. Granett", "A. Grazian", "F. Grupp", "S. V. H. Haugan", "I. M. Hook", "F. Hormuth", "A. Hornstrup", "P. Hudelot", "M. Jhabvala", "E. Keih√§nen", "S. Kermiche", "A. Kiessling", "M. Kilbinger", "B. Kubik", "M. K√ºmmel", "H. Kurki-Suonio", "Q. Le Boulc'h", "A. M. C. Le Brun", "D. Le Mignant", "P. B. Lilje", "V. Lindholm", "I. Lloro", "G. Mainetti", "D. Maino", "E. Maiorano", "O. Marggraf", "M. Martinelli", "N. Martinet", "F. Marulli", "R. Massey", "S. Maurogordato", "H. J. McCracken", "E. Medinaceli", "S. Mei", "M. Melchior", "M. Meneghetti", "E. Merlin", "G. Meylan", "A. Mora", "M. Moresco", "L. Moscardini", "R. Nakajima", "C. Neissner", "S. -M. Niemi", "C. Padilla", "S. Paltani", "F. Pasian", "K. Pedersen", "W. J. Percival", "V. Pettorino", "G. Polenta", "M. Poncet", "L. A. Popa", "L. Pozzetti", "F. Raison", "R. Rebolo", "A. Renzi", "J. Rhodes", "G. Riccio", "E. Romelli", "M. Roncarelli", "R. Saglia", "A. G. S√°nchez", "D. Sapone", "J. A. Schewtschenko", "M. Schirmer", "P. Schneider", "T. Schrabback", "A. Secroun", "S. Serrano", "P. Simon", "C. Sirignano", "G. Sirri", "J. Skottfelt", "L. Stanco", "J. Steinwagner", "P. Tallada-Cresp√≠", "A. N. Taylor", "I. Tereno", "S. Toft", "R. Toledo-Moreo", "F. Torradeflot", "I. Tutusaus", "L. Valenziano", "J. Valiviita", "T. Vassallo", "G. Verdoes Kleijn", "A. Veropalumbo", "Y. Wang", "J. Weller", "A. Zacchei", "G. Zamorani", "F. M. Zerbi", "I. A. Zinchenko", "E. Zucca", "V. Allevato", "M. Ballardini", "M. Bolzonella", "E. Bozzo", "C. Burigana", "R. Cabanac", "A. Cappi", "J. A. Escartin Vigo", "L. Gabarra", "W. G. Hartley", "J. Mart√≠n-Fleitas", "S. Matthew", "R. B. Metcalf", "A. Pezzotta", "M. P√∂ntinen", "I. Risso", "V. Scottez", "M. Sereno", "M. Tenti", "M. Wiesmann", "Y. Akrami", "S. Alvi", "I. T. Andika", "S. Anselmi", "M. Archidiacono", "F. Atrio-Barandela", "D. Bertacca", "M. Bethermin", "L. Bisigello", "A. Blanchard", "L. Blot", "S. Borgani", "M. L. Brown", "S. Bruton", "A. Calabro", "F. Caro", "T. Castro", "F. Cogato", "S. Davini", "G. Desprez", "A. D√≠az-S√°nchez", "J. J. Diaz", "S. Di Domizio", "J. M. Diego", "P. -A. Duc", "A. Enia", "Y. Fang", "A. G. Ferrari", "A. Finoguenov", "A. Fontana", "A. Franco", "J. Garc√≠a-Bellido", "T. Gasparetto", "V. Gautard", "E. Gaztanaga", "F. Giacomini", "F. Gianotti", "M. Guidi", "C. M. Gutierrez", "A. Hall", "S. Hemmati", "H. Hildebrandt", "J. Hjorth", "J. J. E. Kajava", "Y. Kang", "V. Kansal", "D. Karagiannis", "C. C. Kirkpatrick", "S. Kruk", "L. Legrand", "M. Lembo", "F. Lepori", "G. Leroy", "J. Lesgourgues", "L. Leuzzi", "T. I. Liaudat", "J. Macias-Perez", "M. Magliocchetti", "F. Mannucci", "R. Maoli", "C. J. A. P. Martins", "L. Maurin", "M. Miluzio", "P. Monaco", "G. Morgante", "K. Naidoo", "A. Navarro-Alsina", "F. Passalacqua", "K. Paterson", "L. Patrizii", "A. Pisani", "D. Potter", "S. Quai", "M. Radovich", "P. -F. Rocci", "G. Rodighiero", "S. Sacquegna", "M. Sahl√©n", "D. B. Sanders", "E. Sarpa", "A. Schneider", "M. Schultheis", "D. Sciotti", "E. Sellentin", "F. Shankar", "L. C. Smith", "K. Tanidis", "G. Testera", "R. Teyssier", "S. Tosi", "A. Troja", "M. Tucci", "C. Valieri", "D. Vergani", "G. Verza", "N. A. Walton"], "title": "Euclid Quick Data Release (Q1). Active galactic nuclei identification using diffusion-based inpainting of Euclid VIS images", "categories": ["astro-ph.GA", "cs.CV"], "comment": "Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1)', 32 pages, 26 figures", "summary": "Light emission from galaxies exhibit diverse brightness profiles, influenced\nby factors such as galaxy type, structural features and interactions with other\ngalaxies. Elliptical galaxies feature more uniform light distributions, while\nspiral and irregular galaxies have complex, varied light profiles due to their\nstructural heterogeneity and star-forming activity. In addition, galaxies with\nan active galactic nucleus (AGN) feature intense, concentrated emission from\ngas accretion around supermassive black holes, superimposed on regular galactic\nlight, while quasi-stellar objects (QSO) are the extreme case of the AGN\nemission dominating the galaxy. The challenge of identifying AGN and QSO has\nbeen discussed many times in the literature, often requiring multi-wavelength\nobservations. This paper introduces a novel approach to identify AGN and QSO\nfrom a single image. Diffusion models have been recently developed in the\nmachine-learning literature to generate realistic-looking images of everyday\nobjects. Utilising the spatial resolving power of the Euclid VIS images, we\ncreated a diffusion model trained on one million sources, without using any\nsource pre-selection or labels. The model learns to reconstruct light\ndistributions of normal galaxies, since the population is dominated by them. We\ncondition the prediction of the central light distribution by masking the\ncentral few pixels of each source and reconstruct the light according to the\ndiffusion model. We further use this prediction to identify sources that\ndeviate from this profile by examining the reconstruction error of the few\ncentral pixels regenerated in each source's core. Our approach, solely using\nVIS imaging, features high completeness compared to traditional methods of AGN\nand QSO selection, including optical, near-infrared, mid-infrared, and X-rays.\n[abridged]"}
{"id": "2503.15352", "pdf": "https://arxiv.org/pdf/2503.15352", "abs": "https://arxiv.org/abs/2503.15352", "authors": ["Abhi Kamboj", "Minh N. Do"], "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.15390", "pdf": "https://arxiv.org/pdf/2503.15390", "abs": "https://arxiv.org/abs/2503.15390", "authors": ["Yumin Zhang", "Yan Gao", "Haoran Duan", "Hanqing Guo", "Tejal Shah", "Rajiv Ranjan", "Bo Wei"], "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance."}
{"id": "2503.15402", "pdf": "https://arxiv.org/pdf/2503.15402", "abs": "https://arxiv.org/abs/2503.15402", "authors": ["Alejandro Peque√±o-Zurro", "Lyes Khacef", "Stefano Panzeri", "Elisabetta Chicca"], "title": "Towards efficient keyword spotting using spike-based time difference encoders", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.ET"], "comment": "26 pages, 9 figures", "summary": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns."}
{"id": "2503.15414", "pdf": "https://arxiv.org/pdf/2503.15414", "abs": "https://arxiv.org/abs/2503.15414", "authors": ["Can Peng", "Qianhui Men", "Pramit Saha", "Qianye Yang", "Cheng Ouyang", "J. Alison Noble"], "title": "Federated Continual 3D Segmentation With Single-round Communication", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Federated learning seeks to foster collaboration among distributed clients\nwhile preserving the privacy of their local data. Traditionally, federated\nlearning methods assume a fixed setting in which client data and learning\nobjectives remain constant. However, in real-world scenarios, new clients may\njoin, and existing clients may expand the segmentation label set as task\nrequirements evolve. In such a dynamic federated analysis setup, the\nconventional federated communication strategy of model aggregation per\ncommunication round is suboptimal. As new clients join, this strategy requires\nretraining, linearly increasing communication and computation overhead. It also\nimposes requirements for synchronized communication, which is difficult to\nachieve among distributed clients. In this paper, we propose a federated\ncontinual learning strategy that employs a one-time model aggregation at the\nserver through multi-model distillation. This approach builds and updates the\nglobal model while eliminating the need for frequent server communication. When\nintegrating new data streams or onboarding new clients, this approach\nefficiently reuses previous client models, avoiding the need to retrain the\nglobal model across the entire federation. By minimizing communication load and\nbypassing the need to put unchanged clients online, our approach relaxes\nsynchronization requirements among clients, providing an efficient and scalable\nfederated analysis framework suited for real-world applications. Using\nmulti-class 3D abdominal CT segmentation as an application task, we demonstrate\nthe effectiveness of the proposed approach."}
{"id": "2503.15420", "pdf": "https://arxiv.org/pdf/2503.15420", "abs": "https://arxiv.org/abs/2503.15420", "authors": ["Amirhossein Kazerouni", "Soroush Mehraban", "Michael Brudno", "Babak Taati"], "title": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks."}
{"id": "2503.14603", "pdf": "https://arxiv.org/pdf/2503.14603", "abs": "https://arxiv.org/abs/2503.14603", "authors": ["Yazeed Alnumay", "Alexandre Barbet", "Anna Bialas", "William Darling", "Shaan Desai", "Joan Devassy", "Kyle Duffy", "Stephanie Howe", "Olivia Lasche", "Justin Lee", "Anirudh Shrinivason", "Jennifer Tracey"], "title": "Command R7B Arabic: A Small, Enterprise Focused, Multilingual, and Culturally Aware Arabic LLM", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Building high-quality large language models (LLMs) for enterprise Arabic\napplications remains challenging due to the limited availability of digitized\nArabic data. In this work, we present a data synthesis and refinement strategy\nto help address this problem, namely, by leveraging synthetic data generation\nand human-in-the-loop annotation to expand our Arabic training corpus. We\nfurther present our iterative post training recipe that is essential to\nachieving state-of-the-art performance in aligning the model with human\npreferences, a critical aspect to enterprise use cases. The culmination of this\neffort is the release of a small, 7B, open-weight model that outperforms\nsimilarly sized peers in head-to-head comparisons and on Arabic-focused\nbenchmarks covering cultural knowledge, instruction following, RAG, and\ncontextual faithfulness."}
{"id": "2503.14620", "pdf": "https://arxiv.org/pdf/2503.14620", "abs": "https://arxiv.org/abs/2503.14620", "authors": ["Hikaru Shimadzu", "Takehito Utsuro", "Daisuke Kitayama"], "title": "Retrieval-Augmented Simulacra: Generative Agents for Up-to-date and Knowledge-Adaptive Simulations", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "In the 2023 edition of the White Paper on Information and Communications, it\nis estimated that the population of social networking services in Japan will\nexceed 100 million by 2022, and the influence of social networking services in\nJapan is growing significantly. In addition, marketing using SNS and research\non the propagation of emotions and information on SNS are being actively\nconducted, creating the need for a system for predicting trends in SNS\ninteractions. We have already created a system that simulates the behavior of\nvarious communities on SNS by building a virtual SNS environment in which\nagents post and reply to each other in a chat community created by agents using\na LLMs. In this paper, we evaluate the impact of the search extension\ngeneration mechanism used to create posts and replies in a virtual SNS\nenvironment using a simulation system on the ability to generate posts and\nreplies. As a result of the evaluation, we confirmed that the proposed search\nextension generation mechanism, which mimics human search behavior, generates\nthe most natural exchange."}
{"id": "2503.14626", "pdf": "https://arxiv.org/pdf/2503.14626", "abs": "https://arxiv.org/abs/2503.14626", "authors": ["Ramon Ruiz-Dolz", "John Lawrence"], "title": "An Explainable Framework for Misinformation Identification via Critical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Natural language misinformation detection approaches have been, to date,\nlargely dependent on sequence classification methods, producing opaque systems\nin which the reasons behind classification as misinformation are unclear. While\nan effort has been made in the area of automated fact-checking to propose\nexplainable approaches to the problem, this is not the case for automated\nreason-checking systems. In this paper, we propose a new explainable framework\nfor both factual and rational misinformation detection based on the theory of\nArgumentation Schemes and Critical Questions. For that purpose, we create and\nrelease NLAS-CQ, the first corpus combining 3,566 textbook-like natural\nlanguage argumentation scheme instances and 4,687 corresponding answers to\ncritical questions related to these arguments. On the basis of this corpus, we\nimplement and validate our new framework which combines classification with\nquestion answering to analyse arguments in search of misinformation, and\nprovides the explanations in form of critical questions to the human user."}
{"id": "2503.14662", "pdf": "https://arxiv.org/pdf/2503.14662", "abs": "https://arxiv.org/abs/2503.14662", "authors": ["Yicheng Fu", "Zikui Wang", "Liuxin Yang", "Meiqing Huo", "Zhongdongming Dai"], "title": "ConQuer: A Framework for Concept-Based Quiz Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Quizzes play a crucial role in education by reinforcing students'\nunderstanding of key concepts and encouraging self-directed exploration.\nHowever, compiling high-quality quizzes can be challenging and require deep\nexpertise and insight into specific subject matter. Although LLMs have greatly\nenhanced the efficiency of quiz generation, concerns remain regarding the\nquality of these AI-generated quizzes and their educational impact on students.\nTo address these issues, we introduce ConQuer, a concept-based quiz generation\nframework that leverages external knowledge sources. We employ comprehensive\nevaluation dimensions to assess the quality of the generated quizzes, using\nLLMs as judges. Our experiment results demonstrate a 4.8% improvement in\nevaluation scores and a 77.52% win rate in pairwise comparisons against\nbaseline quiz sets. Ablation studies further underscore the effectiveness of\neach component in our framework. Code available at\nhttps://github.com/sofyc/ConQuer."}
{"id": "2503.14513", "pdf": "https://arxiv.org/pdf/2503.14513", "abs": "https://arxiv.org/abs/2503.14513", "authors": ["Seyed Muhammad Hossein Mousavi"], "title": "Synthetic Data Generation of Body Motion Data by Neural Gas Network for Emotion Recognition", "categories": ["cs.CV", "cs.AI", "eess.IV", "A.I"], "comment": "18 pages", "summary": "In the domain of emotion recognition using body motion, the primary challenge\nlies in the scarcity of diverse and generalizable datasets. Automatic emotion\nrecognition uses machine learning and artificial intelligence techniques to\nrecognize a person's emotional state from various data types, such as text,\nimages, sound, and body motion. Body motion poses unique challenges as many\nfactors, such as age, gender, ethnicity, personality, and illness, affect its\nappearance, leading to a lack of diverse and robust datasets specifically for\nemotion recognition. To address this, employing Synthetic Data Generation (SDG)\nmethods, such as Generative Adversarial Networks (GANs) and Variational Auto\nEncoders (VAEs), offers potential solutions, though these methods are often\ncomplex. This research introduces a novel application of the Neural Gas Network\n(NGN) algorithm for synthesizing body motion data and optimizing diversity and\ngeneration speed. By learning skeletal structure topology, the NGN fits the\nneurons or gas particles on body joints. Generated gas particles, which form\nthe skeletal structure later on, will be used to synthesize the new body\nposture. By attaching body postures over frames, the final synthetic body\nmotion appears. We compared our generated dataset against others generated by\nGANs, VAEs, and another benchmark algorithm, using benchmark metrics such as\nFr\\'echet Inception Distance (FID), Diversity, and a few more. Furthermore, we\ncontinued evaluation using classification metrics such as accuracy, precision,\nrecall, and a few others. Joint-related features or kinematic parameters were\nextracted, and the system assessed model performance against unseen data. Our\nfindings demonstrate that the NGN algorithm produces more realistic and\nemotionally distinct body motion data and does so with more synthesizing speed\nthan existing methods."}
{"id": "2503.14671", "pdf": "https://arxiv.org/pdf/2503.14671", "abs": "https://arxiv.org/abs/2503.14671", "authors": ["Xiangyong Chen", "Xiaochuan Lin"], "title": "Generating Medically-Informed Explanations for Depression Detection using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Early detection of depression from social media data offers a valuable\nopportunity for timely intervention. However, this task poses significant\nchallenges, requiring both professional medical knowledge and the development\nof accurate and explainable models. In this paper, we propose LLM-MTD (Large\nLanguage Model for Multi-Task Depression Detection), a novel approach that\nleverages a pre-trained large language model to simultaneously classify social\nmedia posts for depression and generate textual explanations grounded in\nmedical diagnostic criteria. We train our model using a multi-task learning\nframework with a combined loss function that optimizes both classification\naccuracy and explanation quality. We evaluate LLM-MTD on the benchmark Reddit\nSelf-Reported Depression Dataset (RSDD) and compare its performance against\nseveral competitive baseline methods, including traditional machine learning\nand fine-tuned BERT. Our experimental results demonstrate that LLM-MTD achieves\nstate-of-the-art performance in depression detection, showing significant\nimprovements in AUPRC and other key metrics. Furthermore, human evaluation of\nthe generated explanations reveals their relevance, completeness, and medical\naccuracy, highlighting the enhanced interpretability of our approach. This work\ncontributes a novel methodology for depression detection that combines the\npower of large language models with the crucial aspect of explainability."}
{"id": "2503.14517", "pdf": "https://arxiv.org/pdf/2503.14517", "abs": "https://arxiv.org/abs/2503.14517", "authors": ["Hejia Chen", "Haoxian Zhang", "Shoulong Zhang", "Xiaoqiang Liu", "Sisi Zhuang", "Yuan Zhang", "Pengfei Wan", "Di Zhang", "Shuai Li"], "title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICLR'25", "summary": "Speech-driven 3D talking face method should offer both accurate lip\nsynchronization and controllable expressions. Previous methods solely adopt\ndiscrete emotion labels to globally control expressions throughout sequences\nwhile limiting flexible fine-grained facial control within the spatiotemporal\ndomain. We propose a diffusion-transformer-based 3D talking face generation\nmodel, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained\nmultimodal control conditions. Nevertheless, the entanglement of multiple\nconditions challenges achieving satisfying performance. To disentangle speech\naudio and fine-grained conditions, we employ a two-stage training pipeline.\nSpecifically, Cafe-Talk is initially trained using only speech audio and\ncoarse-grained conditions. Then, a proposed fine-grained control adapter\ngradually adds fine-grained instructions represented by action units (AUs),\npreventing unfavorable speech-lip synchronization. To disentangle coarse- and\nfine-grained conditions, we design a swap-label training mechanism, which\nenables the dominance of the fine-grained conditions. We also devise a\nmask-based CFG technique to regulate the occurrence and intensity of\nfine-grained control. In addition, a text-based detector is introduced with\ntext-AU alignment to enable natural language user input and further support\nmultimodal control. Extensive experimental results prove that Cafe-Talk\nachieves state-of-the-art lip synchronization and expressiveness performance\nand receives wide acceptance in fine-grained control in user studies. Project\npage: https://harryxd2018.github.io/cafe-talk/"}
{"id": "2503.14694", "pdf": "https://arxiv.org/pdf/2503.14694", "abs": "https://arxiv.org/abs/2503.14694", "authors": ["Rui Yang", "Lin Song", "Yicheng Xiao", "Runhui Huang", "Yixiao Ge", "Ying Shan", "Hengshuang Zhao"], "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\npropelled the development of large multi-modal models (LMMs), highlighting the\npotential for general and intelligent assistants. However, most LMMs model\nvisual and textual modalities separately, leading to recent efforts to develop\nnative LMMs using a single transformer. Despite the promise, these native\nmodels are resource-intensive and often exhibit performance gaps compared to\ntheir compositional counterparts. To alleviate this issue, we propose a simple\nyet efficient method to construct a baseline for the native and end-to-end\nlarge multi-modal model in a single transformer. First, we propose a new\nearly-fusion LMM that can fuse multi-modal inputs in the early stage and\nrespond to visual instructions in an auto-regressive manner. Second, we devise\nan efficient training recipe for the proposed model, which harnesses the prior\nknowledge of the pre-trained models, addressing both the performance\nlimitations and the challenge of resource consumption. The proposed model\ndemonstrates superior performance compared to other LMMs using one transformer\nand significantly narrows the performance gap with compositional LMMs."}
{"id": "2503.14524", "pdf": "https://arxiv.org/pdf/2503.14524", "abs": "https://arxiv.org/abs/2503.14524", "authors": ["Zhihao Zhu"], "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Representing a dynamic scene using a structured spatial-temporal scene graph\nis a novel and particularly challenging task. To tackle this task, it is\ncrucial to learn the temporal interactions between objects in addition to their\nspatial relations. Due to the lack of explicitly annotated temporal relations\nin current benchmark datasets, most of the existing spatial-temporal scene\ngraph generation methods build dense and abstract temporal connections among\nall objects across frames. However, not all temporal connections are encoding\nmeaningful temporal dynamics. We propose a novel spatial-temporal scene graph\ngeneration method that selectively builds temporal connections only between\ntemporal-relevant objects pairs and represents the temporal relations as\nexplicit edges in the scene graph. The resulting sparse and explicit temporal\nrepresentation allows us to improve upon strong scene graph generation\nbaselines by up to $4.4\\%$ in Scene Graph Detection. In addition, we show that\nour approach can be leveraged to improve downstream vision tasks. Particularly,\napplying our approach to action recognition, shows 0.6\\% gain in mAP in\ncomparison to the state-of-the-art"}
{"id": "2503.14718", "pdf": "https://arxiv.org/pdf/2503.14718", "abs": "https://arxiv.org/abs/2503.14718", "authors": ["Hakyung Sung", "Gyu-Ho Shin"], "title": "Second language Korean Universal Dependency treebank v1.2: Focus on data augmentation and annotation scheme refinement", "categories": ["cs.CL"], "comment": null, "summary": "We expand the second language (L2) Korean Universal Dependencies (UD)\ntreebank with 5,454 manually annotated sentences. The annotation guidelines are\nalso revised to better align with the UD framework. Using this enhanced\ntreebank, we fine-tune three Korean language models and evaluate their\nperformance on in-domain and out-of-domain L2-Korean datasets. The results show\nthat fine-tuning significantly improves their performance across various\nmetrics, thus highlighting the importance of using well-tailored L2 datasets\nfor fine-tuning first-language-based, general-purpose language models for the\nmorphosyntactic analysis of L2 data."}
{"id": "2503.14526", "pdf": "https://arxiv.org/pdf/2503.14526", "abs": "https://arxiv.org/abs/2503.14526", "authors": ["Yu Fang", "Yue Yang", "Xinghao Zhu", "Kaiyuan Zheng", "Gedas Bertasius", "Daniel Szafir", "Mingyu Ding"], "title": "ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis", "categories": ["cs.CV", "cs.GR", "cs.RO"], "comment": "Website: https://yuffish.github.io/rebot/", "summary": "Vision-language-action (VLA) models present a promising paradigm by training\npolicies directly on real robot datasets like Open X-Embodiment. However, the\nhigh cost of real-world data collection hinders further data scaling, thereby\nrestricting the generalizability of VLAs. In this paper, we introduce ReBot, a\nnovel real-to-sim-to-real approach for scaling real robot datasets and adapting\nVLA models to target domains, which is the last-mile deployment challenge in\nrobot manipulation. Specifically, ReBot replays real-world robot trajectories\nin simulation to diversify manipulated objects (real-to-sim), and integrates\nthe simulated movements with inpainted real-world background to synthesize\nphysically realistic and temporally consistent robot videos (sim-to-real). Our\napproach has several advantages: 1) it enjoys the benefit of real data to\nminimize the sim-to-real gap; 2) it leverages the scalability of simulation;\nand 3) it can generalize a pretrained VLA to a target domain with fully\nautomated data pipelines. Extensive experiments in both simulation and\nreal-world environments show that ReBot significantly enhances the performance\nand robustness of VLAs. For example, in SimplerEnv with the WidowX robot, ReBot\nimproved the in-domain performance of Octo by 7.2% and OpenVLA by 21.8%, and\nout-of-domain generalization by 19.9% and 9.4%, respectively. For real-world\nevaluation with a Franka robot, ReBot increased the success rates of Octo by\n17% and OpenVLA by 20%. More information can be found at:\nhttps://yuffish.github.io/rebot/"}
{"id": "2503.14728", "pdf": "https://arxiv.org/pdf/2503.14728", "abs": "https://arxiv.org/abs/2503.14728", "authors": ["Weijie Xu", "Richard Futrell"], "title": "Strategic resource allocation in memory encoding: An efficiency principle shaping language processing", "categories": ["cs.CL"], "comment": "manuscript under review", "summary": "How is the limited capacity of working memory efficiently used to support\nhuman linguistic behaviors? In this paper, we investigate strategic resource\nallocation as an efficiency principle for memory encoding in sentence\nprocessing. The idea is that working memory resources are dynamically and\nstrategically allocated to prioritize novel and unexpected information,\nenhancing their representations to make them less susceptible to memory decay\nand interference. Theoretically, from a resource-rational perspective, we argue\nthat this efficiency principle naturally arises from two functional assumptions\nabout working memory, namely, its limited capacity and its noisy\nrepresentation. Empirically, through naturalistic corpus data, we find\nconverging evidence for strategic resource allocation in the context of\ndependency locality from both the production and the comprehension side, where\nnon-local dependencies with less predictable antecedents are associated with\nreduced locality effect. However, our results also reveal considerable\ncross-linguistic variability, highlighting the need for a closer examination of\nhow strategic resource allocation, as a universal efficiency principle,\ninteracts with language-specific phrase structures."}
{"id": "2503.14530", "pdf": "https://arxiv.org/pdf/2503.14530", "abs": "https://arxiv.org/abs/2503.14530", "authors": ["Qing Li", "Jiahui Geng", "Derui Zhu", "Fengyu Cai", "Chenyang Lyu", "Fakhri Karray"], "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unlearning methods for vision-language models (VLMs) have primarily adapted\ntechniques from large language models (LLMs), relying on weight updates that\ndemand extensive annotated forget sets. Moreover, these methods perform\nunlearning at a coarse granularity, often leading to excessive forgetting and\nreduced model utility. To address this issue, we introduce SAUCE, a novel\nmethod that leverages sparse autoencoders (SAEs) for fine-grained and selective\nconcept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture\nhigh-dimensional, semantically rich sparse features. It then identifies the\nfeatures most relevant to the target concept for unlearning. During inference,\nit selectively modifies these features to suppress specific concepts while\npreserving unrelated information. We evaluate SAUCE on two distinct VLMs,\nLLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:\nconcrete concept unlearning (objects and sports scenes) and abstract concept\nunlearning (emotions, colors, and materials), encompassing a total of 60\nconcepts. Extensive experiments demonstrate that SAUCE outperforms\nstate-of-the-art methods by 18.04% in unlearning quality while maintaining\ncomparable model utility. Furthermore, we investigate SAUCE's robustness\nagainst widely used adversarial attacks, its transferability across models, and\nits scalability in handling multiple simultaneous unlearning requests. Our\nfindings establish SAUCE as an effective and scalable solution for selective\nconcept unlearning in VLMs."}
{"id": "2503.14749", "pdf": "https://arxiv.org/pdf/2503.14749", "abs": "https://arxiv.org/abs/2503.14749", "authors": ["Sophia Hager", "David Mueller", "Kevin Duh", "Nicholas Andrews"], "title": "Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly used for factual\nquestion-answering, it becomes more important for LLMs to have the capability\nto communicate the likelihood that their answer is correct. For these\nverbalized expressions of uncertainty to be meaningful, they should reflect the\nerror rates at the expressed level of confidence. However, when prompted to\nexpress confidence, the error rates of current LLMs are inconsistent with their\ncommunicated confidences, highlighting the need for uncertainty quantification\nmethods. Many prior methods calculate lexical uncertainty, estimating a model's\nconfidence in the specific string it generated. In some cases, however, it may\nbe more useful to estimate semantic uncertainty, or the model's confidence in\nthe answer regardless of how it is verbalized. We propose a simple procedure,\nuncertainty distillation, to teach an LLM to verbalize calibrated semantic\nconfidences. Using held-out data to map initial uncertainty estimates to\nmeaningful probabilities, we create examples annotated with verbalized\nprobabilities for supervised fine-tuning. We demonstrate our method yields\nverbalized confidences that correlate with observed error rates with a small\nfine-tuned language model as well as with larger instruction-tuned models, and\nfind that our semantic uncertainty correlates well with lexical uncertainty on\nshort answers."}
{"id": "2503.14535", "pdf": "https://arxiv.org/pdf/2503.14535", "abs": "https://arxiv.org/abs/2503.14535", "authors": ["Huaqiu Li", "Xiaowan Hu", "Haoqian Wang"], "title": "Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Real-world low-light images often suffer from complex degradations such as\nlocal overexposure, low brightness, noise, and uneven illumination. Supervised\nmethods tend to overfit to specific scenarios, while unsupervised methods,\nthough better at generalization, struggle to model these degradations due to\nthe lack of reference images. To address this issue, we propose an\ninterpretable, zero-reference joint denoising and low-light enhancement\nframework tailored for real-world scenarios. Our method derives a training\nstrategy based on paired sub-images with varying illumination and noise levels,\ngrounded in physical imaging principles and retinex theory. Additionally, we\nleverage the Discrete Cosine Transform (DCT) to perform frequency domain\ndecomposition in the sRGB space, and introduce an implicit-guided hybrid\nrepresentation strategy that effectively separates intricate compounded\ndegradations. In the backbone network design, we develop retinal decomposition\nnetwork guided by implicit degradation representation mechanisms. Extensive\nexperiments demonstrate the superiority of our method. Code will be available\nat https://github.com/huaqlili/unsupervised-light-enhance-ICLR2025."}
{"id": "2503.14755", "pdf": "https://arxiv.org/pdf/2503.14755", "abs": "https://arxiv.org/abs/2503.14755", "authors": ["Omar E. Rakha", "Hazem M. Abbas"], "title": "Language Independent Named Entity Recognition via Orthogonal Transformation of Word Vectors", "categories": ["cs.CL", "cs.AI"], "comment": "Paper was initially released in 2017 but was never published", "summary": "Word embeddings have been a key building block for NLP in which models relied\nheavily on word embeddings in many different tasks. In this paper, a model is\nproposed based on using Bidirectional LSTM/CRF with word embeddings to perform\nnamed entity recognition for any language. This is done by training a model on\na source language (English) and transforming word embeddings from the target\nlanguage into word embeddings of the source language by using an orthogonal\nlinear transformation matrix. Evaluation of the model shows that by training a\nmodel on an English dataset the model was capable of detecting named entities\nin an Arabic dataset without neither training or fine tuning the model on an\nArabic language dataset."}
{"id": "2503.14537", "pdf": "https://arxiv.org/pdf/2503.14537", "abs": "https://arxiv.org/abs/2503.14537", "authors": ["Liewen Liao", "Weihao Yan", "Ming Yang", "Songan Zhang"], "title": "Learning-based 3D Reconstruction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning-based 3D reconstruction has emerged as a transformative technique in\nautonomous driving, enabling precise modeling of both dynamic and static\nenvironments through advanced neural representations. Despite augmenting\nperception, 3D reconstruction inspires pioneering solution for vital tasks in\nthe field of autonomous driving, such as scene understanding and closed-loop\nsimulation. Commencing with an examination of input modalities, we investigates\nthe details of 3D reconstruction and conducts a multi-perspective, in-depth\nanalysis of recent advancements. Specifically, we first provide a systematic\nintroduction of preliminaries, including data formats, benchmarks and technical\npreliminaries of learning-based 3D reconstruction, facilitating instant\nidentification of suitable methods based on hardware configurations and sensor\nsuites. Then, we systematically review learning-based 3D reconstruction methods\nin autonomous driving, categorizing approaches by subtasks and conducting\nmulti-dimensional analysis and summary to establish a comprehensive technical\nreference. The development trends and existing challenges is summarized in the\ncontext of learning-based 3D reconstruction in autonomous driving. We hope that\nour review will inspire future researches."}
{"id": "2503.14797", "pdf": "https://arxiv.org/pdf/2503.14797", "abs": "https://arxiv.org/abs/2503.14797", "authors": ["Varich Boonsanong", "Vidhisha Balachandran", "Xiaochuang Han", "Shangbin Feng", "Lucy Lu Wang", "Yulia Tsvetkov"], "title": "FACTS&EVIDENCE: An Interactive Tool for Transparent Fine-Grained Factual Verification of Machine-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread consumption of AI-generated content, there has been an\nincreased focus on developing automated tools to verify the factual accuracy of\nsuch content. However, prior research and tools developed for fact verification\ntreat it as a binary classification or a linear regression problem. Although\nthis is a useful mechanism as part of automatic guardrails in systems, we argue\nthat such tools lack transparency in the prediction reasoning and diversity in\nsource evidence to provide a trustworthy user experience. We develop\nFacts&Evidence - an interactive and transparent tool for user-driven\nverification of complex text. The tool facilitates the intricate\ndecision-making involved in fact-verification, presenting its users a breakdown\nof complex input texts to visualize the credibility of individual claims along\nwith an explanation of model decisions and attribution to multiple, diverse\nevidence sources. Facts&Evidence aims to empower consumers of machine-generated\ntext and give them agency to understand, verify, selectively trust and use such\ntext."}
{"id": "2503.14547", "pdf": "https://arxiv.org/pdf/2503.14547", "abs": "https://arxiv.org/abs/2503.14547", "authors": ["Shuheng Li", "Jiayun Zhang", "Xiaohan Fu", "Xiyuan Zhang", "Jingbo Shang", "Rajesh K. Gupta"], "title": "Matching Skeleton-based Activity Representations with Heterogeneous Signals for HAR", "categories": ["cs.CV", "cs.LG"], "comment": "This paper is accepted by SenSys 2025", "summary": "In human activity recognition (HAR), activity labels have typically been\nencoded in one-hot format, which has a recent shift towards using textual\nrepresentations to provide contextual knowledge. Here, we argue that HAR should\nbe anchored to physical motion data, as motion forms the basis of activity and\napplies effectively across sensing systems, whereas text is inherently limited.\nWe propose SKELAR, a novel HAR framework that pretrains activity\nrepresentations from skeleton data and matches them with heterogeneous HAR\nsignals. Our method addresses two major challenges: (1) capturing core motion\nknowledge without context-specific details. We achieve this through a\nself-supervised coarse angle reconstruction task that recovers joint rotation\nangles, invariant to both users and deployments; (2) adapting the\nrepresentations to downstream tasks with varying modalities and focuses. To\naddress this, we introduce a self-attention matching module that dynamically\nprioritizes relevant body parts in a data-driven manner. Given the lack of\ncorresponding labels in existing skeleton data, we establish MASD, a new HAR\ndataset with IMU, WiFi, and skeleton, collected from 20 subjects performing 27\nactivities. This is the first broadly applicable HAR dataset with\ntime-synchronized data across three modalities. Experiments show that SKELAR\nachieves the state-of-the-art performance in both full-shot and few-shot\nsettings. We also demonstrate that SKELAR can effectively leverage synthetic\nskeleton data to extend its use in scenarios without skeleton collections."}
{"id": "2503.14827", "pdf": "https://arxiv.org/pdf/2503.14827", "abs": "https://arxiv.org/abs/2503.14827", "authors": ["Chejian Xu", "Jiawei Zhang", "Zhaorun Chen", "Chulin Xie", "Mintong Kang", "Yujin Potter", "Zhun Wang", "Zhuowen Yuan", "Alexander Xiong", "Zidi Xiong", "Chenhui Zhang", "Lingzhi Yuan", "Yi Zeng", "Peiyang Xu", "Chengquan Guo", "Andy Zhou", "Jeffrey Ziwei Tan", "Xuandong Zhao", "Francesco Pinto", "Zhen Xiang", "Yu Gai", "Zinan Lin", "Dan Hendrycks", "Bo Li", "Dawn Song"], "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "ICLR 2025", "summary": "Multimodal foundation models (MMFMs) play a crucial role in various\napplications, including autonomous driving, healthcare, and virtual assistants.\nHowever, several studies have revealed vulnerabilities in these models, such as\ngenerating unsafe content by text-to-image models. Existing benchmarks on\nmultimodal models either predominantly assess the helpfulness of these models,\nor only focus on limited perspectives such as fairness and privacy. In this\npaper, we present the first unified platform, MMDT (Multimodal DecodingTrust),\ndesigned to provide a comprehensive safety and trustworthiness evaluation for\nMMFMs. Our platform assesses models from multiple perspectives, including\nsafety, hallucination, fairness/bias, privacy, adversarial robustness, and\nout-of-distribution (OOD) generalization. We have designed various evaluation\nscenarios and red teaming algorithms under different tasks for each perspective\nto generate challenging data, forming a high-quality benchmark. We evaluate a\nrange of multimodal models using MMDT, and our findings reveal a series of\nvulnerabilities and areas for improvement across these perspectives. This work\nintroduces the first comprehensive and unique safety and trustworthiness\nevaluation platform for MMFMs, paving the way for developing safer and more\nreliable MMFMs and systems. Our platform and benchmark are available at\nhttps://mmdecodingtrust.github.io/."}
{"id": "2503.14552", "pdf": "https://arxiv.org/pdf/2503.14552", "abs": "https://arxiv.org/abs/2503.14552", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Fatemeh Afghah", "Connor Peter McGrath", "Danish Bhatkar", "Mithilesh Anil Biradar", "Abolfazl Razi"], "title": "Fire and Smoke Datasets in 20 Years: An In-depth Review", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fire and smoke phenomena pose a significant threat to the natural\nenvironment, ecosystems, and global economy, as well as human lives and\nwildlife. In this particular circumstance, there is a demand for more\nsophisticated and advanced technologies to implement an effective strategy for\nearly detection, real-time monitoring, and minimizing the overall impacts of\nfires on ecological balance and public safety. Recently, the rapid advancement\nof Artificial Intelligence (AI) and Computer Vision (CV) frameworks has\nsubstantially revolutionized the momentum for developing efficient fire\nmanagement systems. However, these systems extensively rely on the availability\nof adequate and high-quality fire and smoke data to create proficient Machine\nLearning (ML) methods for various tasks, such as detection and monitoring.\nAlthough fire and smoke datasets play a critical role in training, evaluating,\nand testing advanced Deep Learning (DL) models, a comprehensive review of the\nexisting datasets is still unexplored. For this purpose, we provide an in-depth\nreview to systematically analyze and evaluate fire and smoke datasets collected\nover the past 20 years. We investigate the characteristics of each dataset,\nincluding type, size, format, collection methods, and geographical diversities.\nWe also review and highlight the unique features of each dataset, such as\nimaging modalities (RGB, thermal, infrared) and their applicability for\ndifferent fire management tasks (classification, segmentation, detection).\nFurthermore, we summarize the strengths and weaknesses of each dataset and\ndiscuss their potential for advancing research and technology in fire\nmanagement. Ultimately, we conduct extensive experimental analyses across\ndifferent datasets using several state-of-the-art algorithms, such as\nResNet-50, DeepLab-V3, and YoloV8."}
{"id": "2503.14828", "pdf": "https://arxiv.org/pdf/2503.14828", "abs": "https://arxiv.org/abs/2503.14828", "authors": ["Firoj Alam", "Julia Maria Stru√ü", "Tanmoy Chakraborty", "Stefan Dietze", "Salim Hafid", "Katerina Korre", "Arianna Muti", "Preslav Nakov", "Federico Ruggeri", "Sebastian Schellhammer", "Vinay Setty", "Megha Sundriyal", "Konstantin Todorov", "Venktesh V"], "title": "The CLEF-2025 CheckThat! Lab: Subjectivity, Fact-Checking, Claim Normalization, and Retrieval", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "comment": "misinformation, factuality, fact-checking, fact-checkers,\n  check-worthiness, Social Media Platforms", "summary": "The CheckThat! lab aims to advance the development of innovative technologies\ndesigned to identify and counteract online disinformation and manipulation\nefforts across various languages and platforms. The first five editions focused\non key tasks in the information verification pipeline, including\ncheck-worthiness, evidence retrieval and pairing, and verification. Since the\n2023 edition, the lab has expanded its scope to address auxiliary tasks that\nsupport research and decision-making in verification. In the 2025 edition, the\nlab revisits core verification tasks while also considering auxiliary\nchallenges. Task 1 focuses on the identification of subjectivity (a follow-up\nfrom CheckThat! 2024), Task 2 addresses claim normalization, Task 3 targets\nfact-checking numerical claims, and Task 4 explores scientific web discourse\nprocessing. These tasks present challenging classification and retrieval\nproblems at both the document and span levels, including multilingual settings."}
{"id": "2503.14553", "pdf": "https://arxiv.org/pdf/2503.14553", "abs": "https://arxiv.org/abs/2503.14553", "authors": ["Kasra Borazjani", "Payam Abdisarabshali", "Naji Khosravan", "Seyyedali Hosseinalipour"], "title": "Redefining non-IID Data in Federated Learning for Computer Vision Tasks: Migrating from Labels to Embeddings for Task-Specific Data Distributions", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 9 figures, 1 table, (implementations are included at our\n  GitHub repository: https://github.com/KasraBorazjani/task-perspective-het)", "summary": "Federated Learning (FL) represents a paradigm shift in distributed machine\nlearning (ML), enabling clients to train models collaboratively while keeping\ntheir raw data private. This paradigm shift from traditional centralized ML\nintroduces challenges due to the non-iid (non-independent and identically\ndistributed) nature of data across clients, significantly impacting FL's\nperformance. Existing literature, predominantly model data heterogeneity by\nimposing label distribution skew across clients. In this paper, we show that\nlabel distribution skew fails to fully capture the real-world data\nheterogeneity among clients in computer vision tasks beyond classification.\nSubsequently, we demonstrate that current approaches overestimate FL's\nperformance by relying on label/class distribution skew, exposing an overlooked\ngap in the literature. By utilizing pre-trained deep neural networks to extract\ntask-specific data embeddings, we define task-specific data heterogeneity\nthrough the lens of each vision task and introduce a new level of data\nheterogeneity called embedding-based data heterogeneity. Our methodology\ninvolves clustering data points based on embeddings and distributing them among\nclients using the Dirichlet distribution. Through extensive experiments, we\nevaluate the performance of different FL methods under our revamped notion of\ndata heterogeneity, introducing new benchmark performance measures to the\nliterature. We further unveil a series of open research directions that can be\npursued."}
{"id": "2503.14891", "pdf": "https://arxiv.org/pdf/2503.14891", "abs": "https://arxiv.org/abs/2503.14891", "authors": ["Honglin Lin", "Zhuoshi Pan", "Yu Li", "Qizhi Pei", "Xin Gao", "Mengzhang Cai", "Conghui He", "Lijun Wu"], "title": "MetaLadder: Ascending Mathematical Solution Quality via Analogical-Problem Reasoning Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated promising capabilities in\nsolving mathematical reasoning tasks, leveraging Chain-of-Thought (CoT) data as\na vital component in guiding answer generation. Current paradigms typically\ngenerate CoT and answers directly for a given problem, diverging from human\nproblem-solving strategies to some extent. Humans often solve problems by\nrecalling analogous cases and leveraging their solutions to reason about the\ncurrent task. Inspired by this cognitive process, we propose\n\\textbf{MetaLadder}, a novel framework that explicitly prompts LLMs to recall\nand reflect on meta-problems, those structurally or semantically analogous\nproblems, alongside their CoT solutions before addressing the target problem.\nAdditionally, we introduce a problem-restating mechanism to enhance the model's\ncomprehension of the target problem by regenerating the original question,\nwhich further improves reasoning accuracy. Therefore, the model can achieve\nreasoning transfer from analogical problems, mimicking human-like \"learning\nfrom examples\" and generalization abilities. Extensive experiments on\nmathematical benchmarks demonstrate that our MetaLadder significantly boosts\nLLMs' problem-solving accuracy, largely outperforming standard CoT-based\nmethods (\\textbf{10.3\\%} accuracy gain) and other methods. Our code and data\nhas been released at https://github.com/LHL3341/MetaLadder."}
{"id": "2503.14558", "pdf": "https://arxiv.org/pdf/2503.14558", "abs": "https://arxiv.org/abs/2503.14558", "authors": ["Yi Du", "Zhipeng Zhao", "Shaoshu Su", "Sharath Golluri", "Haoze Zheng", "Runmao Yao", "Chen Wang"], "title": "SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Point cloud (PC) processing tasks-such as completion, upsampling, denoising,\nand colorization-are crucial in applications like autonomous driving and 3D\nreconstruction. Despite substantial advancements, prior approaches often\naddress each of these tasks independently, with separate models focused on\nindividual issues. However, this isolated approach fails to account for the\nfact that defects like incompleteness, low resolution, noise, and lack of color\nfrequently coexist, with each defect influencing and correlating with the\nothers. Simply applying these models sequentially can lead to error\naccumulation from each model, along with increased computational costs. To\naddress these challenges, we introduce SuperPC, the first unified diffusion\nmodel capable of concurrently handling all four tasks. Our approach employs a\nthree-level-conditioned diffusion framework, enhanced by a novel\nspatial-mix-fusion strategy, to leverage the correlations among these four\ndefects for simultaneous, efficient processing. We show that SuperPC\noutperforms the state-of-the-art specialized models as well as their\ncombination on all four individual tasks."}
{"id": "2503.14900", "pdf": "https://arxiv.org/pdf/2503.14900", "abs": "https://arxiv.org/abs/2503.14900", "authors": ["Estrid He", "Tabinda Sarwar", "Ibrahim Khalil", "Xun Yi", "Ke Wang"], "title": "Deep Contrastive Unlearning for Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The past a few years have witnessed the great success of large language\nmodels, demonstrating powerful capabilities in comprehending textual data and\ngenerating human-like languages. Large language models achieve success by being\ntrained on vast amounts of textual data, including online sources with\ncopyrighted content and user-generated knowledge. However, this comes at a\ncost: the potential risk of exposing users' privacy and violating copyright\nprotections. Thus, to safeguard individuals' \"right to be forgotten\", there has\nbeen increasing interests in machine unlearning -- the process of removing\ninformation carried by particular training samples from a model while not\ndeteriorating its predictive quality. This is a challenging task due to the\nblack-box nature of language models. Most existing studies focus on mitigating\nthe impact of those forgot samples upon a model's outputs, and do not\nexplicitly consider the geometric distributions of samples in the latent space\nof a model. To address this issue, we propose a machine unlearning framework,\nnamed Deep Contrastive Unlearning for fine-Tuning (DeepCUT) language models.\nOur proposed model achieves machine unlearning by directly optimizing the\nlatent space of a model. Comprehensive experiments on real-world datasets\ndemonstrate the effectiveness and efficiency of DeepCUT with consistent and\nsignificant improvement over baseline methods."}
{"id": "2503.14564", "pdf": "https://arxiv.org/pdf/2503.14564", "abs": "https://arxiv.org/abs/2503.14564", "authors": ["Guowei Wang", "Changxing Ding"], "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025.Code:https://github.com/flash1803/EATTA", "summary": "Long-term test-time adaptation (TTA) is a challenging task due to error\naccumulation. Recent approaches tackle this issue by actively labeling a small\nproportion of samples in each batch, yet the annotation burden quickly grows as\nthe batch number increases. In this paper, we investigate how to achieve\neffortless active labeling so that a maximum of one sample is selected for\nannotation in each batch. First, we annotate the most valuable sample in each\nbatch based on the single-step optimization perspective in the TTA context. In\nthis scenario, the samples that border between the source- and target-domain\ndata distributions are considered the most feasible for the model to learn in\none iteration. Then, we introduce an efficient strategy to identify these\nsamples using feature perturbation. Second, we discover that the gradient\nmagnitudes produced by the annotated and unannotated samples have significant\nvariations. Therefore, we propose balancing their impact on model optimization\nusing two dynamic weights. Extensive experiments on the popular ImageNet-C, -R,\n-K, -A and PACS databases demonstrate that our approach consistently\noutperforms state-of-the-art methods with significantly lower annotation costs."}
{"id": "2503.14917", "pdf": "https://arxiv.org/pdf/2503.14917", "abs": "https://arxiv.org/abs/2503.14917", "authors": ["Jiazheng Li", "Lu Yu", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Yanfang Ye", "Chuxu Zhang"], "title": "MASS: Mathematical Data Selection via Skill Graphs for Pretraining Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High-quality data plays a critical role in the pretraining and fine-tuning of\nlarge language models (LLMs), even determining their performance ceiling to\nsome degree. Consequently, numerous data selection methods have been proposed\nto identify subsets of data that can effectively and efficiently enhance model\nperformance. However, most of these methods focus on general data selection and\ntend to overlook the specific nuances of domain-related data. In this paper, we\nintroduce MASS, a \\textbf{MA}thematical data \\textbf{S}election framework using\nthe \\textbf{S}kill graph for pretraining LLMs in the mathematical reasoning\ndomain. By taking into account the unique characteristics of mathematics and\nreasoning, we construct a skill graph that captures the mathematical skills and\ntheir interrelations from a reference dataset. This skill graph guides us in\nassigning quality scores to the target dataset, enabling us to select the\ntop-ranked subset which is further used to pretrain LLMs. Experimental results\ndemonstrate the efficiency and effectiveness of MASS across different model\nsizes (1B and 7B) and pretraining datasets (web data and synthetic data).\nSpecifically, in terms of efficiency, models trained on subsets selected by\nMASS can achieve similar performance to models trained on the original\ndatasets, with a significant reduction in the number of trained tokens -\nranging from 50\\% to 70\\% fewer tokens. In terms of effectiveness, when trained\non the same amount of tokens, models trained on the data selected by MASS\noutperform those trained on the original datasets by 3.3\\% to 5.9\\%. These\nresults underscore the potential of MASS to improve both the efficiency and\neffectiveness of pretraining LLMs."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2503.14926", "pdf": "https://arxiv.org/pdf/2503.14926", "abs": "https://arxiv.org/abs/2503.14926", "authors": ["Minkyoo Song", "Eugene Jang", "Jaehan Kim", "Seungwon Shin"], "title": "Covering Cracks in Content Moderation: Delexicalized Distant Supervision for Illicit Drug Jargon Detection", "categories": ["cs.CL"], "comment": "Accepted for publication in the KDD 2025 Research Track", "summary": "In light of rising drug-related concerns and the increasing role of social\nmedia, sales and discussions of illicit drugs have become commonplace online.\nSocial media platforms hosting user-generated content must therefore perform\ncontent moderation, which is a difficult task due to the vast amount of jargon\nused in drug discussions. Previous works on drug jargon detection were limited\nto extracting a list of terms, but these approaches have fundamental problems\nin practical application. First, they are trivially evaded using word\nsubstitutions. Second, they cannot distinguish whether euphemistic terms such\nas \"pot\" or \"crack\" are being used as drugs or in their benign meanings. We\nargue that drug content moderation should be done using contexts rather than\nrelying on a banlist. However, manually annotated datasets for training such a\ntask are not only expensive but also prone to becoming obsolete. We present\nJEDIS, a framework for detecting illicit drug jargon terms by analyzing their\ncontexts. JEDIS utilizes a novel approach that combines distant supervision and\ndelexicalization, which allows JEDIS to be trained without human-labeled data\nwhile being robust to new terms and euphemisms. Experiments on two manually\nannotated datasets show JEDIS significantly outperforms state-of-the-art\nword-based baselines in terms of F1-score and detection coverage in drug jargon\ndetection. We also conduct qualitative analysis that demonstrates JEDIS is\nrobust against pitfalls faced by existing approaches."}
{"id": "2503.14607", "pdf": "https://arxiv.org/pdf/2503.14607", "abs": "https://arxiv.org/abs/2503.14607", "authors": ["Shuo Xing", "Zezhou Sun", "Shuangyu Xie", "Kaiyuan Chen", "Yanjia Huang", "Yuping Wang", "Jiachen Li", "Dezhen Song", "Zhengzhong Tu"], "title": "Can Large Vision Language Models Read Maps Like a Human?", "categories": ["cs.CV"], "comment": "35 pages", "summary": "In this paper, we introduce MapBench-the first dataset specifically designed\nfor human-readable, pixel-based map-based outdoor navigation, curated from\ncomplex path finding scenarios. MapBench comprises over 1600 pixel space map\npath finding problems from 100 diverse maps. In MapBench, LVLMs generate\nlanguage-based navigation instructions given a map image and a query with\nbeginning and end landmarks. For each map, MapBench provides Map Space Scene\nGraph (MSSG) as an indexing data structure to convert between natural language\nand evaluate LVLM-generated results. We demonstrate that MapBench significantly\nchallenges state-of-the-art LVLMs both zero-shot prompting and a\nChain-of-Thought (CoT) augmented reasoning framework that decomposes map\nnavigation into sequential cognitive processes. Our evaluation of both\nopen-source and closed-source LVLMs underscores the substantial difficulty\nposed by MapBench, revealing critical limitations in their spatial reasoning\nand structured decision-making capabilities. We release all the code and\ndataset in https://github.com/taco-group/MapBench."}
{"id": "2503.14985", "pdf": "https://arxiv.org/pdf/2503.14985", "abs": "https://arxiv.org/abs/2503.14985", "authors": ["Dewei Wang", "Wei Zhu", "Liyang Ling", "Ettore Tiotto", "Quintin Wang", "Whitney Tsang", "Julian Opperman", "Jacky Deng"], "title": "ML-Triton, A Multi-Level Compilation and Language Extension to Triton GPU Programming", "categories": ["cs.CL"], "comment": null, "summary": "In the era of LLMs, dense operations such as GEMM and MHA are critical\ncomponents. These operations are well-suited for parallel execution using a\ntilebased approach. While traditional GPU programming often relies on low level\ninterfaces like CUDA or SYCL, Triton has emerged as a DSL that offers a more\nuser-friendly and portable alternative by programming at a higher level. The\ncurrent Triton starts at the workgroup (aka threadblock) level, and directly\nlowers to per-thread level. And then attempt to coalesce and amend through a\nseries of passes, promoting information from low-level representation. We\nbelieve this is pre-mature lowering based on the below observations. 1. GPU has\na hierarchical structure both physically and logically. Modern GPUs often\nfeature SIMD units capable of directly operating on tiles on a warp or\nwarpgroup basis, such as blocked load and blocked MMA. 2. Multi-level gradual\nlowering can make compiler decoupled and clean by separating considerations\ninter and intra a logical layer. 3. Kernel developers often need fine control\nto get good performance on the latest hardware. FlashAttention2 advocates\nexplicit data partition between warps to make a performance boost. In this\ncontext, we propose ML-Triton which features multi-level compilation flow and\nprogramming interface. Our approach begins at the workgroup level and\nprogressively lowers to the warp and intrinsic level, implementing a multilevel\nlowering align with the hierarchical nature of GPU. Additionally, we extend\ntriton language to support user-set compiler hint and warp level programming,\nenabling researchers to get good out-of-the box performance without awaiting\ncompiler updates. Experimental results demonstrate that our approach achieves\nperformance above 95% of expert-written kernels on Intel GPU, as measured by\nthe geometric mean."}
{"id": "2503.14640", "pdf": "https://arxiv.org/pdf/2503.14640", "abs": "https://arxiv.org/abs/2503.14640", "authors": ["Yi Liao", "Yongsheng Gao", "Weichuan Zhang"], "title": "Dynamic Accumulated Attention Map for Interpreting Evolution of Decision-Making in Vision Transformer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Various Vision Transformer (ViT) models have been widely used for image\nrecognition tasks. However, existing visual explanation methods can not display\nthe attention flow hidden inside the inner structure of ViT models, which\nexplains how the final attention regions are formed inside a ViT for its\ndecision-making. In this paper, a novel visual explanation approach, Dynamic\nAccumulated Attention Map (DAAM), is proposed to provide a tool that can\nvisualize, for the first time, the attention flow from the top to the bottom\nthrough ViT networks. To this end, a novel decomposition module is proposed to\nconstruct and store the spatial feature information by unlocking the [class]\ntoken generated by the self-attention module of each ViT block. The module can\nalso obtain the channel importance coefficients by decomposing the\nclassification score for supervised ViT models. Because of the lack of\nclassification score in self-supervised ViT models, we propose dimension-wise\nimportance weights to compute the channel importance coefficients. Such spatial\nfeatures are linearly combined with the corresponding channel importance\ncoefficients, forming the attention map for each block. The dynamic attention\nflow is revealed by block-wisely accumulating each attention map. The\ncontribution of this work focuses on visualizing the evolution dynamic of the\ndecision-making attention for any intermediate block inside a ViT model by\nproposing a novel decomposition module and dimension-wise importance weights.\nThe quantitative and qualitative analysis consistently validate the\neffectiveness and superior capacity of the proposed DAAM for not only\ninterpreting ViT models with the fully-connected layers as the classifier but\nalso self-supervised ViT models. The code is available at\nhttps://github.com/ly9802/DynamicAccumulatedAttentionMap."}
{"id": "2503.14991", "pdf": "https://arxiv.org/pdf/2503.14991", "abs": "https://arxiv.org/abs/2503.14991", "authors": ["Stefan Arnold"], "title": "Inspecting the Representation Manifold of Differentially-Private Text", "categories": ["cs.CL"], "comment": null, "summary": "Differential Privacy (DP) for text has recently taken the form of text\nparaphrasing using language models and temperature sampling to better balance\nprivacy and utility. However, the geometric distortion of DP regarding the\nstructure and complexity in the representation space remains unexplored. By\nestimating the intrinsic dimension of paraphrased text across varying privacy\nbudgets, we find that word-level methods severely raise the representation\nmanifold, while sentence-level methods produce paraphrases whose manifolds are\ntopologically more consistent with human-written paraphrases. Among\nsentence-level methods, masked paraphrasing, compared to causal paraphrasing,\ndemonstrates superior preservation of structural complexity, suggesting that\nautoregressive generation propagates distortions from unnatural word choices\nthat cascade and inflate the representation space."}
{"id": "2503.14654", "pdf": "https://arxiv.org/pdf/2503.14654", "abs": "https://arxiv.org/abs/2503.14654", "authors": ["Jonas Dornbusch", "Emanuel Pfarr", "Florin-Alexandru Vasluianu", "Frank Werner", "Radu Timofte"], "title": "A Simple Combination of Diffusion Models for Better Quality Trade-Offs in Image Denoising", "categories": ["cs.CV"], "comment": "10 pages, 7 figures, 2 tables", "summary": "Diffusion models have garnered considerable interest in computer vision,\nowing both to their capacity to synthesize photorealistic images and to their\nproven effectiveness in image reconstruction tasks. However, existing\napproaches fail to efficiently balance the high visual quality of diffusion\nmodels with the low distortion achieved by previous image reconstruction\nmethods. Specifically, for the fundamental task of additive Gaussian noise\nremoval, we first illustrate an intuitive method for leveraging pretrained\ndiffusion models. Further, we introduce our proposed Linear Combination\nDiffusion Denoiser (LCDD), which unifies two complementary inference procedures\n- one that leverages the model's generative potential and another that ensures\nfaithful signal recovery. By exploiting the inherent structure of the denoising\nsamples, LCDD achieves state-of-the-art performance and offers controlled,\nwell-behaved trade-offs through a simple scalar hyperparameter adjustment."}
{"id": "2503.14996", "pdf": "https://arxiv.org/pdf/2503.14996", "abs": "https://arxiv.org/abs/2503.14996", "authors": ["Francesco Maria Molfese", "Luca Moroni", "Luca Gioffr√®", "Alessandro Scir√®", "Simone Conia", "Roberto Navigli"], "title": "Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering", "categories": ["cs.CL"], "comment": "17 pages (9 main), 11 figures, 21 tables", "summary": "One of the most widely used tasks to evaluate Large Language Models (LLMs) is\nMultiple-Choice Question Answering (MCQA). While open-ended question answering\ntasks are more challenging to evaluate, MCQA tasks are, in principle, easier to\nassess, as the model's answer is thought to be simple to extract and is\ndirectly compared to a set of predefined choices. However, recent studies have\nstarted to question the reliability of MCQA evaluation, showing that multiple\nfactors can significantly impact the reported performance of LLMs, especially\nwhen the model generates free-form text before selecting one of the answer\nchoices. In this work, we shed light on the inconsistencies of MCQA evaluation\nstrategies, which can lead to inaccurate and misleading model comparisons. We\nsystematically analyze whether existing answer extraction methods are aligned\nwith human judgment, and how they are influenced by answer constraints in the\nprompt across different domains. Our experiments demonstrate that traditional\nevaluation strategies often underestimate LLM capabilities, while LLM-based\nanswer extractors are prone to systematic errors. Moreover, we reveal a\nfundamental trade-off between including format constraints in the prompt to\nsimplify answer extraction and allowing models to generate free-form text to\nimprove reasoning. Our findings call for standardized evaluation methodologies\nand highlight the need for more reliable and consistent MCQA evaluation\npractices."}
{"id": "2503.14665", "pdf": "https://arxiv.org/pdf/2503.14665", "abs": "https://arxiv.org/abs/2503.14665", "authors": ["Parker Ewen", "Hao Chen", "Seth Isaacson", "Joey Wilson", "Katherine A. Skinner", "Ram Vasudevan"], "title": "These Magic Moments: Differentiable Uncertainty Quantification of Radiance Field Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper introduces a novel approach to uncertainty quantification for\nradiance fields by leveraging higher-order moments of the rendering equation.\nUncertainty quantification is crucial for downstream tasks including view\nplanning and scene understanding, where safety and robustness are paramount.\nHowever, the high dimensionality and complexity of radiance fields pose\nsignificant challenges for uncertainty quantification, limiting the use of\nthese uncertainty quantification methods in high-speed decision-making. We\ndemonstrate that the probabilistic nature of the rendering process enables\nefficient and differentiable computation of higher-order moments for radiance\nfield outputs, including color, depth, and semantic predictions. Our method\noutperforms existing radiance field uncertainty estimation techniques while\noffering a more direct, computationally efficient, and differentiable\nformulation without the need for post-processing.Beyond uncertainty\nquantification, we also illustrate the utility of our approach in downstream\napplications such as next-best-view (NBV) selection and active ray sampling for\nneural radiance field training. Extensive experiments on synthetic and\nreal-world scenes confirm the efficacy of our approach, which achieves\nstate-of-the-art performance while maintaining simplicity."}
{"id": "2503.15003", "pdf": "https://arxiv.org/pdf/2503.15003", "abs": "https://arxiv.org/abs/2503.15003", "authors": ["Amr Keleg"], "title": "LLM Alignment for the Arabs: A Homogenous Culture or Diverse Ones?", "categories": ["cs.CL"], "comment": "Accepted to the C3NLP workshop (Co-located with NAACL 2025)", "summary": "Large language models (LLMs) have the potential of being useful tools that\ncan automate tasks and assist humans. However, these models are more fluent in\nEnglish and more aligned with Western cultures, norms, and values.\nArabic-specific LLMs are being developed to better capture the nuances of the\nArabic language, as well as the views of the Arabs. Yet, Arabs are sometimes\nassumed to share the same culture. In this position paper, I discuss the\nlimitations of this assumption and provide preliminary thoughts for how to\nbuild systems that can better represent the cultural diversity within the Arab\nworld. The invalidity of the cultural homogeneity assumption might seem\nobvious, yet, it is widely adopted in developing multilingual and\nArabic-specific LLMs. I hope that this paper will encourage the NLP community\nto be considerate of the cultural diversity within various communities speaking\nthe same language."}
{"id": "2503.14674", "pdf": "https://arxiv.org/pdf/2503.14674", "abs": "https://arxiv.org/abs/2503.14674", "authors": ["Liu Jing", "Amirul Rahman"], "title": "Elevating Visual Question Answering through Implicitly Learned Reasoning Pathways in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown remarkable progress in\nvarious multimodal tasks, yet they often struggle with complex visual reasoning\nthat requires multi-step inference. To address this limitation, we propose\nMF-SQ-LLaVA, a novel approach that enhances LVLMs by enabling implicit\nself-questioning through end-to-end training. Our method involves augmenting\nvisual question answering datasets with reasoning chains consisting of\nsub-question and answer pairs, and training the LVLM with a multi-task loss\nthat encourages the generation and answering of these intermediate steps, as\nwell as the prediction of the final answer. We conduct extensive experiments on\nthe ScienceQA and VQAv2 datasets, demonstrating that MF-SQ-LLaVA significantly\noutperforms existing state-of-the-art models, including the base LLaVA and the\noriginal SQ-LLaVA. Ablation studies further validate the contribution of each\ncomponent of our approach, and human evaluation confirms the improved accuracy\nand coherence of the reasoning process enabled by our method."}
{"id": "2503.15044", "pdf": "https://arxiv.org/pdf/2503.15044", "abs": "https://arxiv.org/abs/2503.15044", "authors": ["Haoyi Li", "Angela Yifei Yuan", "Soyeon Caren Han", "Christopher Leckie"], "title": "SPADE: Systematic Prompt Framework for Automated Dialogue Expansion in Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "9 pages", "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of systematically\ngenerated, high-quality datasets for training. To address this issue, we\npropose five novel data augmentation frameworks for synthetic user dialogue\ngeneration through a structured prompting approach, reducing the costs\nassociated with traditional data collection methods. Our proposed method yields\n14 new dialogue datasets, which we benchmark against seven MGT detection\nmodels. The results demonstrate improved generalization performance when\nutilizing a mixed dataset produced by our proposed augmentation framework.\nFurthermore, considering that real-world agents lack knowledge of future\nopponent utterances, we simulate online dialogue detection and examine the\nrelationship between chat history length and detection accuracy. We also\nbenchmark online detection performance with limited chat history on our\nframeworks. Our open-source datasets can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."}
{"id": "2503.14698", "pdf": "https://arxiv.org/pdf/2503.14698", "abs": "https://arxiv.org/abs/2503.14698", "authors": ["Yiming Wang", "Lucy Chai", "Xuan Luo", "Michael Niemeyer", "Manuel Lagunas", "Stephen Lombardi", "Siyu Tang", "Tiancheng Sun"], "title": "SplatVoxel: History-Aware Novel View Streaming without Temporal Training", "categories": ["cs.CV"], "comment": null, "summary": "We study the problem of novel view streaming from sparse-view videos, which\naims to generate a continuous sequence of high-quality, temporally consistent\nnovel views as new input frames arrive. However, existing novel view synthesis\nmethods struggle with temporal coherence and visual fidelity, leading to\nflickering and inconsistency. To address these challenges, we introduce\nhistory-awareness, leveraging previous frames to reconstruct the scene and\nimprove quality and stability. We propose a hybrid splat-voxel feed-forward\nscene reconstruction approach that combines Gaussian Splatting to propagate\ninformation over time, with a hierarchical voxel grid for temporal fusion.\nGaussian primitives are efficiently warped over time using a motion graph that\nextends 2D tracking models to 3D motion, while a sparse voxel transformer\nintegrates new temporal observations in an error-aware manner. Crucially, our\nmethod does not require training on multi-view video datasets, which are\ncurrently limited in size and diversity, and can be directly applied to\nsparse-view video streams in a history-aware manner at inference time. Our\napproach achieves state-of-the-art performance in both static and streaming\nscene reconstruction, effectively reducing temporal artifacts and visual\nartifacts while running at interactive rates (15 fps with 350ms delay) on a\nsingle H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/"}
{"id": "2503.15055", "pdf": "https://arxiv.org/pdf/2503.15055", "abs": "https://arxiv.org/abs/2503.15055", "authors": ["Arina Razmyslovich", "Kseniia Murasheva", "Sofia Sedlova", "Julien Capitaine", "Eugene Dmitriev"], "title": "ELTEX: A Framework for Domain-Driven Synthetic Data Generation", "categories": ["cs.CL"], "comment": null, "summary": "We present ELTEX (Efficient LLM Token Extraction), a domain-driven framework\nfor generating high-quality synthetic training data in specialized domains.\nWhile Large Language Models (LLMs) have shown impressive general capabilities,\ntheir performance in specialized domains like cybersecurity remains limited by\nthe scarcity of domain-specific training data. ELTEX addresses this challenge\nby systematically integrating explicit domain indicator extraction with dynamic\nprompting to preserve critical domain knowledge throughout the generation\nprocess. We demonstrate ELTEX's effectiveness in the context of\nblockchain-related cyberattack detection, where we fine-tune Gemma-2B using\nvarious combinations of real and ELTEX-generated data. Our results show that\nthe ELTEX-enhanced model achieves performance competitive with GPT-4 across\nboth standard classification metrics and uncertainty calibration, while\nrequiring significantly fewer computational resources. We release a curated\nsynthetic dataset of social media texts for cyberattack detection in\nblockchain. Our work demonstrates that domain-driven synthetic data generation\ncan effectively bridge the performance gap between resource-efficient models\nand larger architectures in specialized domains."}
{"id": "2503.14716", "pdf": "https://arxiv.org/pdf/2503.14716", "abs": "https://arxiv.org/abs/2503.14716", "authors": ["Pei-Hsin Lin", "Jacob J. Lin", "Shang-Hsien Hsieh"], "title": "Construction Site Scaffolding Completeness Detection Based on Mask R-CNN and Hough Transform", "categories": ["cs.CV", "cs.AI"], "comment": "The 30th EG-ICE: International Conference on Intelligent Computing in\n  Engineering", "summary": "Construction site scaffolding is essential for many building projects, and\nensuring its safety is crucial to prevent accidents. The safety inspector must\ncheck the scaffolding's completeness and integrity, where most violations\noccur. The inspection process includes ensuring all the components are in the\nright place since workers often compromise safety for convenience and\ndisassemble parts such as cross braces. This paper proposes a deep\nlearning-based approach to detect the scaffolding and its cross braces using\ncomputer vision. A scaffold image dataset with annotated labels is used to\ntrain a convolutional neural network (CNN) model. With the proposed approach,\nwe can automatically detect the completeness of cross braces from images taken\nat construction sites, without the need for manual inspection, saving a\nsignificant amount of time and labor costs. This non-invasive and efficient\nsolution for detecting scaffolding completeness can help improve safety in\nconstruction sites."}
{"id": "2503.15057", "pdf": "https://arxiv.org/pdf/2503.15057", "abs": "https://arxiv.org/abs/2503.15057", "authors": ["Jaihyun Park", "Ryan Cordell"], "title": "A Data-driven Investigation of Euphemistic Language: Comparing the usage of \"slave\" and \"servant\" in 19th century US newspapers", "categories": ["cs.CL"], "comment": "The 5th International Conference on Natural Language Processing for\n  Digital Humanities (NLP4DH)", "summary": "This study investigates the usage of \"slave\" and \"servant\" in the 19th\ncentury US newspapers using computational methods. While both terms were used\nto refer to enslaved African Americans, they were used in distinct ways. In the\nChronicling America corpus, we included possible OCR errors by using FastText\nembedding and excluded text reprints to consider text reprint culture in the\n19th century. Word2vec embedding was used to find semantically close words to\n\"slave\" and \"servant\" and log-odds ratio was calculated to identify\nover-represented discourse words in the Southern and Northern newspapers. We\nfound that \"slave\" is associated with socio-economic, legal, and administrative\nwords, however, \"servant\" is linked to religious words in the Northern\nnewspapers while Southern newspapers associated \"servant\" with domestic and\nfamilial words. We further found that slave discourse words in Southern\nnewspapers are more prevalent in Northern newspapers while servant discourse\nwords from each side are prevalent in their own region. This study contributes\nto the understanding of how newspapers created different discourses around\nenslaved African Americans in the 19th century US."}
{"id": "2503.14720", "pdf": "https://arxiv.org/pdf/2503.14720", "abs": "https://arxiv.org/abs/2503.14720", "authors": ["Vihaan Misra", "Peter Schaldenbrand", "Jean Oh"], "title": "ShapeShift: Towards Text-to-Shape Arrangement Synthesis with Content-Aware Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion-based models excel at generating photorealistic images from\ntext, a more nuanced challenge emerges when constrained to using only a fixed\nset of rigid shapes, akin to solving tangram puzzles or arranging real-world\nobjects to match semantic descriptions. We formalize this problem as\nshape-based image generation, a new text-guided image-to-image translation task\nthat requires rearranging the input set of rigid shapes into non-overlapping\nconfigurations and visually communicating the target concept. Unlike\npixel-manipulation approaches, our method, ShapeShift, explicitly parameterizes\neach shape within a differentiable vector graphics pipeline, iteratively\noptimizing placement and orientation through score distillation sampling from\npretrained diffusion models. To preserve arrangement clarity, we introduce a\ncontent-aware collision resolution mechanism that applies minimal semantically\ncoherent adjustments when overlaps occur, ensuring smooth convergence toward\nphysically valid configurations. By bridging diffusion-based semantic guidance\nwith explicit geometric constraints, our approach yields interpretable\ncompositions where spatial relationships clearly embody the textual prompt.\nExtensive experiments demonstrate compelling results across diverse scenarios,\nwith quantitative and qualitative advantages over alternative techniques."}
{"id": "2503.15117", "pdf": "https://arxiv.org/pdf/2503.15117", "abs": "https://arxiv.org/abs/2503.15117", "authors": ["Shichen Li", "Zhongqing Wang", "Zheyu Zhao", "Yue Zhang", "Peifeng Li"], "title": "Exploring Model Editing for LLM-based Aspect-Based Sentiment Classification", "categories": ["cs.CL"], "comment": "AAAI2025", "summary": "Model editing aims at selectively updating a small subset of a neural model's\nparameters with an interpretable strategy to achieve desired modifications. It\ncan significantly reduce computational costs to adapt to large language models\n(LLMs). Given its ability to precisely target critical components within LLMs,\nmodel editing shows great potential for efficient fine-tuning applications. In\nthis work, we investigate model editing to serve an efficient method for\nadapting LLMs to solve aspect-based sentiment classification. Through causal\ninterventions, we trace and determine which neuron hidden states are essential\nfor the prediction of the model. By performing interventions and restorations\non each component of an LLM, we identify the importance of these components for\naspect-based sentiment classification. Our findings reveal that a distinct set\nof mid-layer representations is essential for detecting the sentiment polarity\nof given aspect words. Leveraging these insights, we develop a model editing\napproach that focuses exclusively on these critical parts of the LLM, leading\nto a more efficient method for adapting LLMs. Our in-domain and out-of-domain\nexperiments demonstrate that this approach achieves competitive results\ncompared to the currently strongest methods with significantly fewer trainable\nparameters, highlighting a more efficient and interpretable fine-tuning\nstrategy."}
{"id": "2503.14736", "pdf": "https://arxiv.org/pdf/2503.14736", "abs": "https://arxiv.org/abs/2503.14736", "authors": ["Yilan Dong", "Haohe Liu", "Qing Wang", "Jiahao Yang", "Wenqing Wang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "HandSplat: Embedding-Driven Gaussian Splatting for High-Fidelity Hand Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Existing 3D Gaussian Splatting (3DGS) methods for hand rendering rely on\nrigid skeletal motion with an oversimplified non-rigid motion model, which\nfails to capture fine geometric and appearance details. Additionally, they\nperform densification based solely on per-point gradients and process poses\nindependently, ignoring spatial and temporal correlations. These limitations\nlead to geometric detail loss, temporal instability, and inefficient point\ndistribution. To address these issues, we propose HandSplat, a novel Gaussian\nSplatting-based framework that enhances both fidelity and stability for hand\nrendering. To improve fidelity, we extend standard 3DGS attributes with\nimplicit geometry and appearance embeddings for finer non-rigid motion modeling\nwhile preserving the static hand characteristic modeled by original 3DGS\nattributes. Additionally, we introduce a local gradient-aware densification\nstrategy that dynamically refines Gaussian density in high-variation regions.\nTo improve stability, we incorporate pose-conditioned attribute regularization\nto encourage attribute consistency across similar poses, mitigating temporal\nartifacts. Extensive experiments on InterHand2.6M demonstrate that HandSplat\nsurpasses existing methods in fidelity and stability while achieving real-time\nperformance. We will release the code and pre-trained models upon acceptance."}
{"id": "2503.15128", "pdf": "https://arxiv.org/pdf/2503.15128", "abs": "https://arxiv.org/abs/2503.15128", "authors": ["Dominik Macko", "Robert Moro", "Ivan Srba"], "title": "Increasing the Robustness of the Fine-tuned Multilingual Machine-Generated Text Detectors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the proliferation of LLMs, there have been concerns about their misuse\nfor harmful content creation and spreading. Recent studies justify such fears,\nproviding evidence of LLM vulnerabilities and high potential of their misuse.\nHumans are no longer able to distinguish between high-quality machine-generated\nand authentic human-written texts. Therefore, it is crucial to develop\nautomated means to accurately detect machine-generated content. It would enable\nto identify such content in online information space, thus providing an\nadditional information about its credibility. This work addresses the problem\nby proposing a robust fine-tuning process of LLMs for the detection task,\nmaking the detectors more robust against obfuscation and more generalizable to\nout-of-distribution data."}
{"id": "2503.14757", "pdf": "https://arxiv.org/pdf/2503.14757", "abs": "https://arxiv.org/abs/2503.14757", "authors": ["Marcelo Sanchez", "Gil Triginer", "Ignacio Sarasua", "Lara Raad", "Coloma Ballester"], "title": "RETHINED: A New Benchmark and Baseline for Real-Time High-Resolution Image Inpainting On Edge Devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing image inpainting methods have shown impressive completion results\nfor low-resolution images. However, most of these algorithms fail at high\nresolutions and require powerful hardware, limiting their deployment on edge\ndevices. Motivated by this, we propose the first baseline for REal-Time\nHigh-resolution image INpainting on Edge Devices (RETHINED) that is able to\ninpaint at ultra-high-resolution and can run in real-time ($\\leq$ 30ms) in a\nwide variety of mobile devices. A simple, yet effective novel method formed by\na lightweight Convolutional Neural Network (CNN) to recover structure, followed\nby a resolution-agnostic patch replacement mechanism to provide detailed\ntexture. Specially our pipeline leverages the structural capacity of CNN and\nthe high-level detail of patch-based methods, which is a key component for\nhigh-resolution image inpainting. To demonstrate the real application of our\nmethod, we conduct an extensive analysis on various mobile-friendly devices and\ndemonstrate similar inpainting performance while being $\\mathrm{100 \\times\nfaster}$ than existing state-of-the-art methods. Furthemore, we realease\nDF8K-Inpainting, the first free-form mask UHD inpainting dataset."}
{"id": "2503.15133", "pdf": "https://arxiv.org/pdf/2503.15133", "abs": "https://arxiv.org/abs/2503.15133", "authors": ["Christina Zorenb√∂hmer", "Sebastian Schmidt", "Bernd Resch"], "title": "EmoGRACE: Aspect-based emotion analysis for social media data", "categories": ["cs.CL"], "comment": null, "summary": "While sentiment analysis has advanced from sentence to aspect-level, i.e.,\nthe identification of concrete terms related to a sentiment, the equivalent\nfield of Aspect-based Emotion Analysis (ABEA) is faced with dataset bottlenecks\nand the increased complexity of emotion classes in contrast to binary\nsentiments. This paper addresses these gaps, by generating a first ABEA\ntraining dataset, consisting of 2,621 English Tweets, and fine-tuning a\nBERT-based model for the ABEA sub-tasks of Aspect Term Extraction (ATE) and\nAspect Emotion Classification (AEC).\n  The dataset annotation process was based on the hierarchical emotion theory\nby Shaver et al. [1] and made use of group annotation and majority voting\nstrategies to facilitate label consistency. The resulting dataset contained\naspect-level emotion labels for Anger, Sadness, Happiness, Fear, and a None\nclass. Using the new ABEA training dataset, the state-of-the-art ABSA model\nGRACE by Luo et al. [2] was fine-tuned for ABEA. The results reflected a\nperformance plateau at an F1-score of 70.1% for ATE and 46.9% for joint ATE and\nAEC extraction. The limiting factors for model performance were broadly\nidentified as the small training dataset size coupled with the increased task\ncomplexity, causing model overfitting and limited abilities to generalize well\non new data."}
{"id": "2503.14760", "pdf": "https://arxiv.org/pdf/2503.14760", "abs": "https://arxiv.org/abs/2503.14760", "authors": ["Kai Armstrong", "Alexander Rodrigues", "Alexander P. Willmott", "Lei Zhang", "Xujiong Ye"], "title": "Validation of Human Pose Estimation and Human Mesh Recovery for Extracting Clinically Relevant Motion Data from Videos", "categories": ["cs.CV"], "comment": null, "summary": "This work aims to discuss the current landscape of kinematic analysis tools,\nranging from the state-of-the-art in sports biomechanics such as inertial\nmeasurement units (IMUs) and retroreflective marker-based optical motion\ncapture (MoCap) to more novel approaches from the field of computing such as\nhuman pose estimation and human mesh recovery. Primarily, this comparative\nanalysis aims to validate the use of marker-less MoCap techniques in a clinical\nsetting by showing that these marker-less techniques are within a reasonable\nrange for kinematics analysis compared to the more cumbersome and less portable\nstate-of-the-art tools. Not only does marker-less motion capture using human\npose estimation produce results in-line with the results of both the IMU and\nMoCap kinematics but also benefits from a reduced set-up time and reduced\npractical knowledge and expertise to set up. Overall, while there is still room\nfor improvement when it comes to the quality of the data produced, we believe\nthat this compromise is within the room of error that these low-speed actions\nthat are used in small clinical tests."}
{"id": "2503.15169", "pdf": "https://arxiv.org/pdf/2503.15169", "abs": "https://arxiv.org/abs/2503.15169", "authors": ["Yuting Guo", "Abeed Sarker"], "title": "Comparing Llama3 and DeepSeekR1 on Biomedical Text Classification Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "4 pages", "summary": "This study compares the performance of two open-source large language models\n(LLMs)-Llama3-70B and DeepSeekR1-distill-Llama3-70B-on six biomedical text\nclassification tasks. Four tasks involve data from social media, while two\ntasks focus on clinical notes from electronic health records, and all\nexperiments were performed in zero-shot settings. Performance metrics,\nincluding precision, recall, and F1 scores, were measured for each task, along\nwith their 95% confidence intervals. Results demonstrated that\nDeepSeekR1-distill-Llama3-70B generally performs better in terms of precision\non most tasks, with mixed results on recall. While the zero-shot LLMs\ndemonstrated high F1 scores for some tasks, they grossly underperformed on\nothers, for data from both sources. The findings suggest that model selection\nshould be guided by the specific requirements of the health-related text\nclassification tasks, particularly when considering the precision-recall\ntrade-offs, and that, in the presence of annotated data, supervised\nclassification approaches may be more reliable than zero-shot LLMs."}
{"id": "2503.14774", "pdf": "https://arxiv.org/pdf/2503.14774", "abs": "https://arxiv.org/abs/2503.14774", "authors": ["David Serrano-Lozano", "Aditya Arora", "Luis Herranz", "Konstantinos G. Derpanis", "Michael S. Brown", "Javier Vazquez-Corral"], "title": "Revisiting Image Fusion for Multi-Illuminant White-Balance Correction", "categories": ["cs.CV"], "comment": "10 pages", "summary": "White balance (WB) correction in scenes with multiple illuminants remains a\npersistent challenge in computer vision. Recent methods explored fusion-based\napproaches, where a neural network linearly blends multiple sRGB versions of an\ninput image, each processed with predefined WB presets. However, we demonstrate\nthat these methods are suboptimal for common multi-illuminant scenarios.\nAdditionally, existing fusion-based methods rely on sRGB WB datasets lacking\ndedicated multi-illuminant images, limiting both training and evaluation. To\naddress these challenges, we introduce two key contributions. First, we propose\nan efficient transformer-based model that effectively captures spatial\ndependencies across sRGB WB presets, substantially improving upon linear fusion\ntechniques. Second, we introduce a large-scale multi-illuminant dataset\ncomprising over 16,000 sRGB images rendered with five different WB settings,\nalong with WB-corrected images. Our method achieves up to 100\\% improvement\nover existing techniques on our new multi-illuminant image fusion dataset."}
{"id": "2503.15220", "pdf": "https://arxiv.org/pdf/2503.15220", "abs": "https://arxiv.org/abs/2503.15220", "authors": ["Rrubaa Panchendrarajan", "Arkaitz Zubiaga"], "title": "Entity-aware Cross-lingual Claim Detection for Automated Fact-checking", "categories": ["cs.CL"], "comment": null, "summary": "Identifying claims requiring verification is a critical task in automated\nfact-checking, especially given the proliferation of misinformation on social\nmedia platforms. Despite significant progress in the task, there remain open\nchallenges such as dealing with multilingual and multimodal data prevalent in\nonline discourse. Addressing the multilingual challenge, recent efforts have\nfocused on fine-tuning pre-trained multilingual language models. While these\nmodels can handle multiple languages, their ability to effectively transfer\ncross-lingual knowledge for detecting claims spreading on social media remains\nunder-explored. In this paper, we introduce \\textit{EX-Claim}, an entity-aware\ncross-lingual claim detection model that generalizes well to handle claims\nwritten in any language. The model leverages entity information derived from\nnamed entity recognition and entity linking techniques to improve the\nlanguage-level performance of both seen and unseen languages during training.\nExtensive experiments conducted on three datasets from different social media\nplatforms demonstrate that our proposed model significantly outperforms the\nbaselines, across 27 languages, and achieves the highest rate of knowledge\ntransfer, even with limited training data."}
{"id": "2503.14783", "pdf": "https://arxiv.org/pdf/2503.14783", "abs": "https://arxiv.org/abs/2503.14783", "authors": ["Ge Yan", "Tsui-Wei Weng"], "title": "RAT: Boosting Misclassification Detection Ability without Extra Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "As deep neural networks(DNN) become increasingly prevalent, particularly in\nhigh-stakes areas such as autonomous driving and healthcare, the ability to\ndetect incorrect predictions of models and intervene accordingly becomes\ncrucial for safety. In this work, we investigate the detection of misclassified\ninputs for image classification models from the lens of adversarial\nperturbation: we propose to use robust radius (a.k.a. input-space margin) as a\nconfidence metric and design two efficient estimation algorithms, RR-BS and\nRR-Fast, for misclassification detection. Furthermore, we design a training\nmethod called Radius Aware Training (RAT) to boost models' ability to identify\nmistakes. Extensive experiments show our method could achieve up to 29.3%\nreduction on AURC and 21.62% reduction in FPR@95TPR, compared with previous\nmethods."}
{"id": "2503.15222", "pdf": "https://arxiv.org/pdf/2503.15222", "abs": "https://arxiv.org/abs/2503.15222", "authors": ["Pritam Kadasi", "Sriman Reddy", "Srivathsa Vamsi Chaturvedula", "Rudranshu Sen", "Agnish Saha", "Soumavo Sikdar", "Sayani Sarkar", "Suhani Mittal", "Rohit Jindal", "Mayank Singh"], "title": "Model Hubs and Beyond: Analyzing Model Popularity, Performance, and Documentation", "categories": ["cs.CL"], "comment": "Accepted to ICWSM'25", "summary": "With the massive surge in ML models on platforms like Hugging Face, users\noften lose track and struggle to choose the best model for their downstream\ntasks, frequently relying on model popularity indicated by download counts,\nlikes, or recency. We investigate whether this popularity aligns with actual\nmodel performance and how the comprehensiveness of model documentation\ncorrelates with both popularity and performance. In our study, we evaluated a\ncomprehensive set of 500 Sentiment Analysis models on Hugging Face. This\nevaluation involved massive annotation efforts, with human annotators\ncompleting nearly 80,000 annotations, alongside extensive model training and\nevaluation. Our findings reveal that model popularity does not necessarily\ncorrelate with performance. Additionally, we identify critical inconsistencies\nin model card reporting: approximately 80\\% of the models analyzed lack\ndetailed information about the model, training, and evaluation processes.\nFurthermore, about 88\\% of model authors overstate their models' performance in\nthe model cards. Based on our findings, we provide a checklist of guidelines\nfor users to choose good models for downstream tasks."}
{"id": "2503.14786", "pdf": "https://arxiv.org/pdf/2503.14786", "abs": "https://arxiv.org/abs/2503.14786", "authors": ["Haiyang Ying", "Matthias Zwicker"], "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Edges are one of the most basic parametric primitives to describe structural\ninformation in 3D. In this paper, we study parametric 3D edge reconstruction\nfrom calibrated multi-view images. Previous methods usually reconstruct a 3D\nedge point set from multi-view 2D edge images, and then fit 3D edges to the\npoint set. However, noise in the point set may cause gaps among fitted edges,\nand the recovered edges may not align with input multi-view images since the\nedge fitting depends only on the reconstructed 3D point set. To mitigate these\nproblems, we propose SketchSplat, a method to reconstruct accurate, complete,\nand compact 3D edges via differentiable multi-view sketch splatting. We\nrepresent 3D edges as sketches, which are parametric lines and curves defined\nby attributes including control points, scales, and opacity. During edge\nreconstruction, we iteratively sample Gaussian points from a set of sketches\nand rasterize the Gaussians onto 2D edge images. Then the gradient of the image\nerror with respect to the input 2D edge images can be back-propagated to\noptimize the sketch attributes. Our method bridges 2D edge images and 3D edges\nin a differentiable manner, which ensures that 3D edges align well with 2D\nimages and leads to accurate and complete results. We also propose a series of\nadaptive topological operations and apply them along with the sketch\noptimization. The topological operations help reduce the number of sketches\nrequired while ensuring high accuracy, yielding a more compact reconstruction.\nFinally, we contribute an accurate 2D edge detector that improves the\nperformance of both ours and existing methods. Experiments show that our method\nachieves state-of-the-art accuracy, completeness, and compactness on a\nbenchmark CAD dataset."}
{"id": "2503.15235", "pdf": "https://arxiv.org/pdf/2503.15235", "abs": "https://arxiv.org/abs/2503.15235", "authors": ["Chentian Wei", "Jiewei Chen", "Jinzhu Xu"], "title": "Exploring Large Language Models for Word Games:Who is the Spy?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word games hold significant research value for natural language processing\n(NLP), game theory, and related fields due to their rule-based and situational\nnature. This study explores how large language models (LLMs) can be effectively\ninvolved in word games and proposes a training-free framework. \"Shei Shi Wo Di\"\nor \"Who is the Spy\" in English, is a classic word game. Using this game as an\nexample, we introduce a Chain-of-Thought (CoT)-based scheduling framework to\nenable LLMs to achieve excellent performance in tasks such as inferring role\nwords and disguising their identities. We evaluate the framework's performance\nbased on game success rates and the accuracy of the LLM agents' analytical\nresults. Experimental results affirm the framework's effectiveness,\ndemonstrating notable improvements in LLM performance across multiple datasets.\nThis work highlights the potential of LLMs in mastering situational reasoning\nand social interactions within structured game environments. Our code is\npublicly available at https://github.com/ct-wei/Who-is-The-Spy."}
{"id": "2503.14824", "pdf": "https://arxiv.org/pdf/2503.14824", "abs": "https://arxiv.org/abs/2503.14824", "authors": ["Zikun Zhou", "Yushuai Sun", "Wenjie Pei", "Xin Li", "Yaowei Wang"], "title": "Prototype Perturbation for Relaxing Alignment Constraints in Backward-Compatible Learning", "categories": ["cs.CV"], "comment": null, "summary": "The traditional paradigm to update retrieval models requires re-computing the\nembeddings of the gallery data, a time-consuming and computationally intensive\nprocess known as backfilling. To circumvent backfilling, Backward-Compatible\nLearning (BCL) has been widely explored, which aims to train a new model\ncompatible with the old one. Many previous works focus on effectively aligning\nthe embeddings of the new model with those of the old one to enhance the\nbackward-compatibility. Nevertheless, such strong alignment constraints would\ncompromise the discriminative ability of the new model, particularly when\ndifferent classes are closely clustered and hard to distinguish in the old\nfeature space. To address this issue, we propose to relax the constraints by\nintroducing perturbations to the old feature prototypes. This allows us to\nalign the new feature space with a pseudo-old feature space defined by these\nperturbed prototypes, thereby preserving the discriminative ability of the new\nmodel in backward-compatible learning. We have developed two approaches for\ncalculating the perturbations: Neighbor-Driven Prototype Perturbation (NDPP)\nand Optimization-Driven Prototype Perturbation (ODPP). Particularly, they take\ninto account the feature distributions of not only the old but also the new\nmodels to obtain proper perturbations along with new model updating. Extensive\nexperiments on the landmark and commodity datasets demonstrate that our\napproaches perform favorably against state-of-the-art BCL algorithms."}
{"id": "2503.15242", "pdf": "https://arxiv.org/pdf/2503.15242", "abs": "https://arxiv.org/abs/2503.15242", "authors": ["Pierre Chambon", "Baptiste Roziere", "Benoit Sagot", "Gabriel Synnaeve"], "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?", "categories": ["cs.CL", "cs.AI", "cs.CC"], "comment": null, "summary": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the\ncapabilities of generative language models in understanding and generating code\nwith specified time and space complexities. This benchmark addresses the gap in\ncurrent evaluations that often overlook the ability of models to comprehend and\nproduce code constrained by computational complexity. BigO(Bench) includes\ntooling to infer the algorithmic complexity of any Python function from\nprofiling measurements, including human- or LLM-generated solutions.\nBigO(Bench) also includes of set of 3,105 coding problems and 1,190,250\nsolutions from Code Contests annotated with inferred (synthetic) time and space\ncomplexity labels from the complexity framework, as well as corresponding\nruntime and memory footprint values for a large set of input sizes. We present\nresults from evaluating multiple state-of-the-art language models on this\nbenchmark, highlighting their strengths and weaknesses in handling complexity\nrequirements. In particular, token-space reasoning models are unrivaled in code\ngeneration but not in complexity understanding, hinting that they may not\ngeneralize well to tasks for which no reward was given at training time."}
{"id": "2503.14830", "pdf": "https://arxiv.org/pdf/2503.14830", "abs": "https://arxiv.org/abs/2503.14830", "authors": ["Junfeng Ni", "Yu Liu", "Ruijie Lu", "Zirui Zhou", "Song-Chun Zhu", "Yixin Chen", "Siyuan Huang"], "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior", "categories": ["cs.CV"], "comment": "CVPR'25. Project page: https://dp-recon.github.io/", "summary": "Decompositional reconstruction of 3D scenes, with complete shapes and\ndetailed texture of all objects within, is intriguing for downstream\napplications but remains challenging, particularly with sparse views as input.\nRecent approaches incorporate semantic or geometric regularization to address\nthis issue, but they suffer significant degradation in underconstrained areas\nand fail to recover occluded regions. We argue that the key to solving this\nproblem lies in supplementing missing information for these areas. To this end,\nwe propose DP-Recon, which employs diffusion priors in the form of Score\nDistillation Sampling (SDS) to optimize the neural representation of each\nindividual object under novel views. This provides additional information for\nthe underconstrained areas, but directly incorporating diffusion prior raises\npotential conflicts between the reconstruction and generative guidance.\nTherefore, we further introduce a visibility-guided approach to dynamically\nadjust the per-pixel SDS loss weights. Together these components enhance both\ngeometry and appearance recovery while remaining faithful to input images.\nExtensive experiments across Replica and ScanNet++ demonstrate that our method\nsignificantly outperforms SOTA methods. Notably, it achieves better object\nreconstruction under 10 views than the baselines under 100 views. Our method\nenables seamless text-based editing for geometry and appearance through SDS\noptimization and produces decomposed object meshes with detailed UV maps that\nsupport photorealistic Visual effects (VFX) editing. The project page is\navailable at https://dp-recon.github.io/."}
{"id": "2503.15272", "pdf": "https://arxiv.org/pdf/2503.15272", "abs": "https://arxiv.org/abs/2503.15272", "authors": ["David Wan", "Justin Chih-Yao Chen", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "MAMM-Refine: A Recipe for Improving Faithfulness in Generation with Multi-Agent Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "NAACL 2025, 18 pages. Code:\n  https://github.com/meetdavidwan/mammrefine", "summary": "Multi-agent collaboration among models has shown promise in reasoning tasks\nbut is underexplored in long-form generation tasks like summarization and\nquestion-answering. We extend multi-agent multi-model reasoning to generation,\nspecifically to improving faithfulness through refinement, i.e., revising\nmodel-generated outputs to remove factual inconsistencies. We investigate how\niterative collaboration among multiple instances and types of large language\nmodels (LLMs) enhances subtasks in the refinement process, such as error\ndetection, critiquing unfaithful sentences, and making corrections based on\ncritiques. We design intrinsic evaluations for each subtask, with our findings\nindicating that both multi-agent (multiple instances) and multi-model (diverse\nLLM types) approaches benefit error detection and critiquing. Additionally,\nreframing critiquing and refinement as reranking rather than generation tasks\nimproves multi-agent performance. We consolidate these insights into a final\n\"recipe\" called Multi-Agent Multi-Model Refinement (MAMM-Refine), where\nmulti-agent and multi-model collaboration significantly boosts performance on\nthree summarization datasets as well as on long-form question answering,\ndemonstrating the effectiveness and generalizability of our recipe."}
{"id": "2503.14832", "pdf": "https://arxiv.org/pdf/2503.14832", "abs": "https://arxiv.org/abs/2503.14832", "authors": ["Yuhang Liu", "Wenjie Zhao", "Yunhui Guo"], "title": "H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 8 figures", "summary": "Task Incremental Learning (TIL) is a specialized form of Continual Learning\n(CL) in which a model incrementally learns from non-stationary data streams.\nExisting TIL methodologies operate under the closed-world assumption, presuming\nthat incoming data remains in-distribution (ID). However, in an open-world\nsetting, incoming samples may originate from out-of-distribution (OOD) sources,\nwith their task identities inherently unknown. Continually detecting OOD\nsamples presents several challenges for current OOD detection methods: reliance\non model outputs leads to excessive dependence on model performance, selecting\nsuitable thresholds is difficult, hindering real-world deployment, and binary\nID/OOD classification fails to provide task-level identification. To address\nthese issues, we propose a novel continual OOD detection method called the\nHierarchical Two-sample Tests (H2ST). H2ST eliminates the need for threshold\nselection through hypothesis testing and utilizes feature maps to better\nexploit model capabilities without excessive dependence on model performance.\nThe proposed hierarchical architecture enables task-level detection with\nsuperior performance and lower overhead compared to non-hierarchical classifier\ntwo-sample tests. Extensive experiments and analysis validate the effectiveness\nof H2ST in open-world TIL scenarios and its superiority to the existing\nmethods. Code is available at\n\\href{https://github.com/YuhangLiuu/H2ST}{https://github.com/YuhangLiuu/H2ST}."}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289", "abs": "https://arxiv.org/abs/2503.15289", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "categories": ["cs.CL"], "comment": "15 pages", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains such as healthcare, law, and news, it is\ncrucial to understand where and how the content is created. To address this, we\nintroduce the Text pROVEnance (TROVE) challenge, designed to trace each\nsentence of a target text back to specific source sentences within potentially\nlengthy or multi-document inputs. Beyond identifying sources, TROVE annotates\nthe fine-grained relationships (quotation, compression, inference, and others),\nproviding a deep understanding of how each target sentence is formed. To\nbenchmark TROVE, we construct our dataset by leveraging three public datasets\ncovering 11 diverse scenarios (e.g., QA and summarization) in English and\nChinese, spanning source texts of varying lengths (0-5k, 5-10k, 10k+),\nemphasizing the multi-document and long-document settings essential for\nprovenance. To ensure high-quality data, we employ a three-stage annotation\nprocess: sentence retrieval, GPT provenance, and human provenance. We evaluate\n11 LLMs under direct prompting and retrieval-augmented paradigms, revealing\nthat retrieval is essential for robust performance, larger models perform\nbetter in complex relationship classification, and closed-source models often\nlead, yet open-source models show significant promise, particularly with\nretrieval augmentation."}
{"id": "2503.14837", "pdf": "https://arxiv.org/pdf/2503.14837", "abs": "https://arxiv.org/abs/2503.14837", "authors": ["Yinqi Chen", "Meiying Zhang", "Qi Hao", "Guang Zhou"], "title": "SemanticFlow: A Self-Supervised Framework for Joint Scene Flow Prediction and Instance Segmentation in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate perception of dynamic traffic scenes is crucial for high-level\nautonomous driving systems, requiring robust object motion estimation and\ninstance segmentation. However, traditional methods often treat them as\nseparate tasks, leading to suboptimal performance, spatio-temporal\ninconsistencies, and inefficiency in complex scenarios due to the absence of\ninformation sharing. This paper proposes a multi-task SemanticFlow framework to\nsimultaneously predict scene flow and instance segmentation of full-resolution\npoint clouds. The novelty of this work is threefold: 1) developing a\ncoarse-to-fine prediction based multi-task scheme, where an initial coarse\nsegmentation of static backgrounds and dynamic objects is used to provide\ncontextual information for refining motion and semantic information through a\nshared feature processing module; 2) developing a set of loss functions to\nenhance the performance of scene flow estimation and instance segmentation,\nwhile can help ensure spatial and temporal consistency of both static and\ndynamic objects within traffic scenes; 3) developing a self-supervised learning\nscheme, which utilizes coarse segmentation to detect rigid objects and compute\ntheir transformation matrices between sequential frames, enabling the\ngeneration of self-supervised labels. The proposed framework is validated on\nthe Argoverse and Waymo datasets, demonstrating superior performance in\ninstance segmentation accuracy, scene flow estimation, and computational\nefficiency, establishing a new benchmark for self-supervised methods in dynamic\nscene understanding."}
{"id": "2503.15299", "pdf": "https://arxiv.org/pdf/2503.15299", "abs": "https://arxiv.org/abs/2503.15299", "authors": ["Zorik Gekhman", "Eyal Ben David", "Hadas Orgad", "Eran Ofek", "Yonatan Belinkov", "Idan Szpector", "Jonathan Herzig", "Roi Reichart"], "title": "Inside-Out: Hidden Factual Knowledge in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This work presents a framework for assessing whether large language models\n(LLMs) encode more factual knowledge in their parameters than what they express\nin their outputs. While a few studies hint at this possibility, none has\nclearly defined or demonstrated this phenomenon. We first propose a formal\ndefinition of knowledge, quantifying it for a given question as the fraction of\ncorrect-incorrect answer pairs where the correct one is ranked higher. This\ngives rise to external and internal knowledge, depending on the information\nused to score individual answer candidates: either the model's observable\ntoken-level probabilities or its intermediate computations. Hidden knowledge\narises when internal knowledge exceeds external knowledge. We then present a\ncase study, applying this framework to three popular open-weights LLMs in a\nclosed-book QA setup. Our results indicate that: (1) LLMs consistently encode\nmore factual knowledge internally than what they express externally, with an\naverage gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a\nmodel can internally know an answer perfectly, yet fail to generate it even\nonce, despite large-scale repeated sampling of 1,000 answers. This reveals\nfundamental limitations in the generation capabilities of LLMs, which (3) puts\na practical constraint on scaling test-time compute via repeated answer\nsampling in closed-book QA: significant performance improvements remain\ninaccessible because some answers are practically never sampled, yet if they\nwere, we would be guaranteed to rank them first."}
{"id": "2503.14853", "pdf": "https://arxiv.org/pdf/2503.14853", "abs": "https://arxiv.org/abs/2503.14853", "authors": ["Peipeng Yu", "Jianwei Fei", "Hui Gao", "Xuan Feng", "Zhihua Xia", "Chip Hong Chang"], "title": "Unlocking the Capabilities of Vision-Language Models for Generalizable and Explainable Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Current vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding multimodal data, but their potential remains\nunderexplored for deepfake detection due to the misaligned of their knowledge\nand forensics patterns. To this end, we present a novel paradigm that unlocks\nVLMs' potential capabilities through three components: (1) A knowledge-guided\nforgery adaptation module that aligns VLM's semantic space with forensic\nfeatures through contrastive learning with external manipulation knowledge; (2)\nA multi-modal prompt tuning framework that jointly optimizes visual-textual\nembeddings for both localization and explainability; (3) An iterative\nrefinement strategy enabling multi-turn dialog for evidence-based reasoning.\nOur framework includes a VLM-based Knowledge-guided Forgery Detector (KFD), a\nVLM image encoder, and a Large Language Model (LLM). The VLM image encoder\nextracts visual prompt embeddings from images, while the LLM receives visual\nand question prompt embeddings for inference. The KFD is used to calculate\ncorrelations between image features and pristine/deepfake class embeddings,\nenabling forgery classification and localization. The outputs from these\ncomponents are used to construct forgery prompt embeddings. Finally, we feed\nthese prompt embeddings into the LLM to generate textual detection responses to\nassist judgment. Extensive experiments on multiple benchmarks, including FF++,\nCDF2, DFD, DFDCP, and DFDC, demonstrate that our scheme surpasses\nstate-of-the-art methods in generalization performance, while also supporting\nmulti-turn dialogue capabilities."}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351", "abs": "https://arxiv.org/abs/2503.15351", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals."}
{"id": "2503.14862", "pdf": "https://arxiv.org/pdf/2503.14862", "abs": "https://arxiv.org/abs/2503.14862", "authors": ["Ying Liu", "Yijing Hua", "Haojiang Chai", "Yanbo Wang", "TengQi Ye"], "title": "Fine-Grained Open-Vocabulary Object Detection with Fined-Grained Prompts: Task, Dataset and Benchmark", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "Open-vocabulary detectors are proposed to locate and recognize objects in\nnovel classes. However, variations in vision-aware language vocabulary data\nused for open-vocabulary learning can lead to unfair and unreliable\nevaluations. Recent evaluation methods have attempted to address this issue by\nincorporating object properties or adding locations and characteristics to the\ncaptions. Nevertheless, since these properties and locations depend on the\nspecific details of the images instead of classes, detectors can not make\naccurate predictions without precise descriptions provided through human\nannotation. This paper introduces 3F-OVD, a novel task that extends supervised\nfine-grained object detection to the open-vocabulary setting. Our task is\nintuitive and challenging, requiring a deep understanding of Fine-grained\ncaptions and careful attention to Fine-grained details in images in order to\naccurately detect Fine-grained objects. Additionally, due to the scarcity of\nqualified fine-grained object detection datasets, we have created a new\ndataset, NEU-171K, tailored for both supervised and open-vocabulary settings.\nWe benchmark state-of-the-art object detectors on our dataset for both\nsettings. Furthermore, we propose a simple yet effective post-processing\ntechnique."}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354", "abs": "https://arxiv.org/abs/2503.15354", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "title": "Optimizing Decomposition for Optimal Claim Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims."}
{"id": "2503.14863", "pdf": "https://arxiv.org/pdf/2503.14863", "abs": "https://arxiv.org/abs/2503.14863", "authors": ["Hengkang Wang", "Yang Liu", "Huidong Liu", "Chien-Chih Wang", "Yanhui Guo", "Hongdong Li", "Bryan Wang", "Ju Sun"], "title": "Temporal-Consistent Video Restoration with Pre-trained Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video restoration (VR) aims to recover high-quality videos from degraded\nones. Although recent zero-shot VR methods using pre-trained diffusion models\n(DMs) show good promise, they suffer from approximation errors during reverse\ndiffusion and insufficient temporal consistency. Moreover, dealing with 3D\nvideo data, VR is inherently computationally intensive. In this paper, we\nadvocate viewing the reverse process in DMs as a function and present a novel\nMaximum a Posterior (MAP) framework that directly parameterizes video frames in\nthe seed space of DMs, eliminating approximation errors. We also introduce\nstrategies to promote bilevel temporal consistency: semantic consistency by\nleveraging clustering structures in the seed space, and pixel-level consistency\nby progressive warping with optical flow refinements. Extensive experiments on\nmultiple virtual reality tasks demonstrate superior visual quality and temporal\nconsistency achieved by our method compared to the state-of-the-art."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.14867", "pdf": "https://arxiv.org/pdf/2503.14867", "abs": "https://arxiv.org/abs/2503.14867", "authors": ["Caoshuo Li", "Tanzhe Li", "Xiaobin Hu", "Donghao Luo", "Taisong Jin"], "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, Vision Graph Neural Network (ViG) has gained considerable attention\nin computer vision. Despite its groundbreaking innovation, Vision Graph Neural\nNetwork encounters key issues including the quadratic computational complexity\ncaused by its K-Nearest Neighbor (KNN) graph construction and the limitation of\npairwise relations of normal graphs. To address the aforementioned challenges,\nwe propose a novel vision architecture, termed Dilated Vision HyperGraph Neural\nNetwork (DVHGNN), which is designed to leverage multi-scale hypergraph to\nefficiently capture high-order correlations among objects. Specifically, the\nproposed method tailors Clustering and Dilated HyperGraph Construction (DHGC)\nto adaptively capture multi-scale dependencies among the data samples.\nFurthermore, a dynamic hypergraph convolution mechanism is proposed to\nfacilitate adaptive feature exchange and fusion at the hypergraph level.\nExtensive qualitative and quantitative evaluations of the benchmark image\ndatasets demonstrate that the proposed DVHGNN significantly outperforms the\nstate-of-the-art vision backbones. For instance, our DVHGNN-S achieves an\nimpressive top-1 accuracy of 83.1% on ImageNet-1K, surpassing ViG-S by +1.0%\nand ViHGNN-S by +0.6%."}
{"id": "2503.15374", "pdf": "https://arxiv.org/pdf/2503.15374", "abs": "https://arxiv.org/abs/2503.15374", "authors": ["Anatole Callies", "Quentin Bodinier", "Philippe Ravaud", "Kourosh Davarpanah"], "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Background: Patient recruitment in clinical trials is hindered by complex\neligibility criteria and labor-intensive chart reviews. Prior research using\ntext-only models have struggled to address this problem in a reliable and\nscalable way due to (1) limited reasoning capabilities, (2) information loss\nfrom converting visual records to text, and (3) lack of a generic EHR\nintegration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered\npipeline that automates patient-trial matching using unprocessed documents\nextracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm,\nenabling the assessment of even the most complex criteria, (2) visual\ncapabilities of latest LLMs to interpret medical records without lossy\nimage-to-text conversions, and (3) multimodal embeddings for efficient medical\nrecord search. The pipeline was validated on the n2c2 2018 cohort selection\ndataset (288 diabetic patients) and a real-world dataset composed of 485\npatients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art\ncriterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an\naccuracy of 87\\%, undermined by the difficulty to replicate human\ndecision-making when medical records lack sufficient information. Nevertheless,\nusers were able to review overall eligibility in under 9 minutes per patient on\naverage, representing an 80\\% improvement over traditional manual chart\nreviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial\npatient matching without requiring custom integration with site systems or\ntrial-specific tailoring, thereby enabling scalable deployment across sites\nseeking to leverage AI for patient matching."}
{"id": "2503.14868", "pdf": "https://arxiv.org/pdf/2503.14868", "abs": "https://arxiv.org/abs/2503.14868", "authors": ["Hoigi Seo", "Wongi Jeong", "Kyungryeol Lee", "Se Young Chun"], "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models have shown remarkable performance in image synthesis, but\nthey demand extensive computational and memory resources for training,\nfine-tuning and inference. Although advanced quantization techniques have\nsuccessfully minimized memory usage for inference, training and fine-tuning\nthese quantized models still require large memory possibly due to\ndequantization for accurate computation of gradients and/or backpropagation for\ngradient-based algorithms. However, memory-efficient fine-tuning is\nparticularly desirable for applications such as personalization that often must\nbe run on edge devices like mobile phones with private data. In this work, we\naddress this challenge by quantizing a diffusion model with personalization via\nTextual Inversion and by leveraging a zeroth-order optimization on\npersonalization tokens without dequantization so that it does not require\ngradient and activation storage for backpropagation that consumes considerable\nmemory. Since a gradient estimation using zeroth-order optimization is quite\nnoisy for a single or a few images in personalization, we propose to denoise\nthe estimated gradient by projecting it onto a subspace that is constructed\nwith the past history of the tokens, dubbed Subspace Gradient. In addition, we\ninvestigated the influence of text embedding in image generation, leading to\nour proposed time steps sampling, dubbed Partial Uniform Timestep Sampling for\nsampling with effective diffusion timesteps. Our method achieves comparable\nperformance to prior methods in image and text alignment scores for\npersonalizing Stable Diffusion with only forward passes while reducing training\nmemory demand up to $8.2\\times$."}
{"id": "2503.15438", "pdf": "https://arxiv.org/pdf/2503.15438", "abs": "https://arxiv.org/abs/2503.15438", "authors": ["Yang Tan", "Chen Liu", "Jingyuan Gao", "Banghao Wu", "Mingchen Li", "Ruilin Wang", "Lingrong Zhang", "Huiqun Yu", "Guisheng Fan", "Liang Hong", "Bingxin Zhou"], "title": "VenusFactory: A Unified Platform for Protein Engineering Data Retrieval and Language Model Fine-Tuning", "categories": ["cs.CL", "cs.AI", "q-bio.QM"], "comment": "12 pages, 1 figure, 8 tables", "summary": "Natural language processing (NLP) has significantly influenced scientific\ndomains beyond human language, including protein engineering, where pre-trained\nprotein language models (PLMs) have demonstrated remarkable success. However,\ninterdisciplinary adoption remains limited due to challenges in data\ncollection, task benchmarking, and application. This work presents\nVenusFactory, a versatile engine that integrates biological data retrieval,\nstandardized task benchmarking, and modular fine-tuning of PLMs. VenusFactory\nsupports both computer science and biology communities with choices of both a\ncommand-line execution and a Gradio-based no-code interface, integrating $40+$\nprotein-related datasets and $40+$ popular PLMs. All implementations are\nopen-sourced on https://github.com/tyang816/VenusFactory."}
{"id": "2503.14880", "pdf": "https://arxiv.org/pdf/2503.14880", "abs": "https://arxiv.org/abs/2503.14880", "authors": ["Henrique Morimitsu", "Xiaobin Zhu", "Roberto M. Cesar Jr.", "Xiangyang Ji", "Xu-Cheng Yin"], "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025. The code and dataset are available at\n  https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow. 24\n  pages, 17 figures", "summary": "Optical flow estimation is essential for video processing tasks, such as\nrestoration and action recognition. The quality of videos is constantly\nincreasing, with current standards reaching 8K resolution. However, optical\nflow methods are usually designed for low resolution and do not generalize to\nlarge inputs due to their rigid architectures. They adopt downscaling or input\ntiling to reduce the input size, causing a loss of details and global\ninformation. There is also a lack of optical flow benchmarks to judge the\nactual performance of existing methods on high-resolution samples. Previous\nworks only conducted qualitative high-resolution evaluations on hand-picked\nsamples. This paper fills this gap in optical flow estimation in two ways. We\npropose DPFlow, an adaptive optical flow architecture capable of generalizing\nup to 8K resolution inputs while trained with only low-resolution samples. We\nalso introduce Kubric-NK, a new benchmark for evaluating optical flow methods\nwith input resolutions ranging from 1K to 8K. Our high-resolution evaluation\npushes the boundaries of existing methods and reveals new insights about their\ngeneralization capabilities. Extensive experimental results show that DPFlow\nachieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and\nother high-resolution benchmarks."}
{"id": "2503.15450", "pdf": "https://arxiv.org/pdf/2503.15450", "abs": "https://arxiv.org/abs/2503.15450", "authors": ["Tongyao Zhu", "Qian Liu", "Haonan Wang", "Shiqi Chen", "Xiangming Gu", "Tianyu Pang", "Min-Yen Kan"], "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling", "categories": ["cs.CL"], "comment": "22 pages. Accepted to ICLR 2025 Workshop on Open Science for\n  Foundation Models", "summary": "Recent advancements in LLM pretraining have featured ever-expanding context\nwindows to process longer sequences. However, our pilot study reveals that\nmodels pretrained with shorter context windows consistently outperform their\nlong-context counterparts under a fixed token budget. This finding motivates us\nto explore an optimal context window scheduling strategy to better balance\nlong-context capability with pretraining efficiency. To this end, we propose\nSkyLadder, a simple yet effective approach that implements a short-to-long\ncontext window transition. SkyLadder preserves strong standard benchmark\nperformance, while matching or exceeding baseline results on long context\ntasks. Through extensive experiments, we pre-train 1B-parameter models (up to\n32K context) and 3B-parameter models (8K context) on 100B tokens, demonstrating\nthat SkyLadder yields consistent gains of up to 3.7% on common benchmarks,\nwhile achieving up to 22% faster training speeds compared to baselines. The\ncode is at https://github.com/sail-sg/SkyLadder."}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark."}
{"id": "2503.15454", "pdf": "https://arxiv.org/pdf/2503.15454", "abs": "https://arxiv.org/abs/2503.15454", "authors": ["Yuelyu Ji", "Hang Zhang", "Yanshan Wang"], "title": "Evaluating Bias in Retrieval-Augmented Medical Question-Answering Systems", "categories": ["cs.CL"], "comment": null, "summary": "Medical QA systems powered by Retrieval-Augmented Generation (RAG) models\nsupport clinical decision-making but may introduce biases related to race,\ngender, and social determinants of health. We systematically evaluate biases in\nRAG-based LLM by examining demographic-sensitive queries and measuring\nretrieval discrepancies. Using datasets like MMLU and MedMCQA, we analyze\nretrieval overlap and correctness disparities. Our findings reveal substantial\ndemographic disparities within RAG pipelines, emphasizing the critical need for\nretrieval methods that explicitly account for fairness to ensure equitable\nclinical decision-making."}
{"id": "2503.14897", "pdf": "https://arxiv.org/pdf/2503.14897", "abs": "https://arxiv.org/abs/2503.14897", "authors": ["Vaibhav Rathore", "Shubhranil B", "Saikat Dutta", "Sarthak Mehrotra", "Zsolt Kira", "Biplab Banerjee"], "title": "When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach", "categories": ["cs.CV"], "comment": null, "summary": "Generalized Class Discovery (GCD) clusters base and novel classes in a target\ndomain using supervision from a source domain with only base classes. Current\nmethods often falter with distribution shifts and typically require access to\ntarget data during training, which can sometimes be impractical. To address\nthis issue, we introduce the novel paradigm of Domain Generalization in GCD\n(DG-GCD), where only source data is available for training, while the target\ndomain, with a distinct data distribution, remains unseen until inference. To\nthis end, our solution, DG2CD-Net, aims to construct a domain-independent,\ndiscriminative embedding space for GCD. The core innovation is an episodic\ntraining strategy that enhances cross-domain generalization by adapting a base\nmodel on tasks derived from source and synthetic domains generated by a\nfoundation model. Each episode focuses on a cross-domain GCD task, diversifying\ntask setups over episodes and combining open-set domain adaptation with a novel\nmargin loss and representation learning for optimizing the feature space\nprogressively. To capture the effects of fine-tuning on the base model, we\nextend task arithmetic by adaptively weighting the local task vectors\nconcerning the fine-tuned models based on their GCD performance on a validation\ndistribution. This episodic update mechanism boosts the adaptability of the\nbase model to unseen targets. Experiments across three datasets confirm that\nDG2CD-Net outperforms existing GCD methods customized for DG-GCD."}
{"id": "2503.15463", "pdf": "https://arxiv.org/pdf/2503.15463", "abs": "https://arxiv.org/abs/2503.15463", "authors": ["Jia-Nan Li", "Jian Guan", "Songhao Wu", "Wei Wu", "Rui Yan"], "title": "From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have traditionally been aligned through\none-size-fits-all approaches that assume uniform human preferences,\nfundamentally overlooking the diversity in user values and needs. This paper\nintroduces a comprehensive framework for scalable personalized alignment of\nLLMs. We establish a systematic preference space characterizing psychological\nand behavioral dimensions, alongside diverse persona representations for robust\npreference inference in real-world scenarios. Building upon this foundation, we\nintroduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million\npersonalized preference examples, and develop two complementary alignment\napproaches: \\textit{in-context alignment} directly conditioning on persona\nrepresentations and \\textit{preference-bridged alignment} modeling intermediate\npreference distributions. Extensive experiments demonstrate substantial\nimprovements over existing methods, with an average 17.06\\% accuracy gain\nacross four benchmarks while exhibiting a strong adaptation capability to novel\npreferences, robustness to limited user data, and precise preference\ncontrollability. These results validate our framework's effectiveness,\nadvancing toward truly user-adaptive AI systems."}
{"id": "2503.14905", "pdf": "https://arxiv.org/pdf/2503.14905", "abs": "https://arxiv.org/abs/2503.14905", "authors": ["Siwei Wen", "Junyan Ye", "Peilin Feng", "Hengrui Kang", "Zichen Wen", "Yize Chen", "Jiang Wu", "Wenjun Wu", "Conghui He", "Weijia Li"], "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) technologies, synthetic images have become increasingly prevalent in\neveryday life, posing new challenges for authenticity assessment and detection.\nDespite the effectiveness of existing methods in evaluating image authenticity\nand locating forgeries, these approaches often lack human interpretability and\ndo not fully address the growing complexity of synthetic data. To tackle these\nchallenges, we introduce FakeVLM, a specialized large multimodal model designed\nfor both general synthetic image and DeepFake detection tasks. FakeVLM not only\nexcels in distinguishing real from fake images but also provides clear, natural\nlanguage explanations for image artifacts, enhancing interpretability.\nAdditionally, we present FakeClue, a comprehensive dataset containing over\n100,000 images across seven categories, annotated with fine-grained artifact\nclues in natural language. FakeVLM demonstrates performance comparable to\nexpert models while eliminating the need for additional classifiers, making it\na robust solution for synthetic data detection. Extensive evaluations across\nmultiple datasets confirm the superiority of FakeVLM in both authenticity\nclassification and artifact explanation tasks, setting a new benchmark for\nsynthetic image detection. The dataset and code will be released in:\nhttps://github.com/opendatalab/FakeVLM."}
{"id": "2503.15469", "pdf": "https://arxiv.org/pdf/2503.15469", "abs": "https://arxiv.org/abs/2503.15469", "authors": ["ZhengLin Lai", "MengYao Liao", "Dong Xu"], "title": "Dynamic Bi-Elman Attention Networks (DBEAN): Dual-Directional Context-Aware Representation Learning for Enhanced Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages,1 figure", "summary": "Text classification, a fundamental task in natural language processing (NLP),\naims to categorize textual data into predefined labels. Traditional methods\nstruggled with complex linguistic structures and semantic dependencies. The\nadvent of deep learning, particularly recurrent neural networks (RNNs) and\nTransformer-based models, has significantly advanced the field by enabling\nnuanced feature extraction and context-aware predictions. Despite improvements,\nexisting models exhibit limitations in balancing interpretability,\ncomputational efficiency, and long-range contextual understanding. This paper\nproposes the Dynamic Bidirectional Elman with Attention Network (DBEAN), which\nintegrates bidirectional temporal modelling with self-attention mechanisms.\nDBEAN dynamically assigns weights to critical segments of input, improving\ncontextual representation while maintaining computational efficiency."}
{"id": "2503.14910", "pdf": "https://arxiv.org/pdf/2503.14910", "abs": "https://arxiv.org/abs/2503.14910", "authors": ["Jingyi Liao", "Xun Xu", "Yongyi Su", "Rong-Cheng Tu", "Yifan Liu", "Dacheng Tao", "Xulei Yang"], "title": "Robust Distribution Alignment for Industrial Anomaly Detection under Distribution Shift", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection plays a crucial role in quality control for industrial\napplications. However, ensuring robustness under unseen domain shifts such as\nlighting variations or sensor drift remains a significant challenge. Existing\nmethods attempt to address domain shifts by training generalizable models but\noften rely on prior knowledge of target distributions and can hardly generalise\nto backbones designed for other data modalities. To overcome these limitations,\nwe build upon memory-bank-based anomaly detection methods, optimizing a robust\nSinkhorn distance on limited target training data to enhance generalization to\nunseen target domains. We evaluate the effectiveness on both 2D and 3D anomaly\ndetection benchmarks with simulated distribution shifts. Our proposed method\ndemonstrates superior results compared with state-of-the-art anomaly detection\nand domain adaptation methods."}
{"id": "2503.15484", "pdf": "https://arxiv.org/pdf/2503.15484", "abs": "https://arxiv.org/abs/2503.15484", "authors": ["Taylor Sorensen", "Pushkar Mishra", "Roma Patel", "Michael Henry Tessler", "Michiel Bakker", "Georgina Evans", "Iason Gabriel", "Noah Goodman", "Verena Rieser"], "title": "Value Profiles for Encoding Human Variation", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Modelling human variation in rating tasks is crucial for enabling AI systems\nfor personalization, pluralistic model alignment, and computational social\nscience. We propose representing individuals using value profiles -- natural\nlanguage descriptions of underlying values compressed from in-context\ndemonstrations -- along with a steerable decoder model to estimate ratings\nconditioned on a value profile or other rater information. To measure the\npredictive information in rater representations, we introduce an\ninformation-theoretic methodology. We find that demonstrations contain the most\ninformation, followed by value profiles and then demographics. However, value\nprofiles offer advantages in terms of scrutability, interpretability, and\nsteerability due to their compressed natural language format. Value profiles\neffectively compress the useful information from demonstrations (>70%\ninformation preservation). Furthermore, clustering value profiles to identify\nsimilarly behaving individuals better explains rater variation than the most\npredictive demographic groupings. Going beyond test set performance, we show\nthat the decoder models interpretably change ratings according to semantic\nprofile differences, are well-calibrated, and can help explain instance-level\ndisagreement by simulating an annotator population. These results demonstrate\nthat value profiles offer novel, predictive ways to describe individual\nvariation beyond demographics or group information."}
{"id": "2503.14911", "pdf": "https://arxiv.org/pdf/2503.14911", "abs": "https://arxiv.org/abs/2503.14911", "authors": ["Siyuan Yan", "Ming Hu", "Yiwen Jiang", "Xieji Li", "Hao Fei", "Philipp Tschandl", "Harald Kittler", "Zongyuan Ge"], "title": "Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology", "categories": ["cs.CV"], "comment": "23 pages", "summary": "The emergence of vision-language models has transformed medical AI, enabling\nunprecedented advances in diagnostic capability and clinical applications.\nHowever, progress in dermatology has lagged behind other medical domains due to\nthe lack of standard image-text pairs. Existing dermatological datasets are\nlimited in both scale and depth, offering only single-label annotations across\na narrow range of diseases instead of rich textual descriptions, and lacking\nthe crucial clinical context needed for real-world applications. To address\nthese limitations, we present Derm1M, the first large-scale vision-language\ndataset for dermatology, comprising 1,029,761 image-text pairs. Built from\ndiverse educational resources and structured around a standard ontology\ncollaboratively developed by experts, Derm1M provides comprehensive coverage\nfor over 390 skin conditions across four hierarchical levels and 130 clinical\nconcepts with rich contextual information such as medical history, symptoms,\nand skin tone. To demonstrate Derm1M potential in advancing both AI research\nand clinical application, we pretrained a series of CLIP-like models,\ncollectively called DermLIP, on this dataset. The DermLIP family significantly\noutperforms state-of-the-art foundation models on eight diverse datasets across\nmultiple tasks, including zero-shot skin disease classification, clinical and\nartifacts concept identification, few-shot/full-shot learning, and cross-modal\nretrieval. Our dataset and code will be public."}
{"id": "2503.14521", "pdf": "https://arxiv.org/pdf/2503.14521", "abs": "https://arxiv.org/abs/2503.14521", "authors": ["Yihang Chen", "Haikang Deng", "Kaiqiao Han", "Qingyue Zhao"], "title": "Policy Frameworks for Transparent Chain-of-Thought Reasoning in Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by\ndecomposing complex problems into step-by-step solutions, improving performance\non reasoning tasks. However, current CoT disclosure policies vary widely across\ndifferent models in frontend visibility, API access, and pricing strategies,\nlacking a unified policy framework. This paper analyzes the dual-edged\nimplications of full CoT disclosure: while it empowers small-model\ndistillation, fosters trust, and enables error diagnosis, it also risks\nviolating intellectual property, enabling misuse, and incurring operational\ncosts. We propose a tiered-access policy framework that balances transparency,\naccountability, and security by tailoring CoT availability to academic,\nbusiness, and general users through ethical licensing, structured reasoning\noutputs, and cross-tier safeguards. By harmonizing accessibility with ethical\nand operational considerations, this framework aims to advance responsible AI\ndeployment while mitigating risks of misuse or misinterpretation."}
{"id": "2503.14912", "pdf": "https://arxiv.org/pdf/2503.14912", "abs": "https://arxiv.org/abs/2503.14912", "authors": ["Gahye Lee", "Hyejeong Yoon", "Jungeon Kim", "Seungyong Lee"], "title": "Deep Polycuboid Fitting for Compact 3D Representation of Indoor Scenes", "categories": ["cs.CV", "I.4.8; I.3.5"], "comment": "Accepted to 3DV 2025", "summary": "This paper presents a novel framework for compactly representing a 3D indoor\nscene using a set of polycuboids through a deep learning-based fitting method.\nIndoor scenes mainly consist of man-made objects, such as furniture, which\noften exhibit rectilinear geometry. This property allows indoor scenes to be\nrepresented using combinations of polycuboids, providing a compact\nrepresentation that benefits downstream applications like furniture\nrearrangement. Our framework takes a noisy point cloud as input and first\ndetects six types of cuboid faces using a transformer network. Then, a graph\nneural network is used to validate the spatial relationships of the detected\nfaces to form potential polycuboids. Finally, each polycuboid instance is\nreconstructed by forming a set of boxes based on the aggregated face labels. To\ntrain our networks, we introduce a synthetic dataset encompassing a diverse\nrange of cuboid and polycuboid shapes that reflect the characteristics of\nindoor scenes. Our framework generalizes well to real-world indoor scene\ndatasets, including Replica, ScanNet, and scenes captured with an iPhone. The\nversatility of our method is demonstrated through practical applications, such\nas virtual room tours and scene editing."}
{"id": "2503.14527", "pdf": "https://arxiv.org/pdf/2503.14527", "abs": "https://arxiv.org/abs/2503.14527", "authors": ["Mohammed Alnajjar", "Khalid Alnajjar", "Mika H√§m√§l√§inen"], "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare SMEs", "categories": ["cs.CY", "cs.CL", "cs.HC"], "comment": null, "summary": "This study examines AI adoption among Finnish healthcare SMEs through\nsemi-structured interviews with six health-tech companies. We identify three AI\nengagement categories: AI-curious (exploring AI), AI-embracing (integrating\nAI), and AI-catering (providing AI solutions). Our proposed threefold model\nhighlights key adoption barriers, including regulatory complexities, technical\nexpertise gaps, and financial constraints. While SMEs recognize AI's potential,\nmost remain in early adoption stages. We provide actionable recommendations to\naccelerate AI integration, focusing on regulatory reforms, talent development,\nand inter-company collaboration, offering valuable insights for healthcare\norganizations, policymakers, and researchers."}
{"id": "2503.14919", "pdf": "https://arxiv.org/pdf/2503.14919", "abs": "https://arxiv.org/abs/2503.14919", "authors": ["Junyu Shi", "Lijiang Liu", "Yong Sun", "Zhiyuan Zhang", "Jinni Zhou", "Qiang Nie"], "title": "GenM$^3$: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Scaling up motion datasets is crucial to enhance motion generation\ncapabilities. However, training on large-scale multi-source datasets introduces\ndata heterogeneity challenges due to variations in motion content. To address\nthis, we propose Generative Pretrained Multi-path Motion Model (GenM$^3$), a\ncomprehensive framework designed to learn unified motion representations.\nGenM$^3$ comprises two components: 1) a Multi-Expert VQ-VAE (MEVQ-VAE) that\nadapts to different dataset distributions to learn a unified discrete motion\nrepresentation, and 2) a Multi-path Motion Transformer (MMT) that improves\nintra-modal representations by using separate modality-specific pathways, each\nwith densely activated experts to accommodate variations within that modality,\nand improves inter-modal alignment by the text-motion shared pathway. To enable\nlarge-scale training, we integrate and unify 11 high-quality motion datasets\n(approximately 220 hours of motion data) and augment it with textual\nannotations (nearly 10,000 motion sequences labeled by a large language model\nand 300+ by human experts). After training on our integrated dataset, GenM$^3$\nachieves a state-of-the-art FID of 0.035 on the HumanML3D benchmark, surpassing\nstate-of-the-art methods by a large margin. It also demonstrates strong\nzero-shot generalization on IDEA400 dataset, highlighting its effectiveness and\nadaptability across diverse motion scenarios."}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning."}
{"id": "2503.14928", "pdf": "https://arxiv.org/pdf/2503.14928", "abs": "https://arxiv.org/abs/2503.14928", "authors": ["Jiaxin Ye", "Hongming Shan"], "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "comment": "Project Page: https://imagintalk.github.io", "summary": "Vision-guided speech generation aims to produce authentic speech from facial\nappearance or lip motions without relying on auditory signals, offering\nsignificant potential for applications such as dubbing in filmmaking and\nassisting individuals with aphonia. Despite recent progress, existing methods\nstruggle to achieve unified cross-modal alignment across semantics, timbre, and\nemotional prosody from visual cues, prompting us to propose Consistent\nVideo-to-Speech (CV2S) as an extended task to enhance cross-modal consistency.\nTo tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal\ndiffusion framework that generates faithful speech using only visual input,\noperating within a discrete space. Specifically, we propose a discrete lip\naligner that predicts discrete speech tokens from lip videos to capture\nsemantic information, while an error detector identifies misaligned tokens,\nwhich are subsequently refined through masked language modeling with BERT. To\nfurther enhance the expressiveness of the generated speech, we develop a style\ndiffusion transformer equipped with a face-style adapter that adaptively\ncustomizes identity and prosody dynamics across both the channel and temporal\ndimensions while ensuring synchronization with lip-aware semantic features.\nExtensive experiments demonstrate that ImaginTalk can generate high-fidelity\nspeech with more accurate semantic details and greater expressiveness in timbre\nand emotion compared to state-of-the-art baselines. Demos are shown at our\nproject page: https://imagintalk.github.io."}
{"id": "2503.14604", "pdf": "https://arxiv.org/pdf/2503.14604", "abs": "https://arxiv.org/abs/2503.14604", "authors": ["Sara Sarto", "Marcella Cornia", "Rita Cucchiara"], "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Repo GitHub:\n  https://github.com/aimagelab/awesome-captioning-evaluation", "summary": "The evaluation of machine-generated image captions is a complex and evolving\nchallenge. With the advent of Multimodal Large Language Models (MLLMs), image\ncaptioning has become a core task, increasing the need for robust and reliable\nevaluation metrics. This survey provides a comprehensive overview of\nadvancements in image captioning evaluation, analyzing the evolution,\nstrengths, and limitations of existing metrics. We assess these metrics across\nmultiple dimensions, including correlation with human judgment, ranking\naccuracy, and sensitivity to hallucinations. Additionally, we explore the\nchallenges posed by the longer and more detailed captions generated by MLLMs\nand examine the adaptability of current metrics to these stylistic variations.\nOur analysis highlights some limitations of standard evaluation approaches and\nsuggests promising directions for future research in image captioning\nassessment."}
{"id": "2503.14935", "pdf": "https://arxiv.org/pdf/2503.14935", "abs": "https://arxiv.org/abs/2503.14935", "authors": ["Chongjun Tu", "Lin Zhang", "Pengtao Chen", "Peng Ye", "Xianfang Zeng", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "FAVOR-Bench: A Comprehensive Benchmark for Fine-Grained Video Motion Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "FAVOR-Bench project page: https://favor-bench.github.io/", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable capabilities\nin video content understanding but still struggle with fine-grained motion\ncomprehension. To comprehensively assess the motion understanding ability of\nexisting MLLMs, we introduce FAVOR-Bench, comprising 1,776 videos with\nstructured manual annotations of various motions. Our benchmark includes both\nclose-ended and open-ended tasks. For close-ended evaluation, we carefully\ndesign 8,184 multiple-choice question-answer pairs spanning six distinct\nsub-tasks. For open-ended evaluation, we develop both a novel cost-efficient\nLLM-free and a GPT-assisted caption assessment method, where the former can\nenhance benchmarking interpretability and reproducibility. Comprehensive\nexperiments with 21 state-of-the-art MLLMs reveal significant limitations in\ntheir ability to comprehend and describe detailed temporal dynamics in video\nmotions. To alleviate this limitation, we further build FAVOR-Train, a dataset\nconsisting of 17,152 videos with fine-grained motion annotations. The results\nof finetuning Qwen2.5-VL on FAVOR-Train yield consistent improvements on\nmotion-related tasks of TVBench, MotionBench and our FAVOR-Bench. Comprehensive\nassessment results demonstrate that the proposed FAVOR-Bench and FAVOR-Train\nprovide valuable tools to the community for developing more powerful video\nunderstanding models. Project page:\n\\href{https://favor-bench.github.io/}{https://favor-bench.github.io/}."}
{"id": "2503.14615", "pdf": "https://arxiv.org/pdf/2503.14615", "abs": "https://arxiv.org/abs/2503.14615", "authors": ["Selim Jerad", "Anej Svete", "Jiaoda Li", "Ryan Cotterell"], "title": "Unique Hard Attention: A Tale of Two Sides", "categories": ["cs.LG", "cs.CC", "cs.CL", "cs.FL"], "comment": null, "summary": "Understanding the expressive power of transformers has recently attracted\nattention, as it offers insights into their abilities and limitations. Many\nstudies analyze unique hard attention transformers, where attention selects a\nsingle position that maximizes the attention scores. When multiple positions\nachieve the maximum score, either the rightmost or the leftmost of those is\nchosen. In this paper, we highlight the importance of this seeming triviality.\nRecently, finite-precision transformers with both leftmost- and rightmost-hard\nattention were shown to be equivalent to Linear Temporal Logic (LTL). We show\nthat this no longer holds with only leftmost-hard attention -- in that case,\nthey correspond to a \\emph{strictly weaker} fragment of LTL. Furthermore, we\nshow that models with leftmost-hard attention are equivalent to \\emph{soft}\nattention, suggesting they may better approximate real-world transformers than\nright-attention models. These findings refine the landscape of transformer\nexpressivity and underscore the role of attention directionality."}
{"id": "2503.14938", "pdf": "https://arxiv.org/pdf/2503.14938", "abs": "https://arxiv.org/abs/2503.14938", "authors": ["Zhong Ji", "Ci Liu", "Jingren Liu", "Chen Tang", "Yanwei Pang", "Xuelong Li"], "title": "Optimal Transport Adapter Tuning for Bridging Modality Gaps in Few-Shot Remote Sensing Scene Classification", "categories": ["cs.CV"], "comment": null, "summary": "Few-Shot Remote Sensing Scene Classification (FS-RSSC) presents the challenge\nof classifying remote sensing images with limited labeled samples. Existing\nmethods typically emphasize single-modal feature learning, neglecting the\npotential benefits of optimizing multi-modal representations. To address this\nlimitation, we propose a novel Optimal Transport Adapter Tuning (OTAT)\nframework aimed at constructing an ideal Platonic representational space\nthrough optimal transport (OT) theory. This framework seeks to harmonize rich\nvisual information with less dense textual cues, enabling effective cross-modal\ninformation transfer and complementarity. Central to this approach is the\nOptimal Transport Adapter (OTA), which employs a cross-modal attention\nmechanism to enrich textual representations and facilitate subsequent better\ninformation interaction. By transforming the network optimization into an OT\noptimization problem, OTA establishes efficient pathways for balanced\ninformation exchange between modalities. Moreover, we introduce a sample-level\nEntropy-Aware Weighted (EAW) loss, which combines difficulty-weighted\nsimilarity scores with entropy-based regularization. This loss function\nprovides finer control over the OT optimization process, enhancing its\nsolvability and stability. Our framework offers a scalable and efficient\nsolution for advancing multimodal learning in remote sensing applications.\nExtensive experiments on benchmark datasets demonstrate that OTAT achieves\nstate-of-the-art performance in FS-RSSC, significantly improving the model\nperformance and generalization."}
{"id": "2503.14649", "pdf": "https://arxiv.org/pdf/2503.14649", "abs": "https://arxiv.org/abs/2503.14649", "authors": ["Wenqi Jiang", "Suvinay Subramanian", "Cat Graves", "Gustavo Alonso", "Amir Yazdanbakhsh", "Vidushi Dadu"], "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions."}
{"id": "2503.14939", "pdf": "https://arxiv.org/pdf/2503.14939", "abs": "https://arxiv.org/abs/2503.14939", "authors": ["Tengjin Weng", "Jingyi Wang", "Wenhao Jiang", "Zhong Ming"], "title": "VisNumBench: Evaluating Number Sense of Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Can Multimodal Large Language Models (MLLMs) develop an intuitive number\nsense similar to humans? Targeting this problem, we introduce Visual Number\nBenchmark (VisNumBench) to evaluate the number sense abilities of MLLMs across\na wide range of visual numerical tasks. VisNumBench consists of about 1,900\nmultiple-choice question-answer pairs derived from both synthetic and\nreal-world visual data, covering seven visual numerical attributes and four\ntypes of visual numerical estimation tasks. Our experiments on VisNumBench led\nto the following key findings: (i) The 17 MLLMs we tested, including\nopen-source models such as Qwen2.5-VL and InternVL2.5, as well as proprietary\nmodels like GPT-4o and Gemini 2.0 Flash, perform significantly below human\nlevels in number sense-related tasks. (ii) Multimodal mathematical models and\nmultimodal chain-of-thought (CoT) models did not exhibit significant\nimprovements in number sense abilities. (iii) Stronger MLLMs with larger\nparameter sizes and broader general abilities demonstrate modest gains in\nnumber sense abilities. We believe VisNumBench will serve as a valuable\nresource for the research community, encouraging further advancements in\nenhancing MLLMs' number sense abilities. All benchmark resources, including\ncode and datasets, will be publicly available at\nhttps://wwwtttjjj.github.io/VisNumBench/."}
{"id": "2503.14895", "pdf": "https://arxiv.org/pdf/2503.14895", "abs": "https://arxiv.org/abs/2503.14895", "authors": ["Shuo Li", "Jiajun Sun", "Guodong Zheng", "Xiaoran Fan", "Yujiong Shen", "Yi Lu", "Zhiheng Xi", "Yuming Yang", "Wenming Tan", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have demonstrated\nremarkable performance in visual-language tasks. However, the authenticity of\nthe responses generated by MLLMs is often compromised by object hallucinations.\nWe identify that a key cause of these hallucinations is the model's\nover-susceptibility to specific image frequency features in detecting objects.\nIn this paper, we introduce Multi-Frequency Perturbations (MFP), a simple,\ncost-effective, and pluggable method that leverages both low-frequency and\nhigh-frequency features of images to perturb visual feature representations and\nexplicitly suppress redundant frequency-domain features during inference,\nthereby mitigating hallucinations. Experimental results demonstrate that our\nmethod significantly mitigates object hallucinations across various model\narchitectures. Furthermore, as a training-time method, MFP can be combined with\ninference-time methods to achieve state-of-the-art performance on the CHAIR\nbenchmark."}
{"id": "2503.14941", "pdf": "https://arxiv.org/pdf/2503.14941", "abs": "https://arxiv.org/abs/2503.14941", "authors": ["Qihui Zhang", "Munan Ning", "Zheyuan Liu", "Yanbo Wang", "Jiayi Ye", "Yue Huang", "Shuo Yang", "Xiao Chen", "Yibing Song", "Li Yuan"], "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multimodal Large Language Models (MLLMs) have emerged to tackle the\nchallenges of Visual Question Answering (VQA), sparking a new research focus on\nconducting objective evaluations of these models. Existing evaluation methods\nface limitations due to the significant human workload required to design Q&A\npairs for visual images, which inherently restricts the scale and scope of\nevaluations. Although automated MLLM-as-judge approaches attempt to reduce the\nhuman workload through automatic evaluations, they often introduce biases. To\naddress these problems, we propose an Unsupervised Peer review MLLM Evaluation\nframework. It utilizes only image data, allowing models to automatically\ngenerate questions and conduct peer review assessments of answers from other\nmodels, effectively alleviating the reliance on human workload. Additionally,\nwe introduce the vision-language scoring system to mitigate the bias issues,\nwhich focuses on three aspects: (i) response correctness; (ii) visual\nunderstanding and reasoning; and (iii) image-text correlation. Experimental\nresults demonstrate that UPME achieves a Pearson correlation of 0.944 with\nhuman evaluations on the MMstar dataset and 0.814 on the ScienceQA dataset,\nindicating that our framework closely aligns with human-designed benchmarks and\ninherent human preferences."}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["√Älex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC"}
{"id": "2503.14943", "pdf": "https://arxiv.org/pdf/2503.14943", "abs": "https://arxiv.org/abs/2503.14943", "authors": ["Yifan Wang", "Ivan Molodetskikh", "Ondrej Texler", "Dimitar Dinev"], "title": "3D Engine-ready Photorealistic Avatars via Dynamic Textures", "categories": ["cs.CV"], "comment": null, "summary": "As the digital and physical worlds become more intertwined, there has been a\nlot of interest in digital avatars that closely resemble their real-world\ncounterparts. Current digitization methods used in 3D production pipelines\nrequire costly capture setups, making them impractical for mass usage among\ncommon consumers. Recent academic literature has found success in\nreconstructing humans from limited data using implicit representations (e.g.,\nvoxels used in NeRFs), which are able to produce impressive videos. However,\nthese methods are incompatible with traditional rendering pipelines, making it\ndifficult to use them in applications such as games. In this work, we propose\nan end-to-end pipeline that builds explicitly-represented photorealistic 3D\navatars using standard 3D assets. Our key idea is the use of\ndynamically-generated textures to enhance the realism and visually mask\ndeficiencies in the underlying mesh geometry. This allows for seamless\nintegration with current graphics pipelines while achieving comparable visual\nquality to state-of-the-art 3D avatar generation methods."}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."}
{"id": "2503.14944", "pdf": "https://arxiv.org/pdf/2503.14944", "abs": "https://arxiv.org/abs/2503.14944", "authors": ["Zihan Cao", "Yu Zhong", "Ziqi Wang", "Liang-Jian Deng"], "title": "MMAIF: Multi-task and Multi-degradation All-in-One for Image Fusion with Language Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion, a fundamental low-level vision task, aims to integrate multiple\nimage sequences into a single output while preserving as much information as\npossible from the input. However, existing methods face several significant\nlimitations: 1) requiring task- or dataset-specific models; 2) neglecting\nreal-world image degradations (\\textit{e.g.}, noise), which causes failure when\nprocessing degraded inputs; 3) operating in pixel space, where attention\nmechanisms are computationally expensive; and 4) lacking user interaction\ncapabilities. To address these challenges, we propose a unified framework for\nmulti-task, multi-degradation, and language-guided image fusion. Our framework\nincludes two key components: 1) a practical degradation pipeline that simulates\nreal-world image degradations and generates interactive prompts to guide the\nmodel; 2) an all-in-one Diffusion Transformer (DiT) operating in latent space,\nwhich fuses a clean image conditioned on both the degraded inputs and the\ngenerated prompts. Furthermore, we introduce principled modifications to the\noriginal DiT architecture to better suit the fusion task. Based on this\nframework, we develop two versions of the model: Regression-based and Flow\nMatching-based variants. Extensive qualitative and quantitative experiments\ndemonstrate that our approach effectively addresses the aforementioned\nlimitations and outperforms previous restoration+fusion and all-in-one\npipelines. Codes are available at https://github.com/294coder/MMAIF."}
{"id": "2503.15204", "pdf": "https://arxiv.org/pdf/2503.15204", "abs": "https://arxiv.org/abs/2503.15204", "authors": ["Tittaya Mairittha", "Tanakon Sawanglok", "Panuwit Raden", "Sorrawit Treesuk"], "title": "When Pigs Get Sick: Multi-Agent AI for Swine Disease Detection", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.IR", "cs.MA"], "comment": "14 pages, 2 figures", "summary": "Swine disease surveillance is critical to the sustainability of global\nagriculture, yet its effectiveness is frequently undermined by limited\nveterinary resources, delayed identification of cases, and variability in\ndiagnostic accuracy. To overcome these barriers, we introduce a novel\nAI-powered, multi-agent diagnostic system that leverages Retrieval-Augmented\nGeneration (RAG) to deliver timely, evidence-based disease detection and\nclinical guidance. By automatically classifying user inputs into either\nKnowledge Retrieval Queries or Symptom-Based Diagnostic Queries, the system\nensures targeted information retrieval and facilitates precise diagnostic\nreasoning. An adaptive questioning protocol systematically collects relevant\nclinical signs, while a confidence-weighted decision fusion mechanism\nintegrates multiple diagnostic hypotheses to generate robust disease\npredictions and treatment recommendations. Comprehensive evaluations\nencompassing query classification, disease diagnosis, and knowledge retrieval\ndemonstrate that the system achieves high accuracy, rapid response times, and\nconsistent reliability. By providing a scalable, AI-driven diagnostic\nframework, this approach enhances veterinary decision-making, advances\nsustainable livestock management practices, and contributes substantively to\nthe realization of global food security."}
{"id": "2503.14945", "pdf": "https://arxiv.org/pdf/2503.14945", "abs": "https://arxiv.org/abs/2503.14945", "authors": ["Yanhao Wu", "Haoyang Zhang", "Tianwei Lin", "Lichao Huang", "Shujie Luo", "Rui Wu", "Congpei Qiu", "Wei Ke", "Tong Zhang"], "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Generative models in Autonomous Driving (AD) enable diverse scene creation,\nyet existing methods fall short by only capturing a limited range of\nmodalities, restricting the capability of generating controllable scenes for\ncomprehensive evaluation of AD systems. In this paper, we introduce a\nmultimodal generation framework that incorporates four major data modalities,\nincluding a novel addition of map modality. With tokenized modalities, our\nscene sequence generation framework autoregressively predicts each scene while\nmanaging computational demands through a two-stage approach. The Temporal\nAutoRegressive (TAR) component captures inter-frame dynamics for each modality\nwhile the Ordered AutoRegressive (OAR) component aligns modalities within each\nscene by sequentially predicting tokens in a fixed order. To maintain coherence\nbetween map and ego-action modalities, we introduce the Action-aware Map\nAlignment (AMA) module, which applies a transformation based on the ego-action\nto maintain coherence between these modalities. Our framework effectively\ngenerates complex, realistic driving scenes over extended sequences, ensuring\nmultimodal consistency and offering fine-grained control over scene elements."}
{"id": "2503.15338", "pdf": "https://arxiv.org/pdf/2503.15338", "abs": "https://arxiv.org/abs/2503.15338", "authors": ["Junyi Ao", "Dekun Chen", "Xiaohai Tian", "Wenjie Feng", "Jun Zhang", "Lu Lu", "Yuxuan Wang", "Haizhou Li", "Zhizheng Wu"], "title": "Solla: Towards a Speech-Oriented LLM That Hears Acoustic Context", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": null, "summary": "Large Language Models (LLMs) have recently shown remarkable ability to\nprocess not only text but also multimodal inputs such as speech and audio.\nHowever, most existing models primarily focus on analyzing input signals using\ntext instructions, overlooking scenarios in which speech instructions and audio\nare mixed and serve as inputs to the model. To address these challenges, we\nintroduce Solla, a novel framework designed to understand speech-based\nquestions and hear the acoustic context concurrently. Solla incorporates an\naudio tagging module to effectively identify and represent audio events, as\nwell as an ASR-assisted prediction method to improve comprehension of spoken\ncontent. To rigorously evaluate Solla and other publicly available models, we\npropose a new benchmark dataset called SA-Eval, which includes three tasks:\naudio event classification, audio captioning, and audio question answering.\nSA-Eval has diverse speech instruction with various speaking styles,\nencompassing two difficulty levels, easy and hard, to capture the range of\nreal-world acoustic conditions. Experimental results show that Solla performs\non par with or outperforms baseline models on both the easy and hard test sets,\nunderscoring its effectiveness in jointly understanding speech and audio."}
{"id": "2503.14948", "pdf": "https://arxiv.org/pdf/2503.14948", "abs": "https://arxiv.org/abs/2503.14948", "authors": ["Hao Liang", "Zhipeng Dong", "Yi Yang", "Mengyin Fu"], "title": "ChatStitch: Visualizing Through Structures via Surround-View Unsupervised Deep Image Stitching with Collaborative LLM-Agents", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Collaborative perception has garnered significant attention for its ability\nto enhance the perception capabilities of individual vehicles through the\nexchange of information with surrounding vehicle-agents. However, existing\ncollaborative perception systems are limited by inefficiencies in user\ninteraction and the challenge of multi-camera photorealistic visualization. To\naddress these challenges, this paper introduces ChatStitch, the first\ncollaborative perception system capable of unveiling obscured blind spot\ninformation through natural language commands integrated with external digital\nassets. To adeptly handle complex or abstract commands, ChatStitch employs a\nmulti-agent collaborative framework based on Large Language Models. For\nachieving the most intuitive perception for humans, ChatStitch proposes\nSV-UDIS, the first surround-view unsupervised deep image stitching method under\nthe non-global-overlapping condition. We conducted extensive experiments on the\nUDIS-D, MCOV-SLAM open datasets, and our real-world dataset. Specifically, our\nSV-UDIS method achieves state-of-the-art performance on the UDIS-D dataset for\n3, 4, and 5 image stitching tasks, with PSNR improvements of 9%, 17%, and 21%,\nand SSIM improvements of 8%, 18%, and 26%, respectively."}
{"id": "2503.15477", "pdf": "https://arxiv.org/pdf/2503.15477", "abs": "https://arxiv.org/abs/2503.15477", "authors": ["Noam Razin", "Zixuan Wang", "Hubert Strauss", "Stanley Wei", "Jason D. Lee", "Sanjeev Arora"], "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "Code available at https://github.com/princeton-pli/what-makes-good-rm", "summary": "The success of Reinforcement Learning from Human Feedback (RLHF) critically\ndepends on the quality of the reward model. While this quality is primarily\nevaluated through accuracy, it remains unclear whether accuracy fully captures\nwhat makes a reward model an effective teacher. We address this question from\nan optimization perspective. First, we prove that regardless of how accurate a\nreward model is, if it induces low reward variance, then the RLHF objective\nsuffers from a flat landscape. Consequently, even a perfectly accurate reward\nmodel can lead to extremely slow optimization, underperforming less accurate\nmodels that induce higher reward variance. We additionally show that a reward\nmodel that works well for one language model can induce low reward variance,\nand thus a flat objective landscape, for another. These results establish a\nfundamental limitation of evaluating reward models solely based on accuracy or\nindependently of the language model they guide. Experiments using models of up\nto 8B parameters corroborate our theory, demonstrating the interplay between\nreward variance, accuracy, and reward maximization rate. Overall, our findings\nhighlight that beyond accuracy, a reward model needs to induce sufficient\nvariance for efficient optimization."}
{"id": "2503.14950", "pdf": "https://arxiv.org/pdf/2503.14950", "abs": "https://arxiv.org/abs/2503.14950", "authors": ["Joseph Emmanuel DL Dayo", "Prospero C. Naval Jr"], "title": "USAM-Net: A U-Net-based Network for Improved Stereo Correspondence and Scene Depth Estimation using Features from a Pre-trained Image Segmentation network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The increasing demand for high-accuracy depth estimation in autonomous\ndriving and augmented reality applications necessitates advanced neural\narchitectures capable of effectively leveraging multiple data modalities. In\nthis context, we introduce the Unified Segmentation Attention Mechanism Network\n(USAM-Net), a novel convolutional neural network that integrates stereo image\ninputs with semantic segmentation maps and attention to enhance depth\nestimation performance. USAM-Net employs a dual-pathway architecture, which\ncombines a pre-trained segmentation model (SAM) and a depth estimation model.\nThe segmentation pathway preprocesses the stereo images to generate semantic\nmasks, which are then concatenated with the stereo images as inputs to the\ndepth estimation pathway. This integration allows the model to focus on\nimportant features such as object boundaries and surface textures which are\ncrucial for accurate depth perception. Empirical evaluation on the\nDrivingStereo dataset demonstrates that USAM-Net achieves superior performance\nmetrics, including a Global Difference (GD) of 3.61\\% and an End-Point Error\n(EPE) of 0.88, outperforming traditional models such as CFNet, SegStereo, and\niResNet. These results underscore the effectiveness of integrating segmentation\ninformation into stereo depth estimation tasks, highlighting the potential of\nUSAM-Net in applications demanding high-precision depth data."}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io"}
{"id": "2503.14953", "pdf": "https://arxiv.org/pdf/2503.14953", "abs": "https://arxiv.org/abs/2503.14953", "authors": ["Yang Liu", "Wentao Feng", "Zhuoyao Liu", "Shudong Huang", "Jiancheng Lv"], "title": "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching", "categories": ["cs.CV"], "comment": null, "summary": "Enabling Visual Semantic Models to effectively handle multi-view description\nmatching has been a longstanding challenge. Existing methods typically learn a\nset of embeddings to find the optimal match for each view's text and compute\nsimilarity. However, the visual and text embeddings learned through these\napproaches have limited information capacity and are prone to interference from\nlocally similar negative samples. To address this issue, we argue that the\ninformation capacity of embeddings is crucial and propose Dense-to-Sparse\nFeature Distilled Visual Semantic Embedding (D2S-VSE), which enhances the\ninformation capacity of sparse text by leveraging dense text distillation.\nSpecifically, D2S-VSE is a two-stage framework. In the pre-training stage, we\nalign images with dense text to enhance the information capacity of visual\nsemantic embeddings. In the fine-tuning stage, we optimize two tasks\nsimultaneously, distilling dense text embeddings to sparse text embeddings\nwhile aligning images and sparse texts, enhancing the information capacity of\nsparse text embeddings. Our proposed D2S-VSE model is extensively evaluated on\nthe large-scale MS-COCO and Flickr30K datasets, demonstrating its superiority\nover recent state-of-the-art methods."}
{"id": "2503.14955", "pdf": "https://arxiv.org/pdf/2503.14955", "abs": "https://arxiv.org/abs/2503.14955", "authors": ["Bike Chen", "Antti Tikanm√§ki", "Juha R√∂ning"], "title": "Depth-Aware Range Image-Based Model for Point Cloud Segmentation", "categories": ["cs.CV"], "comment": "No Comments", "summary": "Point cloud segmentation (PCS) aims to separate points into different and\nmeaningful groups. The task plays an important role in robotics because PCS\nenables robots to understand their physical environments directly. To process\nsparse and large-scale outdoor point clouds in real time, range image-based\nmodels are commonly adopted. However, in a range image, the lack of explicit\ndepth information inevitably causes some separate objects in 3D space to touch\neach other, bringing difficulty for the range image-based models in correctly\nsegmenting the objects. Moreover, previous PCS models are usually derived from\nthe existing color image-based models and unable to make full use of the\nimplicit but ordered depth information inherent in the range image, thereby\nachieving inferior performance. In this paper, we propose Depth-Aware Module\n(DAM) and Fast FMVNet V3. DAM perceives the ordered depth information in the\nrange image by explicitly modelling the interdependence among channels. Fast\nFMVNet V3 incorporates DAM by integrating it into the last block in each\narchitecture stage. Extensive experiments conducted on SemanticKITTI, nuScenes,\nand SemanticPOSS demonstrate that DAM brings a significant improvement for Fast\nFMVNet V3 with negligible computational cost."}
{"id": "2503.14957", "pdf": "https://arxiv.org/pdf/2503.14957", "abs": "https://arxiv.org/abs/2503.14957", "authors": ["Thanh-Son Nguyen", "Hong Yang", "Tzeh Yuan Neoh", "Hao Zhang", "Ee Yeo Keat", "Basura Fernando"], "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new video question-answering (VQA) dataset that\nchallenges models to leverage procedural knowledge for complex reasoning. It\nrequires recognizing visual entities, generating hypotheses, and performing\ncontextual, causal, and counterfactual reasoning. To address this, we propose\nneuro symbolic reasoning module that integrates neural networks and LLM-driven\nconstrained reasoning over variables for interpretable answer generation.\nResults show that combining LLMs with structured knowledge reasoning with logic\nenhances procedural reasoning on the STAR benchmark and our dataset. Code and\ndataset at https://github.com/LUNAProject22/KML soon."}
{"id": "2503.14958", "pdf": "https://arxiv.org/pdf/2503.14958", "abs": "https://arxiv.org/abs/2503.14958", "authors": ["Zixuan Zheng", "Yilei Shi", "Chunlei Li", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Reducing Annotation Burden: Exploiting Image Knowledge for Few-Shot Medical Video Object Segmentation via Spatiotemporal Consistency Relearning", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Few-shot video object segmentation aims to reduce annotation costs; however,\nexisting methods still require abundant dense frame annotations for training,\nwhich are scarce in the medical domain. We investigate an extremely low-data\nregime that utilizes annotations from only a few video frames and leverages\nexisting labeled images to minimize costly video annotations. Specifically, we\npropose a two-phase framework. First, we learn a few-shot segmentation model\nusing labeled images. Subsequently, to improve performance without full\nsupervision, we introduce a spatiotemporal consistency relearning approach on\nmedical videos that enforces consistency between consecutive frames.\nConstraints are also enforced between the image model and relearning model at\nboth feature and prediction levels. Experiments demonstrate the superiority of\nour approach over state-of-the-art few-shot segmentation methods. Our model\nbridges the gap between abundant annotated medical images and scarce, sparsely\nlabeled medical videos to achieve strong video segmentation performance in this\nlow data regime. Code is available at https://github.com/MedAITech/RAB."}
{"id": "2503.14960", "pdf": "https://arxiv.org/pdf/2503.14960", "abs": "https://arxiv.org/abs/2503.14960", "authors": ["Seungyeon Cho", "Tae-Kyun Kim"], "title": "Body-Hand Modality Expertized Networks with Cross-attention for Fine-grained Skeleton Action Recognition", "categories": ["cs.CV"], "comment": "7 figures, 8 pages", "summary": "Skeleton-based Human Action Recognition (HAR) is a vital technology in\nrobotics and human-robot interaction. However, most existing methods\nconcentrate primarily on full-body movements and often overlook subtle hand\nmotions that are critical for distinguishing fine-grained actions. Recent work\nleverages a unified graph representation that combines body, hand, and foot\nkeypoints to capture detailed body dynamics. Yet, these models often blur fine\nhand details due to the disparity between body and hand action characteristics\nand the loss of subtle features during the spatial-pooling. In this paper, we\npropose BHaRNet (Body-Hand action Recognition Network), a novel framework that\naugments a typical body-expert model with a hand-expert model. Our model\njointly trains both streams with an ensemble loss that fosters cooperative\nspecialization, functioning in a manner reminiscent of a Mixture-of-Experts\n(MoE). Moreover, cross-attention is employed via an expertized branch method\nand a pooling-attention module to enable feature-level interactions and\nselectively fuse complementary information. Inspired by MMNet, we also\ndemonstrate the applicability of our approach to multi-modal tasks by\nleveraging RGB information, where body features guide RGB learning to capture\nricher contextual cues. Experiments on large-scale benchmarks (NTU RGB+D 60,\nNTU RGB+D 120, PKU-MMD, and Northwestern-UCLA) demonstrate that BHaRNet\nachieves SOTA accuracies -- improving from 86.4\\% to 93.0\\% in hand-intensive\nactions -- while maintaining fewer GFLOPs and parameters than the relevant\nunified methods."}
{"id": "2503.14966", "pdf": "https://arxiv.org/pdf/2503.14966", "abs": "https://arxiv.org/abs/2503.14966", "authors": ["Tingxiu Chen", "Yilei Shi", "Zixuan Zheng", "Bingcong Yan", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "title": "Ultrasound Image-to-Video Synthesis via Latent Dynamic Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": "MICCAI 2024", "summary": "Ultrasound video classification enables automated diagnosis and has emerged\nas an important research area. However, publicly available ultrasound video\ndatasets remain scarce, hindering progress in developing effective video\nclassification models. We propose addressing this shortage by synthesizing\nplausible ultrasound videos from readily available, abundant ultrasound images.\nTo this end, we introduce a latent dynamic diffusion model (LDDM) to\nefficiently translate static images to dynamic sequences with realistic video\ncharacteristics. We demonstrate strong quantitative results and visually\nappealing synthesized videos on the BUSV benchmark. Notably, training video\nclassification models on combinations of real and LDDM-synthesized videos\nsubstantially improves performance over using real data alone, indicating our\nmethod successfully emulates dynamics critical for discrimination. Our\nimage-to-video approach provides an effective data augmentation solution to\nadvance ultrasound video analysis. Code is available at\nhttps://github.com/MedAITech/U_I2V."}
{"id": "2503.14974", "pdf": "https://arxiv.org/pdf/2503.14974", "abs": "https://arxiv.org/abs/2503.14974", "authors": ["Yifan Li", "Shuai Yang", "Jiaying Liu"], "title": "Language-based Image Colorization: A Benchmark and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "Image colorization aims to bring colors back to grayscale images. Automatic\nimage colorization methods, which requires no additional guidance, struggle to\ngenerate high-quality images due to color ambiguity, and provides limited user\ncontrollability. Thanks to the emergency of cross-modality datasets and models,\nlanguage-based colorization methods are proposed to fully utilize the\nefficiency and flexibly of text descriptions to guide colorization. In view of\nthe lack of a comprehensive review of language-based colorization literature,\nwe conduct a thorough analysis and benchmarking. We first briefly summarize\nexisting automatic colorization methods. Then, we focus on language-based\nmethods and point out their core challenge on cross-modal alignment. We further\ndivide these methods into two categories: one attempts to train a\ncross-modality network from scratch, while the other utilizes the pre-trained\ncross-modality model to establish the textual-visual correspondence. Based on\nthe analyzed limitations of existing language-based methods, we propose a\nsimple yet effective method based on distilled diffusion model. Extensive\nexperiments demonstrate that our simple baseline can produces better results\nthan previous complex methods with 14 times speed up. To the best of our\nknowledge, this is the first comprehensive review and benchmark on\nlanguage-based image colorization field, providing meaningful insights for the\ncommunity. The code is available at https://github.com/lyf1212/Color-Turbo."}
{"id": "2503.14975", "pdf": "https://arxiv.org/pdf/2503.14975", "abs": "https://arxiv.org/abs/2503.14975", "authors": ["Zihan Cao", "Yu Zhong", "Liang-Jian Deng"], "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution\npanchromatic and multispectral imagery, has garnered significant research\ninterest. Recent advancements employing diffusion models based on stochastic\ndifferential equations (SDEs) have demonstrated state-of-the-art performance.\nHowever, the inherent multi-step sampling process of SDEs imposes substantial\ncomputational overhead, hindering practical deployment. While existing methods\nadopt efficient samplers, knowledge distillation, or retraining to reduce\nsampling steps (e.g., from 1,000 to fewer steps), such approaches often\ncompromise fusion quality. In this work, we propose the Optimal Transport Flow\nMatching (OTFM) framework, which integrates the dual formulation of unbalanced\noptimal transport (UOT) to achieve one-step, high-quality pansharpening. Unlike\nconventional OT formulations that enforce rigid distribution alignment, UOT\nrelaxes marginal constraints to enhance modeling flexibility, accommodating the\nintrinsic spectral and spatial disparities in remote sensing data. Furthermore,\nwe incorporate task-specific regularization into the UOT objective, enhancing\nthe robustness of the flow model. The OTFM framework enables simulation-free\ntraining and single-step inference while maintaining strict adherence to\npansharpening constraints. Experimental evaluations across multiple datasets\ndemonstrate that OTFM matches or exceeds the performance of previous\nregression-based models and leading diffusion-based methods while only needing\none sampling step. Codes are available at https://github.com/294coder/PAN-OTFM."}
{"id": "2503.14979", "pdf": "https://arxiv.org/pdf/2503.14979", "abs": "https://arxiv.org/abs/2503.14979", "authors": ["Yaxiong Chen", "Junjian Hu", "Chunlei Li", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Video object segmentation is crucial for the efficient analysis of complex\nmedical video data, yet it faces significant challenges in data availability\nand annotation. We introduce the task of one-shot medical video object\nsegmentation, which requires separating foreground and background pixels\nthroughout a video given only the mask annotation of the first frame. To\naddress this problem, we propose a temporal contrastive memory network\ncomprising image and mask encoders to learn feature representations, a temporal\ncontrastive memory bank that aligns embeddings from adjacent frames while\npushing apart distant ones to explicitly model inter-frame relationships and\nstores these features, and a decoder that fuses encoded image features and\nmemory readouts for segmentation. We also collect a diverse, multi-source\nmedical video dataset spanning various modalities and anatomies to benchmark\nthis task. Extensive experiments demonstrate state-of-the-art performance in\nsegmenting both seen and unseen structures from a single exemplar, showing\nability to generalize from scarce labels. This highlights the potential to\nalleviate annotation burdens for medical video analysis. Code is available at\nhttps://github.com/MedAITech/TCMN."}
{"id": "2503.14983", "pdf": "https://arxiv.org/pdf/2503.14983", "abs": "https://arxiv.org/abs/2503.14983", "authors": ["Zanting Ye", "Xiaolong Niu", "Xuanbin Wu", "Wenxiang Yi", "Yuan Chang", "Lijun Lu"], "title": "Semi-KAN: KAN Provides an Effective Representation for Semi-Supervised Learning in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "18 pages, 7 figures, 6 tables", "summary": "Deep learning-based medical image segmentation has shown remarkable success;\nhowever, it typically requires extensive pixel-level annotations, which are\nboth expensive and time-intensive. Semi-supervised medical image segmentation\n(SSMIS) offers a viable alternative, driven by advancements in CNNs and ViTs.\nHowever, these networks often rely on single fixed activation functions and\nlinear modeling patterns, limiting their ability to effectively learn robust\nrepresentations. Given the limited availability of labeled date, achieving\nrobust representation learning becomes crucial. Inspired by Kolmogorov-Arnold\nNetworks (KANs), we propose Semi-KAN, which leverages the untapped potential of\nKANs to enhance backbone architectures for representation learning in SSMIS.\nOur findings indicate that: (1) compared to networks with fixed activation\nfunctions, KANs exhibit superior representation learning capabilities with\nfewer parameters, and (2) KANs excel in high-semantic feature spaces. Building\non these insights, we integrate KANs into tokenized intermediate\nrepresentations, applying them selectively at the encoder's bottleneck and the\ndecoder's top layers within a U-Net pipeline to extract high-level semantic\nfeatures. Although learnable activation functions improve feature expansion,\nthey introduce significant computational overhead with only marginal\nperformance gains. To mitigate this, we reduce the feature dimensions and\nemploy horizontal scaling to capture multiple pattern representations.\nFurthermore, we design a multi-branch U-Net architecture with uncertainty\nestimation to effectively learn diverse pattern representations. Extensive\nexperiments on four public datasets demonstrate that Semi-KAN surpasses\nbaseline networks, utilizing fewer KAN layers and lower computational cost,\nthereby underscoring the potential of KANs as a promising approach for SSMIS."}
{"id": "2503.14990", "pdf": "https://arxiv.org/pdf/2503.14990", "abs": "https://arxiv.org/abs/2503.14990", "authors": ["K√©vin Polisano", "Sylvain Meignen", "Nils Laurent", "Hubert Leterme"], "title": "Disentangling Modes and Interference in the Spectrogram of Multicomponent Signals", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "In this paper, we investigate how the spectrogram of multicomponent signals\ncan be decomposed into a mode part and an interference part. We explore two\napproaches: (i) a variational method inspired by texture-geometry decomposition\nin image processing, and (ii) a supervised learning approach using a U-Net\narchitecture, trained on a dataset encompassing diverse interference patterns\nand noise conditions. Once the interference component is identified, we explain\nhow it enables us to define a criterion to locally adapt the window length used\nin the definition of the spectrogram, for the sake of improving ridge detection\nin the presence of close modes. Numerical experiments illustrate the advantages\nand limitations of both approaches for spectrogram decomposition, highlighting\ntheir potential for enhancing time-frequency analysis in the presence of strong\ninterference."}
{"id": "2503.14998", "pdf": "https://arxiv.org/pdf/2503.14998", "abs": "https://arxiv.org/abs/2503.14998", "authors": ["Marta Hasny", "Maxime Di Folco", "Keno Bressem", "Julia Schnabel"], "title": "TGV: Tabular Data-Guided Learning of Visual Cardiac Representations", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning methods in computer vision typically rely on different\nviews of the same image to form pairs. However, in medical imaging, we often\nseek to compare entire patients with different phenotypes rather than just\nmultiple augmentations of one scan. We propose harnessing clinically relevant\ntabular data to identify distinct patient phenotypes and form more meaningful\npairs in a contrastive learning framework. Our method uses tabular attributes\nto guide the training of visual representations, without requiring a joint\nembedding space. We demonstrate its strength using short-axis cardiac MR images\nand clinical attributes from the UK Biobank, where tabular data helps to more\neffectively distinguish between patient subgroups. Evaluation on downstream\ntasks, including fine-tuning and zero-shot prediction of cardiovascular artery\ndiseases and cardiac phenotypes, shows that incorporating tabular data yields\nstronger visual representations than conventional methods that rely solely on\nimage augmentations or combined image-tabular embeddings. Furthermore, we\ndemonstrate that image encoders trained with tabular guidance are capable of\nembedding demographic information in their representations, allowing them to\nuse insights from tabular data for unimodal predictions, making them\nwell-suited to real-world medical settings where extensive clinical annotations\nmay not be routinely available at inference time. The code will be available on\nGitHub."}
{"id": "2503.15001", "pdf": "https://arxiv.org/pdf/2503.15001", "abs": "https://arxiv.org/abs/2503.15001", "authors": ["Michael Neri", "Federica Battisti"], "title": "Low-Complexity Patch-based No-Reference Point Cloud Quality Metric exploiting Weighted Structure and Texture Features", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Accepted for publication in IEEE Transactions on Broadcasting. Code\n  at https://github.com/michaelneri/PST-PCQA", "summary": "During the compression, transmission, and rendering of point clouds, various\nartifacts are introduced, affecting the quality perceived by the end user.\nHowever, evaluating the impact of these distortions on the overall quality is a\nchallenging task. This study introduces PST-PCQA, a no-reference point cloud\nquality metric based on a low-complexity, learning-based framework. It\nevaluates point cloud quality by analyzing individual patches, integrating\nlocal and global features to predict the Mean Opinion Score. In summary, the\nprocess involves extracting features from patches, combining them, and using\ncorrelation weights to predict the overall quality. This approach allows us to\nassess point cloud quality without relying on a reference point cloud, making\nit particularly useful in scenarios where reference data is unavailable.\nExperimental tests on three state-of-the-art datasets show good prediction\ncapabilities of PST-PCQA, through the analysis of different feature pooling\nstrategies and its ability to generalize across different datasets. The\nablation study confirms the benefits of evaluating quality on a patch-by-patch\nbasis. Additionally, PST-PCQA's light-weight structure, with a small number of\nparameters to learn, makes it well-suited for real-time applications and\ndevices with limited computational capacity. For reproducibility purposes, we\nmade code, model, and pretrained weights available at\nhttps://github.com/michaelneri/PST-PCQA."}
{"id": "2503.15004", "pdf": "https://arxiv.org/pdf/2503.15004", "abs": "https://arxiv.org/abs/2503.15004", "authors": ["Annalena Bl√§nsdorf", "Tristan Wirth", "Arne Rak", "Thomas P√∂llabauer", "Volker Knauthe", "Arjan Kuijper"], "title": "Semantic Segmentation of Transparent and Opaque Drinking Glasses with the Help of Zero-shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Segmenting transparent structures in images is challenging since they are\ndifficult to distinguish from the background. Common examples are drinking\nglasses, which are a ubiquitous part of our lives and appear in many different\nshapes and sizes. In this work we propose TransCaGNet, a modified version of\nthe zero-shot model CaGNet. We exchange the segmentation backbone with the\narchitecture of Trans4Trans to be capable of segmenting transparent objects.\nSince some glasses are rarely captured, we use zeroshot learning to be able to\ncreate semantic segmentations of glass categories not given during training. We\npropose a novel synthetic dataset covering a diverse set of different\nenvironmental conditions. Additionally we capture a real-world evaluation\ndataset since most applications take place in the real world. Comparing our\nmodel with Zeg-Clip we are able to show that TransCaGNet produces better mean\nIoU and accuracy values while ZegClip outperforms it mostly for unseen classes.\nTo improve the segmentation results, we combine the semantic segmentation of\nthe models with the segmentation results of SAM 2. Our evaluation emphasizes\nthat distinguishing between different classes is challenging for the models due\nto similarity, points of view, or coverings. Taking this behavior into account,\nwe assign glasses multiple possible categories. The modification leads to an\nimprovement up to 13.68% for the mean IoU and up to 17.88% for the mean\naccuracy values on the synthetic dataset. Using our difficult synthetic dataset\nfor training, the models produce even better results on the real-world dataset.\nThe mean IoU is improved up to 5.55% and the mean accuracy up to 5.72% on the\nreal-world dataset."}
{"id": "2503.15005", "pdf": "https://arxiv.org/pdf/2503.15005", "abs": "https://arxiv.org/abs/2503.15005", "authors": ["Shengqiong Wu", "Hao Fei", "Tat-Seng Chua"], "title": "Universal Scene Graph Generation", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Scene graph (SG) representations can neatly and efficiently describe scene\nsemantics, which has driven sustained intensive research in SG generation. In\nthe real world, multiple modalities often coexist, with different types, such\nas images, text, video, and 3D data, expressing distinct characteristics.\nUnfortunately, current SG research is largely confined to single-modality scene\nmodeling, preventing the full utilization of the complementary strengths of\ndifferent modality SG representations in depicting holistic scene semantics. To\nthis end, we introduce Universal SG (USG), a novel representation capable of\nfully characterizing comprehensive semantic scenes from any given combination\nof modality inputs, encompassing modality-invariant and modality-specific\nscenes. Further, we tailor a niche-targeting USG parser, USG-Par, which\neffectively addresses two key bottlenecks of cross-modal object alignment and\nout-of-domain challenges. We design the USG-Par with modular architecture for\nend-to-end USG generation, in which we devise an object associator to relieve\nthe modality gap for cross-modal object alignment. Further, we propose a\ntext-centric scene contrasting learning mechanism to mitigate domain imbalances\nby aligning multimodal objects and relations with textual SGs. Through\nextensive experiments, we demonstrate that USG offers a stronger capability for\nexpressing scene semantics than standalone SGs, and also that our USG-Par\nachieves higher efficacy and performance."}
{"id": "2503.15016", "pdf": "https://arxiv.org/pdf/2503.15016", "abs": "https://arxiv.org/abs/2503.15016", "authors": ["Fethi Harkat", "Tiphaine Deuberet", "Guillaume Gey", "Val√©rie Perrier", "K√©vin Polisano"], "title": "Manifold Learning for Hyperspectral Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Traditional feature extraction and projection techniques, such as Principal\nComponent Analysis, struggle to adequately represent X-Ray Transmission (XRT)\nMulti-Energy (ME) images, limiting the performance of neural networks in\ndecision-making processes. To address this issue, we propose a method that\napproximates the dataset topology by constructing adjacency graphs using the\nUniform Manifold Approximation and Projection. This approach captures nonlinear\ncorrelations within the data, significantly improving the performance of\nmachine learning algorithms, particularly in processing Hyperspectral Images\n(HSI) from X-ray transmission spectroscopy. This technique not only preserves\nthe global structure of the data but also enhances feature separability,\nleading to more accurate and robust classification results."}
{"id": "2503.15017", "pdf": "https://arxiv.org/pdf/2503.15017", "abs": "https://arxiv.org/abs/2503.15017", "authors": ["Yunwei Lan", "Zhigao Cui", "Chang Liu", "Jialun Peng", "Nian Wang", "Xin Luo", "Dong Liu"], "title": "Exploiting Diffusion Prior for Real-World Image Dehazing with Unpaired Training", "categories": ["cs.CV"], "comment": "Accepted by AAAI2025", "summary": "Unpaired training has been verified as one of the most effective paradigms\nfor real scene dehazing by learning from unpaired real-world hazy and clear\nimages. Although numerous studies have been proposed, current methods\ndemonstrate limited generalization for various real scenes due to limited\nfeature representation and insufficient use of real-world prior. Inspired by\nthe strong generative capabilities of diffusion models in producing both hazy\nand clear images, we exploit diffusion prior for real-world image dehazing, and\npropose an unpaired framework named Diff-Dehazer. Specifically, we leverage\ndiffusion prior as bijective mapping learners within the CycleGAN, a classic\nunpaired learning framework. Considering that physical priors contain pivotal\nstatistics information of real-world data, we further excavate real-world\nknowledge by integrating physical priors into our framework. Furthermore, we\nintroduce a new perspective for adequately leveraging the representation\nability of diffusion models by removing degradation in image and text\nmodalities, so as to improve the dehazing effect. Extensive experiments on\nmultiple real-world datasets demonstrate the superior performance of our\nmethod. Our code https://github.com/ywxjm/Diff-Dehazer."}
{"id": "2503.15019", "pdf": "https://arxiv.org/pdf/2503.15019", "abs": "https://arxiv.org/abs/2503.15019", "authors": ["Shengqiong Wu", "Hao Fei", "Jingkang Yang", "Xiangtai Li", "Juncheng Li", "Hanwang Zhang", "Tat-seng Chua"], "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The latest emerged 4D Panoptic Scene Graph (4D-PSG) provides an advanced-ever\nrepresentation for comprehensively modeling the dynamic 4D visual real world.\nUnfortunately, current pioneering 4D-PSG research can primarily suffer from\ndata scarcity issues severely, as well as the resulting out-of-vocabulary\nproblems; also, the pipeline nature of the benchmark generation method can lead\nto suboptimal performance. To address these challenges, this paper investigates\na novel framework for 4D-PSG generation that leverages rich 2D visual scene\nannotations to enhance 4D scene learning. First, we introduce a 4D Large\nLanguage Model (4D-LLM) integrated with a 3D mask decoder for end-to-end\ngeneration of 4D-PSG. A chained SG inference mechanism is further designed to\nexploit LLMs' open-vocabulary capabilities to infer accurate and comprehensive\nobject and relation labels iteratively. Most importantly, we propose a 2D-to-4D\nvisual scene transfer learning framework, where a spatial-temporal scene\ntranscending strategy effectively transfers dimension-invariant features from\nabundant 2D SG annotations to 4D scenes, effectively compensating for data\nscarcity in 4D-PSG. Extensive experiments on the benchmark data demonstrate\nthat we strikingly outperform baseline models by a large margin, highlighting\nthe effectiveness of our method."}
{"id": "2503.15022", "pdf": "https://arxiv.org/pdf/2503.15022", "abs": "https://arxiv.org/abs/2503.15022", "authors": ["Saad Lahlali", "Sandra Kara", "Hejer Ammar", "Florian Chabot", "Nicolas Granger", "Herv√© Le Borgne", "Quoc-Cuong Pham"], "title": "xMOD: Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Object discovery, which refers to the task of localizing objects without\nhuman annotations, has gained significant attention in 2D image analysis.\nHowever, despite this growing interest, it remains under-explored in 3D data,\nwhere approaches rely exclusively on 3D motion, despite its several challenges.\nIn this paper, we present a novel framework that leverages advances in 2D\nobject discovery which are based on 2D motion to exploit the advantages of such\nmotion cues being more flexible and generalizable and to bridge the gap between\n2D and 3D modalities. Our primary contributions are twofold: (i) we introduce\nDIOD-3D, the first baseline for multi-object discovery in 3D data using 2D\nmotion, incorporating scene completion as an auxiliary task to enable dense\nobject localization from sparse input data; (ii) we develop xMOD, a cross-modal\ntraining framework that integrates 2D and 3D data while always using 2D motion\ncues. xMOD employs a teacher-student training paradigm across the two\nmodalities to mitigate confirmation bias by leveraging the domain gap. During\ninference, the model supports both RGB-only and point cloud-only inputs.\nAdditionally, we propose a late-fusion technique tailored to our pipeline that\nfurther enhances performance when both modalities are available at inference.\nWe evaluate our approach extensively on synthetic (TRIP-PD) and challenging\nreal-world datasets (KITTI and Waymo). Notably, our approach yields a\nsubstantial performance improvement compared with the 2D object discovery\nstate-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50\nscore. The code is available at https://github.com/CEA-LIST/xMOD"}
{"id": "2503.15023", "pdf": "https://arxiv.org/pdf/2503.15023", "abs": "https://arxiv.org/abs/2503.15023", "authors": ["Chaouki Boufenar", "Mehdi Ayoub Rabiai", "Boualem Nadjib Zahaf", "Khelil Rafik Ouaras"], "title": "Bridging the Gap: Fusing CNNs and Transformers to Decode the Elegance of Handwritten Arabic Script", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten Arabic script recognition is a challenging task due to the\nscript's dynamic letter forms and contextual variations. This paper proposes a\nhybrid approach combining convolutional neural networks (CNNs) and\nTransformer-based architectures to address these complexities. We evaluated\ncustom and fine-tuned models, including EfficientNet-B7 and Vision Transformer\n(ViT-B16), and introduced an ensemble model that leverages confidence-based\nfusion to integrate their strengths. Our ensemble achieves remarkable\nperformance on the IFN/ENIT dataset, with 96.38% accuracy for letter\nclassification and 97.22% for positional classification. The results highlight\nthe complementary nature of CNNs and Transformers, demonstrating their combined\npotential for robust Arabic handwriting recognition. This work advances OCR\nsystems, offering a scalable solution for real-world applications."}
{"id": "2503.15024", "pdf": "https://arxiv.org/pdf/2503.15024", "abs": "https://arxiv.org/abs/2503.15024", "authors": ["Jin Wang", "Chenghui Lv", "Xian Li", "Shichao Dong", "Huadong Li", "kelu Yao", "Chao Li", "Wenqi Shao", "Ping Luo"], "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models", "categories": ["cs.CV"], "comment": "31 pages, 19 figures", "summary": "Recently, the rapid development of AIGC has significantly boosted the\ndiversities of fake media spread in the Internet, posing unprecedented threats\nto social security, politics, law, and etc. To detect the ever-increasingly\ndiverse malicious fake media in the new era of AIGC, recent studies have\nproposed to exploit Large Vision Language Models (LVLMs) to design robust\nforgery detectors due to their impressive performance on a wide range of\nmultimodal tasks. However, it still lacks a comprehensive benchmark designed to\ncomprehensively assess LVLMs' discerning capabilities on forgery media. To fill\nthis gap, we present Forensics-Bench, a new forgery detection evaluation\nbenchmark suite to assess LVLMs across massive forgery detection tasks,\nrequiring comprehensive recognition, location and reasoning capabilities on\ndiverse forgeries. Forensics-Bench comprises 63,292 meticulously curated\nmulti-choice visual questions, covering 112 unique forgery detection types from\n5 perspectives: forgery semantics, forgery modalities, forgery tasks, forgery\ntypes and forgery models. We conduct thorough evaluations on 22 open-sourced\nLVLMs and 3 proprietary models GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet,\nhighlighting the significant challenges of comprehensive forgery detection\nposed by Forensics-Bench. We anticipate that Forensics-Bench will motivate the\ncommunity to advance the frontier of LVLMs, striving for all-around forgery\ndetectors in the era of AIGC. The deliverables will be updated at\nhttps://Forensics-Bench.github.io/."}
{"id": "2503.15056", "pdf": "https://arxiv.org/pdf/2503.15056", "abs": "https://arxiv.org/abs/2503.15056", "authors": ["Suhyeon Lee", "Kwanyoung Kim", "Jong Chul Ye"], "title": "Single-Step Bidirectional Unpaired Image Translation Using Implicit Bridge Consistency Distillation", "categories": ["cs.CV"], "comment": "25 pages, 16 figures", "summary": "Unpaired image-to-image translation has seen significant progress since the\nintroduction of CycleGAN. However, methods based on diffusion models or\nSchr\\\"odinger bridges have yet to be widely adopted in real-world applications\ndue to their iterative sampling nature. To address this challenge, we propose a\nnovel framework, Implicit Bridge Consistency Distillation (IBCD), which enables\nsingle-step bidirectional unpaired translation without using adversarial loss.\nIBCD extends consistency distillation by using a diffusion implicit bridge\nmodel that connects PF-ODE trajectories between distributions. Additionally, we\nintroduce two key improvements: 1) distribution matching for consistency\ndistillation and 2) adaptive weighting method based on distillation difficulty.\nExperimental results demonstrate that IBCD achieves state-of-the-art\nperformance on benchmark datasets in a single generation step. Project page\navailable at https://hyn2028.github.io/project_page/IBCD/index.html"}
{"id": "2503.15060", "pdf": "https://arxiv.org/pdf/2503.15060", "abs": "https://arxiv.org/abs/2503.15060", "authors": ["Imanol G. Estepa", "Jes√∫s M. Rodr√≠guez-de-Vera", "Ignacio Saras√∫a", "Bhalaji Nagarajan", "Petia Radeva"], "title": "Conjuring Positive Pairs for Efficient Unification of Representation Learning and Image Synthesis", "categories": ["cs.CV", "cs.AI", "I.5.4; I.5.1; I.2.10"], "comment": "The source code is available in https://github.com/ImaGonEs/Sorcen", "summary": "While representation learning and generative modeling seek to understand\nvisual data, unifying both domains remains unexplored. Recent Unified\nSelf-Supervised Learning (SSL) methods have started to bridge the gap between\nboth paradigms. However, they rely solely on semantic token reconstruction,\nwhich requires an external tokenizer during training -- introducing a\nsignificant overhead. In this work, we introduce Sorcen, a novel unified SSL\nframework, incorporating a synergic Contrastive-Reconstruction objective. Our\nContrastive objective, \"Echo Contrast\", leverages the generative capabilities\nof Sorcen, eliminating the need for additional image crops or augmentations\nduring training. Sorcen \"generates\" an echo sample in the semantic token space,\nforming the contrastive positive pair. Sorcen operates exclusively on\nprecomputed tokens, eliminating the need for an online token transformation\nduring training, thereby significantly reducing computational overhead.\nExtensive experiments on ImageNet-1k demonstrate that Sorcen outperforms the\nprevious Unified SSL SoTA by 0.4%, 1.48 FID, 1.76%, and 1.53% on linear\nprobing, unconditional image generation, few-shot learning, and transfer\nlearning, respectively, while being 60.8% more efficient. Additionally, Sorcen\nsurpasses previous single-crop MIM SoTA in linear probing and achieves SoTA\nperformance in unconditional image generation, highlighting significant\nimprovements and breakthroughs in Unified SSL models."}
{"id": "2503.15070", "pdf": "https://arxiv.org/pdf/2503.15070", "abs": "https://arxiv.org/abs/2503.15070", "authors": ["Kana Kurata", "Hitoshi Niigaki", "Xiaojun Wu", "Ryuichi Tanida"], "title": "MultiBARF: Integrating Imagery of Different Wavelength Regions by Using Neural Radiance Fields", "categories": ["cs.CV"], "comment": null, "summary": "Optical sensor applications have become popular through digital\ntransformation. Linking observed data to real-world locations and combining\ndifferent image sensors is essential to make the applications practical and\nefficient. However, data preparation to try different sensor combinations\nrequires high sensing and image processing expertise. To make data preparation\neasier for users unfamiliar with sensing and image processing, we have\ndeveloped MultiBARF. This method replaces the co-registration and geometric\ncalibration by synthesizing pairs of two different sensor images and depth\nimages at assigned viewpoints. Our method extends Bundle Adjusting Neural\nRadiance Fields(BARF), a deep neural network-based novel view synthesis method,\nfor the two imagers. Through experiments on visible light and thermographic\nimages, we demonstrate that our method superimposes two color channels of those\nsensor images on NeRF."}
{"id": "2503.15087", "pdf": "https://arxiv.org/pdf/2503.15087", "abs": "https://arxiv.org/abs/2503.15087", "authors": ["Christoph Griesbacher", "Christian Fruhwirth-Reisinger"], "title": "An Investigation of Beam Density on LiDAR Object Detection Performance", "categories": ["cs.CV"], "comment": "Accepted by CVWW 2025", "summary": "Accurate 3D object detection is a critical component of autonomous driving,\nenabling vehicles to perceive their surroundings with precision and make\ninformed decisions. LiDAR sensors, widely used for their ability to provide\ndetailed 3D measurements, are key to achieving this capability. However,\nvariations between training and inference data can cause significant\nperformance drops when object detection models are employed in different sensor\nsettings. One critical factor is beam density, as inference on sparse,\ncost-effective LiDAR sensors is often preferred in real-world applications.\nDespite previous work addressing the beam-density-induced domain gap,\nsubstantial knowledge gaps remain, particularly concerning dense 128-beam\nsensors in cross-domain scenarios. To gain better understanding of the impact\nof beam density on domain gaps, we conduct a comprehensive investigation that\nincludes an evaluation of different object detection architectures. Our\narchitecture evaluation reveals that combining voxel- and point-based\napproaches yields superior cross-domain performance by leveraging the strengths\nof both representations. Building on these findings, we analyze\nbeam-density-induced domain gaps and argue that these domain gaps must be\nevaluated in conjunction with other domain shifts. Contrary to conventional\nbeliefs, our experiments reveal that detectors benefit from training on denser\ndata and exhibit robustness to beam density variations during inference."}
{"id": "2503.15096", "pdf": "https://arxiv.org/pdf/2503.15096", "abs": "https://arxiv.org/abs/2503.15096", "authors": ["Yang Liu", "Qianqian Xu", "Peisong Wen", "Siran Dai", "Qingming Huang"], "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "The past decade has witnessed notable achievements in self-supervised\nlearning for video tasks. Recent efforts typically adopt the Masked Video\nModeling (MVM) paradigm, leading to significant progress on multiple video\ntasks. However, two critical challenges remain: 1) Without human annotations,\nthe random temporal sampling introduces uncertainty, increasing the difficulty\nof model training. 2) Previous MVM methods primarily recover the masked patches\nin the pixel space, leading to insufficient information compression for\ndownstream tasks. To address these challenges jointly, we propose a\nself-supervised framework that leverages Temporal Correspondence for video\nRepresentation learning (T-CoRe). For challenge 1), we propose a sandwich\nsampling strategy that selects two auxiliary frames to reduce reconstruction\nuncertainty in a two-side-squeezing manner. Addressing challenge 2), we\nintroduce an auxiliary branch into a self-distillation architecture to restore\nrepresentations in the latent space, generating high-level semantic\nrepresentations enriched with temporal information. Experiments of T-CoRe\nconsistently present superior performance across several downstream tasks,\ndemonstrating its effectiveness for video representation learning. The code is\navailable at https://github.com/yafeng19/T-CORE."}
{"id": "2503.15106", "pdf": "https://arxiv.org/pdf/2503.15106", "abs": "https://arxiv.org/abs/2503.15106", "authors": ["Amir Hamza", "Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Distilling 3D distinctive local descriptors for 6D pose estimation", "categories": ["cs.CV"], "comment": "Project Website: https://tev-fbk.github.io/dGeDi/", "summary": "Three-dimensional local descriptors are crucial for encoding geometric\nsurface properties, making them essential for various point cloud understanding\ntasks. Among these descriptors, GeDi has demonstrated strong zero-shot 6D pose\nestimation capabilities but remains computationally impractical for real-world\napplications due to its expensive inference process. \\textit{Can we retain\nGeDi's effectiveness while significantly improving its efficiency?} In this\npaper, we explore this question by introducing a knowledge distillation\nframework that trains an efficient student model to regress local descriptors\nfrom a GeDi teacher. Our key contributions include: an efficient large-scale\ntraining procedure that ensures robustness to occlusions and partial\nobservations while operating under compute and storage constraints, and a novel\nloss formulation that handles weak supervision from non-distinctive teacher\ndescriptors. We validate our approach on five BOP Benchmark datasets and\ndemonstrate a significant reduction in inference time while maintaining\ncompetitive performance with existing methods, bringing zero-shot 6D pose\nestimation closer to real-time feasibility. Project Website:\nhttps://tev-fbk.github.io/dGeDi/"}
{"id": "2503.15110", "pdf": "https://arxiv.org/pdf/2503.15110", "abs": "https://arxiv.org/abs/2503.15110", "authors": ["Zinqin Huang", "Gu Wang", "Chenyangguang Zhang", "Ruida Zhang", "Xiu Li", "Xiangyang Ji"], "title": "GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recent advances in RGBD-based category-level object pose estimation have been\nlimited by their reliance on precise depth information, restricting their\nbroader applicability. In response, RGB-based methods have been developed.\nAmong these methods, geometry-guided pose regression that originated from\ninstance-level tasks has demonstrated strong performance. However, we argue\nthat the NOCS map is an inadequate intermediate representation for\ngeometry-guided pose regression method, as its many-to-one correspondence with\ncategory-level pose introduces redundant instance-specific information,\nresulting in suboptimal results. This paper identifies the intra-class\nvariation problem inherent in pose regression based solely on the NOCS map and\nproposes the Intra-class Variation-Free Consensus (IVFC) map, a novel\ncoordinate representation generated from the category-level consensus model. By\nleveraging the complementary strengths of the NOCS map and the IVFC map, we\nintroduce GIVEPose, a framework that implements Gradual Intra-class Variation\nElimination for category-level object pose estimation. Extensive evaluations on\nboth synthetic and real-world datasets demonstrate that GIVEPose significantly\noutperforms existing state-of-the-art RGB-based approaches, achieving\nsubstantial improvements in category-level object pose estimation. Our code is\navailable at https://github.com/ziqin-h/GIVEPose."}
{"id": "2503.15126", "pdf": "https://arxiv.org/pdf/2503.15126", "abs": "https://arxiv.org/abs/2503.15126", "authors": ["Haoyu Ji", "Bowen Chen", "Weihong Ren", "Wenze Huang", "Zhihao Yang", "Zhiyong Wang", "Honghai Liu"], "title": "Text-Derived Relational Graph-Enhanced Network for Skeleton-Based Action Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Skeleton-based Temporal Action Segmentation (STAS) aims to segment and\nrecognize various actions from long, untrimmed sequences of human skeletal\nmovements. Current STAS methods typically employ spatio-temporal modeling to\nestablish dependencies among joints as well as frames, and utilize one-hot\nencoding with cross-entropy loss for frame-wise classification supervision.\nHowever, these methods overlook the intrinsic correlations among joints and\nactions within skeletal features, leading to a limited understanding of human\nmovements. To address this, we propose a Text-Derived Relational Graph-Enhanced\nNetwork (TRG-Net) that leverages prior graphs generated by Large Language\nModels (LLM) to enhance both modeling and supervision. For modeling, the\nDynamic Spatio-Temporal Fusion Modeling (DSFM) method incorporates Text-Derived\nJoint Graphs (TJG) with channel- and frame-level dynamic adaptation to\neffectively model spatial relations, while integrating spatio-temporal core\nfeatures during temporal modeling. For supervision, the Absolute-Relative\nInter-Class Supervision (ARIS) method employs contrastive learning between\naction features and text embeddings to regularize the absolute class\ndistributions, and utilizes Text-Derived Action Graphs (TAG) to capture the\nrelative inter-class relationships among action features. Additionally, we\npropose a Spatial-Aware Enhancement Processing (SAEP) method, which\nincorporates random joint occlusion and axial rotation to enhance spatial\ngeneralization. Performance evaluations on four public datasets demonstrate\nthat TRG-Net achieves state-of-the-art results."}
{"id": "2503.15138", "pdf": "https://arxiv.org/pdf/2503.15138", "abs": "https://arxiv.org/abs/2503.15138", "authors": ["Mingzhe Zheng", "Yongqi Xu", "Haojian Huang", "Xuran Ma", "Yexin Liu", "Wenjie Shu", "Yatian Pang", "Feilong Tang", "Qifeng Chen", "Harry Yang", "Ser-Nam Lim"], "title": "VideoGen-of-Thought: Step-by-step generating multi-shot video with minimal manual intervention", "categories": ["cs.CV"], "comment": "Code: https://github.com/DuNGEOnmassster/VideoGen-of-Thought.git;\n  Webpage: https://cheliosoops.github.io/VGoT/", "summary": "Current video generation models excel at short clips but fail to produce\ncohesive multi-shot narratives due to disjointed visual dynamics and fractured\nstorylines. Existing solutions either rely on extensive manual\nscripting/editing or prioritize single-shot fidelity over cross-scene\ncontinuity, limiting their practicality for movie-like content. We introduce\nVideoGen-of-Thought (VGoT), a step-by-step framework that automates multi-shot\nvideo synthesis from a single sentence by systematically addressing three core\nchallenges: (1) Narrative Fragmentation: Existing methods lack structured\nstorytelling. We propose dynamic storyline modeling, which first converts the\nuser prompt into concise shot descriptions, then elaborates them into detailed,\ncinematic specifications across five domains (character dynamics, background\ncontinuity, relationship evolution, camera movements, HDR lighting), ensuring\nlogical narrative progression with self-validation. (2) Visual Inconsistency:\nExisting approaches struggle with maintaining visual consistency across shots.\nOur identity-aware cross-shot propagation generates identity-preserving\nportrait (IPP) tokens that maintain character fidelity while allowing trait\nvariations (expressions, aging) dictated by the storyline. (3) Transition\nArtifacts: Abrupt shot changes disrupt immersion. Our adjacent latent\ntransition mechanisms implement boundary-aware reset strategies that process\nadjacent shots' features at transition points, enabling seamless visual flow\nwhile preserving narrative continuity. VGoT generates multi-shot videos that\noutperform state-of-the-art baselines by 20.4% in within-shot face consistency\nand 17.4% in style consistency, while achieving over 100% better cross-shot\nconsistency and 10x fewer manual adjustments than alternatives."}
{"id": "2503.15141", "pdf": "https://arxiv.org/pdf/2503.15141", "abs": "https://arxiv.org/abs/2503.15141", "authors": ["Nikola ƒêukiƒá", "Tim Lebailly", "Tinne Tuytelaars"], "title": "Object-Centric Pretraining via Target Encoder Bootstrapping", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Object-centric representation learning has recently been successfully applied\nto real-world datasets. This success can be attributed to pretrained\nnon-object-centric foundation models, whose features serve as reconstruction\ntargets for slot attention. However, targets must remain frozen throughout the\ntraining, which sets an upper bound on the performance object-centric models\ncan attain. Attempts to update the target encoder by bootstrapping result in\nlarge performance drops, which can be attributed to its lack of object-centric\ninductive biases, causing the object-centric model's encoder to drift away from\nrepresentations useful as reconstruction targets. To address these limitations,\nwe propose Object-CEntric Pretraining by Target Encoder BOotstrapping, a\nself-distillation setup for training object-centric models from scratch, on\nreal-world data, for the first time ever. In OCEBO, the target encoder is\nupdated as an exponential moving average of the object-centric model, thus\nexplicitly being enriched with object-centric inductive biases introduced by\nslot attention while removing the upper bound on performance present in other\nmodels. We mitigate the slot collapse caused by random initialization of the\ntarget encoder by introducing a novel cross-view patch filtering approach that\nlimits the supervision to sufficiently informative patches. When pretrained on\n241k images from COCO, OCEBO achieves unsupervised object discovery performance\ncomparable to that of object-centric models with frozen non-object-centric\ntarget encoders pretrained on hundreds of millions of images. The code and\npretrained models are publicly available at https://github.com/djukicn/ocebo."}
{"id": "2503.15144", "pdf": "https://arxiv.org/pdf/2503.15144", "abs": "https://arxiv.org/abs/2503.15144", "authors": ["Xing He", "Zhe Zhu", "Liangliang Nan", "Honghua Chen", "Jing Qin", "Mingqiang Wei"], "title": "PointSFDA: Source-free Domain Adaptation for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Conventional methods for point cloud completion, typically trained on\nsynthetic datasets, face significant challenges when applied to\nout-of-distribution real-world scans. In this paper, we propose an effective\nyet simple source-free domain adaptation framework for point cloud completion,\ntermed \\textbf{PointSFDA}. Unlike unsupervised domain adaptation that reduces\nthe domain gap by directly leveraging labeled source data, PointSFDA uses only\na pretrained source model and unlabeled target data for adaptation, avoiding\nthe need for inaccessible source data in practical scenarios. Being the first\nsource-free domain adaptation architecture for point cloud completion, our\nmethod offers two core contributions. First, we introduce a coarse-to-fine\ndistillation solution to explicitly transfer the global geometry knowledge\nlearned from the source dataset. Second, as noise may be introduced due to\ndomain gaps, we propose a self-supervised partial-mask consistency training\nstrategy to learn local geometry information in the target domain. Extensive\nexperiments have validated that our method significantly improves the\nperformance of state-of-the-art networks in cross-domain shape completion. Our\ncode is available at\n\\emph{\\textcolor{magenta}{https://github.com/Starak-x/PointSFDA}}."}
{"id": "2503.15156", "pdf": "https://arxiv.org/pdf/2503.15156", "abs": "https://arxiv.org/abs/2503.15156", "authors": ["Joost Luijmes", "Alexander Gielisse", "Roman Knyazhitskiy", "Jan van Gemert"], "title": "ARC: Anchored Representation Clouds for High-Resolution INR Classification", "categories": ["cs.CV"], "comment": "Accepted at the ICLR 2025 Workshop on Neural Network Weights as a New\n  Data Modality", "summary": "Implicit neural representations (INRs) encode signals in neural network\nweights as a memory-efficient representation, decoupling sampling resolution\nfrom the associated resource costs. Current INR image classification methods\nare demonstrated on low-resolution data and are sensitive to image-space\ntransformations. We attribute these issues to the global, fully-connected MLP\nneural network architecture encoding of current INRs, which lack mechanisms for\nlocal representation: MLPs are sensitive to absolute image location and\nstruggle with high-frequency details. We propose ARC: Anchored Representation\nClouds, a novel INR architecture that explicitly anchors latent vectors locally\nin image-space. By introducing spatial structure to the latent vectors, ARC\ncaptures local image data which in our testing leads to state-of-the-art\nimplicit image classification of both low- and high-resolution images and\nincreased robustness against image-space translation. Code can be found at\nhttps://github.com/JLuij/anchored_representation_clouds."}
{"id": "2503.15161", "pdf": "https://arxiv.org/pdf/2503.15161", "abs": "https://arxiv.org/abs/2503.15161", "authors": ["Yang Li", "Soumya Snigdha Kundu", "Maxence Boels", "Toktam Mahmoodi", "Sebastien Ourselin", "Tom Vercauteren", "Prokar Dasgupta", "Jonathan Shapey", "Alejandro Granados"], "title": "UltraFlwr -- An Efficient Federated Medical and Surgical Object Detection Framework", "categories": ["cs.CV"], "comment": "10 pages, 2 figures, under review @ MICCAI", "summary": "Object detection shows promise for medical and surgical applications such as\ncell counting and tool tracking. However, its faces multiple real-world edge\ndeployment challenges including limited high-quality annotated data, data\nsharing restrictions, and computational constraints. In this work, we introduce\nUltraFlwr, a framework for federated medical and surgical object detection. By\nleveraging Federated Learning (FL), UltraFlwr enables decentralized model\ntraining across multiple sites without sharing raw data. To further enhance\nUltraFlwr's efficiency, we propose YOLO-PA, a set of novel Partial Aggregation\n(PA) strategies specifically designed for YOLO models in FL. YOLO-PA\nsignificantly reduces communication overhead by up to 83% per round while\nmaintaining performance comparable to Full Aggregation (FA) strategies. Our\nextensive experiments on BCCD and m2cai16-tool-locations datasets demonstrate\nthat YOLO-PA not only provides better client models compared to client-wise\ncentralized training and FA strategies, but also facilitates efficient training\nand deployment across resource-constrained edge devices. Further, we also\nestablish one of the first benchmarks in federated medical and surgical object\ndetection. This paper advances the feasibility of training and deploying\ndetection models on the edge, making federated object detection more practical\nfor time-critical and resource-constrained medical and surgical applications.\nUltraFlwr is publicly available at https://github.com/KCL-BMEIS/UltraFlwr."}
{"id": "2503.15166", "pdf": "https://arxiv.org/pdf/2503.15166", "abs": "https://arxiv.org/abs/2503.15166", "authors": ["√Älex Pujol Vidal", "Sergio Escalera", "Kamal Nasrollahi", "Thomas B. Moeslund"], "title": "Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive Learning: Adapting Alignment Calibration to MERU", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Preprint", "summary": "Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC"}
{"id": "2503.15185", "pdf": "https://arxiv.org/pdf/2503.15185", "abs": "https://arxiv.org/abs/2503.15185", "authors": ["Gyeongrok Oh", "Sungjune Kim", "Heeju Ko", "Hyung-gun Chi", "Jinkyu Kim", "Dongwook Lee", "Daehyun Ji", "Sungjoon Choi", "Sujin Jang", "Sangpil Kim"], "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR2025", "summary": "The resolution of voxel queries significantly influences the quality of view\ntransformation in camera-based 3D occupancy prediction. However, computational\nconstraints and the practical necessity for real-time deployment require\nsmaller query resolutions, which inevitably leads to an information loss.\nTherefore, it is essential to encode and preserve rich visual details within\nlimited query sizes while ensuring a comprehensive representation of 3D\noccupancy. To this end, we introduce ProtoOcc, a novel occupancy network that\nleverages prototypes of clustered image segments in view transformation to\nenhance low-resolution context. In particular, the mapping of 2D prototypes\nonto 3D voxel queries encodes high-level visual geometries and complements the\nloss of spatial information from reduced query resolutions. Additionally, we\ndesign a multi-perspective decoding strategy to efficiently disentangle the\ndensely compressed visual cues into a high-dimensional 3D occupancy scene.\nExperimental results on both Occ3D and SemanticKITTI benchmarks demonstrate the\neffectiveness of the proposed method, showing clear improvements over the\nbaselines. More importantly, ProtoOcc achieves competitive performance against\nthe baselines even with 75\\% reduced voxel resolution."}
{"id": "2503.15195", "pdf": "https://arxiv.org/pdf/2503.15195", "abs": "https://arxiv.org/abs/2503.15195", "authors": ["Giorgia Crosilla", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Large Language Models for Handwritten Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traditional machine learning models for Handwritten Text Recognition (HTR)\nrely on supervised training, requiring extensive manual annotations, and often\nproduce errors due to the separation between layout and text processing. In\ncontrast, Multimodal Large Language Models (MLLMs) offer a general approach to\nrecognizing diverse handwriting styles without the need for model-specific\ntraining. The study benchmarks various proprietary and open-source LLMs against\nTranskribus models, evaluating their performance on both modern and historical\ndatasets written in English, French, German, and Italian. In addition, emphasis\nis placed on testing the models' ability to autonomously correct previously\ngenerated outputs. Findings indicate that proprietary models, especially Claude\n3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs\nachieve excellent results in recognizing modern handwriting and exhibit a\npreference for the English language due to their pre-training dataset\ncomposition. Comparisons with Transkribus show no consistent advantage for\neither approach. Moreover, LLMs demonstrate limited ability to autonomously\ncorrect errors in zero-shot transcriptions."}
{"id": "2503.15197", "pdf": "https://arxiv.org/pdf/2503.15197", "abs": "https://arxiv.org/abs/2503.15197", "authors": ["Feifei Li", "Mi Zhang", "Yiming Sun", "Min Yang"], "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization", "categories": ["cs.CV"], "comment": "CVPR25", "summary": "Text-to-image diffusion models have achieved state-of-the-art results in\nsynthesis tasks; however, there is a growing concern about their potential\nmisuse in creating harmful content. To mitigate these risks, post-hoc model\nintervention techniques, such as concept unlearning and safety guidance, have\nbeen developed. However, fine-tuning model weights or adapting the hidden\nstates of the diffusion model operates in an uninterpretable way, making it\nunclear which part of the intermediate variables is responsible for unsafe\ngeneration. These interventions severely affect the sampling trajectory when\nerasing harmful concepts from complex, multi-concept prompts, thus hindering\ntheir practical use in real-world settings. In this work, we propose the safe\ngeneration framework Detect-and-Guide (DAG), leveraging the internal knowledge\nof diffusion models to perform self-diagnosis and fine-grained self-regulation\nduring the sampling process. DAG first detects harmful concepts from noisy\nlatents using refined cross-attention maps of optimized tokens, then applies\nsafety guidance with adaptive strength and editing regions to negate unsafe\ngeneration. The optimization only requires a small annotated dataset and can\nprovide precise detection maps with generalizability and concept specificity.\nMoreover, DAG does not require fine-tuning of diffusion models, and therefore\nintroduces no loss to their generation diversity. Experiments on erasing sexual\ncontent show that DAG achieves state-of-the-art safe generation performance,\nbalancing harmfulness mitigation and text-following performance on\nmulti-concept real-world prompts."}
{"id": "2503.15208", "pdf": "https://arxiv.org/pdf/2503.15208", "abs": "https://arxiv.org/abs/2503.15208", "authors": ["Jiazhe Guo", "Yikang Ding", "Xiwu Chen", "Shuo Chen", "Bohan Li", "Yingshuang Zou", "Xiaoyang Lyu", "Feiyang Tan", "Xiaojuan Qi", "Zhiheng Li", "Hao Zhao"], "title": "DiST-4D: Disentangled Spatiotemporal Diffusion with Metric Depth for 4D Driving Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current generative models struggle to synthesize dynamic 4D driving scenes\nthat simultaneously support temporal extrapolation and spatial novel view\nsynthesis (NVS) without per-scene optimization. A key challenge lies in finding\nan efficient and generalizable geometric representation that seamlessly\nconnects temporal and spatial synthesis. To address this, we propose DiST-4D,\nthe first disentangled spatiotemporal diffusion framework for 4D driving scene\ngeneration, which leverages metric depth as the core geometric representation.\nDiST-4D decomposes the problem into two diffusion processes: DiST-T, which\npredicts future metric depth and multi-view RGB sequences directly from past\nobservations, and DiST-S, which enables spatial NVS by training only on\nexisting viewpoints while enforcing cycle consistency. This cycle consistency\nmechanism introduces a forward-backward rendering constraint, reducing the\ngeneralization gap between observed and unseen viewpoints. Metric depth is\nessential for both accurate reliable forecasting and accurate spatial NVS, as\nit provides a view-consistent geometric representation that generalizes well to\nunseen perspectives. Experiments demonstrate that DiST-4D achieves\nstate-of-the-art performance in both temporal prediction and NVS tasks, while\nalso delivering competitive performance in planning-related evaluations."}
{"id": "2503.15211", "pdf": "https://arxiv.org/pdf/2503.15211", "abs": "https://arxiv.org/abs/2503.15211", "authors": ["Zechuan Li", "Hongshan Yu", "Yihao Ding", "Jinhao Qiao", "Basim Azam", "Naveed Akhtar"], "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "We propose GO-N3RDet, a scene-geometry optimized multi-view 3D object\ndetector enhanced by neural radiance fields. The key to accurate 3D object\ndetection is in effective voxel representation. However, due to occlusion and\nlack of 3D information, constructing 3D features from multi-view 2D images is\nchallenging. Addressing that, we introduce a unique 3D positional information\nembedded voxel optimization mechanism to fuse multi-view features. To\nprioritize neural field reconstruction in object regions, we also devise a\ndouble importance sampling scheme for the NeRF branch of our detector. We\nadditionally propose an opacity optimization module for precise voxel opacity\nprediction by enforcing multi-view consistency constraints. Moreover, to\nfurther improve voxel density consistency across multiple perspectives, we\nincorporate ray distance as a weighting factor to minimize cumulative ray\nerrors. Our unique modules synergetically form an end-to-end neural model that\nestablishes new state-of-the-art in NeRF-based multi-view 3D detection,\nverified with extensive experiments on ScanNet and ARKITScenes. Code will be\navailable at https://github.com/ZechuanLi/GO-N3RDet."}
{"id": "2503.15234", "pdf": "https://arxiv.org/pdf/2503.15234", "abs": "https://arxiv.org/abs/2503.15234", "authors": ["Wenlong Yu", "Qilong Wang", "Chuang Liu", "Dong Li", "Qinghua Hu"], "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by CVPR2025", "summary": "Explainability is a critical factor influencing the wide deployment of deep\nvision models (DVMs). Concept-based post-hoc explanation methods can provide\nboth global and local insights into model decisions. However, current methods\nin this field face challenges in that they are inflexible to automatically\nconstruct accurate and sufficient linguistic explanations for global concepts\nand local circuits. Particularly, the intrinsic polysemanticity in semantic\nVisual Concepts (VCs) impedes the interpretability of concepts and DVMs, which\nis underestimated severely. In this paper, we propose a Chain-of-Explanation\n(CoE) approach to address these issues. Specifically, CoE automates the\ndecoding and description of VCs to construct global concept explanation\ndatasets. Further, to alleviate the effect of polysemanticity on model\nexplainability, we design a concept polysemanticity disentanglement and\nfiltering mechanism to distinguish the most contextually relevant concept\natoms. Besides, a Concept Polysemanticity Entropy (CPE), as a measure of model\ninterpretability, is formulated to quantify the degree of concept uncertainty.\nThe modeling of deterministic concepts is upgraded to uncertain concept atom\ndistributions. Finally, CoE automatically enables linguistic local explanations\nof the decision-making process of DVMs by tracing the concept circuit. GPT-4o\nand human-based experiments demonstrate the effectiveness of CPE and the\nsuperiority of CoE, achieving an average absolute improvement of 36% in terms\nof explainability scores."}
{"id": "2503.15260", "pdf": "https://arxiv.org/pdf/2503.15260", "abs": "https://arxiv.org/abs/2503.15260", "authors": ["Lei Shi", "Xi Fang", "Naiyu Wang", "Junxing Zhang"], "title": "DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Automatic medical image segmentation plays a crucial role in computer aided\ndiagnosis. However, fully supervised learning approaches often require\nextensive and labor-intensive annotation efforts. To address this challenge,\nweakly supervised learning methods, particularly those using extreme points as\nsupervisory signals, have the potential to offer an effective solution. In this\npaper, we introduce Deep Extreme Point Tracing (DEPT) integrated with\nFeature-Guided Extreme Point Masking (FGEPM) algorithm for ultrasound image\nsegmentation. Notably, our method generates pseudo labels by identifying the\nlowest-cost path that connects all extreme points on the feature map-based cost\nmatrix. Additionally, an iterative training strategy is proposed to refine\npseudo labels progressively, enabling continuous network improvement.\nExperimental results on two public datasets demonstrate the effectiveness of\nour proposed method. The performance of our method approaches that of the fully\nsupervised method and outperforms several existing weakly supervised methods."}
{"id": "2503.15264", "pdf": "https://arxiv.org/pdf/2503.15264", "abs": "https://arxiv.org/abs/2503.15264", "authors": ["Hengrui Kang", "Siwei Wen", "Zichen Wen", "Junyan Ye", "Weijia Li", "Peilin Feng", "Baichuan Zhou", "Bin Wang", "Dahua Lin", "Linfeng Zhang", "Conghui He"], "title": "LEGION: Learning to Ground and Explain for Synthetic Image Detection", "categories": ["cs.CV"], "comment": "Project Page: https://opendatalab.github.io/LEGION", "summary": "The rapid advancements in generative technology have emerged as a\ndouble-edged sword. While offering powerful tools that enhance convenience,\nthey also pose significant social concerns. As defenders, current synthetic\nimage detection methods often lack artifact-level textual interpretability and\nare overly focused on image manipulation detection, and current datasets\nusually suffer from outdated generators and a lack of fine-grained annotations.\nIn this paper, we introduce SynthScars, a high-quality and diverse dataset\nconsisting of 12,236 fully synthetic images with human-expert annotations. It\nfeatures 4 distinct image content types, 3 categories of artifacts, and\nfine-grained annotations covering pixel-level segmentation, detailed textual\nexplanations, and artifact category labels. Furthermore, we propose LEGION\n(LEarning to Ground and explain for Synthetic Image detectiON), a multimodal\nlarge language model (MLLM)-based image forgery analysis framework that\nintegrates artifact detection, segmentation, and explanation. Building upon\nthis capability, we further explore LEGION as a controller, integrating it into\nimage refinement pipelines to guide the generation of higher-quality and more\nrealistic images. Extensive experiments show that LEGION outperforms existing\nmethods across multiple benchmarks, particularly surpassing the second-best\ntraditional expert on SynthScars by 3.31% in mIoU and 7.75% in F1 score.\nMoreover, the refined images generated under its guidance exhibit stronger\nalignment with human preferences. The code, model, and dataset will be\nreleased."}
{"id": "2503.15265", "pdf": "https://arxiv.org/pdf/2503.15265", "abs": "https://arxiv.org/abs/2503.15265", "authors": ["Ruowen Zhao", "Junliang Ye", "Zhengyi Wang", "Guangce Liu", "Yiwen Chen", "Yikai Wang", "Jun Zhu"], "title": "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning", "categories": ["cs.CV"], "comment": "Project page: https://zhaorw02.github.io/DeepMesh/", "summary": "Triangle meshes play a crucial role in 3D applications for efficient\nmanipulation and rendering. While auto-regressive methods generate structured\nmeshes by predicting discrete vertex tokens, they are often constrained by\nlimited face counts and mesh incompleteness. To address these challenges, we\npropose DeepMesh, a framework that optimizes mesh generation through two key\ninnovations: (1) an efficient pre-training strategy incorporating a novel\ntokenization algorithm, along with improvements in data curation and\nprocessing, and (2) the introduction of Reinforcement Learning (RL) into 3D\nmesh generation to achieve human preference alignment via Direct Preference\nOptimization (DPO). We design a scoring standard that combines human evaluation\nwith 3D metrics to collect preference pairs for DPO, ensuring both visual\nappeal and geometric accuracy. Conditioned on point clouds and images, DeepMesh\ngenerates meshes with intricate details and precise topology, outperforming\nstate-of-the-art methods in both precision and quality. Project page:\nhttps://zhaorw02.github.io/DeepMesh/"}
{"id": "2503.15275", "pdf": "https://arxiv.org/pdf/2503.15275", "abs": "https://arxiv.org/abs/2503.15275", "authors": ["Xiang Li", "Heqian Qiu", "Lanxiao Wang", "Hanwen Zhang", "Chenghao Qi", "Linfeng Han", "Huiyu Xiong", "Hongliang Li"], "title": "Challenges and Trends in Egocentric Vision: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of artificial intelligence technologies and\nwearable devices, egocentric vision understanding has emerged as a new and\nchallenging research direction, gradually attracting widespread attention from\nboth academia and industry. Egocentric vision captures visual and multimodal\ndata through cameras or sensors worn on the human body, offering a unique\nperspective that simulates human visual experiences. This paper provides a\ncomprehensive survey of the research on egocentric vision understanding,\nsystematically analyzing the components of egocentric scenes and categorizing\nthe tasks into four main areas: subject understanding, object understanding,\nenvironment understanding, and hybrid understanding. We explore in detail the\nsub-tasks within each category. We also summarize the main challenges and\ntrends currently existing in the field. Furthermore, this paper presents an\noverview of high-quality egocentric vision datasets, offering valuable\nresources for future research. By summarizing the latest advancements, we\nanticipate the broad applications of egocentric vision technologies in fields\nsuch as augmented reality, virtual reality, and embodied intelligence, and\npropose future research directions based on the latest developments in the\nfield."}
{"id": "2503.15283", "pdf": "https://arxiv.org/pdf/2503.15283", "abs": "https://arxiv.org/abs/2503.15283", "authors": ["Teng-Fang Hsiao", "Bo-Kai Ruan", "Yi-Lun Wu", "Tzu-Ling Lin", "Hong-Han Shuai"], "title": "TF-TI2I: Training-Free Text-and-Image-to-Image Generation via Multi-Modal Implicit-Context Learning in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-and-Image-To-Image (TI2I), an extension of Text-To-Image (T2I),\nintegrates image inputs with textual instructions to enhance image generation.\nExisting methods often partially utilize image inputs, focusing on specific\nelements like objects or styles, or they experience a decline in generation\nquality with complex, multi-image instructions. To overcome these challenges,\nwe introduce Training-Free Text-and-Image-to-Image (TF-TI2I), which adapts\ncutting-edge T2I models such as SD3 without the need for additional training.\nOur method capitalizes on the MM-DiT architecture, in which we point out that\ntextual tokens can implicitly learn visual information from vision tokens. We\nenhance this interaction by extracting a condensed visual representation from\nreference images, facilitating selective information sharing through Reference\nContextual Masking -- this technique confines the usage of contextual tokens to\ninstruction-relevant visual information. Additionally, our Winner-Takes-All\nmodule mitigates distribution shifts by prioritizing the most pertinent\nreferences for each vision token. Addressing the gap in TI2I evaluation, we\nalso introduce the FG-TI2I Bench, a comprehensive benchmark tailored for TI2I\nand compatible with existing T2I methods. Our approach shows robust performance\nacross various benchmarks, confirming its effectiveness in handling complex\nimage-generation tasks."}
{"id": "2503.15284", "pdf": "https://arxiv.org/pdf/2503.15284", "abs": "https://arxiv.org/abs/2503.15284", "authors": ["Yuanchao Yue", "Hui Yuan", "Qinglong Miao", "Xiaolong Mao", "Raouf Hamzaoui", "Peter Eisert"], "title": "EdgeRegNet: Edge Feature-based Multimodal Registration Network between Images and LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal data registration has long been a critical task in computer\nvision, with extensive applications in autonomous driving and robotics.\nAccurate and robust registration methods are essential for aligning data from\ndifferent modalities, forming the foundation for multimodal sensor data fusion\nand enhancing perception systems' accuracy and reliability. The registration\ntask between 2D images captured by cameras and 3D point clouds captured by\nLight Detection and Ranging (LiDAR) sensors is usually treated as a visual pose\nestimation problem. High-dimensional feature similarities from different\nmodalities are leveraged to identify pixel-point correspondences, followed by\npose estimation techniques using least squares methods. However, existing\napproaches often resort to downsampling the original point cloud and image data\ndue to computational constraints, inevitably leading to a loss in precision.\nAdditionally, high-dimensional features extracted using different feature\nextractors from various modalities require specific techniques to mitigate\ncross-modal differences for effective matching. To address these challenges, we\npropose a method that uses edge information from the original point clouds and\nimages for cross-modal registration. We retain crucial information from the\noriginal data by extracting edge points and pixels, enhancing registration\naccuracy while maintaining computational efficiency. The use of edge points and\nedge pixels allows us to introduce an attention-based feature exchange block to\neliminate cross-modal disparities. Furthermore, we incorporate an optimal\nmatching layer to improve correspondence identification. We validate the\naccuracy of our method on the KITTI and nuScenes datasets, demonstrating its\nstate-of-the-art performance."}
{"id": "2503.15285", "pdf": "https://arxiv.org/pdf/2503.15285", "abs": "https://arxiv.org/abs/2503.15285", "authors": ["Yuanchao Yue", "Zhengxin Li", "Wei Zhang", "Hui Yuan"], "title": "PAPI-Reg: Patch-to-Pixel Solution for Efficient Cross-Modal Registration between LiDAR Point Cloud and Camera Image", "categories": ["cs.CV"], "comment": null, "summary": "The primary requirement for cross-modal data fusion is the precise alignment\nof data from different sensors. However, the calibration between LiDAR point\nclouds and camera images is typically time-consuming and needs external\ncalibration board or specific environmental features. Cross-modal registration\neffectively solves this problem by aligning the data directly without requiring\nexternal calibration. However, due to the domain gap between the point cloud\nand the image, existing methods rarely achieve satisfactory registration\naccuracy while maintaining real-time performance. To address this issue, we\npropose a framework that projects point clouds into several 2D representations\nfor matching with camera images, which not only leverages the geometric\ncharacteristic of LiDAR point clouds more effectively but also bridge the\ndomain gap between the point cloud and image. Moreover, to tackle the\nchallenges of cross modal differences and the limited overlap between LiDAR\npoint clouds and images in the image matching task, we introduce a multi-scale\nfeature extraction network to effectively extract features from both camera\nimages and the projection maps of LiDAR point cloud. Additionally, we propose a\npatch-to-pixel matching network to provide more effective supervision and\nachieve higher accuracy. We validate the performance of our model through\nexperiments on the KITTI and nuScenes datasets. Our network achieves real-time\nperformance and extremely high registration accuracy. On the KITTI dataset, our\nmodel achieves a registration accuracy rate of over 99\\%."}
{"id": "2503.15293", "pdf": "https://arxiv.org/pdf/2503.15293", "abs": "https://arxiv.org/abs/2503.15293", "authors": ["Hangtao Zhang", "Yichen Wang", "Shihui Yan", "Chenyu Zhu", "Ziqi Zhou", "Linshan Hou", "Shengshan Hu", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "title": "Test-Time Backdoor Detection for Object Detection Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Object detection models are vulnerable to backdoor attacks, where attackers\npoison a small subset of training samples by embedding a predefined trigger to\nmanipulate prediction. Detecting poisoned samples (i.e., those containing\ntriggers) at test time can prevent backdoor activation. However, unlike image\nclassification tasks, the unique characteristics of object detection --\nparticularly its output of numerous objects -- pose fresh challenges for\nbackdoor detection. The complex attack effects (e.g., \"ghost\" object emergence\nor \"vanishing\" object) further render current defenses fundamentally\ninadequate. To this end, we design TRAnsformation Consistency Evaluation\n(TRACE), a brand-new method for detecting poisoned samples at test time in\nobject detection. Our journey begins with two intriguing observations: (1)\npoisoned samples exhibit significantly more consistent detection results than\nclean ones across varied backgrounds. (2) clean samples show higher detection\nconsistency when introduced to different focal information. Based on these\nphenomena, TRACE applies foreground and background transformations to each test\nsample, then assesses transformation consistency by calculating the variance in\nobjects confidences. TRACE achieves black-box, universal backdoor detection,\nwith extensive experiments showing a 30% improvement in AUROC over\nstate-of-the-art defenses and resistance to adaptive attacks."}
{"id": "2503.15295", "pdf": "https://arxiv.org/pdf/2503.15295", "abs": "https://arxiv.org/abs/2503.15295", "authors": ["Aoting Zhang", "Dongbao Yang", "Chang Liu", "Xiaopeng Hong", "Miao Shang", "Yu Zhou"], "title": "DCA: Dividing and Conquering Amnesia in Incremental Object Detection", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Incremental object detection (IOD) aims to cultivate an object detector that\ncan continuously localize and recognize novel classes while preserving its\nperformance on previous classes. Existing methods achieve certain success by\nimproving knowledge distillation and exemplar replay for transformer-based\ndetection frameworks, but the intrinsic forgetting mechanisms remain\nunderexplored. In this paper, we dive into the cause of forgetting and discover\nforgetting imbalance between localization and recognition in transformer-based\nIOD, which means that localization is less-forgetting and can generalize to\nfuture classes, whereas catastrophic forgetting occurs primarily on\nrecognition. Based on these insights, we propose a Divide-and-Conquer Amnesia\n(DCA) strategy, which redesigns the transformer-based IOD into a\nlocalization-then-recognition process. DCA can well maintain and transfer the\nlocalization ability, leaving decoupled fragile recognition to be specially\nconquered. To reduce feature drift in recognition, we leverage semantic\nknowledge encoded in pre-trained language models to anchor class\nrepresentations within a unified feature space across incremental tasks. This\ninvolves designing a duplex classifier fusion and embedding class semantic\nfeatures into the recognition decoding process in the form of queries.\nExtensive experiments validate that our approach achieves state-of-the-art\nperformance, especially for long-term incremental scenarios. For example, under\nthe four-step setting on MS-COCO, our DCA strategy significantly improves the\nfinal AP by 6.9%."}
{"id": "2503.15300", "pdf": "https://arxiv.org/pdf/2503.15300", "abs": "https://arxiv.org/abs/2503.15300", "authors": ["Weixiao Gao", "Liangliang Nan", "Hugo Ledoux"], "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes", "categories": ["cs.CV"], "comment": "22 pages, 24 figures", "summary": "Semantic segmentation in urban scene analysis has mainly focused on images or\npoint clouds, while textured meshes - offering richer spatial representation -\nremain underexplored. This paper introduces SUM Parts, the first large-scale\ndataset for urban textured meshes with part-level semantic labels, covering\nabout 2.5 km2 with 21 classes. The dataset was created using our own annotation\ntool, which supports both face- and texture-based annotations with efficient\ninteractive selection. We also provide a comprehensive evaluation of 3D\nsemantic segmentation and interactive annotation methods on this dataset. Our\nproject page is available at https://tudelft3d.github.io/SUMParts/."}
{"id": "2503.15337", "pdf": "https://arxiv.org/pdf/2503.15337", "abs": "https://arxiv.org/abs/2503.15337", "authors": ["Hao Tan", "Zichang Tan", "Jun Li", "Ajian Liu", "Jun Wan", "Zhen Lei"], "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Identifying multiple novel classes in an image, known as open-vocabulary\nmulti-label recognition, is a challenging task in computer vision. Recent\nstudies explore the transfer of powerful vision-language models such as CLIP.\nHowever, these approaches face two critical challenges: (1) The local semantics\nof CLIP are disrupted due to its global pre-training objectives, resulting in\nunreliable regional predictions. (2) The matching property between image\nregions and candidate labels has been neglected, relying instead on naive\nfeature aggregation such as average pooling, which leads to spurious\npredictions from irrelevant regions. In this paper, we present RAM (Recover And\nMatch), a novel framework that effectively addresses the above issues. To\ntackle the first problem, we propose Ladder Local Adapter (LLA) to enforce\nrefocusing on local regions, recovering local semantics in a memory-friendly\nway. For the second issue, we propose Knowledge-Constrained Optimal Transport\n(KCOT) to suppress meaningless matching to non-GT labels by formulating the\ntask as an optimal transport problem. As a result, RAM achieves\nstate-of-the-art performance on various datasets from three distinct domains,\nand shows great potential to boost the existing methods. Code:\nhttps://github.com/EricTan7/RAM."}
{"id": "2503.15342", "pdf": "https://arxiv.org/pdf/2503.15342", "abs": "https://arxiv.org/abs/2503.15342", "authors": ["Ritabrata Chakraborty", "Rajatsubhra Chakraborty", "Ali Khaleghi Rahimian", "Thomas MacDougall"], "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The proliferation of synthetic images generated by advanced AI models poses\nsignificant challenges in identifying and understanding manipulated visual\ncontent. Current fake image detection methods predominantly rely on binary\nclassification models that focus on accuracy while often neglecting\ninterpretability, leaving users without clear insights into why an image is\ndeemed real or fake. To bridge this gap, we introduce TruthLens, a novel\ntraining-free framework that reimagines deepfake detection as a visual\nquestion-answering (VQA) task. TruthLens utilizes state-of-the-art large\nvision-language models (LVLMs) to observe and describe visual artifacts and\ncombines this with the reasoning capabilities of large language models (LLMs)\nlike GPT-4 to analyze and aggregate evidence into informed decisions. By\nadopting a multimodal approach, TruthLens seamlessly integrates visual and\nsemantic reasoning to not only classify images as real or fake but also provide\ninterpretable explanations for its decisions. This transparency enhances trust\nand provides valuable insights into the artifacts that signal synthetic\ncontent. Extensive evaluations demonstrate that TruthLens outperforms\nconventional methods, achieving high accuracy on challenging datasets while\nmaintaining a strong emphasis on explainability. By reframing deepfake\ndetection as a reasoning-driven process, TruthLens establishes a new paradigm\nin combating synthetic media, combining cutting-edge performance with\ninterpretability to address the growing threats of visual disinformation."}
{"id": "2503.15361", "pdf": "https://arxiv.org/pdf/2503.15361", "abs": "https://arxiv.org/abs/2503.15361", "authors": ["Qingsen Yan", "Tao Hu", "Genggeng Chen", "Wei Dong", "Yanning Zhang"], "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer", "categories": ["cs.CV"], "comment": null, "summary": "Recovering High Dynamic Range (HDR) images from multiple Low Dynamic Range\n(LDR) images becomes challenging when the LDR images exhibit noticeable\ndegradation and missing content. Leveraging scene-specific semantic priors\noffers a promising solution for restoring heavily degraded regions. However,\nthese priors are typically extracted from sRGB Standard Dynamic Range (SDR)\nimages, the domain/format gap poses a significant challenge when applying it to\nHDR imaging. To address this issue, we propose a general framework that\ntransfers semantic knowledge derived from SDR domain via self-distillation to\nboost existing HDR reconstruction. Specifically, the proposed framework first\nintroduces the Semantic Priors Guided Reconstruction Model (SPGRM), which\nleverages SDR image semantic knowledge to address ill-posed problems in the\ninitial HDR reconstruction results. Subsequently, we leverage a\nself-distillation mechanism that constrains the color and content information\nwith semantic knowledge, aligning the external outputs between the baseline and\nSPGRM. Furthermore, to transfer the semantic knowledge of the internal\nfeatures, we utilize a semantic knowledge alignment module (SKAM) to fill the\nmissing semantic contents with the complementary masks. Extensive experiments\ndemonstrate that our method can significantly improve the HDR imaging quality\nof existing methods."}
{"id": "2503.15369", "pdf": "https://arxiv.org/pdf/2503.15369", "abs": "https://arxiv.org/abs/2503.15369", "authors": ["Yinan Liang", "Ziwei Wang", "Xiuwei Xu", "Jie Zhou", "Jiwen Lu"], "title": "EfficientLLaVA:Generalizable Auto-Pruning for Large Vision-language Models", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "While multimodal large language models demonstrate strong performance in\ncomplex reasoning tasks, they pose significant challenges related to model\ncomplexity during deployment, especially for resource-limited devices. In this\npaper, we propose an automatic pruning method for large vision-language models\nto enhance the efficiency of multimodal reasoning. Conventional methods rely on\nthe training data of the original model to select the proper pruning ratio for\ndifferent network components. However, these methods are impractical for large\nvision-language models due to the unaffordable search costs caused by web-scale\ntraining corpus. In contrast, our approach only leverages a small number of\nsamples to search for the desired pruning policy by maximizing its\ngeneralization ability on unknown training data while maintaining the model\naccuracy, which enables the achievement of an optimal trade-off between\naccuracy and efficiency for large visual language models. Specifically, we\nformulate the generalization gap of the pruning strategy using the structural\nrisk minimization principle. Based on both task performance and generalization\ncapability, we iteratively search for the optimal pruning policy within a given\nsearch space and optimize the vision projector to evolve the search space with\nhigher upper bound of performance. We conduct extensive experiments on the\nScienceQA, Vizwiz, MM-vet, and LLaVA-Bench datasets for the task of visual\nquestion answering. Using only 64 samples for pruning policy search,\nEfficientLLaVA achieves an accuracy of 83.05% on ScienceQA, along with a\n$\\times$ 1.8 speedup compared to the dense LLaVA-v1.5-7B model."}
{"id": "2503.15404", "pdf": "https://arxiv.org/pdf/2503.15404", "abs": "https://arxiv.org/abs/2503.15404", "authors": ["Yuchen Ren", "Zhengyu Zhao", "Chenhao Lin", "Bo Yang", "Lu Zhou", "Zhe Liu", "Chao Shen"], "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement", "categories": ["cs.CV", "cs.CR"], "comment": "CVPR2025", "summary": "Vision Transformers (ViTs) have been widely applied in various computer\nvision and vision-language tasks. To gain insights into their robustness in\npractical scenarios, transferable adversarial examples on ViTs have been\nextensively studied. A typical approach to improving adversarial\ntransferability is by refining the surrogate model. However, existing work on\nViTs has restricted their surrogate refinement to backward propagation. In this\nwork, we instead focus on Forward Propagation Refinement (FPR) and specifically\nrefine two key modules of ViTs: attention maps and token embeddings. For\nattention maps, we propose Attention Map Diversification (AMD), which\ndiversifies certain attention maps and also implicitly imposes beneficial\ngradient vanishing during backward propagation. For token embeddings, we\npropose Momentum Token Embedding (MTE), which accumulates historical token\nembeddings to stabilize the forward updates in both the Attention and MLP\nblocks. We conduct extensive experiments with adversarial examples transferred\nfrom ViTs to various CNNs and ViTs, demonstrating that our FPR outperforms the\ncurrent best (backward) surrogate refinement by up to 7.0\\% on average. We also\nvalidate its superiority against popular defenses and its compatibility with\nother transfer methods. Codes and appendix are available at\nhttps://github.com/RYC-98/FPR."}
{"id": "2503.15406", "pdf": "https://arxiv.org/pdf/2503.15406", "abs": "https://arxiv.org/abs/2503.15406", "authors": ["Jisu Nam", "Soowon Son", "Zhan Xu", "Jing Shi", "Difan Liu", "Feng Liu", "Aashish Misraa", "Seungryong Kim", "Yang Zhou"], "title": "Visual Persona: Foundation Model for Full-Body Human Customization", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page is available at\n  https://cvlab-kaist.github.io/Visual-Persona", "summary": "We introduce Visual Persona, a foundation model for text-to-image full-body\nhuman customization that, given a single in-the-wild human image, generates\ndiverse images of the individual guided by text descriptions. Unlike prior\nmethods that focus solely on preserving facial identity, our approach captures\ndetailed full-body appearance, aligning with text descriptions for body\nstructure and scene variations. Training this model requires large-scale paired\nhuman data, consisting of multiple images per individual with consistent\nfull-body identities, which is notoriously difficult to obtain. To address\nthis, we propose a data curation pipeline leveraging vision-language models to\nevaluate full-body appearance consistency, resulting in Visual Persona-500K, a\ndataset of 580k paired human images across 100k unique identities. For precise\nappearance transfer, we introduce a transformer encoder-decoder architecture\nadapted to a pre-trained text-to-image diffusion model, which augments the\ninput image into distinct body regions, encodes these regions as local\nappearance features, and projects them into dense identity embeddings\nindependently to condition the diffusion model for synthesizing customized\nimages. Visual Persona consistently surpasses existing approaches, generating\nhigh-quality, customized images from in-the-wild inputs. Extensive ablation\nstudies validate design choices, and we demonstrate the versatility of Visual\nPersona across various downstream tasks."}
{"id": "2503.15412", "pdf": "https://arxiv.org/pdf/2503.15412", "abs": "https://arxiv.org/abs/2503.15412", "authors": ["Fereshteh Forghani", "Jason J. Yu", "Tristan Aumentado-Armstrong", "Konstantinos G. Derpanis", "Marcus A. Brubaker"], "title": "Learn Your Scales: Towards Scale-Consistent Generative Novel View Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Conventional depth-free multi-view datasets are captured using a moving\nmonocular camera without metric calibration. The scales of camera positions in\nthis monocular setting are ambiguous. Previous methods have acknowledged scale\nambiguity in multi-view data via various ad-hoc normalization pre-processing\nsteps, but have not directly analyzed the effect of incorrect scene scales on\ntheir application. In this paper, we seek to understand and address the effect\nof scale ambiguity when used to train generative novel view synthesis methods\n(GNVS). In GNVS, new views of a scene or object can be minimally synthesized\ngiven a single image and are, thus, unconstrained, necessitating the use of\ngenerative methods. The generative nature of these models captures all aspects\nof uncertainty, including any uncertainty of scene scales, which act as\nnuisance variables for the task. We study the effect of scene scale ambiguity\nin GNVS when sampled from a single image by isolating its effect on the\nresulting models and, based on these intuitions, define new metrics that\nmeasure the scale inconsistency of generated views. We then propose a framework\nto estimate scene scales jointly with the GNVS model in an end-to-end fashion.\nEmpirically, we show that our method reduces the scale inconsistency of\ngenerated views without the complexity or downsides of previous scale\nnormalization methods. Further, we show that removing this ambiguity improves\ngenerated image quality of the resulting GNVS model."}
{"id": "2503.15415", "pdf": "https://arxiv.org/pdf/2503.15415", "abs": "https://arxiv.org/abs/2503.15415", "authors": ["Giovanni Floreale", "Piero Baraldi", "Enrico Zio", "Olga Fink"], "title": "Automated Processing of eXplainable Artificial Intelligence Outputs in Deep Learning Models for Fault Diagnostics of Large Infrastructures", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep Learning (DL) models processing images to recognize the health state of\nlarge infrastructure components can exhibit biases and rely on non-causal\nshortcuts. eXplainable Artificial Intelligence (XAI) can address these issues\nbut manually analyzing explanations generated by XAI techniques is\ntime-consuming and prone to errors. This work proposes a novel framework that\ncombines post-hoc explanations with semi-supervised learning to automatically\nidentify anomalous explanations that deviate from those of correctly classified\nimages and may therefore indicate model abnormal behaviors. This significantly\nreduces the workload for maintenance decision-makers, who only need to manually\nreclassify images flagged as having anomalous explanations. The proposed\nframework is applied to drone-collected images of insulator shells for power\ngrid infrastructure monitoring, considering two different Convolutional Neural\nNetworks (CNNs), GradCAM explanations and Deep Semi-Supervised Anomaly\nDetection. The average classification accuracy on two faulty classes is\nimproved by 8% and maintenance operators are required to manually reclassify\nonly 15% of the images. We compare the proposed framework with a\nstate-of-the-art approach based on the faithfulness metric: the experimental\nresults obtained demonstrate that the proposed framework consistently achieves\nF_1 scores larger than those of the faithfulness-based approach. Additionally,\nthe proposed framework successfully identifies correct classifications that\nresult from non-causal shortcuts, such as the presence of ID tags printed on\ninsulator shells."}
{"id": "2503.15417", "pdf": "https://arxiv.org/pdf/2503.15417", "abs": "https://arxiv.org/abs/2503.15417", "authors": ["Harold Haodong Chen", "Haojian Huang", "Xianfeng Wu", "Yexin Liu", "Yajing Bai", "Wen-Jie Shu", "Harry Yang", "Ser-Nam Lim"], "title": "Temporal Regularization Makes Your Video Generator Stronger", "categories": ["cs.CV", "cs.AI"], "comment": "Project: https://haroldchen19.github.io/FluxFlow/", "summary": "Temporal quality is a critical aspect of video generation, as it ensures\nconsistent motion and realistic dynamics across frames. However, achieving high\ntemporal coherence and diversity remains challenging. In this work, we explore\ntemporal augmentation in video generation for the first time, and introduce\nFluxFlow for initial investigation, a strategy designed to enhance temporal\nquality. Operating at the data level, FluxFlow applies controlled temporal\nperturbations without requiring architectural modifications. Extensive\nexperiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow\nsignificantly improves temporal coherence and diversity across various video\ngeneration models, including U-Net, DiT, and AR-based architectures, while\npreserving spatial fidelity. These findings highlight the potential of temporal\naugmentation as a simple yet effective approach to advancing video generation\nquality."}
{"id": "2503.15426", "pdf": "https://arxiv.org/pdf/2503.15426", "abs": "https://arxiv.org/abs/2503.15426", "authors": ["Wei Tang", "Yanpeng Sun", "Qinying Gu", "Zechao Li"], "title": "Visual Position Prompt for MLLM based Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although Multimodal Large Language Models (MLLMs) excel at various\nimage-related tasks, they encounter challenges in precisely aligning\ncoordinates with spatial information within images, particularly in\nposition-aware tasks such as visual grounding. This limitation arises from two\nkey factors. First, MLLMs lack explicit spatial references, making it difficult\nto associate textual descriptions with precise image locations. Second, their\nfeature extraction processes prioritize global context over fine-grained\nspatial details, leading to weak localization capability. To address this\nissue, we introduce VPP-LLaVA, an MLLM equipped with Visual Position Prompt\n(VPP) to improve its grounding capability. VPP-LLaVA integrates two\ncomplementary mechanisms. The global VPP overlays learnable, axis-like\nembeddings onto the input image to provide structured spatial cues. The local\nVPP focuses on fine-grained localization by incorporating position-aware\nqueries, which suggests probable object locations. We also introduce a VPP-SFT\ndataset with 0.6M samples, consolidating high-quality visual grounding data\ninto a compact format for efficient model training. Training on this dataset\nwith VPP enhances the model's performance, achieving state-of-the-art results\non standard grounding benchmarks despite using fewer training samples compared\nto other MLLMs like MiniGPT-v2, which rely on much larger datasets ($\\sim$21M\nsamples). The code and VPP-SFT dataset will be available at\nhttps://github.com/WayneTomas/VPP-LLaVA upon acceptance."}
{"id": "2503.15435", "pdf": "https://arxiv.org/pdf/2503.15435", "abs": "https://arxiv.org/abs/2503.15435", "authors": ["Baolu Li", "Zongzhe Xu", "Jinlong Li", "Xinyu Liu", "Jianwu Fang", "Xiaopeng Li", "Hongkai Yu"], "title": "V2X-DG: Domain Generalization for Vehicle-to-Everything Cooperative Perception", "categories": ["cs.CV"], "comment": "accepted by ICRA 2025", "summary": "LiDAR-based Vehicle-to-Everything (V2X) cooperative perception has\ndemonstrated its impact on the safety and effectiveness of autonomous driving.\nSince current cooperative perception algorithms are trained and tested on the\nsame dataset, the generalization ability of cooperative perception systems\nremains underexplored. This paper is the first work to study the Domain\nGeneralization problem of LiDAR-based V2X cooperative perception (V2X-DG) for\n3D detection based on four widely-used open source datasets: OPV2V, V2XSet,\nV2V4Real and DAIR-V2X. Our research seeks to sustain high performance not only\nwithin the source domain but also across other unseen domains, achieved solely\nthrough training on source domain. To this end, we propose Cooperative Mixup\nAugmentation based Generalization (CMAG) to improve the model generalization\ncapability by simulating the unseen cooperation, which is designed compactly\nfor the domain gaps in cooperative perception. Furthermore, we propose a\nconstraint for the regularization of the robust generalized feature\nrepresentation learning: Cooperation Feature Consistency (CFC), which aligns\nthe intermediately fused features of the generalized cooperation by CMAG and\nthe early fused features of the original cooperation in source domain.\nExtensive experiments demonstrate that our approach achieves significant\nperformance gains when generalizing to other unseen datasets while it also\nmaintains strong performance on the source dataset."}
{"id": "2503.15451", "pdf": "https://arxiv.org/pdf/2503.15451", "abs": "https://arxiv.org/abs/2503.15451", "authors": ["Lixing Xiao", "Shunlin Lu", "Huaijin Pi", "Ke Fan", "Liang Pan", "Yueer Zhou", "Ziyong Feng", "Xiaowei Zhou", "Sida Peng", "Jingbo Wang"], "title": "MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space", "categories": ["cs.CV"], "comment": "Project Page: https://zju3dv.github.io/MotionStreamer/", "summary": "This paper addresses the challenge of text-conditioned streaming motion\ngeneration, which requires us to predict the next-step human pose based on\nvariable-length historical motions and incoming texts. Existing methods\nstruggle to achieve streaming motion generation, e.g., diffusion models are\nconstrained by pre-defined motion lengths, while GPT-based methods suffer from\ndelayed response and error accumulation problem due to discretized non-causal\ntokenization. To solve these problems, we propose MotionStreamer, a novel\nframework that incorporates a continuous causal latent space into a\nprobabilistic autoregressive model. The continuous latents mitigate information\nloss caused by discretization and effectively reduce error accumulation during\nlong-term autoregressive generation. In addition, by establishing temporal\ncausal dependencies between current and historical motion latents, our model\nfully utilizes the available information to achieve accurate online motion\ndecoding. Experiments show that our method outperforms existing approaches\nwhile offering more applications, including multi-round generation, long-term\ngeneration, and dynamic motion composition. Project Page:\nhttps://zju3dv.github.io/MotionStreamer/"}
{"id": "2503.15457", "pdf": "https://arxiv.org/pdf/2503.15457", "abs": "https://arxiv.org/abs/2503.15457", "authors": ["Yuanzhi Zhu", "Xi Wang", "St√©phane Lathuili√®re", "Vicky Kalogeiton"], "title": "Di$\\mathtt{[M]}$O: Distilling Masked Diffusion Models into One-step Generator", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Masked Diffusion Models (MDMs) have emerged as a powerful generative modeling\ntechnique. Despite their remarkable results, they typically suffer from slow\ninference with several steps. In this paper, we propose Di$\\mathtt{[M]}$O, a\nnovel approach that distills masked diffusion models into a one-step generator.\nDi$\\mathtt{[M]}$O addresses two key challenges: (1) the intractability of using\nintermediate-step information for one-step generation, which we solve through\ntoken-level distribution matching that optimizes model output logits by an\n'on-policy framework' with the help of an auxiliary model; and (2) the lack of\nentropy in the initial distribution, which we address through a token\ninitialization strategy that injects randomness while maintaining similarity to\nteacher training distribution. We show Di$\\mathtt{[M]}$O's effectiveness on\nboth class-conditional and text-conditional image generation, impressively\nachieving performance competitive to multi-step teacher outputs while\ndrastically reducing inference time. To our knowledge, we are the first to\nsuccessfully achieve one-step distillation of masked diffusion models and the\nfirst to apply discrete distillation to text-to-image generation, opening new\npaths for efficient generative modeling."}
{"id": "2503.15465", "pdf": "https://arxiv.org/pdf/2503.15465", "abs": "https://arxiv.org/abs/2503.15465", "authors": ["Ruichen Chen", "Keith G. Mills", "Di Niu"], "title": "FP4DiT: Towards Effective Floating Point Quantization for Diffusion Transformers", "categories": ["cs.CV"], "comment": "The code is available at https://github.com/cccrrrccc/FP4DiT", "summary": "Diffusion Models (DM) have revolutionized the text-to-image visual generation\nprocess. However, the large computational cost and model footprint of DMs\nhinders practical deployment, especially on edge devices. Post-training\nquantization (PTQ) is a lightweight method to alleviate these burdens without\nthe need for training or fine-tuning. While recent DM PTQ methods achieve W4A8\non integer-based PTQ, two key limitations remain: First, while most existing DM\nPTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,\nwhich use convolutional U-Nets, newer Diffusion Transformer (DiT) models like\nthe PixArt series, Hunyuan and others adopt fundamentally different transformer\nbackbones to achieve superior image synthesis. Second, integer (INT)\nquantization is prevailing in DM PTQ but doesn't align well with the network\nweight and activation distribution, while Floating-Point Quantization (FPQ) is\nstill under-investigated, yet it holds the potential to better align the weight\nand activation distributions in low-bit settings for DiT. In response, we\nintroduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 quantization.\nSpecifically, we extend and generalize the Adaptive Rounding PTQ technique to\nadequately calibrate weight quantization for FPQ and demonstrate that DiT\nactivations depend on input patch data, necessitating robust online activation\nquantization techniques. Experimental results demonstrate that FP4DiT\noutperforms integer-based PTQ at W4A6 and W4A8 precision and generates\nconvincing visual content on PixArt-$\\alpha$, PixArt-$\\Sigma$ and Hunyuan in\nterms of several T2I metrics such as HPSv2 and CLIP."}
{"id": "2503.15470", "pdf": "https://arxiv.org/pdf/2503.15470", "abs": "https://arxiv.org/abs/2503.15470", "authors": ["Boshen Xu", "Yuting Mei", "Xinbi Liu", "Sipeng Zheng", "Qin Jin"], "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "Code will be released at: https://github.com/xuboshen/EgoDTM", "summary": "Egocentric video-language pretraining has significantly advanced video\nrepresentation learning. Humans perceive and interact with a fully 3D world,\ndeveloping spatial awareness that extends beyond text-based understanding.\nHowever, most previous works learn from 1D text or 2D visual cues, such as\nbounding boxes, which inherently lack 3D understanding. To bridge this gap, we\nintroduce EgoDTM, an Egocentric Depth- and Text-aware Model, jointly trained\nthrough large-scale 3D-aware video pretraining and video-text contrastive\nlearning. EgoDTM incorporates a lightweight 3D-aware decoder to efficiently\nlearn 3D-awareness from pseudo depth maps generated by depth estimation models.\nTo further facilitate 3D-aware video pretraining, we enrich the original brief\ncaptions with hand-object visual cues by organically combining several\nfoundation models. Extensive experiments demonstrate EgoDTM's superior\nperformance across diverse downstream tasks, highlighting its superior 3D-aware\nvisual understanding. Our code will be released at\nhttps://github.com/xuboshen/EgoDTM."}
{"id": "2503.15474", "pdf": "https://arxiv.org/pdf/2503.15474", "abs": "https://arxiv.org/abs/2503.15474", "authors": ["Maciej Ziaja", "Pawel Kowaleczko", "Daniel Kostrzewa", "Nicolas Long√©p√©", "Michal Kawulok"], "title": "Toward task-driven satellite image super-resolution", "categories": ["cs.CV"], "comment": "Submitted to IEEE IGARSS 2024", "summary": "Super-resolution is aimed at reconstructing high-resolution images from\nlow-resolution observations. State-of-the-art approaches underpinned with deep\nlearning allow for obtaining outstanding results, generating images of high\nperceptual quality. However, it often remains unclear whether the reconstructed\ndetails are close to the actual ground-truth information and whether they\nconstitute a more valuable source for image analysis algorithms. In the\nreported work, we address the latter problem, and we present our efforts toward\nlearning super-resolution algorithms in a task-driven way to make them suitable\nfor generating high-resolution images that can be exploited for automated image\nanalysis. In the reported initial research, we propose a methodological\napproach for assessing the existing models that perform computer vision tasks\nin terms of whether they can be used for evaluating super-resolution\nreconstruction algorithms, as well as training them in a task-driven way. We\nsupport our analysis with experimental study and we expect it to establish a\nsolid foundation for selecting appropriate computer vision tasks that will\nadvance the capabilities of real-world super-resolution."}
{"id": "2503.15475", "pdf": "https://arxiv.org/pdf/2503.15475", "abs": "https://arxiv.org/abs/2503.15475", "authors": ["Foundation AI Team", "Kiran Bhat", "Nishchaie Khanna", "Karun Channa", "Tinghui Zhou", "Yiheng Zhu", "Xiaoxia Sun", "Charles Shang", "Anirudh Sudarshan", "Maurice Chu", "Daiqing Li", "Kangle Deng", "Jean-Philippe Fauconnier", "Tijmen Verhulsdonck", "Maneesh Agrawala", "Kayvon Fatahalian", "Alexander Weiss", "Christian Reiser", "Ravi Kiran Chirravuri", "Ravali Kandur", "Alejandro Pelaez", "Akash Garg", "Michael Palleschi", "Jessica Wang", "Skylar Litz", "Leon Liu", "Anying Li", "David Harmon", "Derek Liu", "Liangjun Feng", "Denis Goupil", "Lukas Kuczynski", "Jihyun Yoon", "Naveen Marri", "Peiye Zhuang", "Yinan Zhang", "Brian Yin", "Haomiao Jiang", "Marcel van Workum", "Thomas Lane", "Bryce Erickson", "Salil Pathare", "Kyle Price", "Anupam Singh", "David Baszucki"], "title": "Cube: A Roblox View of 3D Intelligence", "categories": ["cs.CV"], "comment": "Our code and model weights can be found at:\n  https://github.com/Roblox/cube", "summary": "Foundation models trained on vast amounts of data have demonstrated\nremarkable reasoning and generation capabilities in the domains of text,\nimages, audio and video. Our goal at Roblox is to build such a foundation model\nfor 3D intelligence, a model that can support developers in producing all\naspects of a Roblox experience, from generating 3D objects and scenes to\nrigging characters for animation to producing programmatic scripts describing\nobject behaviors. We discuss three key design requirements for such a 3D\nfoundation model and then present our first step towards building such a model.\nWe expect that 3D geometric shapes will be a core data type and describe our\nsolution for 3D shape tokenizer. We show how our tokenization scheme can be\nused in applications for text-to-shape generation, shape-to-text generation and\ntext-to-scene generation. We demonstrate how these applications can collaborate\nwith existing large language models (LLMs) to perform scene analysis and\nreasoning. We conclude with a discussion outlining our path to building a fully\nunified foundation model for 3D intelligence."}
{"id": "2503.15485", "pdf": "https://arxiv.org/pdf/2503.15485", "abs": "https://arxiv.org/abs/2503.15485", "authors": ["Zineng Tang", "Long Lian", "Seun Eisape", "XuDong Wang", "Roei Herzig", "Adam Yala", "Alane Suhr", "Trevor Darrell", "David M. Chan"], "title": "TULIP: Towards Unified Language-Image Pretraining", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite the recent success of image-text contrastive models like CLIP and\nSigLIP, these models often struggle with vision-centric tasks that demand\nhigh-fidelity image understanding, such as counting, depth estimation, and\nfine-grained object recognition. These models, by performing language\nalignment, tend to prioritize high-level semantics over visual understanding,\nweakening their image understanding. On the other hand, vision-focused models\nare great at processing visual information but struggle to understand language,\nlimiting their flexibility for language-driven tasks. In this work, we\nintroduce TULIP, an open-source, drop-in replacement for existing CLIP-like\nmodels. Our method leverages generative data augmentation, enhanced image-image\nand text-text contrastive learning, and image/text reconstruction\nregularization to learn fine-grained visual features while preserving global\nsemantic alignment. Our approach, scaling to over 1B parameters, outperforms\nexisting state-of-the-art (SOTA) models across multiple benchmarks,\nestablishing a new SOTA zero-shot performance on ImageNet-1K, delivering up to\na $2\\times$ enhancement over SigLIP on RxRx1 in linear probing for few-shot\nclassification, and improving vision-language models, achieving over $3\\times$\nhigher scores than SigLIP on MMVP. Our code/checkpoints are available at\nhttps://tulip-berkeley.github.io"}
{"id": "2503.14523", "pdf": "https://arxiv.org/pdf/2503.14523", "abs": "https://arxiv.org/abs/2503.14523", "authors": ["Siyi Wu", "Leyi Zhao", "Haitian Ma", "Xinyuan Song"], "title": "SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation via SDF Pre-training and Topology-Aware Fine-Tuning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of tubular and curvilinear structures, such as blood\nvessels, neurons, and road networks, is crucial in various applications. A key\nchallenge is ensuring topological correctness while maintaining computational\nefficiency. Existing approaches often employ topological loss functions based\non persistent homology, such as Betti error, to enforce structural consistency.\nHowever, these methods suffer from high computational costs and are insensitive\nto pixel-level accuracy, often requiring additional loss terms like Dice or MSE\nto compensate. To address these limitations, we propose \\textbf{SDF-TopoNet},\nan improved topology-aware segmentation framework that enhances both\nsegmentation accuracy and training efficiency. Our approach introduces a novel\ntwo-stage training strategy. In the pre-training phase, we utilize the signed\ndistance function (SDF) as an auxiliary learning target, allowing the model to\nencode topological information without directly relying on computationally\nexpensive topological loss functions. In the fine-tuning phase, we incorporate\na dynamic adapter alongside a refined topological loss to ensure topological\ncorrectness while mitigating overfitting and computational overhead. We\nevaluate our method on five benchmark datasets. Experimental results\ndemonstrate that SDF-TopoNet outperforms existing methods in both topological\naccuracy and quantitative segmentation metrics, while significantly reducing\ntraining complexity."}
{"id": "2503.14525", "pdf": "https://arxiv.org/pdf/2503.14525", "abs": "https://arxiv.org/abs/2503.14525", "authors": ["Frans Zdyb", "Albert Alonso", "Julius B. Kirkegaard"], "title": "Spline refinement with differentiable rendering", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.AP", "stat.ML"], "comment": null, "summary": "Detecting slender, overlapping structures remains a challenge in\ncomputational microscopy. While recent coordinate-based approaches improve\ndetection, they often produce less accurate splines than pixel-based methods.\nWe introduce a training-free differentiable rendering approach to spline\nrefinement, achieving both high reliability and sub-pixel accuracy. Our method\nimproves spline quality, enhances robustness to distribution shifts, and\nshrinks the gap between synthetic and real-world data. Being fully\nunsupervised, the method is a drop-in replacement for the popular active\ncontour model for spline refinement. Evaluated on C. elegans nematodes, a\npopular model organism for drug discovery and biomedical research, we\ndemonstrate that our approach combines the strengths of both coordinate- and\npixel-based methods."}
{"id": "2503.14534", "pdf": "https://arxiv.org/pdf/2503.14534", "abs": "https://arxiv.org/abs/2503.14534", "authors": ["Bibi Erum Ayesha", "T. Satyanarayana Murthy", "Palamakula Ramesh Babu", "Ramu Kuchipudi"], "title": "Ship Detection in Remote Sensing Imagery for Arbitrarily Oriented Object Detection", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This research paper presents an innovative ship detection system tailored for\napplications like maritime surveillance and ecological monitoring. The study\nemploys YOLOv8 and repurposed U-Net, two advanced deep learning models, to\nsignificantly enhance ship detection accuracy. Evaluation metrics include Mean\nAverage Precision (mAP), processing speed, and overall accuracy. The research\nutilizes the \"Airbus Ship Detection\" dataset, featuring diverse remote sensing\nimages, to assess the models' versatility in detecting ships with varying\norientations and environmental contexts. Conventional ship detection faces\nchallenges with arbitrary orientations, complex backgrounds, and obscured\nperspectives. Our approach incorporates YOLOv8 for real-time processing and\nU-Net for ship instance segmentation. Evaluation focuses on mAP, processing\nspeed, and overall accuracy. The dataset is chosen for its diverse images,\nmaking it an ideal benchmark. Results demonstrate significant progress in ship\ndetection. YOLOv8 achieves an 88% mAP, excelling in accurate and rapid ship\ndetection. U Net, adapted for ship instance segmentation, attains an 89% mAP,\nimproving boundary delineation and handling occlusions. This research enhances\nmaritime surveillance, disaster response, and ecological monitoring,\nexemplifying the potential of deep learning models in ship detection."}
{"id": "2503.14536", "pdf": "https://arxiv.org/pdf/2503.14536", "abs": "https://arxiv.org/abs/2503.14536", "authors": ["Praveen Shastry", "Sowmya Chowdary Muthulur", "Naveen Kumarasami", "Anandakumar D", "Mounigasri M", "Keerthana R", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam", "Revathi Ezhumalai", "Abitha Marimuthu"], "title": "Advancing Chronic Tuberculosis Diagnostics Using Vision-Language Models: A Multi modal Framework for Precision Analysis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 92C55, 68U10, 92C50, 60G35"], "comment": "10 pages , 3 figures", "summary": "Background This study proposes a Vision-Language Model (VLM) leveraging the\nSIGLIP encoder and Gemma-3b transformer decoder to enhance automated chronic\ntuberculosis (TB) screening. By integrating chest X-ray images with clinical\ndata, the model addresses the challenges of manual interpretation, improving\ndiagnostic consistency and accessibility, particularly in resource-constrained\nsettings.\n  Methods The VLM architecture combines a Vision Transformer (ViT) for visual\nencoding and a transformer-based text encoder to process clinical context, such\nas patient histories and treatment records. Cross-modal attention mechanisms\nalign radiographic features with textual information, while the Gemma-3b\ndecoder generates comprehensive diagnostic reports. The model was pre-trained\non 5 million paired medical images and texts and fine-tuned using 100,000\nchronic TB-specific chest X-rays.\n  Results The model demonstrated high precision (94 percent) and recall (94\npercent) for detecting key chronic TB pathologies, including fibrosis,\ncalcified granulomas, and bronchiectasis. Area Under the Curve (AUC) scores\nexceeded 0.93, and Intersection over Union (IoU) values were above 0.91,\nvalidating its effectiveness in detecting and localizing TB-related\nabnormalities.\n  Conclusion The VLM offers a robust and scalable solution for automated\nchronic TB diagnosis, integrating radiographic and clinical data to deliver\nactionable and context-aware insights. Future work will address subtle\npathologies and dataset biases to enhance the model's generalizability,\nensuring equitable performance across diverse populations and healthcare\nsettings."}
{"id": "2503.14538", "pdf": "https://arxiv.org/pdf/2503.14538", "abs": "https://arxiv.org/abs/2503.14538", "authors": ["Ananya Ganapthy", "Praveen Shastry", "Naveen Kumarasami", "Anandakumar D", "Keerthana R", "Mounigasri M", "Varshinipriya M", "Kishore Prasath Venkatesh", "Bargava Subramanian", "Kalyan Sivasailam"], "title": "Vision-Language Models for Acute Tuberculosis Diagnosis: A Multimodal Approach Combining Imaging and Clinical Data", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07, 68T45, 92C55, 92C50, 68U10"], "comment": "11 pages, 3 figures", "summary": "Background: This study introduces a Vision-Language Model (VLM) leveraging\nSIGLIP and Gemma-3b architectures for automated acute tuberculosis (TB)\nscreening. By integrating chest X-ray images and clinical notes, the model aims\nto enhance diagnostic accuracy and efficiency, particularly in resource-limited\nsettings.\n  Methods: The VLM combines visual data from chest X-rays with clinical context\nto generate detailed, context-aware diagnostic reports. The architecture\nemploys SIGLIP for visual encoding and Gemma-3b for decoding, ensuring\neffective representation of acute TB-specific pathologies and clinical\ninsights.\n  Results: Key acute TB pathologies, including consolidation, cavities, and\nnodules, were detected with high precision (97percent) and recall (96percent).\nThe model demonstrated strong spatial localization capabilities and robustness\nin distinguishing TB-positive cases, making it a reliable tool for acute TB\ndiagnosis.\n  Conclusion: The multimodal capability of the VLM reduces reliance on\nradiologists, providing a scalable solution for acute TB screening. Future work\nwill focus on improving the detection of subtle pathologies and addressing\ndataset biases to enhance its generalizability and application in diverse\nglobal healthcare settings."}
{"id": "2503.14542", "pdf": "https://arxiv.org/pdf/2503.14542", "abs": "https://arxiv.org/abs/2503.14542", "authors": ["Agnieszka Sroka-Oleksiak", "Adam Pardyl", "Dawid Rymarczyk", "Aldona Olechowska-JarzƒÖb", "Katarzyna Biegun-Dro≈ºd≈º", "Dorota Ocho≈Ñska", "Micha≈Ç Wronka", "Adriana Borowa", "Tomasz Gosiewski", "Mi≈Çosz Adamczyk", "Henryk Telega", "Bartosz Zieli≈Ñski", "Monika Brzychczy-W≈Çoch"], "title": "AI-Driven Rapid Identification of Bacterial and Fungal Pathogens in Blood Smears of Septic Patients", "categories": ["eess.IV", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": null, "summary": "Sepsis is a life-threatening condition which requires rapid diagnosis and\ntreatment. Traditional microbiological methods are time-consuming and\nexpensive. In response to these challenges, deep learning algorithms were\ndeveloped to identify 14 bacteria species and 3 yeast-like fungi from\nmicroscopic images of Gram-stained smears of positive blood samples from sepsis\npatients.\n  A total of 16,637 Gram-stained microscopic images were used in the study. The\nanalysis used the Cellpose 3 model for segmentation and Attention-based Deep\nMultiple Instance Learning for classification. Our model achieved an accuracy\nof 77.15% for bacteria and 71.39% for fungi, with ROC AUC of 0.97 and 0.88,\nrespectively. The highest values, reaching up to 96.2%, were obtained for\nCutibacterium acnes, Enterococcus faecium, Stenotrophomonas maltophilia and\nNakaseomyces glabratus. Classification difficulties were observed in closely\nrelated species, such as Staphylococcus hominis and Staphylococcus\nhaemolyticus, due to morphological similarity, and within Candida albicans due\nto high morphotic diversity.\n  The study confirms the potential of our model for microbial classification,\nbut it also indicates the need for further optimisation and expansion of the\ntraining data set. In the future, this technology could support microbial\ndiagnosis, reducing diagnostic time and improving the effectiveness of sepsis\ntreatment due to its simplicity and accessibility. Part of the results\npresented in this publication was covered by a patent application at the\nEuropean Patent Office EP24461637.1 \"A computer implemented method for\nidentifying a microorganism in a blood and a data processing system therefor\"."}
{"id": "2503.14546", "pdf": "https://arxiv.org/pdf/2503.14546", "abs": "https://arxiv.org/abs/2503.14546", "authors": ["Gustavo Correia", "Victor Alves", "Paulo Novais"], "title": "The Impact of Artificial Intelligence on Emergency Medicine: A Review of Recent Advances", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "68T07"], "comment": "20 pages, 2 tables, 2 figures", "summary": "Artificial Intelligence (AI) is revolutionizing emergency medicine by\nenhancing diagnostic processes and improving patient outcomes. This article\nprovides a review of the current applications of AI in emergency imaging\nstudies, focusing on the last five years of advancements. AI technologies,\nparticularly machine learning and deep learning, are pivotal in interpreting\ncomplex imaging data, offering rapid, accurate diagnoses and potentially\nsurpassing traditional diagnostic methods. Studies highlighted within the\narticle demonstrate AI's capabilities in accurately detecting conditions such\nas fractures, pneumothorax, and pulmonary diseases from various imaging\nmodalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to\npredict clinical outcomes like mechanical ventilation needs illustrates its\npotential in crisis resource optimization. Despite these advancements, the\nintegration of AI into clinical practice presents challenges such as data\nprivacy, algorithmic bias, and the need for extensive validation across diverse\nsettings. This review underscores the transformative potential of AI in\nemergency settings, advocating for a future where AI and clinical expertise\nsynergize to elevate patient care standards."}
{"id": "2503.14550", "pdf": "https://arxiv.org/pdf/2503.14550", "abs": "https://arxiv.org/abs/2503.14550", "authors": ["Theodorus Dapamede", "Aisha Urooj", "Vedant Joshi", "Gabrielle Gershon", "Frank Li", "Mohammadreza Chavoshi", "Beatrice Brown-Mulry", "Rohan Satya Isaac", "Aawez Mansuri", "Chad Robichaux", "Chadi Ayoub", "Reza Arsanjani", "Laurence Sperling", "Judy Gichoya", "Marly van Assen", "Charles W. ONeill", "Imon Banerjee", "Hari Trivedi"], "title": "Novel AI-Based Quantification of Breast Arterial Calcification to Predict Cardiovascular Risk", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Women are underdiagnosed and undertreated for cardiovascular disease.\nAutomatic quantification of breast arterial calcification on screening\nmammography can identify women at risk for cardiovascular disease and enable\nearlier treatment and management of disease. In this retrospective study of\n116,135 women from two healthcare systems, a transformer-based neural network\nquantified BAC severity (no BAC, mild, moderate, and severe) on screening\nmammograms. Outcomes included major adverse cardiovascular events (MACE) and\nall-cause mortality. BAC severity was independently associated with MACE after\nadjusting for cardiovascular risk factors, with increasing hazard ratios from\nmild (HR 1.18-1.22), moderate (HR 1.38-1.47), to severe BAC (HR 2.03-2.22)\nacross datasets (all p<0.001). This association remained significant across all\nage groups, with even mild BAC indicating increased risk in women under 50. BAC\nremained an independent predictor when analyzed alongside ASCVD risk scores,\nshowing significant associations with myocardial infarction, stroke, heart\nfailure, and mortality (all p<0.005). Automated BAC quantification enables\nopportunistic cardiovascular risk assessment during routine mammography without\nadditional radiation or cost. This approach provides value beyond traditional\nrisk factors, particularly in younger women, offering potential for early CVD\nrisk stratification in the millions of women undergoing annual mammography."}
{"id": "2503.14554", "pdf": "https://arxiv.org/pdf/2503.14554", "abs": "https://arxiv.org/abs/2503.14554", "authors": ["Ali Parsaee", "Fahim Shahriar", "Chuxin He", "Ruiqing Tan"], "title": "Synchronous vs Asynchronous Reinforcement Learning in a Real World Robot", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Presented at Alberta Robotics & Intelligent Systems Expo (RISE)\n  Conference", "summary": "In recent times, reinforcement learning (RL) with physical robots has\nattracted the attention of a wide range of researchers. However,\nstate-of-the-art RL algorithms do not consider that physical environments do\nnot wait for the RL agent to make decisions or updates. RL agents learn by\nperiodically conducting computationally expensive gradient updates. When\ndecision-making and gradient update tasks are carried out sequentially by the\nRL agent in a physical robot, it significantly increases the agent's response\ntime. In a rapidly changing environment, this increased response time may be\ndetrimental to the performance of the learning agent. Asynchronous RL methods,\nwhich separate the computation of decision-making and gradient updates, are a\npotential solution to this problem. However, only a few comparisons between\nasynchronous and synchronous RL have been made with physical robots. For this\nreason, the exact performance benefits of using asynchronous RL methods over\nsynchronous RL methods are still unclear. In this study, we provide a\nperformance comparison between asynchronous and synchronous RL using a physical\nrobotic arm called Franka Emika Panda. Our experiments show that the agents\nlearn faster and attain significantly more returns using asynchronous RL. Our\nexperiments also demonstrate that the learning agent with a faster response\ntime performs better than the agent with a slower response time, even if the\nagent with a slower response time performs a higher number of gradient updates."}
{"id": "2503.14559", "pdf": "https://arxiv.org/pdf/2503.14559", "abs": "https://arxiv.org/abs/2503.14559", "authors": ["Weixiong Lin", "Chen Ju", "Haicheng Wang", "Shengchao Hu", "Shuai Xiao", "Mengting Chen", "Yuheng Jiao", "Mingshuai Yao", "Jinsong Lan", "Qingwen Liu", "Ying Chen"], "title": "Squeeze Out Tokens from Sample for Finer-Grained Data Governance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Widely observed data scaling laws, in which error falls off as a power of the\ntraining size, demonstrate the diminishing returns of unselective data\nexpansion. Hence, data governance is proposed to downsize datasets through\npruning non-informative samples. Yet, isolating the impact of a specific sample\non overall model performance is challenging, due to the vast computation\nrequired for tryout all sample combinations. Current data governors circumvent\nthis complexity by estimating sample contributions through heuristic-derived\nscalar scores, thereby discarding low-value ones. Despite thorough sample\nsieving, retained samples contain substantial undesired tokens intrinsically,\nunderscoring the potential for further compression and purification. In this\nwork, we upgrade data governance from a 'sieving' approach to a 'juicing' one.\nInstead of scanning for least-flawed samples, our dual-branch DataJuicer\napplies finer-grained intra-sample governance. It squeezes out informative\ntokens and boosts image-text alignments. Specifically, the vision branch\nretains salient image patches and extracts relevant object classes, while the\ntext branch incorporates these classes to enhance captions. Consequently,\nDataJuicer yields more refined datasets through finer-grained governance.\nExtensive experiments across datasets demonstrate that DataJuicer significantly\noutperforms existing DataSieve in image-text retrieval, classification, and\ndense visual reasoning."}
{"id": "2503.14562", "pdf": "https://arxiv.org/pdf/2503.14562", "abs": "https://arxiv.org/abs/2503.14562", "authors": ["A. I. Medvedeva", "V. V. Bakutkin"], "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "in Russian language", "summary": "Subject of research: is the study of methods for analyzing perimetric images\nfor the diagnosis and control of glaucoma diseases. Objects of research: is a\ndataset collected on the ophthalmological perimeter with the results of various\npatient pathologies, since the ophthalmological community is acutely aware of\nthe issue of disease control and import substitution. [5]. Purpose of research:\nis to consider various machine learning methods that can classify glaucoma.\nThis is possible thanks to the classifier built after labeling the dataset. It\nis able to determine from the image whether the visual fields depicted on it\nare the results of the impact of glaucoma on the eyes or other visual diseases.\nEarlier in the work [3], a dataset was described that was collected on the\nTomey perimeter. The average age of the examined patients ranged from 30 to 85\nyears. Methods of research: machine learning methods for classifying image\nresults (stochastic gradient descent, logistic regression, random forest, naive\nBayes). Main results of research: the result of the study is computer modeling\nthat can determine from the image whether the result is glaucoma or another\ndisease (binary classification)."}
{"id": "2503.14573", "pdf": "https://arxiv.org/pdf/2503.14573", "abs": "https://arxiv.org/abs/2503.14573", "authors": ["Wanxin Yu", "Zhemin Zhu", "Cong Wang", "Yihang Bao", "Chunjie Xia", "Rongshan Cheng", "Yan Yu", "Tsung-Yuan Tsai"], "title": "Three-dimensional Reconstruction of the Lumbar Spine with Submillimeter Accuracy Using Biplanar X-ray Images", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "21 pages, 10 figures, 4 tables", "summary": "Three-dimensional reconstruction of the spine under weight-bearing conditions\nfrom biplanar X-ray images is of great importance for the clinical assessment\nof spinal diseases. However, the current fully automated reconstruction methods\nhave low accuracy and fail to meet the clinical application standards. This\nstudy developed and validated a fully automated method for high-accuracy 3D\nreconstruction of the lumbar spine from biplanar X-ray images. The method\ninvolves lumbar decomposition and landmark detection from the raw X-ray images,\nfollowed by a deformable model and landmark-weighted 2D-3D registration\napproach. The reconstruction accuracy was validated by the gold standard\nobtained through the registration of CT-segmented vertebral models with the\nbiplanar X-ray images. The proposed method achieved a 3D reconstruction\naccuracy of 0.80 mm, representing a significant improvement over the mainstream\napproaches. This study will contribute to the clinical diagnosis of lumbar in\nweight-bearing positions."}
{"id": "2503.14637", "pdf": "https://arxiv.org/pdf/2503.14637", "abs": "https://arxiv.org/abs/2503.14637", "authors": ["Merkourios Simos", "Alberto Silvio Chiappa", "Alexander Mathis"], "title": "Reinforcement learning-based motion imitation for physiologically plausible musculoskeletal motor control", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "How do humans move? The quest to understand human motion has broad\napplications in numerous fields, ranging from computer animation and motion\nsynthesis to neuroscience, human prosthetics and rehabilitation. Although\nadvances in reinforcement learning (RL) have produced impressive results in\ncapturing human motion using simplified humanoids, controlling physiologically\naccurate models of the body remains an open challenge. In this work, we present\na model-free motion imitation framework (KINESIS) to advance the understanding\nof muscle-based motor control. Using a musculoskeletal model of the lower body\nwith 80 muscle actuators and 20 DoF, we demonstrate that KINESIS achieves\nstrong imitation performance on 1.9 hours of motion capture data, is\ncontrollable by natural language through pre-trained text-to-motion generative\nmodels, and can be fine-tuned to carry out high-level tasks such as target goal\nreaching. Importantly, KINESIS generates muscle activity patterns that\ncorrelate well with human EMG activity. The physiological plausibility makes\nKINESIS a promising model for tackling challenging problems in human motor\ncontrol theory, which we highlight by investigating Bernstein's redundancy\nproblem in the context of locomotion. Code, videos and benchmarks will be\navailable at https://github.com/amathislab/Kinesis."}
{"id": "2503.14655", "pdf": "https://arxiv.org/pdf/2503.14655", "abs": "https://arxiv.org/abs/2503.14655", "authors": ["Minheng Chen", "Xiaowei Yu", "Jing Zhang", "Tong Chen", "Chao Cao", "Yan Zhuang", "Yanjun Lyu", "Lu Zhang", "Tianming Liu", "Dajiang Zhu"], "title": "Core-Periphery Principle Guided State Space Model for Functional Connectome Classification", "categories": ["q-bio.NC", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Understanding the organization of human brain networks has become a central\nfocus in neuroscience, particularly in the study of functional connectivity,\nwhich plays a crucial role in diagnosing neurological disorders. Advances in\nfunctional magnetic resonance imaging and machine learning techniques have\nsignificantly improved brain network analysis. However, traditional machine\nlearning approaches struggle to capture the complex relationships between brain\nregions, while deep learning methods, particularly Transformer-based models,\nface computational challenges due to their quadratic complexity in\nlong-sequence modeling. To address these limitations, we propose a\nCore-Periphery State-Space Model (CP-SSM), an innovative framework for\nfunctional connectome classification. Specifically, we introduce Mamba, a\nselective state-space model with linear complexity, to effectively capture\nlong-range dependencies in functional brain networks. Furthermore, inspired by\nthe core-periphery (CP) organization, a fundamental characteristic of brain\nnetworks that enhances efficient information transmission, we design CP-MoE, a\nCP-guided Mixture-of-Experts that improves the representation learning of brain\nconnectivity patterns. We evaluate CP-SSM on two benchmark fMRI datasets: ABIDE\nand ADNI. Experimental results demonstrate that CP-SSM surpasses\nTransformer-based models in classification performance while significantly\nreducing computational complexity. These findings highlight the effectiveness\nand efficiency of CP-SSM in modeling brain functional connectivity, offering a\npromising direction for neuroimaging-based neurological disease diagnosis."}
{"id": "2503.14694", "pdf": "https://arxiv.org/pdf/2503.14694", "abs": "https://arxiv.org/abs/2503.14694", "authors": ["Rui Yang", "Lin Song", "Yicheng Xiao", "Runhui Huang", "Yixiao Ge", "Ying Shan", "Hengshuang Zhao"], "title": "HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\npropelled the development of large multi-modal models (LMMs), highlighting the\npotential for general and intelligent assistants. However, most LMMs model\nvisual and textual modalities separately, leading to recent efforts to develop\nnative LMMs using a single transformer. Despite the promise, these native\nmodels are resource-intensive and often exhibit performance gaps compared to\ntheir compositional counterparts. To alleviate this issue, we propose a simple\nyet efficient method to construct a baseline for the native and end-to-end\nlarge multi-modal model in a single transformer. First, we propose a new\nearly-fusion LMM that can fuse multi-modal inputs in the early stage and\nrespond to visual instructions in an auto-regressive manner. Second, we devise\nan efficient training recipe for the proposed model, which harnesses the prior\nknowledge of the pre-trained models, addressing both the performance\nlimitations and the challenge of resource consumption. The proposed model\ndemonstrates superior performance compared to other LMMs using one transformer\nand significantly narrows the performance gap with compositional LMMs."}
{"id": "2503.14754", "pdf": "https://arxiv.org/pdf/2503.14754", "abs": "https://arxiv.org/abs/2503.14754", "authors": ["Matt Franchi", "Nikhil Garg", "Wendy Ju", "Emma Pierson"], "title": "Bayesian Modeling of Zero-Shot Classifications for Urban Flood Detection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "In review", "summary": "Street scene datasets, collected from Street View or dashboard cameras, offer\na promising means of detecting urban objects and incidents like street\nflooding. However, a major challenge in using these datasets is their lack of\nreliable labels: there are myriad types of incidents, many types occur rarely,\nand ground-truth measures of where incidents occur are lacking. Here, we\npropose BayFlood, a two-stage approach which circumvents this difficulty.\nFirst, we perform zero-shot classification of where incidents occur using a\npretrained vision-language model (VLM). Second, we fit a spatial Bayesian model\non the VLM classifications. The zero-shot approach avoids the need to annotate\nlarge training sets, and the Bayesian model provides frequent desiderata in\nurban settings - principled measures of uncertainty, smoothing across\nlocations, and incorporation of external data like stormwater accumulation\nzones. We comprehensively validate this two-stage approach, showing that VLMs\nprovide strong zero-shot signal for floods across multiple cities and time\nperiods, the Bayesian model improves out-of-sample prediction relative to\nbaseline methods, and our inferred flood risk correlates with known external\npredictors of risk. Having validated our approach, we show it can be used to\nimprove urban flood detection: our analysis reveals 113,738 people who are at\nhigh risk of flooding overlooked by current methods, identifies demographic\nbiases in existing methods, and suggests locations for new flood sensors. More\nbroadly, our results showcase how Bayesian modeling of zero-shot LM annotations\nrepresents a promising paradigm because it avoids the need to collect large\nlabeled datasets and leverages the power of foundation models while providing\nthe expressiveness and uncertainty quantification of Bayesian models."}
{"id": "2503.14756", "pdf": "https://arxiv.org/pdf/2503.14756", "abs": "https://arxiv.org/abs/2503.14756", "authors": ["Hou In Ivan Tam", "Hou In Derek Pun", "Austin T. Wang", "Angel X. Chang", "Manolis Savva"], "title": "SceneEval: Evaluating Semantic Coherence in Text-Conditioned 3D Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "20 pages, 6 figures, 6 tables", "summary": "Despite recent advances in text-conditioned 3D indoor scene generation, there\nremain gaps in the evaluation of these methods. Existing metrics primarily\nassess the realism of generated scenes by comparing them to a set of\nground-truth scenes, often overlooking alignment with the input text - a\ncritical factor in determining how effectively a method meets user\nrequirements. We present SceneEval, an evaluation framework designed to address\nthis limitation. SceneEval includes metrics for both explicit user\nrequirements, such as the presence of specific objects and their attributes\ndescribed in the input text, and implicit expectations, like the absence of\nobject collisions, providing a comprehensive assessment of scene quality. To\nfacilitate evaluation, we introduce SceneEval-100, a dataset of scene\ndescriptions with annotated ground-truth scene properties. We evaluate recent\nscene generation methods using SceneEval and demonstrate its ability to provide\ndetailed assessments of the generated scenes, highlighting strengths and areas\nfor improvement across multiple dimensions. Our results show that current\nmethods struggle at generating scenes that meet user requirements, underscoring\nthe need for further research in this direction."}
{"id": "2503.14779", "pdf": "https://arxiv.org/pdf/2503.14779", "abs": "https://arxiv.org/abs/2503.14779", "authors": ["Akram Khatami-Rizi", "Ahmad Mahmoudi-Aznaveh"], "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Single Image Super-Resolution (SISR) aims to reconstruct high-resolution (HR)\nimages from low-resolution (LR) inputs. Deep learning, especially Convolutional\nNeural Networks (CNNs), has advanced SISR. However, increasing network depth\nincreases parameters, and memory usage, and slows training, which is\nproblematic for resource-limited devices. To address this, lightweight models\nare developed to balance accuracy and efficiency. We propose the Involution &\nBSConv Multi-Depth Distillation Network (IBMDN), combining Involution & BSConv\nMulti-Depth Distillation Block (IBMDB) and the Contrast and High-Frequency\nAttention Block (CHFAB). IBMDB integrates Involution and BSConv to balance\ncomputational efficiency and feature extraction. CHFAB enhances high-frequency\ndetails for better visual quality. IBMDB is compatible with other SISR\narchitectures and reduces complexity, improving evaluation metrics like PSNR\nand SSIM. In transformer-based models, IBMDB reduces memory usage while\nimproving feature extraction. In GANs, it enhances perceptual quality,\nbalancing pixel-level accuracy with perceptual details. Our experiments show\nthat the method achieves high accuracy with minimal computational cost. The\ncode is available at GitHub."}
{"id": "2503.14836", "pdf": "https://arxiv.org/pdf/2503.14836", "abs": "https://arxiv.org/abs/2503.14836", "authors": ["Kunyang Li", "Jean-Charles Noirot Ferrand", "Ryan Sheatsley", "Blaine Hoak", "Yohan Beugin", "Eric Pauley", "Patrick McDaniel"], "title": "On the Robustness Tradeoff in Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Fine-tuning has become the standard practice for adapting pre-trained\n(upstream) models to downstream tasks. However, the impact on model robustness\nis not well understood. In this work, we characterize the robustness-accuracy\ntrade-off in fine-tuning. We evaluate the robustness and accuracy of fine-tuned\nmodels over 6 benchmark datasets and 7 different fine-tuning strategies. We\nobserve a consistent trade-off between adversarial robustness and accuracy.\nPeripheral updates such as BitFit are more effective for simple tasks--over 75%\nabove the average measured with area under the Pareto frontiers on CIFAR-10 and\nCIFAR-100. In contrast, fine-tuning information-heavy layers, such as attention\nlayers via Compacter, achieves a better Pareto frontier on more complex\ntasks--57.5% and 34.6% above the average on Caltech-256 and CUB-200,\nrespectively. Lastly, we observe that robustness of fine-tuning against\nout-of-distribution data closely tracks accuracy. These insights emphasize the\nneed for robustness-aware fine-tuning to ensure reliable real-world\ndeployments."}
{"id": "2503.14845", "pdf": "https://arxiv.org/pdf/2503.14845", "abs": "https://arxiv.org/abs/2503.14845", "authors": ["Yuezhen Xie", "Meiying Zhang", "Qi Hao"], "title": "ClimateGS: Real-Time Climate Simulation with 3D Gaussian Style Transfer", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Adverse climate conditions pose significant challenges for autonomous\nsystems, demanding reliable perception and decision-making across diverse\nenvironments. To better simulate these conditions, physically-based NeRF\nrendering methods have been explored for their ability to generate realistic\nscene representations. However, these methods suffer from slow rendering speeds\nand long preprocessing times, making them impractical for real-time testing and\nuser interaction. This paper presents ClimateGS, a novel framework integrating\n3D Gaussian representations with physical simulation to enable real-time\nclimate effects rendering. The novelty of this work is threefold: 1) developing\na linear transformation for 3D Gaussian photorealistic style transfer, enabling\ndirect modification of spherical harmonics across bands for efficient and\nconsistent style adaptation; 2) developing a joint training strategy for 3D\nstyle transfer, combining supervised and self-supervised learning to accelerate\nconvergence while preserving original scene details; 3) developing a real-time\nrendering method for climate simulation, integrating physics-based effects with\n3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS\non MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with\ncomparable or superior visual quality to SOTA 2D/3D methods, making it suitable\nfor interactive applications."}
{"id": "2503.14881", "pdf": "https://arxiv.org/pdf/2503.14881", "abs": "https://arxiv.org/abs/2503.14881", "authors": ["Bo Chen", "Xiaoyu Li", "Yekun Ke", "Yingyu Liang", "Zhenmei Shi", "Zhao Song"], "title": "Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "A fundamental challenge in Visual Autoregressive models is the substantial\nmemory overhead required during inference to store previously generated\nrepresentations. Despite various attempts to mitigate this issue through\ncompression techniques, prior works have not explicitly formalized the problem\nof KV-cache compression in this context. In this work, we take the first step\nin formally defining the KV-cache compression problem for Visual Autoregressive\ntransformers. We then establish a fundamental negative result, proving that any\nmechanism for sequential visual token generation under attention-based\narchitectures must use at least $\\Omega(n^2 d)$ memory, when $d = \\Omega(\\log\nn)$, where $n$ is the number of tokens generated and $d$ is the embedding\ndimensionality. This result demonstrates that achieving truly sub-quadratic\nmemory usage is impossible without additional structural constraints. Our proof\nis constructed via a reduction from a computational lower bound problem,\nleveraging randomized embedding techniques inspired by dimensionality reduction\nprinciples. Finally, we discuss how sparsity priors on visual representations\ncan influence memory efficiency, presenting both impossibility results and\npotential directions for mitigating memory overhead."}
{"id": "2503.14892", "pdf": "https://arxiv.org/pdf/2503.14892", "abs": "https://arxiv.org/abs/2503.14892", "authors": ["He Huang", "Yong Chen", "Yujun Guo", "Wei He"], "title": "Degradation Alchemy: Self-Supervised Unknown-to-Known Transformation for Blind Hyperspectral Image Fusion", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) fusion is an efficient technique that combines\nlow-resolution HSI (LR-HSI) and high-resolution multispectral images (HR-MSI)\nto generate high-resolution HSI (HR-HSI). Existing supervised learning methods\n(SLMs) can yield promising results when test data degradation matches the\ntraining ones, but they face challenges in generalizing to unknown\ndegradations. To unleash the potential and generalization ability of SLMs, we\npropose a novel self-supervised unknown-to-known degradation transformation\nframework (U2K) for blind HSI fusion, which adaptively transforms unknown\ndegradation into the same type of degradation as those handled by pre-trained\nSLMs. Specifically, the proposed U2K framework consists of: (1) spatial and\nspectral Degradation Wrapping (DW) modules that map HR-HSI to unknown degraded\nHR-MSI and LR-HSI, and (2) Degradation Transformation (DT) modules that convert\nthese wrapped data into predefined degradation patterns. The transformed HR-MSI\nand LR-HSI pairs are then processed by a pre-trained network to reconstruct the\ntarget HR-HSI. We train the U2K framework in a self-supervised manner using\nconsistency loss and greedy alternating optimization, significantly improving\nthe flexibility of blind HSI fusion. Extensive experiments confirm the\neffectiveness of our proposed U2K framework in boosting the adaptability of\nfive existing SLMs under various degradation settings and surpassing\nstate-of-the-art blind methods."}
{"id": "2503.14906", "pdf": "https://arxiv.org/pdf/2503.14906", "abs": "https://arxiv.org/abs/2503.14906", "authors": ["Yaofei Duan", "Tao Tan", "Zhiyuan Zhu", "Yuhao Huang", "Yuanji Zhang", "Rui Gao", "Patrick Cheong-Iao Pang", "Xinru Gao", "Guowei Tao", "Xiang Cong", "Zhou Li", "Lianying Liang", "Guangzhi He", "Linliang Yin", "Xuedong Deng", "Xin Yang", "Dong Ni"], "title": "FetalFlex: Anatomy-Guided Diffusion Model for Flexible Control on Fetal Ultrasound Image Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures", "summary": "Fetal ultrasound (US) examinations require the acquisition of multiple\nplanes, each providing unique diagnostic information to evaluate fetal\ndevelopment and screening for congenital anomalies. However, obtaining a\ncomprehensive, multi-plane annotated fetal US dataset remains challenging,\nparticularly for rare or complex anomalies owing to their low incidence and\nnumerous subtypes. This poses difficulties in training novice radiologists and\ndeveloping robust AI models, especially for detecting abnormal fetuses. In this\nstudy, we introduce a Flexible Fetal US image generation framework (FetalFlex)\nto address these challenges, which leverages anatomical structures and\nmultimodal information to enable controllable synthesis of fetal US images\nacross diverse planes. Specifically, FetalFlex incorporates a pre-alignment\nmodule to enhance controllability and introduces a repaint strategy to ensure\nconsistent texture and appearance. Moreover, a two-stage adaptive sampling\nstrategy is developed to progressively refine image quality from coarse to fine\nlevels. We believe that FetalFlex is the first method capable of generating\nboth in-distribution normal and out-of-distribution abnormal fetal US images,\nwithout requiring any abnormal data. Experiments on multi-center datasets\ndemonstrate that FetalFlex achieved state-of-the-art performance across\nmultiple image quality metrics. A reader study further confirms the close\nalignment of the generated results with expert visual assessments. Furthermore,\nsynthetic images by FetalFlex significantly improve the performance of six\ntypical deep models in downstream classification and anomaly detection tasks.\nLastly, FetalFlex's anatomy-level controllable generation offers a unique\nadvantage for anomaly simulation and creating paired or counterfactual data at\nthe pixel level. The demo is available at:\nhttps://dyf1023.github.io/FetalFlex/."}
{"id": "2503.14908", "pdf": "https://arxiv.org/pdf/2503.14908", "abs": "https://arxiv.org/abs/2503.14908", "authors": ["Haoyu Chen", "Xiaojie Xu", "Wenbo Li", "Jingjing Ren", "Tian Ye", "Songhua Liu", "Ying-Cong Chen", "Lei Zhu", "Xinchao Wang"], "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Poster design is a critical medium for visual communication. Prior work has\nexplored automatic poster design using deep learning techniques, but these\napproaches lack text accuracy, user customization, and aesthetic appeal,\nlimiting their applicability in artistic domains such as movies and\nexhibitions, where both clear content delivery and visual impact are essential.\nTo address these limitations, we present POSTA: a modular framework powered by\ndiffusion models and multimodal large language models (MLLMs) for customized\nartistic poster generation. The framework consists of three modules. Background\nDiffusion creates a themed background based on user input. Design MLLM then\ngenerates layout and typography elements that align with and complement the\nbackground style. Finally, to enhance the poster's aesthetic appeal, ArtText\nDiffusion applies additional stylization to key text elements. The final result\nis a visually cohesive and appealing poster, with a fully modular process that\nallows for complete customization. To train our models, we develop the\nPosterArt dataset, comprising high-quality artistic posters annotated with\nlayout, typography, and pixel-level stylized text segmentation. Our\ncomprehensive experimental analysis demonstrates POSTA's exceptional\ncontrollability and design diversity, outperforming existing models in both\ntext accuracy and aesthetic quality."}
{"id": "2503.14933", "pdf": "https://arxiv.org/pdf/2503.14933", "abs": "https://arxiv.org/abs/2503.14933", "authors": ["Yi Luo", "Hamed Hooshangnejad", "Xue Feng", "Gaofeng Huang", "Xiaojian Chen", "Rui Zhang", "Quan Chen", "Wil Ngwa", "Kai Ding"], "title": "A Language Vision Model Approach for Automated Tumor Contouring in Radiation Oncology", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "19 pages, 4 figures", "summary": "Background: Lung cancer ranks as the leading cause of cancer-related\nmortality worldwide. The complexity of tumor delineation, crucial for radiation\ntherapy, requires expertise often unavailable in resource-limited settings.\nArtificial Intelligence(AI), particularly with advancements in deep learning\n(DL) and natural language processing (NLP), offers potential solutions yet is\nchallenged by high false positive rates. Purpose: The Oncology Contouring\nCopilot (OCC) system is developed to leverage oncologist expertise for precise\ntumor contouring using textual descriptions, aiming to increase the efficiency\nof oncological workflows by combining the strengths of AI with human oversight.\nMethods: Our OCC system initially identifies nodule candidates from CT scans.\nEmploying Language Vision Models (LVMs) like GPT-4V, OCC then effectively\nreduces false positives with clinical descriptive texts, merging textual and\nvisual data to automate tumor delineation, designed to elevate the quality of\noncology care by incorporating knowledge from experienced domain experts.\nResults: Deployments of the OCC system resulted in a significant reduction in\nthe false discovery rate by 35.0%, a 72.4% decrease in false positives per\nscan, and an F1-score of 0.652 across our dataset for unbiased evaluation.\nConclusions: OCC represents a significant advance in oncology care,\nparticularly through the use of the latest LVMs to improve contouring results\nby (1) streamlining oncology treatment workflows by optimizing tumor\ndelineation, reducing manual processes; (2) offering a scalable and intuitive\nframework to reduce false positives in radiotherapy planning using LVMs; (3)\nintroducing novel medical language vision prompt techniques to minimize LVMs\nhallucinations with ablation study, and (4) conducting a comparative analysis\nof LVMs, highlighting their potential in addressing medical language vision\nchallenges."}
{"id": "2503.15008", "pdf": "https://arxiv.org/pdf/2503.15008", "abs": "https://arxiv.org/abs/2503.15008", "authors": ["Aamir Mehmood", "Yue Hu", "Saddam Hussain Khan"], "title": "A Novel Channel Boosted Residual CNN-Transformer with Regional-Boundary Learning for Breast Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "12 pages, 10 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2405.12986", "summary": "Recent advancements in detecting tumors using deep learning on breast\nultrasound images (BUSI) have demonstrated significant success. Deep CNNs and\nvision-transformers (ViTs) have demonstrated individually promising initial\nperformance. However, challenges related to model complexity and contrast,\ntexture, and tumor morphology variations introduce uncertainties that hinder\nthe effectiveness of current methods. This study introduces a novel hybrid\nframework, CB-Res-RBCMT, combining customized residual CNNs and new ViT\ncomponents for detailed BUSI cancer analysis. The proposed RBCMT uses stem\nconvolution blocks with CNN Meet Transformer (CMT) blocks, followed by new\nRegional and boundary (RB) feature extraction operations for capturing contrast\nand morphological variations. Moreover, the CMT block incorporates global\ncontextual interactions through multi-head attention, enhancing computational\nefficiency with a lightweight design. Additionally, the customized inverse\nresidual and stem CNNs within the CMT effectively extract local texture\ninformation and handle vanishing gradients. Finally, the new channel-boosted\n(CB) strategy enriches the feature diversity of the limited dataset by\ncombining the original RBCMT channels with transfer learning-based residual\nCNN-generated maps. These diverse channels are processed through a spatial\nattention block for optimal pixel selection, reducing redundancy and improving\nthe discrimination of minor contrast and texture variations. The proposed\nCB-Res-RBCMT achieves an F1-score of 95.57%, accuracy of 95.63%, sensitivity of\n96.42%, and precision of 94.79% on the standard harmonized stringent BUSI\ndataset, outperforming existing ViT and CNN methods. These results demonstrate\nthe versatility of our integrated CNN-Transformer framework in capturing\ndiverse features and delivering superior performance in BUSI cancer diagnosis."}
{"id": "2503.15029", "pdf": "https://arxiv.org/pdf/2503.15029", "abs": "https://arxiv.org/abs/2503.15029", "authors": ["Jianbo Zhao", "Taiyu Ban", "Zhihao Liu", "Hangning Zhou", "Xiyang Wang", "Qibin Zhou", "Hailong Qin", "Mu Yang", "Lei Liu", "Bin Li"], "title": "DRoPE: Directional Rotary Position Embedding for Efficient Agent Interaction Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of agent interactions is essential for\ntrajectory generation, the core of autonomous driving systems. Existing\nmethods, scene-centric, agent-centric, and query-centric frameworks, each\npresent distinct advantages and drawbacks, creating an impossible triangle\namong accuracy, computational time, and memory efficiency. To break this\nlimitation, we propose Directional Rotary Position Embedding (DRoPE), a novel\nadaptation of Rotary Position Embedding (RoPE), originally developed in natural\nlanguage processing. Unlike traditional relative position embedding (RPE),\nwhich introduces significant space complexity, RoPE efficiently encodes\nrelative positions without explicitly increasing complexity but faces inherent\nlimitations in handling angular information due to periodicity. DRoPE overcomes\nthis limitation by introducing a uniform identity scalar into RoPE's 2D rotary\ntransformation, aligning rotation angles with realistic agent headings to\nnaturally encode relative angular information. We theoretically analyze DRoPE's\ncorrectness and efficiency, demonstrating its capability to simultaneously\noptimize trajectory generation accuracy, time complexity, and space complexity.\nEmpirical evaluations compared with various state-of-the-art trajectory\ngeneration models, confirm DRoPE's good performance and significantly reduced\nspace complexity, indicating both theoretical soundness and practical\neffectiveness. The video documentation is available at\nhttps://drope-traj.github.io/."}
{"id": "2503.15058", "pdf": "https://arxiv.org/pdf/2503.15058", "abs": "https://arxiv.org/abs/2503.15058", "authors": ["Francesco Di Feola", "Ludovica Pompilio", "Cecilia Assolito", "Valerio Guarrasi", "Paolo Soda"], "title": "Texture-Aware StarGAN for CT data harmonisation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Computed Tomography (CT) plays a pivotal role in medical diagnosis; however,\nvariability across reconstruction kernels hinders data-driven approaches, such\nas deep learning models, from achieving reliable and generalized performance.\nTo this end, CT data harmonization has emerged as a promising solution to\nminimize such non-biological variances by standardizing data across different\nsources or conditions. In this context, Generative Adversarial Networks (GANs)\nhave proved to be a powerful framework for harmonization, framing it as a\nstyle-transfer problem. However, GAN-based approaches still face limitations in\ncapturing complex relationships within the images, which are essential for\neffective harmonization. In this work, we propose a novel texture-aware StarGAN\nfor CT data harmonization, enabling one-to-many translations across different\nreconstruction kernels. Although the StarGAN model has been successfully\napplied in other domains, its potential for CT data harmonization remains\nunexplored. Furthermore, our approach introduces a multi-scale texture loss\nfunction that embeds texture information across different spatial and angular\nscales into the harmonization process, effectively addressing kernel-induced\ntexture variations. We conducted extensive experimentation on a publicly\navailable dataset, utilizing a total of 48667 chest CT slices from 197 patients\ndistributed over three different reconstruction kernels, demonstrating the\nsuperiority of our method over the baseline StarGAN."}
{"id": "2503.15168", "pdf": "https://arxiv.org/pdf/2503.15168", "abs": "https://arxiv.org/abs/2503.15168", "authors": ["Javier Del Ser", "Jesus L. Lobo", "Heimo M√ºller", "Andreas Holzinger"], "title": "World Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child", "categories": ["cs.AI", "cs.CV", "cs.ET", "cs.LG", "68T05"], "comment": "11 pages, 1 figure", "summary": "World Models help Artificial Intelligence (AI) predict outcomes, reason about\nits environment, and guide decision-making. While widely used in reinforcement\nlearning, they lack the structured, adaptive representations that even young\nchildren intuitively develop. Advancing beyond pattern recognition requires\ndynamic, interpretable frameworks inspired by Piaget's cognitive development\ntheory. We highlight six key research areas -- physics-informed learning,\nneurosymbolic learning, continual learning, causal inference, human-in-the-loop\nAI, and responsible AI -- as essential for enabling true reasoning in AI. By\nintegrating statistical learning with advances in these areas, AI can evolve\nfrom pattern recognition to genuine understanding, adaptation and reasoning\ncapabilities."}
{"id": "2503.15176", "pdf": "https://arxiv.org/pdf/2503.15176", "abs": "https://arxiv.org/abs/2503.15176", "authors": ["Navya Sonal Agarwal", "Sanjay Kumar Sonbhadra"], "title": "A Review on Large Language Models for Visual Analytics", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper provides a comprehensive review of the integration of Large\nLanguage Models (LLMs) with visual analytics, addressing their foundational\nconcepts, capabilities, and wide-ranging applications. It begins by outlining\nthe theoretical underpinnings of visual analytics and the transformative\npotential of LLMs, specifically focusing on their roles in natural language\nunderstanding, natural language generation, dialogue systems, and text-to-media\ntransformations. The review further investigates how the synergy between LLMs\nand visual analytics enhances data interpretation, visualization techniques,\nand interactive exploration capabilities. Key tools and platforms including\nLIDA, Chat2VIS, Julius AI, and Zoho Analytics, along with specialized\nmultimodal models such as ChartLlama and CharXIV, are critically evaluated. The\npaper discusses their functionalities, strengths, and limitations in supporting\ndata exploration, visualization enhancement, automated reporting, and insight\nextraction. The taxonomy of LLM tasks, ranging from natural language\nunderstanding (NLU), natural language generation (NLG), to dialogue systems and\ntext-to-media transformations, is systematically explored. This review provides\na SWOT analysis of integrating Large Language Models (LLMs) with visual\nanalytics, highlighting strengths like accessibility and flexibility,\nweaknesses such as computational demands and biases, opportunities in\nmultimodal integration and user collaboration, and threats including privacy\nconcerns and skill degradation. It emphasizes addressing ethical considerations\nand methodological improvements for effective integration."}
{"id": "2503.15288", "pdf": "https://arxiv.org/pdf/2503.15288", "abs": "https://arxiv.org/abs/2503.15288", "authors": ["Justin Le Lou√´dec", "Maike Bauer", "Tanja Amerstorfer", "Jackie A. Davies"], "title": "Beacon2Science: Enhancing STEREO/HI beacon data1 with machine learning for efficient CME tracking", "categories": ["physics.space-ph", "cs.CV", "cs.LG"], "comment": "24 pages, 11 figures, 1 tables, submitted to AGU Space Weather on\n  14th Marc 2025", "summary": "Observing and forecasting coronal mass ejections (CME) in real-time is\ncrucial due to the strong geomagnetic storms they can generate that can have a\npotentially damaging effect, for example, on satellites and electrical devices.\nWith its near-real-time availability, STEREO/HI beacon data is the perfect\ncandidate for early forecasting of CMEs. However, previous work concluded that\nCME arrival prediction based on beacon data could not achieve the same accuracy\nas with high-resolution science data due to data gaps and lower quality. We\npresent our novel pipeline entitled ''Beacon2Science'', bridging the gap\nbetween beacon and science data to improve CME tracking. Through this pipeline,\nwe first enhance the quality (signal-to-noise ratio and spatial resolution) of\nbeacon data. We then increase the time resolution of enhanced beacon images\nthrough learned interpolation to match science data's 40-minute resolution. We\nmaximize information coherence between consecutive frames with adapted model\narchitecture and loss functions through the different steps. The improved\nbeacon images are comparable to science data, showing better CME visibility\nthan the original beacon data. Furthermore, we compare CMEs tracked in beacon,\nenhanced beacon, and science images. The tracks extracted from enhanced beacon\ndata are closer to those from science images, with a mean average error of\n$\\sim 0.5 ^\\circ$ of elongation compared to $1^\\circ$ with original beacon\ndata. The work presented in this paper paves the way for its application to\nforthcoming missions such as Vigil and PUNCH."}
{"id": "2503.15321", "pdf": "https://arxiv.org/pdf/2503.15321", "abs": "https://arxiv.org/abs/2503.15321", "authors": ["Euclid Collaboration", "G. Stevens", "S. Fotopoulou", "M. N. Bremer", "T. Matamoro Zatarain", "K. Jahnke", "B. Margalef-Bentabol", "M. Huertas-Company", "M. J. Smith", "M. Walmsley", "M. Salvato", "M. Mezcua", "A. Paulino-Afonso", "M. Siudek", "M. Talia", "F. Ricci", "W. Roster", "N. Aghanim", "B. Altieri", "S. Andreon", "H. Aussel", "C. Baccigalupi", "M. Baldi", "S. Bardelli", "P. Battaglia", "A. Biviano", "A. Bonchi", "E. Branchini", "M. Brescia", "J. Brinchmann", "S. Camera", "G. Ca√±as-Herrera", "V. Capobianco", "C. Carbone", "J. Carretero", "M. Castellano", "G. Castignani", "S. Cavuoti", "K. C. Chambers", "A. Cimatti", "C. Colodro-Conde", "G. Congedo", "C. J. Conselice", "L. Conversi", "Y. Copin", "A. Costille", "F. Courbin", "H. M. Courtois", "M. Cropper", "A. Da Silva", "H. Degaudenzi", "G. De Lucia", "C. Dolding", "H. Dole", "M. Douspis", "F. Dubath", "X. Dupac", "S. Dusini", "S. Escoffier", "M. Farina", "S. Ferriol", "K. George", "C. Giocoli", "B. R. Granett", "A. Grazian", "F. Grupp", "S. V. H. Haugan", "I. M. Hook", "F. Hormuth", "A. Hornstrup", "P. Hudelot", "M. Jhabvala", "E. Keih√§nen", "S. Kermiche", "A. Kiessling", "M. Kilbinger", "B. Kubik", "M. K√ºmmel", "H. Kurki-Suonio", "Q. Le Boulc'h", "A. M. C. Le Brun", "D. Le Mignant", "P. B. Lilje", "V. Lindholm", "I. Lloro", "G. Mainetti", "D. Maino", "E. Maiorano", "O. Marggraf", "M. Martinelli", "N. Martinet", "F. Marulli", "R. Massey", "S. Maurogordato", "H. J. McCracken", "E. Medinaceli", "S. Mei", "M. Melchior", "M. Meneghetti", "E. Merlin", "G. Meylan", "A. Mora", "M. Moresco", "L. Moscardini", "R. Nakajima", "C. Neissner", "S. -M. Niemi", "C. Padilla", "S. Paltani", "F. Pasian", "K. Pedersen", "W. J. Percival", "V. Pettorino", "G. Polenta", "M. Poncet", "L. A. Popa", "L. Pozzetti", "F. Raison", "R. Rebolo", "A. Renzi", "J. Rhodes", "G. Riccio", "E. Romelli", "M. Roncarelli", "R. Saglia", "A. G. S√°nchez", "D. Sapone", "J. A. Schewtschenko", "M. Schirmer", "P. Schneider", "T. Schrabback", "A. Secroun", "S. Serrano", "P. Simon", "C. Sirignano", "G. Sirri", "J. Skottfelt", "L. Stanco", "J. Steinwagner", "P. Tallada-Cresp√≠", "A. N. Taylor", "I. Tereno", "S. Toft", "R. Toledo-Moreo", "F. Torradeflot", "I. Tutusaus", "L. Valenziano", "J. Valiviita", "T. Vassallo", "G. Verdoes Kleijn", "A. Veropalumbo", "Y. Wang", "J. Weller", "A. Zacchei", "G. Zamorani", "F. M. Zerbi", "I. A. Zinchenko", "E. Zucca", "V. Allevato", "M. Ballardini", "M. Bolzonella", "E. Bozzo", "C. Burigana", "R. Cabanac", "A. Cappi", "J. A. Escartin Vigo", "L. Gabarra", "W. G. Hartley", "J. Mart√≠n-Fleitas", "S. Matthew", "R. B. Metcalf", "A. Pezzotta", "M. P√∂ntinen", "I. Risso", "V. Scottez", "M. Sereno", "M. Tenti", "M. Wiesmann", "Y. Akrami", "S. Alvi", "I. T. Andika", "S. Anselmi", "M. Archidiacono", "F. Atrio-Barandela", "D. Bertacca", "M. Bethermin", "L. Bisigello", "A. Blanchard", "L. Blot", "S. Borgani", "M. L. Brown", "S. Bruton", "A. Calabro", "F. Caro", "T. Castro", "F. Cogato", "S. Davini", "G. Desprez", "A. D√≠az-S√°nchez", "J. J. Diaz", "S. Di Domizio", "J. M. Diego", "P. -A. Duc", "A. Enia", "Y. Fang", "A. G. Ferrari", "A. Finoguenov", "A. Fontana", "A. Franco", "J. Garc√≠a-Bellido", "T. Gasparetto", "V. Gautard", "E. Gaztanaga", "F. Giacomini", "F. Gianotti", "M. Guidi", "C. M. Gutierrez", "A. Hall", "S. Hemmati", "H. Hildebrandt", "J. Hjorth", "J. J. E. Kajava", "Y. Kang", "V. Kansal", "D. Karagiannis", "C. C. Kirkpatrick", "S. Kruk", "L. Legrand", "M. Lembo", "F. Lepori", "G. Leroy", "J. Lesgourgues", "L. Leuzzi", "T. I. Liaudat", "J. Macias-Perez", "M. Magliocchetti", "F. Mannucci", "R. Maoli", "C. J. A. P. Martins", "L. Maurin", "M. Miluzio", "P. Monaco", "G. Morgante", "K. Naidoo", "A. Navarro-Alsina", "F. Passalacqua", "K. Paterson", "L. Patrizii", "A. Pisani", "D. Potter", "S. Quai", "M. Radovich", "P. -F. Rocci", "G. Rodighiero", "S. Sacquegna", "M. Sahl√©n", "D. B. Sanders", "E. Sarpa", "A. Schneider", "M. Schultheis", "D. Sciotti", "E. Sellentin", "F. Shankar", "L. C. Smith", "K. Tanidis", "G. Testera", "R. Teyssier", "S. Tosi", "A. Troja", "M. Tucci", "C. Valieri", "D. Vergani", "G. Verza", "N. A. Walton"], "title": "Euclid Quick Data Release (Q1). Active galactic nuclei identification using diffusion-based inpainting of Euclid VIS images", "categories": ["astro-ph.GA", "cs.CV"], "comment": "Paper submitted as part of the A&A Special Issue `Euclid Quick Data\n  Release (Q1)', 32 pages, 26 figures", "summary": "Light emission from galaxies exhibit diverse brightness profiles, influenced\nby factors such as galaxy type, structural features and interactions with other\ngalaxies. Elliptical galaxies feature more uniform light distributions, while\nspiral and irregular galaxies have complex, varied light profiles due to their\nstructural heterogeneity and star-forming activity. In addition, galaxies with\nan active galactic nucleus (AGN) feature intense, concentrated emission from\ngas accretion around supermassive black holes, superimposed on regular galactic\nlight, while quasi-stellar objects (QSO) are the extreme case of the AGN\nemission dominating the galaxy. The challenge of identifying AGN and QSO has\nbeen discussed many times in the literature, often requiring multi-wavelength\nobservations. This paper introduces a novel approach to identify AGN and QSO\nfrom a single image. Diffusion models have been recently developed in the\nmachine-learning literature to generate realistic-looking images of everyday\nobjects. Utilising the spatial resolving power of the Euclid VIS images, we\ncreated a diffusion model trained on one million sources, without using any\nsource pre-selection or labels. The model learns to reconstruct light\ndistributions of normal galaxies, since the population is dominated by them. We\ncondition the prediction of the central light distribution by masking the\ncentral few pixels of each source and reconstruct the light according to the\ndiffusion model. We further use this prediction to identify sources that\ndeviate from this profile by examining the reconstruction error of the few\ncentral pixels regenerated in each source's core. Our approach, solely using\nVIS imaging, features high completeness compared to traditional methods of AGN\nand QSO selection, including optical, near-infrared, mid-infrared, and X-rays.\n[abridged]"}
{"id": "2503.15352", "pdf": "https://arxiv.org/pdf/2503.15352", "abs": "https://arxiv.org/abs/2503.15352", "authors": ["Abhi Kamboj", "Minh N. Do"], "title": "Leveraging Perfect Multimodal Alignment and Gaussian Assumptions for Cross-modal Transfer", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "Multimodal alignment aims to construct a joint latent vector space where two\nmodalities representing the same concept map to the same vector. We formulate\nthis as an inverse problem and show that under certain conditions perfect\nalignment can be achieved. We then address a specific application of alignment\nreferred to as cross-modal transfer. Unsupervised cross-modal transfer aims to\nleverage a model trained with one modality to perform inference on another\nmodality, without any labeled fine-tuning on the new modality. Assuming that\nsemantic classes are represented as a mixture of Gaussians in the latent space,\nwe show how cross-modal transfer can be performed by projecting the data points\nfrom the representation space onto different subspaces representing each\nmodality. Our experiments on synthetic multimodal Gaussian data verify the\neffectiveness of our perfect alignment and cross-modal transfer method. We hope\nthese findings inspire further exploration of the applications of perfect\nalignment and the use of Gaussian models for cross-modal learning."}
{"id": "2503.15358", "pdf": "https://arxiv.org/pdf/2503.15358", "abs": "https://arxiv.org/abs/2503.15358", "authors": ["Thomas Pickard", "Aline Villavicencio", "Maggie Mi", "Wei He", "Dylan Phelps", "Carolina Scarton", "Marco Idiart"], "title": "SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation", "categories": ["cs.CL", "cs.CV", "I.2.7; I.4.m"], "comment": "Preprint; SemEval-2025 proceedings to appear at ACL 2025", "summary": "Idiomatic expressions present a unique challenge in NLP, as their meanings\nare often not directly inferable from their constituent words. Despite recent\nadvancements in Large Language Models (LLMs), idiomaticity remains a\nsignificant obstacle to robust semantic representation. We present datasets and\ntasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity\nRepresentation), which challenges the community to assess and improve models'\nability to interpret idiomatic expressions in multimodal contexts and in\nmultiple languages. Participants competed in two subtasks: ranking images based\non their alignment with idiomatic or literal meanings, and predicting the next\nimage in a sequence. The most effective methods achieved human-level\nperformance by leveraging pretrained LLMs and vision-language models in\nmixture-of-experts settings, with multiple queries used to smooth over the\nweaknesses in these models' representations of idiomaticity."}
{"id": "2503.15390", "pdf": "https://arxiv.org/pdf/2503.15390", "abs": "https://arxiv.org/abs/2503.15390", "authors": ["Yumin Zhang", "Yan Gao", "Haoran Duan", "Hanqing Guo", "Tejal Shah", "Rajiv Ranjan", "Bo Wei"], "title": "FedSCA: Federated Tuning with Similarity-guided Collaborative Aggregation for Heterogeneous Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Transformer-based foundation models (FMs) have recently demonstrated\nremarkable performance in medical image segmentation. However, scaling these\nmodels is challenging due to the limited size of medical image datasets within\nisolated hospitals, where data centralization is restricted due to privacy\nconcerns. These constraints, combined with the data-intensive nature of FMs,\nhinder their broader application. Integrating federated learning (FL) with\nfoundation models (FLFM) fine-tuning offers a potential solution to these\nchallenges by enabling collaborative model training without data sharing, thus\nallowing FMs to take advantage of a diverse pool of sensitive medical image\ndata across hospitals/clients. However, non-independent and identically\ndistributed (non-IID) data among clients, paired with computational and\ncommunication constraints in federated environments, presents an additional\nchallenge that limits further performance improvements and remains inadequately\naddressed in existing studies. In this work, we propose a novel FLFM\nfine-tuning framework, \\underline{\\textbf{Fed}}erated tuning with\n\\underline{\\textbf{S}}imilarity-guided \\underline{\\textbf{C}}ollaborative\n\\underline{\\textbf{A}}ggregation (FedSCA), encompassing all phases of the FL\nprocess. This includes (1) specially designed parameter-efficient fine-tuning\n(PEFT) for local client training to enhance computational efficiency; (2)\npartial low-level adapter transmission for communication efficiency; and (3)\nsimilarity-guided collaborative aggregation (SGCA) on the server side to\naddress non-IID issues. Extensive experiments on three FL benchmarks for\nmedical image segmentation demonstrate the effectiveness of our proposed\nFedSCA, establishing new SOTA performance."}
{"id": "2503.15402", "pdf": "https://arxiv.org/pdf/2503.15402", "abs": "https://arxiv.org/abs/2503.15402", "authors": ["Alejandro Peque√±o-Zurro", "Lyes Khacef", "Stefano Panzeri", "Elisabetta Chicca"], "title": "Towards efficient keyword spotting using spike-based time difference encoders", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.ET"], "comment": "26 pages, 9 figures", "summary": "Keyword spotting in edge devices is becoming increasingly important as\nvoice-activated assistants are widely used. However, its deployment is often\nlimited by the extreme low-power constraints of the target embedded systems.\nHere, we explore the Temporal Difference Encoder (TDE) performance in keyword\nspotting. This recent neuron model encodes the time difference in instantaneous\nfrequency and spike count to perform efficient keyword spotting with\nneuromorphic processors. We use the TIdigits dataset of spoken digits with a\nformant decomposition and rate-based encoding into spikes. We compare three\nSpiking Neural Networks (SNNs) architectures to learn and classify\nspatio-temporal signals. The proposed SNN architectures are made of three\nlayers with variation in its hidden layer composed of either (1) feedforward\nTDE, (2) feedforward Current-Based Leaky Integrate-and-Fire (CuBa-LIF), or (3)\nrecurrent CuBa-LIF neurons. We first show that the spike trains of the\nfrequency-converted spoken digits have a large amount of information in the\ntemporal domain, reinforcing the importance of better exploiting temporal\nencoding for such a task. We then train the three SNNs with the same number of\nsynaptic weights to quantify and compare their performance based on the\naccuracy and synaptic operations. The resulting accuracy of the feedforward TDE\nnetwork (89%) is higher than the feedforward CuBa-LIF network (71%) and close\nto the recurrent CuBa-LIF network (91%). However, the feedforward TDE-based\nnetwork performs 92% fewer synaptic operations than the recurrent CuBa-LIF\nnetwork with the same amount of synapses. In addition, the results of the TDE\nnetwork are highly interpretable and correlated with the frequency and\ntimescale features of the spoken keywords in the dataset. Our findings suggest\nthat the TDE is a promising neuron model for scalable event-driven processing\nof spatio-temporal patterns."}
{"id": "2503.15414", "pdf": "https://arxiv.org/pdf/2503.15414", "abs": "https://arxiv.org/abs/2503.15414", "authors": ["Can Peng", "Qianhui Men", "Pramit Saha", "Qianye Yang", "Cheng Ouyang", "J. Alison Noble"], "title": "Federated Continual 3D Segmentation With Single-round Communication", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Federated learning seeks to foster collaboration among distributed clients\nwhile preserving the privacy of their local data. Traditionally, federated\nlearning methods assume a fixed setting in which client data and learning\nobjectives remain constant. However, in real-world scenarios, new clients may\njoin, and existing clients may expand the segmentation label set as task\nrequirements evolve. In such a dynamic federated analysis setup, the\nconventional federated communication strategy of model aggregation per\ncommunication round is suboptimal. As new clients join, this strategy requires\nretraining, linearly increasing communication and computation overhead. It also\nimposes requirements for synchronized communication, which is difficult to\nachieve among distributed clients. In this paper, we propose a federated\ncontinual learning strategy that employs a one-time model aggregation at the\nserver through multi-model distillation. This approach builds and updates the\nglobal model while eliminating the need for frequent server communication. When\nintegrating new data streams or onboarding new clients, this approach\nefficiently reuses previous client models, avoiding the need to retrain the\nglobal model across the entire federation. By minimizing communication load and\nbypassing the need to put unchanged clients online, our approach relaxes\nsynchronization requirements among clients, providing an efficient and scalable\nfederated analysis framework suited for real-world applications. Using\nmulti-class 3D abdominal CT segmentation as an application task, we demonstrate\nthe effectiveness of the proposed approach."}
{"id": "2503.15420", "pdf": "https://arxiv.org/pdf/2503.15420", "abs": "https://arxiv.org/abs/2503.15420", "authors": ["Amirhossein Kazerouni", "Soroush Mehraban", "Michael Brudno", "Babak Taati"], "title": "LIFT: Latent Implicit Functions for Task- and Data-Agnostic Encoding", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) are proving to be a powerful paradigm\nin unifying task modeling across diverse data domains, offering key advantages\nsuch as memory efficiency and resolution independence. Conventional deep\nlearning models are typically modality-dependent, often requiring custom\narchitectures and objectives for different types of signals. However, existing\nINR frameworks frequently rely on global latent vectors or exhibit\ncomputational inefficiencies that limit their broader applicability. We\nintroduce LIFT, a novel, high-performance framework that addresses these\nchallenges by capturing multiscale information through meta-learning. LIFT\nleverages multiple parallel localized implicit functions alongside a\nhierarchical latent generator to produce unified latent representations that\nspan local, intermediate, and global features. This architecture facilitates\nsmooth transitions across local regions, enhancing expressivity while\nmaintaining inference efficiency. Additionally, we introduce ReLIFT, an\nenhanced variant of LIFT that incorporates residual connections and expressive\nfrequency encodings. With this straightforward approach, ReLIFT effectively\naddresses the convergence-capacity gap found in comparable methods, providing\nan efficient yet powerful solution to improve capacity and speed up\nconvergence. Empirical results show that LIFT achieves state-of-the-art (SOTA)\nperformance in generative modeling and classification tasks, with notable\nreductions in computational costs. Moreover, in single-task settings, the\nstreamlined ReLIFT architecture proves effective in signal representations and\ninverse problem tasks."}
