<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 41]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估框架LEGO-Eval，旨在更精确地评估三维场景与细粒度指令的对齐度，并介绍了配套的LEGO-Bench基准测试集。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型生成的3D场景缺乏真实空间布局和对象属性，且现有评估方法无法可靠评估场景与指令的对齐度。

Method: 引入了LEGO-Eval框架，具备多样化工具以显式地基础场景组件，并发布了包含复杂布局和真实环境属性的LEGO-Bench基准测试集。

Result: 实验表明，LEGO-Eval在场景-指令对齐评估中的F1得分比VLM-as-a-judge高出0.41。LEGO-Bench的评估结果显示，当前生成方法在生成完全符合细粒度指令的场景方面成功率最高仅为10%。

Conclusion: LEGO-Eval和LEGO-Bench为提升3D场景生成的真实性和对齐度提供了新的评估手段和基准，揭示了当前生成方法的显著局限性。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [2] [Targeted Error Correction in Knowledge Distillation: Small Language Models Surpass GPT](https://arxiv.org/abs/2511.03005)
*Hee-Jin Lee,Zhen Guo,Luchao Jin,Morteza Moazami Goudarzi*

Main category: cs.CL

TL;DR: 介绍了一种Analyze-Revise-Finetune (ARF)流程，使小型开源语言模型在客户服务摘要任务中超越大型专有模型。


<details>
  <summary>Details</summary>
Motivation: 提升小型开源语言模型在客户服务摘要任务中的表现，以在成本效率和数据隐私方面取得优势。

Method: 通过ARF流程，首先分析和分类教师模型（GPT-3.5）生成的摘要中的常见错误，然后使用紧凑的编辑模型（Llama 3.1 70B）进行有针对性的修订，从而生成高质量的训练数据，最后对小型学生模型（Llama 3.1 8B）进行微调。

Result: 经过ARF流程微调的小型模型在摘要任务中表现优于GPT-3.5，实现了更高的成本效率和数据隐私保护。

Conclusion: ARF流程为提升开源语言模型在多样化下游应用中的表现提供了一个可推广的框架。

Abstract: We introduce an Analyze-Revise-Finetune (ARF) pipeline that enables smaller
open-source language models (LLMs) to surpass substantially larger proprietary
models in customer service summarization tasks. The pipeline first analyzes and
categorizes common errors in summaries produced by a teacher model (GPT-3.5),
then performs a targeted revision using a compact editor model (Llama 3.1 70B)
to generate high-quality, refined training data. Fine-tuning a smaller student
model (Llama 3.1 8B) on this refined data resulted in superior summarization
performance compared to GPT-3.5. The ARF pipeline improves cost efficiency and
data privacy while maintaining competitive accuracy, illustrating a
generalizable framework for enhancing open-source LLMs across diverse
downstream applications.

</details>


### [3] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 提出了一种新的ABSA评价方法FTS-OBP，研究了小模型在ABSA任务中的表现，并发布了教育资源数据集。


<details>
  <summary>Details</summary>
Motivation: ABSA研究和资源主要集中在商业领域，教育、医疗等低资源领域的需求未得到满足，且现有方法依赖资源密集型知识注入，传统评价方法过于严格。

Method: 1) 提出FTS-OBP评价方法；2) 进行小模型在ABSA任务中的首次研究，探索无数据和轻量微调方法，并提出多任务微调策略；3) 发布首个公开教育资源ABSA数据集。

Result: 1) FTS-OBP方法能适应实际提取边界变化，与传统指标有强相关性；2) 1.5-3.8B参数的小模型在多任务微调策略下表现超越大模型，仅用200-1,000个样本即可达到基准结果；3) 发布教育资源数据集。

Conclusion: 通过新评价方法、小模型优化和发布教育资源数据集，有效解决了ABSA在低资源领域的挑战，为未来研究提供了支持。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [4] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: ROBOTO2是一个开源的、基于Web的平台，通过大语言模型辅助临床实验的偏倚风险评估（ROB2）。


<details>
  <summary>Details</summary>
Motivation: 传统ROB2标注过程非常耗时，需要一种更高效的方法来简化这一过程。

Method: ROBOTO2结合了PDF解析、增强检索LLM提示和人工实时审核，用户可上传临床实验报告，获取ROB2信号问题的初步答案和支持证据，并实时反馈或纠正系统建议。

Result: ROBOTO2公开可用，并发布了一个包含521个儿科临床实验报告的数据集（8954个信号问题和1202个证据片段），通过人工和LLM辅助方法标注，作为基准测试。

Conclusion: 该平台提升了ROB2评估的效率，并通过基准测试分析了四个LLM模型的表现，指出了自动化该关键任务中的现有能力和持续挑战。

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [5] [Reading Between the Lines: The One-Sided Conversation Problem](https://arxiv.org/abs/2511.03056)
*Victoria Ebert,Rishabh Singh,Tuochao Chen,Noah A. Smith,Shyamnath Gollakota*

Main category: cs.CL

TL;DR: 这篇论文介绍了一种新颖的挑战，即单侧对话问题（1SC），并提出了解决方案，能够在仅记录对话一方的情况下推断和学习对话内容。


<details>
  <summary>Details</summary>
Motivation: 许多真实世界场景（如远程医疗、呼叫中心和智能眼镜）中只能记录对话一方，因此需要研究如何在仅有单侧对话记录的情况下进行推断和学习。

Method: 研究包括两项任务：(1) 实时重建缺失说话者的对话内容，(2) 从单侧对话记录中生成摘要。使用MultiWOZ、DailyDialog和Candor数据集，通过提示和微调模型进行评估，并采用人工A/B测试和LLM-as-a-judge指标。

Result: 研究发现，获取一次未来对话及关于话语长度的信息能够改善重建效果，占位符提示有助于减少幻觉，大模型通过提示可以生成有希望的重建，而小模型需要微调。此外，无需重建缺失对话即可生成高质量摘要。

Conclusion: 1SC作为一个新挑战，展示了有希望的结果，是迈向隐私保护对话AI的一步。

Abstract: Conversational AI is constrained in many real-world settings where only one
side of a dialogue can be recorded, such as telemedicine, call centers, and
smart glasses. We formalize this as the one-sided conversation problem (1SC):
inferring and learning from one side of a conversation. We study two tasks: (1)
reconstructing the missing speaker's turns for real-time use cases, and (2)
generating summaries from one-sided transcripts. Evaluating prompting and
finetuned models on MultiWOZ, DailyDialog, and Candor with both human A/B
testing and LLM-as-a-judge metrics, we find that access to one future turn and
information about utterance length improves reconstruction, placeholder
prompting helps to mitigate hallucination, and while large models generate
promising reconstructions with prompting, smaller models require finetuning.
Further, high-quality summaries can be generated without reconstructing missing
turns. We present 1SC as a novel challenge and report promising results that
mark a step toward privacy-aware conversational AI.

</details>


### [6] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: 提出PolyNorm，一种使用大型语言模型（LLMs）的基于提示的文本标准化（TN）方法，旨在减少人工规则依赖，提升多语言适用性。


<details>
  <summary>Details</summary>
Motivation: 传统TN系统准确率高但工程量大、难以扩展，且在低资源语言中表现不佳，需减少人工干预并提升多语言覆盖。

Method: 采用LLMs的提示学习方法（PolyNorm），并提出自动化数据整理与评估流程，支持多语言实验。

Result: 在八种语言实验中，相比生产级系统，词错误率（WER）持续下降。

Conclusion: PolyNorm通过减少人工规则依赖和发布多语言数据集PolyNorm-Benchmark，支持了更广泛的语言标准化研究。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [7] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 该研究利用计算语言学方法，通过惊讶度和语义连贯性两个指标，分析精神分裂症患者的语言障碍。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症患者常有语言混乱和语篇连贯性受损，这些语言障碍反映了潜在的认知障碍，并可能成为症状严重程度和诊断的客观标记。

Method: 通过计算模型计算语言的惊讶度和语义连贯性，并比较精神分裂症患者与健康对照组之间的差异。

Result: 研究发现，精神分裂症患者与对照组在惊讶度和语义连贯性方面存在显著差异，并且这些语言障碍随着症状严重程度的变化而变化。

Conclusion: 惊讶度和语义连贯性可以作为精神分裂症症状严重程度和诊断的客观标记。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [8] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: 本文介绍了CARMA，首个大规模自动注释的阿拉伯语Reddit帖子数据集，用于六种心理健康状况的检测。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍在全球范围内影响数百万人，但早期检测仍然是一个重大挑战，尤其是在阿拉伯语言群体中，资源有限且文化污名化使心理健康讨论受限。

Method: 提出并构建了一个名为CARMA的大规模自动注释数据集，涵盖六种心理健康状况及一个对照组，并进行了定性和定量分析，包括词汇和语义差异，以及使用多种模型进行分类实验。

Result: CARMA在规模和多样性上超越了现有资源，分类实验展示了其在大规模心理健康检测中的潜力。

Conclusion: 该数据集展示了在未充分研究语言（如阿拉伯语）中推进心理健康检测的潜力。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [9] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 提出了一种基于控制屏障函数（CBF）的框架，用于对齐大型语言模型（LLMs），以确保生成用户期望的文本。


<details>
  <summary>Details</summary>
Motivation: 确保生成文本符合用户期望，通过安全过滤器干预生成文本，避免对基础模型进行微调。

Method: 应用CBF安全过滤器到基础LLM生成的预测token，安全过滤器为附加类型，无需微调基础模型，并可直接应用评估模型到过滤器设计。

Result: 实现了基于开源语言模型的文本生成系统，目标是生成积极文本。

Conclusion: 该框架提供了一种有效且灵活的方式，用于对齐大型语言模型的文本生成，同时避免了模型微调的复杂性和成本。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [10] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出了MME-CC，一个以视觉为基础的多模态认知能力评估基准，包含11个具有代表性的推理任务，分为空间、几何和基于知识的推理三大类。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准过于强调文本推理，未能系统捕捉以视觉为中心的认知行为，导致无法充分评估MLLMs的认知能力。

Method: 引入MME-CC基准，对16个典型MLLMs进行了广泛实验，分析模型在空间、几何和知识推理方面的表现，并识别常见错误模式。

Result: 闭源模型在整体表现上领先，而空间和几何推理普遍较弱，发现包括方向错误、跨视图身份保持能力差和对反事实指令遵循不佳等常见错误模式。

Conclusion: MME-CC揭示了MLLMs在认知能力上的不足，并呼吁在评估和模型设计中更加重视认知能力。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [11] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本文提出了一种利用大语言模型（LLMs）作为判断者，预测和解释AI系统中风险的框架，旨在评估不同利益相关者的风险感知。


<details>
  <summary>Details</summary>
Motivation: 理解不同利益相关者如何看待AI系统中的风险，对于其负责任的部署至关重要。

Method: 采用Risk Atlas Nexus和GloVE解释方法，通过大语言模型生成特定于利益相关者的可解释策略，并通过三个实际AI用例进行演示。

Result: 结果显示，利益相关者的观点显著影响风险感知和冲突模式，并且提出的互动可视化揭示了冲突产生的原因。

Conclusion: 该框架强调了考虑利益相关者的观点对LLM评估的必要性，使其更加透明、可解释，并与以人为本的AI治理目标相一致。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [12] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文对大语言模型中不确定性估计的多种方法进行了实证研究，发现不同方法在分布内和分布外数据上的表现各异。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型输出的可信度至关重要，不确定性估计在其中扮演关键角色。

Method: 研究了十二种不确定性估计方法，并使用四种生成质量指标，评估在问答任务中大语言模型生成答案的不确定性。

Result: 信息基方法在分布内数据上表现优异，密度基方法和P(True)指标在分布外数据上表现更佳，语义一致性方法在不同数据集上表现可靠。

Conclusion: 不同不确定性估计方法在不同数据分布下各有优劣，选择合适的方法取决于具体应用场景。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [13] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: 介绍BengaliMoralBench，首个针对孟加拉语及社会文化背景的大规模伦理基准，涵盖5个道德领域和50个相关子主题，对多语言大模型进行系统性零-shot评估。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在南亚越来越受欢迎，但针对孟加拉语等语言的本地伦理标准仍缺乏探索。现有的伦理基准主要基于英语和西方框架，忽视了文化差异。

Method: 构建了BengaliMoralBench，包括5个道德领域和50个子主题，通过母语者共识标注，并使用三种伦理视角：美德伦理、常识伦理和正义伦理。采用统一的提示协议和标准指标，对主流多语言大模型进行零-shot评估。

Result: 模型在基准测试中表现差异较大（准确率50-91%），定性分析显示模型在文化基础、常识推理和道德公平性方面存在持续弱点。

Conclusion: BengaliMoralBench为负责任的本地化提供了基础，支持在孟加拉国等多元、低资源多语言环境中进行符合文化的评估，助力于部署道德上更稳健的AI。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [14] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 提出语言图模型（LGM）以增强概念清晰度，通过提取元关系并使用概念迭代检索算法，改进大语言模型处理模糊或概念不一致术语的能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在用户指令涉及模糊或概念不一致的术语时表现不佳，因此需要提升其语义理解能力。

Method: LGM提取自然语言中的元关系（继承、别名和组合），采用反射机制验证这些元关系，并使用概念迭代检索算法动态提供给LLM。

Result: 实验证明，LGM在标准基准测试中始终优于现有的RAG基线方法。

Conclusion: 与传统的依赖扩展上下文窗口的RAG方法不同，LGM使大语言模型能够处理任意长度的文本而无需截断，有效提升了模型的概念解释和响应生成能力。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [15] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: 本文介绍了ISI的SARAL团队在IARPA的MATERIAL项目中为推进跨语言信息检索（CLIR）所做的努力，特别是在检索相关文档集合方面的创新方法及其在多个语言中的优异表现。


<details>
  <summary>Details</summary>
Motivation: 该项目旨在提升跨语言信息检索的技术水平，使得从不同语言中检索相关信息更加高效和准确。

Method: SARAL团队采用了一种新颖的方法，专注于开发能够检索相关文档集合而非仅仅是排名文档列表的技术。

Result: 在MATERIAL的第三阶段评估中，SARAL团队在六个评估条件中的五个中超过了其他团队，涵盖了三种不同语言（波斯语、哈萨克语和格鲁吉亚语）。

Conclusion: SARAL团队的方法在跨语言信息检索领域取得了显著的成效，展示了其在多语言环境下的高效性和准确性。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [16] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 该论文介绍了一种新的多语言分词器IndicSuperTokenizer，结合了子词和多词分词技术，显著提高了多语言环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 分词器对大型语言模型的性能、训练效率和推理成本至关重要，尤其在多语言环境下，面对多样化的文字和丰富的形态变化，设计有效的分词器极具挑战性。

Method: 提出了IndicSuperTokenizer，结合了子词和多词分词，以及特定语言的前处理分词，从而产生更符合语言学的分词效果。

Result: 在英语、22种印度语言和代码数据上评估，该分词器在fertility score上优于LLaMA4 39.5%，优于Sutra 18%，并在推理效率上比LLaMA4提高44%。

Conclusion: IndicSuperTokenizer通过结合多种分词策略和语言特定处理，在多语言环境下取得了显著的性能提升，验证了设计选择的稳健性。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [17] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 该论文评估了多种开源和闭源大型语言模型在问答任务中的性能，强调了RAG技术在提升模型表现方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着RAG和LLMs的普及，研究旨在比较不同LLMs在多样化领域的问答任务中的表现，以探索开源模型是否能够与专有模型相媲美。

Method: 通过比较Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct, Orca-mini-v3-7b和GPT-3.5在计算机科学文献的问答任务中的表现，使用准确性和精确度（针对二分类问题）、人工和Google的Gemini模型排名，以及长答案的余弦相似性作为评估指标。

Result: GPT-3.5与RAG结合在二分类和长问答任务中表现出色。开源模型中，Mistral-7b-instruct与RAG结合表现最佳，而Orca-mini-v3-7b生成响应的平均延迟最短，LLaMa2-7b-chat的延迟最长。

Conclusion: 研究表明，开源LLMs在合适的架构支持下，能够在问答任务中与GPT-3.5等专有模型表现相媲美。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [18] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: 本文提出了一种名为SCALE的宽度扩展架构，用于大型语言模型的持续预训练，强调结构扩展比参数扩展更重要。


<details>
  <summary>Details</summary>
Motivation: 当前持续预训练进展更多依赖于结构扩展而非仅参数扩展，旨在解决深度扩展导致的遗忘问题，提高模型的稳定性和适应性。

Method: 引入SCALE架构，通过轻量级扩展线性模块并保持预训练参数冻结，采用Persistent Preservation和Collaborative Adaptation原则，设计了SCALE-Preserve、SCALE-Adapt和SCALE-Route三种变体。

Result: 在合成传记基准测试中，SCALE减轻了深度扩展的严重遗忘问题；在韩语语料持续预训练中，SCALE变体在英语评测中遗忘更少，在韩语基准上表现有竞争力。

Conclusion: SCALE架构通过保持基础模型行为与选择性地训练扩展组件，实现了稳定性和可塑性的最佳平衡，为持续学习优化提供了新思路。

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [19] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文评估了“推理型”多模态大语言模型在临床任务中的表现，发现其“思考模式”相较于“非思考模式”性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨新兴的“双态”MLLMs在医疗应用中的实际效果，尤其是在推理过程被显式控制时，是否能提升性能和可靠性。

Method: 选取了Seed1.5-VL和Gemini-2.5-Flash两个领先的MLLMs，在VQA-RAD和ROCOv2数据集上评估了它们在四种视觉医疗任务中的表现。

Result: 激活思考模式所带来的性能提升有限，尤其在复杂任务如开放式问答和医学图像解读中表现不佳。

Conclusion: 思考模式在医疗任务中的优势不显著，强调了需要更多领域特定的医学数据和更先进的医学知识整合方法。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [20] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 本文综述了生成式人工智能（GenAI）在生物信息学中的应用，通过六个研究问题（RQs）系统评估了其在方法学进步、预测性能和专业化方面的影响，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: GenAI在生物信息学领域具有变革性影响，能够推动基因组学、蛋白质组学、转录组学、结构生物学和药物发现等多个领域的发展。因此，有必要系统评估这些进展。

Method: 采用系统综述和meta分析的方法，提出了六个研究问题（RQs），评估GenAI在方法学进步、预测性能和专业化方面的影响，并探讨其应用、优势、局限性和数据集支持。

Result: GenAI在序列分析、分子设计和整合数据建模等多个子领域展示了优于传统方法的性能。专门化模型架构在特定任务上优于通用模型，且通过分子、细胞和文本数据集的训练和泛化能力得到验证。

Conclusion: GenAI在生物信息学中具有广泛的应用前景，但仍面临可扩展性和数据偏差等挑战。未来的研究应注重稳健评估和生物学基础建模，以进一步提升GenAI在复杂分析中的性能和适用性。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [21] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 该论文提出了“沉默偏见”概念，并开发了Silenced Bias Benchmark (SBB)框架，以揭示安全对齐大型语言模型（LLMs）中隐藏的偏见。


<details>
  <summary>Details</summary>
Motivation: 安全对齐的LLMs在敏感应用中至关重要，但现有评估方法往往因模型拒绝回答而错误地将其解读为公平，忽略了模型潜在的不公平偏见。

Method: 提出SBB框架，通过激活引导减少模型在问答过程中的拒绝，从而揭示隐藏在模型潜在空间中的偏见。

Result: 在多个LLMs上的实验揭示了模型直接响应与潜在公平性问题之间的显著差异。

Conclusion: SBB为评估和提升模型公平性提供了新工具，鼓励开发超越安全对齐掩盖效应的公平模型和工具。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [22] [EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models for Edge-Deployable Credit Negotiation](https://arxiv.org/abs/2511.03370)
*Yunbo Long,Yuhan Liu,Alexandra Brintrup*

Main category: cs.CL

TL;DR: 本文介绍了一种名为EQ-Negotiator的新框架，通过情感角色缩小了小语言模型（SLMs）和大型语言模型（LLMs）在自动谈判中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动谈判中性能出色，但计算成本高且对数据隐私要求严格，不适合许多隐私敏感的移动应用。小型语言模型虽实用，但在情感复杂的场景中表现较差。

Method: EQ-Negotiator框架结合博弈论和隐马尔可夫模型（HMM）在线学习并跟踪债务人情感状态，无需预训练。

Result: 在广泛的信贷谈判模拟中，EQ-Negotiator框架下7B参数的模型在债务回收和谈判效率方面优于超过其10倍大小的LLMs基线。

Conclusion: 该研究推动情感角色建模从静态描述到动态情感架构，强调战略情感智能而非模型规模，是实现有效、道德和隐私保护AI谈判者的关键。

Abstract: The deployment of large language models (LLMs) in automated negotiation has
set a high performance benchmark, but their computational cost and data privacy
requirements render them unsuitable for many privacy-sensitive, on-device
applications such as mobile assistants, embodied AI agents or private client
interactions. While small language models (SLMs) offer a practical alternative,
they suffer from a significant performance gap compared to LLMs in playing
emotionally charged complex personas, especially for credit negotiation. This
paper introduces EQ-Negotiator, a novel framework that bridges this capability
gap using emotional personas. Its core is a reasoning system that integrates
game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional
states online, without pre-training. This allows EQ-Negotiator to equip SLMs
with the strategic intelligence to counter manipulation while de-escalating
conflict and upholding ethical standards. Through extensive agent-to-agent
simulations across diverse credit negotiation scenarios, including adversarial
debtor strategies like cheating, threatening, and playing the victim, we show
that a 7B parameter language model with EQ-Negotiator achieves better debt
recovery and negotiation efficiency than baseline LLMs more than 10 times its
size. This work advances persona modeling from descriptive character profiles
to dynamic emotional architectures that operate within privacy constraints.
Besides, this paper establishes that strategic emotional intelligence, not raw
model scale, is the critical factor for success in automated negotiation,
paving the way for effective, ethical, and privacy-preserving AI negotiators
that can operate on the edge.

</details>


### [23] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: 提出LFC-DA，一种符号逻辑控制的逻辑数据增强流程，确保逻辑严谨性和多样性。


<details>
  <summary>Details</summary>
Motivation: 人工标注复杂逻辑数据成本高，大语言模型直接生成结果不可解释且逻辑同质化。

Method: 将逻辑文本映射为命题表达式，构建紧凑规则库，有界状态空间搜索发现有效公式，再转化为自然语言问题。

Result: 在ReClor和LogiQA上，预训练模型的逻辑推理准确率显著提高。

Conclusion: LFC-DA在大模型引导的逻辑数据增强中有效，兼具逻辑严谨性和多样性。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [24] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文探讨了非对称BPE在机器翻译中的优势，发现源语言和目标语言采用不同合并操作次数（NMO）能显著提升翻译效果，尤其在低资源语言对上。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译中常采用对称BPE，即源语言和目标语言使用相同的NMO，但这未必在所有语言对和数据量下都最优。该研究旨在探讨不同数据量和语言对下BPE分割方案对MT性能的影响。

Method: 研究考察了不同数据量和语言对下的BPE分割方案，比较了对称BPE和非对称BPE在机器翻译系统上的表现，验证了非对称BPE的优势。

Result: 非对称BPE在低资源设置下显著提升翻译效果，在英语-印地语翻译中，CHRF++指标分别提高了5.32, 4.46, 和0.7。在另外六种语言对中也观察到类似趋势，12个系统中有10个表现显著提升。

Conclusion: 源语言采用高NMO（4K到32K），目标语言采用低NMO（0.5K到2K）的方案能最优地提升翻译效果，尤其对低资源机器翻译有益。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [25] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本文研究了小型语言模型（SLMs）在关系抽取（RE）中的应用，特别是在提取包含数据类型属性和对象属性的完整RDF图时面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在基于SHACL形状指导的关系抽取中表现出潜力，但在处理长尾分布的稀有属性时存在瓶颈。

Method: 评估了几种策略：分层抽样、加权损失、数据集扩展和基于模板的增强数据合成。

Result: 最佳策略是构建一个训练集，其中每个属性的出现次数超过某个阈值，从而在不平衡目标属性上表现良好。

Conclusion: 公开发布了数据集、实验结果和代码，为训练形状感知的小型语言模型提供了实用指导，并指出了未来在语义关系抽取中的研究方向。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [26] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 本文提出了一种名为3TF（Thought-Training and Thought-Free inference）的新框架，旨在通过从短到长（Short-to-Long）的视角提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型推理方法主要依赖于冗长的推理输出，虽然有一些压缩方法，但这些方法在推理时仍然需要显式推理。为了提高效率并减少推理时间，作者提出了一种新框架。

Method: 3TF框架首先训练一个混合模型，该模型可以在推理和非推理模式下操作。然后在带有CoT标注的数据上进一步训练模型以内部化结构化推理，并在推理时使用非推理模式强制生成简明的、无思维的输出。

Result: 经验证明，3TF训练的模型在思维自由推理下，推理基准测试获得了很大的改进，表明高质量推理可以在没有显式逐步生成的情况下隐式地学习和执行。

Conclusion: 3TF框架通过内部化推理过程，能够在保持外部输出简洁的同时，提高非推理输出的推理质量，从而在不显式生成推理步骤的情况下实现高效推理。

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [27] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: CareMedEval 是一个用于评估大型语言模型在生物医学批判性评价和推理任务上的新数据集，揭示了当前模型在该领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学文献批判性评价中的应用前景广阔，但在专业领域中的可靠性仍然有限，尤其是在关键推理任务上。

Method: 引入了 CareMedEval 数据集，源自法国医学生的实际考试，包含 534 个基于 37 篇科学文章的问题，用于评估不同上下文条件下的大型语言模型。

Result: 即使在生成中间推理令的情况下，开放和商业模型在精确匹配率上未能超过 0.5，尤其在研究限制和统计分析问题上表现不佳。

Conclusion: CareMedEval 为基于科学文献的推理提供了一个具有挑战性的基准，揭示了当前大型语言模型的局限性，并为未来自动化批判性评价支持工具的发展铺平了道路。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [28] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: 提出QuestionRAG框架，通过外部知识增强和强化学习解决LLMs在问题纠正中的误解释和过度校正问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答系统中经常误解用户意图或过度校正问题结构，导致错误响应。

Method: QuestionRAG框架结合外部知识增强（如搜索结果、相关实体）和强化学习，以精确校正问题而非简单改写。

Result: 知识增强对于理解错误问题至关重要，且基于强化学习的方法比传统监督微调更有效，能提升模型遵循指令和泛化的能力。

Conclusion: 通过结合知识增强和强化学习，QuestionRAG充分发挥了LLMs在问题纠正任务中的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [29] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 本文旨在为大型语言模型（LLMs）创建多语言逻辑推理能力测试数据集，并探索增加难度的方法。


<details>
  <summary>Details</summary>
Motivation: 需要一个能够衡量LLMs在不同语言下逻辑推理能力的基准，且适合不同推理能力的模型。

Method: 生成多种语言、主题、尺寸和线索类型及干扰项的斑马谜题，考察不同难度对模型的影响。

Result: 尺寸为2x3和4x5的谜题分别对非推理模型和推理模型具有挑战性，加入5个干扰项会降低15±7%的准确率。

Conclusion: 数据集MultiZebraLogic包含128+1024个谜题，已发布在九种日耳曼语言中，并提供了可适配更多语言和主题的代码。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [30] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor是一个利用RDF pattern-based extraction优化小型语言模型（SLMs）的框架，通过评估SHACL形状的所有属性组合来增强模型泛化能力，并采用迭代学习过程改进知识库。


<details>
  <summary>Details</summary>
Motivation: 解决在特定领域中对知识库进行完善和修正的需求，尤其是在训练数据有限的情况下，提高小型语言模型的性能。

Method: Kastor通过将传统的SHACL形状验证任务转变为评估所有可能的属性组合，并选择每个训练示例的最优组合来增强模型泛化能力。此外，通过迭代学习过程来完善含噪声的知识库。

Result: 该框架显著提高了模型泛化能力和性能，使得开发出的模型能够发现新的、相关的事实。

Conclusion: Kastor框架提供了一种有效的方法来优化小型语言模型，在有限的数据条件下，实现知识库的自动完善和修正，具有在特定领域应用的潜力。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [31] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文首次展示了transformer语言模型中的可控局部性，通过可调参数实现局部和分布式表征间的动态调整。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型仅依赖分布式表征，缺乏可解释性；本研究旨在实现局部表征和分布式表征之间的动态平衡，以提升模型的可解释性和性能。

Method: 使用WikiText语料库和两层transformer架构，通过调节局部性参数λ（1.0为完全局部性，0.0为完全分布式），进行实验分析。

Result: 局部性配置显著降低了注意力熵，λ=1.0时熵为5.36比特，λ=0.0时为7.18比特；λ=0.6时测试困惑度为4.65，准确率为84.7%。

Conclusion: 局部语言模型为需要透明性和高性能的受监管领域提供了实用框架，通过明确的惩罚阈值和信息论设计原则，实现可解释性和性能的平衡。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [32] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 该论文介绍了一个名为BanglaSTEM的数据集，旨在改善孟加拉语到英语的翻译，尤其是在STEM领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在英语技术问题解决方面表现良好，但在孟加拉语中表现不佳。现有的翻译系统在技术术语上常出错，导致问题含义改变和答案错误。

Method: 创建了一个包含5,000个孟加拉语-英语句子对的数据集BanglaSTEM，并使用语言模型生成超过12,000个翻译，通过人工评估选出最高质量的翻译对。训练了一个基于T5的翻译模型，并在生成代码和解决数学问题两个任务上测试。

Result: 结果显示，BanglaSTEM在技术内容翻译准确性方面有显著提升，帮助孟加拉语使用者更有效地使用以英语为中心的语言模型。

Conclusion: BanglaSTEM数据集和训练好的翻译模型的公开发布，有望促进孟加拉语使用者在STEM领域的技术问题解决能力。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [33] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX是首个开源的基于LLM的音频模型，擅长表达性和迭代音频编辑，涵盖情感、说话风格及副语言特征，并具备零样本文本到语音(TTS)能力。


<details>
  <summary>Details</summary>
Motivation: 当前音频编辑模型多依赖嵌入先验或辅助模块，需要大量人工标注数据。本文旨在通过大间隔合成数据训练，摆脱对表示级解缠的依赖，实现高表现力的迭代音频编辑。

Method: 利用大间隔合成数据进行学习，无需嵌入先验或辅助模块，从而实现迭代控制和高度表达性。

Result: 在情感编辑和细粒度控制任务中，Step-Audio-EditX优于MiniMax-2.6-hd和Doubao-Seed-TTS-2.0。

Conclusion: 通过大间隔学习的方法，Step-Audio-EditX在音频编辑任务中实现了突破，不再依赖于传统的表示级解缠方法。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [34] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本文提出HaluMem，首个针对记忆系统的操作级幻觉评估基准，通过三项任务揭示不同阶段的幻觉行为。


<details>
  <summary>Details</summary>
Motivation: 现有记忆幻觉评估局限于端到端问答，难以定位幻觉产生的具体操作阶段，因此需要细粒度的评估工具。

Method: 设计了HaluMem，包含记忆提取、更新和问答三项任务，并构建了HaluMem-Medium和HaluMem-Long两个人类-AI交互数据集，支持多轮对话和复杂任务评估。

Result: 基于HaluMem的实证研究表明，现有记忆系统在提取和更新阶段容易产生幻觉，并会传播至问答阶段。

Conclusion: 未来研究应聚焦于开发可解释且受约束的记忆操作机制，以系统性抑制幻觉并提升记忆可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [35] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 我们介绍了中文多文档问答数据集ChiMDQA，适用于学术、教育、金融、法律、医疗和新闻等多个领域。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理（NLP）技术的快速发展，对高质量中文文档问答数据集的需求不断增长。

Method: 通过精心筛选文档和系统化的问题设计方法，ChiMDQA涵盖了来自六个不同领域的长文档，并包含6,068个高质量的问题-答案对，分为十个细粒度类别。

Result: ChiMDQA保证了多样性和高质量，使其适用于各种NLP任务，如文档理解、知识提取和智能问答系统。

Conclusion: 该数据集为中文问答的未来研究和实际应用提供了重要基础。代码和数据可在https://anonymous.4open.science/r/Foxit-CHiMDQA/获取。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [36] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种用于评估大型语言模型在多轮对话中遵循用户指令能力的可扩展框架，并构建了演化式指令遵循基准EvolIF，实验显示GPT-5在该任务上表现最优。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试受限于固定对话轮次，易饱和且无法充分评估用户交互体验，亟需更动态、真实的评估方法。

Method: 提出三层机制（约束、指令、主题）解耦语言形式与用户意图，支持状态变化和回溯，以用户耐心耗尽为终止条件，并设计多维度交互质量指标。

Result: GPT-5平均维持18.54轮对话，鲁棒性达70.31%，显著优于Gemini-2.5-Pro（领先11.41%），其他模型差距更大；构建了含9类约束的EvolIF基准。

Conclusion: 动态可扩展的评估框架能更真实反映模型多轮指令遵循能力，GPT-5在该任务中表现领先，代码与数据将开源。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [37] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 提出了一种针对不对称对话中的共同理解问题的新标注方案，并应用LLM进行标注和分析。


<details>
  <summary>Details</summary>
Motivation: 协作对话需要逐步建立共同理解，但在不对称环境中，参与者可能误以为他们指的是相同的事物。

Method: 引入了HCRC MapTask语料库的perspectivist标注方案，使用LLM标注流程对每个参考表达式的发言者和接收者分别标注，并分析理解状态。

Result: 结果表明，一旦词汇变体被统一，完全误解很少见，但多重性差异系统性地引发分歧。

Conclusion: 该框架为研究基础误解和评估(V)LLMs在协作对话中建模基于视角的共同理解能力提供了资源和新的分析视角。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


### [38] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 该研究通过微调大型语言模型和集成检索增强生成（RAG）方法，帮助政策制定者更好地理解、分析和制定法律法规。


<details>
  <summary>Details</summary>
Motivation: 为了更好地支持政策制定者理解、分析和制定法律法规，该研究旨在提高大型语言模型在法律领域的应用能力。

Method: 研究采用了专门策划的监督数据集进行模型微调，并集成了检索增强生成（RAG）方法，使模型能够访问和整合最新的外部法律知识。

Result: 结果表明，该结合微调与RAG增强的方法显著提高了法律研究和法规制定的效率。

Conclusion: 该研究提供了一个有价值的工具，能够主动协助政策制定者解释法规和起草新法规，适应法律领域的不断变化。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [39] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 本文系统回顾了基于Transformer模型的关系抽取（RE）研究，分析了2019至2024年间的大量相关文献、数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 关系抽取（RE）研究在近年来取得了显著进展，尤其是在引入基于Transformer的模型后。本文旨在通过系统性回顾，提供一份全面的参考，帮助研究人员和从业者理解RE的演变和未来方向。

Method: 使用自动化框架收集并标注了34项调查、64个数据集和104个模型，通过多维度整合分析，识别出当前趋势、局限性和开放挑战。

Result: 突出了方法论的进步、基准资源以及语义网技术的整合，为RE领域的研究提供了全面的参考。

Conclusion: 该研究为理解RE领域的当前状态和未来发展提供了重要见解，指出了该领域的研究趋势和未解决问题。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [40] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 本文提出了一种新的可解释零样本立场检测（ZSSD）框架IRIS，通过隐式和显式理由进行立场分析，提高了模型的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在零样本立场检测中存在泛化能力不足、文本与目标之间缺乏连贯性，以及对显式推理过度依赖等问题，导致模型预测难以解释。

Method: IRIS框架将立场检测视为信息检索排序任务，利用文本中的序列（隐式理由）和语言测量（显式理由）来理解不同立场的相关性，无需理由的真实标签即可指导模型预测。

Result: 在VAST、EZ-STANCE、P-Stance和RFD基准数据集上的广泛实验，使用50%、30%甚至10%的训练数据，证明了IRIS模型的泛化能力。

Conclusion: IRIS通过其架构和可解释设计，提供了对作者立场的可解释理解，解决了现有方法在可解释性和泛化性方面的局限性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [41] [Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models](https://arxiv.org/abs/2511.03699)
*Francesco Corso,Francesco Pierri,Gianmarco De Francisci Morales*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）是否表现出阴谋论倾向，以及它们在这一领域是否显示社会人口统计偏见，并探讨了模型在采用阴谋论观点时的易感性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解LLMs是否能再现类似人类的阴谋论思维，以及这种思维如何影响对机构的信任和错误信息的传播。

Method: 采用经过验证的心理测量调查来测量不同提示和条件策略下多个模型的阴谋论思维。

Result: LLMs对阴谋信念的部分元素显示出认同，社会人口统计属性条件化产生不均衡的影响，并且有针对性的提示可以轻松将模型响应转向阴谋论方向。

Conclusion: 强调了批判性评估LLMs中嵌入的心理维度的重要性，以便推进计算社会科学，并制定可能的缓解策略以应对有害使用。

Abstract: In this paper, we investigate whether Large Language Models (LLMs) exhibit
conspiratorial tendencies, whether they display sociodemographic biases in this
domain, and how easily they can be conditioned into adopting conspiratorial
perspectives. Conspiracy beliefs play a central role in the spread of
misinformation and in shaping distrust toward institutions, making them a
critical testbed for evaluating the social fidelity of LLMs. LLMs are
increasingly used as proxies for studying human behavior, yet little is known
about whether they reproduce higher-order psychological constructs such as a
conspiratorial mindset. To bridge this research gap, we administer validated
psychometric surveys measuring conspiracy mindset to multiple models under
different prompting and conditioning strategies. Our findings reveal that LLMs
show partial agreement with elements of conspiracy belief, and conditioning
with socio-demographic attributes produces uneven effects, exposing latent
demographic biases. Moreover, targeted prompts can easily shift model responses
toward conspiratorial directions, underscoring both the susceptibility of LLMs
to manipulation and the potential risks of their deployment in sensitive
contexts. These results highlight the importance of critically evaluating the
psychological dimensions embedded in LLMs, both to advance computational social
science and to inform possible mitigation strategies against harmful uses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [42] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent是一个多智能体框架，通过专门化的智能体解决非专家在开放数据仓库中进行数据分析的难题。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库对非专家不可及，而大型语言模型在端到端分析流程中存在注意力稀释和错误传播等局限。

Method: 提出PublicAgent，将任务分解为意图澄清、数据集发现、分析和报告生成四个专门智能体，并在五个模型和50个查询上评估。

Result: 专业化智能体设计带来显著效益，最强模型仍有97.5%的智能体胜率；发现和分析智能体稳定性较高，报告与意图智能体则因模型而异。

Conclusion: 专业化在复杂分析流程中必要，通过自然语言接口使公共数据更易于获取，并指导多智能体LLM系统的设计。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [43] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建了一个基准，用于评估大型语言模型（LLMs）在真实世界概率分布知识方面的能力，发现LLMs表现不佳，未能自然内化真实世界统计数据。


<details>
  <summary>Details</summary>
Motivation: 尽管AI系统在许多科学领域取得了显著进展，但在实现更一般类型的智能方面仍有待提高。本文旨在探讨LLMs是否能内化真实世界的概率分布。

Method: 开发首个基准，测试LLMs在经济学、健康、教育和社会行为等领域的真实世界概率分布知识，并结合Pearl的因果层次（PCH）进行解释。

Result: LLMs整体表现较差，未能内化真实世界统计数据，且根据因果层次定理，LLMs在干预性（第2层）和假设性（第3层）知识方面也有限。

Conclusion: LLMs在真实世界概率分布知识方面表现不佳，表明其在因果推理和分布学习方面存在根本性限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [44] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型在工业部署中面临的KV缓存内存挑战，提出了SnapStream这一可部署的KV缓存压缩方法，并在实际生产环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的KV缓存需求巨大，现有技术在工业框架中应用受限，且对现代模型精度影响不明确。

Method: 分析Llama-3.1-8B-Instruct和DeepSeek-R1上的精度影响，开发SnapStream方法，并在SambaNova SN40L加速器上实现16路并行部署。

Result: SnapStream在4倍内存优化的同时，在多个基准测试中保持模型精度，实现128k上下文长度下1832 tokens/s的吞吐。

Conclusion: SnapStream是首个在静态图和连续批处理的实际生产系统中部署的稀疏KV注意力技术，有效解决了内存瓶颈问题。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [45] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 提出了一种新的、可扩展的以能力为基础的监控原则，用于大型语言模型在医疗保健中的监控。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗中的快速应用带来了对其监督的审查，而传统的基于任务的监控方法不适用于这些模型。

Method: 基于大型语言模型作为通用系统的特性，提出围绕共享模型能力（如总结、推理、翻译或安全护栏）进行监控，而不是对每个下游任务单独监控。

Result: 该方法能够跨任务检测系统性弱点、长尾错误和新出现的行为，这些是传统任务基础监控可能遗漏的。

Conclusion: 以能力为基础的监控为安全、自适应和协作的大型语言模型监控提供了可扩展的基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [46] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 本文分析了miniF2F基准测试中正式与非正式陈述之间的差异，并推出了经过修正和验证的miniF2F-v2版本，从而提高了整体准确率。


<details>
  <summary>Details</summary>
Motivation: 在数学奥林匹克竞赛的环境中，AI系统需要理解自然语言问题，将其形式化，然后进行证明。然而，现有的自动形式化和定理证明的单独最佳准确率远高于整个流程的准确率，这提示正式与非正式陈述之间存在差异。

Method: 通过分析miniF2F基准测试中正式与非正式陈述的差异，修正并验证了所有错误和简化，推出了miniF2F-v2。

Result: 在miniF2F-v2上，完整定理证明流程的最佳准确率提高到70%，而原始miniF2F的准确率仅为40%。

Conclusion: 高质量的基准测试有助于社区更好地评估形式推理领域的进展，并诊断自动形式化和定理证明模型的成功与失败模式。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [47] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 本文提出了一种结合多模态大语言模型（MLLM）和烟花算法（FWA）的新优化方法，用于解决复杂高维优化问题，并在TSP和EDA问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统零阶或一阶优化方法在面对非凸性、高维性和黑箱性等复杂优化问题时效率低且信息利用不足。近年来，大语言模型在语言理解和代码生成上的进步促使研究者探索其在优化算法设计中的应用。

Method: 选择烟花算法（FWA）作为基础优化器，通过引入多模态大语言模型（MLLM）和关键部分（Critical Part, CP）的概念，增强FWA在复杂高维任务中的性能。

Result: 在旅行商问题（TSP）和电子设计自动化（EDA）问题上的实验结果表明，新框架下的FWA在多个实例上达到或超越了当前最佳（SOTA）结果。

Conclusion: 通过融合MLLM和FWA，提出的新优化框架能有效应对复杂优化挑战，具有广泛的应用潜力。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [48] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出了一种新型安全响应框架，以在输入和输出层面系统性地保护大型语言模型（LLMs）。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，其相关的安全问题日益突出，严重制约了其在关键领域的可信部署。

Method: 在输入层面，该框架采用基于监督微调的安全分类模型，通过细粒度四层分类（安全、不安全、条件安全、重点关注），对用户查询进行精确风险识别和差异处理。在输出层面，该框架结合了检索增强生成（RAG）和专门微调的解释模型，以确保响应基于实时可信的知识库。

Result: 实验结果表明，该安全控制模型在公共安全评估基准上显著优于基准模型TinyR1-Safety-8B。在高风险测试集上，框架组件达到了完美的100%安全评分。

Conclusion: 该研究为构建高安全性和高信任度的LLM应用提供了有效的工程路径。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [49] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式可解释人工智能（XAI）的新方法，并评估了公开可用的形式解释器PyXAI。


<details>
  <summary>Details</summary>
Motivation: 尽管形式XAI提供了比其他非形式解释方法更严格的理论保证，但对形式解释器的实际实现验证关注较少。

Method: 开发了一种新方法来验证形式解释器，并对PyXAI进行了评估。

Result: 在大多数实验数据集上，PyXAI计算出的解释是不正确的。

Conclusion: 验证形式解释器的新方法非常重要，因为发现了PyXAI在多数情况下提供错误解释。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [50] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 通过多智能体AI框架形式化工程设计过程，提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计方法复杂且资源密集，需要多学科专业知识，容易出现效率低下。

Method: 引入多智能体AI框架，包括三个关键智能体：图本体专家、设计工程师和系统工程师，利用领域特定知识图和计算工具协作生成和优化设计。

Result: 成功应用于4位NACA翼型的空气动力学优化，通过迭代反馈回路，最终设计在性能参数上得到优化。

Conclusion: 协作AI智能体结合结构化知识表示，可以增强工程设计过程的效率、一致性和质量。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [51] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文介绍了Summit Concierge，一个为Adobe Summit开发的特定领域AI助手，通过人机协同开发流程应对数据稀疏、质量保证和快速部署等挑战，展示了敏捷反馈驱动开发在冷启动场景下实现可扩展且可靠的AI助手的潜力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI助手在企业环境中提升生产力、简化信息访问和改善用户体验具有巨大潜力。然而，在真实世界条件下开发AI助手面临数据稀疏、质量保障和快速部署等挑战。

Method: 采用人机协同开发流程，结合提示工程、检索接地和轻量级人工验证，以应对挑战。描述了系统架构、开发过程和实际部署结果。

Result: 经验表明，敏捷、反馈驱动的开发方法即使在冷启动场景下，也能实现可扩展且可靠的AI助手。

Conclusion: 通过敏捷开发流程，可以有效地在真实世界条件下构建和部署可靠的AI助手，为类似项目提供了可行的解决方案。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [52] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 大型语言模型 (LLMs) 能从少量定量输入中模拟人类心理特质的相关性结构。


<details>
  <summary>Details</summary>
Motivation: 心理构念在个体内部是相互关联的，研究者希望了解LLMs是否能模拟这种人类心理特质的相关性结构。

Method: 用来自816个个体的五大人格量表数据提示不同的LLMs，让它们在其他九个心理量表上模拟这些个体的反应，并分析推理过程。

Result: LLMs在捕捉人类心理结构方面表现出显著准确性，生成的反应间相关模式与人类数据高度一致（R² > 0.89）。

Conclusion: LLMs通过抽象和推理过程，能从少量数据中精确预测个体心理特质，为心理模拟提供了强大工具，并揭示了其涌现的推理能力。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [53] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 本文提出了一种名为 AAA 的审计框架，通过人机协作实现可拓展的网页无障碍审计。


<details>
  <summary>Details</summary>
Motivation: 当前网页无障碍审计存在资源密集、难以扩展的问题，导致大多数网站不合规。WCAG-EM 方法虽然结构化，但人力成本高且缺乏大规模实践支持。

Method: AAA 框架包含两个主要创新：GRASP，一种基于图的多模态采样方法，确保页面的代表性覆盖；MaC，一种多模态大语言模型辅助工具，支持审计员进行跨模态推理和智能协助。

Result: 实验表明，所提出的方法在审计流程的各个核心阶段均有效，并且小模型在微调后可以作为专家使用。

Conclusion: AAA 框架通过人机协作实现了高效、可拓展的网页无障碍审计，并提供了四个用于基准测试的新数据集。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [54] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文对机器学习模型中解释问题的参数化复杂性进行了全面的理论研究。


<details>
  <summary>Details</summary>
Motivation: 当前，大多数模型被视为黑箱，缺乏透明性，因此本研究关注具有透明内部机制的模型，填补可解释AI（XAI）领域的空白。

Method: 分析包括决策树、决策集、决策列表、布尔电路及其集成在内的多种机器学习模型，处理局部和全局的两种主要解释问题：溯因和对比。

Result: 提供了生成这些模型解释的复杂性的基础理解。

Conclusion: 该研究为XAI领域的进一步研究提供了重要见解，有助于更广泛的AI系统透明度和责任性讨论。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>
