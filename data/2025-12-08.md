<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 25]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Fine-Tuning BERT for Domain-Specific Question Answering: Toward Educational NLP Resources at University Scale](https://arxiv.org/abs/2512.05179)
*Aurélie Montfrond*

Main category: cs.CL

TL;DR: 本研究开发了一个用于利默里克大学电子与计算机工程系的聊天机器人，通过微调BERT模型，提供课程信息。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要集中在聊天机器人风格的系统，很少探索对基础模型进行领域特定推理的微调。此外，尚未有针对大学课程材料的基础模型。

Method: 使用大学模块手册构建了一个包含1,203个问题-答案对的定制数据集，格式为SQuAD，并进行了手动和合成生成条目的补充。使用PyTorch对BERT模型进行微调，并通过精确匹配和F1分数评估性能。

Result: 结果显示，即使进行适度的微调，也能改善假设框架和知识提取，证明了将基础模型适应教育领域的可行性。

Conclusion: 通过对BERT进行学术QA对的微调，可以有效填补大学课程材料领域基础模型的空白，并显示出向大学特定领域问答模型扩展的潜力，从而实现自主教育知识系统。

Abstract: Prior work on scientific question answering has largely emphasized chatbot-style systems, with limited exploration of fine-tuning foundation models for domain-specific reasoning. In this study, we developed a chatbot for the University of Limerick's Department of Electronic and Computer Engineering to provide course information to students. A custom dataset of 1,203 question-answer pairs in SQuAD format was constructed using the university book of modules, supplemented with manually and synthetically generated entries. We fine-tuned BERT (Devlin et al., 2019) using PyTorch and evaluated performance with Exact Match and F1 scores. Results show that even modest fine-tuning improves hypothesis framing and knowledge extraction, demonstrating the feasibility of adapting foundation models to educational domains. While domain-specific BERT variants such as BioBERT and SciBERT exist for biomedical and scientific literature, no foundation model has yet been tailored to university course materials. Our work addresses this gap by showing that fine-tuning BERT with academic QA pairs yields effective results, highlighting the potential to scale towards the first domain-specific QA model for universities and enabling autonomous educational knowledge systems.

</details>


### [2] [Decoding the Black Box: Discerning AI Rhetorics About and Through Poetic Prompting](https://arxiv.org/abs/2512.05243)
*P. D. Edgar,Alia Hall*

Main category: cs.CL

TL;DR: 该论文探讨了创意文本提示（特别是诗歌提示模式）在提示工程中的应用，并通过对著名诗人的三种模型进行测试，评估了模型对原创作品的适应性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大型语言模型在创意写作中的潜力，以及如何通过特定的提示模式来优化模型的文本生成能力。

Method: 采用诗歌提示模式，对著名诗人的不同模型进行描述和评估，并测试模型在不同受众假设下对原作的适应性。

Result: 诗歌提示模式被证明是提示工程中有用的工具，模型在适应或重写创意作品方面展现出不同的灵活性和倾向。

Conclusion: 创意文本提示，尤其是诗歌提示模式，可以成为提示工程师工具箱中的有益补充，有助于探索和提升LLMs的创意写作能力。

Abstract: Prompt engineering has emerged as a useful way studying the algorithmic tendencies and biases of large language models. Meanwhile creatives and academics have leveraged LLMs to develop creative works and explore the boundaries of their writing capabilities through text generation and code. This study suggests that creative text prompting, specifically Poetry Prompt Patterns, may be a useful addition to the toolbox of the prompt engineer, and outlines the process by which this approach may be taken. Then, the paper uses poetic prompts to assess descriptions and evaluations of three models of a renowned poet and test the consequences of the willingness of models to adapt or rewrite original creative works for presumed audiences.

</details>


### [3] [Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4](https://arxiv.org/abs/2512.05256)
*Ivan Makohon,Mohamad Najafi,Jian Wu,Mathias Brochhausen,Yaohang Li*

Main category: cs.CL

TL;DR: 该论文研究了通过结合思维链（CoT）提示工程和语义搜索，以及注入临床本体构建的知识图谱（KG），来提升大语言模型在生成临床笔记方面的效果。


<details>
  <summary>Details</summary>
Motivation: 由于医生花费大量时间手动撰写临床笔记，增加了患者等待时间并可能延误诊断，需要自动化方法提高临床笔记生成效率。

Method: 使用ICD代码和基本患者信息作为输入，通过结合传统CoT和语义搜索结果，并注入临床本体构建的知识图谱，提升生成临床笔记的质量。

Result: 在CodiEsp测试数据集的六个临床案例上测试，结果表明该方法优于标准的一次性提示生成的临床笔记。

Conclusion: 结合CoT、语义搜索和知识图谱的提示技术显著提升了GPT-4生成临床笔记的质量，具有实际应用潜力。

Abstract: In the past decade a surge in the amount of electronic health record (EHR) data in the United States, attributed to a favorable policy environment created by the Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 and the 21st Century Cures Act of 2016. Clinical notes for patients' assessments, diagnoses, and treatments are captured in these EHRs in free-form text by physicians, who spend a considerable amount of time entering and editing them. Manually writing clinical notes takes a considerable amount of a doctor's valuable time, increasing the patient's waiting time and possibly delaying diagnoses. Large language models (LLMs) possess the ability to generate news articles that closely resemble human-written ones. We investigate the usage of Chain-of-Thought (CoT) prompt engineering to improve the LLM's response in clinical note generation. In our prompts, we use as input International Classification of Diseases (ICD) codes and basic patient information. We investigate a strategy that combines the traditional CoT with semantic search results to improve the quality of generated clinical notes. Additionally, we infuse a knowledge graph (KG) built from clinical ontology to further enrich the domain-specific knowledge of generated clinical notes. We test our prompting technique on six clinical cases from the CodiEsp test dataset using GPT-4 and our results show that it outperformed the clinical notes generated by standard one-shot prompts.

</details>


### [4] [To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples](https://arxiv.org/abs/2512.05318)
*Vignesh Kothapalli,Ata Fatahibaarzi,Hamed Firooz,Maziar Sanjabi*

Main category: cs.CL

TL;DR: 提出CoT-Recipe方法，通过调节元训练中CoT和非CoT示例的比例，提高模型在新任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 在少量样本上下文学习（ICL）中，链式思维（CoT）提示能增强大语言模型的推理能力，但在预训练知识不足的新任务上效果有限。

Method: 提出CoT-Recipe，通过调节元训练序列中CoT和非CoT示例的比例，缓解过度包含CoT示例带来的性能下降。

Result: 在新任务上，通过CoT-Recipe调节，模型准确率可提高达300%；应用于预训练模型Qwen2.5系列，符号推理任务上的准确率提高达130%。

Conclusion: CoT-Recipe是一种有效的方法，可以显著提高模型在缺乏CoT示例上下文时处理新任务的能力。

Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.

</details>


### [5] [Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats](https://arxiv.org/abs/2512.05331)
*Sadat Shahriar,Navid Ayoobi,Arjun Mukherjee,Mostafa Musharrat,Sai Vishnu Vamsi*

Main category: cs.CL

TL;DR: 本文研究了低质量自动生成的“粉红泥新闻”检测方法及其面临的挑战，并提出了一种能抵抗大语言模型对抗攻击的鲁棒学习框架。


<details>
  <summary>Details</summary>
Motivation: 本地新闻对数百万美国人至关重要，而“粉红泥新闻”模仿正规新闻报道，威胁信息的可靠性，因此需要精准检测。

Method: 通过分析“粉红泥新闻”的语言、风格和词汇特征，提出检测策略，并针对大语言模型修改引入鲁棒学习框架。

Result: 消费级大语言模型可使现有检测系统性能降低40%，而提出的框架能提升检测性能达27%。

Conclusion: 该研究揭示了检测“粉红泥新闻”的新挑战，并提供了一种有效应对大语言模型对抗性攻击的解决方案。

Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.

</details>


### [6] [Transformer-Enabled Diachronic Analysis of Vedic Sanskrit: Neural Methods for Quantifying Types of Language Change](https://arxiv.org/abs/2512.05364)
*Ananth Hariharan,David Mortensen*

Main category: cs.CL

TL;DR: 该研究展示了混合神经符号方法如何为低资源形态丰富语言的演化提供新见解。


<details>
  <summary>Details</summary>
Motivation: 挑战语言变化即简化的简单假设，通过定量分析梵语2000多年的变化，揭示混合方法在低资源语言演化研究中的潜力。

Method: 采用弱监督混合方法，通过100多个高精度正则表达式生成伪标签，微调多语言BERT，并通过新颖的置信度加权集合融合符号和神经输出。

Result: 在147万词的历史语料库上，集合方法实现了52.4%的特征检测率，发现梵语形态复杂性并未减少，而是动态重新分布，如复合词和哲学术语的扩展。

Conclusion: 系统生成良好校准的不确定性估计，置信度与准确性高度相关，支持计算语言学的可靠性。

Abstract: This study demonstrates how hybrid neural-symbolic methods can yield significant new insights into the evolution of a morphologically rich, low-resource language. We challenge the naive assumption that linguistic change is simplification by quantitatively analyzing over 2,000 years of Sanskrit, demonstrating how weakly-supervised hybrid methods can yield new insights into the evolution of morphologically rich, low-resource languages. Our approach addresses data scarcity through weak supervision, using 100+ high-precision regex patterns to generate pseudo-labels for fine-tuning a multilingual BERT. We then fuse symbolic and neural outputs via a novel confidence-weighted ensemble, creating a system that is both scalable and interpretable. Applying this framework to a 1.47-million-word diachronic corpus, our ensemble achieves a 52.4% overall feature detection rate. Our findings reveal that Sanskrit's overall morphological complexity does not decrease but is instead dynamically redistributed: while earlier verbal features show cyclical patterns of decline, complexity shifts to other domains, evidenced by a dramatic expansion in compounding and the emergence of new philosophical terminology. Critically, our system produces well-calibrated uncertainty estimates, with confidence strongly correlating with accuracy (Pearson r = 0.92) and low overall calibration error (ECE = 0.043), bolstering the reliability of these findings for computational philology.

</details>


### [7] [Mitigating Self-Preference by Authorship Obfuscation](https://arxiv.org/abs/2512.05379)
*Taslim Mahbub,Shi Feng*

Main category: cs.CL

TL;DR: 本文研究了语言模型（LMs）评委在评估LM输出质量时的自我偏好偏见，并提出通过应用黑箱扰动来减少自我识别，从而缓解这种偏见。


<details>
  <summary>Details</summary>
Motivation: 语言模型评委存在自我偏好偏见，会偏向自己的答案，这影响评估的完整性。该研究旨在减少LM评委识别自己输出的能力，从而减轻这种偏见。

Method: 通过在成对比较中应用黑箱扰动，例如同义词替换，来混淆作者身份并减少自我识别。

Result: 简单的扰动如同义词替换可减少自我偏好，但完全消除风格差异后，自我偏好会恢复。

Conclusion: 自我识别和偏好可在多个语义层面发生，尽管初始结果有希望，但完全缓解这种偏见仍具有挑战性。

Abstract: Language models (LMs) judges are widely used to evaluate the quality of LM outputs. Despite many advantages, LM judges display concerning biases that can impair their integrity in evaluations. One such bias is self-preference: LM judges preferring their own answers over those produced by other LMs or humans. The bias is hard to eliminate as frontier LM judges can distinguish their own outputs from those of others, even when the evaluation candidates are not labeled with their sources. In this paper, we investigate strategies to mitigate self-preference by reducing the LM judges' ability to recognize their own outputs. We apply black-box perturbations to evaluation candidates in pairwise comparison to obfuscate the authorship and reduce self-recognition. We find that perturbations as simple as synonym replacement for a few words predictably reduce self-preference. However, we also uncover fundamental challenges to eliminating the bias: when we extrapolate our perturbations to a more complete neutralization of stylistic differences between the evaluation candidates, self-preference recovers. Our findings suggest that self-recognition and self-preference can happen on many semantic levels, and complete mitigation remains challenging despite promising initial results.

</details>


### [8] [Learning from Self Critique and Refinement for Faithful LLM Summarization](https://arxiv.org/abs/2512.05387)
*Ting-Yao Hu,Hema Swetha Koppula,Hadi Pouransari,Cem Koc,Oncel Tuzel,Raviteja Vemulapalli*

Main category: cs.CL

TL;DR: 本文提出了一种自我批判和优化框架（SCRPO），通过自我监督的方式减少大型语言模型在生成摘要时的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成长文本时容易出现幻觉，即生成的内容与输入上下文不一致。以往的方法需要额外的计算资源或更强大的教师模型，不够实用。

Method: 提出SCRPO框架，利用模型自身的批判和优化能力构建偏好数据集，并应用偏好学习来提升模型的忠实摘要能力。

Result: 在XSUM、CNNDM和SAMSum三个摘要基准测试中，该方法在忠实性指标上优于最先进的自我监督学习方法，并在其他质量指标上维持或提升性能。

Conclusion: SCRPO不仅提高了摘要的忠实性，还比测试时优化更高效，且无需依赖更强大的教师模型。

Abstract: Large Language Models (LLMs) often suffer from hallucinations: output content that is not grounded in the input context, when performing long-form text generation tasks such as summarization. Prior works have shown that hallucinations can be reduced by iteratively critiquing and refining previously generated outputs using either the same model or a more powerful teacher model as the critique. However, these approaches either require additional test-time compute or assume access to more powerful teacher models, making them costly and less practical. In this work, we propose Self Critique and Refinement-based Preference Optimization (SCRPO), which is a self-supervised training framework that first constructs a preference dataset by leveraging the LLM's own critique and refinement capabilities, and then applies preference learning to improve the same LLM for faithful summarization. Experiments on three summarization benchmarks (XSUM CNNDM and SAMSum), demonstrate that our approach outperforms state-of-the-art self-supervised learning methods in terms of faithfulness metrics while either maintaining or improving other metrics that measure the overall quality of the summary. Moreover, compared to test-time refinement, our approach not only improves efficiency but also results in more faithful summaries.

</details>


### [9] [SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs](https://arxiv.org/abs/2512.05409)
*Ruixuan Huang,Hao Zeng,Hantao Huang,Jinyuan Shi,Minghui Yu,Ian En-Hsu Yen,Shuai Wang*

Main category: cs.CL

TL;DR: 提出了一种新的稀疏-量化格式（SQ-format），以在大型语言模型（LLMs）中更好地平衡精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的低比特量化和稀疏化技术由于硬件支持有限，难以平衡精度和效率。W4A8和GPU支持的稀疏数据格式（2:4半结构稀疏）存在各自的局限性。

Method: 提出SQ-format，这是一个统一的数据格式，用于量化和稀疏化，利用稀疏矩阵在高精度下的加速能力和低精度矩阵乘法的加速能力。

Result: SQ-format实现了性能与吞吐量的帕累托改进，特别适合具有异常不平等状态的激活，并使其静态压缩成为可能，展示了最先进的PTQ性能。

Conclusion: SQ-format是一种有前景的格式，提出了支持该格式的硬件需求，并为下一代AI加速器的设计提供了探索和见解。

Abstract: Post-training quantization (PTQ) plays a crucial role in the democratization of large language models (LLMs). However, existing low-bit quantization and sparsification techniques are difficult to balance accuracy and efficiency due to the limited hardware support. For example, W4A8 can only achieve the same peak TOPS as W8A8 whereas the GPU-supported sparse data format (2:4 semi-structure sparse) is seldomly adopted due to the loss of accuracy. To bridge this gap, in this paper, we propose the Sparse-Quantized Format (SQ-format), which is a unified data format for quantization and sparsification potentially easily supported by new hardware and existing GPUs. SQ-format makes use of the fact that sparse matrix can be accelerated in high-precision, and low-precision matrix multiplication can also be accelerated accordingly. As such, SQ-format is proposed to achieve Pareto improvement between performance and throughput. This format is particularly suitable for activations with outlier inequality status and makes their static compression possible. We show the state-of-the-art PTQ performance with SQ-format, propose the hardware required to support it, and further offer the design exploration and insights for the next-generation AI accelerators.

</details>


### [10] [LMSpell: Neural Spell Checking for Low-Resource Languages](https://arxiv.org/abs/2512.05414)
*Akesh Gunathilakea,Nadil Karunarathnea,Tharusha Bandaranayakea,Nisansa de Silvaa,Surangika Ranathunga*

Main category: cs.CL

TL;DR: 本文研究了预训练语言模型在低资源语言拼写纠正中的有效性，发现大语言模型在微调数据集较大时表现更优。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的拼写纠正仍然是一个挑战，尤其是在预训练语言模型的使用上受到语言数量的限制，且缺乏对各类模型的有效比较。

Method: 通过实证方法比较了不同预训练语言模型在拼写纠正任务中的表现，并发布了LMSpell工具包，支持多种模型。

Result: 大语言模型在微调数据集较大时优于编码器-解码器等模型，即使在模型未预训练的语言中也是如此。

Conclusion: LMSpell工具包为低资源语言的拼写纠正提供了实用工具，并通过评估功能减少大语言模型的幻觉问题。

Abstract: Spell correction is still a challenging problem for low-resource languages (LRLs). While pretrained language models (PLMs) have been employed for spell correction, their use is still limited to a handful of languages, and there has been no proper comparison across PLMs. We present the first empirical study on the effectiveness of PLMs for spell correction, which includes LRLs. We find that Large Language Models (LLMs) outperform their counterparts (encoder-based and encoder-decoder) when the fine-tuning dataset is large. This observation holds even in languages for which the LLM is not pre-trained. We release LMSpell, an easy- to use spell correction toolkit across PLMs. It includes an evaluation function that compensates for the hallucination of LLMs. Further, we present a case study with Sinhala to shed light on the plight of spell correction for LRLs.

</details>


### [11] [ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering](https://arxiv.org/abs/2512.05430)
*Daeyong Kwon,SeungHeon Doh,Juhan Nam*

Main category: cs.CL

TL;DR: 本文介绍了MusWikiDB和ArtistMus，用于提升音乐领域问答的检索增强生成性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在音乐相关推理方面效果有限，因为预训练数据中音乐知识稀疏，且缺乏支持基于艺术家元数据或历史背景的音乐问答资源。

Method: 引入了MusWikiDB（来自144K音乐相关Wikipedia页面的3.2M段落的向量数据库）和ArtistMus（包含500位不同艺术家的1000个问题的基准），并进行了检索增强生成（RAG）实验。

Result: RAG显著提高了事实准确性，开源模型的性能提升达+56.8个百分点，接近专有模型。RAG风格的微调进一步提升了事实回忆和上下文推理能力。MusWikiDB比通用Wikipedia语料库检索速度快40%，准确性高6个百分点。

Conclusion: MusWikiDB和ArtistMus的发布推进了音乐信息检索和领域特定问答的研究，为音乐等文化丰富领域的检索增强推理奠定了基础。

Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.

</details>


### [12] [Dynamic Alignment for Collective Agency: Toward a Scalable Self-Improving Framework for Open-Ended LLM Alignment](https://arxiv.org/abs/2512.05464)
*Panatchakorn Anantaprayoon,Nataliia Babina,Jad Tarifi,Nima Asgharbeygi*

Main category: cs.CL

TL;DR: 本文提出了一种超越传统价值观的对齐方法，引入集体能动性（CA）作为统一且开放的对齐目标，并设计了动态对齐（Dynamic Alignment）框架，通过自动化数据生成和自奖励机制实现LLM的自我迭代对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI向通用智能（AGI/ASI）发展，传统基于人类偏好或预设原则（如帮助性、诚实性）的对齐方法存在不足且难以扩展，而现有AI自我改进方法仍受限于传统价值观。

Method: 1. 提出集体能动性（CA）作为开放对齐目标；2. 设计动态对齐框架，包含：(a) LLM自动化生成训练数据，(b) 基于GRPO的自奖励机制，模型对输出自我评分。

Result: 实验证明该方法能有效对齐CA目标，同时保持模型的通用自然语言处理能力。

Conclusion: 动态对齐框架结合CA价值观，为可扩展的、面向未来智能系统的自我改进对齐提供了新方向。

Abstract: Large Language Models (LLMs) are typically aligned with human values using preference data or predefined principles such as helpfulness, honesty, and harmlessness. However, as AI systems progress toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI), such value systems may become insufficient. In addition, human feedback-based alignment remains resource-intensive and difficult to scale. While AI-feedback-based self-improving alignment methods have been explored as a scalable alternative, they have largely remained constrained to conventional alignment values. In this work, we explore both a more holistic alignment objective and a scalable, self-improving alignment approach. Aiming to transcend conventional alignment norms, we introduce Collective Agency (CA)-a unified and open-ended alignment value that encourages integrated agentic capabilities. We also propose Dynamic Alignment-an alignment framework that enables an LLM to iteratively align itself. Dynamic Alignment comprises two key components: (1) automated training dataset generation with LLMs, and (2) a self-rewarding mechanism, where the policy model evaluates its own output candidates and assigns rewards for GRPO-based learning. Experimental results demonstrate that our approach successfully aligns the model to CA while preserving general NLP capabilities.

</details>


### [13] [SEA-SafeguardBench: Evaluating AI Safety in SEA Languages and Cultures](https://arxiv.org/abs/2512.05501)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: 本文介绍了SEA-SafeguardBench，这是首个针对东南亚语言的人类验证安全基准。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型安全评估多以英语为中心，忽视了语言和文化的多样性，东南亚语言在安全评估中代表性不足。

Method: 作者推出了SEA-SafeguardBench，涵盖8种东南亚语言，21,640个样本，包括一般、实际和内容生成三个子集。

Result: 实验结果表明，即使是先进的LLMs和防护栏在东南亚文化和危害场景中表现不佳，尤其在对比英语文本时。

Conclusion: 需要本地创作的基准来反映地方规范和危害场景，以改善东南亚语言的安全性评估。

Abstract: Safeguard models help large language models (LLMs) detect and block harmful content, but most evaluations remain English-centric and overlook linguistic and cultural diversity. Existing multilingual safety benchmarks often rely on machine-translated English data, which fails to capture nuances in low-resource languages. Southeast Asian (SEA) languages are underrepresented despite the region's linguistic diversity and unique safety concerns, from culturally sensitive political speech to region-specific misinformation. Addressing these gaps requires benchmarks that are natively authored to reflect local norms and harm scenarios. We introduce SEA-SafeguardBench, the first human-verified safety benchmark for SEA, covering eight languages, 21,640 samples, across three subsets: general, in-the-wild, and content generation. The experimental results from our benchmark demonstrate that even state-of-the-art LLMs and guardrails are challenged by SEA cultural and harm scenarios and underperform when compared to English texts.

</details>


### [14] [Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches](https://arxiv.org/abs/2512.05537)
*Namu Park,Farzad Ahmed,Zhaoyi Sun,Kevin Lybarger,Ethan Breinhorst,Julie Hu,Ozlem Uzuner,Martin Gunn,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型在细粒度病变级别检测偶发瘤方面的性能，提出了一种新颖的推理策略，通过病变标记输入和解剖感知提示，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前文档级分类系统在偶发瘤检测方面存在局限性，需要更细粒度的评估方法。

Method: 使用400份注释的放射学报告数据集，比较三种监督式基于Transformer的编码器和四种生成式LLM配置，引入病变标记输入和解剖感知提示的推理策略。

Result: 解剖感知的GPT-OSS-20b模型表现最佳，偶发瘤阳性macro-F1为0.79，超过了所有监督基线，且与人工标注的一致性接近。多数投票集成进一步将macro-F1提升至0.90。

Conclusion: 生成式LLM结合结构化病变标记和解剖上下文，显著优于传统监督编码器，性能接近人类专家，为放射学工作流中的自动化偶发瘤监测提供了可靠且可解释的方法。

Abstract: Objective: To evaluate large language models (LLMs) against supervised baselines for fine-grained, lesion-level detection of incidentalomas requiring follow-up, addressing the limitations of current document-level classification systems.
  Methods: We utilized a dataset of 400 annotated radiology reports containing 1,623 verified lesion findings. We compared three supervised transformer-based encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) against four generative LLM configurations (Llama 3.1-8B, GPT-4o, GPT-OSS-20b). We introduced a novel inference strategy using lesion-tagged inputs and anatomy-aware prompting to ground model reasoning. Performance was evaluated using class-specific F1-scores.
  Results: The anatomy-informed GPT-OSS-20b model achieved the highest performance, yielding an incidentaloma-positive macro-F1 of 0.79. This surpassed all supervised baselines (maximum macro-F1: 0.70) and closely matched the inter-annotator agreement of 0.76. Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05), while a majority-vote ensemble of the top systems further improved the macro-F1 to 0.90. Error analysis revealed that anatomy-aware LLMs demonstrated superior contextual reasoning in distinguishing actionable findings from benign lesions.
  Conclusion: Generative LLMs, when enhanced with structured lesion tagging and anatomical context, significantly outperform traditional supervised encoders and achieve performance comparable to human experts. This approach offers a reliable, interpretable pathway for automated incidental finding surveillance in radiology workflows.

</details>


### [15] [Structured Reasoning with Tree-of-Thoughts for Bengali Math Word Problems](https://arxiv.org/abs/2512.05580)
*Aurprita Mahmood,Sabrin alam,Neloy kumer Sagor,Md. Abdul Hadi,Md. Sehab Al Islam,Minhajul Islam*

Main category: cs.CL

TL;DR: 该论文系统地研究了树状思维（ToT）推理在孟加拉语数学问题中的应用，结果表明ToT在中等至大型模型中表现优于链式思维（CoT）。


<details>
  <summary>Details</summary>
Motivation: 数学文字问题（MWPs）是自然语言处理中极具挑战性的任务，需要语言理解和多步数值推理。当前的CoT方法存在误差传播的局限性，需要更有效的解决方案。

Method: 研究使用SOMADHAN数据集中的100个代表性数学问题，并在多种大模型（包括GPT-OSS和LLaMA变种）下评估标准提示、CoT和ToT策略。

Result: CoT将基线准确率从78%提高到83%，而ToT进一步将性能提高最多5个百分点，其中GPT-OSS-120B模型达到88%的准确率。

Conclusion: ToT在解决低资源语言（如孟加拉语）的数学问题时是一个稳健的框架，并且结构化推理方法如ToT在整体一致性和可靠性上优于CoT，为多语言NLP提供了更好的推理策略。

Abstract: Mathematical Word Problems (MWPs) are among the most challenging tasks in natural language processing because they require both linguistic understanding and multi-step numerical reasoning. While Chain-of-Thought (CoT) prompting has shown promise, its linear structure often propagates errors, limiting overall effectiveness. To address this limitation, we present the a systematic study of Tree-of-Thought (ToT) reasoning for Bengali MWPs using the SOMADHAN dataset. Owing to computational and token-cost constraints, we evaluate a curated set of 100 representative problems across multiple large language models (LLMs), including GPT-OSS and LLaMA variants, under standard prompting, CoT, and ToT strategies. Our results show that CoT improves baseline accuracy from 78% (standard prompting) to 83% on average, while ToT further increases performance by up to 5 percentage points, achieving 88% accuracy with GPT-OSS-120B. These improvements highlight that ToT is particularly effective in medium-to-large-scale models but may offer less advantage for smaller ones. Overall, our findings establish ToT as a robust framework for solving mathematical problems in low-resource languages such as Bengali. More broadly, this study shows that structured reasoning methods like ToT can provide more reliable and globally consistent outcomes than CoT, paving the way for better reasoning strategies in multilingual NLP.

</details>


### [16] [Grounded Multilingual Medical Reasoning for Question Answering with Large Language Models](https://arxiv.org/abs/2512.05658)
*Pietro Ferrazzi,Aitor Soroa,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文提出了一种生成基于事实医学知识的多语言推理轨迹的方法，并在医学问答任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答方法主要集中于英语，且依赖通用大型语言模型的蒸馏，这引发了对医学知识可靠性的担忧。

Method: 使用检索增强生成方法，基于维基百科医学信息生成英语、意大利语和西班牙语的500k条推理轨迹，解决来自MedQA和MedMCQA的医学问题。

Result: 在医学问答基准测试中，推理轨迹在域内和域外设置下均提升了性能，实现了8B参数模型中的最先进结果。

Conclusion: 这些资源有助于开发更安全、更透明的多语言临床决策支持工具。

Abstract: Large Language Models (LLMs) with reasoning capabilities have recently demonstrated strong potential in medical Question Answering (QA). Existing approaches are largely English-focused and primarily rely on distillation from general-purpose LLMs, raising concerns about the reliability of their medical knowledge. In this work, we present a method to generate multilingual reasoning traces grounded in factual medical knowledge. We produce 500k traces in English, Italian, and Spanish, using a retrievalaugmented generation approach over medical information from Wikipedia. The traces are generated to solve medical questions drawn from MedQA and MedMCQA, which we extend to Italian and Spanish. We test our pipeline in both in-domain and outof-domain settings across Medical QA benchmarks, and demonstrate that our reasoning traces improve performance both when utilized via in-context learning (few-shot) and supervised fine-tuning, yielding state-of-the-art results among 8B-parameter LLMs. We believe that these resources can support the development of safer, more transparent clinical decision-support tools in multilingual settings. We release the full suite of resources: reasoning traces, translated QA datasets, Medical-Wikipedia, and fine-tuned models.

</details>


### [17] [Interleaved Latent Visual Reasoning with Selective Perceptual Modeling](https://arxiv.org/abs/2512.05665)
*Shuai Dong,Siyuan Wang,Xingyu Liu,Zhongyu Wei*

Main category: cs.CL

TL;DR: ILVR框架通过动态状态演进与精确感知建模的结合，解决了多模态大型语言模型中重复编码图像的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态推理方法在计算成本和感知建模之间存在权衡，ILVR旨在通过结合动态和精确的视觉建模来克服这一限制。

Method: ILVR采用文本生成与潜在视觉表示交替进行的方法，通过动量教师模型自我监督地提取相关特征，生成上下文相关的视觉信号。

Result: 在多个多模态推理基准测试中，ILVR显著优于现有方法，有效缩小了精细感知和序列多模态推理之间的差距。

Conclusion: ILVR成功统一了动态状态演进与精确感知建模，是多模态推理领域的一项重大进展。

Abstract: Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of repeatedly re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet currently forces a critical trade-off: methods either sacrifice precise perceptual modeling by over-compressing features or fail to model dynamic problems due to static, non-interleaved structures. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. To enable this, we employ a self-supervision strategy where a Momentum Teacher Model selectively distills relevant features from helper images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR significantly outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.

</details>


### [18] [MedTutor-R1: Socratic Personalized Medical Teaching with Multi-Agent Simulation](https://arxiv.org/abs/2512.05671)
*Zhitao He,Haolin Yang,Zeyu Qin,Yi R Fung*

Main category: cs.CL

TL;DR: 提出ClinEdu多智能体教学模拟器和MedTutor-R1多模态苏格拉底导师，解决临床医学教育中一对多小组教学的数据稀缺与协作推理训练问题。


<details>
  <summary>Details</summary>
Motivation: 临床医学教育面临专家指导资源短缺与团队场景（如病房巡视）协作推理训练缺失的双重挑战，现有LLM研究多聚焦一对一教学，忽略小组互动复杂性。

Method: 1) 开发ClinEdu模拟器（含拟人化患者和多样化学生群体）生成可控教学数据；2) 构建ClinTeach苏格拉底小组教学对话数据集；3) 训练MedTutor-R1（先微调后强化学习优化，采用结构保真度/分析质量/临床安全三维度奖励机制）。

Result: MedTutor-R1较基线模型教学评分提升20%，性能接近o3模型，且在动态学生数量场景下展现强适应性；ClinEdu通过模拟交互评估验证有效性。

Conclusion: ClinEdu为临床医学教育研究提供可扩展的模拟平台，MedTutor-R1证明多模态苏格拉底策略在一对多教学场景中的高效性，为教育AI设计提供新范式。

Abstract: The significant gap between rising demands for clinical training and the scarcity of expert instruction poses a major challenge to medical education. With powerful capabilities in personalized guidance, Large Language Models (LLMs) offer a promising solution to bridge this gap. However, current research focuses mainly on one-on-one knowledge instruction, overlooking collaborative reasoning, a key skill for students developed in teamwork like ward rounds. To this end, we develop ClinEdu, a multi-agent pedagogical simulator with personality-driven patients and diverse student cohorts, enabling controlled testing of complex pedagogical processes and scalable generation of teaching data. Based on ClinEdu, we construct ClinTeach, a large Socratic teaching dialogue dataset that captures the complexities of group instruction. We then train MedTutor-R1, the first multimodal Socratic tutor designed for one-to-many instruction in clinical medical education. MedTutor-R1 is first instruction-tuned on our ClinTeach dataset and then optimized with reinforcement learning, using rewards derived from a three-axis rubric, covering structural fidelity, analytical quality, and clinical safety, to refine its adaptive Socratic strategies. For authentic in-situ assessment, we use simulation-based interactive evaluation that redeploys the tutor back into ClinEdu. Experimental results demonstrate that our MedTutor-R1 outperforms the base model by over 20% in average pedagogical score and is comparable to o3, while also exhibiting high adaptability in handling a varying number of students. This promising performance underscores the effectiveness of our pedagogical simulator, ClinEdu.

</details>


### [19] [Retrieving Semantically Similar Decisions under Noisy Institutional Labels: Robust Comparison of Embedding Methods](https://arxiv.org/abs/2512.05681)
*Tereza Novotna,Jakub Harasta*

Main category: cs.CL

TL;DR: 比较了两种模型在捷克宪法法院判决检索中的表现，通用OpenAI模型优于领域特定BERT模型。


<details>
  <summary>Details</summary>
Motivation: 案例法检索耗时且复杂，需要评估不同模型在特定法律领域的有效性。

Method: 使用了两种模型：(i) 大型通用嵌入模型 (OpenAI)，(ii) 在约30,000条判决上使用滑动窗口和注意力池化的领域特定BERT模型；并提出了包含IDF加权关键词重叠、通过两个阈值的二值化、配对自举法显著性检验和nDCG诊断的噪声感知评估方法。

Result: 通用OpenAI嵌入模型在两种设定下的@10/@20/@100均显著优于领域预训练BERT模型，且差异具有统计学显著性。

Conclusion: 尽管nDCG的绝对值较低，但诊断结果表明标签漂移和理想标准较强，而非缺乏实用性；提出的框架在带有异质标签的噪声数据集中具有稳健性。

Abstract: Retrieving case law is a time-consuming task predominantly carried out by querying databases. We provide a comparison of two models in three different settings for Czech Constitutional Court decisions: (i) a large general-purpose embedder (OpenAI), (ii) a domain-specific BERT-trained from scratch on ~30,000 decisions using sliding windows and attention pooling. We propose a noise-aware evaluation including IDF-weighted keyword overlap as graded relevance, binarization via two thresholds (0.20 balanced, 0.28 strict), significance via paired bootstrap, and an nDCG diagnosis supported with qualitative analysis. Despite modest absolute nDCG (expected under noisy labels), the general OpenAI embedder decisively outperforms the domain pre-trained BERT in both settings at @10/@20/@100 across both thresholds; differences are statistically significant. Diagnostics attribute low absolutes to label drift and strong ideals rather than lack of utility. Additionally, our framework is robust enough to be used for evaluation under a noisy gold dataset, which is typical when handling data with heterogeneous labels stemming from legacy judicial databases.

</details>


### [20] [Faithfulness metric fusion: Improving the evaluation of LLM trustworthiness across domains](https://arxiv.org/abs/2512.05700)
*Ben Malin,Tatiana Kalganova,Nikolaos Boulgouris*

Main category: cs.CL

TL;DR: 提出了一种通过结合基本忠实度指标和树模型来提升大型语言模型忠实度评估准确性的方法。


<details>
  <summary>Details</summary>
Motivation: 提高LLM输出的忠实度评估准确性，以增加对模型的信任并扩展其应用场景。

Method: 将基本忠实度指标组合成一个融合指标，利用树模型确定每个指标的重要性，并结合人类判断进行评估。

Result: 融合指标在所有测试领域中都与人类判断的相关性更强。

Conclusion: 该方法提高了LLM忠实度评估能力，并创建了可跨领域测试和复现的数据集。

Abstract: We present a methodology for improving the accuracy of faithfulness evaluation in Large Language Models (LLMs). The proposed methodology is based on the combination of elementary faithfulness metrics into a combined (fused) metric, for the purpose of improving the faithfulness of LLM outputs. The proposed strategy for metric fusion deploys a tree-based model to identify the importance of each metric, which is driven by the integration of human judgements evaluating the faithfulness of LLM responses. This fused metric is demonstrated to correlate more strongly with human judgements across all tested domains for faithfulness. Improving the ability to evaluate the faithfulness of LLMs, allows for greater confidence to be placed within models, allowing for their implementation in a greater diversity of scenarios. Additionally, we homogenise a collection of datasets across question answering and dialogue-based domains and implement human judgements and LLM responses within this dataset, allowing for the reproduction and trialling of faithfulness evaluation across domains.

</details>


### [21] [Efficient Text Classification with Conformal In-Context Learning](https://arxiv.org/abs/2512.05732)
*Ippokratis Pantelidis,Korbinian Randl,Aron Henriksson*

Main category: cs.CL

TL;DR: 本文评估了CICLe在不同NLP分类基准上的表现，证明其能提升分类效果并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本分类中依赖于提示设计并消耗大量计算资源，CICLe被提出以改善这些问题，但其广泛适用性尚未系统研究。

Method: 通过在不同NLP分类基准上全面评估CICLe，结合轻量级基础分类器和共形预测，指导LLM提示设计。

Result: CICLe在足够样本时优于基础分类器和少量提示基线，在低数据环境下表现相当，并显著提升数据与计算效率。

Conclusion: CICLe是一种实用且可扩展的高效文本分类方法，将传统分类器的稳健性与LLM的适应性结合，显著提升了数据与计算效率。

Abstract: Large Language Models (LLMs) demonstrate strong in-context learning abilities, yet their effectiveness in text classification depends heavily on prompt design and incurs substantial computational cost. Conformal In-Context Learning (CICLe) has been proposed as a resource-efficient framework that integrates a lightweight base classifier with Conformal Prediction to guide LLM prompting by adaptively reducing the set of candidate classes. However, its broader applicability and efficiency benefits beyond a single domain have not yet been systematically explored. In this paper, we present a comprehensive evaluation of CICLe across diverse NLP classification benchmarks. The results show that CICLe consistently improves over its base classifier and outperforms few-shot prompting baselines when the sample size is sufficient for training the base classifier, and performs comparably in low-data regimes. In terms of efficiency, CICLe reduces the number of shots and prompt length by up to 34.45% and 25.16%, respectively, and enables the use of smaller models with competitive performance. CICLe is furthermore particularly advantageous for text classification tasks with high class imbalance. These findings highlight CICLe as a practical and scalable approach for efficient text classification, combining the robustness of traditional classifiers with the adaptability of LLMs, and achieving substantial gains in data and computational efficiency.

</details>


### [22] [Heard or Halted? Gender, Interruptions, and Emotional Tone in U.S. Supreme Court Oral Arguments](https://arxiv.org/abs/2512.05832)
*Yifei Tong*

Main category: cs.CL

TL;DR: 研究分析了美国最高法院口头辩论中的中断现象对律师发言内容和情感的影响，发现中断未显著改变论点内容，但对女性律师的中断更具负面情绪。


<details>
  <summary>Details</summary>
Motivation: 探究司法话语中的性别动态，以及中断对律师发言内容和情感的影响，尤其是在精英机构环境中的表现。

Method: 使用ConvoKit最高法院语料库（2010-2019），分析了12,663个律师与法官互动的发言片段，通过GloVe句子嵌入量化语义变化，并通过词典分析测量情感。

Result: 中断前后的语义相似度较高，表明中断未显著改变论证内容；但对女性律师的中断包含显著的负面情绪。

Conclusion: 加深了对精英机构中性别沟通的理解，并展示了计算语言学方法在司法程序中研究权力、话语和公平性的价值。

Abstract: This study examines how interruptions during U.S. Supreme Court oral arguments shape both the semantic content and emotional tone of advocates' speech, with a focus on gendered dynamics in judicial discourse. Using the ConvoKit Supreme Court Corpus (2010-2019), we analyze 12,663 speech chunks from advocate-justice interactions to assess whether interruptions alter the meaning of an advocate's argument and whether interruptions toward female advocates exhibit more negative emotional valence. Semantic shifts are quantified using GloVe-based sentence embeddings, while sentiment is measured through lexicon-based analysis. We find that semantic similarity between pre- and post-interruption speech remains consistently high, suggesting that interruptions do not substantially alter argumentative content. However, interruptions directed at female advocates contain significantly higher levels of negative sentiment. These results deepen empirical understanding of gendered communication in elite institutional settings and demonstrate the value of computational linguistic methods for studying power, discourse, and equity in judicial proceedings.

</details>


### [23] [Prompting Science Report 4: Playing Pretend: Expert Personas Don't Improve Factual Accuracy](https://arxiv.org/abs/2512.05858)
*Savir Basil,Ina Shapiro,Dan Shapiro,Ethan Mollick,Lilach Mollick,Lennart Meincke*

Main category: cs.CL

TL;DR: 本文研究了在AI模型中分配角色是否能提高其在困难多项选择题上的表现，发现角色设定对准确率影响有限。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导者了解AI技术细节，通过严谨测试探讨角色分配对AI模型性能的影响。

Method: 在GPQA Diamond和MMLU-Pro两个基准上，对六个模型测试了三种角色分配方法：领域内专家、领域外专家和低知识角色。

Result: 角色设定对准确率没有显著提升，专家角色在某些情况下没有益处，低知识角色通常会降低准确率。

Conclusion: 角色设定在提高答案准确率方面效果有限，但可能在改变输出语气等其他方面有用。

Abstract: This is the fourth in a series of short reports that help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. Here, we ask whether assigning personas to models improves performance on difficult objective multiple-choice questions. We study both domain-specific expert personas and low-knowledge personas, evaluating six models on GPQA Diamond (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024), graduate-level questions spanning science, engineering, and law.
  We tested three approaches:
  -In-Domain Experts: Assigning the model an expert persona ("you are a physics expert") matched to the problem type (physics problems) had no significant impact on performance (with the exception of the Gemini 2.0 Flash model).
  -Off-Domain Experts (Domain-Mismatched): Assigning the model an expert persona ("you are a physics expert") not matched to the problem type (law problems) resulted in marginal differences.
  -Low-Knowledge Personas: We assigned the model negative capability personas (layperson, young child, toddler), which were generally harmful to benchmark accuracy.
  Across both benchmarks, persona prompts generally did not improve accuracy relative to a no-persona baseline. Expert personas showed no consistent benefit across models, with few exceptions. Domain-mismatched expert personas sometimes degraded performance. Low-knowledge personas often reduced accuracy. These results are about the accuracy of answers only; personas may serve other purposes (such as altering the tone of outputs), beyond improving factual performance.

</details>


### [24] [Optimizing Medical Question-Answering Systems: A Comparative Study of Fine-Tuned and Zero-Shot Large Language Models with RAG Framework](https://arxiv.org/abs/2512.05863)
*Tasnimul Hassan,Md Faisal Karim,Haziq Jeelani,Elham Behnam,Robert Green,Fayeq Jeelani Syed*

Main category: cs.CL

TL;DR: 本文提出了一种基于检索增强生成（RAG）的医疗问答系统，结合领域特定的知识检索和开源大语言模型（LLMs）来回答医学问题。


<details>
  <summary>Details</summary>
Motivation: 医疗问答系统需要保持事实准确性和避免幻觉，而直接应用LLMs到临床领域存在挑战。

Method: 使用Low-Rank Adaptation（LoRA）对两种先进的开源LLMs（LLaMA 2和Falcon）进行微调，并结合医学文献检索以增强模型回答的准确性。

Result: 在PubMedQA和MedMCQA基准数据集上的评估显示，与单独使用LLMs相比，检索增强的方法在答案准确性方面有明显提升。微调后的LLaMA 2在PubMedQA上达到71.8%的准确率，显著高于55.4%的零样本基线。

Conclusion: RAG增强的开源LLMs在可靠的生物医学问答中具有潜力，并展示了在实际临床信息学应用中的前景。

Abstract: Medical question-answering (QA) systems can benefit from advances in large language models (LLMs), but directly applying LLMs to the clinical domain poses challenges such as maintaining factual accuracy and avoiding hallucinations. In this paper, we present a retrieval-augmented generation (RAG) based medical QA system that combines domain-specific knowledge retrieval with open-source LLMs to answer medical questions. We fine-tune two state-of-the-art open LLMs (LLaMA~2 and Falcon) using Low-Rank Adaptation (LoRA) for efficient domain specialization. The system retrieves relevant medical literature to ground the LLM's answers, thereby improving factual correctness and reducing hallucinations. We evaluate the approach on benchmark datasets (PubMedQA and MedMCQA) and show that retrieval augmentation yields measurable improvements in answer accuracy compared to using LLMs alone. Our fine-tuned LLaMA~2 model achieves 71.8% accuracy on PubMedQA, substantially improving over the 55.4% zero-shot baseline, while maintaining transparency by providing source references. We also detail the system design and fine-tuning methodology, demonstrating that grounding answers in retrieved evidence reduces unsupported content by approximately 60%. These results highlight the potential of RAG-augmented open-source LLMs for reliable biomedical QA, pointing toward practical clinical informatics applications.

</details>


### [25] [M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG](https://arxiv.org/abs/2512.05959)
*David Anugraha,Patrick Amadeus Irawan,Anshul Singh,En-Shiun Annie Lee,Genta Indra Winata*

Main category: cs.CL

TL;DR: M4-RAG是一个大规模多语言多模态检索增强视觉问答（VQA）基准测试，涵盖42种语言和56种区域方言，包含超过80,000个具有文化多样性的图像-问题对。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在视觉问答（VQA）中表现优异，但它们受限于静态训练数据。虽然检索增强生成（RAG）可以缓解这一限制，但多语言多模态RAG仍大多未被探索。

Method: 作者构建了一个受控检索环境，包含数百万条精心策划的多语言文档，与查询领域相关，以在保持实验一致性的同时模拟真实检索条件。

Result: 系统评估发现，尽管RAG持续有利于小型VLMs，但它未能扩展到大模型，甚至常常降低其性能，揭示了模型规模与当前检索效果之间的关键不匹配。

Conclusion: M4-RAG为推进下一代能够在语言、模态和文化背景之间无缝推理的RAG系统奠定了基础。

Abstract: Vision-language models (VLMs) have achieved strong performance in visual question answering (VQA), yet they remain constrained by static training data. Retrieval-Augmented Generation (RAG) mitigates this limitation by enabling access to up-to-date, culturally grounded, and multilingual information; however, multilingual multimodal RAG remains largely underexplored. We introduce M4-RAG, a massive-scale benchmark covering 42 languages and 56 regional dialects and registers, comprising over 80,000 culturally diverse image-question pairs for evaluating retrieval-augmented VQA across languages and modalities. To balance realism with reproducibility, we build a controlled retrieval environment containing millions of carefully curated multilingual documents relevant to the query domains, approximating real-world retrieval conditions while ensuring consistent experimentation. Our systematic evaluation reveals that although RAG consistently benefits smaller VLMs, it fails to scale to larger models and often even degrades their performance, exposing a critical mismatch between model size and current retrieval effectiveness. M4-RAG provides a foundation for advancing next-generation RAG systems capable of reasoning seamlessly across languages, modalities, and cultural contexts.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations](https://arxiv.org/abs/2512.05156)
*Igor Halperin*

Main category: cs.AI

TL;DR: 本文提出了两种新的无监督度量标准，基于信息论和热力学，用于评估大型语言模型（LLM）的忠实性。


<details>
  <summary>Details</summary>
Motivation: 评估LLM对给定任务的忠实性是一个复杂挑战，当前方法存在局限，需要新的评估手段。

Method: 将LLM视为二分信息引擎，隐藏层作为Maxwell demon控制上下文C通过提示Q转化为答案A。使用QCA三元组建模为共享主题的概率分布，主题变换建模为转移矩阵Q和A，通过KL散度的凸优化推断矩阵，并映射到[0,1]区间得到语义忠实性（SF）指标。提出基于热力学的语义熵产生（SEP）指标。

Result: SF和SEP指标可以单独或联合使用，用于LLM评估和幻觉控制，并在企业SEC 10-K文件摘要任务上验证了框架的有效性。

Conclusion: 提出的SF和SEP指标为LLM的忠实性评估提供了新方法，高忠实性通常意味着低熵产生，这对幻觉控制具有重要意义。

Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.

</details>


### [27] [Bridging Traditional Machine Learning and Large Language Models: A Two-Part Course Design for Modern AI Education](https://arxiv.org/abs/2512.05167)
*Fang Li*

Main category: cs.AI

TL;DR: 本文介绍了一种创新的人工智能与数据科学教学方法，系统地将传统机器学习技术与现代大语言模型（LLMs）相结合。


<details>
  <summary>Details</summary>
Motivation: 为了帮助学生全面理解人工智能的演变，并掌握传统和现代技术的实用技能，设计了这门课程。

Method: 课程分为两个互补的部分：基础机器学习概念和现代LLM应用。详细描述了课程架构、实施策略、评估方法以及学习成果。

Result: 通过两个七周的暑期课程实施，发现这种综合方法增强了学生对AI领域的理解，并更好地为快速发展的AI行业的行业需求做好准备。

Conclusion: 整合传统机器学习技术和现代LLMs的教学方法，有效提升了学生对AI领域的理解和实践技能。

Abstract: This paper presents an innovative pedagogical approach for teaching artificial intelligence and data science that systematically bridges traditional machine learning techniques with modern Large Language Models (LLMs). We describe a course structured in two sequential and complementary parts: foundational machine learning concepts and contemporary LLM applications. This design enables students to develop a comprehensive understanding of AI evolution while building practical skills with both established and cutting-edge technologies. We detail the course architecture, implementation strategies, assessment methods, and learning outcomes from our summer course delivery spanning two seven-week terms. Our findings demonstrate that this integrated approach enhances student comprehension of the AI landscape and better prepares them for industry demands in the rapidly evolving field of artificial intelligence.

</details>


### [28] [On the Computability of Artificial General Intelligence](https://arxiv.org/abs/2512.05212)
*Georgios Mappouras,Charalambos Rossides*

Main category: cs.AI

TL;DR: 该论文探讨了人工智能（AI）和算法的极限，并试图定义机器可计算过程的上限。


<details>
  <summary>Details</summary>
Motivation: 近年来AI的快速发展引发了许多关于人工智能是否能达到人类水平智能（即人工通用智能，AGI）的讨论。本文旨在通过定义AGI并研究其计算极限来回答这一问题。

Method: 借鉴先前对AGI的定义，即在某些研究领域具有创造力和创新能力，从而解锁新的、以前未知的功能能力。在此基础上，论文通过形式化证明来界定算法的极限。

Result: 论文证明没有任何算法可以展示初始算法本身未包含的新功能能力，因此没有任何算法（包括AI模型）能在任何研究领域真正具备创造力。AI模型只能展示、组合和排列现有的功能能力。

Conclusion: 该证明对AI发展的未来和人类智能的起源有深远影响。AI无法实现真正的创新，而只能基于已有能力进行组合和排列。

Abstract: In recent years we observed rapid and significant advancements in artificial intelligence (A.I.). So much so that many wonder how close humanity is to developing an A.I. model that can achieve human level of intelligence, also known as artificial general intelligence (A.G.I.). In this work we look at this question and we attempt to define the upper bounds, not just of A.I., but rather of any machine-computable process (a.k.a. an algorithm). To answer this question however, one must first precisely define A.G.I. We borrow prior work's definition of A.G.I. [1] that best describes the sentiment of the term, as used by the leading developers of A.I. That is, the ability to be creative and innovate in some field of study in a way that unlocks new and previously unknown functional capabilities in that field. Based on this definition we draw new bounds on the limits of computation. We formally prove that no algorithm can demonstrate new functional capabilities that were not already present in the initial algorithm itself. Therefore, no algorithm (and thus no A.I. model) can be truly creative in any field of study, whether that is science, engineering, art, sports, etc. In contrast, A.I. models can demonstrate existing functional capabilities, as well as combinations and permutations of existing functional capabilities. We conclude this work by discussing the implications of this proof both as it regards to the future of A.I. development, as well as to what it means for the origins of human intelligence.

</details>


### [29] [Resolving Zadehs Paradox Axiomatic Possibility Theory as a Foundation for Reliable Artificial Intelligence](https://arxiv.org/abs/2512.05257)
*Bychkov Oleksii,Bychkova Sophia,Lytvynchuk Khrystyna*

Main category: cs.AI

TL;DR: 本文主张通过可能性理论，尤其是Bychkovs文章中的公理化方法，来解决不确定性处理中的危机，并展示了其在处理DST悖论方面的优势。


<details>
  <summary>Details</summary>
Motivation: 该工作的动机是解决Dempster-Shafer理论（DST）中的悖论和不确定性问题，通过可能性理论提供一个逻辑上一致且数学上严谨的基础。

Method: 采用可能性理论和必要性测度的二元工具，从头开始构建不确定性处理的基础，并进行概率、证据和可能性三种范式的比较分析。

Result: 通过一个经典的医学诊断困境示例，展示了可能性理论如何正确处理矛盾数据，避免DST的逻辑陷阱，使形式推理更接近自然智能的逻辑。

Conclusion: 可能性理论不仅是一种替代方法，而且为DST悖论提供了根本性的解决方案，展示了其在处理不确定性方面的优越性。

Abstract: This work advances and substantiates the thesis that the resolution of this crisis lies in the domain of possibility theory, specifically in the axiomatic approach developed in Bychkovs article. Unlike numerous attempts to fix Dempster rule, this approach builds from scratch a logically consistent and mathematically rigorous foundation for working with uncertainty, using the dualistic apparatus of possibility and necessity measures. The aim of this work is to demonstrate that possibility theory is not merely an alternative, but provides a fundamental resolution to DST paradoxes. A comparative analysis of three paradigms will be conducted probabilistic, evidential, and possibilistic. Using a classic medical diagnostic dilemma as an example, it will be shown how possibility theory allows for correct processing of contradictory data, avoiding the logical traps of DST and bringing formal reasoning closer to the logic of natural intelligence.

</details>


### [30] [AI & Human Co-Improvement for Safer Co-Superintelligence](https://arxiv.org/abs/2512.05356)
*Jason Weston,Jakob Foerster*

Main category: cs.AI

TL;DR: 该论文主张通过人类与AI的合作来实现共同的超级智能，以最大化协同改进。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域追求自我改进，但这充满危险且难以完全实现。因此，作者认为最大化协同改进是更可行和更好的目标。

Method: 通过合作，人类研究人员和AI共同进行AI研究，从构思到实验，提高AI系统与人类协作的能力。

Result: 通过人类和AI的共生，加速AI研究，并赋予两者更安全的超级智能。

Conclusion: 将人类研究的改进纳入循环，可以更快、更安全地实现目标。

Abstract: Self-improvement is a goal currently exciting the field of AI, but is fraught with danger, and may take time to fully achieve. We advocate that a more achievable and better goal for humanity is to maximize co-improvement: collaboration between human researchers and AIs to achieve co-superintelligence. That is, specifically targeting improving AI systems' ability to work with human researchers to conduct AI research together, from ideation to experimentation, in order to both accelerate AI research and to generally endow both AIs and humans with safer superintelligence through their symbiosis. Focusing on including human research improvement in the loop will both get us there faster, and more safely.

</details>


### [31] [ChipMind: Retrieval-Augmented Reasoning for Long-Context Circuit Design Specifications](https://arxiv.org/abs/2512.05371)
*Changwen Xing,SamZaak Wong,Xinlai Wan,Yanfeng Lu,Mengli Zhang,Zebin Ma,Lei Qi,Zhengxiong Li,Nan Guan,Zhe Jiang,Xi Wang,Jun Yang*

Main category: cs.AI

TL;DR: ChipMind是一个新颖的知识图谱增强推理框架，专为处理长集成电路（IC）规范而设计，通过构建领域特定的知识图谱ChipKG和增强推理机制，解决了大型语言模型在IC开发中受限的上下文窗口问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自动化集成电路开发中具有巨大潜力，但其实际部署受到有限的上下文窗口的限制，现有方法在实现有效的语义建模和多跳推理方面存在困难。

Method: ChipMind首先通过电路语义感知知识图谱构建方法将电路规范转化为领域特定的知识图谱ChipKG，然后利用ChipKG增强推理机制，结合信息理论自适应检索和意图感知语义过滤，动态追踪逻辑依赖并减少无关噪音。

Result: 在工业规模的规范推理基准测试中，ChipMind显著优于最先进的基线模型，平均提高了34.59%（最高达72.73%）。

Conclusion: ChipMind填补了学术研究与LLM辅助硬件设计（LAD）实际工业部署之间的关键空白。

Abstract: While Large Language Models (LLMs) demonstrate immense potential for automating integrated circuit (IC) development, their practical deployment is fundamentally limited by restricted context windows. Existing context-extension methods struggle to achieve effective semantic modeling and thorough multi-hop reasoning over extensive, intricate circuit specifications. To address this, we introduce ChipMind, a novel knowledge graph-augmented reasoning framework specifically designed for lengthy IC specifications. ChipMind first transforms circuit specifications into a domain-specific knowledge graph ChipKG through the Circuit Semantic-Aware Knowledge Graph Construction methodology. It then leverages the ChipKG-Augmented Reasoning mechanism, combining information-theoretic adaptive retrieval to dynamically trace logical dependencies with intent-aware semantic filtering to prune irrelevant noise, effectively balancing retrieval completeness and precision. Evaluated on an industrial-scale specification reasoning benchmark, ChipMind significantly outperforms state-of-the-art baselines, achieving an average improvement of 34.59% (up to 72.73%). Our framework bridges a critical gap between academic research and practical industrial deployment of LLM-aided Hardware Design (LAD).

</details>


### [32] [BEAVER: An Efficient Deterministic LLM Verifier](https://arxiv.org/abs/2512.05439)
*Tarun Suresh,Nalin Wadhwa,Debangshu Banerjee,Gagandeep Singh*

Main category: cs.AI

TL;DR: 提出BEAVER框架，用于计算大型语言模型约束满足的概率界限。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型从研究原型过渡到生产系统时，需要可靠的方法验证模型输出是否满足所需约束。

Method: BEAVER使用前缀封闭语义约束，通过新型标记前缀树和前沿数据结构系统探索生成空间，保持每一步迭代的可证明的可靠界限。

Result: 在多个先进的大型语言模型上，BEAVER在正确性验证、隐私验证和安全代码生成任务中实现了6到8倍更紧的概率界限，并比基线方法多识别3到4倍的高风险实例。

Conclusion: BEAVER提供了精确的特征刻画和风险评估，超越了松散界限或经验评估的能力。

Abstract: As large language models (LLMs) transition from research prototypes to production systems, practitioners often need reliable methods to verify that model outputs satisfy required constraints. While sampling-based estimates provide an intuition of model behavior, they offer no sound guarantees. We present BEAVER, the first practical framework for computing deterministic, sound probability bounds on LLM constraint satisfaction. Given any prefix-closed semantic constraint, BEAVER systematically explores the generation space using novel token trie and frontier data structures, maintaining provably sound bounds at every iteration. We formalize the verification problem, prove soundness of our approach, and evaluate BEAVER on correctness verification, privacy verification and secure code generation tasks across multiple state of the art LLMs. BEAVER achieves 6 to 8 times tighter probability bounds and identifies 3 to 4 times more high risk instances compared to baseline methods under identical computational budgets, enabling precise characterization and risk assessment that loose bounds or empirical evaluation cannot provide.

</details>


### [33] [MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models](https://arxiv.org/abs/2512.05530)
*Chuang Yu,Jinmiao Zhao,Mingxuan Zhao,Yunpeng Liu,Xiujun Shu,Yuanhao Feng,Bo Wang,Xiangyu Yue*

Main category: cs.AI

TL;DR: 提出了一种新型多模态大型语言模型（MLLMs）推理框架MIND，通过模拟人类“理解-反思-纠正”的认知过程，提升多理由语义建模和逻辑鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在复杂场景中存在多理由建模不足、逻辑不稳健和易受误导的问题，因此需要一种能实现主动判别推理的框架。

Method: 引入理由增强与判别（RAD）范式，设计渐进式两阶段矫正学习（P2CL）策略，并提出多理由对比对齐（MCA）优化策略。

Result: 在科学、常识和数学等多个公共数据集上实现了最先进的性能，显著提升了MLLMs的推理能力。

Conclusion: MIND框架为MLLMs向更高层次的认知智能发展提供了新视角，并通过实验验证了其在多场景下的有效性。

Abstract: Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -> Rethink -> Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND

</details>


### [34] [CureAgent: A Training-Free Executor-Analyst Framework for Clinical Reasoning](https://arxiv.org/abs/2512.05576)
*Ting-Ting Xie,Yixin Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的Executor-Analyst框架，通过分层集成策略和模块化架构改善小LLMs的上下文利用失败问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于小LLMs的临床智能体存在上下文利用失败问题，尽管能够检索生物医学证据，但无法将这些信息有效用于诊断。

Method: 提出了一种Executor-Analyst框架，将工具执行的句法精度与临床推理的语义稳健性分离，并采用分层集成策略以保留证据多样性。

Result: 该方法在不进行昂贵的端到端微调的情况下，在CURE-Bench上实现了最先进的性能。

Conclusion: 该框架为构建可信赖的AI驱动治疗学提供了一个可扩展且灵活的基础架构，并揭示了关键的可伸缩性洞见。

Abstract: Current clinical agent built on small LLMs, such as TxAgent suffer from a \textit{Context Utilization Failure}, where models successfully retrieve biomedical evidence due to supervised finetuning but fail to ground their diagnosis in that information. In this work, we propose the Executor-Analyst Framework, a modular architecture that decouples the syntactic precision of tool execution from the semantic robustness of clinical reasoning. By orchestrating specialized TxAgents (Executors) with long-context foundation models (Analysts), we mitigate the reasoning deficits observed in monolithic models. Beyond simple modularity, we demonstrate that a Stratified Ensemble strategy significantly outperforms global pooling by preserving evidentiary diversity, effectively addressing the information bottleneck. Furthermore, our stress tests reveal critical scaling insights: (1) a \textit{Context-Performance Paradox}, where extending reasoning contexts beyond 12k tokens introduces noise that degrades accuracy; and (2) the \textit{Curse of Dimensionality} in action spaces, where expanding toolsets necessitates hierarchical retrieval strategies. Crucially, our approach underscores the potential of training-free architectural engineering, achieving state-of-the-art performance on CURE-Bench without the need for expensive end-to-end finetuning. This provides a scalable, agile foundation for the next generation of trustworthy AI-driven therapeutics. Code has been released on https://github.com/June01/CureAgent.

</details>


### [35] [Ontology Learning with LLMs: A Benchmark Study on Axiom Identification](https://arxiv.org/abs/2512.05594)
*Roos M. Bakker,Daan L. Di Scala,Maaike H. T. de Boer,Stephan A. Raaijmakers*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLMs）在自动识别本体公理方面的表现，并提出了一个基准测试集OntoAxiom。


<details>
  <summary>Details</summary>
Motivation: 本体开发是一项复杂任务，需要大量建模和领域专业知识。本体学习旨在自动化这一过程，近年来由于自然语言处理技术的进步，尤其是大型语言模型的发展，取得了显著进展。

Method: 引入OntoAxiom基准测试集，系统测试不同LLMs在不同提示策略、本体和公理类型下的公理识别能力。比较了十二种LLMs，三种样本设置和两种提示策略：直接方法（一次性查询所有公理）和逐公理方法（每次查询一个公理）。

Result: 逐公理方法（AbA）比直接方法F1分数更高，但不同公理类型的识别性能有差异，且受领域影响。较大的LLMs表现优于较小的，但较小的模型在资源受限环境中仍可能有用。FOAF本体在子类公理上的得分为0.642，而音乐本体仅为0.218。

Conclusion: 尽管LLMs在公理识别方面的表现还不足以完全自动化，但可以为领域专家提供有价值的候选公理，支持本体的开发和优化。

Abstract: Ontologies are an important tool for structuring domain knowledge, but their development is a complex task that requires significant modelling and domain expertise. Ontology learning, aimed at automating this process, has seen advancements in the past decade with the improvement of Natural Language Processing techniques, and especially with the recent growth of Large Language Models (LLMs). This paper investigates the challenge of identifying axioms: fundamental ontology components that define logical relations between classes and properties. In this work, we introduce an Ontology Axiom Benchmark OntoAxiom, and systematically test LLMs on that benchmark for axiom identification, evaluating different prompting strategies, ontologies, and axiom types. The benchmark consists of nine medium-sized ontologies with together 17.118 triples, and 2.771 axioms. We focus on subclass, disjoint, subproperty, domain, and range axioms. To evaluate LLM performance, we compare twelve LLMs with three shot settings and two prompting strategies: a Direct approach where we query all axioms at once, versus an Axiom-by-Axiom (AbA) approach, where each prompt queries for one axiom only. Our findings show that the AbA prompting leads to higher F1 scores than the direct approach. However, performance varies across axioms, suggesting that certain axioms are more challenging to identify. The domain also influences performance: the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218. Larger LLMs outperform smaller ones, but smaller models may still be viable for resource-constrained settings. Although performance overall is not high enough to fully automate axiom identification, LLMs can provide valuable candidate axioms to support ontology engineers with the development and refinement of ontologies.

</details>


### [36] [Enhancing Local Search for MaxSAT with Deep Differentiation Clause Weighting](https://arxiv.org/abs/2512.05619)
*Menghua Jiang,Haokai Gao,Shuhao Chen,Yin Chen*

Main category: cs.AI

TL;DR: 本文提出了一种新的子句加权方案，首次在PMS和WPMS实例中分别更新子句权重，并引入新初始化方法和优选法，开发了SLS求解器DeepDist，在MaxSAT评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决PMS和WPMS问题时，未能充分区分两者差异，通常采用统一的子句权重更新策略，忽略了两种问题类型的关键结构差异。

Method: 提出了一种新的子句加权方案，分别针对PMS和WPMS实例更新子句权重，引入了新的初始化方法，并提出了优先满足单元和硬子句的优选法。

Result: 实验结果显示，DeepDist在MaxSAT评估的基准测试中优于当前先进的SLS求解器，与TT-Open-WBO-Inc结合的混合求解器超越了2024年MaxSAT评估的获胜者。

Conclusion: 该方案通过区分PMS和WPMS实例，采用不同的更新策略和初始化方法，显著提高了求解效率，展示了其有效性和应用潜力。

Abstract: Partial Maximum Satisfiability (PMS) and Weighted Partial Maximum Satisfiability (WPMS) generalize Maximum Satisfiability (MaxSAT), with broad real-world applications. Recent advances in Stochastic Local Search (SLS) algorithms for solving (W)PMS have mainly focused on designing clause weighting schemes. However, existing methods often fail to adequately distinguish between PMS and WPMS, typically employing uniform update strategies for clause weights and overlooking critical structural differences between the two problem types. In this work, we present a novel clause weighting scheme that, for the first time, updates the clause weights of PMS and WPMS instances according to distinct conditions. This scheme also introduces a new initialization method, which better accommodates the unique characteristics of both instance types. Furthermore, we propose a decimation method that prioritizes satisfying unit and hard clauses, effectively complementing our proposed clause weighting scheme. Building on these methods, we develop a new SLS solver for (W)PMS named DeepDist. Experimental results on benchmarks from the anytime tracks of recent MaxSAT Evaluations show that DeepDist outperforms state-of-the-art SLS solvers. Notably, a hybrid solver combining DeepDist with TT-Open-WBO-Inc surpasses the performance of the MaxSAT Evaluation 2024 winners, SPB-MaxSAT-c-Band and SPB-MaxSAT-c-FPS, highlighting the effectiveness of our approach. The code is available at https://github.com/jmhmaxsat/DeepDist

</details>


### [37] [Evolutionary System 2 Reasoning: An Empirical Proof](https://arxiv.org/abs/2512.05760)
*Zeyuan Ma,Wenqi Huang,Guo-Huan Song,Hongshu Guo,Sijie Ma,Zhiguang Cao,Yue-Jiao Gong*

Main category: cs.AI

TL;DR: 本文提出了一种进化推理优化（ERO）框架，旨在通过模拟自然选择过程提高大型语言模型（LLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在特定任务上表现出色，但它们在通用智能尤其是推理能力方面仍存在不足。作者希望通过模仿人类慢思考（系统2推理）的方式提升模型的推理能力。

Method: 提出了进化推理优化（ERO）框架，该框架在一个LLM种群中执行“适者生存”策略，以寻找具有强推理能力的个体。具体步骤包括初始化多个LLM作为种群，然后通过进化策略优化种群，最大化最佳个体的推理评分。

Result: 实验表明，最新的LLMs（如GPT-5）在系统2推理能力方面仍然有限；但通过简单的ERO进化循环，相对较弱的模型（如Qwen-7B）可以被增强，展现出强大的推理能力。

Conclusion: 通过进化推理优化框架，可以在不依赖于特定技能的情况下提升LLMs的推理能力，为发展机器智能提供了一种新的途径。

Abstract: Machine intelligence marks the ultimate dream of making machines' intelligence comparable to human beings. While recent progress in Large Language Models (LLMs) show substantial specific skills for a wide array of downstream tasks, they more or less fall shorts in general intelligence. Following correlation between intelligence and system 2 reasoning (slow thinking), in this paper, we aim to answering a worthwhile research question: could machine intelligence such as LLMs be evolved to acquire reasoning ability (not specific skill) just like our human beings? To this end, we propose evolutionary reasoning optimization (ERO) framework which performs survival of the fittest over a population of LLMs to search for individual with strong reasoning ability. Given a reasoning task, ERO first initializes multiple LLMs as a population, after which an evolutionary strategy evolves the population to maximize quantified reasoning score of the best individual. Based on experiments on representative testsuites, we claim two surprising empirical discoveries: i) the latest LLMs such as GPT-5 still show limited system 2 reasoning ability; ii) with simple evolution-loop of ERO, a relatively weak model (Qwen-7B) could be enhanced to emerge powerful reasoning ability. Our project can be accessed at https://github.com/MetaEvo/ERO for reproduction needs.

</details>


### [38] [The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics](https://arxiv.org/abs/2512.05765)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）被误认为仅能进行模式匹配，无法推理或规划。本文提出，瓶颈在于缺少一个能够选择和约束模式的System-2协调层，并通过UCCT理论和MACI架构实现。


<details>
  <summary>Details</summary>
Motivation: 反驳批评者认为LLMs无法实现通用人工智能（AGI）的观点，提出LLMs可以通过添加协调层来实现推理和规划。

Method: 引入UCCT理论，将推理建模为由有效支持、表示不匹配和自适应锚定预算控制的相变，并实现MACI架构，包含诱饵、过滤和持久化机制。

Result: 通过UCCT和MACI，将未锚定的生成视为最大似然先验的检索，而“推理”则通过锚点调整后验，实现目标导向的约束。

Conclusion: 通过LLMs实现AGI的路径在于增加System-2协调层，而不是避开LLMs。

Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.

</details>


### [39] [To Err Is Human: Systematic Quantification of Errors in Published AI Papers via LLM Analysis](https://arxiv.org/abs/2512.05925)
*Federico Bianchi,Yongchan Kwon,Zachary Izzo,Linjun Zhang,James Zou*

Main category: cs.AI

TL;DR: 使用GPT-5开发的Paper Correctness Checker检测已发表AI论文中的客观错误，发现错误数量随时间增加，并且AI可以纠正大多数错误。


<details>
  <summary>Details</summary>
Motivation: 同行评审出版物构成了新研究和知识的基础，但文献中的错误可能会被忽视，从而在后续研究中造成困惑并影响可复现性。

Method: 开发了一种基于GPT-5的论文正确性检查器，系统性地识别在顶级AI会议和期刊发表过的论文中的错误。分析聚焦于客观错误，例如公式、推导、计算、图表和表格的错误。

Result: 已发表的论文包含相当数量的客观错误，每篇论文的平均错误数量随时间增加。AI检查器识别出的潜在错误中，人类专家确认83.2%为真实错误。AI检查器还能为75.8%的错误提出修正建议。

Conclusion: 前沿大规模语言模型在检测和纠正已发表论文的客观错误方面具有潜力，有助于建立更坚实的知识基础。

Abstract: How many mistakes do published AI papers contain? Peer-reviewed publications form the foundation upon which new research and knowledge are built. Errors that persist in the literature can propagate unnoticed, creating confusion in follow-up studies and complicating reproducibility. The accelerating pace of research and the increasing demands on the peer-review system make such mistakes harder to detect and avoid. To address this, we developed a Paper Correctness Checker based on GPT-5 to systematically identify mistakes in papers previously published at top AI conferences and journals. Our analysis focuses on objective mistakes-e.g., errors in formulas, derivations, calculations, figures, and tables-that have a clearly verifiable ground truth. We intentionally exclude subjective considerations such as novelty, importance, or writing quality. We find that published papers contain a non-negligible number of objective mistakes and that the average number of mistakes per paper has increased over time-from 3.8 in NeurIPS 2021 to 5.9 in NeurIPS 2025 (55.3% increase); from 4.1 in ICLR 2018 to 5.2 in ICLR 2025; and from 5.0 in TMLR 2022/23 to 5.5 in TMLR 2025. Human experts reviewed 316 potential mistakes identified by the AI Checker and confirmed that 263 were actual mistakes, corresponding to a precision of 83.2%. While most identified issues are relatively minor, correcting them would reduce confusion in the literature and strengthen reproducibility. The AI Checker also surfaced potentially more substantive mistakes that could affect the interpretation of results. Moreover, we show that the AI Checker can propose correct fixes for 75.8% of the identified mistakes. Overall, this study highlights the potential of frontier LLMs to detect and correct objective mistakes in published papers, helping to establish a firmer foundation of knowledge.

</details>


### [40] [TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.05943)
*Shima Imani,Seungwhan Moon,Lambert Mathias,Lu Zhang,Babak Damavandi*

Main category: cs.AI

TL;DR: TRACE是一个用于透明推理和一致性评估的框架，通过辅助推理集（ARS）诊断推理路径而非仅关注最终答案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在数学和科学推理中的可靠性仍是一个挑战，标准最终答案评估容易掩盖推理错误。

Method: TRACE利用辅助推理集（ARS）分解复杂问题，通过基于一致性的指标评估中间步骤，并揭示被标准评估忽略的错误。

Result: 实验表明，ARS的一致性与最终答案的正确性相关，有助于精确定位推理步骤中的错误。

Conclusion: TRACE定义了置信区域，区分可靠与不可靠的推理路径，支持有效的过滤、调试和模型优化。

Abstract: Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.

</details>


### [41] [Variational Quantum Rainbow Deep Q-Network for Optimizing Resource Allocation Problem](https://arxiv.org/abs/2512.05946)
*Truong Thanh Hung Nguyen,Truong Thinh Nguyen,Hung Cao*

Main category: cs.AI

TL;DR: 提出VQR-DQN，结合变分量子电路与Rainbow DQN解决资源分配问题，在HRAP基准测试中表现优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 经典DRL方法受限于函数逼近器的表示能力，而量子计算的超位置和纠缠特性可能提升策略质量，解决组合复杂性导致的NP难问题。

Method: 将人力资源分配问题建模为MDP，提出VQR-DQN框架，集成环形拓扑变分量子电路与Rainbow DQN，利用量子特性增强策略表达。

Result: 在四个HRAP基准测试中，相比随机基线降低26.8%归一化makespan，比Double DQN和经典Rainbow DQN提升4.9-13.4%。

Conclusion: 量子增强DRL在大规模资源分配中展现潜力，性能提升与量子电路的表达能力和纠缠特性理论相符。

Abstract: Resource allocation remains NP-hard due to combinatorial complexity. While deep reinforcement learning (DRL) methods, such as the Rainbow Deep Q-Network (DQN), improve scalability through prioritized replay and distributional heads, classical function approximators limit their representational power. We introduce Variational Quantum Rainbow DQN (VQR-DQN), which integrates ring-topology variational quantum circuits with Rainbow DQN to leverage quantum superposition and entanglement. We frame the human resource allocation problem (HRAP) as a Markov decision process (MDP) with combinatorial action spaces based on officer capabilities, event schedules, and transition times. On four HRAP benchmarks, VQR-DQN achieves 26.8% normalized makespan reduction versus random baselines and outperforms Double DQN and classical Rainbow DQN by 4.9-13.4%. These gains align with theoretical connections between circuit expressibility, entanglement, and policy quality, demonstrating the potential of quantum-enhanced DRL for large-scale resource allocation. Our implementation is available at: https://github.com/Analytics-Everywhere-Lab/qtrl/.

</details>


### [42] [SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code](https://arxiv.org/abs/2512.05954)
*Shima Imani,Seungwhan Moon,Adel Ahmadyan,Lu Zhang,Kirmani Ahmed,Babak Damavandi*

Main category: cs.AI

TL;DR: 介绍了一个大规模合成基准SymPyBench，包含15,045个大学水平物理问题，支持多种题型，并提出了三种新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 通过多样化题型和新评估指标，测试和提升语言模型在科学推理中的鲁棒性和可解释性。

Method: 构建了包含三种题型（MC-Symbolic, MC-Numerical, free-form）的基准，并提出Consistency Score, Failure Rate, 和 Confusion Rate三种新评估指标。

Result: 实验揭示了当前语言模型在科学推理中的优势和局限性。

Conclusion: SymPyBench为开发更鲁棒和可解释的推理系统奠定了基础。

Abstract: We introduce, a large-scale synthetic benchmark of 15,045 university-level physics problems (90/10% train/test split). Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by structured, step-by-step reasoning and executable Python code that produces the ground-truth solution for any parameter set. The benchmark contains three question types: MC-Symbolic (multiple-choice with symbolic options), MC-Numerical (multiple-choice with numerical options), and free-form (open-ended responses). These diverse formats test complementary reasoning skills. By leveraging the dynamic, code-driven nature of the benchmark, we introduce three novel evaluation metrics in addition to standard accuracy: Consistency Score, Failure Rate, and Confusion Rate, that quantify variability and uncertainty across problem variants. Experiments with state-of-the-art instruction-tuned language models reveal both strengths and limitations in scientific reasoning, positioning SymPyBench as a foundation for developing more robust and interpretable reasoning systems

</details>
