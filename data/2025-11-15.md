<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Know Your Limits: Entropy Estimation Modeling for Compression and Generalization](https://arxiv.org/abs/2511.10618)
*Benjamin L. Badger,Matthew Neligeorge*

Main category: cs.CL

TL;DR: 本文介绍了一种新型编码器增强的因果解码器模型架构，提高了训练效率和压缩性能，并展示了如何在每个标记的基础上估算信息熵，以及因果模型在逼近信息熵时表现出更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前因果大型语言模型在估算语言熵方面计算上不可行，因此需要更高效的模型架构来逼近语言的信息熵，实现更高效的语言压缩。

Method: 引入编码器增强的因果解码器模型架构，通过每个标记估算信息熵，并比较不同训练策略下模型的泛化能力。

Result: 编码器增强的因果解码器模型在训练效率和压缩性能上优于因果变换器，且在逼近但不超越信息熵的情况下，模型具有更好的泛化能力。

Conclusion: 通过逼近语言熵进行训练的因果模型在泛化能力上优于忽略熵训练的模型，为语言模型训练提供了新的优化方向。

Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.

</details>


### [2] [SSR: Socratic Self-Refine for Large Language Model Reasoning](https://arxiv.org/abs/2511.10621)
*Haizhou Shi,Ye Liu,Bo Pang,Zeyu Leo Liu,Hao Wang,Silvio Savarese,Caiming Xiong,Yingbo Zhou,Semih Yavuz*

Main category: cs.CL

TL;DR: 提出了一种新颖的框架Socratic Self-Refine (SSR)，用于细粒度评估和精确改进大型语言模型的推理。


<details>
  <summary>Details</summary>
Motivation: 现有测试时框架依赖于粗略的自我验证和自我修正，限制了其在复杂任务上的有效性。

Method: 将模型响应分解为可验证的子问题-子答案对，通过控制再解决和自我一致性检查实现步骤级置信度估计。

Result: 在五个推理基准和三个大型语言模型上的实证结果表明，SSR始终优于最先进的迭代自我改进基线。

Conclusion: SSR提供了一种原则性的黑盒方法，用于评估和理解决策过程，提高推理链的准确性和可解释性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.

</details>


### [3] [Instella: Fully Open Language Models with Stellar Performance](https://arxiv.org/abs/2511.10628)
*Jiang Liu,Jialian Wu,Xiaodong Yu,Yusheng Su,Prakamya Mishra,Gowtham Ramesh,Sudhanshu Ranjan,Chaitanya Manem,Ximeng Sun,Ze Wang,Pratik Prabhanjan Brahma,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: 介绍Instella，这是一组完全开源的三十亿参数语言模型，尽管使用的训练数据少于许多同类模型，但在完全开源模型中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中表现出色，但大多数高性能模型仍然是闭源的，限制了透明性和可复现性。

Method: 利用AMD Instinct MI300X GPU进行大规模预训练、通用指令调优，并与人类偏好对齐。发布了两个专门变体：Instella-Long和Instella-Math。

Result: 尽管使用更少的预训练token，Instella在完全开源模型中实现了最先进的结果，并且与同等大小的主流开源模型性能相当。

Conclusion: Instella作为一个透明、高性能和多功能的开源模型，为社区提供了一个可复现的语言建模研究替代方案。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.

</details>


### [4] [Black-Box On-Policy Distillation of Large Language Models](https://arxiv.org/abs/2511.10643)
*Tianzhu Ye,Li Dong,Zewen Chi,Xun Wu,Shaohan Huang,Furu Wei*

Main category: cs.CL

TL;DR: 引入生成对抗蒸馏（GAD）实现黑盒蒸馏，通过生成器和判别器的对抗训练提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 黑盒蒸馏中无法访问教师模型的内部信息，现有方法存在局限性，需要更有效的蒸馏策略。

Method: 将学生模型作为生成器，训练判别器区分学生和教师模型的输出，形成对抗性训练框架。

Result: GAD在实验中表现优于常规序列级知识蒸馏，学生模型Qwen2.5-14B-Instruct与教师模型GPT-5-Chat相当。

Conclusion: GAD是黑盒大语言模型蒸馏的一种有前途且有效的范式。

Abstract: Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.

</details>


### [5] [ParoQuant: Pairwise Rotation Quantization for Efficient Reasoning LLM Inference](https://arxiv.org/abs/2511.10645)
*Yesheng Liang,Haisheng Chen,Song Han,Zhijian Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为Pairwise Rotation Quantization (ParoQuant)的权重-only后训练量化方法，通过结合硬件高效和优化的独立Givens旋转和通道缩放，以减少大语言模型（LLMs）中的异常值影响，提高量化精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法在处理权重和激活中的异常值时，要么无法充分抑制异常值，要么在推理过程中引入显著开销，尤其是在推理LLMs中误差会累积。因此，需要一种新的方法来处理异常值，减少量化误差，并提高推理效率。

Method: ParoQuant结合了硬件高效和优化的独立Givens旋转与通道缩放，以平衡通道间的大小并缩小每个量化组内的动态范围。同时，该方法还设计了推理内核，以充分利用GPU并行性，并在运行时保持旋转和缩放的轻量级。

Result: 在推理任务上，ParoQuant相较于AWQ实现了平均2.4%的精度提升，且开销不到10%。

Conclusion: ParoQuant为更高效和准确地部署推理LLMs提供了可能，是一种有效的权重-only后训练量化方法。

Abstract: Weight-only post-training quantization (PTQ) compresses the weights of Large Language Models (LLMs) into low-precision representations to reduce memory footprint and accelerate inference. However, the presence of outliers in weights and activations often leads to large quantization errors and severe accuracy degradation, especially in recent reasoning LLMs where errors accumulate across long chains of thought. Existing PTQ methods either fail to sufficiently suppress outliers or introduce significant overhead during inference. In this paper, we propose Pairwise Rotation Quantization (ParoQuant), a weight-only PTQ method that combines hardware-efficient and optimizable independent Givens rotations with channel-wise scaling to even out the magnitude across channels and narrow the dynamic range within each quantization group. We further co-design the inference kernel to fully exploit GPU parallelism and keep the rotations and scaling lightweight at runtime. ParoQuant achieves an average 2.4% accuracy improvement over AWQ on reasoning tasks with less than 10% overhead. This paves the way for more efficient and accurate deployment of reasoning LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [Querying Labeled Time Series Data with Scenario Programs](https://arxiv.org/abs/2511.10627)
*Edward Kim,Devan Shanker,Varun Bharadwaj,Hongbeen Park,Jinkyu Kim,Hazem Torfah,Daniel J Fremont,Sanjit A Seshia*

Main category: cs.AI

TL;DR: 本文提出了一种通过查询现实世界数据集以验证模拟环境中的AV失败场景是否真实再现的方法。


<details>
  <summary>Details</summary>
Motivation: 模拟环境中的失败场景是否能在实际系统中复现仍然是一个未解决的问题，这关系到模拟测试的有效性。

Method: 引入了一种正式定义，说明标记的时间序列传感器数据如何与抽象场景匹配，并使用Scenic概率编程语言表示场景程序，提出了一种查询算法。

Result: 实验显示，该算法在查询场景方面比现有商业视觉大型语言模型更准确、速度更快，并且能随时间序列数据的持续而扩展。

Conclusion: 该方法有效地验证了模拟环境中的失败场景在真实世界中的再现性，弥补了模拟与现实的差距。

Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.

</details>
