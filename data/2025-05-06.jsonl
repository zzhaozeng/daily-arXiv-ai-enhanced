{"id": "2505.01428", "pdf": "https://arxiv.org/pdf/2505.01428", "abs": "https://arxiv.org/abs/2505.01428", "authors": ["Han Yang", "Chuanguang Yang", "Qiuli Wang", "Zhulin An", "Weilun Feng", "Libo Huang", "Yongjun Xu"], "title": "Multi-party Collaborative Attention Control for Image Customization", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of diffusion models has increased the need for\ncustomized image generation. However, current customization methods face\nseveral limitations: 1) typically accept either image or text conditions alone;\n2) customization in complex visual scenarios often leads to subject leakage or\nconfusion; 3) image-conditioned outputs tend to suffer from inconsistent\nbackgrounds; and 4) high computational costs. To address these issues, this\npaper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a\ntuning-free method that enables high-quality image customization using both\ntext and complex visual conditions. Specifically, MCA-Ctrl leverages two key\noperations within the self-attention layer to coordinate multiple parallel\ndiffusion processes and guide the target image generation. This approach allows\nMCA-Ctrl to capture the content and appearance of specific subjects while\nmaintaining semantic consistency with the conditional input. Additionally, to\nmitigate subject leakage and confusion issues common in complex visual\nscenarios, we introduce a Subject Localization Module that extracts precise\nsubject and editable image layers based on user instructions. Extensive\nquantitative and human evaluation experiments show that MCA-Ctrl outperforms\nexisting methods in zero-shot image customization, effectively resolving the\nmentioned issues."}
{"id": "2505.01429", "pdf": "https://arxiv.org/pdf/2505.01429", "abs": "https://arxiv.org/abs/2505.01429", "authors": ["Md. Zahid Hossain", "Md. Rakibul Islam", "Most. Sharmin Sultana Samu"], "title": "Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Since mpox can spread from person to person, it is a zoonotic viral illness\nthat poses a significant public health concern. It is difficult to make an\nearly clinical diagnosis because of how closely its symptoms match those of\nmeasles and chickenpox. Medical imaging combined with deep learning (DL)\ntechniques has shown promise in improving disease detection by analyzing\naffected skin areas. Our study explore the feasibility to train deep learning\nand vision transformer-based models from scratch with publicly available skin\nlesion image dataset. Our experimental results show dataset limitation as a\nmajor drawback to build better classifier models trained from scratch. We used\ntransfer learning with the help of pre-trained models to get a better\nclassifier. The MobileNet-v2 outperformed other state of the art pre-trained\nmodels with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and\nResNet-50 also achieved satisfactory performance compared to already available\nstudies with accuracy 92.12% and 86.21% respectively. To further validate the\nperformance of the models, we applied explainable AI techniques."}
{"id": "2505.01430", "pdf": "https://arxiv.org/pdf/2505.01430", "abs": "https://arxiv.org/abs/2505.01430", "authors": ["Muna Numan Said", "Aarib Zaidi", "Rabia Usman", "Sonia Okon", "Praneeth Medepalli", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models", "categories": ["cs.CV"], "comment": "Published at ICLR 2025 Workshop SynthData", "summary": "The transformative potential of text-to-image (T2I) models hinges on their\nability to synthesize culturally diverse, photorealistic images from textual\nprompts. However, these models often perpetuate cultural biases embedded within\ntheir training data, leading to systemic misrepresentations. This paper\nbenchmarks the Component Inclusion Score (CIS), a metric designed to evaluate\nthe fidelity of image generation across cultural contexts. Through extensive\nanalysis involving 2,400 images, we quantify biases in terms of compositional\nfragility and contextual misalignment, revealing significant performance gaps\nbetween Western and non-Western cultural prompts. Our findings underscore the\nimpact of data imbalance, attention entropy, and embedding superposition on\nmodel fairness. By benchmarking models like Stable Diffusion with CIS, we\nprovide insights into architectural and data-centric interventions for\nenhancing cultural inclusivity in AI-generated imagery. This work advances the\nfield by offering a comprehensive tool for diagnosing and mitigating biases in\nT2I generation, advocating for more equitable AI systems."}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs."}
{"id": "2505.01431", "pdf": "https://arxiv.org/pdf/2505.01431", "abs": "https://arxiv.org/abs/2505.01431", "authors": ["Wenqi Guo", "Shan Du"], "title": "ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Camouflaged object segmentation presents unique challenges compared to\ntraditional segmentation tasks, primarily due to the high similarity in\npatterns and colors between camouflaged objects and their backgrounds.\nEffective solutions to this problem have significant implications in critical\nareas such as pest control, defect detection, and lesion segmentation in\nmedical imaging. Prior research has predominantly emphasized supervised or\nunsupervised pre-training methods, leaving zero-shot approaches significantly\nunderdeveloped. Existing zero-shot techniques commonly utilize the Segment\nAnything Model (SAM) in automatic mode or rely on vision-language models to\ngenerate cues for segmentation; however, their performances remain\nunsatisfactory, likely due to the similarity of the camouflaged object and the\nbackground. Optical flow, commonly utilized for detecting moving objects, has\ndemonstrated effectiveness even with camouflaged entities. Our method\nintegrates optical flow, a vision-language model, and SAM 2 into a sequential\npipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding\nperformance improvements, significantly outperforming existing zero-shot\nmethods by raising the F-measure ($F_\\beta^w$) from 0.296 to 0.628. Remarkably,\nour approach also surpasses supervised methods, increasing the F-measure from\n0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset\ndemonstrates an increase in the success rate from 0.628 to 0.697 when compared\nwith FlowSAM, a supervised transfer method. A thorough ablation study further\nvalidates the individual contributions of each component. More details can be\nfound on https://github.com/weathon/vcos."}
{"id": "2505.01459", "pdf": "https://arxiv.org/pdf/2505.01459", "abs": "https://arxiv.org/abs/2505.01459", "authors": ["Abdoul Majid O. Thiombiano", "Brahim Hnich", "Ali Ben Mrad", "Mohamed Wiem Mkaouer"], "title": "MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces MoxE, a novel architecture that synergistically\ncombines the Extended Long Short-Term Memory (xLSTM) with the Mixture of\nExperts (MoE) framework to address critical scalability and efficiency\nchallenges in large language models (LLMs). The proposed method effectively\nleverages xLSTM's innovative memory structures while strategically introducing\nsparsity through MoE to substantially reduce computational overhead. At the\nheart of our approach is a novel entropy-based routing mechanism, designed to\ndynamically route tokens to specialized experts, thereby ensuring efficient and\nbalanced resource utilization. This entropy awareness enables the architecture\nto effectively manage both rare and common tokens, with mLSTM blocks being\nfavored to handle rare tokens. To further enhance generalization, we introduce\na suite of auxiliary losses, including entropy-based and group-wise balancing\nlosses, ensuring robust performance and efficient training. Theoretical\nanalysis and empirical evaluations rigorously demonstrate that MoxE achieves\nsignificant efficiency gains and enhanced effectiveness compared to existing\napproaches, marking a notable advancement in scalable LLM architectures."}
{"id": "2505.01917", "pdf": "https://arxiv.org/pdf/2505.01917", "abs": "https://arxiv.org/abs/2505.01917", "authors": ["Javier E. Santos", "Agnese Marcato", "Roman Colman", "Nicholas Lubbers", "Yen Ting Lin"], "title": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling", "categories": ["cs.GR", "cond-mat.mtrl-sci", "cs.LG", "eess.IV"], "comment": null, "summary": "Generative diffusion models have achieved remarkable success in producing\nhigh-quality images. However, because these models typically operate in\ncontinuous intensity spaces - diffusing independently per pixel and color\nchannel - they are fundamentally ill-suited for applications where quantities\nsuch as particle counts or material units are inherently discrete and governed\nby strict conservation laws such as mass preservation, limiting their\napplicability in scientific workflows. To address this limitation, we propose\nDiscrete Spatial Diffusion (DSD), a framework based on a continuous-time,\ndiscrete-state jump stochastic process that operates directly in discrete\nspatial domains while strictly preserving mass in both forward and reverse\ndiffusion processes. By using spatial diffusion to achieve mass preservation,\nwe introduce stochasticity naturally through a discrete formulation. We\ndemonstrate the expressive flexibility of DSD by performing image synthesis,\nclass conditioning, and image inpainting across widely-used image benchmarks,\nwith the ability to condition on image intensity. Additionally, we highlight\nits applicability to domain-specific scientific data for materials\nmicrostructure, bridging the gap between diffusion models and mass-conditioned\nscientific applications."}
{"id": "2505.01481", "pdf": "https://arxiv.org/pdf/2505.01481", "abs": "https://arxiv.org/abs/2505.01481", "authors": ["Zongxia Li", "Xiyang Wu", "Yubin Qin", "Guangyao Shi", "Hongyang Du", "Dinesh Manocha", "Tianyi Zhou", "Jordan Lee Boyd-Graber"], "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Synthetic video generation with foundation models has gained attention for\nits realism and wide applications. While these models produce high-quality\nframes, they often fail to respect common sense and physical laws, resulting in\nabnormal content. Existing metrics like VideoScore emphasize general quality\nbut ignore such violations and lack interpretability. A more insightful\napproach is using multi-modal large language models (MLLMs) as interpretable\nevaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities\nin synthetic videos remains underexplored. To address this, we introduce\nVideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora,\nand Kling, paired with expert-designed QA tasks solvable via human-level\nreasoning across various categories. We assess several SoTA MLLMs, including\nGPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and\nVideoChat-R1. Despite strong real-world performance on MVBench and MovieChat,\nthese models still hallucinate on basic commonsense and physics tasks in\nsynthetic settings, underscoring the challenge of hallucination. We further\nfine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real\nand synthetic commonsense/physics data. Results show notable accuracy gains,\nespecially with counterexample integration, advancing MLLMs' reasoning\ncapabilities. Our data is available at https://github.com/zli12321/VideoHallu."}
{"id": "2505.01479", "pdf": "https://arxiv.org/pdf/2505.01479", "abs": "https://arxiv.org/abs/2505.01479", "authors": ["Siheng Xiong", "Jieyu Zhou", "Zhangding Liu", "Yusen Su"], "title": "SymPlanner: Deliberate Planning in Language Models with Symbolic Representation", "categories": ["cs.CL"], "comment": null, "summary": "Planning remains a core challenge for language models (LMs), particularly in\ndomains that require coherent multi-step action sequences grounded in external\nconstraints. We introduce SymPlanner, a novel framework that equips LMs with\nstructured planning capabilities by interfacing them with a symbolic\nenvironment that serves as an explicit world model. Rather than relying purely\non natural language reasoning, SymPlanner grounds the planning process in a\nsymbolic state space, where a policy model proposes actions and a symbolic\nenvironment deterministically executes and verifies their effects. To enhance\nexploration and improve robustness, we introduce Iterative Correction (IC),\nwhich refines previously proposed actions by leveraging feedback from the\nsymbolic environment to eliminate invalid decisions and guide the model toward\nvalid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained\ncomparison of candidate plans by evaluating them jointly. We evaluate\nSymPlanner on PlanBench, demonstrating that it produces more coherent, diverse,\nand verifiable plans than pure natural language baselines."}
{"id": "2505.01932", "pdf": "https://arxiv.org/pdf/2505.01932", "abs": "https://arxiv.org/abs/2505.01932", "authors": ["Xinmu Wang", "Xiang Gao", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Liang Peng", "Xianfeng Gu"], "title": "OT-Talk: Animating 3D Talking Head with Optimal Transportation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animating 3D head meshes using audio inputs has significant applications in\nAR/VR, gaming, and entertainment through 3D avatars. However, bridging the\nmodality gap between speech signals and facial dynamics remains a challenge,\noften resulting in incorrect lip syncing and unnatural facial movements. To\naddress this, we propose OT-Talk, the first approach to leverage optimal\ntransportation to optimize the learning model in talking head animation.\nBuilding on existing learning frameworks, we utilize a pre-trained Hubert model\nto extract audio features and a transformer model to process temporal\nsequences. Unlike previous methods that focus solely on vertex coordinates or\ndisplacements, we introduce Chebyshev Graph Convolution to extract geometric\nfeatures from triangulated meshes. To measure mesh dissimilarities, we go\nbeyond traditional mesh reconstruction errors and velocity differences between\nadjacent frames. Instead, we represent meshes as probability measures and\napproximate their surfaces. This allows us to leverage the sliced Wasserstein\ndistance for modeling mesh variations. This approach facilitates the learning\nof smooth and accurate facial motions, resulting in coherent and natural facial\nanimations. Our experiments on two public audio-mesh datasets demonstrate that\nour method outperforms state-of-the-art techniques both quantitatively and\nqualitatively in terms of mesh reconstruction accuracy and temporal alignment.\nIn addition, we conducted a user perception study with 20 volunteers to further\nassess the effectiveness of our approach."}
{"id": "2505.01490", "pdf": "https://arxiv.org/pdf/2505.01490", "abs": "https://arxiv.org/abs/2505.01490", "authors": ["Daoan Zhang", "Che Jiang", "Ruoshi Xu", "Biaoxiang Chen", "Zijian Jin", "Yutian Lu", "Jianguo Zhang", "Liang Yong", "Jiebo Luo", "Shengda Luo"], "title": "WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have achieved impressive\nresults, yet existing models still struggle with prompts that require rich\nworld knowledge and implicit reasoning: both of which are critical for\nproducing semantically accurate, coherent, and contextually appropriate images\nin real-world scenarios. To address this gap, we introduce\n\\textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I\nmodels' world knowledge grounding and implicit inferential capabilities,\ncovering both the humanities and nature domains. We propose the\n\\textbf{Knowledge Checklist Score}, a structured metric that measures how well\ngenerated images satisfy key semantic expectations. Experiments across 21\nstate-of-the-art models reveal that while diffusion models lead among\nopen-source methods, proprietary auto-regressive models like GPT-4o exhibit\nsignificantly stronger reasoning and knowledge integration. Our findings\nhighlight the need for deeper understanding and inference capabilities in\nnext-generation T2I systems. Project Page:\n\\href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}"}
{"id": "2505.01559", "pdf": "https://arxiv.org/pdf/2505.01559", "abs": "https://arxiv.org/abs/2505.01559", "authors": ["Daniele Grandi", "Fabian Riquelme"], "title": "On the effectiveness of Large Language Models in the mechanical design domain", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we seek to understand the performance of large language models\nin the mechanical engineering domain. We leverage the semantic data found in\nthe ABC dataset, specifically the assembly names that designers assigned to the\noverall assemblies, and the individual semantic part names that were assigned\nto each part. After pre-processing the data we developed two unsupervised tasks\nto evaluate how different model architectures perform on domain-specific data:\na binary sentence-pair classification task and a zero-shot classification task.\nWe achieved a 0.62 accuracy for the binary sentence-pair classification task\nwith a fine-tuned model that focuses on fighting over-fitting: 1) modifying\nlearning rates, 2) dropout values, 3) Sequence Length, and 4) adding a\nmulti-head attention layer. Our model on the zero-shot classification task\noutperforms the baselines by a wide margin, and achieves a top-1 classification\naccuracy of 0.386. The results shed some light on the specific failure modes\nthat arise when learning from language in this domain."}
{"id": "2505.02017", "pdf": "https://arxiv.org/pdf/2505.02017", "abs": "https://arxiv.org/abs/2505.02017", "authors": ["Yingrong Fang", "Qitong Wang", "Wei Wang"], "title": "Aokana: A GPU-Driven Voxel Rendering Framework for Open World Games", "categories": ["cs.GR"], "comment": null, "summary": "Voxels are among the most popular 3D geometric representations today. Due to\ntheir intuitiveness and ease-of-editing, voxels have been widely adopted in\nstylized games and low-cost independent games. However, the high storage cost\nof voxels, along with the significant time overhead associated with large-scale\nvoxel rendering, limits the further development of open-world voxel games. In\nthis paper, we introduce Aokana, a GPU-Driven Voxel Rendering Framework for\nOpen World Games. Aokana is based on a Sparse Voxel Directed Acyclic Graph\n(SVDAG). It incorporates a Level-of-Details (LOD) mechanism and a streaming\nsystem, enabling seamless map loading as players traverse the open-world game\nenvironment. We also designed a corresponding high-performance GPU-driven voxel\nrendering pipeline to support real-time rendering of the voxel scenes that\ncontain tens of billions of voxels. Aokana can be directly applied to existing\ngame engines and easily integrated with mesh-based rendering methods,\ndemonstrating its practical applicability in game development. Experimental\nevaluations show that, with increasing voxel scene resolution, Aokana can\nreduce memory usage by up to ninefold and achieves rendering speeds up to 4.8\ntimes faster than those of previous state-of-the-art approaches."}
{"id": "2505.01530", "pdf": "https://arxiv.org/pdf/2505.01530", "abs": "https://arxiv.org/abs/2505.01530", "authors": ["Muhammad Tayyab Khan", "Zane Yong", "Lequn Chen", "Jun Ming Tan", "Wenhe Feng", "Seung Ki Moon"], "title": "Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been submitted to the IEEE International Conference on\n  Industrial Engineering and Engineering Management (IEEM 2025)", "summary": "Accurate extraction of key information from 2D engineering drawings is\ncrucial for high-precision manufacturing. Manual extraction is time-consuming\nand error-prone, while traditional Optical Character Recognition (OCR)\ntechniques often struggle with complex layouts and overlapping symbols,\nresulting in unstructured outputs. To address these challenges, this paper\nproposes a novel hybrid deep learning framework for structured information\nextraction by integrating an oriented bounding box (OBB) detection model with a\ntransformer-based document parsing model (Donut). An in-house annotated dataset\nis used to train YOLOv11 for detecting nine key categories: Geometric\nDimensioning and Tolerancing (GD&T), General Tolerances, Measures, Materials,\nNotes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are\ncropped into images and labeled to fine-tune Donut for structured JSON output.\nFine-tuning strategies include a single model trained across all categories and\ncategory-specific models. Results show that the single model consistently\noutperforms category-specific ones across all evaluation metrics, achieving\nhigher precision (94.77% for GD&T), recall (100% for most), and F1 score\n(97.3%), while reducing hallucination (5.23%). The proposed framework improves\naccuracy, reduces manual effort, and supports scalable deployment in\nprecision-driven industries."}
{"id": "2505.01560", "pdf": "https://arxiv.org/pdf/2505.01560", "abs": "https://arxiv.org/abs/2505.01560", "authors": ["Vicent Briva Iglesias", "Gokhan Dogru"], "title": "AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) and multi-agent orchestration are touted as the\nnext leap in machine translation (MT), but their benefits relative to\nconventional neural MT (NMT) remain unclear. This paper offers an empirical\nreality check. We benchmark five paradigms, Google Translate (strong NMT\nbaseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM),\nand two GPT-4o-powered agentic workflows (sequential three-stage and iterative\nrefinement), on test data drawn from a legal contract and news prose in three\nEnglish-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is\nperformed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with\nexpert ratings of adequacy and fluency; efficiency with total input-plus-output\ntoken counts mapped to April 2025 pricing.\n  Automatic scores still favour the mature NMT system, which ranks first in\nseven of twelve metric-language combinations; o1-preview ties or places second\nin most remaining cases, while both multi-agent workflows trail. Human\nevaluation reverses part of this narrative: o1-preview produces the most\nadequate and fluent output in five of six comparisons, and the iterative agent\nedges ahead once, indicating that reasoning layers capture semantic nuance\nundervalued by surface metrics. Yet these qualitative gains carry steep costs.\nThe sequential agent consumes roughly five times, and the iterative agent\nfifteen times, the tokens used by NMT or single-pass LLMs.\n  We advocate multidimensional, cost-aware evaluation protocols and highlight\nresearch directions that could tip the balance: leaner coordination strategies,\nselective agent activation, and hybrid pipelines combining single-pass LLMs\nwith targeted agent intervention."}
{"id": "2505.02041", "pdf": "https://arxiv.org/pdf/2505.02041", "abs": "https://arxiv.org/abs/2505.02041", "authors": ["Rouli Freeman", "Alexander Sannikov", "Adrian Margel"], "title": "Holographic Radiance Cascades for 2D Global Illumination", "categories": ["cs.GR", "I.3.7"], "comment": "9 pages, 15 figures", "summary": "Efficiently calculating global illumination has always been one of the\ngreatest challenges in computer graphics. Algorithms for approximating global\nillumination have always struggled to run in realtime for fully dynamic scenes,\nand have had to rely heavily on stochastic raytracing, spatialtemporal\ndenoising, or undersampled representations, resulting in much lower quality of\nlighting compared to reference solutions. Even though the problem of\ncalculating global illumination in 2D is significantly simpler than that of 3D,\nmost contemporary approaches still struggle to accurately approximate 2D global\nillumination under realtime constraints.\n  We present Holographic Radiance Cascades: a new single-shot scene-agnostic\nradiance transfer algorithm for global illumination, which is capable of\nachieving results visually indistinguishable from the 2D reference solution at\nrealtime framerates. Our method uses a multi-level radiance probe system, and\ncomputes rays via combining short ray intervals as a replacement for\nconventional raytracing. It runs at constant cost for a given scene size,\ntaking 1.85ms for a 512x512 pixel image and 7.67ms for 1024x1024 on an RTX 3080\nLaptop."}
{"id": "2505.01548", "pdf": "https://arxiv.org/pdf/2505.01548", "abs": "https://arxiv.org/abs/2505.01548", "authors": ["Zhen Yao", "Xiaowen Ying", "Mooi Choo Chuah"], "title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation", "categories": ["cs.CV"], "comment": "12 pages, 9 figures", "summary": "Event cameras capture motion dynamics, offering a unique modality with great\npotential in various computer vision tasks. However, RGB-Event fusion faces\nthree intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal\nmisalignment. Existing voxel grid representations neglect temporal correlations\nbetween consecutive event windows, and their formulation with simple\naccumulation of asynchronous and sparse events is incompatible with the\nsynchronous and dense nature of RGB modality. To tackle these challenges, we\npropose a novel event representation, Motion-enhanced Event Tensor (MET), which\ntransforms sparse event voxels into a dense and temporally coherent form by\nleveraging dense optical flows and event temporal features. In addition, we\nintroduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a\nTemporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to\nmitigate modal misalignment, while bidirectional flow aggregation and temporal\nfusion mechanisms resolve spatiotemporal misalignment. Experimental results on\ntwo large-scale datasets demonstrate that our framework significantly\noutperforms state-of-the-art RGB-Event semantic segmentation approaches. Our\ncode is available at: https://github.com/zyaocoder/BRENet."}
{"id": "2505.01592", "pdf": "https://arxiv.org/pdf/2505.01592", "abs": "https://arxiv.org/abs/2505.01592", "authors": ["Takyoung Kim", "Janvijay Singh", "Shuhaib Mehri", "Emre Can Acikgoz", "Sagnik Mukherjee", "Nimet Beyza Bozdag", "Sumuk Shashidhar", "Gokhan Tur", "Dilek Hakkani-TÃ¼r"], "title": "PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint in progress", "summary": "The growing capabilities of large language models (LLMs) in\ninstruction-following and context-understanding lead to the era of agents with\nnumerous applications. Among these, task planning agents have become especially\nprominent in realistic scenarios involving complex internal pipelines, such as\ncontext understanding, tool management, and response generation. However,\nexisting benchmarks predominantly evaluate agent performance based on task\ncompletion as a proxy for overall effectiveness. We hypothesize that merely\nimproving task completion is misaligned with maximizing user satisfaction, as\nusers interact with the entire agentic process and not only the end result. To\naddress this gap, we propose PIPA, a unified evaluation protocol that\nconceptualizes the behavioral process of interactive task planning agents\nwithin a partially observable Markov Decision Process (POMDP) paradigm. The\nproposed protocol offers a comprehensive assessment of agent performance\nthrough a set of atomic evaluation criteria, allowing researchers and\npractitioners to diagnose specific strengths and weaknesses within the agent's\ndecision-making pipeline. Our analyses show that agents excel in different\nbehavioral stages, with user satisfaction shaped by both outcomes and\nintermediate behaviors. We also highlight future directions, including systems\nthat leverage multiple agents and the limitations of user simulators in task\nplanning."}
{"id": "2505.02061", "pdf": "https://arxiv.org/pdf/2505.02061", "abs": "https://arxiv.org/abs/2505.02061", "authors": ["Shafeequdheen P", "Jyotiranjan Nayak", "Vijayakrishna Rowthu"], "title": "Diffeomorphic Reconstruction Of A 2D Simple Non Parametric Manifold From Level Set Data Via Shape Gradients", "categories": ["cs.GR", "math.AP"], "comment": "21 pages, 105 figures", "summary": "A variational approach to the reconstruction of a shape (2D simple manifolds)\nas triangulated surface from given level set using shape gradients is\npresented. It involves an energy functional that depends on the local shape\ncharacteristics of the surface. Minimization of the energy through an iterative\nprocedure using the gradient descent method yields a triangulated surface mesh\nwhich matches the boundary of the object of interest and this model ensures the\nsmoothness of the boundary."}
{"id": "2505.01558", "pdf": "https://arxiv.org/pdf/2505.01558", "abs": "https://arxiv.org/abs/2505.01558", "authors": ["Anan Yaghmour", "Melba M. Crawford", "Saurabh Prasad"], "title": "A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning", "categories": ["cs.CV"], "comment": "Accepted in the 2025 CVPR Workshop on Foundation and Large Vision\n  Models in Remote Sensing, to appear in CVPR 2025 Workshop Proceedings", "summary": "Remote sensing enables a wide range of critical applications such as land\ncover and land use mapping, crop yield prediction, and environmental\nmonitoring. Advances in satellite technology have expanded remote sensing\ndatasets, yet high-performance segmentation models remain dependent on\nextensive labeled data, challenged by annotation scarcity and variability\nacross sensors, illumination, and geography. Domain adaptation offers a\npromising solution to improve model generalization. This paper introduces a\ndomain generalization approach to leveraging emerging geospatial foundation\nmodels by combining soft-alignment pseudo-labeling with source-to-target\ngenerative pre-training. We further provide new mathematical insights into\nMAE-based generative learning for domain-invariant feature learning.\nExperiments with hyperspectral and multispectral remote sensing datasets\nconfirm our method's effectiveness in enhancing adaptability and segmentation."}
{"id": "2505.01595", "pdf": "https://arxiv.org/pdf/2505.01595", "abs": "https://arxiv.org/abs/2505.01595", "authors": ["Liaoyaqi Wang", "Zhengping Jiang", "Anqi Liu", "Benjamin Van Durme"], "title": "Always Tell Me The Odds: Fine-grained Conditional Probability Estimation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a state-of-the-art model for fine-grained probability estimation\nof propositions conditioned on context. Recent advances in large language\nmodels (LLMs) have significantly enhanced their reasoning capabilities,\nparticularly on well-defined tasks with complete information. However, LLMs\ncontinue to struggle with making accurate and well-calibrated probabilistic\npredictions under uncertainty or partial information. While incorporating\nuncertainty into model predictions often boosts performance, obtaining reliable\nestimates of that uncertainty remains understudied. In particular, LLM\nprobability estimates tend to be coarse and biased towards more frequent\nnumbers. Through a combination of human and synthetic data creation and\nassessment, scaling to larger models, and better supervision, we propose a set\nof strong and precise probability estimation models. We conduct systematic\nevaluations across tasks that rely on conditional probability estimation and\nshow that our approach consistently outperforms existing fine-tuned and\nprompting-based methods by a large margin."}
{"id": "2505.02350", "pdf": "https://arxiv.org/pdf/2505.02350", "abs": "https://arxiv.org/abs/2505.02350", "authors": ["Bobo Lian", "Dandan Wang", "Chenjian Wu", "Minxin Chen"], "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud surface representation is a fundamental problem in computer\ngraphics and vision. This paper presents a machine learning approach for\napproximating the signed distance function (SDF) of a point cloud using sparse\nellipsoidal radial basis function networks, enabling a compact and accurate\nsurface representation. Given the SDF values defined on the grid points\nconstructed from the point cloud, our method approximates the SDF accurately\nwith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,\nrepresent the SDF of a point cloud by sparse ERBFs. To balance sparsity and\napproximation precision, a dynamic multi-objective optimization strategy is\nintroduced, which adaptively adds the regularization terms and jointly\noptimizes the weights, centers, shapes, and orientations of ERBFs. To improve\ncomputational efficiency, a nearest-neighbor-based data structure is employed,\nrestricting function calculations to points near each Gaussian kernel center.\nThe computations for each kernel are further parallelized on CUDA, which\nsignificantly improves the optimization speed. Additionally, a hierarchical\noctree-based refinement strategy is designed for training. Specifically, the\ninitialization and optimization of network parameters are conducted using\ncoarse grid points in the octree lattice structure. Subsequently, fine lattice\npoints are progressively incorporated to accelerate model convergence and\nenhance training efficiency. Extensive experiments on multiple benchmark\ndatasets demonstrate that our method outperforms previous sparse representation\napproaches in terms of accuracy, robustness, and computational efficiency. The\ncorresponding code is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git."}
{"id": "2505.01571", "pdf": "https://arxiv.org/pdf/2505.01571", "abs": "https://arxiv.org/abs/2505.01571", "authors": ["Stefanos Gkikas", "Raul Fernandez Rojas", "Manolis Tsiknakis"], "title": "PainFormer: a Vision Foundation Model for Automatic Pain Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Pain is a manifold condition that impacts a significant percentage of the\npopulation. Accurate and reliable pain evaluation for the people suffering is\ncrucial to developing effective and advanced pain management protocols.\nAutomatic pain assessment systems provide continuous monitoring and support\ndecision-making processes, ultimately aiming to alleviate distress and prevent\nfunctionality decline. This study introduces PainFormer, a vision foundation\nmodel based on multi-task learning principles trained simultaneously on 14\ntasks/datasets with a total of 10.9 million samples. Functioning as an\nembedding extractor for various input modalities, the foundation model provides\nfeature representations to the Embedding-Mixer, a transformer-based module that\nperforms the final pain assessment. Extensive experiments employing behavioral\nmodalities-including RGB, synthetic thermal, and estimated depth videos-and\nphysiological modalities such as ECG, EMG, GSR, and fNIRS revealed that\nPainFormer effectively extracts high-quality embeddings from diverse input\nmodalities. The proposed framework is evaluated on two pain datasets, BioVid\nand AI4Pain, and directly compared to 73 different methodologies documented in\nthe literature. Experiments conducted in unimodal and multimodal settings\ndemonstrate state-of-the-art performances across modalities and pave the way\ntoward general-purpose models for automatic pain assessment."}
{"id": "2505.01658", "pdf": "https://arxiv.org/pdf/2505.01658", "abs": "https://arxiv.org/abs/2505.01658", "authors": ["Sihyeong Park", "Sungryeol Jeon", "Chaelyn Lee", "Seokhun Jeon", "Byung-Soo Kim", "Jemin Lee"], "title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency", "categories": ["cs.CL"], "comment": "Under review; 65 pages; 27 figures", "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"}
{"id": "2505.02592", "pdf": "https://arxiv.org/pdf/2505.02592", "abs": "https://arxiv.org/abs/2505.02592", "authors": ["Yuki Tatsukawa", "Anran Qi", "I-Chao Shen", "Takeo Igarashi"], "title": "GarmentImage: Raster Encoding of Garment Sewing Patterns with Diverse Topologies", "categories": ["cs.GR"], "comment": "10 pages, SIGGRAPH 2025 (Conference Track)", "summary": "Garment sewing patterns are the design language behind clothing, yet their\ncurrent vector-based digital representations weren't built with machine\nlearning in mind. Vector-based representation encodes a sewing pattern as a\ndiscrete set of panels, each defined as a sequence of lines and curves,\nstitching information between panels and the placement of each panel around a\nbody. However, this representation causes two major challenges for neural\nnetworks: discontinuity in latent space between patterns with different\ntopologies and limited generalization to garments with unseen topologies in the\ntraining data. In this work, we introduce GarmentImage, a unified raster-based\nsewing pattern representation. GarmentImage encodes a garment sewing pattern's\ngeometry, topology and placement into multi-channel regular grids. Machine\nlearning models trained on GarmentImage achieve seamless transitions between\npatterns with different topologies and show better generalization capabilities\ncompared to models trained on vector-based representation. We demonstrate the\neffectiveness of GarmentImage across three applications: pattern exploration in\nlatent space, text-based pattern editing, and image-to-pattern prediction. The\nresults show that GarmentImage achieves superior performance on these\napplications using only simple convolutional networks."}
{"id": "2505.01578", "pdf": "https://arxiv.org/pdf/2505.01578", "abs": "https://arxiv.org/abs/2505.01578", "authors": ["Gabriel Sarch", "Balasaravanan Thoravi Kumaravel", "Sahithya Ravi", "Vibhav Vineet", "Andrew D. Wilson"], "title": "Grounding Task Assistance with Multimodal Cues from a Single Demonstration", "categories": ["cs.CV"], "comment": null, "summary": "A person's demonstration often serves as a key reference for others learning\nthe same task. However, RGB video, the dominant medium for representing these\ndemonstrations, often fails to capture fine-grained contextual cues such as\nintent, safety-critical environmental factors, and subtle preferences embedded\nin human behavior. This sensory gap fundamentally limits the ability of Vision\nLanguage Models (VLMs) to reason about why actions occur and how they should\nadapt to individual users. To address this, we introduce MICA (Multimodal\nInteractive Contextualized Assistance), a framework that improves\nconversational agents for task assistance by integrating eye gaze and speech\ncues. MICA segments demonstrations into meaningful sub-tasks and extracts\nkeyframes and captions that capture fine-grained intent and user-specific cues,\nenabling richer contextual grounding for visual question answering. Evaluations\non questions derived from real-time chat-assisted task replication show that\nmultimodal cues significantly improve response quality over frame-based\nretrieval. Notably, gaze cues alone achieves 93% of speech performance, and\ntheir combination yields the highest accuracy. Task type determines the\neffectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the\nneed for adaptable multimodal models. These results highlight the limitations\nof frame-based context and demonstrate the value of multimodal signals for\nreal-world AI task assistance."}
{"id": "2505.01693", "pdf": "https://arxiv.org/pdf/2505.01693", "abs": "https://arxiv.org/abs/2505.01693", "authors": ["Brian Wong", "Kaito Tanaka"], "title": "High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers", "categories": ["cs.CL"], "comment": null, "summary": "Automated labeling of chest X-ray reports is essential for enabling\ndownstream tasks such as training image-based diagnostic models, population\nhealth studies, and clinical decision support. However, the high variability,\ncomplexity, and prevalence of negation and uncertainty in these free-text\nreports pose significant challenges for traditional Natural Language Processing\nmethods. While large language models (LLMs) demonstrate strong text\nunderstanding, their direct application for large-scale, efficient labeling is\nlimited by computational cost and speed. This paper introduces DeBERTa-RAD, a\nnovel two-stage framework that combines the power of state-of-the-art LLM\npseudo-labeling with efficient DeBERTa-based knowledge distillation for\naccurate and fast chest X-ray report labeling. We leverage an advanced LLM to\ngenerate high-quality pseudo-labels, including certainty statuses, for a large\ncorpus of reports. Subsequently, a DeBERTa-Base model is trained on this\npseudo-labeled data using a tailored knowledge distillation strategy. Evaluated\non the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a\nstate-of-the-art Macro F1 score of 0.9120, significantly outperforming\nestablished rule-based systems, fine-tuned transformer models, and direct LLM\ninference, while maintaining a practical inference speed suitable for\nhigh-throughput applications. Our analysis shows particular strength in\nhandling uncertain findings. This work demonstrates a promising path to\novercome data annotation bottlenecks and achieve high-performance medical text\nprocessing through the strategic combination of LLM capabilities and efficient\nstudent models trained via distillation."}
{"id": "2505.01486", "pdf": "https://arxiv.org/pdf/2505.01486", "abs": "https://arxiv.org/abs/2505.01486", "authors": ["Mingfeng Tang", "Ziyuan Xie", "Ke Xie", "Hui Huang", "Jianwei Hu", "Ningna Wang", "Xiaohu Guo"], "title": "Aerial Path Online Planning for Urban Scene Updation", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "We present the first scene-update aerial path planning algorithm specifically\ndesigned for detecting and updating change areas in urban environments. While\nexisting methods for large-scale 3D urban scene reconstruction focus on\nachieving high accuracy and completeness, they are inefficient for scenarios\nrequiring periodic updates, as they often re-explore and reconstruct entire\nscenes, wasting significant time and resources on unchanged areas. To address\nthis limitation, our method leverages prior reconstructions and change\nprobability statistics to guide UAVs in detecting and focusing on areas likely\nto have changed. Our approach introduces a novel changeability heuristic to\nevaluate the likelihood of changes, driving the planning of two flight paths: a\nprior path informed by static priors and a dynamic real-time path that adapts\nto newly detected changes. The framework integrates surface sampling and\ncandidate view generation strategies, ensuring efficient coverage of change\nareas with minimal redundancy. Extensive experiments on real-world urban\ndatasets demonstrate that our method significantly reduces flight time and\ncomputational overhead, while maintaining high-quality updates comparable to\nfull-scene re-exploration and reconstruction. These contributions pave the way\nfor efficient, scalable, and adaptive UAV-based scene updates in complex urban\nenvironments."}
{"id": "2505.01583", "pdf": "https://arxiv.org/pdf/2505.01583", "abs": "https://arxiv.org/abs/2505.01583", "authors": ["Jen-Hao Cheng", "Vivian Wang", "Huayu Wang", "Huapeng Zhou", "Yi-Hao Peng", "Hou-I Liu", "Hsiang-Wei Huang", "Kuang-Ming Chen", "Cheng-Yen Yang", "Wenhao Chai", "Yi-Ling Chen", "Vibhav Vineet", "Qin Cai", "Jenq-Neng Hwang"], "title": "TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding causal event relationships and achieving fine-grained temporal\ngrounding in videos remain challenging for vision-language models. Existing\nmethods either compress video tokens to reduce temporal resolution, or treat\nvideos as unsegmented streams, which obscures fine-grained event boundaries and\nlimits the modeling of causal dependencies. We propose TEMPURA (Temporal Event\nMasked Prediction and Understanding for Reasoning in Action), a two-stage\ntraining framework that enhances video temporal understanding. TEMPURA first\napplies masked event prediction reasoning to reconstruct missing events and\ngenerate step-by-step causal explanations from dense event annotations, drawing\ninspiration from effective infilling techniques. TEMPURA then learns to perform\nvideo segmentation and dense captioning to decompose videos into\nnon-overlapping events with detailed, timestamp-aligned descriptions. We train\nTEMPURA on VER, a large-scale dataset curated by us that comprises 1M training\ninstances and 500K videos with temporally aligned event descriptions and\nstructured reasoning steps. Experiments on temporal grounding and highlight\ndetection benchmarks demonstrate that TEMPURA outperforms strong baseline\nmodels, confirming that integrating causal reasoning with fine-grained temporal\nsegmentation leads to improved video understanding."}
{"id": "2505.01731", "pdf": "https://arxiv.org/pdf/2505.01731", "abs": "https://arxiv.org/abs/2505.01731", "authors": ["Chuan Sun", "Han Yu", "Lizhen Cui"], "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Pruning large language models (LLMs) is a promising solution for reducing\nmodel sizes and computational complexity while preserving performance.\nTraditional layer-wise pruning methods often adopt a uniform sparsity approach\nacross all layers, which leads to suboptimal performance due to the varying\nsignificance of individual transformer layers within the model not being\naccounted for. To this end, we propose the \\underline{S}hapley\n\\underline{V}alue-based \\underline{N}on-\\underline{U}niform \\underline{P}runing\n(\\methodname{}) method for LLMs. This approach quantifies the contribution of\neach transformer layer to the overall model performance, enabling the\nassignment of tailored pruning budgets to different layers to retain critical\nparameters. To further improve efficiency, we design the Sliding Window-based\nShapley Value approximation method. It substantially reduces computational\noverhead compared to exact SV calculation methods. Extensive experiments on\nvarious LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness\nof the proposed approach. The results reveal that non-uniform pruning\nsignificantly enhances the performance of pruned models. Notably, \\methodname{}\nachieves a reduction in perplexity (PPL) of 18.01\\% and 19.55\\% on LLaMA-7B and\nLLaMA-13B, respectively, compared to SparseGPT at 70\\% sparsity."}
{"id": "2505.01779", "pdf": "https://arxiv.org/pdf/2505.01779", "abs": "https://arxiv.org/abs/2505.01779", "authors": ["Antoine Chan-Lock", "Miguel Otaduy"], "title": "Polar Interpolants for Thin-Shell Microstructure Homogenization", "categories": ["physics.comp-ph", "cs.GR"], "comment": null, "summary": "This paper introduces a new formulation for material homogenization of\nthin-shell microstructures. It addresses important challenges that limit the\nquality of previous approaches: methods that fit the energy response neglect\nvisual impact, methods that fit the stress response are not conservative, and\nall of them are limited to a low-dimensional interplay between deformation\nmodes. The new formulation is rooted on the following design principles: the\nmaterial energy functions are conservative by definition, they are formulated\non the high-dimensional membrane and bending domain to capture the complex\ninterplay of the different deformation modes, the material function domain is\nmaximally aligned with the training data, and the material parameters and the\noptimization are formulated on stress instead of energy for better correlation\nwith visual impact. The key novelty of our formulation is a new type of\nhigh-order RBF interpolant for polar coordinates, which allows us to fulfill\nall the design principles. We design a material function using this novel\ninterpolant, as well as an overall homogenization workflow. Our results\ndemonstrate very accurate fitting of diverse microstructure behaviors, both\nquantitatively and qualitatively superior to previous work."}
{"id": "2505.01615", "pdf": "https://arxiv.org/pdf/2505.01615", "abs": "https://arxiv.org/abs/2505.01615", "authors": ["Dimitrios Dagdilelis", "Panagiotis Grigoriadis", "Roberto Galeazzi"], "title": "Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a cross attention transformer based method for multimodal sensor\nfusion to build a birds eye view of a vessels surroundings supporting safer\nautonomous marine navigation. The model deeply fuses multiview RGB and long\nwave infrared images with sparse LiDAR point clouds. Training also integrates X\nband radar and electronic chart data to inform predictions. The resulting view\nprovides a detailed reliable scene representation improving navigational\naccuracy and robustness. Real world sea trials confirm the methods\neffectiveness even in adverse weather and complex maritime settings."}
{"id": "2505.01761", "pdf": "https://arxiv.org/pdf/2505.01761", "abs": "https://arxiv.org/abs/2505.01761", "authors": ["Tobias Domhan", "Dawei Zhu"], "title": "Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Accurately evaluating machine-translated text remains a long-standing\nchallenge, particularly for long documents. Recent work has shown that large\nlanguage models (LLMs) can serve as reliable and interpretable sentence-level\ntranslation evaluators via MQM error span annotations. With modern LLMs\nsupporting larger context windows, a natural question arises: can we feed\nentire document translations into an LLM for quality assessment? Ideally,\nevaluation should be invariant to text length, producing consistent error spans\nregardless of input granularity. However, our analysis shows that text length\nsignificantly impacts evaluation: longer texts lead to fewer error spans and\nreduced system ranking accuracy. To address this limitation, we evaluate\nseveral strategies, including granularity-aligned prompting, Focus Sentence\nPrompting (FSP), and a fine-tuning approach to better align LLMs with the\nevaluation task. The latter two methods largely mitigate this length bias,\nmaking LLMs more reliable for long-form translation evaluation."}
{"id": "2505.01650", "pdf": "https://arxiv.org/pdf/2505.01650", "abs": "https://arxiv.org/abs/2505.01650", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability", "categories": ["cs.CV", "eess.IV"], "comment": "This paper has been accepted at the 18th International Conference on\n  Space Operations (SpaceOps 2025)", "summary": "The rapid expansion of advanced low-Earth orbit (LEO) satellites in large\nconstellations is positioning space assets as key to the future, enabling\nglobal internet access and relay systems for deep space missions. A solution to\nthe challenge is effective space object detection (SOD) for collision\nassessment and avoidance. In SOD, an LEO satellite must detect other satellites\nand objects with high precision and minimal delay. This paper investigates the\nfeasibility and effectiveness of employing vision sensors for SOD tasks based\non deep learning (DL) models. It introduces models based on the\nSqueeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the\nGeneralized Efficient Layer Aggregation Network (GELAN) and evaluates their\nperformance under SOD scenarios. Experimental results show that the proposed\nmodels achieve mean average precision at intersection over union threshold 0.5\n(mAP50) scores of up to 0.751 and mean average precision averaged over\nintersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to\n0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model\nincreases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from\n0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to\n5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\\%."}
{"id": "2505.01794", "pdf": "https://arxiv.org/pdf/2505.01794", "abs": "https://arxiv.org/abs/2505.01794", "authors": ["Jared D. T. Guerrero-Sosa", "Francisco P. Romero", "VÃ­ctor Hugo MenÃ©ndez-DomÃ­nguez", "Jesus Serrano-Guerrero", "Andres Montoro-Montarroso", "Jose A. Olivas"], "title": "A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "In the rapidly evolving educational landscape, the unbiased assessment of\nsoft skills is a significant challenge, particularly in higher education. This\npaper presents a fuzzy logic approach that employs a Granular Linguistic Model\nof Phenomena integrated with multimodal analysis to evaluate soft skills in\nundergraduate students. By leveraging computational perceptions, this approach\nenables a structured breakdown of complex soft skill expressions, capturing\nnuanced behaviours with high granularity and addressing their inherent\nuncertainties, thereby enhancing interpretability and reliability. Experiments\nwere conducted with undergraduate students using a developed tool that assesses\nsoft skills such as decision-making, communication, and creativity. This tool\nidentifies and quantifies subtle aspects of human interaction, such as facial\nexpressions and gesture recognition. The findings reveal that the framework\neffectively consolidates multiple data inputs to produce meaningful and\nconsistent assessments of soft skills, showing that integrating multiple\nmodalities into the evaluation process significantly improves the quality of\nsoft skills scores, making the assessment work transparent and understandable\nto educational stakeholders."}
{"id": "2505.01656", "pdf": "https://arxiv.org/pdf/2505.01656", "abs": "https://arxiv.org/abs/2505.01656", "authors": ["Chenyang Fan", "Xujie Zhu", "Taige Luo", "Sheng Xu", "Zhulin Chen", "Hongxin Yang"], "title": "A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory", "categories": ["cs.CV"], "comment": null, "summary": "The pattern analysis of tree structure holds significant scientific value for\ngenetic breeding and forestry management. The current trunk and branch\nextraction technologies are mainly LiDAR-based or UAV-based. The former\napproaches obtain high-precision 3D data, but its equipment cost is high and\nthe three-dimensional (3D) data processing is complex. The latter approaches\nefficiently capture canopy information, but they miss the 3-D structure of\ntrees. In order to deal with the branch information extraction from the complex\nbackground interference and occlusion, this work proposes a novel WaveInst\ninstance segmentation framework, involving a discrete wavelet transform, to\nenhance multi-scale edge information for accurately improving tree structure\nextraction. Experimental results of the proposed model show superior\nperformance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset.\nMoreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated\nto extract tree structure and pattern analysis from artificial forest. The\nproposed method achieves a mean average precision of 49.6 and 24.3 for the\nstructure extraction of mature and juvenile trees, respectively, surpassing the\nexisting state-of-the-art method by 9.9. Furthermore, by in tegrating the\nsegmentation model within the regression model, we accurately achieve\nsignificant tree grown parameters, such as the location of trees, the\ndiameter-at-breast-height of individual trees, and the plant height, from 2D\nimages directly. This study provides a scientific and plenty of data for tree\nstructure analysis in related to the phenotype research, offering a platform\nfor the significant applications in precision forestry, ecological monitoring,\nand intelligent breeding."}
{"id": "2505.01800", "pdf": "https://arxiv.org/pdf/2505.01800", "abs": "https://arxiv.org/abs/2505.01800", "authors": ["Chidimma Opara"], "title": "Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8", "summary": "The increasing sophistication of AI-generated texts highlights the urgent\nneed for accurate and transparent detection tools, especially in educational\nsettings, where verifying authorship is essential. Existing literature has\ndemonstrated that the application of stylometric features with machine learning\nclassifiers can yield excellent results. Building on this foundation, this\nstudy proposes a comprehensive framework that integrates stylometric analysis\nwith psycholinguistic theories, offering a clear and interpretable approach to\ndistinguishing between AI-generated and human-written texts. This research\nspecifically maps 31 distinct stylometric features to cognitive processes such\nas lexical retrieval, discourse planning, cognitive load management, and\nmetacognitive self-monitoring. In doing so, it highlights the unique\npsycholinguistic patterns found in human writing. Through the intersection of\ncomputational linguistics and cognitive science, this framework contributes to\nthe development of reliable tools aimed at preserving academic integrity in the\nera of generative AI."}
{"id": "2505.01664", "pdf": "https://arxiv.org/pdf/2505.01664", "abs": "https://arxiv.org/abs/2505.01664", "authors": ["Yi-Ming Zhai", "Chuan-Xian Ren", "Hong Yan"], "title": "Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual domain adaptation aims to learn discriminative and domain-invariant\nrepresentation for an unlabeled target domain by leveraging knowledge from a\nlabeled source domain. Partial domain adaptation (PDA) is a general and\npractical scenario in which the target label space is a subset of the source\none. The challenges of PDA exist due to not only domain shift but also the\nnon-identical label spaces of domains. In this paper, a Soft-masked Semi-dual\nOptimal Transport (SSOT) method is proposed to deal with the PDA problem.\nSpecifically, the class weights of domains are estimated, and then a reweighed\nsource domain is constructed, which is favorable in conducting\nclass-conditional distribution matching with the target domain. A soft-masked\ntransport distance matrix is constructed by category predictions, which will\nenhance the class-oriented representation ability of optimal transport in the\nshared feature space. To deal with large-scale optimal transport problems, the\nsemi-dual formulation of the entropy-regularized Kantorovich problem is\nemployed since it can be optimized by gradient-based algorithms. Further, a\nneural network is exploited to approximate the Kantorovich potential due to its\nstrong fitting ability. This network parametrization also allows the\ngeneralization of the dual variable outside the supports of the input\ndistribution. The SSOT model is built upon neural networks, which can be\noptimized alternately in an end-to-end manner. Extensive experiments are\nconducted on four benchmark datasets to demonstrate the effectiveness of SSOT."}
{"id": "2505.01812", "pdf": "https://arxiv.org/pdf/2505.01812", "abs": "https://arxiv.org/abs/2505.01812", "authors": ["Core Francisco Park", "Zechen Zhang", "Hidenori Tanaka"], "title": "$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Humans and intelligent animals can effortlessly internalize new information\n(\"news\") and accurately extract the implications for performing downstream\ntasks. While large language models (LLMs) can achieve this through in-context\nlearning (ICL) when the news is explicitly given as context, fine-tuning\nremains challenging for the models to consolidate learning in weights. In this\npaper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet\nplausible news spanning multiple domains (mathematics, coding, discoveries,\nleaderboards, events), accompanied by downstream evaluation questions whose\ncorrect answers critically depend on understanding and internalizing the news.\nWe first demonstrate a substantial gap between naive fine-tuning and in-context\nlearning (FT-ICL gap) on our news dataset. To address this gap, we explore a\nsuite of self-play data generation protocols -- paraphrases, implications and\nSelf-QAs -- designed to distill the knowledge from the model with context into\nthe weights of the model without the context, which we term $\\textit{System-2\nFine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance\nacross data domains and model scales with the Qwen 2.5 family of models. Our\nresults demonstrate that the self-QA protocol of Sys2-FT significantly improves\nmodels' in-weight learning of the news. Furthermore, we discover the\n$\\textit{contexual shadowing effect}$, where training with the news $\\textit{in\ncontext}$ followed by its rephrases or QAs degrade learning of the news.\nFinally, we show preliminary evidence of an emerging scaling law of Sys2-FT."}
{"id": "2505.01680", "pdf": "https://arxiv.org/pdf/2505.01680", "abs": "https://arxiv.org/abs/2505.01680", "authors": ["Tamim Ahmed", "Thanassis Rikakis"], "title": "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study", "categories": ["cs.CV", "cs.AI", "cs.HC", "math.PR"], "comment": null, "summary": "Manual scoring of the Action Research Arm Test (ARAT) for upper extremity\nassessment in stroke rehabilitation is time-intensive and variable. We propose\nan automated ARAT scoring system integrating multimodal video analysis with\nSlowFast, I3D, and Transformer-based models using OpenPose keypoints and object\nlocations. Our approach employs multi-view data (ipsilateral, contralateral,\nand top perspectives), applying early and late fusion to combine features\nacross views and models. Hierarchical Bayesian Models (HBMs) infer movement\nquality components, enhancing interpretability. A clinician dashboard displays\ntask scores, execution times, and quality assessments. We conducted a study\nwith five clinicians who reviewed 500 video ratings generated by our system,\nproviding feedback on its accuracy and usability. Evaluated on a stroke\nrehabilitation dataset, our framework achieves 89.0% validation accuracy with\nlate fusion, with HBMs aligning closely with manual assessments. This work\nadvances automated rehabilitation by offering a scalable, interpretable\nsolution with clinical validation."}
{"id": "2505.01855", "pdf": "https://arxiv.org/pdf/2505.01855", "abs": "https://arxiv.org/abs/2505.01855", "authors": ["Anthony Nguyen", "Wenjun Lin"], "title": "Intra-Layer Recurrence in Transformers for Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Canadian AI 2025. Code available at\n  https://github.com/ant-8/Layer-Recurrent-Transformers", "summary": "Transformer models have established new benchmarks in natural language\nprocessing; however, their increasing depth results in substantial growth in\nparameter counts. While existing recurrent transformer methods address this\nissue by reprocessing layers multiple times, they often apply recurrence\nindiscriminately across entire blocks of layers. In this work, we investigate\nIntra-Layer Recurrence (ILR), a more targeted approach that applies recurrence\nselectively to individual layers within a single forward pass. Our experiments\nshow that allocating more iterations to earlier layers yields optimal results.\nThese findings suggest that ILR offers a promising direction for optimizing\nrecurrent structures in transformer architectures."}
{"id": "2505.01694", "pdf": "https://arxiv.org/pdf/2505.01694", "abs": "https://arxiv.org/abs/2505.01694", "authors": ["Dazhi Huang"], "title": "Topology-Aware CLIP Few-Shot Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficiently adapting large Vision-Language Models (VLMs) like CLIP for\nfew-shot learning poses challenges in balancing pre-trained knowledge retention\nand task-specific adaptation. Existing methods often overlook valuable\nstructural information within the VLM's latent space. We introduce a\ntopology-aware tuning approach integrating Representation Topology Divergence\n(RTD) into the Task Residual (TR) framework. By explicitly aligning the\ntopological structures of visual and text representations using a combined RTD\nand Cross-Entropy loss, while freezing base VLM encoders, our method enhances\nfew-shot performance. We optimize only lightweight Task Residual parameters,\neffectively leveraging topological information. Across 6 diverse benchmark\ndatasets, our approach demonstrates significant gains, achieving an average\naccuracy improvement of 1-2\\% over relevant baseline methods in few-shot\nsettings. This work presents an effective strategy to boost VLM few-shot\ncapabilities by incorporating topological alignment."}
{"id": "2505.01868", "pdf": "https://arxiv.org/pdf/2505.01868", "abs": "https://arxiv.org/abs/2505.01868", "authors": ["Mo Sun", "Siheng Xiong", "Yuankai Cai", "Bowen Zuo"], "title": "Positional Attention for Efficient BERT-Based Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a framework for Named Entity Recognition (NER) leveraging\nthe Bidirectional Encoder Representations from Transformers (BERT) model in\nnatural language processing (NLP). NER is a fundamental task in NLP with broad\napplicability across downstream applications. While BERT has established itself\nas a state-of-the-art model for entity recognition, fine-tuning it from scratch\nfor each new application is computationally expensive and time-consuming. To\naddress this, we propose a cost-efficient approach that integrates positional\nattention mechanisms into the entity recognition process and enables effective\ncustomization using pre-trained parameters. The framework is evaluated on a\nKaggle dataset derived from the Groningen Meaning Bank corpus and achieves\nstrong performance with fewer training epochs. This work contributes to the\nfield by offering a practical solution for reducing the training cost of\nBERT-based NER systems while maintaining high accuracy."}
{"id": "2505.01699", "pdf": "https://arxiv.org/pdf/2505.01699", "abs": "https://arxiv.org/abs/2505.01699", "authors": ["Yifan Liu", "Ruichen Yao", "Yaokun Liu", "Ruohan Zong", "Zelin Li", "Yang Zhang", "Dong Wang"], "title": "Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning", "categories": ["cs.CV", "cs.AI", "I.2.10; K.4.1"], "comment": "Accepted by ACM FAccT 2025", "summary": "The widespread integration of face recognition technologies into various\napplications (e.g., access control and personalized advertising) necessitates a\ncritical emphasis on fairness. While previous efforts have focused on\ndemographic fairness, the fairness of individual biological face components\nremains unexplored. In this paper, we focus on face component fairness, a\nfairness notion defined by biological face features. To our best knowledge, our\nwork is the first work to mitigate bias of face attribute prediction at the\nbiological feature level. In this work, we identify two key challenges in\noptimizing face component fairness: attribute label scarcity and attribute\ninter-dependencies, both of which limit the effectiveness of bias mitigation\nfrom previous approaches. To address these issues, we propose \\textbf{B}ayesian\n\\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which\nincorporates a Bayesian Network calibrator to guide an adaptive\nmeta-learning-based sample reweighting process. During the training process of\nour approach, the Bayesian Network calibrator dynamically tracks model bias and\nencodes prior probabilities for face component attributes to overcome the above\nchallenges. To demonstrate the efficacy of our approach, we conduct extensive\nexperiments on a large-scale real-world human face dataset. Our results show\nthat BNMR is able to consistently outperform recent face bias mitigation\nbaselines. Moreover, our results suggest a positive impact of face component\nfairness on the commonly considered demographic fairness (e.g.,\n\\textit{gender}). Our findings pave the way for new research avenues on face\ncomponent fairness, suggesting that face component fairness could serve as a\npotential surrogate objective for demographic fairness. The code for our work\nis publicly\navailable~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}."}
{"id": "2505.01877", "pdf": "https://arxiv.org/pdf/2505.01877", "abs": "https://arxiv.org/abs/2505.01877", "authors": ["JiÅÃ­ MiliÄka", "Anna MarklovÃ¡", "OndÅej Drobil", "Eva PospÃ­Å¡ilovÃ¡"], "title": "Humans can learn to detect AI-generated texts, or at least learn when they can't", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates whether individuals can learn to accurately\ndiscriminate between human-written and AI-produced texts when provided with\nimmediate feedback, and if they can use this feedback to recalibrate their\nself-perceived competence. We also explore the specific criteria individuals\nrely upon when making these decisions, focusing on textual style and perceived\nreadability.\n  We used GPT-4o to generate several hundred texts across various genres and\ntext types comparable to Koditex, a multi-register corpus of human-written\ntexts. We then presented randomized text pairs to 255 Czech native speakers who\nidentified which text was human-written and which was AI-generated.\nParticipants were randomly assigned to two conditions: one receiving immediate\nfeedback after each trial, the other receiving no feedback until experiment\ncompletion. We recorded accuracy in identification, confidence levels, response\ntimes, and judgments about text readability along with demographic data and\nparticipants' engagement with AI technologies prior to the experiment.\n  Participants receiving immediate feedback showed significant improvement in\naccuracy and confidence calibration. Participants initially held incorrect\nassumptions about AI-generated text features, including expectations about\nstylistic rigidity and readability. Notably, without feedback, participants\nmade the most errors precisely when feeling most confident -- an issue largely\nresolved among the feedback group.\n  The ability to differentiate between human and AI-generated texts can be\neffectively learned through targeted training with explicit feedback, which\nhelps correct misconceptions about AI stylistic features and readability, as\nwell as potential other variables that were not explored, while facilitating\nmore accurate self-assessment. This finding might be particularly important in\neducational contexts."}
{"id": "2505.01711", "pdf": "https://arxiv.org/pdf/2505.01711", "abs": "https://arxiv.org/abs/2505.01711", "authors": ["Alexander Davis", "Rafael Souza", "Jia-Hao Lim"], "title": "Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings", "categories": ["cs.CV"], "comment": null, "summary": "Automated interpretation of chest X-rays (CXR) is a critical task with the\npotential to significantly improve clinical workflow and patient care. While\nrecent advances in multimodal foundation models have shown promise, effectively\nleveraging the full power of large language models (LLMs) for this visual task\nremains an underexplored area. This paper introduces CXR-TextInter, a novel\nframework that repurposes powerful text-centric LLMs for CXR interpretation by\noperating solely on a rich, structured textual representation of the image\ncontent, generated by an upstream image analysis pipeline. We augment this\nLLM-centric approach with an integrated medical knowledge module to enhance\nclinical reasoning. To facilitate training and evaluation, we developed the\nMediInstruct-CXR dataset, containing structured image representations paired\nwith diverse, clinically relevant instruction-response examples, and the\nCXR-ClinEval benchmark for comprehensive assessment across various\ninterpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that\nCXR-TextInter achieves state-of-the-art quantitative performance across\npathology detection, report generation, and visual question answering,\nsurpassing existing multimodal foundation models. Ablation studies confirm the\ncritical contribution of the knowledge integration module. Furthermore, blinded\nhuman evaluation by board-certified radiologists shows a significant preference\nfor the clinical quality of outputs generated by CXR-TextInter. Our work\nvalidates an alternative paradigm for medical image AI, showcasing the\npotential of harnessing advanced LLM capabilities when visual information is\neffectively structured and domain knowledge is integrated."}
{"id": "2505.01883", "pdf": "https://arxiv.org/pdf/2505.01883", "abs": "https://arxiv.org/abs/2505.01883", "authors": ["Yiwen Lu", "Siheng Xiong", "Zhaowei Li"], "title": "Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams", "categories": ["cs.CL"], "comment": null, "summary": "We present a framework for large-scale sentiment and topic analysis of\nTwitter discourse. Our pipeline begins with targeted data collection using\nconflict-specific keywords, followed by automated sentiment labeling via\nmultiple pre-trained models to improve annotation robustness. We examine the\nrelationship between sentiment and contextual features such as timestamp,\ngeolocation, and lexical content. To identify latent themes, we apply Latent\nDirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and\nmetadata attributes. Finally, we develop an interactive visualization interface\nto support exploration of sentiment trends and topic distributions across time\nand regions. This work contributes a scalable methodology for social media\nanalysis in dynamic geopolitical contexts."}
{"id": "2505.01713", "pdf": "https://arxiv.org/pdf/2505.01713", "abs": "https://arxiv.org/abs/2505.01713", "authors": ["Congqi Cao", "Lanshu Hu", "Yating Yu", "Yanning Zhang"], "title": "Vision and Intention Boost Large Language Model in Long-Term Action Anticipation", "categories": ["cs.CV"], "comment": null, "summary": "Long-term action anticipation (LTA) aims to predict future actions over an\nextended period. Previous approaches primarily focus on learning exclusively\nfrom video data but lack prior knowledge. Recent researches leverage large\nlanguage models (LLMs) by utilizing text-based inputs which suffer severe\ninformation loss. To tackle these limitations single-modality methods face, we\npropose a novel Intention-Conditioned Vision-Language (ICVL) model in this\nstudy that fully leverages the rich semantic information of visual data and the\npowerful reasoning capabilities of LLMs. Considering intention as a high-level\nconcept guiding the evolution of actions, we first propose to employ a\nvision-language model (VLM) to infer behavioral intentions as comprehensive\ntextual features directly from video inputs. The inferred intentions are then\nfused with visual features through a multi-modality fusion strategy, resulting\nin intention-enhanced visual representations. These enhanced visual\nrepresentations, along with textual prompts, are fed into LLM for future action\nanticipation. Furthermore, we propose an effective example selection strategy\njointly considers visual and textual similarities, providing more relevant and\ninformative examples for in-context learning. Extensive experiments with\nstate-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+\ndatasets fully demonstrate the effectiveness and superiority of the proposed\nmethod."}
{"id": "2505.01900", "pdf": "https://arxiv.org/pdf/2505.01900", "abs": "https://arxiv.org/abs/2505.01900", "authors": ["Mazal Bethany", "Nishant Vishwamitra", "Cho-Yu Jason Chiang", "Peyman Najafirad"], "title": "CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation", "categories": ["cs.CL"], "comment": null, "summary": "Automated evidence-based misinformation detection systems, which evaluate the\nveracity of short claims against evidence, lack comprehensive analysis of their\nadversarial vulnerabilities. Existing black-box text-based adversarial attacks\nare ill-suited for evidence-based misinformation detection systems, as these\nattacks primarily focus on token-level substitutions involving gradient or\nlogit-based optimization strategies, which are incapable of fooling the\nmulti-component nature of these detection systems. These systems incorporate\nboth retrieval and claim-evidence comparison modules, which requires attacks to\nbreak the retrieval of evidence and/or the comparison module so that it draws\nincorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach\nthat employs a two-agent system, a Prompt Optimization Agent and an Attacker\nAgent, to create adversarial claim rewritings that manipulate evidence\nretrieval and mislead claim-evidence comparison, effectively bypassing the\nsystem without altering the meaning of the claim. The Attacker Agent produces\nsemantically equivalent rewrites that attempt to mislead detectors, while the\nPrompt Optimization Agent analyzes failed attack attempts and refines the\nprompt of the Attacker to guide subsequent rewrites. This enables larger\nstructural and stylistic transformations of the text rather than token-level\nsubstitutions, adapting the magnitude of changes based on previous outcomes.\nUnlike existing approaches, CAMOUFLAGE optimizes its attack solely based on\nbinary model decisions to guide its rewriting process, eliminating the need for\nclassifier logits or extensive querying. We evaluate CAMOUFLAGE on four\nsystems, including two recent academic systems and two real-world APIs, with an\naverage attack success rate of 46.92\\% while preserving textual coherence and\nsemantic equivalence to the original claims."}
{"id": "2505.01726", "pdf": "https://arxiv.org/pdf/2505.01726", "abs": "https://arxiv.org/abs/2505.01726", "authors": ["Jie Liu", "Pan Zhou", "Zehao Xiao", "Jiayi Shen", "Wenzhe Yin", "Jan-Jakob Sonke", "Efstratios Gavves"], "title": "Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes", "categories": ["cs.CV"], "comment": "ICML 2025 Proceedings", "summary": "Interactive 3D segmentation has emerged as a promising solution for\ngenerating accurate object masks in complex 3D scenes by incorporating\nuser-provided clicks. However, two critical challenges remain underexplored:\n(1) effectively generalizing from sparse user clicks to produce accurate\nsegmentation, and (2) quantifying predictive uncertainty to help users identify\nunreliable regions. In this work, we propose NPISeg3D, a novel probabilistic\nframework that builds upon Neural Processes (NPs) to address these challenges.\nSpecifically, NPISeg3D introduces a hierarchical latent variable structure with\nscene-specific and object-specific latent variables to enhance few-shot\ngeneralization by capturing both global context and object-specific\ncharacteristics. Additionally, we design a probabilistic prototype modulator\nthat adaptively modulates click prototypes with object-specific latent\nvariables, improving the model's ability to capture object-aware context and\nquantify predictive uncertainty. Experiments on four 3D point cloud datasets\ndemonstrate that NPISeg3D achieves superior segmentation performance with fewer\nclicks while providing reliable uncertainty estimations."}
{"id": "2505.01967", "pdf": "https://arxiv.org/pdf/2505.01967", "abs": "https://arxiv.org/abs/2505.01967", "authors": ["Jiatao Li", "Yanheng Li", "Xiaojun Wan"], "title": "Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) have become integral to daily life, widely\nadopted in communication, decision-making, and information retrieval, raising\ncritical questions about how these systems implicitly form and express\nsocio-cognitive attitudes or \"worldviews\". While existing research extensively\naddresses demographic and ethical biases, broader dimensions-such as attitudes\ntoward authority, equality, autonomy, and fate-remain under-explored. In this\npaper, we introduce the Social Worldview Taxonomy (SWT), a structured framework\ngrounded in Cultural Theory, operationalizing four canonical worldviews\n(Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable\nsub-dimensions. Using SWT, we empirically identify distinct and interpretable\ncognitive profiles across 28 diverse LLMs. Further, inspired by Social\nReferencing Theory, we experimentally demonstrate that explicit social cues\nsystematically shape these cognitive attitudes, revealing both general response\npatterns and nuanced model-specific variations. Our findings enhance the\ninterpretability of LLMs by revealing implicit socio-cognitive biases and their\nresponsiveness to social feedback, thus guiding the development of more\ntransparent and socially responsible language technologies."}
{"id": "2505.01729", "pdf": "https://arxiv.org/pdf/2505.01729", "abs": "https://arxiv.org/abs/2505.01729", "authors": ["Bu Jin", "Weize Li", "Baihan Yang", "Zhenxin Zhu", "Junpeng Jiang", "Huan-ang Gao", "Haiyang Sun", "Kun Zhan", "Hengtong Hu", "Xueyang Zhang", "Peng Jia", "Hao Zhao"], "title": "PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth", "categories": ["cs.CV"], "comment": "8 pages, 3 figures", "summary": "Recent advancements in autonomous driving (AD) systems have highlighted the\npotential of world models in achieving robust and generalizable performance\nacross both ordinary and challenging driving conditions. However, a key\nchallenge remains: precise and flexible camera pose control, which is crucial\nfor accurate viewpoint transformation and realistic simulation of scene\ndynamics. In this paper, we introduce PosePilot, a lightweight yet powerful\nframework that significantly enhances camera pose controllability in generative\nworld models. Drawing inspiration from self-supervised depth estimation,\nPosePilot leverages structure-from-motion principles to establish a tight\ncoupling between camera pose and video generation. Specifically, we incorporate\nself-supervised depth and pose readouts, allowing the model to infer depth and\nrelative camera motion directly from video sequences. These outputs drive\npose-aware frame warping, guided by a photometric warping loss that enforces\ngeometric consistency across synthesized frames. To further refine camera pose\nestimation, we introduce a reverse warping step and a pose regression loss,\nimproving viewpoint precision and adaptability. Extensive experiments on\nautonomous driving and general-domain video datasets demonstrate that PosePilot\nsignificantly enhances structural understanding and motion reasoning in both\ndiffusion-based and auto-regressive world models. By steering camera pose with\nself-supervised depth, PosePilot sets a new benchmark for pose controllability,\nenabling physically consistent, reliable viewpoint synthesis in generative\nworld models."}
{"id": "2505.01980", "pdf": "https://arxiv.org/pdf/2505.01980", "abs": "https://arxiv.org/abs/2505.01980", "authors": ["Theo Guidroz", "Diego Ardila", "Jimmy Li", "Adam Mansour", "Paul Jhun", "Nina Gonzalez", "Xiang Ji", "Mike Sanchez", "Sujay Kakarmath", "Mathias MJ Bellaiche", "Miguel Ãngel Garrido", "Faruk Ahmed", "Divyansh Choudhary", "Jay Hartford", "Chenwei Xu", "Henry Javier Serrano Echeverria", "Yifan Wang", "Jeff Shaffer", "Eric", "Cao", "Yossi Matias", "Avinatan Hassidim", "Dale R Webster", "Yun Liu", "Sho Fujiwara", "Peggy Bui", "Quang Duong"], "title": "LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load", "categories": ["cs.CL"], "comment": null, "summary": "Information on the web, such as scientific publications and Wikipedia, often\nsurpasses users' reading level. To help address this, we used a self-refinement\napproach to develop a LLM capability for minimally lossy text simplification.\nTo validate our approach, we conducted a randomized study involving 4563\nparticipants and 31 texts spanning 6 broad subject areas: PubMed (biomedical\nscientific articles), biology, law, finance, literature/philosophy, and\naerospace/computer science. Participants were randomized to viewing original or\nsimplified texts in a subject area, and answered multiple-choice questions\n(MCQs) that tested their comprehension of the text. The participants were also\nasked to provide qualitative feedback such as task difficulty. Our results\nindicate that participants who read the simplified text answered more MCQs\ncorrectly than their counterparts who read the original text (3.9% absolute\nincrease, p<0.05). This gain was most striking with PubMed (14.6%), while more\nmoderate gains were observed for finance (5.5%), aerospace/computer science\n(3.8%) domains, and legal (3.5%). Notably, the results were robust to whether\nparticipants could refer back to the text while answering MCQs. The absolute\naccuracy decreased by up to ~9% for both original and simplified setups where\nparticipants could not refer back to the text, but the ~4% overall improvement\npersisted. Finally, participants' self-reported perceived ease based on a\nsimplified NASA Task Load Index was greater for those who read the simplified\ntext (absolute change on a 5-point scale 0.33, p<0.05). This randomized study,\ninvolving an order of magnitude more participants than prior works,\ndemonstrates the potential of LLMs to make complex information easier to\nunderstand. Our work aims to enable a broader audience to better learn and make\nuse of expert knowledge available on the web, improving information\naccessibility."}
{"id": "2505.01737", "pdf": "https://arxiv.org/pdf/2505.01737", "abs": "https://arxiv.org/abs/2505.01737", "authors": ["Seong Hyeon Park", "Jinwoo Shin"], "title": "Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "In monocular videos that capture dynamic scenes, estimating the 3D geometry\nof video contents has been a fundamental challenge in computer vision.\nSpecifically, the task is significantly challenged by the object motion, where\nexisting models are limited to predict only partial attributes of the dynamic\nscenes, such as depth or pointmaps spanning only over a pair of frames. Since\nthese attributes are inherently noisy under multiple frames, test-time global\noptimizations are often employed to fully recover the geometry, which is liable\nto failure and incurs heavy inference costs. To address the challenge, we\npresent a new model, coined MMP, to estimate the geometry in a feed-forward\nmanner, which produces a dynamic pointmap representation that evolves over\nmultiple frames. Specifically, based on the recent Siamese architecture, we\nintroduce a new trajectory encoding module to project point-wise dynamics on\nthe representation for each frame, which can provide significantly improved\nexpressiveness for dynamic scenes. In our experiments, we find MMP can achieve\nstate-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%\nenhancement in the regression error."}
{"id": "2505.02009", "pdf": "https://arxiv.org/pdf/2505.02009", "abs": "https://arxiv.org/abs/2505.02009", "authors": ["Sai Krishna Mendu", "Harish Yenala", "Aditi Gulati", "Shanu Kumar", "Parag Agrawal"], "title": "Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to various real-world\napplications, leveraging massive, web-sourced datasets like Common Crawl, C4,\nand FineWeb for pretraining. While these datasets provide linguistic data\nessential for high-quality natural language generation, they often contain\nharmful content, such as hate speech, misinformation, and biased narratives.\nTraining LLMs on such unfiltered data risks perpetuating toxic behaviors,\nspreading misinformation, and amplifying societal biases which can undermine\ntrust in LLM-driven applications and raise ethical concerns about their use.\nThis paper presents a large-scale analysis of inappropriate content across\nthese datasets, offering a comprehensive taxonomy that categorizes harmful\nwebpages into Topical and Toxic based on their intent. We also introduce a\nprompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and\na transformer-based model (HarmFormer) for content filtering. Additionally, we\ncreate a new multi-harm open-ended toxicity benchmark (HAVOC) and provide\ncrucial insights into how models respond to adversarial toxic inputs. Upon\npublishing, we will also opensource our model signal on the entire C4 dataset.\nOur work offers insights into ensuring safer LLM pretraining and serves as a\nresource for Responsible AI (RAI) compliance."}
{"id": "2505.01743", "pdf": "https://arxiv.org/pdf/2505.01743", "abs": "https://arxiv.org/abs/2505.01743", "authors": ["Siyang Jiang", "Bufang Yang", "Lilin Xu", "Mu Yuan", "Yeerzhati Abudunuer", "Kaiwei Liu", "Liekang Zeng", "Hongkai Chen", "Zhenyu Yan", "Xiaofan Jiang", "Guoliang Xing"], "title": "An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancements in Large Vision Language Models (LVLMs) offer the\npotential to surpass conventional labeling by generating richer, more detailed\ndescriptions of on-device human behavior understanding (HBU) in low-resolution\nvision systems, such as depth, thermal, and infrared. However, existing large\nvision language model (LVLM) approaches are unable to understand low-resolution\ndata well as they are primarily designed for high-resolution data, such as RGB\nimages. A quick fixing approach is to caption a large amount of low-resolution\ndata, but it requires a significant amount of labor-intensive annotation\nefforts. In this paper, we propose a novel, labor-saving system, Llambda,\ndesigned to support low-resolution HBU. The core idea is to leverage limited\nlabeled data and a large amount of unlabeled data to guide LLMs in generating\ninformative captions, which can be combined with raw data to effectively\nfine-tune LVLM models for understanding low-resolution videos in HBU. First, we\npropose a Contrastive-Oriented Data Labeler, which can capture\nbehavior-relevant information from long, low-resolution videos and generate\nhigh-quality pseudo labels for unlabeled data via contrastive learning. Second,\nwe propose a Physical-Knowledge Guided Captioner, which utilizes spatial and\ntemporal consistency checks to mitigate errors in pseudo labels. Therefore, it\ncan improve LLMs' understanding of sequential data and then generate\nhigh-quality video captions. Finally, to ensure on-device deployability, we\nemploy LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data.\nWe evaluate Llambda using a region-scale real-world testbed and three distinct\nlow-resolution datasets, and the experiments show that Llambda outperforms\nseveral state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score."}
{"id": "2505.02032", "pdf": "https://arxiv.org/pdf/2505.02032", "abs": "https://arxiv.org/abs/2505.02032", "authors": ["Anisia Katinskaia"], "title": "An overview of artificial intelligence in computer-assisted language learning", "categories": ["cs.CL"], "comment": null, "summary": "Computer-assisted language learning -- CALL -- is an established research\nfield. We review how artificial intelligence can be applied to support language\nlearning and teaching. The need for intelligent agents that assist language\nlearners and teachers is increasing: the human teacher's time is a scarce and\ncostly resource, which does not scale with growing demand. Further factors\ncontribute to the need for CALL: pandemics and increasing demand for distance\nlearning, migration of large populations, the need for sustainable and\naffordable support for learning, etc. CALL systems are made up of many\ncomponents that perform various functions, and AI is applied to many different\naspects in CALL, corresponding to their own expansive research areas. Most of\nwhat we find in the research literature and in practical use are prototypes or\npartial implementations -- systems that perform some aspects of the overall\ndesired functionality. Complete solutions -- most of them commercial -- are\nfew, because they require massive resources. Recent advances in AI should\nresult in improvements in CALL, yet there is a lack of surveys that focus on AI\nin the context of this research field. This paper aims to present a perspective\non the AI methods that can be employed for language learning from a position of\na developer of a CALL system. We also aim to connect work from different\ndisciplines, to build bridges for interdisciplinary work."}
{"id": "2505.01746", "pdf": "https://arxiv.org/pdf/2505.01746", "abs": "https://arxiv.org/abs/2505.01746", "authors": ["Xingqun Qi", "Yatian Wang", "Hengyuan Zhang", "Jiahao Pan", "Wei Xue", "Shanghang Zhang", "Wenhan Luo", "Qifeng Liu", "Yike Guo"], "title": "Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion", "categories": ["cs.CV"], "comment": "Accepted as ICLR 2025 (Spotlight)", "summary": "Generating gestures from human speech has gained tremendous progress in\nanimating virtual avatars. While the existing methods enable synthesizing\ngestures cooperated by individual self-talking, they overlook the practicality\nof concurrent gesture modeling with two-person interactive conversations.\nMoreover, the lack of high-quality datasets with concurrent co-speech gestures\nalso limits handling this issue. To fulfill this goal, we first construct a\nlarge-scale concurrent co-speech gesture dataset that contains more than 7M\nframes for diverse two-person interactive posture sequences, dubbed GES-Inter.\nAdditionally, we propose Co$^3$Gesture, a novel framework that enables coherent\nconcurrent co-speech gesture synthesis including two-person interactive\nmovements. Considering the asymmetric body dynamics of two speakers, our\nframework is built upon two cooperative generation branches conditioned on\nseparated speaker audio. Specifically, to enhance the coordination of human\npostures with respect to corresponding speaker audios while interacting with\nthe conversational partner, we present a Temporal Interaction Module (TIM). TIM\ncan effectively model the temporal association representation between two\nspeakers' gesture sequences as interaction guidance and fuse it into the\nconcurrent gesture generation. Then, we devise a mutual attention mechanism to\nfurther holistically boost learning dependencies of interacted concurrent\nmotions, thereby enabling us to generate vivid and coherent gestures. Extensive\nexperiments demonstrate that our method outperforms the state-of-the-art models\non our newly collected GES-Inter dataset. The dataset and source code are\npublicly available at\n\\href{https://mattie-e.github.io/Co3/}{\\textit{https://mattie-e.github.io/Co3/}}."}
{"id": "2505.02072", "pdf": "https://arxiv.org/pdf/2505.02072", "abs": "https://arxiv.org/abs/2505.02072", "authors": ["Eitan Wagner", "Omri Abend"], "title": "What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The notion of language modeling has gradually shifted in recent years from a\ndistribution over finite-length strings to general-purpose prediction models\nfor textual inputs and outputs, following appropriate alignment phases. This\npaper analyzes the distinction between distribution estimation and response\nprediction in the context of LLMs, and their often conflicting goals. We\nexamine the training phases of LLMs, which include pretraining, in-context\nlearning, and preference tuning, and also the common use cases for their output\nprobabilities, which include completion probabilities and explicit\nprobabilities as output. We argue that the different settings lead to three\ndistinct intended output distributions. We demonstrate that NLP works often\nassume that these distributions should be similar, which leads to\nmisinterpretations of their experimental findings. Our work sets firmer formal\nfoundations for the interpretation of LLMs, which will inform ongoing work on\nthe interpretation and use of LLMs' induced distributions."}
{"id": "2505.01766", "pdf": "https://arxiv.org/pdf/2505.01766", "abs": "https://arxiv.org/abs/2505.01766", "authors": ["Long Bai", "Boyi Ma", "Ruohan Wang", "Guankun Wang", "Beilei Cui", "Zhongliang Jiang", "Mobarakol Islam", "Zhe Min", "Jiewen Lai", "Nassir Navab", "Hongliang Ren"], "title": "Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by Information Fusion", "summary": "Surgical workflow recognition is vital for automating tasks, supporting\ndecision-making, and training novice surgeons, ultimately improving patient\nsafety and standardizing procedures. However, data corruption can lead to\nperformance degradation due to issues like occlusion from bleeding or smoke in\nsurgical scenes and problems with data storage and transmission. In this case,\nwe explore a robust graph-based multimodal approach to integrating vision and\nkinematic data to enhance accuracy and reliability. Vision data captures\ndynamic surgical scenes, while kinematic data provides precise movement\ninformation, overcoming limitations of visual recognition under adverse\nconditions. We propose a multimodal Graph Representation network with\nAdversarial feature Disentanglement (GRAD) for robust surgical workflow\nrecognition in challenging scenarios with domain shifts or corrupted data.\nSpecifically, we introduce a Multimodal Disentanglement Graph Network that\ncaptures fine-grained visual information while explicitly modeling the complex\nrelationships between vision and kinematic embeddings through graph-based\nmessage modeling. To align feature spaces across modalities, we propose a\nVision-Kinematic Adversarial framework that leverages adversarial training to\nreduce modality gaps and improve feature consistency. Furthermore, we design a\nContextual Calibrated Decoder, incorporating temporal and contextual priors to\nenhance robustness against domain shifts and corrupted data. Extensive\ncomparative and ablation experiments demonstrate the effectiveness of our model\nand proposed modules. Moreover, our robustness experiments show that our method\neffectively handles data corruption during storage and transmission, exhibiting\nexcellent stability and robustness. Our approach aims to advance automated\nsurgical workflow recognition, addressing the complexities and dynamism\ninherent in surgical procedures."}
{"id": "2505.02078", "pdf": "https://arxiv.org/pdf/2505.02078", "abs": "https://arxiv.org/abs/2505.02078", "authors": ["Joy Lim Jia Yin", "Daniel Zhang-Li", "Jifan Yu", "Haoxuan Li", "Shangqing Tu", "Yuanchun Wang", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Evaluating the quality of slide-based multimedia instruction is challenging.\nExisting methods like manual assessment, reference-based metrics, and large\nlanguage model evaluators face limitations in scalability, context capture, or\nbias. In this paper, we introduce LecEval, an automated metric grounded in\nMayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal\nknowledge acquisition in slide-based learning. LecEval assesses effectiveness\nusing four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical\nStructure (LS), and Audience Engagement (AE). We curate a large-scale dataset\nof over 2,000 slides from more than 50 online course videos, annotated with\nfine-grained human ratings across these rubrics. A model trained on this\ndataset demonstrates superior accuracy and adaptability compared to existing\nmetrics, bridging the gap between automated and human assessments. We release\nour dataset and toolkits at https://github.com/JoylimJY/LecEval."}
{"id": "2505.01790", "pdf": "https://arxiv.org/pdf/2505.01790", "abs": "https://arxiv.org/abs/2505.01790", "authors": ["Markos Stamatakis", "Joshua Berger", "Christian Wartena", "Ralph Ewerth", "Anett Hoppe"], "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "12 pages (excluding references), 8 tables, 1 equation", "summary": "Web-based educational videos offer flexible learning opportunities and are\nbecoming increasingly popular. However, improving user engagement and knowledge\nretention remains a challenge. Automatically generated questions can activate\nlearners and support their knowledge acquisition. Further, they can help\nteachers and learners assess their understanding. While large language and\nvision-language models have been employed in various tasks, their application\nto question generation for educational videos remains underexplored. In this\npaper, we investigate the capabilities of current vision-language models for\ngenerating learning-oriented questions for educational video content. We assess\n(1) out-of-the-box models' performance; (2) fine-tuning effects on\ncontent-specific question generation; (3) the impact of different video\nmodalities on question quality; and (4) in a qualitative study, question\nrelevance, answerability, and difficulty levels of generated questions. Our\nfindings delineate the capabilities of current vision-language models,\nhighlighting the need for fine-tuning and addressing challenges in question\ndiversity and relevance. We identify requirements for future multimodal\ndatasets and outline promising research directions."}
{"id": "2505.02091", "pdf": "https://arxiv.org/pdf/2505.02091", "abs": "https://arxiv.org/abs/2505.02091", "authors": ["Xinyue Peng", "Yanming Liu", "Yihan Cang", "Chaoqun Cao", "Ming Chen"], "title": "LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications", "categories": ["cs.CL", "cs.LG"], "comment": "6 pages,4 figures", "summary": "Solving non-convex resource allocation problems poses significant challenges\nin wireless communication systems, often beyond the capability of traditional\noptimization techniques. To address this issue, we propose LLM-OptiRA, the\nfirst framework that leverages large language models (LLMs) to automatically\ndetect and transform non-convex components into solvable forms, enabling fully\nautomated resolution of non-convex resource allocation problems in wireless\ncommunication systems. LLM-OptiRA not only simplifies problem-solving by\nreducing reliance on expert knowledge, but also integrates error correction and\nfeasibility validation mechanisms to ensure robustness. Experimental results\nshow that LLM-OptiRA achieves an execution rate of 96% and a success rate of\n80% on GPT-4, significantly outperforming baseline approaches in complex\noptimization tasks across diverse scenarios."}
{"id": "2505.01799", "pdf": "https://arxiv.org/pdf/2505.01799", "abs": "https://arxiv.org/abs/2505.01799", "authors": ["Junhao Shi", "Jisheng Xu", "Jianping He", "Zhiliang Lin"], "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Underwater scene reconstruction is a critical tech-nology for underwater\noperations, enabling the generation of 3D models from images captured by\nunderwater platforms. However, the quality of underwater images is often\ndegraded due to medium interference, which limits the effectiveness of\nStructure-from-Motion (SfM) pose estimation, leading to subsequent\nreconstruction failures. Additionally, SfM methods typically operate at slower\nspeeds, further hindering their applicability in real-time scenarios. In this\npaper, we introduce AquaGS, an SfM-free underwater scene reconstruction model\nbased on the SeaThru algorithm, which facilitates rapid and accurate separation\nof scene details and medium features. Our approach initializes Gaussians by\nintegrating state-of-the-art multi-view stereo (MVS) technology, employs\nimplicit Neural Radiance Fields (NeRF) for rendering translucent media and\nutilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render\nobject surfaces, which effectively addresses the limitations of traditional\nmethods and accurately simulates underwater optical phenomena. Experimental\nresults on the data set and the robot platform show that our model can complete\nhigh-precision reconstruction in 30 seconds with only 3 image inputs,\nsignificantly enhancing the practical application of the algorithm in robotic\nplatforms."}
{"id": "2505.02142", "pdf": "https://arxiv.org/pdf/2505.02142", "abs": "https://arxiv.org/abs/2505.02142", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advances in long-context reasoning by large language\nmodels (LLMs), primarily through Online Reinforcement Learning (RL) methods,\nthese approaches incur substantial computational costs and complexity. In\ncontrast, simpler and more economical Offline RL methods remain underexplored.\nTo address this gap, we investigate the effectiveness of Offline RL methods,\nspecifically Direct Preference Optimization (DPO) and its length-desensitized\nvariant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive\nexperiments across multiple reasoning benchmarks demonstrate that these simpler\nOffline RL methods substantially improve model performance, achieving an\naverage enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on\nthe challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity\nto output length, emphasizing that increasing reasoning length should align\nwith semantic richness, as indiscriminate lengthening may adversely affect\nmodel performance. We provide comprehensive descriptions of our data processing\nand training methodologies, offering empirical evidence and practical insights\nfor developing more cost-effective Offline RL approaches."}
{"id": "2505.01802", "pdf": "https://arxiv.org/pdf/2505.01802", "abs": "https://arxiv.org/abs/2505.01802", "authors": ["Georgios Fotios Angelis", "Savas Ozkan", "Sinan Mutlu", "Paul Wisbey", "Anastasios Drosou", "Mete Ozay"], "title": "Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows", "categories": ["cs.CV"], "comment": "Accepted to CVPRW2025 - 4D Vision Workshop", "summary": "To have a seamless user experience on immersive AR/VR applications, the\nimportance of efficient and effective Neural Network (NN) models is undeniable,\nsince missing body parts that cannot be captured by limited sensors should be\ngenerated using these models for a complete 3D full-body reconstruction in\nvirtual environment. However, the state-of-the-art NN-models are typically\ncomputational expensive and they leverage longer sequences of sparse tracking\ninputs to generate full-body movements by capturing temporal context.\nInevitably, longer sequences increase the computation overhead and introduce\nnoise in longer temporal dependencies that adversely affect the generation\nperformance. In this paper, we propose a novel Multi-Layer Perceptron\n(MLP)-based method that enhances the overall performance while balancing the\ncomputational cost and memory overhead for efficient 3D full-body generation.\nPrecisely, we introduce a NN-mechanism that divides the longer sequence of\ninputs into smaller temporal windows. Later, the current motion is merged with\nthe information from these windows through latent representations to utilize\nthe past context for the generation. Our experiments demonstrate that\ngeneration accuracy of our method with this NN-mechanism is significantly\nimproved compared to the state-of-the-art methods while greatly reducing\ncomputational costs and memory overhead, making our method suitable for\nresource-constrained devices."}
{"id": "2505.02146", "pdf": "https://arxiv.org/pdf/2505.02146", "abs": "https://arxiv.org/abs/2505.02146", "authors": ["Shouyang Dong", "Yuanbo Wen", "Jun Bi", "Di Huang", "Jiaming Guo", "Jianxing Xu", "Ruibai Xu", "Xinkai Song", "Yifan Hao", "Xuehai Zhou", "Tianshi Chen", "Qi Guo", "Yunji Chen"], "title": "QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach", "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": "Accepted to OSDI 2025", "summary": "Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been\nwidely deployed in industrial data centers, which requires to develop multiple\nlow-level tensor programs for different platforms. An attractive solution to\nrelieve the programming burden is to transcompile the legacy code of one\nplatform to others. However, current transcompilation techniques struggle with\neither tremendous manual efforts or functional incorrectness, rendering \"Write\nOnce, Run Anywhere\" of tensor programs an open question.\n  We propose a novel transcompiler, i.e., QiMeng-Xpiler, for automatically\ntranslating tensor programs across DLS via both large language models (LLMs)\nand symbolic program synthesis, i.e., neural-symbolic synthesis. The key\ninsight is leveraging the powerful code generation ability of LLM to make\ncostly search-based symbolic synthesis computationally tractable. Concretely,\nwe propose multiple LLM-assisted compilation passes via pre-defined\nmeta-prompts for program transformation. During each program transformation,\nefficient symbolic program synthesis is employed to repair incorrect code\nsnippets with a limited scale. To attain high performance, we propose a\nhierarchical auto-tuning approach to systematically explore both the parameters\nand sequences of transformation passes. Experiments on 4 DLS with distinct\nprogramming interfaces, i.e., Intel DL Boost with VNNI, NVIDIA GPU with CUDA,\nAMD MI with HIP, and Cambricon MLU with BANG, demonstrate that QiMeng-Xpiler\ncorrectly translates different tensor programs at the accuracy of 95% on\naverage, and the performance of translated programs achieves up to 2.0x over\nvendor-provided manually-optimized libraries. As a result, the programming\nproductivity of DLS is improved by up to 96.0x via transcompiling legacy tensor\nprograms."}
{"id": "2505.01805", "pdf": "https://arxiv.org/pdf/2505.01805", "abs": "https://arxiv.org/abs/2505.01805", "authors": ["Yuchang Jiang", "Maxim Neumann"], "title": "Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Developing accurate and reliable models for forest types mapping is critical\nto support efforts for halting deforestation and for biodiversity conservation\n(such as European Union Deforestation Regulation (EUDR)). This work introduces\nForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal\nsatellite data1. The benchmark comprises 200,000 time series of image patches,\neach consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each\ntime series captures variations at monthly or seasonal cadence. Per-pixel\nannotations, including forest types and other land use classes, support image\nsegmentation tasks. Unlike most existing land use products that often\ncategorize all forest areas into a single class, our benchmark differentiates\nbetween three forest types classes: natural forest, planted forest, and tree\ncrops. By leveraging multiple public data sources, we achieve global coverage\nwith this benchmark. We evaluate the forest types dataset using several\nbaseline models, including convolution neural networks and transformer-based\nmodels. Additionally, we propose a novel transformer-based model specifically\ndesigned to handle multi-modal, multi-temporal satellite data for forest types\nmapping. Our experimental results demonstrate that the proposed model surpasses\nthe baseline models in performance."}
{"id": "2505.02156", "pdf": "https://arxiv.org/pdf/2505.02156", "abs": "https://arxiv.org/abs/2505.02156", "authors": ["Minzheng Wang", "Yongbin Li", "Haobo Wang", "Xinghua Zhang", "Nan Xu", "Bingli Wu", "Fei Huang", "Haiyang Yu", "Wenji Mao"], "title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "The code and data are available, see\n  https://github.com/MozerWang/AMPO. arXiv admin note: text overlap with\n  arXiv:2502.15538 by other authors", "summary": "Effective social intelligence simulation requires language agents to\ndynamically adjust reasoning depth, a capability notably absent in current\napproaches. While existing methods either lack this kind of reasoning\ncapability or enforce uniform long chain-of-thought reasoning across all\nscenarios, resulting in excessive token usage and inappropriate social\nsimulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode\n$\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four\nthinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on\nreal-time context. Our framework's core innovation, the $\\textbf{A}$daptive\n$\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$)\nalgorithm, introduces three key advancements over existing methods: (1)\nMulti-granular thinking mode design, (2) Context-aware mode switching across\nsocial interaction, and (3) Token-efficient reasoning via depth-adaptive\nprocessing. Extensive experiments on social intelligence tasks confirm that AML\nachieves 15.6% higher task performance than state-of-the-art methods. Notably,\nour method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These\nresults demonstrate that context-sensitive thinking mode selection, as\nimplemented in AMPO, enables more human-like adaptive reasoning than GRPO's\nfixed-depth approach"}
{"id": "2505.01809", "pdf": "https://arxiv.org/pdf/2505.01809", "abs": "https://arxiv.org/abs/2505.01809", "authors": ["Xiaoqi Li", "Jiaming Liu", "Nuowei Han", "Liang Heng", "Yandong Guo", "Hao Dong", "Yang Liu"], "title": "3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment", "categories": ["cs.CV"], "comment": "ICRA 2025", "summary": "The 3D weakly-supervised visual grounding task aims to localize oriented 3D\nboxes in point clouds based on natural language descriptions without requiring\nannotations to guide model learning. This setting presents two primary\nchallenges: category-level ambiguity and instance-level complexity.\nCategory-level ambiguity arises from representing objects of fine-grained\ncategories in a highly sparse point cloud format, making category distinction\nchallenging. Instance-level complexity stems from multiple instances of the\nsame category coexisting in a scene, leading to distractions during grounding.\nTo address these challenges, we propose a novel weakly-supervised grounding\napproach that explicitly differentiates between categories and instances. In\nthe category-level branch, we utilize extensive category knowledge from a\npre-trained external detector to align object proposal features with\nsentence-level category features, thereby enhancing category awareness. In the\ninstance-level branch, we utilize spatial relationship descriptions from\nlanguage queries to refine object proposal features, ensuring clear\ndifferentiation among objects. These designs enable our model to accurately\nidentify target-category objects while distinguishing instances within the same\ncategory. Compared to previous methods, our approach achieves state-of-the-art\nperformance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef."}
{"id": "2505.02164", "pdf": "https://arxiv.org/pdf/2505.02164", "abs": "https://arxiv.org/abs/2505.02164", "authors": ["Justin Ho", "Alexandra Colby", "William Fisher"], "title": "Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use", "categories": ["cs.CL", "I.2.7; K.5; H.3.3"], "comment": "Submitted to the 7th Workshop on Automated Semantic Analysis of\n  Information in Legal Text. 8 pages, 5 Figures", "summary": "This paper presents a domain-specific implementation of Retrieval-Augmented\nGeneration (RAG) tailored to the Fair Use Doctrine in U.S. copyright law.\nMotivated by the increasing prevalence of DMCA takedowns and the lack of\naccessible legal support for content creators, we propose a structured approach\nthat combines semantic search with legal knowledge graphs and court citation\nnetworks to improve retrieval quality and reasoning reliability. Our prototype\nmodels legal precedents at the statutory factor level (e.g., purpose, nature,\namount, market effect) and incorporates citation-weighted graph representations\nto prioritize doctrinally authoritative sources. We use Chain-of-Thought\nreasoning and interleaved retrieval steps to better emulate legal reasoning.\nPreliminary testing suggests this method improves doctrinal relevance in the\nretrieval process, laying groundwork for future evaluation and deployment of\nLLM-based legal assistance tools."}
{"id": "2505.01823", "pdf": "https://arxiv.org/pdf/2505.01823", "abs": "https://arxiv.org/abs/2505.01823", "authors": ["Nitin Rai", "Arnold W. Schumann", "Nathan Boyd"], "title": "PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Collecting large-scale crop disease images in the field is labor-intensive\nand time-consuming. Generative models (GMs) offer an alternative by creating\nsynthetic samples that resemble real-world images. However, existing research\nprimarily relies on Generative Adversarial Networks (GANs)-based image-to-image\ntranslation and lack a comprehensive analysis of computational requirements in\nagriculture. Therefore, this research explores a multi-modal text-to-image\napproach for generating synthetic crop disease images and is the first to\nprovide computational benchmarking in this context. We trained three Stable\nDiffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and\nfine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning\ntechniques to enhance generalization. SD3.5M outperformed the others, with an\naverage memory usage of 18 GB, power consumption of 180 W, and total energy use\nof 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results\ndemonstrate SD3.5M's ability to generate 500 synthetic images from just 36\nin-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease\ndata generation."}
{"id": "2505.02171", "pdf": "https://arxiv.org/pdf/2505.02171", "abs": "https://arxiv.org/abs/2505.02171", "authors": ["Henrik BrÃ¥dland", "Morten Goodwin", "Per-Arne Andersen", "Alexander S. Nossum", "Aditya Gupta"], "title": "A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, To be published in SIGIR25", "summary": "Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG)\nby determining how source materials are segmented before indexing. Despite\nevidence that Large Language Models (LLMs) are sensitive to the layout and\nstructure of retrieved data, there is currently no framework to analyze the\nimpact of different chunking methods. In this paper, we introduce a novel\nmethodology that defines essential characteristics of the chunking process at\nthree levels: intrinsic passage properties, extrinsic passage properties, and\npassages-document coherence. We propose HOPE (Holistic Passage Evaluation), a\ndomain-agnostic, automatic evaluation metric that quantifies and aggregates\nthese characteristics. Our empirical evaluations across seven domains\ndemonstrate that the HOPE metric correlates significantly (p > 0.13) with\nvarious RAG performance indicators, revealing contrasts between the importance\nof extrinsic and intrinsic properties of passages. Semantic independence\nbetween passages proves essential for system performance with a performance\ngain of up to 56.2% in factual correctness and 21.1% in answer correctness. On\nthe contrary, traditional assumptions about maintaining concept unity within\npassages show minimal impact. These findings provide actionable insights for\noptimizing chunking strategies, thus improving RAG system design to produce\nmore factually correct responses."}
{"id": "2505.01837", "pdf": "https://arxiv.org/pdf/2505.01837", "abs": "https://arxiv.org/abs/2505.01837", "authors": ["Xiangru Li", "Wei Song", "Yingda Huang", "Wei Meng", "Le Chang"], "title": "CVVNet: A Cross-Vertical-View Network for Gait Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Gait recognition enables contact-free, long-range person identification that\nis robust to clothing variations and non-cooperative scenarios. While existing\nmethods perform well in controlled indoor environments, they struggle with\ncross-vertical view scenarios, where surveillance angles vary significantly in\nelevation. Our experiments show up to 60\\% accuracy degradation in low-to-high\nvertical view settings due to severe deformations and self-occlusions of key\nanatomical features. Current CNN and self-attention-based methods fail to\neffectively handle these challenges, due to their reliance on single-scale\nconvolutions or simplistic attention mechanisms that lack effective\nmulti-frequency feature integration. To tackle this challenge, we propose\nCVVNet (Cross-Vertical-View Network), a frequency aggregation architecture\nspecifically designed for robust cross-vertical-view gait recognition. CVVNet\nemploys a High-Low Frequency Extraction module (HLFE) that adopts parallel\nmulti-scale convolution/max-pooling path and self-attention path as high- and\nlow-frequency mixers for effective multi-frequency feature extraction from\ninput silhouettes. We also introduce the Dynamic Gated Aggregation (DGA)\nmechanism to adaptively adjust the fusion ratio of high- and low-frequency\nfeatures. The integration of our core Multi-Scale Attention Gated Aggregation\n(MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions\nfrom view changes, significantly improving the recognition robustness across\ndifferent vertical views. Experimental results show that our CVVNet achieves\nstate-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$\non Gait3D compared with the best existing methods."}
{"id": "2505.02172", "pdf": "https://arxiv.org/pdf/2505.02172", "abs": "https://arxiv.org/abs/2505.02172", "authors": ["Chuck Arvin"], "title": "Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization", "categories": ["cs.CL"], "comment": "Presented as a short paper at International Conference on Artificial\n  Intelligence and Law 2025 (Chicago, IL)", "summary": "As large language models (LLMs) continue to advance in capabilities, it is\nessential to assess how they perform on established benchmarks. In this study,\nwe present a suite of experiments to assess the performance of modern LLMs\n(ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for\nidentifying case holdings. Our experiments demonstrate ``scaling effects'' -\nperformance on this task improves with model size, with more capable models\nlike GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720\nrespectively. These scores are competitive with the best published results on\nthis dataset, and do not require any technically sophisticated model training,\nfine-tuning or few-shot prompting. To ensure that these strong results are not\ndue to memorization of judicial opinions contained in the training data, we\ndevelop and utilize a novel citation anonymization test that preserves semantic\nmeaning while ensuring case names and citations are fictitious. Models maintain\nstrong performance under these conditions (macro F1 of 0.728), suggesting the\nperformance is not due to rote memorization. These findings demonstrate both\nthe promise and current limitations of LLMs for legal tasks with important\nimplications for the development and measurement of automated legal analytics\nand legal benchmarks."}
{"id": "2505.01838", "pdf": "https://arxiv.org/pdf/2505.01838", "abs": "https://arxiv.org/abs/2505.01838", "authors": ["Chenghong Li", "Hongjie Liao", "Yihao Zhi", "Xihe Yang", "Zhengwentai Sun", "Jiahao Chang", "Shuguang Cui", "Xiaoguang Han"], "title": "MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization", "categories": ["cs.CV"], "comment": "project page: https://kevinlee09.github.io/research/MVHumanNet++/.\n  arXiv admin note: substantial text overlap with arXiv:2312.02963", "summary": "In this era, the success of large language models and text-to-image models\ncan be attributed to the driving force of large-scale datasets. However, in the\nrealm of 3D vision, while significant progress has been achieved in\nobject-centric tasks through large-scale datasets like Objaverse and MVImgNet,\nhuman-centric tasks have seen limited advancement, largely due to the absence\nof a comparable large-scale human dataset. To bridge this gap, we present\nMVHumanNet++, a dataset that comprises multi-view human action sequences of\n4,500 human identities. The primary focus of our work is on collecting human\ndata that features a large number of diverse identities and everyday clothing\nusing multi-view human capture systems, which facilitates easily scalable data\ncollection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences\nand 645 million frames with extensive annotations, including human masks,\ncamera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and\ncorresponding textual descriptions. Additionally, the proposed MVHumanNet++\ndataset is enhanced with newly processed normal maps and depth maps,\nsignificantly expanding its applicability and utility for advanced\nhuman-centric research. To explore the potential of our proposed MVHumanNet++\ndataset in various 2D and 3D visual tasks, we conducted several pilot studies\nto demonstrate the performance improvements and effective applications enabled\nby the scale provided by MVHumanNet++. As the current largest-scale 3D human\ndataset, we hope that the release of MVHumanNet++ dataset with annotations will\nfoster further innovations in the domain of 3D human-centric tasks at scale.\nMVHumanNet++ is publicly available at\nhttps://kevinlee09.github.io/research/MVHumanNet++/."}
{"id": "2505.02177", "pdf": "https://arxiv.org/pdf/2505.02177", "abs": "https://arxiv.org/abs/2505.02177", "authors": ["Chuxue Cao", "Zhenghao Zhu", "Junqi Zhu", "Guoying Lu", "Siyu Peng", "Juntao Dai", "Weijie Shi", "Sirui Han", "Yike Guo"], "title": "Measuring Hong Kong Massive Multi-Task Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual understanding is crucial for the cross-cultural applicability of\nLarge Language Models (LLMs). However, evaluation benchmarks designed for Hong\nKong's unique linguistic landscape, which combines Traditional Chinese script\nwith Cantonese as the spoken form and its cultural context, remain\nunderdeveloped. To address this gap, we introduce HKMMLU, a multi-task language\nunderstanding benchmark that evaluates Hong Kong's linguistic competence and\nsocio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions\nacross 66 subjects, organized into four categories: Science, Technology,\nEngineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To\nevaluate the multilingual understanding ability of LLMs, 90,550\nMandarin-Cantonese translation tasks were additionally included. We conduct\ncomprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs\nof varying sizes on HKMMLU. The results show that the best-performing model,\nDeepSeek-V3, struggles to achieve an accuracy of 75\\%, significantly lower than\nthat of MMLU and CMMLU. This performance gap highlights the need to improve\nLLMs' capabilities in Hong Kong-specific language and knowledge domains.\nFurthermore, we investigate how question language, model size, prompting\nstrategies, and question and reasoning token lengths affect model performance.\nWe anticipate that HKMMLU will significantly advance the development of LLMs in\nmultilingual and cross-cultural contexts, thereby enabling broader and more\nimpactful applications."}
{"id": "2505.01851", "pdf": "https://arxiv.org/pdf/2505.01851", "abs": "https://arxiv.org/abs/2505.01851", "authors": ["Chaomeng Chen", "Zitong Yu", "Junhao Dong", "Sen Su", "Linlin Shen", "Shutao Xia", "Xiaochun Cao"], "title": "Mitigating Group-Level Fairness Disparities in Federated Visual Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Visual language models (VLMs) have shown remarkable capabilities in\nmultimodal tasks but face challenges in maintaining fairness across demographic\ngroups, particularly when deployed in federated learning (FL) environments.\nThis paper addresses the critical issue of group fairness in federated VLMs by\nintroducing FVL-FP, a novel framework that combines FL with fair prompt tuning\ntechniques. We focus on mitigating demographic biases while preserving model\nperformance through three innovative components: (1) Cross-Layer Demographic\nFair Prompting (CDFP), which adjusts potentially biased embeddings through\ncounterfactual regularization; (2) Demographic Subspace Orthogonal Projection\n(DSOP), which removes demographic bias in image representations by mapping fair\nprompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which\ndynamically balances client contributions based on both performance and\nfairness metrics. Extensive evaluations across four benchmark datasets\ndemonstrate that our approach reduces demographic disparity by an average of\n45\\% compared to standard FL approaches, while maintaining task performance\nwithin 6\\% of state-of-the-art results. FVL-FP effectively addresses the\nchallenges of non-IID data distributions in federated settings and introduces\nminimal computational overhead while providing significant fairness benefits.\nOur work presents a parameter-efficient solution to the critical challenge of\nensuring equitable performance across demographic groups in privacy-preserving\nmultimodal systems."}
{"id": "2505.02235", "pdf": "https://arxiv.org/pdf/2505.02235", "abs": "https://arxiv.org/abs/2505.02235", "authors": ["Tanguy Herserant", "Vincent Guigue"], "title": "SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating text summarization quality remains a critical challenge in Natural\nLanguage Processing. Current approaches face a trade-off between performance\nand interpretability. We present SEval-Ex, a framework that bridges this gap by\ndecomposing summarization evaluation into atomic statements, enabling both high\nperformance and explainability. SEval-Ex employs a two-stage pipeline: first\nextracting atomic statements from text source and summary using LLM, then a\nmatching between generated statements. Unlike existing approaches that provide\nonly summary-level scores, our method generates detailed evidence for its\ndecisions through statement-level alignments. Experiments on the SummEval\nbenchmark demonstrate that SEval-Ex achieves state-of-the-art performance with\n0.580 correlation on consistency with human consistency judgments, surpassing\nGPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our\nframework shows robustness against hallucination."}
{"id": "2505.01857", "pdf": "https://arxiv.org/pdf/2505.01857", "abs": "https://arxiv.org/abs/2505.01857", "authors": ["Haoteng Li", "Zhao Yang", "Zezhong Qian", "Gongpeng Zhao", "Yuqi Huang", "Jun Yu", "Huazheng Zhou", "Longjun Liu"], "title": "DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion", "categories": ["cs.CV"], "comment": "8 pages, 6 figures,", "summary": "Accurate and high-fidelity driving scene reconstruction relies on fully\nleveraging scene information as conditioning. However, existing approaches,\nwhich primarily use 3D bounding boxes and binary maps for foreground and\nbackground control, fall short in capturing the complexity of the scene and\nintegrating multi-modal information. In this paper, we propose DualDiff, a\ndual-branch conditional diffusion model designed to enhance multi-view driving\nscene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D\nrepresentation, alongside numerical driving scene representation, for\ncomprehensive foreground and background control. To improve cross-modal\ninformation integration, we propose a Semantic Fusion Attention (SFA) mechanism\nthat aligns and fuses features across modalities. Furthermore, we design a\nforeground-aware masked (FGM) loss to enhance the generation of tiny objects.\nDualDiff achieves state-of-the-art performance in FID score, as well as\nconsistently better results in downstream BEV segmentation and 3D object\ndetection tasks."}
{"id": "2505.02252", "pdf": "https://arxiv.org/pdf/2505.02252", "abs": "https://arxiv.org/abs/2505.02252", "authors": ["Paloma Piot", "Patricia MartÃ­n-Rodilla", "Javier Parapar"], "title": "Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Commercial Large Language Models (LLMs) have recently incorporated memory\nfeatures to deliver personalised responses. This memory retains details such as\nuser demographics and individual characteristics, allowing LLMs to adjust their\nbehaviour based on personal information. However, the impact of integrating\npersonalised information into the context has not been thoroughly assessed,\nleading to questions about its influence on LLM behaviour. Personalisation can\nbe challenging, particularly with sensitive topics. In this paper, we examine\nvarious state-of-the-art LLMs to understand their behaviour in different\npersonalisation scenarios, specifically focusing on hate speech. We prompt the\nmodels to assume country-specific personas and use different languages for hate\nspeech detection. Our findings reveal that context personalisation\nsignificantly influences LLMs' responses in this sensitive area. To mitigate\nthese unwanted biases, we fine-tune the LLMs by penalising inconsistent hate\nspeech classifications made with and without country or language-specific\ncontext. The refined models demonstrate improved performance in both\npersonalised contexts and when no context is provided."}
{"id": "2505.01869", "pdf": "https://arxiv.org/pdf/2505.01869", "abs": "https://arxiv.org/abs/2505.01869", "authors": ["Guoxi Huang", "Haoran Wang", "Brett Seymour", "Evan Kovacs", "John Ellerbrock", "Dave Blackham", "Nantheera Anantrasirichai"], "title": "Visual enhancement and 3D representation for underwater scenes: a review", "categories": ["cs.CV"], "comment": null, "summary": "Underwater visual enhancement (UVE) and underwater 3D reconstruction pose\nsignificant challenges in\n  computer vision and AI-based tasks due to complex imaging conditions in\naquatic environments. Despite\n  the development of numerous enhancement algorithms, a comprehensive and\nsystematic review covering both\n  UVE and underwater 3D reconstruction remains absent. To advance research in\nthese areas, we present an\n  in-depth review from multiple perspectives. First, we introduce the\nfundamental physical models, highlighting the\n  peculiarities that challenge conventional techniques. We survey advanced\nmethods for visual enhancement and\n  3D reconstruction specifically designed for underwater scenarios. The paper\nassesses various approaches from\n  non-learning methods to advanced data-driven techniques, including Neural\nRadiance Fields and 3D Gaussian\n  Splatting, discussing their effectiveness in handling underwater distortions.\nFinally, we conduct both quantitative\n  and qualitative evaluations of state-of-the-art UVE and underwater 3D\nreconstruction algorithms across multiple\n  benchmark datasets. Finally, we highlight key research directions for future\nadvancements in underwater vision."}
{"id": "2505.02266", "pdf": "https://arxiv.org/pdf/2505.02266", "abs": "https://arxiv.org/abs/2505.02266", "authors": ["Henry Ndubuaku", "Mouad Talhi"], "title": "Parameter-Efficient Transformer Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07 (Primary) 68T50 (Secondary)"], "comment": "7 pages, 2 tables. Code available at https://github.com/HMUNACHI/pete", "summary": "Embedding layers in transformer-based NLP models typically account for the\nlargest share of model parameters, scaling with vocabulary size but not\nyielding performance gains proportional to scale. We propose an alternative\napproach in which token embedding vectors are first generated\ndeterministically, directly from the token IDs using a Fourier expansion of\ntheir normalized values, followed by a lightweight multilayer perceptron (MLP)\nthat captures higher-order interactions. We train standard transformers and our\narchitecture on natural language inference tasks (SNLI and MNLI), and evaluate\nzero-shot performance on sentence textual similarity (STS-B). Our results\ndemonstrate that the proposed method achieves competitive performance using\nsignificantly fewer parameters, trains faster, and operates effectively without\nthe need for dropout. This proof-of-concept study highlights the potential for\nscalable, memory-efficient language models and motivates further large-scale\nexperimentation based on our findings."}
{"id": "2505.01881", "pdf": "https://arxiv.org/pdf/2505.01881", "abs": "https://arxiv.org/abs/2505.01881", "authors": ["Trisanth Srinivasan", "Santosh Patapati"], "title": "PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "comment": "9 pages, 5 figures", "summary": "Robust navigation in diverse environments and domains requires both accurate\nstate estimation and transparent decision making. We present PhysNav-DG, a\nnovel framework that integrates classical sensor fusion with the semantic power\nof vision-language models. Our dual-branch architecture predicts navigation\nactions from multi-sensor inputs while simultaneously generating detailed\nchain-of-thought explanations. A modified Adaptive Kalman Filter dynamically\nadjusts its noise parameters based on environmental context. It leverages\nseveral streams of raw sensor data along with semantic insights from models\nsuch as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the\nMD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation,\nautonomous driving, and social navigation tasks with ground-truth actions and\nhuman-validated explanations. Extensive experiments and ablations show that\nPhysNav-DG improves navigation success rates by over 20% and achieves high\nefficiency, with explanations that are both highly grounded and clear. This\nwork connects high-level semantic reasoning and geometric planning for safer\nand more trustworthy autonomous systems."}
{"id": "2505.02273", "pdf": "https://arxiv.org/pdf/2505.02273", "abs": "https://arxiv.org/abs/2505.02273", "authors": ["Rimon Melamed", "Lucas H. McCabe", "H. Howie Huang"], "title": "Demystifying optimized prompts in language models", "categories": ["cs.CL"], "comment": null, "summary": "Modern language models (LMs) are not robust to out-of-distribution inputs.\nMachine generated (``optimized'') prompts can be used to modulate LM outputs\nand induce specific behaviors while appearing completely uninterpretable. In\nthis work, we investigate the composition of optimized prompts, as well as the\nmechanisms by which LMs parse and build predictions from optimized prompts. We\nfind that optimized prompts primarily consist of punctuation and noun tokens\nwhich are more rare in the training data. Internally, optimized prompts are\nclearly distinguishable from natural language counterparts based on sparse\nsubsets of the model's activations. Across various families of\ninstruction-tuned models, optimized prompts follow a similar path in how their\nrepresentations form through the network."}
{"id": "2505.01882", "pdf": "https://arxiv.org/pdf/2505.01882", "abs": "https://arxiv.org/abs/2505.01882", "authors": ["Vladimir Frants", "Sos Agaian", "Karen Panetta", "Peter Huang"], "title": "CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture", "categories": ["cs.CV"], "comment": null, "summary": "Images used in real-world applications such as image or video retrieval,\noutdoor surveillance, and autonomous driving suffer from poor weather\nconditions. When designing robust computer vision systems, removing adverse\nweather such as haze, rain, and snow is a significant problem. Recently,\ndeep-learning methods offered a solution for a single type of degradation.\nCurrent state-of-the-art universal methods struggle with combinations of\ndegradations, such as haze and rain-streak. Few algorithms have been developed\nthat perform well when presented with images containing multiple adverse\nweather conditions. This work focuses on developing an efficient solution for\nmultiple adverse weather removal using a unified quaternion neural architecture\ncalled CMAWRNet. It is based on a novel texture-structure decomposition block,\na novel lightweight encoder-decoder quaternion transformer architecture, and an\nattentive fusion block with low-light correction. We also introduce a\nquaternion similarity loss function to preserve color information better. The\nquantitative and qualitative evaluation of the current state-of-the-art\nbenchmarking datasets and real-world images shows the performance advantages of\nthe proposed CMAWRNet compared to other state-of-the-art weather removal\napproaches dealing with multiple weather artifacts. Extensive computer\nsimulations validate that CMAWRNet improves the performance of downstream\napplications such as object detection. This is the first time the decomposition\napproach has been applied to the universal weather removal task."}
{"id": "2505.02304", "pdf": "https://arxiv.org/pdf/2505.02304", "abs": "https://arxiv.org/abs/2505.02304", "authors": ["Siyu Liang", "Yunan Li", "Wentian Xin", "Huizhou Chen", "Xujie Liu", "Kang Liu", "Qiguang Miao"], "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition", "categories": ["cs.CL", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies."}
{"id": "2505.01888", "pdf": "https://arxiv.org/pdf/2505.01888", "abs": "https://arxiv.org/abs/2505.01888", "authors": ["Xingyu Miao", "Haoran Duan", "Yang Long", "Jungong Han"], "title": "Rethinking Score Distilling Sampling for 3D Editing and Generation", "categories": ["cs.CV"], "comment": null, "summary": "Score Distillation Sampling (SDS) has emerged as a prominent method for\ntext-to-3D generation by leveraging the strengths of 2D diffusion models.\nHowever, SDS is limited to generation tasks and lacks the capability to edit\nexisting 3D assets. Conversely, variants of SDS that introduce editing\ncapabilities often can not generate new 3D assets effectively. In this work, we\nobserve that the processes of generation and editing within SDS and its\nvariants have unified underlying gradient terms. Building on this insight, we\npropose Unified Distillation Sampling (UDS), a method that seamlessly\nintegrates both the generation and editing of 3D assets. Essentially, UDS\nrefines the gradient terms used in vanilla SDS methods, unifying them to\nsupport both tasks. Extensive experiments demonstrate that UDS not only\noutperforms baseline methods in generating 3D assets with richer details but\nalso excels in editing tasks, thereby bridging the gap between 3D generation\nand editing. The code is available on: https://github.com/xingy038/UDS."}
{"id": "2505.02311", "pdf": "https://arxiv.org/pdf/2505.02311", "abs": "https://arxiv.org/abs/2505.02311", "authors": ["Jihao Zhao", "Chunlai Zhou", "Biao Qin"], "title": "Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "The collaborative paradigm of large and small language models (LMs)\neffectively balances performance and cost, yet its pivotal challenge lies in\nprecisely pinpointing the moment of invocation when hallucinations arise in\nsmall LMs. Previous optimization efforts primarily focused on post-processing\ntechniques, which were separate from the reasoning process of LMs, resulting in\nhigh computational costs and limited effectiveness. In this paper, we propose a\npractical invocation evaluation metric called AttenHScore, which calculates the\naccumulation and propagation of hallucinations during the generation process of\nsmall LMs, continuously amplifying potential reasoning errors. By dynamically\nadjusting the detection threshold, we achieve more accurate real-time\ninvocation of large LMs. Additionally, considering the limited reasoning\ncapacity of small LMs, we leverage uncertainty-aware knowledge reorganization\nto assist them better capture critical information from different text chunks.\nExtensive experiments reveal that our AttenHScore outperforms most baseline in\nenhancing real-time hallucination detection capabilities across multiple QA\ndatasets, especially when addressing complex queries. Moreover, our strategies\neliminate the need for additional model training and display flexibility in\nadapting to various transformer-based LMs."}
{"id": "2505.01928", "pdf": "https://arxiv.org/pdf/2505.01928", "abs": "https://arxiv.org/abs/2505.01928", "authors": ["Anushka Agarwal", "Muhammad Yusuf Hassan", "Talha Chafekar"], "title": "GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GenSync, a novel framework for multi-identity lip-synced video\nsynthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that\nrequire training a new model for each identity , GenSync learns a unified\nnetwork that synthesizes lip-synced videos for multiple speakers. By\nincorporating a Disentanglement Module, our approach separates\nidentity-specific features from audio representations, enabling efficient\nmulti-identity video synthesis. This design reduces computational overhead and\nachieves 6.8x faster training compared to state-of-the-art models, while\nmaintaining high lip-sync accuracy and visual quality."}
{"id": "2505.02363", "pdf": "https://arxiv.org/pdf/2505.02363", "abs": "https://arxiv.org/abs/2505.02363", "authors": ["Tianjian Li", "Daniel Khashabi"], "title": "SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning", "categories": ["cs.CL"], "comment": "To appear in ICML 2025", "summary": "Aligning language models with human preferences relies on pairwise preference\ndatasets. While some studies suggest that on-policy data consistently\noutperforms off -policy data for preference learning, others indicate that the\nadvantages of on-policy data may be task-dependent, highlighting the need for a\nsystematic exploration of their interplay.\n  In this work, we show that on-policy and off-policy data offer complementary\nstrengths in preference optimization: on-policy data is particularly effective\nfor reasoning tasks like math and coding, while off-policy data performs better\non open-ended tasks such as creative writing and making personal\nrecommendations. Guided by these findings, we introduce SIMPLEMIX, an approach\nto combine the complementary strengths of on-policy and off-policy preference\nlearning by simply mixing these two data sources. Our empirical results across\ndiverse tasks and benchmarks demonstrate that SIMPLEMIX substantially improves\nlanguage model alignment. Specifically, SIMPLEMIX improves upon on-policy DPO\nand off-policy DPO by an average of 6.03% on Alpaca Eval 2.0. Moreover, it\noutperforms prior approaches that are much more complex in combining on- and\noff-policy data, such as HyPO and DPO-Mix-P, by an average of 3.05%."}
{"id": "2505.01934", "pdf": "https://arxiv.org/pdf/2505.01934", "abs": "https://arxiv.org/abs/2505.01934", "authors": ["Yongxin Su", "Lin Chen", "Kaiting Zhang", "Zhongliang Zhao", "Chenfeng Hou", "Ziping Yu"], "title": "GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels", "categories": ["cs.CV"], "comment": null, "summary": "We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian\nsurfels to achieve robust tracking and high-fidelity mapping. Our\ninvestigations reveal that Gaussian-based scene representations exhibit\ngeometry distortion under novel viewpoints, which significantly degrades the\naccuracy of Gaussian-based tracking methods. These geometry inconsistencies\narise primarily from the depth modeling of Gaussian primitives and the mutual\ninterference between surfaces during the depth blending. To address these, we\npropose a 2D Gaussian-based incremental reconstruction strategy coupled with a\nSurface-aware Depth Rendering mechanism, which significantly enhances geometry\naccuracy and multi-view consistency. Additionally, the proposed local map\ndesign dynamically isolates visible surfaces during tracking, mitigating\nmisalignment caused by occluded regions in global maps while maintaining\ncomputational efficiency with increasing Gaussian density. Extensive\nexperiments across multiple datasets demonstrate that GauS-SLAM outperforms\ncomparable methods, delivering superior tracking precision and rendering\nfidelity. The project page will be made available at\nhttps://gaus-slam.github.io."}
{"id": "2505.02366", "pdf": "https://arxiv.org/pdf/2505.02366", "abs": "https://arxiv.org/abs/2505.02366", "authors": ["Tianyu Zong", "Hongzhu Yi", "Bingkang Shi", "Yuanxiang Wang", "Jungang Xu"], "title": "JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Unsupervised contrastive learning has become a hot research topic in natural\nlanguage processing. Existing works usually aim at constraining the orientation\ndistribution of the representations of positive and negative samples in the\nhigh-dimensional semantic space in contrastive learning, but the semantic\nrepresentation tensor possesses both modulus and orientation features, and the\nexisting works ignore the modulus feature of the representations and cause\ninsufficient contrastive learning. % Therefore, we firstly propose a training\nobjective that aims at modulus constraints on the semantic representation\ntensor, to strengthen the alignment between the positive samples in contrastive\nlearning. Therefore, we first propose a training objective that is designed to\nimpose modulus constraints on the semantic representation tensor, to strengthen\nthe alignment between positive samples in contrastive learning. Then, the\nBERT-like model suffers from the phenomenon of sinking attention, leading to a\nlack of attention to CLS tokens that aggregate semantic information. In\nresponse, we propose a cross-attention structure among the twin-tower ensemble\nmodels to enhance the model's attention to CLS token and optimize the quality\nof CLS Pooling. Combining the above two motivations, we propose a new\n\\textbf{J}oint \\textbf{T}ensor representation modulus constraint and\n\\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence\n\\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven\nsemantic text similarity computation tasks, and the experimental results show\nthat JTCSE's twin-tower ensemble model and single-tower distillation model\noutperform the other baselines and become the current SOTA. In addition, we\nhave conducted an extensive zero-shot downstream task evaluation, which shows\nthat JTCSE outperforms other baselines overall on more than 130 tasks."}
{"id": "2505.01938", "pdf": "https://arxiv.org/pdf/2505.01938", "abs": "https://arxiv.org/abs/2505.01938", "authors": ["Qi Yang", "Le Yang", "Geert Van Der Auwera", "Zhu Li"], "title": "HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by ICML2025", "summary": "Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on\nproducing compact 3DGS representation via implicit data embedding. They have\nlong coding times and highly customized data format, making it difficult for\nwidespread deployment. This paper presents a new 3DGS compression framework\ncalled HybridGS, which takes advantage of both compact generation and\nstandardized point cloud data encoding. HybridGS first generates compact and\nexplicit 3DGS data. A dual-channel sparse representation is introduced to\nsupervise the primitive position and feature bit depth. It then utilizes a\ncanonical point cloud encoder to perform further data compression and form\nstandard output bitstreams. A simple and effective rate control scheme is\nproposed to pivot the interpretable data compression scheme. At the current\nstage, HybridGS does not include any modules aimed at improving 3DGS quality\nduring generation. But experiment results show that it still provides\ncomparable reconstruction performance against state-of-the-art methods, with\nevidently higher encoding and decoding speed. The code is publicly available at\nhttps://github.com/Qi-Yangsjtu/HybridGS."}
{"id": "2505.02387", "pdf": "https://arxiv.org/pdf/2505.02387", "abs": "https://arxiv.org/abs/2505.02387", "authors": ["Xiusi Chen", "Gaotang Li", "Ziqi Wang", "Bowen Jin", "Cheng Qian", "Yu Wang", "Hongru Wang", "Yu Zhang", "Denghui Zhang", "Tong Zhang", "Hanghang Tong", "Heng Ji"], "title": "RM-R1: Reward Modeling as Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 7 figures", "summary": "Reward modeling is essential for aligning large language models (LLMs) with\nhuman preferences, especially through reinforcement learning from human\nfeedback (RLHF). To provide accurate reward signals, a reward model (RM) should\nstimulate deep thinking and conduct interpretable reasoning before assigning a\nscore or a judgment. However, existing RMs either produce opaque scalar scores\nor directly generate the prediction of a preferred answer, making them struggle\nto integrate natural language critiques, thus lacking interpretability.\nInspired by recent advances of long chain-of-thought (CoT) on\nreasoning-intensive tasks, we hypothesize and validate that integrating\nreasoning capabilities into reward modeling significantly enhances RM's\ninterpretability and performance. In this work, we introduce a new class of\ngenerative reward models -- Reasoning Reward Models (ReasRMs) -- which\nformulate reward modeling as a reasoning task. We propose a reasoning-oriented\ntraining pipeline and train a family of ReasRMs, RM-R1. The training consists\nof two key stages: (1) distillation of high-quality reasoning chains and (2)\nreinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by\nself-generating reasoning traces or chat-specific rubrics and evaluating\ncandidate responses against them. Empirically, our models achieve\nstate-of-the-art or near state-of-the-art performance of generative RMs across\nmultiple comprehensive reward model benchmarks, outperforming much larger\nopen-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by\nup to 13.8%. Beyond final performance, we perform thorough empirical analysis\nto understand the key ingredients of successful ReasRM training. To facilitate\nfuture research, we release six ReasRM models along with code and data at\nhttps://github.com/RM-R1-UIUC/RM-R1."}
{"id": "2505.01950", "pdf": "https://arxiv.org/pdf/2505.01950", "abs": "https://arxiv.org/abs/2505.01950", "authors": ["Dong Xing", "Xianxun Zhu", "Wei Zhou", "Qika Lin", "Hang Yang", "Yuqing Wang"], "title": "Segment Any RGB-Thermal Model with Language-aided Distillation", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2412.04220 by other authors", "summary": "The recent Segment Anything Model (SAM) demonstrates strong instance\nsegmentation performance across various downstream tasks. However, SAM is\ntrained solely on RGB data, limiting its direct applicability to RGB-thermal\n(RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for\nscene understanding in adverse weather and lighting conditions, such as low\nlight and overexposure, we propose a novel framework, SARTM, which customizes\nthe powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash\nthe potential of SAM while introduce semantic understanding modules for RGB-T\ndata pairs. Specifically, our framework first involves fine tuning the original\nSAM by adding extra LoRA layers, aiming at preserving SAM's strong\ngeneralization and segmentation capabilities for downstream tasks. Secondly, we\nintroduce language information as guidance for training our SARTM. To address\ncross-modal inconsistencies, we introduce a Cross-Modal Knowledge\nDistillation(CMKD) module that effectively achieves modality adaptation while\nmaintaining its generalization capabilities. This semantic module enables the\nminimization of modality gaps and alleviates semantic ambiguity, facilitating\nthe combination of any modality under any visual conditions. Furthermore, we\nenhance the segmentation performance by adjusting the segmentation head of SAM\nand incorporating an auxiliary semantic segmentation head, which integrates\nmulti-scale features for effective fusion. Extensive experiments are conducted\nacross three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900,\nand FMB. Both quantitative and qualitative results consistently demonstrate\nthat the proposed SARTM significantly outperforms state-of-the-art approaches\nacross a variety of conditions."}
{"id": "2505.02410", "pdf": "https://arxiv.org/pdf/2505.02410", "abs": "https://arxiv.org/abs/2505.02410", "authors": ["Krzysztof Ociepa", "Åukasz Flis", "Krzysztof WrÃ³bel", "Adrian GwoÅºdziej", "Remigiusz Kinas"], "title": "Bielik 11B v2 Technical Report", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": null, "summary": "We present Bielik 11B v2, a state-of-the-art language model optimized for\nPolish text processing. Built on the Mistral 7B v0.2 architecture and scaled to\n11B parameters using depth up-scaling, this model demonstrates exceptional\nperformance across Polish language benchmarks while maintaining strong\ncross-lingual capabilities. We introduce two key technical innovations:\nWeighted Instruction Cross-Entropy Loss, which optimizes learning across\ndiverse instruction types by assigning quality-based weights to training\nexamples, and Adaptive Learning Rate, which dynamically adjusts based on\ncontext length. Comprehensive evaluation across multiple benchmarks\ndemonstrates that Bielik 11B v2 outperforms many larger models, including those\nwith 2-6 times more parameters, and significantly surpasses other specialized\nPolish language models on tasks ranging from linguistic understanding to\ncomplex reasoning. The model's parameter efficiency and extensive quantization\noptions enable deployment across various hardware configurations, advancing\nPolish language AI capabilities and establishing new benchmarks for\nresource-efficient language modeling in less-represented languages."}
{"id": "2505.01958", "pdf": "https://arxiv.org/pdf/2505.01958", "abs": "https://arxiv.org/abs/2505.01958", "authors": ["Liqiang Jing", "Guiming Hardy Chen", "Ehsan Aghazadeh", "Xin Eric Wang", "Xinya Du"], "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks, but visual object hallucination remains a persistent issue.\nIt refers to scenarios where models generate inaccurate visual object-related\ninformation based on the query input, potentially leading to misinformation and\nconcerns about safety and reliability. Previous works focus on the evaluation\nand mitigation of visual hallucinations, but the underlying causes have not\nbeen comprehensively investigated. In this paper, we analyze each component of\nLLaVA-like LVLMs -- the large language model, the vision backbone, and the\nprojector -- to identify potential sources of error and their impact. Based on\nour observations, we propose methods to mitigate hallucination for each\nproblematic component. Additionally, we developed two hallucination benchmarks:\nQA-VisualGenome, which emphasizes attribute and relation hallucinations, and\nQA-FB15k, which focuses on cognition-based hallucinations."}
{"id": "2505.02456", "pdf": "https://arxiv.org/pdf/2505.02456", "abs": "https://arxiv.org/abs/2505.02456", "authors": ["Elisa Forcada RodrÃ­guez", "Olatz Perez-de-ViÃ±aspre", "Jon Ander Campos", "Dietrich Klakow", "Vagrant Gautam"], "title": "Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs", "categories": ["cs.CL"], "comment": null, "summary": "One of the goals of fairness research in NLP is to measure and mitigate\nstereotypical biases that are propagated by NLP systems. However, such work\ntends to focus on single axes of bias (most often gender) and the English\nlanguage. Addressing these limitations, we contribute the first study of\nmultilingual intersecting country and gender biases, with a focus on occupation\nrecommendations generated by large language models. We construct a benchmark of\nprompts in English, Spanish and German, where we systematically vary country\nand gender, using 25 countries and four pronoun sets. Then, we evaluate a suite\nof 5 Llama-based models on this benchmark, finding that LLMs encode significant\ngender and country biases. Notably, we find that even when models show parity\nfor gender or country individually, intersectional occupational biases based on\nboth country and gender persist. We also show that the prompting language\nsignificantly affects bias, and instruction-tuned models consistently\ndemonstrate the lowest and most stable levels of bias. Our findings highlight\nthe need for fairness researchers to use intersectional and multilingual lenses\nin their work."}
{"id": "2505.01969", "pdf": "https://arxiv.org/pdf/2505.01969", "abs": "https://arxiv.org/abs/2505.01969", "authors": ["Jiayi Cheng", "Can Gao", "Jie Zhou", "Jiajun Wen", "Tao Dai", "Jinbao Wang"], "title": "MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection", "categories": ["cs.CV"], "comment": "7 pages of main text, 3 pages of appendix, accepted to IJCAI 2025", "summary": "3D Anomaly Detection (AD) is a promising means of controlling the quality of\nmanufactured products. However, existing methods typically require carefully\ntraining a task-specific model for each category independently, leading to high\ncost, low efficiency, and weak generalization. Therefore, this paper presents a\nnovel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims\nto utilize both local and global geometry-aware information to reconstruct\nnormal representations of all categories. First, to learn robust and\ngeneralized features of different categories, we propose an adaptive\ngeometry-aware masked attention module that extracts geometry variation\ninformation to guide mask attention. Then, we introduce a local geometry-aware\nencoder reinforced by the improved mask attention to encode group-level feature\ntokens. Finally, we design a global query decoder that utilizes point cloud\nposition embeddings to improve the decoding process and reconstruction ability.\nThis leads to local and global geometry-aware reconstructed feature tokens for\nthe AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and\nAnomaly-ShapeNet datasets, and exhibits significant superiority over current\nstate-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement\nin object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The\nsource code will be released upon acceptance."}
{"id": "2505.02463", "pdf": "https://arxiv.org/pdf/2505.02463", "abs": "https://arxiv.org/abs/2505.02463", "authors": ["Richard Kimera", "Dongnyeong Heo", "Daniela N. Rim", "Heeyoul Choi"], "title": "Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda", "categories": ["cs.CL"], "comment": "NLPIR '24: Proceedings of the 2024 8th International Conference on\n  Natural Language Processing and Information Retrieval", "summary": "In this paper,we explore the application of Back translation (BT) as a\nsemi-supervised technique to enhance Neural Machine Translation(NMT) models for\nthe English-Luganda language pair, specifically addressing the challenges faced\nby low-resource languages. The purpose of our study is to demonstrate how BT\ncan mitigate the scarcity of bilingual data by generating synthetic data from\nmonolingual corpora. Our methodology involves developing custom NMT models\nusing both publicly available and web-crawled data, and applying Iterative and\nIncremental Back translation techniques. We strategically select datasets for\nincremental back translation across multiple small datasets, which is a novel\nelement of our approach. The results of our study show significant\nimprovements, with translation performance for the English-Luganda pair\nexceeding previous benchmarks by more than 10 BLEU score units across all\ntranslation directions. Additionally, our evaluation incorporates comprehensive\nassessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced\nunderstanding of translation quality. The conclusion drawn from our research\nconfirms the efficacy of BT when strategically curated datasets are utilized,\nestablishing new performance benchmarks and demonstrating the potential of BT\nin enhancing NMT models for low-resource languages."}
{"id": "2505.01973", "pdf": "https://arxiv.org/pdf/2505.01973", "abs": "https://arxiv.org/abs/2505.01973", "authors": ["Anthony Dontoh", "Stephanie Ivey", "Logan Sirbaugh", "Andrews Danyo", "Armstrong Aboah"], "title": "Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques", "categories": ["cs.CV"], "comment": null, "summary": "Distracted driving continues to be a significant cause of road traffic\ninjuries and fatalities worldwide, even with advancements in driver monitoring\ntechnologies. Recent developments in machine learning (ML) and deep learning\n(DL) have primarily focused on visual data to detect distraction, often\nneglecting the complex, multimodal nature of driver behavior. This systematic\nreview assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL\ntechniques for distracted driving detection across visual, sensor-based,\nmultimodal, and emerging modalities. The review highlights a significant\nprevalence of visual-only models, particularly convolutional neural networks\n(CNNs) and temporal architectures, which achieve high accuracy but show limited\ngeneralizability in real-world scenarios. Sensor-based and physiological models\nprovide complementary strengths by capturing internal states and vehicle\ndynamics, while emerging techniques, such as auditory sensing and radio\nfrequency (RF) methods, offer privacy-aware alternatives. Multimodal\narchitecture consistently surpasses unimodal baselines, demonstrating enhanced\nrobustness, context awareness, and scalability by integrating diverse data\nstreams. These findings emphasize the need to move beyond visual-only\napproaches and adopt multimodal systems that combine visual, physiological, and\nvehicular cues while keeping in checking the need to balance computational\nrequirements. Future research should focus on developing lightweight,\ndeployable multimodal frameworks, incorporating personalized baselines, and\nestablishing cross-modality benchmarks to ensure real-world reliability in\nadvanced driver assistance systems (ADAS) and road safety interventions."}
{"id": "2505.02518", "pdf": "https://arxiv.org/pdf/2505.02518", "abs": "https://arxiv.org/abs/2505.02518", "authors": ["Muhammad Hazim Al Farouq", "Aman Kassahun Wassie", "Yasmin Moslem"], "title": "Bemba Speech Translation: Exploring a Low-Resource African Language", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "This paper describes our system submission to the International Conference on\nSpoken Language Translation (IWSLT 2025), low-resource languages track, namely\nfor Bemba-to-English speech translation. We built cascaded speech translation\nsystems based on Whisper and NLLB-200, and employed data augmentation\ntechniques, such as back-translation. We investigate the effect of using\nsynthetic data and discuss our experimental setup."}
{"id": "2505.01984", "pdf": "https://arxiv.org/pdf/2505.01984", "abs": "https://arxiv.org/abs/2505.01984", "authors": ["Doanh C. Bui", "Hoai Luan Pham", "Vu Trung Duong Le", "Tuan Hai Vu", "Van Duy Tran", "Khang Nguyen", "Yasuhiko Nakashima"], "title": "Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis\nand prognosis, as they provide tissue details at the cellular level. However,\nthe rapid growth of computational tasks involving WSIs poses significant\nchallenges. Given that WSIs are gigapixels in size, they present difficulties\nin terms of storage, processing, and model training. Therefore, it is essential\nto develop lifelong learning approaches for WSI analysis. In scenarios where\nslides are distributed across multiple institutes, we aim to leverage them to\ndevelop a unified online model as a computational tool for cancer diagnosis in\nclinical and hospital settings. In this study, we introduce ADaFGrad, a method\ndesigned to enhance lifelong learning for whole-slide image (WSI) analysis.\nFirst, we leverage pathology vision-language foundation models to develop a\nframework that enables interaction between a slide's regional tissue features\nand a predefined text-based prototype buffer. Additionally, we propose a\ngradient-distillation mechanism that mimics the gradient of a logit with\nrespect to the classification-head parameters across past and current\niterations in a continual-learning setting. We construct a sequence of six TCGA\ndatasets for training and evaluation. Experimental results show that ADaFGrad\noutperforms both state-of-the-art WSI-specific and conventional\ncontinual-learning methods after only a few training epochs, exceeding them by\nup to +5.068% in the class-incremental learning scenario while exhibiting the\nleast forgetting (i.e., retaining the most knowledge from previous tasks).\nMoreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy,\nfurther demonstrating the effectiveness of the proposed modules."}
{"id": "2505.02579", "pdf": "https://arxiv.org/pdf/2505.02579", "abs": "https://arxiv.org/abs/2505.02579", "authors": ["Lingxiao Kong", "Cong Yang", "Susanne Neufang", "Oya Deniz Beyan", "Zeyd Boukhers"], "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 9 figures, submitted to SIGDIAL 2025 conference", "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including complex objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the training to improve efficiency and\nflexibility. Our method is the first to aggregate the last hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text-scoring LLMs to evaluate the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."}
{"id": "2505.01986", "pdf": "https://arxiv.org/pdf/2505.01986", "abs": "https://arxiv.org/abs/2505.01986", "authors": ["Yongming Li", "Peng Wang", "Bangdong Han"], "title": "Drug classification based on X-ray spectroscopy combined with machine learning", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of new types of drugs necessitates the urgent development\nof faster and more accurate detection methods. Traditional detection methods\nhave high requirements for instruments and environments, making the operation\ncomplex. X-ray absorption spectroscopy, a non-destructive detection technique,\noffers advantages such as ease of operation, penetrative observation, and\nstrong substance differentiation capabilities, making it well-suited for\napplication in the field of drug detection and identification. In this study,\nwe constructed a classification model using Convolutional Neural Networks\n(CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to\nclassify and identify drugs based on their X-ray spectral profiles. In the\nexperiments, we selected 14 chemical reagents with chemical formulas similar to\ndrugs as samples. We utilized CNN to extract features from the spectral data of\nthese 14 chemical reagents and used the extracted features to train an SVM\nmodel. We also utilized PSO to optimize two critical initial parameters of the\nSVM. The experimental results demonstrate that this model achieved higher\nclassification accuracy compared to two other common methods, with a prediction\naccuracy of 99.14%. Additionally, the model exhibited fast execution speed,\nmitigating the drawback of a drastic increase in running time and efficiency\nreduction that may result from the direct fusion of PSO and SVM. Therefore, the\ncombined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM\nprovides a rapid, highly accurate, and reliable classification and\nidentification method for the field of drug detection, holding promising\nprospects for widespread application."}
{"id": "2505.02590", "pdf": "https://arxiv.org/pdf/2505.02590", "abs": "https://arxiv.org/abs/2505.02590", "authors": ["Diksha Bhandari", "Alessandro Lopopolo", "Milena Rabovsky", "Sebastian Reich"], "title": "Ensemble Kalman filter for uncertainty in human language comprehension", "categories": ["cs.CL", "stat.AP", "stat.ML"], "comment": null, "summary": "Artificial neural networks (ANNs) are widely used in modeling sentence\nprocessing but often exhibit deterministic behavior, contrasting with human\nsentence comprehension, which manages uncertainty during ambiguous or\nunexpected inputs. This is exemplified by reversal anomalies-sentences with\nunexpected role reversals that challenge syntax and semantics-highlighting the\nlimitations of traditional ANN models, such as the Sentence Gestalt (SG) Model.\nTo address these limitations, we propose a Bayesian framework for sentence\ncomprehension, applying an extension of the ensemble Kalman filter (EnKF) for\nBayesian inference to quantify uncertainty. By framing language comprehension\nas a Bayesian inverse problem, this approach enhances the SG model's ability to\nreflect human sentence processing with respect to the representation of\nuncertainty. Numerical experiments and comparisons with maximum likelihood\nestimation (MLE) demonstrate that Bayesian methods improve uncertainty\nrepresentation, enabling the model to better approximate human cognitive\nprocessing when dealing with linguistic ambiguities."}
{"id": "2505.02005", "pdf": "https://arxiv.org/pdf/2505.02005", "abs": "https://arxiv.org/abs/2505.02005", "authors": ["Zhenxing Mi", "Ping Yin", "Xue Xiao", "Dan Xu"], "title": "Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Recent NeRF methods on large-scale scenes have underlined the importance of\nscene decomposition for scalable NeRFs. Although achieving reasonable\nscalability, there are several critical problems remaining unexplored, i.e.,\nlearnable decomposition, modeling scene heterogeneity, and modeling efficiency.\nIn this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash\nExperts (HMoHE) network that addresses these challenges within a unified\nframework. It is a highly scalable NeRF that learns heterogeneous decomposition\nand heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end\nmanner. In our framework, a gating network learns to decomposes scenes and\nallocates 3D points to specialized NeRF experts. This gating network is\nco-optimized with the experts, by our proposed Sparsely Gated Mixture of\nExperts (MoE) NeRF framework. We incorporate a hash-based gating network and\ndistinct heterogeneous hash experts. The hash-based gating efficiently learns\nthe decomposition of the large-scale scene. The distinct heterogeneous hash\nexperts consist of hash grids of different resolution ranges, enabling\neffective learning of the heterogeneous representation of different scene\nparts. These design choices make our framework an end-to-end and highly\nscalable NeRF solution for real-world large-scale scene modeling to achieve\nboth quality and efficiency. We evaluate our accuracy and scalability on\nexisting large-scale NeRF datasets and a new dataset with very large-scale\nscenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our\napproach can be easily scaled to various large-scale scenes and achieve\nstate-of-the-art scene rendering accuracy. Furthermore, our method exhibits\nsignificant efficiency, with an 8x acceleration in training and a 16x\nacceleration in rendering compared to Switch-NeRF. Codes will be released in\nhttps://github.com/MiZhenxing/Switch-NeRF."}
{"id": "2505.02615", "pdf": "https://arxiv.org/pdf/2505.02615", "abs": "https://arxiv.org/abs/2505.02615", "authors": ["Armita Mohammadi", "Alessandro Lameiras Koerich", "Laureano Moro-Velazquez", "Patrick Cardinal"], "title": "Automatic Proficiency Assessment in L2 English Learners", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "6 pages", "summary": "Second language proficiency (L2) in English is usually perceptually evaluated\nby English teachers or expert evaluators, with the inherent intra- and\ninter-rater variability. This paper explores deep learning techniques for\ncomprehensive L2 proficiency assessment, addressing both the speech signal and\nits correspondent transcription. We analyze spoken proficiency classification\nprediction using diverse architectures, including 2D CNN, frequency-based CNN,\nResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based\nproficiency assessment by fine-tuning a BERT language model within resource\nconstraints. Finally, we tackle the complex task of spontaneous dialogue\nassessment, managing long-form audio and speaker interactions through separate\napplications of wav2vec 2.0 and BERT models. Results from experiments on\nEFCamDat and ANGLISH datasets and a private dataset highlight the potential of\ndeep learning, especially the pretrained wav2vec 2.0 model, for robust\nautomated L2 proficiency evaluation."}
{"id": "2505.02007", "pdf": "https://arxiv.org/pdf/2505.02007", "abs": "https://arxiv.org/abs/2505.02007", "authors": ["Onat Dalmaz", "Arjun D. Desai", "Reinhard Heckel", "Tolga Ãukur", "Akshay S. Chaudhari", "Brian A. Hargreaves"], "title": "Efficient Noise Calculation in Deep Learning-based MRI Reconstructions", "categories": ["cs.CV", "65C60, 94A08, 68T07", "I.4.5; I.2.10; G.1.2"], "comment": "Accepted ICML 2025. Supplementary material included", "summary": "Accelerated MRI reconstruction involves solving an ill-posed inverse problem\nwhere noise in acquired data propagates to the reconstructed images. Noise\nanalyses are central to MRI reconstruction for providing an explicit measure of\nsolution fidelity and for guiding the design and deployment of novel\nreconstruction methods. However, deep learning (DL)-based reconstruction\nmethods have often overlooked noise propagation due to inherent analytical and\ncomputational challenges, despite its critical importance. This work proposes a\ntheoretically grounded, memory-efficient technique to calculate voxel-wise\nvariance for quantifying uncertainty due to acquisition noise in accelerated\nMRI reconstructions. Our approach approximates noise covariance using the DL\nnetwork's Jacobian, which is intractable to calculate. To circumvent this, we\nderive an unbiased estimator for the diagonal of this covariance matrix\n(voxel-wise variance) and introduce a Jacobian sketching technique to\nefficiently implement it. We evaluate our method on knee and brain MRI datasets\nfor both data- and physics-driven networks trained in supervised and\nunsupervised manners. Compared to empirical references obtained via Monte Carlo\nsimulations, our technique achieves near-equivalent performance while reducing\ncomputational and memory demands by an order of magnitude or more. Furthermore,\nour method is robust across varying input noise levels, acceleration factors,\nand diverse undersampling schemes, highlighting its broad applicability. Our\nwork reintroduces accurate and efficient noise analysis as a central tenet of\nreconstruction algorithms, holding promise to reshape how we evaluate and\ndeploy DL-based MRI. Our code will be made publicly available upon acceptance."}
{"id": "2505.02625", "pdf": "https://arxiv.org/pdf/2505.02625", "abs": "https://arxiv.org/abs/2505.02625", "authors": ["Qingkai Fang", "Yan Zhou", "Shoutao Guo", "Shaolei Zhang", "Yang Feng"], "title": "LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Preprint. Project: https://github.com/ictnlp/LLaMA-Omni2", "summary": "Real-time, intelligent, and natural speech interaction is an essential part\nof the next-generation human-computer interaction. Recent advancements have\nshowcased the potential of building intelligent spoken chatbots based on large\nlanguage models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of\nspeech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable\nof achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built\nupon the Qwen2.5 series models, integrating a speech encoder and an\nautoregressive streaming speech decoder. Despite being trained on only 200K\nmulti-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong\nperformance on several spoken question answering and speech instruction\nfollowing benchmarks, surpassing previous state-of-the-art SpeechLMs like\nGLM-4-Voice, which was trained on millions of hours of speech data."}
{"id": "2505.02013", "pdf": "https://arxiv.org/pdf/2505.02013", "abs": "https://arxiv.org/abs/2505.02013", "authors": ["Siran Peng", "Zipei Wang", "Li Gao", "Xiangyu Zhu", "Tianshuo Zhang", "Ajian Liu", "Haoyuan Zhang", "Zhen Lei"], "title": "MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution", "categories": ["cs.CV"], "comment": null, "summary": "Reliable face forgery detection algorithms are crucial for countering the\ngrowing threat of deepfake-driven disinformation. Previous research has\ndemonstrated the potential of Multimodal Large Language Models (MLLMs) in\nidentifying manipulated faces. However, existing methods typically depend on\neither the Large Language Model (LLM) alone or an external detector to generate\nclassification results, which often leads to sub-optimal integration of visual\nand textual modalities. In this paper, we propose VLF-FFD, a novel\nVision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our\nkey contributions are twofold. First, we present EFF++, a frame-level,\nexplainability-driven extension of the widely used FaceForensics++ (FF++)\ndataset. In EFF++, each manipulated video frame is paired with a textual\nannotation that describes both the forgery artifacts and the specific\nmanipulation technique applied, enabling more effective and informative MLLM\ntraining. Second, we design a Vision-Language Fusion Network (VLF-Net) that\npromotes bidirectional interaction between visual and textual features,\nsupported by a three-stage training pipeline to fully leverage its potential.\nVLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and\nintra-dataset evaluations, underscoring its exceptional effectiveness in face\nforgery detection."}
{"id": "2505.02656", "pdf": "https://arxiv.org/pdf/2505.02656", "abs": "https://arxiv.org/abs/2505.02656", "authors": ["Rawan Bondok", "Mayar Nassar", "Salam Khalifa", "Kurt Micallaf", "Nizar Habash"], "title": "Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset", "categories": ["cs.CL"], "comment": null, "summary": "Proper names in Arabic Wikipedia are frequently undiacritized, creating\nambiguity in pronunciation and interpretation, especially for transliterated\nnamed entities of foreign origin. While transliteration and diacritization have\nbeen well-studied separately in Arabic NLP,their intersection remains\nunderexplored. In this paper, we introduce a new manually diacritized dataset\nof Arabic proper names of various origins with their English Wikipedia\nequivalent glosses, and present the challenges and guidelines we followed to\ncreate it. We benchmark GPT-4o on the task of recovering full diacritization\ngiven the undiacritized Arabic and English forms, and analyze its performance.\nAchieving 73% accuracy, our results underscore both the difficulty of the task\nand the need for improved models and resources. We release our dataset to\nfacilitate further research on Arabic Wikipedia proper name diacritization."}
{"id": "2505.02018", "pdf": "https://arxiv.org/pdf/2505.02018", "abs": "https://arxiv.org/abs/2505.02018", "authors": ["Meng-Hao Guo", "Jiajun Xu", "Yi Zhang", "Jiaxi Song", "Haoyang Peng", "Yi-Xuan Deng", "Xinzhi Dong", "Kiyohiro Nakayama", "Zhengyang Geng", "Chen Wang", "Bolin Ni", "Guo-Wei Yang", "Yongming Rao", "Houwen Peng", "Han Hu", "Gordon Wetzstein", "Shi-min Hu"], "title": "R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation", "categories": ["cs.CV"], "comment": "18pages", "summary": "Reasoning stands as a cornerstone of intelligence, enabling the synthesis of\nexisting knowledge to solve complex problems. Despite remarkable progress,\nexisting reasoning benchmarks often fail to rigorously evaluate the nuanced\nreasoning capabilities required for complex, real-world problemsolving,\nparticularly in multi-disciplinary and multimodal contexts. In this paper, we\nintroduce a graduate-level, multi-disciplinary, EnglishChinese benchmark,\ndubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of\nboth language and multimodal models. RBench spans 1,094 questions across 108\nsubjects for language model evaluation and 665 questions across 83 subjects for\nmultimodal model testing in both English and Chinese. These questions are\nmeticulously curated to ensure rigorous difficulty calibration, subject\nbalance, and crosslinguistic alignment, enabling the assessment to be an\nOlympiad-level multi-disciplinary benchmark. We evaluate widely used models,\nincluding OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate\nthat advanced models perform poorly on complex reasoning, especially multimodal\nreasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy\non our multimodal evaluation. Data and code are made publicly available at\nhere."}
{"id": "2505.02666", "pdf": "https://arxiv.org/pdf/2505.02666", "abs": "https://arxiv.org/abs/2505.02666", "authors": ["Miaomiao Ji", "Yanqiu Wu", "Zhibin Wu", "Shoujin Wang", "Jian Yang", "Mark Dras", "Usman Naseem"], "title": "A Survey on Progress in LLM Alignment from the Perspective of Reward Design", "categories": ["cs.CL"], "comment": "Preprint", "summary": "The alignment of large language models (LLMs) with human values and\nintentions represents a core challenge in current AI research, where reward\nmechanism design has become a critical factor in shaping model behavior. This\nstudy conducts a comprehensive investigation of reward mechanisms in LLM\nalignment through a systematic theoretical framework, categorizing their\ndevelopment into three key phases: (1) feedback (diagnosis), (2) reward design\n(prescription), and (3) optimization (treatment). Through a four-dimensional\nanalysis encompassing construction basis, format, expression, and granularity,\nthis research establishes a systematic classification framework that reveals\nevolutionary trends in reward modeling. The field of LLM alignment faces\nseveral persistent challenges, while recent advances in reward design are\ndriving significant paradigm shifts. Notable developments include the\ntransition from reinforcement learning-based frameworks to novel optimization\nparadigms, as well as enhanced capabilities to address complex alignment\nscenarios involving multimodal integration and concurrent task coordination.\nFinally, this survey outlines promising future research directions for LLM\nalignment through innovative reward design strategies."}
{"id": "2505.02025", "pdf": "https://arxiv.org/pdf/2505.02025", "abs": "https://arxiv.org/abs/2505.02025", "authors": ["Hongbo Zhao", "Ziwei Long", "Mengtan Zhang", "Hanli Wang", "Qijun Chen", "Rui Fan"], "title": "A Birotation Solution for Relative Pose Problems", "categories": ["cs.CV"], "comment": null, "summary": "Relative pose estimation, a fundamental computer vision problem, has been\nextensively studied for decades. Existing methods either estimate and decompose\nthe essential matrix or directly estimate the rotation and translation to\nobtain the solution. In this article, we break the mold by tackling this\ntraditional problem with a novel birotation solution. We first introduce three\nbasis transformations, each associated with a geometric metric to quantify the\ndistance between the relative pose to be estimated and its corresponding basis\ntransformation. Three energy functions, designed based on these metrics, are\nthen minimized on the Riemannian manifold $\\mathrm{SO(3)}$ by iteratively\nupdating the two rotation matrices. The two rotation matrices and the basis\ntransformation corresponding to the minimum energy are ultimately utilized to\nrecover the relative pose. Extensive quantitative and qualitative evaluations\nacross diverse relative pose estimation tasks demonstrate the superior\nperformance of our proposed birotation solution. Source code, demo video, and\ndatasets will be available at\n\\href{https://mias.group/birotation-solution}{mias.group/birotation-solution}\nupon publication."}
{"id": "2505.02686", "pdf": "https://arxiv.org/pdf/2505.02686", "abs": "https://arxiv.org/abs/2505.02686", "authors": ["Xiaobao Wu"], "title": "Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models", "categories": ["cs.CL"], "comment": "35 Pages", "summary": "Recent developments in Large Language Models (LLMs) have shifted from\npre-training scaling to post-training and test-time scaling. Across these\ndevelopments, a key unified paradigm has arisen: Learning from Rewards, where\nreward signals act as the guiding stars to steer LLM behavior. It has\nunderpinned a wide range of prevalent techniques, such as reinforcement\nlearning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc\ncorrection. Crucially, this paradigm enables the transition from passive\nlearning from static data to active learning from dynamic feedback. This endows\nLLMs with aligned preferences and deep reasoning capabilities. In this survey,\nwe present a comprehensive overview of the paradigm of learning from rewards.\nWe categorize and analyze the strategies under this paradigm across training,\ninference, and post-inference stages. We further discuss the benchmarks for\nreward models and the primary applications. Finally we highlight the challenges\nand future directions. We maintain a paper collection at\nhttps://github.com/bobxwu/learning-from-rewards-llm-papers."}
{"id": "2505.02043", "pdf": "https://arxiv.org/pdf/2505.02043", "abs": "https://arxiv.org/abs/2505.02043", "authors": ["Cheng Wang", "Xinzhu Ma", "Bin Wang", "Shixiang Tang", "Yuan Meng", "Ping Jiang"], "title": "Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Recovering CAD models from point clouds, especially the sketch-extrusion\nprocess, can be seen as the process of rebuilding the topology and extrusion\nprimitives. Previous methods utilize implicit fields for sketch representation,\nleading to shape reconstruction of curved edges. In this paper, we proposed a\nCAD reconstruction network that produces editable CAD models from input point\nclouds (Point2Primitive) by directly predicting every element of the extrusion\nprimitives. Point2Primitive can directly detect and predict sketch curves (type\nand parameter) from point clouds based on an improved transformer. The sketch\ncurve parameters are formulated as position queries and optimized in an\nautoregressive way, leading to high parameter accuracy. The topology is rebuilt\nby extrusion segmentation, and each extrusion parameter (sketch and extrusion\noperation) is recovered by combining the predicted curves and the computed\nextrusion operation. Extensive experiments demonstrate that our method is\nsuperior in primitive prediction accuracy and CAD reconstruction. The\nreconstructed shapes are of high geometrical fidelity."}
{"id": "2505.02692", "pdf": "https://arxiv.org/pdf/2505.02692", "abs": "https://arxiv.org/abs/2505.02692", "authors": ["Maxime Poli", "Emmanuel Chemla", "Emmanuel Dupoux"], "title": "fastabx: A library for efficient computation of ABX discriminability", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "8 pages, 6 figures", "summary": "We introduce fastabx, a high-performance Python library for building ABX\ndiscrimination tasks. ABX is a measure of the separation between generic\ncategories of interest. It has been used extensively to evaluate phonetic\ndiscriminability in self-supervised speech representations. However, its\nbroader adoption has been limited by the absence of adequate tools. fastabx\naddresses this gap by providing a framework capable of constructing any type of\nABX task while delivering the efficiency necessary for rapid development\ncycles, both in task creation and in calculating distances between\nrepresentations. We believe that fastabx will serve as a valuable resource for\nthe broader representation learning community, enabling researchers to\nsystematically investigate what information can be directly extracted from\nlearned representations across several domains beyond speech processing. The\nsource code is available at https://github.com/bootphon/fastabx."}
{"id": "2505.02046", "pdf": "https://arxiv.org/pdf/2505.02046", "abs": "https://arxiv.org/abs/2505.02046", "authors": ["Priyanka Kumari", "Sampriti Soor", "Amba Shetty", "Archana M. Nair"], "title": "A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars", "categories": ["cs.CV"], "comment": null, "summary": "Accurate mineral identification on the Martian surface is critical for\nunderstanding the planet's geological history. This paper presents a UNet-based\nautoencoder model for efficient spectral preprocessing of CRISM MTRDR\nhyperspectral data, addressing the limitations of traditional methods that are\ncomputationally intensive and time-consuming. The proposed model automates key\npreprocessing steps, such as smoothing and continuum removal, while preserving\nessential mineral absorption features. Trained on augmented spectra from the\nMICA spectral library, the model introduces realistic variability to simulate\nMTRDR data conditions. By integrating this framework, preprocessing time for an\n800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA\nT1600 GPU. The preprocessed spectra are subsequently classified using MICAnet,\na deep learning model for Martian mineral identification. Evaluation on labeled\nCRISM TRDR data demonstrates that the proposed approach achieves competitive\naccuracy while significantly enhancing preprocessing efficiency. This work\nhighlights the potential of the UNet-based preprocessing framework to improve\nthe speed and reliability of mineral mapping on Mars."}
{"id": "2505.02763", "pdf": "https://arxiv.org/pdf/2505.02763", "abs": "https://arxiv.org/abs/2505.02763", "authors": ["Matthew Dahl"], "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Legal practice requires careful adherence to procedural rules. In the United\nStates, few are more complex than those found in The Bluebook: A Uniform System\nof Citation. Compliance with this system's 500+ pages of byzantine formatting\ninstructions is the raison d'etre of thousands of student law review editors\nand the bete noire of lawyers everywhere. To evaluate whether large language\nmodels (LLMs) are able to adhere to the procedures of such a complicated\nsystem, we construct an original dataset of 866 Bluebook tasks and test\nflagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1)\nthat these models produce fully compliant Bluebook citations only 69%-74% of\nthe time and (2) that in-context learning on the Bluebook's underlying system\nof rules raises accuracy only to 77%. These results caution against using\noff-the-shelf LLMs to automate aspects of the law where fidelity to procedure\nis paramount."}
{"id": "2505.02048", "pdf": "https://arxiv.org/pdf/2505.02048", "abs": "https://arxiv.org/abs/2505.02048", "authors": ["Sebastian Rassmann", "David KÃ¼gler", "Christian Ewert", "Martin Reuter"], "title": "Regression s all you need for medical image translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The acquisition of information-rich images within a limited time budget is\ncrucial in medical imaging. Medical image translation (MIT) can help enhance\nand supplement existing datasets by generating synthetic images from acquired\ndata. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have\nachieved remarkable success in natural image generation, their benefits -\ncreativity and image realism - do not necessarily transfer to medical\napplications where highly accurate anatomical information is required. In fact,\nthe imitation of acquisition noise or content hallucination hinder clinical\nutility. Here, we introduce YODA (You Only Denoise once - or Average), a novel\n2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and\nregression paradigms to produce realistic or noise-free outputs. Furthermore,\nwe propose Expectation-Approximation (ExpA) DM sampling, which draws\ninspiration from MRI signal averaging. ExpA-sampling suppresses generated noise\nand, thus, eliminates noise from biasing the evaluation of image quality.\nThrough extensive experiments on four diverse multi-modal datasets - comprising\nmulti-contrast brain MRI and pelvic MRI-CT - we show that diffusion and\nregression sampling yield similar results in practice. As such, the\ncomputational overhead of diffusion sampling does not provide systematic\nbenefits in medical information translation. Building on these insights, we\ndemonstrate that YODA outperforms several state-of-the-art GAN and DM methods.\nNotably, YODA-generated images are shown to be interchangeable with, or even\nsuperior to, physical acquisitions for several downstream tasks. Our findings\nchallenge the presumed advantages of DMs in MIT and pave the way for the\npractical application of MIT in medical imaging."}
{"id": "2505.02819", "pdf": "https://arxiv.org/pdf/2505.02819", "abs": "https://arxiv.org/abs/2505.02819", "authors": ["Dmitriy Shopkhoev", "Ammar Ali", "Magauiya Zhussip", "Valentin Malykh", "Stamatios Lefkimmiatis", "Nikos Komodakis", "Sergey Zagoruyko"], "title": "ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations", "categories": ["cs.CL"], "comment": null, "summary": "We introduce ReplaceMe, a generalized training-free depth pruning method that\neffectively replaces transformer blocks with a linear operation, while\nmaintaining high performance for low compression ratios. In contrast to\nconventional pruning approaches that require additional training or\nfine-tuning, our approach requires only a small calibration dataset that is\nused to estimate a linear transformation to approximate the pruned blocks. This\nestimated linear mapping can be seamlessly merged with the remaining\ntransformer blocks, eliminating the need for any additional network parameters.\nOur experiments show that ReplaceMe consistently outperforms other\ntraining-free approaches and remains highly competitive with state-of-the-art\npruning methods that involve extensive retraining/fine-tuning and architectural\nmodifications. Applied to several large language models (LLMs), ReplaceMe\nachieves up to 25% pruning while retaining approximately 90% of the original\nmodel's performance on open benchmarks - without any training or healing steps,\nresulting in minimal computational overhead (see Fig.1). We provide an\nopen-source library implementing ReplaceMe alongside several state-of-the-art\ndepth pruning techniques, available at this repository."}
{"id": "2505.02056", "pdf": "https://arxiv.org/pdf/2505.02056", "abs": "https://arxiv.org/abs/2505.02056", "authors": ["Yuchen Wang", "Xuefeng Bai", "Xiucheng Li", "Weili Guan", "Liqiang Nie", "Xinyang Chen"], "title": "Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Adapting vision-language models (VLMs) to downstream tasks with pseudolabels\nhas gained increasing attention. A major obstacle is that the pseudolabels\ngenerated by VLMs tend to be imbalanced, leading to inferior performance. While\nexisting methods have explored various strategies to address this, the\nunderlying causes of imbalance remain insufficiently investigated. To fill this\ngap, we delve into imbalanced pseudolabels and identify two primary\ncontributing factors: concept mismatch and concept confusion. To mitigate these\ntwo issues, we propose a novel framework incorporating concept alignment and\nconfusion-aware calibrated margin mechanisms. The core of our approach lies in\nenhancing underperforming classes and promoting balanced predictions across\ncategories, thus mitigating imbalance. Extensive experiments on six benchmark\ndatasets with three learning paradigms demonstrate that the proposed method\neffectively enhances the accuracy and balance of pseudolabels, achieving a\nrelative improvement of 6.29% over the SoTA method. Our code is avaliable at\nhttps://anonymous.4open.science/r/CAP-C642/"}
{"id": "2505.01433", "pdf": "https://arxiv.org/pdf/2505.01433", "abs": "https://arxiv.org/abs/2505.01433", "authors": ["Cong Qi", "Hanzhang Fang", "Siqi jiang", "Tianxing Hu", "Wei Zhi"], "title": "Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations", "categories": ["q-bio.QM", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the binding specificity between T-cell receptors (TCRs) and\npeptide-major histocompatibility complexes (pMHCs) is central to immunotherapy\nand vaccine development. However, current predictive models struggle with\ngeneralization, especially in data-scarce settings and when faced with novel\nepitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced\nRecognition Network), a deep learning framework that combines large-scale\nprotein language models with chemical representations of peptides. By encoding\nTCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences\ninto SMILES strings processed by MolFormer, LANTERN captures rich biological\nand chemical features critical for TCR-peptide recognition. Through extensive\nbenchmarking against existing models such as ChemBERTa, TITAN, and NetTCR,\nLANTERN demonstrates superior performance, particularly in zero-shot and\nfew-shot learning scenarios. Our model also benefits from a robust negative\nsampling strategy and shows significant clustering improvements via embedding\nanalysis. These results highlight the potential of LANTERN to advance TCR-pMHC\nbinding prediction and support the development of personalized immunotherapies."}
{"id": "2505.02060", "pdf": "https://arxiv.org/pdf/2505.02060", "abs": "https://arxiv.org/abs/2505.02060", "authors": ["Branko BrkljaÄ", "Vladimir KaluÅ¡ev", "Branislav PopoviÄ", "Milan SeÄujski"], "title": "Transforming faces into video stories -- VideoFace2.0", "categories": ["cs.CV", "68T07, 68T45, 68U10, 94A08, 68T05,", "I.2.10; I.5.4; I.5.5; I.4.8; C.3; J.7"], "comment": "4 pages, 2 figures, 1 algorithm; associated VideoFace2.0 code\n  implementation, test videos and results visualizations are available at\n  https://github.com/brkljac/VideoFace2.0 ; Preprint submitted to the 14th\n  Mediterranean Conference on Embedded Computing (MECO), 10-14 June 2025,\n  Budva, Montenegro", "summary": "Face detection and face recognition have been in the focus of vision\ncommunity since the very beginnings. Inspired by the success of the original\nVideoface digitizer, a pioneering device that allowed users to capture video\nsignals from any source, we have designed an advanced video analytics tool to\nefficiently create structured video stories, i.e. identity-based information\ncatalogs. VideoFace2.0 is the name of the developed system for spatial and\ntemporal localization of each unique face in the input video, i.e. face\nre-identification (ReID), which also allows their cataloging, characterization\nand creation of structured video outputs for later downstream tasks. Developed\nnear real-time solution is primarily designed to be utilized in application\nscenarios involving TV production, media analysis, and as an efficient tool for\ncreating large video datasets necessary for training machine learning (ML)\nmodels in challenging vision tasks such as lip reading and multimodal speech\nrecognition. Conducted experiments confirm applicability of the proposed face\nReID algorithm that is combining the concepts of face detection, face\nrecognition and passive tracking-by-detection in order to achieve robust and\nefficient face ReID. The system is envisioned as a compact and modular\nextensions of the existing video production equipment. We hope that the\npresented work and shared code will stimulate further interest in development\nof similar, application specific video analysis tools, and lower the entry\nbarrier for production of high-quality multi-modal ML datasets in the future."}
{"id": "2505.01435", "pdf": "https://arxiv.org/pdf/2505.01435", "abs": "https://arxiv.org/abs/2505.01435", "authors": ["Carlo Siebenschuh", "Kyle Hippe", "Ozan Gokdemir", "Alexander Brace", "Arham Khan", "Khalid Hossain", "Yadu Babuji", "Nicholas Chia", "Venkatram Vishwanath", "Rick Stevens", "Arvind Ramanathan", "Ian Foster", "Robert Underwood"], "title": "AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine", "categories": ["cs.IR", "cs.CL", "cs.DC", "cs.LG"], "comment": "This paper has been accepted at the The Eighth Annual Conference on\n  Machine Learning and Systems (MLSys 2025)", "summary": "Language models for scientific tasks are trained on text from scientific\npublications, most distributed as PDFs that require parsing. PDF parsing\napproaches range from inexpensive heuristics (for simple documents) to\ncomputationally intensive ML-driven systems (for complex or degraded ones). The\nchoice of the \"best\" parser for a particular document depends on its\ncomputational cost and the accuracy of its output. To address these issues, we\nintroduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine\n(AdaParse), a data-driven strategy for assigning an appropriate parser to each\ndocument. We enlist scientists to select preferred parser outputs and\nincorporate this information through direct preference optimization (DPO) into\nAdaParse, thereby aligning its selection process with human judgment. AdaParse\nthen incorporates hardware requirements and predicted accuracy of each parser\nto orchestrate computational resources efficiently for large-scale parsing\ncampaigns. We demonstrate that AdaParse, when compared to state-of-the-art\nparsers, improves throughput by $17\\times$ while still achieving comparable\naccuracy (0.2 percent better) on a benchmark set of 1000 scientific documents.\nAdaParse's combination of high accuracy and parallel scalability makes it\nfeasible to parse large-scale scientific document corpora to support the\ndevelopment of high-quality, trillion-token-scale text datasets. The\nimplementation is available at https://github.com/7shoe/AdaParse/"}
{"id": "2505.02064", "pdf": "https://arxiv.org/pdf/2505.02064", "abs": "https://arxiv.org/abs/2505.02064", "authors": ["Shuhang Xun", "Sicheng Tao", "Jungang Li", "Yibo Shi", "Zhixin Lin", "Zhanhui Zhu", "Yibo Yan", "Hanqian Li", "Linghao Zhang", "Shikang Wang", "Yixin Liu", "Hanbo Zhang", "Xuming Hu", "Ying Ma"], "title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, 5 tables", "summary": "Multimodal Large Language Models (MLLMs) increasingly excel at perception,\nunderstanding, and reasoning. However, current benchmarks inadequately evaluate\ntheir ability to perform these tasks continuously in dynamic, real-world\nenvironments. To bridge this gap, we introduce RTV-Bench, a fine-grained\nbenchmark for MLLM real-time video analysis. RTV-Bench uses three key\nprinciples: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve\nwith scene changes; (2) Hierarchical Question Structure, combining basic and\nadvanced queries; and (3) Multi-dimensional Evaluation, assessing the ability\nof continuous perception, understanding, and reasoning. RTV-Bench contains 552\ndiverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated\nleading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline\n(Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5,\nInternLM-XComposer2.5-OmniLive) models. Experiment results show open-source\nreal-time models largely outperform offline ones but still trail top\nproprietary models. Our analysis also reveals that larger model size or higher\nframe sampling rates do not significantly boost RTV-Bench performance,\nsometimes causing slight decreases. This underscores the need for better model\narchitectures optimized for video stream processing and long sequences to\nadvance real-time video analysis with MLLMs. Our benchmark toolkit is available\nat: https://github.com/LJungang/RTV-Bench."}
{"id": "2505.01485", "pdf": "https://arxiv.org/pdf/2505.01485", "abs": "https://arxiv.org/abs/2505.01485", "authors": ["Tasnim Ahmed", "Salimur Choudhury"], "title": "CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code", "categories": ["cs.AI", "cs.CL"], "comment": "This paper has been accepted for presentation at the 19th Learning\n  and Intelligent Optimization Conference (LION 19)", "summary": "Linear Programming (LP) problems aim to find the optimal solution to an\nobjective under constraints. These problems typically require domain knowledge,\nmathematical skills, and programming ability, presenting significant challenges\nfor non-experts. This study explores the efficiency of Large Language Models\n(LLMs) in generating solver-specific LP code. We propose CHORUS, a\nretrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP\ncode from natural language problem statements. CHORUS incorporates a\nhierarchical tree-like chunking strategy for theoretical contents and generates\nadditional metadata based on code examples from documentation to facilitate\nself-contained, semantically coherent retrieval. Two-stage retrieval approach\nof CHORUS followed by cross-encoder reranking further ensures contextual\nrelevance. Finally, expertly crafted prompt and structured parser with\nreasoning steps improve code generation performance significantly. Experiments\non the NL4Opt-Code benchmark show that CHORUS improves the performance of\nopen-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1\n(32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and\nconventional RAG. It also allows these open-source LLMs to outperform or match\nthe performance of much stronger baselines-GPT3.5 and GPT4 while requiring far\nfewer computational resources. Ablation studies further demonstrate the\nimportance of expert prompting, hierarchical chunking, and structured\nreasoning."}
{"id": "2505.02071", "pdf": "https://arxiv.org/pdf/2505.02071", "abs": "https://arxiv.org/abs/2505.02071", "authors": ["Can KÃ¼Ã§Ã¼ksÃ¶zen", "YÃ¼cel Yemez"], "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "We propose the Compact Clustering Attention (COCA) layer, an effective\nbuilding block that introduces a hierarchical strategy for object-centric\nrepresentation learning, while solving the unsupervised object discovery task\non single images. COCA is an attention-based clustering module capable of\nextracting object-centric representations from multi-object scenes, when\ncascaded into a bottom-up hierarchical network architecture, referred to as\nCOCA-Net. At its core, COCA utilizes a novel clustering algorithm that\nleverages the physical concept of compactness, to highlight distinct object\ncentroids in a scene, providing a spatial inductive bias. Thanks to this\nstrategy, COCA-Net generates high-quality segmentation masks on both the\ndecoder side and, notably, the encoder side of its pipeline. Additionally,\nCOCA-Net is not bound by a predetermined number of object masks that it\ngenerates and handles the segmentation of background elements better than its\ncompetitors. We demonstrate COCA-Net's segmentation performance on six widely\nadopted datasets, achieving superior or competitive results against the\nstate-of-the-art models across nine different evaluation metrics."}
{"id": "2505.01636", "pdf": "https://arxiv.org/pdf/2505.01636", "abs": "https://arxiv.org/abs/2505.01636", "authors": ["Amit Rath"], "title": "Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; H.2.8; D.2.13"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and task generalization. However, their\napplication to structured data analysis remains fragile due to inconsistencies\nin schema interpretation, misalignment between user intent and model output,\nand limited mechanisms for self-correction when failures occur. This paper\nintroduces the STROT Framework (Structured Task Reasoning and Output\nTransformation), a method for structured prompting and feedback-driven\ntransformation logic generation aimed at improving the reliability and semantic\nalignment of LLM-based analytical workflows. STROT begins with lightweight\nschema introspection and sample-based field classification, enabling dynamic\ncontext construction that captures both the structure and statistical profile\nof the input data. This contextual information is embedded in structured\nprompts that guide the model toward generating task-specific, interpretable\noutputs. To address common failure modes in complex queries, STROT incorporates\na refinement mechanism in which the model iteratively revises its outputs based\non execution feedback and validation signals. Unlike conventional approaches\nthat rely on static prompts or single-shot inference, STROT treats the LLM as a\nreasoning agent embedded within a controlled analysis loop -- capable of\nadjusting its output trajectory through planning and correction. The result is\na robust and reproducible framework for reasoning over structured data with\nLLMs, applicable to diverse data exploration and analysis tasks where\ninterpretability, stability, and correctness are essential."}
{"id": "2505.02075", "pdf": "https://arxiv.org/pdf/2505.02075", "abs": "https://arxiv.org/abs/2505.02075", "authors": ["Volodymyr Havrylov", "Haiwen Huang", "Dan Zhang", "Andreas Geiger"], "title": "Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision Foundation Models (VFMs) are large-scale, pre-trained models that\nserve as general-purpose backbones for various computer vision tasks. As VFMs'\npopularity grows, there is an increasing interest in understanding their\neffectiveness for dense prediction tasks. However, VFMs typically produce\nlow-resolution features, limiting their direct applicability in this context.\nOne way to tackle this limitation is by employing a task-agnostic feature\nupsampling module that refines VFM features resolution. To assess the\neffectiveness of this approach, we investigate Interactive Segmentation (IS) as\na novel benchmark for evaluating feature upsampling methods on VFMs. Due to its\ninherent multimodal input, consisting of an image and a set of user-defined\nclicks, as well as its dense mask output, IS creates a challenging environment\nthat demands comprehensive visual scene understanding. Our benchmarking\nexperiments show that selecting appropriate upsampling strategies significantly\nimproves VFM features quality. The code is released at\nhttps://github.com/havrylovv/iSegProbe"}
{"id": "2505.01706", "pdf": "https://arxiv.org/pdf/2505.01706", "abs": "https://arxiv.org/abs/2505.01706", "authors": ["Sarvesh Shashidhar", "Ritik", "Nachiketa Patil", "Suraj Racha", "Ganesh Ramakrishnan"], "title": "Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Updated abstract, algorithm and experimental results", "summary": "Direct Preference Optimisation (DPO) has emerged as a powerful method for\naligning Large Language Models (LLMs) with human preferences, offering a stable\nand efficient alternative to approaches that use Reinforcement learning via\nHuman Feedback. In this work, we investigate the performance of DPO using\nopen-source preference datasets. One of the major drawbacks of DPO is that it\ndoesn't induce granular scoring and treats all the segments of the responses\nwith equal propensity. However, this is not practically true for human\npreferences since even \"good\" responses have segments that may not be preferred\nby the annotator. To resolve this, a 2-dimensional scoring for DPO alignment\ncalled 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the\nadvantages it provides over the standard DPO by comparing their win rates. It\nis observed that these methods, even though effective, are not robust to\nlabel/score noise. To counter this, we propose an approach of incorporating\nsegment-level score noise robustness to the 2D-DPO algorithm. Along with\ntheoretical backing, we also provide empirical verification in favour of the\nalgorithm and introduce other noise models that can be present."}
{"id": "2505.02079", "pdf": "https://arxiv.org/pdf/2505.02079", "abs": "https://arxiv.org/abs/2505.02079", "authors": ["Maksym Ivashechkin", "Oscar Mendez", "Richard Bowden"], "title": "HandOcc: NeRF-based Hand Rendering with Occupancy Networks", "categories": ["cs.CV"], "comment": null, "summary": "We propose HandOcc, a novel framework for hand rendering based upon\noccupancy. Popular rendering methods such as NeRF are often combined with\nparametric meshes to provide deformable hand models. However, in doing so, such\napproaches present a trade-off between the fidelity of the mesh and the\ncomplexity and dimensionality of the parametric model. The simplicity of\nparametric mesh structures is appealing, but the underlying issue is that it\nbinds methods to mesh initialization, making it unable to generalize to objects\nwhere a parametric model does not exist. It also means that estimation is tied\nto mesh resolution and the accuracy of mesh fitting. This paper presents a\npipeline for meshless 3D rendering, which we apply to the hands. By providing\nonly a 3D skeleton, the desired appearance is extracted via a convolutional\nmodel. We do this by exploiting a NeRF renderer conditioned upon an\noccupancy-based representation. The approach uses the hand occupancy to resolve\nhand-to-hand interactions further improving results, allowing fast rendering,\nand excellent hand appearance transfer. On the benchmark InterHand2.6M dataset,\nwe achieved state-of-the-art results."}
{"id": "2505.01754", "pdf": "https://arxiv.org/pdf/2505.01754", "abs": "https://arxiv.org/abs/2505.01754", "authors": ["Orlando JÃ¤hde", "Thorsten Weber", "RÃ¼diger Buchkremer"], "title": "Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.MA", "68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05,\n  62R07, 68U15, 68T27, 68T20 68T09, 68T50, 68T05, 62R07, 68U15, 68T27, 68T20", "I.2; H.3; I.5; I.7; H.5; H.1"], "comment": null, "summary": "Biased news reporting poses a significant threat to informed decision-making\nand the functioning of democracies. This study introduces a novel methodology\nfor scalable, minimally biased analysis of media bias in political news. The\nproposed approach examines event selection, labeling, word choice, and\ncommission and omission biases across news sources by leveraging natural\nlanguage processing techniques, including hierarchical topic modeling,\nsentiment analysis, and ontology learning with large language models. Through\nthree case studies related to current political events, we demonstrate the\nmethodology's effectiveness in identifying biases across news sources at\nvarious levels of granularity. This work represents a significant step towards\nscalable, minimally biased media bias analysis, laying the groundwork for tools\nto help news consumers navigate an increasingly complex media landscape."}
{"id": "2505.02108", "pdf": "https://arxiv.org/pdf/2505.02108", "abs": "https://arxiv.org/abs/2505.02108", "authors": ["Maksym Ivashechkin", "Oscar Mendez", "Richard Bowden"], "title": "SignSplat: Rendering Sign Language via Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art approaches for conditional human body rendering via Gaussian\nsplatting typically focus on simple body motions captured from many views. This\nis often in the context of dancing or walking. However, for more complex use\ncases, such as sign language, we care less about large body motion and more\nabout subtle and complex motions of the hands and face. The problems of\nbuilding high fidelity models are compounded by the complexity of capturing\nmulti-view data of sign. The solution is to make better use of sequence data,\nensuring that we can overcome the limited information from only a few views by\nexploiting temporal variability. Nevertheless, learning from sequence-level\ndata requires extremely accurate and consistent model fitting to ensure that\nappearance is consistent across complex motions. We focus on how to achieve\nthis, constraining mesh parameters to build an accurate Gaussian splatting\nframework from few views capable of modelling subtle human motion. We leverage\nregularization techniques on the Gaussian parameters to mitigate overfitting\nand rendering artifacts. Additionally, we propose a new adaptive control method\nto densify Gaussians and prune splat points on the mesh surface. To demonstrate\nthe accuracy of our approach, we render novel sequences of sign language video,\nbuilding on neural machine translation approaches to sign stitching. On\nbenchmark datasets, our approach achieves state-of-the-art performance; and on\nhighly articulated and complex sign language motion, we significantly\noutperform competing approaches."}
{"id": "2505.01790", "pdf": "https://arxiv.org/pdf/2505.01790", "abs": "https://arxiv.org/abs/2505.01790", "authors": ["Markos Stamatakis", "Joshua Berger", "Christian Wartena", "Ralph Ewerth", "Anett Hoppe"], "title": "Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "12 pages (excluding references), 8 tables, 1 equation", "summary": "Web-based educational videos offer flexible learning opportunities and are\nbecoming increasingly popular. However, improving user engagement and knowledge\nretention remains a challenge. Automatically generated questions can activate\nlearners and support their knowledge acquisition. Further, they can help\nteachers and learners assess their understanding. While large language and\nvision-language models have been employed in various tasks, their application\nto question generation for educational videos remains underexplored. In this\npaper, we investigate the capabilities of current vision-language models for\ngenerating learning-oriented questions for educational video content. We assess\n(1) out-of-the-box models' performance; (2) fine-tuning effects on\ncontent-specific question generation; (3) the impact of different video\nmodalities on question quality; and (4) in a qualitative study, question\nrelevance, answerability, and difficulty levels of generated questions. Our\nfindings delineate the capabilities of current vision-language models,\nhighlighting the need for fine-tuning and addressing challenges in question\ndiversity and relevance. We identify requirements for future multimodal\ndatasets and outline promising research directions."}
{"id": "2505.02109", "pdf": "https://arxiv.org/pdf/2505.02109", "abs": "https://arxiv.org/abs/2505.02109", "authors": ["Yingkai Zhang", "Zeqiang Lai", "Tao Zhang", "Ying Fu", "Chenghu Zhou"], "title": "Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral images super-resolution aims to improve the spatial resolution,\nyet its performance is often limited at high-resolution ratios. The recent\nadoption of high-resolution reference images for super-resolution is driven by\nthe poor spatial detail found in low-resolution HSIs, presenting it as a\nfavorable method. However, these approaches cannot effectively utilize\ninformation from the reference image, due to the inaccuracy of alignment and\nits inadequate interaction between alignment and fusion modules. In this paper,\nwe introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution\n(SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the\nissues of inaccurate alignment and poor interactivity of the previous\napproaches. Specifically, to ensure spatial concordance, i.e., align images\nmore accurately across resolutions and refine textures, we construct a\nTwo-Stage Image Alignment with a synthetic generation pipeline in the image\nalignment module, where the fine-tuned optical flow model can produce a more\naccurate optical flow in the first stage and warp model can refine damaged\ntextures in the second stage. To enhance the interaction between alignment and\nfusion modules and ensure spectral concordance during reconstruction, we\npropose a Feature Aggregation module and an Attention Fusion module. In the\nfeature aggregation module, we introduce an Iterative Deformable Feature\nAggregation block to achieve significant feature matching and texture\naggregation with the fusion multi-scale results guidance, iteratively\ngenerating learnable offset. Besides, we introduce two basic spectral-wise\nattention blocks in the attention fusion module to model the inter-spectra\ninteractions. Extensive experiments on three natural or remote-sensing datasets\nshow that our method outperforms state-of-the-art approaches on both\nquantitative and qualitative evaluations."}
{"id": "2505.01944", "pdf": "https://arxiv.org/pdf/2505.01944", "abs": "https://arxiv.org/abs/2505.01944", "authors": ["Matteo Cristani", "Guido Governatori", "Francesco Olivieri", "Monica Palmirani", "Gabriele Buriola"], "title": "Explainability by design: an experimental analysis of the legal coding process", "categories": ["cs.LO", "cs.AI", "cs.CL"], "comment": null, "summary": "Behind a set of rules in Deontic Defeasible Logic, there is a mapping process\nof normative background fragments. This process goes from text to rules and\nimplicitly encompasses an explanation of the coded fragments.\n  In this paper we deliver a methodology for \\textit{legal coding} that starts\nwith a fragment and goes onto a set of Deontic Defeasible Logic rules,\ninvolving a set of \\textit{scenarios} to test the correctness of the coded\nfragments. The methodology is illustrated by the coding process of an example\ntext. We then show the results of a series of experiments conducted with humans\nencoding a variety of normative backgrounds and corresponding cases in which we\nhave measured the efforts made in the coding process, as related to some\nmeasurable features. To process these examples, a recently developed\ntechnology, Houdini, that allows reasoning in Deontic Defeasible Logic, has\nbeen employed.\n  Finally we provide a technique to forecast time required in coding, that\ndepends on factors such as knowledge of the legal domain, knowledge of the\ncoding processes, length of the text, and a measure of \\textit{depth} that\nrefers to the length of the paths of legal references."}
{"id": "2505.02126", "pdf": "https://arxiv.org/pdf/2505.02126", "abs": "https://arxiv.org/abs/2505.02126", "authors": ["Zhihao Tang", "Shenghao Yang", "Hongtao Zhang", "Mingbo Zhao"], "title": "GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Traditional 3D garment creation requires extensive manual operations,\nresulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved\nbreakthrough progress in 3D scene reconstruction and rendering, attracting\nwidespread attention and opening new pathways for 3D garment reconstruction.\nHowever, due to the unstructured and irregular nature of Gaussian primitives,\nit is difficult to reconstruct high-fidelity, non-watertight 3D garments. In\nthis paper, we present GarmentGS, a dense point cloud-guided method that can\nreconstruct high-fidelity garment surfaces with high geometric accuracy and\ngenerate non-watertight, single-layer meshes. Our method introduces a fast\ndense point cloud reconstruction module that can complete garment point cloud\nreconstruction in 10 minutes, compared to traditional methods that require\nseveral hours. Furthermore, we use dense point clouds to guide the movement,\nflattening, and rotation of Gaussian primitives, enabling better distribution\non the garment surface to achieve superior rendering effects and geometric\naccuracy. Through numerical and visual comparisons, our method achieves fast\ntraining and real-time rendering while maintaining competitive quality."}
{"id": "2505.01958", "pdf": "https://arxiv.org/pdf/2505.01958", "abs": "https://arxiv.org/abs/2505.01958", "authors": ["Liqiang Jing", "Guiming Hardy Chen", "Ehsan Aghazadeh", "Xin Eric Wang", "Xinya Du"], "title": "A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in\nmultimodal tasks, but visual object hallucination remains a persistent issue.\nIt refers to scenarios where models generate inaccurate visual object-related\ninformation based on the query input, potentially leading to misinformation and\nconcerns about safety and reliability. Previous works focus on the evaluation\nand mitigation of visual hallucinations, but the underlying causes have not\nbeen comprehensively investigated. In this paper, we analyze each component of\nLLaVA-like LVLMs -- the large language model, the vision backbone, and the\nprojector -- to identify potential sources of error and their impact. Based on\nour observations, we propose methods to mitigate hallucination for each\nproblematic component. Additionally, we developed two hallucination benchmarks:\nQA-VisualGenome, which emphasizes attribute and relation hallucinations, and\nQA-FB15k, which focuses on cognition-based hallucinations."}
{"id": "2505.02134", "pdf": "https://arxiv.org/pdf/2505.02134", "abs": "https://arxiv.org/abs/2505.02134", "authors": ["Xiaorui Zhao", "Xinyue Zhou", "Peibei Cao", "Junyu Lou", "Shuhang Gu"], "title": "HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Developing effective approaches to generate enhanced results that align well\nwith human visual preferences for high-quality well-lit images remains a\nchallenge in low-light image enhancement (LLIE). In this paper, we propose a\nhuman-in-the-loop LLIE training framework that improves the visual quality of\nunsupervised LLIE model outputs through iterative training stages, named\nHiLLIE. At each stage, we introduce human guidance into the training process\nthrough efficient visual quality annotations of enhanced outputs. Subsequently,\nwe employ a tailored image quality assessment (IQA) model to learn human visual\npreferences encoded in the acquired labels, which is then utilized to guide the\ntraining process of an enhancement model. With only a small amount of pairwise\nranking annotations required at each stage, our approach continually improves\nthe IQA model's capability to simulate human visual assessment of enhanced\noutputs, thus leading to visually appealing LLIE results. Extensive experiments\ndemonstrate that our approach significantly improves unsupervised LLIE model\nperformance in terms of both quantitative and qualitative performance. The code\nand collected ranking dataset will be available at\nhttps://github.com/LabShuHangGU/HiLLIE."}
{"id": "2505.02130", "pdf": "https://arxiv.org/pdf/2505.02130", "abs": "https://arxiv.org/abs/2505.02130", "authors": ["Zhong Guan", "Likang Wu", "Hongke Zhao", "Ming He", "Jianpin Fan"], "title": "Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data", "categories": ["cs.AI", "cs.CL"], "comment": "ICML2025 Accept", "summary": "Attention mechanisms are critical to the success of large language models\n(LLMs), driving significant advancements in multiple fields. However, for\ngraph-structured data, which requires emphasis on topological connections, they\nfall short compared to message-passing mechanisms on fixed links, such as those\nemployed by Graph Neural Networks (GNNs). This raises a question: ``Does\nattention fail for graphs in natural language settings?'' Motivated by these\nobservations, we embarked on an empirical study from the perspective of\nattention mechanisms to explore how LLMs process graph-structured data. The\ngoal is to gain deeper insights into the attention behavior of LLMs over graph\nstructures. We uncovered unique phenomena regarding how LLMs apply attention to\ngraph-structured data and analyzed these findings to improve the modeling of\nsuch data by LLMs. The primary findings of our research are: 1) While LLMs can\nrecognize graph data and capture text-node interactions, they struggle to model\ninter-node relationships within graph structures due to inherent architectural\nconstraints. 2) The attention distribution of LLMs across graph nodes does not\nalign with ideal structural patterns, indicating a failure to adapt to graph\ntopology nuances. 3) Neither fully connected attention nor fixed connectivity\nis optimal; each has specific limitations in its application scenarios.\nInstead, intermediate-state attention windows improve LLM training performance\nand seamlessly transition to fully connected windows during inference. Source\ncode: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}"}
{"id": "2505.02148", "pdf": "https://arxiv.org/pdf/2505.02148", "abs": "https://arxiv.org/abs/2505.02148", "authors": ["Alexey Nekrasov", "Malcolm Burdorf", "Stewart Worrall", "Bastian Leibe", "Julie Stephany Berrio Perez"], "title": "Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted for publication at CVPR 2025. Project page:\n  https://www.vision.rwth-aachen.de/stu-dataset", "summary": "To operate safely, autonomous vehicles (AVs) need to detect and handle\nunexpected objects or anomalies on the road. While significant research exists\nfor anomaly detection and segmentation in 2D, research progress in 3D is\nunderexplored. Existing datasets lack high-quality multimodal data that are\ntypically found in AVs. This paper presents a novel dataset for anomaly\nsegmentation in driving scenarios. To the best of our knowledge, it is the\nfirst publicly available dataset focused on road anomaly segmentation with\ndense 3D semantic labeling, incorporating both LiDAR and camera data, as well\nas sequential information to enable anomaly detection across various ranges.\nThis capability is critical for the safe navigation of autonomous vehicles. We\nadapted and evaluated several baseline models for 3D segmentation, highlighting\nthe challenges of 3D anomaly detection in driving environments. Our dataset and\nevaluation code will be openly available, facilitating the testing and\nperformance comparison of different approaches."}
{"id": "2505.02199", "pdf": "https://arxiv.org/pdf/2505.02199", "abs": "https://arxiv.org/abs/2505.02199", "authors": ["Manak Raj", "Nidhi Mishra"], "title": "Exploring new Approaches for Information Retrieval through Natural Language Processing", "categories": ["cs.IR", "cs.CL", "68T50", "H.3.3; I.2.7"], "comment": "12 pages, 4 figures, comprehensive literature review covering six key\n  IR-NLP papers, plus keywords and full reference list", "summary": "This review paper explores recent advancements and emerging approaches in\nInformation Retrieval (IR) applied to Natural Language Processing (NLP). We\nexamine traditional IR models such as Boolean, vector space, probabilistic, and\ninference network models, and highlight modern techniques including deep\nlearning, reinforcement learning, and pretrained transformer models like BERT.\nWe discuss key tools and libraries - Lucene, Anserini, and Pyserini - for\nefficient text indexing and search. A comparative analysis of sparse, dense,\nand hybrid retrieval methods is presented, along with applications in web\nsearch engines, cross-language IR, argument mining, private information\nretrieval, and hate speech detection. Finally, we identify open challenges and\nfuture research directions to enhance retrieval accuracy, scalability, and\nethical considerations."}
{"id": "2505.02159", "pdf": "https://arxiv.org/pdf/2505.02159", "abs": "https://arxiv.org/abs/2505.02159", "authors": ["Xingyu Zhou", "Wei Long", "Jingbo Lu", "Shiyin Jiang", "Weiyi You", "Haifeng Wu", "Shuhang Gu"], "title": "Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution", "categories": ["cs.CV"], "comment": "15 pages, 11 figures", "summary": "Video super-resolution (VSR) can achieve better performance compared to\nsingle image super-resolution by additionally leveraging temporal information.\nIn particular, the recurrent-based VSR model exploits long-range temporal\ninformation during inference and achieves superior detail restoration. However,\neffectively learning these long-term dependencies within long videos remains a\nkey challenge. To address this, we propose LRTI-VSR, a novel training framework\nfor recurrent VSR that efficiently leverages Long-Range Refocused Temporal\nInformation. Our framework includes a generic training strategy that utilizes\ntemporal propagation features from long video clips while training on shorter\nvideo clips. Additionally, we introduce a refocused intra&inter-frame\ntransformer block which allows the VSR model to selectively prioritize useful\ntemporal information through its attention module while further improving\ninter-frame information utilization in the FFN module. We evaluate LRTI-VSR on\nboth CNN and transformer-based VSR architectures, conducting extensive ablation\nstudies to validate the contribution of each component. Experiments on\nlong-video test sets demonstrate that LRTI-VSR achieves state-of-the-art\nperformance while maintaining training and computational efficiency."}
{"id": "2505.02206", "pdf": "https://arxiv.org/pdf/2505.02206", "abs": "https://arxiv.org/abs/2505.02206", "authors": ["Lei Mao", "Yuanhe Tian", "Yan Song"], "title": "DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 pages, 3 figures", "summary": "Genome modeling conventionally treats gene sequence as a language, reflecting\nits structured motifs and long-range dependencies analogous to linguistic units\nand organization principles such as words and syntax. Recent studies utilize\nadvanced neural networks, ranging from convolutional and recurrent models to\nTransformer-based models, to capture contextual information of gene sequence,\nwith the primary goal of obtaining effective gene sequence representations and\nthus enhance the models' understanding of various running gene samples.\nHowever, these approaches often directly apply language modeling techniques to\ngene sequences and do not fully consider the intrinsic information organization\nin them, where they do not consider how units at different granularities\ncontribute to representation. In this paper, we propose DNAZEN, an enhanced\ngenomic representation framework designed to learn from various granularities\nin gene sequences, including small polymers and G-grams that are combinations\nof several contiguous polymers. Specifically, we extract the G-grams from\nlarge-scale genomic corpora through an unsupervised approach to construct the\nG-gram vocabulary, which is used to provide G-grams in the learning process of\nDNA sequences through dynamically matching from running gene samples. A\nTransformer-based G-gram encoder is also proposed and the matched G-grams are\nfed into it to compute their representations and integrated into the encoder\nfor basic unit (E4BU), which is responsible for encoding small units and\nmaintaining the learning and inference process. To further enhance the learning\nprocess, we propose whole G-gram masking to train DNAZEN, where the model\nlargely favors the selection of each entire G-gram to mask rather than an\nordinary masking mechanism performed on basic units. Experiments on benchmark\ndatasets demonstrate the effectiveness of DNAZEN on various downstream tasks."}
{"id": "2505.02161", "pdf": "https://arxiv.org/pdf/2505.02161", "abs": "https://arxiv.org/abs/2505.02161", "authors": ["Dongyue Li"], "title": "Focus What Matters: Matchability-Based Reweighting for Local Feature Matching", "categories": ["cs.CV"], "comment": null, "summary": "Since the rise of Transformers, many semi-dense matching methods have adopted\nattention mechanisms to extract feature descriptors. However, the attention\nweights, which capture dependencies between pixels or keypoints, are often\nlearned from scratch. This approach can introduce redundancy and noisy\ninteractions from irrelevant regions, as it treats all pixels or keypoints\nequally. Drawing inspiration from keypoint selection processes, we propose to\nfirst classify all pixels into two categories: matchable and non-matchable.\nMatchable pixels are expected to receive higher attention weights, while\nnon-matchable ones are down-weighted. In this work, we propose a novel\nattention reweighting mechanism that simultaneously incorporates a learnable\nbias term into the attention logits and applies a matchability-informed\nrescaling to the input value features. The bias term, injected prior to the\nsoftmax operation, selectively adjusts attention scores based on the confidence\nof query-key interactions. Concurrently, the feature rescaling acts\npost-attention by modulating the influence of each value vector in the final\noutput. This dual design allows the attention mechanism to dynamically adjust\nboth its internal weighting scheme and the magnitude of its output\nrepresentations. Extensive experiments conducted on three benchmark datasets\nvalidate the effectiveness of our method, consistently outperforming existing\nstate-of-the-art approaches."}
{"id": "2505.02215", "pdf": "https://arxiv.org/pdf/2505.02215", "abs": "https://arxiv.org/abs/2505.02215", "authors": ["Mannan Bhardwaj"], "title": "Interpretable Emergent Language Using Inter-Agent Transformers", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This paper explores the emergence of language in multi-agent reinforcement\nlearning (MARL) using transformers. Existing methods such as RIAL, DIAL, and\nCommNet enable agent communication but lack interpretability. We propose\nDifferentiable Inter-Agent Transformers (DIAT), which leverage self-attention\nto learn symbolic, human-understandable communication protocols. Through\nexperiments, DIAT demonstrates the ability to encode observations into\ninterpretable vocabularies and meaningful embeddings, effectively solving\ncooperative tasks. These results highlight the potential of DIAT for\ninterpretable communication in complex multi-agent environments."}
{"id": "2505.02175", "pdf": "https://arxiv.org/pdf/2505.02175", "abs": "https://arxiv.org/abs/2505.02175", "authors": ["Shubhendu Jena", "Shishir Reddy Vutukur", "Adnane Boukhayma"], "title": "SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Project page : https://shubhendu-jena.github.io/SparSplat/", "summary": "Recovering 3D information from scenes via multi-view stereo reconstruction\n(MVS) and novel view synthesis (NVS) is inherently challenging, particularly in\nscenarios involving sparse-view setups. The advent of 3D Gaussian Splatting\n(3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian\nSplatting (2DGS) leveraged perspective accurate 2D Gaussian primitive\nrasterization to achieve accurate geometry representation during rendering,\nimproving 3D scene reconstruction while maintaining real-time performance.\nRecent approaches have tackled the problem of sparse real-time NVS using 3DGS\nwithin a generalizable, MVS-based learning framework to regress 3D Gaussian\nparameters. Our work extends this line of research by addressing the challenge\nof generalizable sparse 3D reconstruction and NVS jointly, and manages to\nperform successfully at both tasks. We propose an MVS-based learning pipeline\nthat regresses 2DGS surface element parameters in a feed-forward fashion to\nperform 3D shape reconstruction and NVS from sparse-view images. We further\nshow that our generalizable pipeline can benefit from preexisting foundational\nmulti-view deep visual features. The resulting model attains the\nstate-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms\nof Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also\ndemonstrates strong generalization on the BlendedMVS and Tanks and Temples\ndatasets. We note that our model outperforms the prior state-of-the-art in\nfeed-forward sparse view reconstruction based on volume rendering of implicit\nrepresentations, while offering an almost 2 orders of magnitude higher\ninference speed."}
{"id": "2505.02309", "pdf": "https://arxiv.org/pdf/2505.02309", "abs": "https://arxiv.org/abs/2505.02309", "authors": ["Sanjay Surendranath Girija", "Shashank Kapoor", "Lakshit Arora", "Dipen Pradhan", "Aman Raj", "Ankit Shetgaonkar"], "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE COMPSAC 2025", "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial\nintelligence (AI), but their substantial resource requirements limit their\ndeployment on mobile and edge devices. This survey paper provides a\ncomprehensive overview of techniques for compressing LLMs to enable efficient\ninference in resource-constrained environments. We examine three primary\napproaches: Knowledge Distillation, Model Quantization, and Model Pruning. For\neach technique, we discuss the underlying principles, present different\nvariants, and provide examples of successful applications. We also briefly\ndiscuss complementary techniques such as mixture-of-experts and early-exit\nstrategies. Finally, we highlight promising future directions, aiming to\nprovide a valuable resource for both researchers and practitioners seeking to\noptimize LLMs for edge deployment."}
{"id": "2505.02176", "pdf": "https://arxiv.org/pdf/2505.02176", "abs": "https://arxiv.org/abs/2505.02176", "authors": ["Samuel Webster", "Adam Czajka"], "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection", "categories": ["cs.CV"], "comment": "19 pages (8 main, 2 references, 9 appendix), 2 figures, 19 tables (2\n  main, 17 appendix)", "summary": "Saliency-guided training, which directs model learning to important regions\nof images, has demonstrated generalization improvements across various\nbiometric presentation attack detection (PAD) tasks. This paper presents its\nfirst application to fingerprint PAD. We conducted a 50-participant study to\ncreate a dataset of 800 human-annotated fingerprint perceptually-important\nmaps, explored alongside algorithmically-generated \"pseudosaliency,\" including\nminutiae-based, image quality-based, and autoencoder-based saliency maps.\nEvaluating on the 2021 Fingerprint Liveness Detection Competition testing set,\nwe explore various configurations within five distinct training scenarios to\nassess the impact of saliency-guided training on accuracy and generalization.\nOur findings demonstrate the effectiveness of saliency-guided training for\nfingerprint PAD in both limited and large data contexts, and we present a\nconfiguration capable of earning the first place on the LivDet-2021 benchmark.\nOur results highlight saliency-guided training's promise for increased model\ngeneralization capabilities, its effectiveness when data is limited, and its\npotential to scale to larger datasets in fingerprint PAD. All collected\nsaliency data and trained models are released with the paper to support\nreproducible research."}
{"id": "2505.02391", "pdf": "https://arxiv.org/pdf/2505.02391", "abs": "https://arxiv.org/abs/2505.02391", "authors": ["Jiarui Yao", "Yifan Hao", "Hanning Zhang", "Hanze Dong", "Wei Xiong", "Nan Jiang", "Tong Zhang"], "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning in large language models (LLMs) can be\nformalized as a latent variable problem, where the model needs to generate\nintermediate reasoning steps. While prior approaches such as iterative\nreward-ranked fine-tuning (RAFT) have relied on such formulations, they\ntypically apply uniform inference budgets across prompts, which fails to\naccount for variability in difficulty and convergence behavior. This work\nidentifies the main bottleneck in CoT training as inefficient stochastic\ngradient estimation due to static sampling strategies. We propose GVM-RAFT, a\nprompt-specific Dynamic Sample Allocation Strategy designed to minimize\nstochastic gradient variance under a computational budget constraint. The\nmethod dynamically allocates computational resources by monitoring prompt\nacceptance rates and stochastic gradient norms, ensuring that the resulting\ngradient variance is minimized. Our theoretical analysis shows that the\nproposed dynamic sampling strategy leads to accelerated convergence guarantees\nunder suitable conditions. Experiments on mathematical reasoning show that\nGVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over\nvanilla RAFT. The proposed dynamic sampling strategy is general and can be\nincorporated into other reinforcement learning algorithms, such as GRPO,\nleading to similar improvements in convergence and test accuracy. Our code is\navailable at https://github.com/RLHFlow/GVM."}
{"id": "2505.02178", "pdf": "https://arxiv.org/pdf/2505.02178", "abs": "https://arxiv.org/abs/2505.02178", "authors": ["Shubhendu Jena", "Amine Ouasfi", "Mae Younes", "Adnane Boukhayma"], "title": "Sparfels: Fast Reconstruction from Sparse Unposed Imagery", "categories": ["cs.CV"], "comment": "Project page : https://shubhendu-jena.github.io/Sparfels/", "summary": "We present a method for Sparse view reconstruction with surface element\nsplatting that runs within 3 minutes on a consumer grade GPU. While few methods\naddress sparse radiance field learning from noisy or unposed sparse cameras,\nshape recovery remains relatively underexplored in this setting. Several\nradiance and shape learning test-time optimization methods address the sparse\nposed setting by learning data priors or using combinations of external\nmonocular geometry priors. Differently, we propose an efficient and simple\npipeline harnessing a single recent 3D foundation model. We leverage its\nvarious task heads, notably point maps and camera initializations to\ninstantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image\ncorrespondences to guide camera optimization midst 2DGS training. Key to our\ncontribution is a novel formulation of splatted color variance along rays,\nwhich can be computed efficiently. Reducing this moment in training leads to\nmore accurate shape reconstructions. We demonstrate state-of-the-art\nperformances in the sparse uncalibrated setting in reconstruction and novel\nview benchmarks based on established multi-view datasets."}
{"id": "2505.02462", "pdf": "https://arxiv.org/pdf/2505.02462", "abs": "https://arxiv.org/abs/2505.02462", "authors": ["Enpei Zhang", "Jingyi Chai", "Rui Ye", "Yanfeng Wang", "Siheng Chen"], "title": "Incentivizing Inclusive Contributions in Model Sharing Markets", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "While data plays a crucial role in training contemporary AI models, it is\nacknowledged that valuable public data will be exhausted in a few years,\ndirecting the world's attention towards the massive decentralized private data.\nHowever, the privacy-sensitive nature of raw data and lack of incentive\nmechanism prevent these valuable data from being fully exploited. Addressing\nthese challenges, this paper proposes inclusive and incentivized personalized\nfederated learning (iPFL), which incentivizes data holders with diverse\npurposes to collaboratively train personalized models without revealing raw\ndata. iPFL constructs a model-sharing market by solving a graph-based training\noptimization and incorporates an incentive mechanism based on game theory\nprinciples. Theoretical analysis shows that iPFL adheres to two key incentive\nproperties: individual rationality and truthfulness. Empirical studies on\neleven AI tasks (e.g., large language models' instruction-following tasks)\ndemonstrate that iPFL consistently achieves the highest economic utility, and\nbetter or comparable model performance compared to baseline methods. We\nanticipate that our iPFL can serve as a valuable technique for boosting future\nAI models on decentralized private data while making everyone satisfied."}
{"id": "2505.02179", "pdf": "https://arxiv.org/pdf/2505.02179", "abs": "https://arxiv.org/abs/2505.02179", "authors": ["Tao Zhu", "Qi Yu", "Xinru Dong", "Shiyu Li", "Yue Liu", "Jinlong Jiang", "Lei Shu"], "title": "ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications", "categories": ["cs.CV"], "comment": null, "summary": "Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance\nLearning (MIL) suffers from label ambiguity, hindering discriminative feature\nlearning. We propose ProDisc-VAD, an efficient framework tackling this via two\nsynergistic components. The Prototype Interaction Layer (PIL) provides\ncontrolled normality modeling using a small set of learnable prototypes,\nestablishing a robust baseline without being overwhelmed by dominant normal\ndata. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts\nseparability by applying targeted contrastive learning exclusively to the most\nreliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD\nachieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M\nparameters, over 800x fewer than recent ViT-based methods like VadCLIP,\ndemonstrating exceptional efficiency alongside state-of-the-art performance.\nCode is available at https://github.com/modadundun/ProDisc-VAD."}
{"id": "2505.02550", "pdf": "https://arxiv.org/pdf/2505.02550", "abs": "https://arxiv.org/abs/2505.02550", "authors": ["Krzysztof Ociepa", "Åukasz Flis", "Remigiusz Kinas", "Krzysztof WrÃ³bel", "Adrian GwoÅºdziej"], "title": "Bielik v3 Small: Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "We introduce Bielik v3, a series of parameter-efficient generative text\nmodels (1.5B and 4.5B) optimized for Polish language processing. These models\ndemonstrate that smaller, well-optimized architectures can achieve performance\ncomparable to much larger counterparts while requiring substantially fewer\ncomputational resources. Our approach incorporates several key innovations: a\ncustom Polish tokenizer (APT4) that significantly improves token efficiency,\nWeighted Instruction Cross-Entropy Loss to balance learning across instruction\ntypes, and Adaptive Learning Rate that dynamically adjusts based on training\nprogress. Trained on a meticulously curated corpus of 292 billion tokens\nspanning 303 million documents, these models excel across multiple benchmarks,\nincluding the Open PL LLM Leaderboard, Complex Polish Text Understanding\nBenchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter\nmodel achieves results competitive with models 2-3 times its size, while the\n1.5B model delivers strong performance despite its extremely compact profile.\nThese advances establish new benchmarks for parameter-efficient language\nmodeling in less-represented languages, making high-quality Polish language AI\nmore accessible for resource-constrained applications."}
{"id": "2505.02182", "pdf": "https://arxiv.org/pdf/2505.02182", "abs": "https://arxiv.org/abs/2505.02182", "authors": ["Yamini Sri Krubha", "Aryana Hou", "Braden Vester", "Web Walker", "Xin Wang", "Li Lin", "Shu Hu"], "title": "Robust AI-Generated Face Detection with Imbalanced Data", "categories": ["cs.CV"], "comment": null, "summary": "Deepfakes, created using advanced AI techniques such as Variational\nAutoencoder and Generative Adversarial Networks, have evolved from research and\nentertainment applications into tools for malicious activities, posing\nsignificant threats to digital trust. Current deepfake detection techniques\nhave evolved from CNN-based methods focused on local artifacts to more advanced\napproaches using vision transformers and multimodal models like CLIP, which\ncapture global anomalies and improve cross-domain generalization. Despite\nrecent progress, state-of-the-art deepfake detectors still face major\nchallenges in handling distribution shifts from emerging generative models and\naddressing severe class imbalance between authentic and fake samples in\ndeepfake datasets, which limits their robustness and detection accuracy. To\naddress these challenges, we propose a framework that combines dynamic loss\nreweighting and ranking-based optimization, which achieves superior\ngeneralization and performance under imbalanced dataset conditions. The code is\navailable at https://github.com/Purdue-M2/SP_CUP."}
{"id": "2505.02639", "pdf": "https://arxiv.org/pdf/2505.02639", "abs": "https://arxiv.org/abs/2505.02639", "authors": ["Xuan Lin", "Qingrui Liu", "Hongxin Xiang", "Daojian Zeng", "Xiangxiang Zeng"], "title": "Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Chemical reaction and retrosynthesis prediction are fundamental tasks in drug\ndiscovery. Recently, large language models (LLMs) have shown potential in many\ndomains. However, directly applying LLMs to these tasks faces two major\nchallenges: (i) lacking a large-scale chemical synthesis-related instruction\ndataset; (ii) ignoring the close correlation between reaction and\nretrosynthesis prediction for the existing fine-tuning strategies. To address\nthese challenges, we propose ChemDual, a novel LLM framework for accurate\nchemical synthesis. Specifically, considering the high cost of data acquisition\nfor reaction and retrosynthesis, ChemDual regards the\nreaction-and-retrosynthesis of molecules as a related\nrecombination-and-fragmentation process and constructs a large-scale of 4.4\nmillion instruction dataset. Furthermore, ChemDual introduces an enhanced\nLLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy,\nto jointly optimize the process of recombination and fragmentation as well as\nthe tasks between reaction and retrosynthesis prediction. Extensive experiments\non Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves\nstate-of-the-art performance in both predictions of reaction and\nretrosynthesis, outperforming the existing conventional single-task approaches\nand the general open-source LLMs. Through molecular docking analysis, ChemDual\ngenerates compounds with diverse and strong protein binding affinity, further\nhighlighting its strong potential in drug design."}
{"id": "2505.02192", "pdf": "https://arxiv.org/pdf/2505.02192", "abs": "https://arxiv.org/abs/2505.02192", "authors": ["Wenchuan Wang", "Mengqi Huang", "Yijing Tu", "Zhendong Mao"], "title": "DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Customized text-to-video generation with pre-trained large-scale models has\nrecently garnered significant attention through focusing on identity and motion\nconsistency. Existing works typically follow the isolated customized paradigm,\nwhere the subject identity or motion dynamics are customized exclusively.\nHowever, this paradigm completely ignores the intrinsic mutual constraints and\nsynergistic interdependencies between identity and motion, resulting in\nidentity-motion conflicts throughout the generation process that systematically\ndegrades. To address this, we introduce DualReal, a novel framework that,\nemploys adaptive joint training to collaboratively construct interdependencies\nbetween dimensions. Specifically, DualReal is composed of two units: (1)\nDual-aware Adaptation dynamically selects a training phase (i.e., identity or\nmotion), learns the current information guided by the frozen dimension prior,\nand employs a regularization strategy to avoid knowledge leakage; (2)\nStageBlender Controller leverages the denoising stages and Diffusion\nTransformer depths to guide different dimensions with adaptive granularity,\navoiding conflicts at various stages and ultimately achieving lossless fusion\nof identity and motion patterns. We constructed a more comprehensive benchmark\nthan existing methods. The experimental results show that DualReal improves\nCLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top\nperformance on nearly all motion quality metrics."}
{"id": "2505.02693", "pdf": "https://arxiv.org/pdf/2505.02693", "abs": "https://arxiv.org/abs/2505.02693", "authors": ["Shaghayegh Agah", "Yejin Kim", "Neeraj Sharma", "Mayur Nankani", "Kevin Foley", "H. Howie Huang", "Sardar Hamidian"], "title": "Predicting Movie Hits Before They Happen with LLMs", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted at ACM UMAP 2025 Industry Track", "summary": "Addressing the cold-start issue in content recommendation remains a critical\nongoing challenge. In this work, we focus on tackling the cold-start problem\nfor movies on a large entertainment platform. Our primary goal is to forecast\nthe popularity of cold-start movies using Large Language Models (LLMs)\nleveraging movie metadata. This method could be integrated into retrieval\nsystems within the personalization pipeline or could be adopted as a tool for\neditorial teams to ensure fair promotion of potentially overlooked movies that\nmay be missed by traditional or algorithmic solutions. Our study validates the\neffectiveness of this approach compared to established baselines and those we\ndeveloped."}
{"id": "2505.02236", "pdf": "https://arxiv.org/pdf/2505.02236", "abs": "https://arxiv.org/abs/2505.02236", "authors": ["Tianle Chen", "Chaitanya Chakka", "Deepti Ghadiyaram"], "title": "Improving Physical Object State Representation in Text-to-Image Generative Systems", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to Synthetic Data for Computer Vision - CVPR 2025 Workshop", "summary": "Current text-to-image generative models struggle to accurately represent\nobject states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this\nwork, we first design a fully-automatic pipeline to generate high-quality\nsynthetic data that accurately captures objects in varied states. Next, we\nfine-tune several open-source text-to-image models on this synthetic data. We\nevaluate the performance of the fine-tuned models by quantifying the alignment\nof the generated images to their prompts using GPT4o-mini, and achieve an\naverage absolute improvement of 8+% across four models on the public\nGenAI-Bench dataset. We also curate a collection of 200 prompts with a specific\nfocus on common objects in various physical states. We demonstrate a\nsignificant improvement of an average of 24+% over the baseline on this\ndataset. We release all evaluation prompts and code."}
{"id": "2505.02707", "pdf": "https://arxiv.org/pdf/2505.02707", "abs": "https://arxiv.org/abs/2505.02707", "authors": ["Yemin Shi", "Yu Shu", "Siwei Dong", "Guangyi Liu", "Jaward Sesay", "Jingwen Li", "Zhiting Hu"], "title": "Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play", "categories": ["cs.AI", "cs.CL", "cs.SD"], "comment": "18 pages, 7 figures, Website: https://voila.maitrix.org", "summary": "A voice AI agent that blends seamlessly into daily life would interact with\nhumans in an autonomous, real-time, and emotionally expressive manner. Rather\nthan merely reacting to commands, it would continuously listen, reason, and\nrespond proactively, fostering fluid, dynamic, and emotionally resonant\ninteractions. We introduce Voila, a family of large voice-language foundation\nmodels that make a step towards this vision. Voila moves beyond traditional\npipeline systems by adopting a new end-to-end architecture that enables\nfull-duplex, low-latency conversations while preserving rich vocal nuances such\nas tone, rhythm, and emotion. It achieves a response latency of just 195\nmilliseconds, surpassing the average human response time. Its hierarchical\nmulti-scale Transformer integrates the reasoning capabilities of large language\nmodels (LLMs) with powerful acoustic modeling, enabling natural, persona-aware\nvoice generation -- where users can simply write text instructions to define\nthe speaker's identity, tone, and other characteristics. Moreover, Voila\nsupports over one million pre-built voices and efficient customization of new\nones from brief audio samples as short as 10 seconds. Beyond spoken dialogue,\nVoila is designed as a unified model for a wide range of voice-based\napplications, including automatic speech recognition (ASR), Text-to-Speech\n(TTS), and, with minimal adaptation, multilingual speech translation. Voila is\nfully open-sourced to support open research and accelerate progress toward\nnext-generation human-machine interactions."}
{"id": "2505.02242", "pdf": "https://arxiv.org/pdf/2505.02242", "abs": "https://arxiv.org/abs/2505.02242", "authors": ["Qian Zeng", "Jie Song", "Yuanyu Wan", "Huiqiong Wang", "Mingli Song"], "title": "Quantizing Diffusion Models from a Sampling-Aware Perspective", "categories": ["cs.CV"], "comment": "11 pages, 4 figures", "summary": "Diffusion models have recently emerged as the dominant approach in visual\ngeneration tasks. However, the lengthy denoising chains and the computationally\nintensive noise estimation networks hinder their applicability in low-latency\nand resource-limited environments. Previous research has endeavored to address\nthese limitations in a decoupled manner, utilizing either advanced samplers or\nefficient model quantization techniques. In this study, we uncover that\nquantization-induced noise disrupts directional estimation at each sampling\nstep, further distorting the precise directional estimations of higher-order\nsamplers when solving the sampling equations through discretized numerical\nmethods, thereby altering the optimal sampling trajectory. To attain dual\nacceleration with high fidelity, we propose a sampling-aware quantization\nstrategy, wherein a Mixed-Order Trajectory Alignment technique is devised to\nimpose a more stringent constraint on the error bounds at each sampling step,\nfacilitating a more linear probability flow. Extensive experiments on\nsparse-step fast sampling across multiple datasets demonstrate that our\napproach preserves the rapid convergence characteristics of high-speed samplers\nwhile maintaining superior generation quality. Code will be made publicly\navailable soon."}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746", "abs": "https://arxiv.org/abs/2505.02746", "authors": ["Simon Ging", "Sebastian Walter", "Jelena BratuliÄ", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time."}
{"id": "2505.02246", "pdf": "https://arxiv.org/pdf/2505.02246", "abs": "https://arxiv.org/abs/2505.02246", "authors": ["Shree K. Nayar", "Jeremy Klotz", "Nikhil Nanda", "Mikhail Fridberg"], "title": "Cricket: A Self-Powered Chirping Pixel", "categories": ["cs.CV"], "comment": "13 pages, 18 figures. Project page:\n  https://cave.cs.columbia.edu/projects/categories/project?cid=Computational%20Imaging&pid=Cricket%20A%20Self-Powered%20Chirping%20Pixel", "summary": "We present a sensor that can measure light and wirelessly communicate the\nmeasurement, without the need for an external power source or a battery. Our\nsensor, called cricket, harvests energy from incident light. It is asleep for\nmost of the time and transmits a short and strong radio frequency chirp when\nits harvested energy reaches a specific level. The carrier frequency of each\ncricket is fixed and reveals its identity, and the duration between consecutive\nchirps is a measure of the incident light level. We have characterized the\nradiometric response function, signal-to-noise ratio and dynamic range of\ncricket. We have experimentally verified that cricket can be miniaturized at\nthe expense of increasing the duration between chirps. We show that a cube with\na cricket on each of its sides can be used to estimate the centroid of any\ncomplex illumination, which has value in applications such as solar tracking.\nWe also demonstrate the use of crickets for creating untethered sensor arrays\nthat can produce video and control lighting for energy conservation. Finally,\nwe modified cricket's circuit to develop battery-free electronic sunglasses\nthat can instantly adapt to environmental illumination."}
{"id": "2505.02811", "pdf": "https://arxiv.org/pdf/2505.02811", "abs": "https://arxiv.org/abs/2505.02811", "authors": ["Diji Yang", "Linda Zeng", "Jinmeng Rao", "Yi Zhang"], "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR 2025", "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance.\n  This paper aims to address these limitations by introducing a new framework,\n\\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and\nmulti-round retrieval capabilities. To train SIM-RAG, we first let a RAG system\nself-practice multi-round retrieval, augmenting existing question-answer pairs\nwith intermediate inner monologue reasoning steps to generate synthetic\ntraining data. For each pair, the system may explore multiple retrieval paths,\nwhich are labeled as successful if they reach the correct answer and\nunsuccessful otherwise. Using this data, we train a lightweight information\nsufficiency Critic. At inference time, the Critic evaluates whether the RAG\nsystem has retrieved sufficient information at each round, guiding retrieval\ndecisions and improving system-level self-awareness through in-context\nreinforcement learning.\n  Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an\neffective multi-round RAG solution. Furthermore, this framework is\nsystem-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."}
{"id": "2505.02255", "pdf": "https://arxiv.org/pdf/2505.02255", "abs": "https://arxiv.org/abs/2505.02255", "authors": ["Jakub WÄsala", "BartÅomiej Wrzalski", "Kornelia Noculak", "Yuliia Tarasenko", "Oliwer Krupa", "Jan KocoÅ", "Grzegorz Chodak"], "title": "Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "25th International Conference on Computational Science", "summary": "This study presents a novel approach to enhance the cost-to-quality ratio of\nimage generation with diffusion models. We hypothesize that differences between\ndistilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are\nconsistent and, therefore, learnable within a specialized domain, like portrait\ngeneration. We generate a synthetic paired dataset and train a fast\nimage-to-image translation head. Using two sets of low- and high-quality\nsynthetic images, our model is trained to refine the output of a distilled\ngenerator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like\nFLUX.1-dev, which is more computationally intensive. Our results show that the\npipeline, which combines a distilled version of a large generative model with\nour enhancement layer, delivers similar photorealistic portraits to the\nbaseline version with up to an 82% decrease in computational cost compared to\nFLUX.1-dev. This study demonstrates the potential for improving the efficiency\nof AI solutions involving large-scale image generation."}
{"id": "2505.02820", "pdf": "https://arxiv.org/pdf/2505.02820", "abs": "https://arxiv.org/abs/2505.02820", "authors": ["Hao Zhu", "Phil Cuvin", "Xinkai Yu", "Charlotte Ka Yee Yan", "Jason Zhang", "Diyi Yang"], "title": "AutoLibra: Agent Metric Induction from Open-Ended Feedback", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "https://opensocial.world/", "summary": "Agents are predominantly evaluated and optimized via task success metrics,\nwhich are coarse, rely on manual design from experts, and fail to reward\nintermediate emergent behaviors. We propose AutoLibra, a framework for agent\nevaluation, that transforms open-ended human feedback, e.g., \"If you find that\nthe button is disabled, don't click it again\", or \"This agent has too much\nautonomy to decide what to do on its own\", into metrics for evaluating\nfine-grained behaviors in agent trajectories. AutoLibra accomplishes this by\ngrounding feedback to an agent's behavior, clustering similar positive and\nnegative behaviors, and creating concrete metrics with clear definitions and\nconcrete examples, which can be used for prompting LLM-as-a-Judge as\nevaluators. We further propose two meta-metrics to evaluate the alignment of a\nset of (induced) metrics with open feedback: \"coverage\" and \"redundancy\".\nThrough optimizing these meta-metrics, we experimentally demonstrate\nAutoLibra's ability to induce more concrete agent evaluation metrics than the\nones proposed in previous agent evaluation benchmarks and discover new metrics\nto analyze agents. We also present two applications of AutoLibra in agent\nimprovement: First, we show that AutoLibra-induced metrics serve as better\nprompt-engineering targets than the task success rate on a wide range of text\ngame tasks, improving agent performance over baseline by a mean of 20%. Second,\nwe show that AutoLibra can iteratively select high-quality fine-tuning data for\nweb navigation agents. Our results suggest that AutoLibra is a powerful\ntask-agnostic tool for evaluating and improving language agents."}
{"id": "2505.02278", "pdf": "https://arxiv.org/pdf/2505.02278", "abs": "https://arxiv.org/abs/2505.02278", "authors": ["Madhukar Reddy Vongala", "Saurabh Srivastava", "Jana KoÅ¡eckÃ¡"], "title": "Compositional Image-Text Matching and Retrieval by Grounding Entities", "categories": ["cs.CV"], "comment": "Accepted at CVPR-W", "summary": "Vision-language pretraining on large datasets of images-text pairs is one of\nthe main building blocks of current Vision-Language Models. While with\nadditional training, these models excel in various downstream tasks, including\nvisual question answering, image captioning, and visual commonsense reasoning.\nHowever, a notable weakness of pretrained models like CLIP, is their inability\nto perform entity grounding and compositional image and text\nmatching~\\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR,\nlearninglocalizeCVPR24}. In this work we propose a novel learning-free\nzero-shot augmentation of CLIP embeddings that has favorable compositional\nproperties. We compute separate embeddings of sub-images of object entities and\nrelations that are localized by the state of the art open vocabulary detectors\nand dynamically adjust the baseline global image embedding. % The final\nembedding is obtained by computing a weighted combination of the sub-image\nembeddings. The resulting embedding is then utilized for similarity computation\nwith text embedding, resulting in a average 1.5\\% improvement in image-text\nmatching accuracy on the Visual Genome and SVO Probes\ndatasets~\\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings\ndemonstrate superior retrieval performance, thus achieving significant gains on\nthe Flickr30K and MS-COCO retrieval benchmarks~\\cite{flickr30ke, mscoco},\nimproving the state-of-the-art Recall@1 by 12\\% and 0.4\\%, respectively. Our\ncode is available at https://github.com/madhukarreddyvongala/GroundingCLIP."}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830", "abs": "https://arxiv.org/abs/2505.02830", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks."}
{"id": "2505.02287", "pdf": "https://arxiv.org/pdf/2505.02287", "abs": "https://arxiv.org/abs/2505.02287", "authors": ["Shipeng Liu", "Ziliang Xiong", "Bastian Wandt", "Per-Erik ForssÃ©n"], "title": "Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by SCIA2025", "summary": "Human Pose Estimation (HPE) is increasingly important for applications like\nvirtual reality and motion analysis, yet current methods struggle with\nbalancing accuracy, computational efficiency, and reliable uncertainty\nquantification (UQ). Traditional regression-based methods assume fixed\ndistributions, which might lead to poor UQ. Heatmap-based methods effectively\nmodel the output distribution using likelihood heatmaps, however, they demand\nsignificant resources. To address this, we propose Continuous Flow Residual\nEstimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into\nregression-based models, which allows for dynamic distribution adaptation.\nThrough extensive experiments, we show that CFRE leads to better accuracy and\nuncertainty quantification with retained computational efficiency on both 2D\nand 3D human pose estimation tasks."}
{"id": "2505.02835", "pdf": "https://arxiv.org/pdf/2505.02835", "abs": "https://arxiv.org/abs/2505.02835", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Xiao Hu", "Chaoyou Fu", "Bin Wen", "Tianke Zhang", "Changyi Liu", "Kaiyu Jiang", "Kaibing Chen", "Kaiyu Tang", "Haojie Ding", "Jiankang Chen", "Fan Yang", "Zhang Zhang", "Tingting Gao", "Liang Wang"], "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Home page: https://github.com/yfzhang114/r1_reward", "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs."}
{"id": "2505.02325", "pdf": "https://arxiv.org/pdf/2505.02325", "abs": "https://arxiv.org/abs/2505.02325", "authors": ["Zhichuan Wang", "Yang Zhou", "Jinhai Xiang", "Yulong Wang", "Xinwei He"], "title": "TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment", "categories": ["cs.CV"], "comment": "Accepted by ICMR 2025", "summary": "Learning discriminative 3D representations that generalize well to unknown\ntesting categories is an emerging requirement for many real-world 3D\napplications. Existing well-established methods often struggle to attain this\ngoal due to insufficient 3D training data from broader concepts. Meanwhile,\npre-trained large vision-language models (e.g., CLIP) have shown remarkable\nzero-shot generalization capabilities. Yet, they are limited in extracting\nsuitable 3D representations due to substantial gaps between their 2D training\nand 3D testing distributions. To address these challenges, we propose\nTesting-time Distribution Alignment (TeDA), a novel framework that adapts a\npretrained 2D vision-language model CLIP for unknown 3D object retrieval at\ntest time. To our knowledge, it is the first work that studies the test-time\nadaptation of a vision-language model for 3D feature learning. TeDA projects 3D\nobjects into multi-view images, extracts features using CLIP, and refines 3D\nquery embeddings with an iterative optimization strategy by confident\nquery-target sample pairs in a self-boosting manner. Additionally, TeDA\nintegrates textual descriptions generated by a multimodal language model\n(InternVL) to enhance 3D object understanding, leveraging CLIP's aligned\nfeature space to fuse visual and textual cues. Extensive experiments on four\nopen-set 3D object retrieval benchmarks demonstrate that TeDA greatly\noutperforms state-of-the-art methods, even those requiring extensive training.\nWe also experimented with depth maps on Objaverse-LVIS, further validating its\neffectiveness. Code is available at https://github.com/wangzhichuan123/TeDA."}
{"id": "2505.02331", "pdf": "https://arxiv.org/pdf/2505.02331", "abs": "https://arxiv.org/abs/2505.02331", "authors": ["Hao Cheng", "Zhiwei Zhao", "Yichao He", "Zhenzhen Hu", "Jia Li", "Meng Wang", "Richang Hong"], "title": "VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection", "categories": ["cs.CV", "cs.SD"], "comment": "Source code and pre-trained models will be available at\n  https://github.com/MSA-LMC/VAEmo", "summary": "Audiovisual emotion recognition (AVER) aims to infer human emotions from\nnonverbal visual-audio (VA) cues, offering modality-complementary and\nlanguage-agnostic advantages. However, AVER remains challenging due to the\ninherent ambiguity of emotional expressions, cross-modal expressive\ndisparities, and the scarcity of reliably annotated data. Recent\nself-supervised AVER approaches have introduced strong multimodal\nrepresentations, yet they predominantly rely on modality-specific encoders and\ncoarse content-level alignment, limiting fine-grained emotional semantic\nmodeling. To address these issues, we propose VAEmo, an efficient two-stage\nframework for emotion-centric joint VA representation learning with external\nknowledge injection. In Stage 1, a unified and lightweight representation\nnetwork is pre-trained on large-scale speaker-centric VA corpora via masked\nreconstruction and contrastive objectives, mitigating the modality gap and\nlearning expressive, complementary representations without emotion labels. In\nStage 2, multimodal large language models automatically generate detailed\naffective descriptions according to our well-designed chain-of-thought\nprompting for only a small subset of VA samples; these rich textual semantics\nare then injected by aligning their corresponding embeddings with VA\nrepresentations through dual-path contrastive learning, further bridging the\nemotion gap. Extensive experiments on multiple downstream AVER benchmarks show\nthat VAEmo achieves state-of-the-art performance with a compact design,\nhighlighting the benefit of unified cross-modal encoding and emotion-aware\nsemantic guidance for efficient, generalizable VA emotion representations."}
{"id": "2505.02335", "pdf": "https://arxiv.org/pdf/2505.02335", "abs": "https://arxiv.org/abs/2505.02335", "authors": ["Kevin Tan", "Fan Yang", "Yuhao Chen"], "title": "6D Pose Estimation on Spoons and Hands", "categories": ["cs.CV"], "comment": null, "summary": "Accurate dietary monitoring is essential for promoting healthier eating\nhabits. A key area of research is how people interact and consume food using\nutensils and hands. By tracking their position and orientation, it is possible\nto estimate the volume of food being consumed, or monitor eating behaviours,\nhighly useful insights into nutritional intake that can be more reliable than\npopular methods such as self-reporting. Hence, this paper implements a system\nthat analyzes stationary video feed of people eating, using 6D pose estimation\nto track hand and spoon movements to capture spatial position and orientation.\nIn doing so, we examine the performance of two state-of-the-art (SOTA) video\nobject segmentation (VOS) models, both quantitatively and qualitatively, and\nidentify main sources of error within the system."}
{"id": "2505.02364", "pdf": "https://arxiv.org/pdf/2505.02364", "abs": "https://arxiv.org/abs/2505.02364", "authors": ["Weihua Yang", "Yicong Zhou"], "title": "Quaternion Infrared Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Visible images provide rich details and color information only under\nwell-lighted conditions while infrared images effectively highlight thermal\ntargets under challenging conditions such as low visibility and adverse\nweather. Infrared-visible image fusion aims to integrate complementary\ninformation from infrared and visible images to generate a high-quality fused\nimage. Existing methods exhibit critical limitations such as neglecting color\nstructure information in visible images and performance degradation when\nprocessing low-quality color-visible inputs. To address these issues, we\npropose a quaternion infrared-visible image fusion (QIVIF) framework to\ngenerate high-quality fused images completely in the quaternion domain. QIVIF\nproposes a quaternion low-visibility feature learning model to adaptively\nextract salient thermal targets and fine-grained texture details from input\ninfrared and visible images respectively under diverse degraded conditions.\nQIVIF then develops a quaternion adaptive unsharp masking method to adaptively\nimprove high-frequency feature enhancement with balanced illumination. QIVIF\nfurther proposes a quaternion hierarchical Bayesian fusion model to integrate\ninfrared saliency and enhanced visible details to obtain high-quality fused\nimages. Extensive experiments across diverse datasets demonstrate that our\nQIVIF surpasses state-of-the-art methods under challenging low-visibility\nconditions."}
{"id": "2505.02365", "pdf": "https://arxiv.org/pdf/2505.02365", "abs": "https://arxiv.org/abs/2505.02365", "authors": ["Weihua Yang", "Yicong Zhou"], "title": "Quaternion Multi-focus Color Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-focus color image fusion refers to integrating multiple partially\nfocused color images to create a single all-in-focus color image. However,\nexisting methods struggle with complex real-world scenarios due to limitations\nin handling color information and intricate textures. To address these\nchallenges, this paper proposes a quaternion multi-focus color image fusion\nframework to perform high-quality color image fusion completely in the\nquaternion domain. This framework introduces 1) a quaternion sparse\ndecomposition model to jointly learn fine-scale image details and structure\ninformation of color images in an iterative fashion for high-precision focus\ndetection, 2) a quaternion base-detail fusion strategy to individually fuse\nbase-scale and detail-scale results across multiple color images for preserving\nstructure and detail information, and 3) a quaternion structural similarity\nrefinement strategy to adaptively select optimal patches from initial fusion\nresults and obtain the final fused result for preserving fine details and\nensuring spatially consistent outputs. Extensive experiments demonstrate that\nthe proposed framework outperforms state-of-the-art methods."}
{"id": "2505.02370", "pdf": "https://arxiv.org/pdf/2505.02370", "abs": "https://arxiv.org/abs/2505.02370", "authors": ["Ming Li", "Xin Gu", "Fan Chen", "Xiaoying Xing", "Longyin Wen", "Chen Chen", "Sijie Zhu"], "title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code, Data and Models are available at:\n  https://github.com/bytedance/SuperEdit", "summary": "Due to the challenges of manually collecting accurate editing data, existing\ndatasets are typically constructed using various automated methods, leading to\nnoisy supervision signals caused by the mismatch between editing instructions\nand original-edited image pairs. Recent efforts attempt to improve editing\nmodels through generating higher-quality edited images, pre-training on\nrecognition tasks, or introducing vision-language models (VLMs) but fail to\nresolve this fundamental issue. In this paper, we offer a novel solution by\nconstructing more effective editing instructions for given image pairs. This\nincludes rectifying the editing instructions to better align with the\noriginal-edited image pairs and using contrastive editing instructions to\nfurther enhance their effectiveness. Specifically, we find that editing models\nexhibit specific generation attributes at different inference steps,\nindependent of the text. Based on these prior attributes, we define a unified\nguide for VLMs to rectify editing instructions. However, there are some\nchallenging editing scenarios that cannot be resolved solely with rectified\ninstructions. To this end, we further construct contrastive supervision signals\nwith positive and negative instructions and introduce them into the model\ntraining using triplet loss, thereby further facilitating supervision\neffectiveness. Our method does not require the VLM modules or pre-training\ntasks used in previous work, offering a more direct and efficient way to\nprovide better supervision signals, and providing a novel, simple, and\neffective solution for instruction-based image editing. Results on multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches. Compared with previous SOTA SmartEdit, we achieve 9.19%\nimprovements on the Real-Edit benchmark with 30x less training data and 13x\nsmaller model size."}
{"id": "2505.02388", "pdf": "https://arxiv.org/pdf/2505.02388", "abs": "https://arxiv.org/abs/2505.02388", "authors": ["Huangyue Yu", "Baoxiong Jia", "Yixin Chen", "Yandan Yang", "Puhao Li", "Rongpeng Su", "Jiaxin Li", "Qing Li", "Wei Liang", "Song-Chun Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025", "summary": "Embodied AI (EAI) research requires high-quality, diverse 3D scenes to\neffectively support skill acquisition, sim-to-real transfer, and\ngeneralization. Achieving these quality standards, however, necessitates the\nprecise replication of real-world object diversity. Existing datasets\ndemonstrate that this process heavily relies on artist-driven designs, which\ndemand substantial human effort and present significant scalability challenges.\nTo scalably produce realistic and interactive 3D scenes, we first present\nMetaScenes, a large-scale, simulatable 3D scene dataset constructed from\nreal-world scans, which includes 15366 objects spanning 831 fine-grained\ncategories. Then, we introduce Scan2Sim, a robust multi-modal alignment model,\nwhich enables the automated, high-quality replacement of assets, thereby\neliminating the reliance on artist-driven designs for scaling 3D scenes. We\nfurther propose two benchmarks to evaluate MetaScenes: a detailed scene\nsynthesis task focused on small item layouts for robotic manipulation and a\ndomain transfer task in vision-and-language navigation (VLN) to validate\ncross-domain transfer. Results confirm MetaScene's potential to enhance EAI by\nsupporting more generalizable agent learning and sim-to-real applications,\nintroducing new possibilities for EAI research. Project website:\nhttps://meta-scenes.github.io/."}
{"id": "2505.02393", "pdf": "https://arxiv.org/pdf/2505.02393", "abs": "https://arxiv.org/abs/2505.02393", "authors": ["Sungheon Jeong", "Jihong Park", "Mohsen Imani"], "title": "Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Most existing video anomaly detectors rely solely on RGB frames, which lack\nthe temporal resolution needed to capture abrupt or transient motion cues, key\nindicators of anomalous events. To address this limitation, we propose\nImage-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that\nsynthesizes event representations directly from RGB videos and fuses them with\nimage features through a principled, uncertainty-aware process. The system (i)\nmodels heavy-tailed sensor noise with a Student`s-t likelihood, deriving\nvalue-level inverse-variance weights via a Laplace approximation; (ii) applies\nKalman-style frame-wise updates to balance modalities over time; and (iii)\niteratively refines the fused latent state to erase residual cross-modal noise.\nWithout any dedicated event sensor or frame-level labels, IEF-VAD sets a new\nstate of the art across multiple real-world anomaly detection benchmarks. These\nfindings highlight the utility of synthetic event representations in\nemphasizing motion cues that are often underrepresented in RGB frames, enabling\naccurate and robust video understanding across diverse applications without\nrequiring dedicated event sensors. Code and models are available at\nhttps://github.com/EavnJeong/IEF-VAD."}
{"id": "2505.02406", "pdf": "https://arxiv.org/pdf/2505.02406", "abs": "https://arxiv.org/abs/2505.02406", "authors": ["Zichen Liu", "Xu Zou", "Gang Hua", "Jiahuan Zhou"], "title": "Token Coordinated Prompt Attention is Needed for Visual Prompting", "categories": ["cs.CV"], "comment": null, "summary": "Visual prompting techniques are widely used to efficiently fine-tune\npretrained Vision Transformers (ViT) by learning a small set of shared prompts\nfor all tokens. However, existing methods overlook the unique roles of\ndifferent tokens in conveying discriminative information and interact with all\ntokens using the same prompts, thereby limiting the representational capacity\nof ViT. This often leads to indistinguishable and biased prompt-extracted\nfeatures, hindering performance. To address this issue, we propose a\nplug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns\nspecific coordinated prompts to different tokens for attention-based\ninteractions. Firstly, recognizing the distinct functions of CLS and image\ntokens-global information aggregation and local feature extraction, we\ndisentangle the prompts into CLS Prompts and Image Prompts, which interact\nexclusively with CLS tokens and image tokens through attention mechanisms. This\nenhances their respective discriminative abilities. Furthermore, as different\nimage tokens correspond to distinct image patches and contain diverse\ninformation, we employ a matching function to automatically assign coordinated\nprompts to individual tokens. This enables more precise attention interactions,\nimproving the diversity and representational capacity of the extracted\nfeatures. Extensive experiments across various benchmarks demonstrate that TCPA\nsignificantly enhances the diversity and discriminative power of the extracted\nfeatures. The code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-TCPA."}
{"id": "2505.02448", "pdf": "https://arxiv.org/pdf/2505.02448", "abs": "https://arxiv.org/abs/2505.02448", "authors": ["Chaohua Li", "Enhao Zhang", "Chuanxing Geng", "Songcan Chen"], "title": "Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-distribution detection (OOD) is a pivotal task for real-world\napplications that trains models to identify samples that are distributionally\ndifferent from the in-distribution (ID) data during testing. Recent advances in\nAI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized\nOOD detection by shifting from traditional unimodal image detectors to\nmultimodal image-text detectors. This shift has inspired extensive research;\nhowever, existing categorization schemes (e.g., few- or zero-shot types) still\nrely solely on the availability of ID images, adhering to a unimodal paradigm.\nTo better align with CLIP's cross-modal nature, we propose a new categorization\nframework rooted in both image and text modalities. Specifically, we categorize\nexisting methods based on how visual and textual information of OOD data is\nutilized within image + text modalities, and further divide them into four\ngroups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e.,\nlearnable vectors or class names) Known or Unknown, across two training\nstrategies (i.e., train-free or training-required). More importantly, we\ndiscuss open problems in CLIP-like OOD detection and highlight promising\ndirections for future research, including cross-domain integration, practical\napplications, and theoretical understanding."}
{"id": "2505.02467", "pdf": "https://arxiv.org/pdf/2505.02467", "abs": "https://arxiv.org/abs/2505.02467", "authors": ["Valerio Guarrasi", "Klara Mogensen", "Sara Tassinari", "Sara Qvarlander", "Paolo Soda"], "title": "Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal deep learning harnesses diverse imaging modalities, such as MRI\nsequences, to enhance diagnostic accuracy in medical imaging. A key challenge\nis determining the optimal timing for integrating these\nmodalities-specifically, identifying the network layers where fusion modules\nshould be inserted. Current approaches often rely on manual tuning or\nexhaustive search, which are computationally expensive without any guarantee of\nconverging to optimal results. We propose a sequential forward search algorithm\nthat incrementally activates and evaluates candidate fusion modules at\ndifferent layers of a multimodal network. At each step, the algorithm retrains\nfrom previously learned weights and compares validation loss to identify the\nbest-performing configuration. This process systematically reduces the search\nspace, enabling efficient identification of the optimal fusion timing without\nexhaustively testing all possible module placements. The approach is validated\non two multimodal MRI datasets, each addressing different classification tasks.\nOur algorithm consistently identified configurations that outperformed unimodal\nbaselines, late fusion, and a brute-force ensemble of all potential fusion\nplacements. These architectures demonstrated superior accuracy, F-score, and\nspecificity while maintaining competitive or improved AUC values. Furthermore,\nthe sequential nature of the search significantly reduced computational\noverhead, making the optimization process more practical. By systematically\ndetermining the optimal timing to fuse imaging modalities, our method advances\nmultimodal deep learning for medical imaging. It provides an efficient and\nrobust framework for fusion optimization, paving the way for improved clinical\ndecision-making and more adaptable, scalable architectures in medical AI\napplications."}
{"id": "2505.02471", "pdf": "https://arxiv.org/pdf/2505.02471", "abs": "https://arxiv.org/abs/2505.02471", "authors": ["Biao Gong", "Cheng Zou", "Dandan Zheng", "Hu Yu", "Jingdong Chen", "Jianxin Sun", "Junbo Zhao", "Jun Zhou", "Kaixiang Ji", "Lixiang Ru", "Libin Wang", "Qingpei Guo", "Rui Liu", "Weilong Chai", "Xinyu Xiao", "Ziyuan Huang"], "title": "Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction", "categories": ["cs.CV"], "comment": "https://github.com/inclusionAI/Ming/tree/main/Ming-unify", "summary": "We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a\nnewly designed unified visual generator and a native multimodal autoregressive\nmodel tailored for unifying vision and language. Specifically, this project\nprovides an open-source implementation of the integrated MetaQueries and\nM2-omni framework, while introducing the novel multi-scale learnable tokens and\nmulti-scale representation alignment strategy. By leveraging a fixed MLLM and a\nlearnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to\nperform both text-to-image generation and instruction based image editing\ntasks, expanding their capabilities beyond pure visual understanding. Our\nexperimental results demonstrate the strong performance of Ming-Lite-Uni and\nillustrate the impressive fluid nature of its interactive process. All code and\nmodel weights are open-sourced to foster further exploration within the\ncommunity. Notably, this work aligns with concurrent multimodal AI milestones -\nsuch as ChatGPT-4o with native image generation updated in March 25, 2025 -\nunderscoring the broader significance of unified models like Ming-Lite-Uni on\nthe path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further\nrefined."}
{"id": "2505.02481", "pdf": "https://arxiv.org/pdf/2505.02481", "abs": "https://arxiv.org/abs/2505.02481", "authors": ["Xiongjun Guan", "Zhiyu Pan", "Jianjiang Feng", "Jie Zhou"], "title": "Finger Pose Estimation for Under-screen Fingerprint Sensor", "categories": ["cs.CV"], "comment": null, "summary": "Two-dimensional pose estimation plays a crucial role in fingerprint\nrecognition by facilitating global alignment and reduce pose-induced\nvariations. However, existing methods are still unsatisfactory when handling\nwith large angle or small area inputs. These limitations are particularly\npronounced on fingerprints captured by under-screen fingerprint sensors in\nsmartphones. In this paper, we present a novel dual-modal input based network\nfor under-screen fingerprint pose estimation. Our approach effectively\nintegrates two distinct yet complementary modalities: texture details extracted\nfrom ridge patches through the under-screen fingerprint sensor, and rough\ncontours derived from capacitive images obtained via the touch screen. This\ncollaborative integration endows our network with more comprehensive and\ndiscriminative information, substantially improving the accuracy and stability\nof pose estimation. A decoupled probability distribution prediction task is\ndesigned, instead of the traditional supervised forms of numerical regression\nor heatmap voting, to facilitate the training process. Additionally, we\nincorporate a Mixture of Experts (MoE) based feature fusion mechanism and a\nrelationship driven cross-domain knowledge transfer strategy to further\nstrengthen feature extraction and fusion capabilities. Extensive experiments\nare conducted on several public datasets and two private datasets. The results\nindicate that our method is significantly superior to previous state-of-the-art\n(SOTA) methods and remarkably boosts the recognition ability of fingerprint\nrecognition algorithms. Our code is available at\nhttps://github.com/XiongjunGuan/DRACO."}
{"id": "2505.02501", "pdf": "https://arxiv.org/pdf/2505.02501", "abs": "https://arxiv.org/abs/2505.02501", "authors": ["Asma Brazi", "Boris Meden", "Fabrice Mayran de Chamisso", "Steve Bourgeois", "Vincent Lepetit"], "title": "Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages, 5 figures", "summary": "We introduce Corr2Distrib, the first correspondence-based method which\nestimates a 6D camera pose distribution from an RGB image, explaining the\nobservations. Indeed, symmetries and occlusions introduce visual ambiguities,\nleading to multiple valid poses. While a few recent methods tackle this\nproblem, they do not rely on local correspondences which, according to the BOP\nChallenge, are currently the most effective way to estimate a single 6DoF pose\nsolution. Using correspondences to estimate a pose distribution is not\nstraightforward, since ambiguous correspondences induced by visual ambiguities\ndrastically decrease the performance of PnP. With Corr2Distrib, we turn these\nambiguities into an advantage to recover all valid poses. Corr2Distrib first\nlearns a symmetry-aware representation for each 3D point on the object's\nsurface, characterized by a descriptor and a local frame. This representation\nenables the generation of 3DoF rotation hypotheses from single 2D-3D\ncorrespondences. Next, we refine these hypotheses into a 6DoF pose distribution\nusing PnP and pose scoring. Our experimental evaluations on complex\nnon-synthetic scenes show that Corr2Distrib outperforms state-of-the-art\nsolutions for both pose distribution estimation and single pose estimation from\nan RGB image, demonstrating the potential of correspondences-based approaches."}
{"id": "2505.02527", "pdf": "https://arxiv.org/pdf/2505.02527", "abs": "https://arxiv.org/abs/2505.02527", "authors": ["Pengfei Yang", "Ngai-Man Cheung", "Xinda Ma"], "title": "Text to Image Generation and Editing: A Survey", "categories": ["cs.CV"], "comment": "49 pages,3 figures,3 tables", "summary": "Text-to-image generation (T2I) refers to the text-guided generation of\nhigh-quality images. In the past few years, T2I has attracted widespread\nattention and numerous works have emerged. In this survey, we comprehensively\nreview 141 works conducted from 2021 to 2024. First, we introduce four\nfoundation model architectures of T2I (autoregression, non-autoregression, GAN\nand diffusion) and the commonly used key technologies (autoencoder, attention\nand classifier-free guidance). Secondly, we systematically compare the methods\nof these studies in two directions, T2I generation and T2I editing, including\nthe encoders and the key technologies they use. In addition, we also compare\nthe performance of these researches side by side in terms of datasets,\nevaluation metrics, training resources, and inference speed. In addition to the\nfour foundation models, we survey other works on T2I, such as energy-based\nmodels and recent Mamba and multimodality. We also investigate the potential\nsocial impact of T2I and provide some solutions. Finally, we propose unique\ninsights of improving the performance of T2I models and possible future\ndevelopment directions. In summary, this survey is the first systematic and\ncomprehensive overview of T2I, aiming to provide a valuable guide for future\nresearchers and stimulate continued progress in this field."}
{"id": "2505.02529", "pdf": "https://arxiv.org/pdf/2505.02529", "abs": "https://arxiv.org/abs/2505.02529", "authors": ["Aiman Farooq", "Azad Singh", "Deepak Mishra", "Santanu Chaudhury"], "title": "RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cancer survival prediction using multi-modal medical imaging presents a\ncritical challenge in oncology, mainly due to the vulnerability of deep\nlearning models to noise and protocol variations across imaging centers.\nCurrent approaches struggle to extract consistent features from heterogeneous\nCT and PET images, limiting their clinical applicability. We address these\nchallenges by introducing RobSurv, a robust deep-learning framework that\nleverages vector quantization for resilient multi-modal feature learning. The\nkey innovation of our approach lies in its dual-path architecture: one path\nmaps continuous imaging features to learned discrete codebooks for\nnoise-resistant representation, while the parallel path preserves fine-grained\ndetails through continuous feature processing. This dual representation is\nintegrated through a novel patch-wise fusion mechanism that maintains local\nspatial relationships while capturing global context via Transformer-based\nprocessing. In extensive evaluations across three diverse datasets (HECKTOR,\nH\\&N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance,\nachieving concordance index of 0.771, 0.742, and 0.734 respectively -\nsignificantly outperforming existing methods. Most notably, our model maintains\nrobust performance even under severe noise conditions, with performance\ndegradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These\nresults, combined with strong generalization across different cancer types and\nimaging protocols, establish RobSurv as a promising solution for reliable\nclinical prognosis that can enhance treatment planning and patient care."}
{"id": "2505.02539", "pdf": "https://arxiv.org/pdf/2505.02539", "abs": "https://arxiv.org/abs/2505.02539", "authors": ["Nahuel Garcia-D'Urso", "Bernabe Sanchez-Sos", "Jorge Azorin-Lopez", "Andres Fuster-Guillo", "Antonio Macia-Lillo", "Higinio Mora-Mora"], "title": "Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Accurate 3D reconstruction using multi-camera RGB-D systems critically\ndepends on precise extrinsic calibration to achieve proper alignment between\ncaptured views. In this paper, we introduce an iterative extrinsic calibration\nmethod that leverages the geometric constraints provided by a three-dimensional\nmarker to significantly improve calibration accuracy. Our proposed approach\nsystematically segments and refines marker planes through clustering,\nregression analysis, and iterative reassignment techniques, ensuring robust\ngeometric correspondence across camera views. We validate our method\ncomprehensively in both controlled environments and practical real-world\nsettings within the Tech4Diet project, aimed at modeling the physical\nprogression of patients undergoing nutritional treatments. Experimental results\ndemonstrate substantial reductions in alignment errors, facilitating accurate\nand reliable 3D reconstructions."}
{"id": "2505.02549", "pdf": "https://arxiv.org/pdf/2505.02549", "abs": "https://arxiv.org/abs/2505.02549", "authors": ["Yongxiang Li", "Yuan Sun", "Yang Qin", "Dezhong Peng", "Xi Peng", "Peng Hu"], "title": "Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Unsupervised visible-infrared person re-identification (UVI-ReID) aims to\nretrieve pedestrian images across different modalities without costly\nannotations, but faces challenges due to the modality gap and lack of\nsupervision. Existing methods often adopt self-training with\nclustering-generated pseudo-labels but implicitly assume these labels are\nalways correct. In practice, however, this assumption fails due to inevitable\npseudo-label noise, which hinders model learning. To address this, we introduce\na new learning paradigm that explicitly considers Pseudo-Label Noise (PLN),\ncharacterized by three key challenges: noise overfitting, error accumulation,\nand noisy cluster correspondence. To this end, we propose a novel Robust\nDuality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy\npseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning\nmechanism (RAL) is proposed to dynamically emphasize clean samples while\ndown-weighting noisy ones. Second, to alleviate error accumulation-where the\nmodel reinforces its own mistakes-RoDE employs dual distinct models that are\nalternately trained using pseudo-labels from each other, encouraging diversity\nand preventing collapse. However, this dual-model strategy introduces\nmisalignment between clusters across models and modalities, creating noisy\ncluster correspondence. To resolve this, we introduce Cluster Consistency\nMatching (CCM), which aligns clusters across models and modalities by measuring\ncross-cluster similarity. Extensive experiments on three benchmarks demonstrate\nthe effectiveness of RoDE."}
{"id": "2505.02567", "pdf": "https://arxiv.org/pdf/2505.02567", "abs": "https://arxiv.org/abs/2505.02567", "authors": ["Xinjie Zhang", "Jintao Guo", "Shanshan Zhao", "Minghao Fu", "Lunhao Duan", "Guo-Hua Wang", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "categories": ["cs.CV"], "comment": "This work is still in progress", "summary": "Recent years have seen remarkable progress in both multimodal understanding\nmodels and image generation models. Despite their respective successes, these\ntwo domains have evolved independently, leading to distinct architectural\nparadigms: While autoregressive-based architectures have dominated multimodal\nunderstanding, diffusion-based models have become the cornerstone of image\ngeneration. Recently, there has been growing interest in developing unified\nframeworks that integrate these tasks. The emergence of GPT-4o's new\ncapabilities exemplifies this trend, highlighting the potential for\nunification. However, the architectural differences between the two domains\npose significant challenges. To provide a clear overview of current efforts\ntoward unification, we present a comprehensive survey aimed at guiding future\nresearch. First, we introduce the foundational concepts and recent advancements\nin multimodal understanding and text-to-image generation models. Next, we\nreview existing unified models, categorizing them into three main architectural\nparadigms: diffusion-based, autoregressive-based, and hybrid approaches that\nfuse autoregressive and diffusion mechanisms. For each category, we analyze the\nstructural designs and innovations introduced by related works. Additionally,\nwe compile datasets and benchmarks tailored for unified models, offering\nresources for future exploration. Finally, we discuss the key challenges facing\nthis nascent field, including tokenization strategy, cross-modal attention, and\ndata. As this area is still in its early stages, we anticipate rapid\nadvancements and will regularly update this survey. Our goal is to inspire\nfurther research and provide a valuable reference for the community. The\nreferences associated with this survey will be available on GitHub soon."}
{"id": "2505.02586", "pdf": "https://arxiv.org/pdf/2505.02586", "abs": "https://arxiv.org/abs/2505.02586", "authors": ["Eliraz Orfaig", "Inna Stainvas", "Igal Bilik"], "title": "RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces RGBX-DiffusionDet, an object detection framework\nextending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB\nimagery via an adaptive multimodal encoder. To enable cross-modal interaction,\nwe design the dynamic channel reduction within a convolutional block attention\nmodule (DCR-CBAM), which facilitates cross-talk between subnetworks by\ndynamically highlighting salient channel features. Furthermore, the dynamic\nmulti-level aggregation block (DMLAB) is proposed to refine spatial feature\nrepresentations through adaptive multiscale fusion. Finally, novel\nregularization losses that enforce channel saliency and spatial selectivity are\nintroduced, leading to compact and discriminative feature embeddings. Extensive\nexperiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric\ndataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We\ndemonstrate consistent superiority of the proposed approach over the baseline\nRGB-only DiffusionDet. The modular architecture maintains the original decoding\ncomplexity, ensuring efficiency. These results establish the proposed\nRGBX-DiffusionDet as a flexible multimodal object detection approach, providing\nnew insights into integrating diverse 2D sensing modalities into\ndiffusion-based detection pipelines."}
{"id": "2505.02593", "pdf": "https://arxiv.org/pdf/2505.02593", "abs": "https://arxiv.org/abs/2505.02593", "authors": ["Vincent Brebion", "Julien Moreau", "Franck Davoine"], "title": "DELTA: Dense Depth from Events and LiDAR using Transformer's Attention", "categories": ["cs.CV", "I.4.8"], "comment": "Accepted for the CVPR 2025 Workshop on Event-based Vision. For the\n  project page, see https://vbrebion.github.io/DELTA/", "summary": "Event cameras and LiDARs provide complementary yet distinct data:\nrespectively, asynchronous detections of changes in lighting versus sparse but\naccurate depth information at a fixed rate. To this day, few works have\nexplored the combination of these two modalities. In this article, we propose a\nnovel neural-network-based method for fusing event and LiDAR data in order to\nestimate dense depth maps. Our architecture, DELTA, exploits the concepts of\nself- and cross-attention to model the spatial and temporal relations within\nand between the event and LiDAR data. Following a thorough evaluation, we\ndemonstrate that DELTA sets a new state of the art in the event-based depth\nestimation problem, and that it is able to reduce the errors up to four times\nfor close ranges compared to the previous SOTA."}
{"id": "2505.02626", "pdf": "https://arxiv.org/pdf/2505.02626", "abs": "https://arxiv.org/abs/2505.02626", "authors": ["Sassan Mokhtar", "Arian Mousakhan", "Silvio Galesso", "Jawad Tayyub", "Thomas Brox"], "title": "Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models", "categories": ["cs.CV"], "comment": "Accepted as a spotlight presentation paper at the VAND Workshop, CVPR\n  2025. 10 pages, 6 figures", "summary": "Recent advances in visual industrial anomaly detection have demonstrated\nexceptional performance in identifying and segmenting anomalous regions while\nmaintaining fast inference speeds. However, anomaly\nclassification-distinguishing different types of anomalies-remains largely\nunexplored despite its critical importance in real-world inspection tasks. To\naddress this gap, we propose VELM, a novel LLM-based pipeline for anomaly\nclassification. Given the critical importance of inference speed, we first\napply an unsupervised anomaly detection method as a vision expert to assess the\nnormality of an observation. If an anomaly is detected, the LLM then classifies\nits type. A key challenge in developing and evaluating anomaly classification\nmodels is the lack of precise annotations of anomaly classes in existing\ndatasets. To address this limitation, we introduce MVTec-AC and VisA-AC,\nrefined versions of the widely used MVTec-AD and VisA datasets, which include\naccurate anomaly class labels for rigorous evaluation. Our approach achieves a\nstate-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD,\nexceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the\neffectiveness of VELM in understanding and categorizing anomalies. We hope our\nmethodology and benchmark inspire further research in anomaly classification,\nhelping bridge the gap between detection and comprehensive anomaly\ncharacterization."}
{"id": "2505.02648", "pdf": "https://arxiv.org/pdf/2505.02648", "abs": "https://arxiv.org/abs/2505.02648", "authors": ["Mingcheng Li", "Xiaolu Hou", "Ziyang Liu", "Dingkang Yang", "Ziyun Qian", "Jiawei Chen", "Jinjie Wei", "Yue Jiang", "Qingyao Xu", "Lihua Zhang"], "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown excellent performance in text-to-image\ngeneration. Nevertheless, existing methods often suffer from performance\nbottlenecks when handling complex prompts that involve multiple objects,\ncharacteristics, and relations. Therefore, we propose a Multi-agent\nCollaboration-based Compositional Diffusion (MCCD) for text-to-image generation\nfor complex scenes. Specifically, we design a multi-agent collaboration-based\nscene parsing module that generates an agent system comprising multiple agents\nwith distinct tasks, utilizing MLLMs to extract various scene elements\neffectively. In addition, Hierarchical Compositional diffusion utilizes a\nGaussian mask and filtering to refine bounding box regions and enhance objects\nthrough region enhancement, resulting in the accurate and high-fidelity\ngeneration of complex scenes. Comprehensive experiments demonstrate that our\nMCCD significantly improves the performance of the baseline models in a\ntraining-free manner, providing a substantial advantage in complex scene\ngeneration."}
{"id": "2505.02654", "pdf": "https://arxiv.org/pdf/2505.02654", "abs": "https://arxiv.org/abs/2505.02654", "authors": ["Clara Tomasini", "Luis Riazuelo", "Ana C. Murillo"], "title": "Sim2Real in endoscopy segmentation with a novel structure aware image translation", "categories": ["cs.CV", "I.2.10; I.4.6"], "comment": null, "summary": "Automatic segmentation of anatomical landmarks in endoscopic images can\nprovide assistance to doctors and surgeons for diagnosis, treatments or medical\ntraining. However, obtaining the annotations required to train commonly used\nsupervised learning methods is a tedious and difficult task, in particular for\nreal images. While ground truth annotations are easier to obtain for synthetic\ndata, models trained on such data often do not generalize well to real data.\nGenerative approaches can add realistic texture to it, but face difficulties to\nmaintain the structure of the original scene. The main contribution in this\nwork is a novel image translation model that adds realistic texture to\nsimulated endoscopic images while keeping the key scene layout information. Our\napproach produces realistic images in different endoscopy scenarios. We\ndemonstrate these images can effectively be used to successfully train a model\nfor a challenging end task without any real labeled data. In particular, we\ndemonstrate our approach for the task of fold segmentation in colonoscopy\nimages. Folds are key anatomical landmarks that can occlude parts of the colon\nmucosa and possible polyps. Our approach generates realistic images maintaining\nthe shape and location of the original folds, after the\nimage-style-translation, better than existing methods. We run experiments both\non a novel simulated dataset for fold segmentation, and real data from the\nEndoMapper (EM) dataset. All our new generated data and new EM metadata is\nbeing released to facilitate further research, as no public benchmark is\ncurrently available for the task of fold segmentation."}
{"id": "2505.02677", "pdf": "https://arxiv.org/pdf/2505.02677", "abs": "https://arxiv.org/abs/2505.02677", "authors": ["Saeed Shurrab", "Aadim Nepal", "Terrence J. Lee-St. John", "Nicola G. Ghazi", "Bartlomiej Piechowski-Jozwiak", "Farah E. Shamout"], "title": "Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Stroke is a major public health problem, affecting millions worldwide. Deep\nlearning has recently demonstrated promise for enhancing the diagnosis and risk\nprediction of stroke. However, existing methods rely on costly medical imaging\nmodalities, such as computed tomography. Recent studies suggest that retinal\nimaging could offer a cost-effective alternative for cerebrovascular health\nassessment due to the shared clinical pathways between the retina and the\nbrain. Hence, this study explores the impact of leveraging retinal images and\nclinical data for stroke detection and risk prediction. We propose a multimodal\ndeep neural network that processes Optical Coherence Tomography (OCT) and\ninfrared reflectance retinal scans, combined with clinical data, such as\ndemographics, vital signs, and diagnosis codes. We pretrained our model using a\nself-supervised learning framework using a real-world dataset consisting of\n$37$ k scans, and then fine-tuned and evaluated the model using a smaller\nlabeled subset. Our empirical findings establish the predictive ability of the\nconsidered modalities in detecting lasting effects in the retina associated\nwith acute stroke and forecasting future risk within a specific time horizon.\nThe experimental results demonstrate the effectiveness of our proposed\nframework by achieving $5$\\% AUROC improvement as compared to the unimodal\nimage-only baseline, and $8$\\% improvement compared to an existing\nstate-of-the-art foundation model. In conclusion, our study highlights the\npotential of retinal imaging in identifying high-risk patients and improving\nlong-term outcomes."}
{"id": "2505.02690", "pdf": "https://arxiv.org/pdf/2505.02690", "abs": "https://arxiv.org/abs/2505.02690", "authors": ["Haotian Chen", "Ziyu Liu", "Xi Cheng", "Chuangqi Li"], "title": "Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation", "categories": ["cs.CV"], "comment": "21 pages, 13 figures", "summary": "This study introduces Dance of Fireworks, an interactive system designed to\ncombat sedentary health risks by enhancing engagement in radio calisthenics.\nLeveraging mobile device cameras and lightweight pose estimation\n(PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint\nangles, and compares them with standardized motions to deliver real-time\ncorrective feedback. To incentivize participation, it dynamically maps users'\nmovements (such as joint angles and velocity) to customizable fireworks\nanimations, rewarding improved accuracy with richer visual effects. Experiments\ninvolving 136 participants demonstrated a significant reduction in average\njoint angle errors from 21.3 degrees to 9.8 degrees (p < 0.01) over four\nsessions, with 93.4 percent of users affirming its exercise-promoting efficacy\nand 85.4 percent praising its entertainment value. The system operates without\npredefined motion templates or specialised hardware, enabling seamless\nintegration into office environments. Future enhancements will focus on\nimproving pose recognition accuracy, reducing latency, and adding features such\nas multiplayer interaction and music synchronisation. This work presents a\ncost-effective, engaging solution to promote physical activity in sedentary\npopulations."}
{"id": "2505.02703", "pdf": "https://arxiv.org/pdf/2505.02703", "abs": "https://arxiv.org/abs/2505.02703", "authors": ["Zibo Xu", "Qiang Li", "Weizhi Nie", "Weijie Wang", "Anan Liu"], "title": "Structure Causal Models and LLMs Integration in Medical Visual Question Answering", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025", "summary": "Medical Visual Question Answering (MedVQA) aims to answer medical questions\naccording to medical images. However, the complexity of medical data leads to\nconfounders that are difficult to observe, so bias between images and questions\nis inevitable. Such cross-modal bias makes it challenging to infer medically\nmeaningful answers. In this work, we propose a causal inference framework for\nthe MedVQA task, which effectively eliminates the relative confounding effect\nbetween the image and the question to ensure the precision of the\nquestion-answering (QA) session. We are the first to introduce a novel causal\ngraph structure that represents the interaction between visual and textual\nelements, explicitly capturing how different questions influence visual\nfeatures. During optimization, we apply the mutual information to discover\nspurious correlations and propose a multi-variable resampling front-door\nadjustment method to eliminate the relative confounding effect, which aims to\nalign features based on their true causal relevance to the question-answering\ntask. In addition, we also introduce a prompt strategy that combines multiple\nprompt forms to improve the model's ability to understand complex medical data\nand answer accurately. Extensive experiments on three MedVQA datasets\ndemonstrate that 1) our method significantly improves the accuracy of MedVQA,\nand 2) our method achieves true causal correlations in the face of complex\nmedical data."}
{"id": "2505.02704", "pdf": "https://arxiv.org/pdf/2505.02704", "abs": "https://arxiv.org/abs/2505.02704", "authors": ["Bojin Wu", "Jing Chen"], "title": "Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery", "categories": ["cs.CV"], "comment": "21 pages, conference", "summary": "We propose a robust method for monocular depth scale recovery. Monocular\ndepth estimation can be divided into two main directions: (1) relative depth\nestimation, which provides normalized or inverse depth without scale\ninformation, and (2) metric depth estimation, which involves recovering depth\nwith absolute scale. To obtain absolute scale information for practical\ndownstream tasks, utilizing textual information to recover the scale of a\nrelative depth map is a highly promising approach. However, since a single\nimage can have multiple descriptions from different perspectives or with\nvarying styles, it has been shown that different textual descriptions can\nsignificantly affect the scale recovery process. To address this issue, our\nmethod, VGLD, stabilizes the influence of textual information by incorporating\nhigh-level semantic information from the corresponding image alongside the\ntextual description. This approach resolves textual ambiguities and robustly\noutputs a set of linear transformation parameters (scalars) that can be\nglobally applied to the relative depth map, ultimately generating depth\npredictions with metric-scale accuracy. We validate our method across several\npopular relative depth models(MiDas, DepthAnything), using both indoor scenes\n(NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions\nas a universal alignment module when trained on multiple datasets, achieving\nstrong performance even in zero-shot scenarios. Code is available at:\nhttps://github.com/pakinwu/VGLD."}
{"id": "2505.02720", "pdf": "https://arxiv.org/pdf/2505.02720", "abs": "https://arxiv.org/abs/2505.02720", "authors": ["Sang NguyenQuang", "Cheng-Wei Chen", "Xiem HoangVan", "Wen-Hsiao Peng"], "title": "A Rate-Quality Model for Learned Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "Learned video coding (LVC) has recently achieved superior coding performance.\nIn this paper, we model the rate-quality (R-Q) relationship for learned video\ncoding by a parametric function. We learn a neural network, termed RQNet, to\ncharacterize the relationship between the bitrate and quality level according\nto video content and coding context. The predicted (R,Q) results are further\nintegrated with those from previously coded frames using the least-squares\nmethod to determine the parameters of our R-Q model on-the-fly. Compared to the\nconventional approaches, our method accurately estimates the R-Q relationship,\nenabling the online adaptation of model parameters to enhance both flexibility\nand precision. Experimental results show that our R-Q model achieves\nsignificantly smaller bitrate deviations than the baseline method on commonly\nused datasets with minimal additional complexity."}
{"id": "2505.02746", "pdf": "https://arxiv.org/pdf/2505.02746", "abs": "https://arxiv.org/abs/2505.02746", "authors": ["Simon Ging", "Sebastian Walter", "Jelena BratuliÄ", "Johannes Dienert", "Hannah Bast", "Thomas Brox"], "title": "Using Knowledge Graphs to harvest datasets for efficient CLIP model training", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Training high-quality CLIP models typically requires enormous datasets, which\nlimits the development of domain-specific models -- especially in areas that\neven the largest CLIP models do not cover well -- and drives up training costs.\nThis poses challenges for scientific research that needs fine-grained control\nover the training procedure of CLIP models. In this work, we show that by\nemploying smart web search strategies enhanced with knowledge graphs, a robust\nCLIP model can be trained from scratch with considerably less data.\nSpecifically, we demonstrate that an expert foundation model for living\norganisms can be built using just 10M images. Moreover, we introduce EntityNet,\na dataset comprising 33M images paired with 46M text descriptions, which\nenables the training of a generic CLIP model in significantly reduced time."}
{"id": "2505.02751", "pdf": "https://arxiv.org/pdf/2505.02751", "abs": "https://arxiv.org/abs/2505.02751", "authors": ["H. Martin Gillis", "Yogeshwar Shendye", "Paul Hollensen", "Alan Fine", "Thomas Trappenberg"], "title": "Platelet enumeration in dense aggregates", "categories": ["eess.IV", "cs.CV"], "comment": "International Joint Conference on Neural Networks (IJCNN 2025)", "summary": "Identifying and counting blood components such as red blood cells, various\ntypes of white blood cells, and platelets is a critical task for healthcare\npractitioners. Deep learning approaches, particularly convolutional neural\nnetworks (CNNs) using supervised learning strategies, have shown considerable\nsuccess for such tasks. However, CNN based architectures such as U-Net, often\nstruggles to accurately identify platelets due to their sizes and high\nvariability of features. To address these challenges, researchers have commonly\nemployed strategies such as class weighted loss functions, which have\ndemonstrated some success. However, this does not address the more significant\nchallenge of platelet variability in size and tendency to form aggregates and\nassociations with other blood components. In this study, we explored an\nalternative approach by investigating the role of convolutional kernels in\nmitigating these issues. We also assigned separate classes to singular\nplatelets and platelet aggregates and performed semantic segmentation using\nvarious U-Net architectures for identifying platelets. We then evaluated and\ncompared two common methods (pixel area method and connected component\nanalysis) for counting platelets and proposed an alternative approach\nspecialized for single platelets and platelet aggregates. Our experiments\nprovided results that showed significant improvements in the identification of\nplatelets, highlighting the importance of optimizing convolutional operations\nand class designations. We show that the common practice of pixel area-based\ncounting often over estimate platelet counts, whereas the proposed method\npresented in this work offers significant improvements. We discuss in detail\nabout these methods from segmentation masks."}
{"id": "2505.02753", "pdf": "https://arxiv.org/pdf/2505.02753", "abs": "https://arxiv.org/abs/2505.02753", "authors": ["Yankai Jiang", "Peng Zhang", "Donglin Yang", "Yuan Tian", "Hai Lin", "Xiaosong Wang"], "title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models", "categories": ["cs.CV"], "comment": "This paper is accepted to CVPR 2025", "summary": "We explore Generalizable Tumor Segmentation, aiming to train a single model\nfor zero-shot tumor segmentation across diverse anatomical regions. Existing\nmethods face limitations related to segmentation quality, scalability, and the\nrange of applicable imaging modalities. In this paper, we uncover the potential\nof the internal representations within frozen medical foundation diffusion\nmodels as highly efficient zero-shot learners for tumor segmentation by\nintroducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware\nopen-vocabulary attention maps based on text prompts to enable generalizable\nanomaly segmentation without being restricted by a predefined training category\nlist. To further improve and refine anomaly segmentation masks, DiffuGTS\nleverages the diffusion model, transforming pathological regions into\nhigh-quality pseudo-healthy counterparts through latent space inpainting, and\napplies a novel pixel-level and feature-level residual learning approach,\nresulting in segmentation masks with significantly enhanced quality and\ngeneralization. Comprehensive experiments on four datasets and seven tumor\ncategories demonstrate the superior performance of our method, surpassing\ncurrent state-of-the-art models across multiple zero-shot settings. Codes are\navailable at https://github.com/Yankai96/DiffuGTS."}
{"id": "2505.02779", "pdf": "https://arxiv.org/pdf/2505.02779", "abs": "https://arxiv.org/abs/2505.02779", "authors": ["David Rivas-Villar", "Ãlvaro S. Hervella", "JosÃ© Rouco", "Jorge Novo"], "title": "Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance", "categories": ["cs.CV"], "comment": null, "summary": "Retinal image registration, particularly for color fundus images, is a\nchallenging yet essential task with diverse clinical applications. Existing\nregistration methods for color fundus images typically rely on keypoints and\ndescriptors for alignment; however, a significant limitation is their reliance\non labeled data, which is particularly scarce in the medical domain.\n  In this work, we present a novel unsupervised registration pipeline that\nentirely eliminates the need for labeled data. Our approach is based on the\nprinciple that locations with distinctive descriptors constitute reliable\nkeypoints. This fully inverts the conventional state-of-the-art approach,\nconditioning the detector on the descriptor rather than the opposite.\n  First, we propose an innovative descriptor learning method that operates\nwithout keypoint detection or any labels, generating descriptors for arbitrary\nlocations in retinal images. Next, we introduce a novel, label-free keypoint\ndetector network which works by estimating descriptor performance directly from\nthe input image.\n  We validate our method through a comprehensive evaluation on four hold-out\ndatasets, demonstrating that our unsupervised descriptor outperforms\nstate-of-the-art supervised descriptors and that our unsupervised detector\nsignificantly outperforms existing unsupervised detection methods. Finally, our\nfull registration pipeline achieves performance comparable to the leading\nsupervised methods, while not employing any labeled data. Additionally, the\nlabel-free nature and design of our method enable direct adaptation to other\ndomains and modalities."}
{"id": "2505.02784", "pdf": "https://arxiv.org/pdf/2505.02784", "abs": "https://arxiv.org/abs/2505.02784", "authors": ["Vladyslav Zalevskyi", "Thomas Sanchez", "Misha Kaandorp", "Margaux Roulet", "Diego Fajardo-Rojas", "Liu Li", "Jana Hutter", "Hongwei Bran Li", "Matthew Barkovich", "Hui Ji", "Luca Wilhelmi", "Aline DÃ¤ndliker", "CÃ©line Steger", "MÃ©riam Koob", "Yvan Gomez", "Anton JakovÄiÄ", "Melita KlaiÄ", "Ana AdÅ¾iÄ", "Pavel MarkoviÄ", "Gracia GrabariÄ", "Milan Rados", "Jordina Aviles Verdera", "Gregor Kasprian", "Gregor Dovjak", "Raphael Gaubert-RachmÃ¼hl", "Maurice Aschwanden", "Qi Zeng", "Davood Karimi", "Denis Peruzzo", "Tommaso Ciceri", "Giorgio Longari", "Rachika E. Hamadache", "Amina Bouzid", "Xavier LladÃ³", "Simone Chiarella", "Gerard MartÃ­-Juan", "Miguel Ãngel GonzÃ¡lez Ballester", "Marco Castellaro", "Marco Pinamonti", "Valentina Visani", "Robin Cremese", "KeÃ¯n Sam", "Fleur Gaudfernau", "Param Ahir", "Mehul Parikh", "Maximilian Zenk", "Michael Baumgartner", "Klaus Maier-Hein", "Li Tianhong", "Yang Hong", "Zhao Longfei", "Domen Preloznik", "Å½iga Å piclin", "Jae Won Choi", "Muyang Li", "Jia Fu", "Guotai Wang", "Jingwen Jiang", "Lyuyang Tong", "Bo Du", "Andrea Gondova", "Sungmin You", "Kiho Im", "Abdul Qayyum", "Moona Mazher", "Steven A Niederer", "Maya Yanko", "Bella Specktor-Fadida", "Dafna Ben Bashat", "Andras Jakab", "Roxane Licandro", "Kelly Payette", "Meritxell Bach Cuadra"], "title": "Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge", "categories": ["cs.CV"], "comment": null, "summary": "Accurate fetal brain tissue segmentation and biometric analysis are essential\nfor studying brain development in utero. The FeTA Challenge 2024 advanced\nautomated fetal brain MRI analysis by introducing biometry prediction as a new\ntask alongside tissue segmentation. For the first time, our diverse\nmulti-centric test set included data from a new low-field (0.55T) MRI dataset.\nEvaluation metrics were also expanded to include the topology-specific Euler\ncharacteristic difference (ED). Sixteen teams submitted segmentation methods,\nmost of which performed consistently across both high- and low-field scans.\nHowever, longitudinal trends indicate that segmentation accuracy may be\nreaching a plateau, with results now approaching inter-rater variability. The\nED metric uncovered topological differences that were missed by conventional\nmetrics, while the low-field dataset achieved the highest segmentation scores,\nhighlighting the potential of affordable imaging systems when paired with\nhigh-quality reconstruction. Seven teams participated in the biometry task, but\nmost methods failed to outperform a simple baseline that predicted measurements\nbased solely on gestational age, underscoring the challenge of extracting\nreliable biometric estimates from image data alone. Domain shift analysis\nidentified image quality as the most significant factor affecting model\ngeneralization, with super-resolution pipelines also playing a substantial\nrole. Other factors, such as gestational age, pathology, and acquisition site,\nhad smaller, though still measurable, effects. Overall, FeTA 2024 offers a\ncomprehensive benchmark for multi-class segmentation and biometry estimation in\nfetal brain MRI, underscoring the need for data-centric approaches, improved\ntopological evaluation, and greater dataset diversity to enable clinically\nrobust and generalizable AI tools."}
{"id": "2505.02787", "pdf": "https://arxiv.org/pdf/2505.02787", "abs": "https://arxiv.org/abs/2505.02787", "authors": ["David Rivas-Villar", "Ãlvaro S. Hervella", "JosÃ© Rouco", "Jorge Novo"], "title": "Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration", "categories": ["cs.CV"], "comment": null, "summary": "Current color fundus image registration approaches are limited, among other\nthings, by the lack of labeled data, which is even more significant in the\nmedical domain, motivating the use of unsupervised learning. Therefore, in this\nwork, we develop a novel unsupervised descriptor learning method that does not\nrely on keypoint detection. This enables the resulting descriptor network to be\nagnostic to the keypoint detector used during the registration inference.\n  To validate this approach, we perform an extensive and comprehensive\ncomparison on the reference public retinal image registration dataset.\nAdditionally, we test our method with multiple keypoint detectors of varied\nnature, even proposing some novel ones. Our results demonstrate that the\nproposed approach offers accurate registration, not incurring in any\nperformance loss versus supervised methods. Additionally, it demonstrates\naccurate performance regardless of the keypoint detector used. Thus, this work\nrepresents a notable step towards leveraging unsupervised learning in the\nmedical domain."}
{"id": "2505.02797", "pdf": "https://arxiv.org/pdf/2505.02797", "abs": "https://arxiv.org/abs/2505.02797", "authors": ["Luqi Gong", "Haotian Chen", "Yikun Chen", "Tianliang Yao", "Chao Li", "Shuai Zhao", "Guangjie Han"], "title": "DPNet: Dynamic Pooling Network for Tiny Object Detection", "categories": ["cs.CV"], "comment": "15 pages, 12 figures Haotian Chen and Luqi Gong contributed equally\n  to this work", "summary": "In unmanned aerial systems, especially in complex environments, accurately\ndetecting tiny objects is crucial. Resizing images is a common strategy to\nimprove detection accuracy, particularly for small objects. However, simply\nenlarging images significantly increases computational costs and the number of\nnegative samples, severely degrading detection performance and limiting its\napplicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny\nobject detection to mitigate these issues. DPNet employs a flexible\ndown-sampling strategy by introducing a factor (df) to relax the fixed\ndownsampling process of the feature map to an adjustable one. Furthermore, we\ndesign a lightweight predictor to predict df for each input image, which is\nused to decrease the resolution of feature maps in the backbone. Thus, we\nachieve input-aware downsampling. We also design an Adaptive Normalization\nModule (ANM) to make a unified detector compatible with different dfs. A\nguidance loss supervises the predictor's training. DPNet dynamically allocates\ncomputing resources to trade off between detection accuracy and efficiency.\nExperiments on the TinyCOCO and TinyPerson datasets show that DPNet can save\nover 35% and 25% GFLOPs, respectively, while maintaining comparable detection\nperformance. The code will be made publicly available."}
{"id": "2505.02815", "pdf": "https://arxiv.org/pdf/2505.02815", "abs": "https://arxiv.org/abs/2505.02815", "authors": ["Nicoleta Basoc", "Adrian Cosma", "Andy CÇtrunÇ", "Emilian RÇdoi"], "title": "Database-Agnostic Gait Enrollment using SetTransformers", "categories": ["cs.CV"], "comment": "5 Tables, 6 Figures", "summary": "Gait recognition has emerged as a powerful tool for unobtrusive and\nlong-range identity analysis, with growing relevance in surveillance and\nmonitoring applications. Although recent advances in deep learning and\nlarge-scale datasets have enabled highly accurate recognition under closed-set\nconditions, real-world deployment demands open-set gait enrollment, which means\ndetermining whether a new gait sample corresponds to a known identity or\nrepresents a previously unseen individual. In this work, we introduce a\ntransformer-based framework for open-set gait enrollment that is both\ndataset-agnostic and recognition-architecture-agnostic. Our method leverages a\nSetTransformer to make enrollment decisions based on the embedding of a probe\nsample and a context set drawn from the gallery, without requiring\ntask-specific thresholds or retraining for new environments. By decoupling\nenrollment from the main recognition pipeline, our model is generalized across\ndifferent datasets, gallery sizes, and identity distributions. We propose an\nevaluation protocol that uses existing datasets in different ratios of\nidentities and walks per identity. We instantiate our method using\nskeleton-based gait representations and evaluate it on two benchmark datasets\n(CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition\nmodels (GaitGraph, GaitFormer, and GaitPT). We show that our method is\nflexible, is able to accurately perform enrollment in different scenarios, and\nscales better with data compared to traditional approaches. We will make the\ncode and dataset scenarios publicly available."}
{"id": "2505.02823", "pdf": "https://arxiv.org/pdf/2505.02823", "abs": "https://arxiv.org/abs/2505.02823", "authors": ["Zinan Guo", "Pengze Zhang", "Yanze Wu", "Chong Mou", "Songtao Zhao", "Qian He"], "title": "MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing", "categories": ["cs.CV"], "comment": "Project page at https://github.com/guozinan126/MUSAR", "summary": "Current multi-subject customization approaches encounter two critical\nchallenges: the difficulty in acquiring diverse multi-subject training data,\nand attribute entanglement across different subjects. To bridge these gaps, we\npropose MUSAR - a simple yet effective framework to achieve robust\nmulti-subject customization while requiring only single-subject training data.\nFirstly, to break the data limitation, we introduce debiased diptych learning.\nIt constructs diptych training pairs from single-subject images to facilitate\nmulti-subject learning, while actively correcting the distribution bias\nintroduced by diptych construction via static attention routing and dual-branch\nLoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic\nattention routing mechanism, which adaptively establishes bijective mappings\nbetween generated images and conditional subjects. This design not only\nachieves decoupling of multi-subject representations but also maintains\nscalable generalization performance with increasing reference subjects.\nComprehensive experiments demonstrate that our MUSAR outperforms existing\nmethods - even those trained on multi-subject dataset - in image quality,\nsubject consistency, and interaction naturalness, despite requiring only\nsingle-subject dataset."}
{"id": "2505.02824", "pdf": "https://arxiv.org/pdf/2505.02824", "abs": "https://arxiv.org/abs/2505.02824", "authors": ["Kuofeng Gao", "Yufei Zhu", "Yiming Li", "Jiawang Bai", "Yong Yang", "Zhifeng Li", "Shu-Tao Xia"], "title": "Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Text-to-image (T2I) diffusion models have rapidly advanced, enabling\nhigh-quality image generation conditioned on textual prompts. However, the\ngrowing trend of fine-tuning pre-trained models for personalization raises\nserious concerns about unauthorized dataset usage. To combat this, dataset\nownership verification (DOV) has emerged as a solution, embedding watermarks\ninto the fine-tuning datasets using backdoor techniques. These watermarks\nremain inactive under benign samples but produce owner-specified outputs when\ntriggered. Despite the promise of DOV for T2I diffusion models, its robustness\nagainst copyright evasion attacks (CEA) remains unexplored. In this paper, we\nexplore how attackers can bypass these mechanisms through CEA, allowing models\nto circumvent watermarks even when trained on watermarked datasets. We propose\nthe first copyright evasion attack (i.e., CEAT2I) specifically designed to\nundermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three\nstages: watermarked sample detection, trigger identification, and efficient\nwatermark mitigation. A key insight driving our approach is that T2I models\nexhibit faster convergence on watermarked samples during the fine-tuning,\nevident through intermediate feature deviation. Leveraging this, CEAT2I can\nreliably detect the watermarked samples. Then, we iteratively ablate tokens\nfrom the prompts of detected watermarked samples and monitor shifts in\nintermediate features to pinpoint the exact trigger tokens. Finally, we adopt a\nclosed-form concept erasure method to remove the injected watermark. Extensive\nexperiments show that our CEAT2I effectively evades DOV mechanisms while\npreserving model performance."}
{"id": "2505.02825", "pdf": "https://arxiv.org/pdf/2505.02825", "abs": "https://arxiv.org/abs/2505.02825", "authors": ["Alex Hoi Hang Chan", "Otto Brookes", "Urs Waldmann", "Hemal Naik", "Iain D. Couzin", "Majid Mirmehdi", "NoÃ«l Adiko Houa", "Emmanuelle Normand", "Christophe Boesch", "Lukas Boesch", "Mimi Arandjelovic", "Hjalmar KÃ¼hl", "Tilo Burghardt", "Fumihiro Kano"], "title": "Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology", "categories": ["cs.CV"], "comment": "Accepted at CVPR Workshop, CV4Animals 2025", "summary": "Computer vision methods have demonstrated considerable potential to\nstreamline ecological and biological workflows, with a growing number of\ndatasets and models becoming available to the research community. However,\nthese resources focus predominantly on evaluation using machine learning\nmetrics, with relatively little emphasis on how their application impacts\ndownstream analysis. We argue that models should be evaluated using\napplication-specific metrics that directly represent model performance in the\ncontext of its final use case. To support this argument, we present two\ndisparate case studies: (1) estimating chimpanzee abundance and density with\ncamera trap distance sampling when using a video-based behaviour classifier and\n(2) estimating head rotation in pigeons using a 3D posture estimator. We show\nthat even models with strong machine learning performance (e.g., 87% mAP) can\nyield data that leads to discrepancies in abundance estimates compared to\nexpert-derived data. Similarly, the highest-performing models for posture\nestimation do not produce the most accurate inferences of gaze direction in\npigeons. Motivated by these findings, we call for researchers to integrate\napplication-specific metrics in ecological/biological datasets, allowing for\nmodels to be benchmarked in the context of their downstream application and to\nfacilitate better integration of models into application workflows."}
{"id": "2505.02830", "pdf": "https://arxiv.org/pdf/2505.02830", "abs": "https://arxiv.org/abs/2505.02830", "authors": ["Qingqiu Li", "Zihang Cui", "Seongsu Bae", "Jilan Xu", "Runtian Yuan", "Yuejie Zhang", "Rui Feng", "Quanli Shen", "Xiaobo Zhang", "Junjun He", "Shujun Wang"], "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Chest X-rays (CXRs) are the most frequently performed imaging examinations in\nclinical settings. Recent advancements in Large Multimodal Models (LMMs) have\nenabled automated CXR interpretation, enhancing diagnostic accuracy and\nefficiency. However, despite their strong visual understanding, current Medical\nLMMs (MLMMs) still face two major challenges: (1) Insufficient region-level\nunderstanding and interaction, and (2) Limited accuracy and interpretability\ndue to single-step reasoning. In this paper, we empower MLMMs with\nanatomy-centric reasoning capabilities to enhance their interactivity and\nexplainability. Specifically, we first propose an Anatomical Ontology-Guided\nReasoning (AOR) framework, which centers on cross-modal region-level\ninformation to facilitate multi-step reasoning. Next, under the guidance of\nexpert physicians, we develop AOR-Instruction, a large instruction dataset for\nMLMMs training. Our experiments demonstrate AOR's superior performance in both\nVQA and report generation tasks."}
{"id": "2505.02831", "pdf": "https://arxiv.org/pdf/2505.02831", "abs": "https://arxiv.org/abs/2505.02831", "authors": ["Dengyang Jiang", "Mengmeng Wang", "Liuzhuozheng Li", "Lei Zhang", "Haoyu Wang", "Wei Wei", "Guang Dai", "Yanning Zhang", "Jingdong Wang"], "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves", "categories": ["cs.CV"], "comment": "Self-Representation Alignment for Diffusion Transformers. arXiv admin\n  note: text overlap with arXiv:2410.06940 by other authors", "summary": "Recent studies have demonstrated that learning a meaningful internal\nrepresentation can both accelerate generative training and enhance generation\nquality of the diffusion transformers. However, existing approaches necessitate\nto either introduce an additional and complex representation training framework\nor rely on a large-scale, pre-trained representation foundation model to\nprovide representation guidance during the original generative training\nprocess. In this study, we posit that the unique discriminative process\ninherent to diffusion transformers enables them to offer such guidance without\nrequiring external representation components. We therefore propose\nSelf-Representation A}lignment (SRA), a simple yet straightforward method that\nobtain representation guidance through a self-distillation manner.\nSpecifically, SRA aligns the output latent representation of the diffusion\ntransformer in earlier layer with higher noise to that in later layer with\nlower noise to progressively enhance the overall representation learning during\nonly generative training process. Experimental results indicate that applying\nSRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA\nnot only significantly outperforms approaches relying on auxiliary, complex\nrepresentation training frameworks but also achieves performance comparable to\nmethods that heavily dependent on powerful external representation priors."}
{"id": "2505.02835", "pdf": "https://arxiv.org/pdf/2505.02835", "abs": "https://arxiv.org/abs/2505.02835", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Xiao Hu", "Chaoyou Fu", "Bin Wen", "Tianke Zhang", "Changyi Liu", "Kaiyu Jiang", "Kaibing Chen", "Kaiyu Tang", "Haojie Ding", "Jiankang Chen", "Fan Yang", "Zhang Zhang", "Tingting Gao", "Liang Wang"], "title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Home page: https://github.com/yfzhang114/r1_reward", "summary": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the\nperformance of Multimodal Large Language Models (MLLMs). While recent\nadvancements have primarily focused on improving the model structure and\ntraining data of MRMs, there has been limited exploration into the\neffectiveness of long-term reasoning capabilities for reward modeling and how\nto activate these capabilities in MRMs. In this paper, we explore how\nReinforcement Learning (RL) can be used to improve reward modeling.\nSpecifically, we reformulate the reward modeling problem as a rule-based RL\ntask. However, we observe that directly applying existing RL algorithms, such\nas Reinforce++, to reward modeling often leads to training instability or even\ncollapse due to the inherent limitations of these algorithms. To address this\nissue, we propose the StableReinforce algorithm, which refines the training\nloss, advantage estimation strategy, and reward design of existing RL methods.\nThese refinements result in more stable training dynamics and superior\nperformance. To facilitate MRM training, we collect 200K preference data from\ndiverse datasets. Our reward model, R1-Reward, trained using the\nStableReinforce algorithm on this dataset, significantly improves performance\non multimodal reward modeling benchmarks. Compared to previous SOTA models,\nR1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$\nimprovement on the Multimodal Reward Bench. Moreover, with more inference\ncompute, R1-Reward's performance is further enhanced, highlighting the\npotential of RL algorithms in optimizing MRMs."}
{"id": "2505.02836", "pdf": "https://arxiv.org/pdf/2505.02836", "abs": "https://arxiv.org/abs/2505.02836", "authors": ["Lu Ling", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yifan Ding", "Yu Zeng", "Yichen Sheng", "Yunhao Ge", "Ming-Yu Liu", "Aniket Bera", "Zhaoshuo Li"], "title": "Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing interactive 3D scenes from text is essential for gaming, virtual\nreality, and embodied AI. However, existing methods face several challenges.\nLearning-based approaches depend on small-scale indoor datasets, limiting the\nscene diversity and layout complexity. While large language models (LLMs) can\nleverage diverse text-domain knowledge, they struggle with spatial realism,\noften producing unnatural object placements that fail to respect common sense.\nOur key insight is that vision perception can bridge this gap by providing\nrealistic spatial guidance that LLMs lack. To this end, we introduce\nScenethesis, a training-free agentic framework that integrates LLM-based scene\nplanning with vision-guided layout refinement. Given a text prompt, Scenethesis\nfirst employs an LLM to draft a coarse layout. A vision module then refines it\nby generating an image guidance and extracting scene structure to capture\ninter-object relations. Next, an optimization module iteratively enforces\naccurate pose alignment and physical plausibility, preventing artifacts like\nobject penetration and instability. Finally, a judge module verifies spatial\ncoherence. Comprehensive experiments show that Scenethesis generates diverse,\nrealistic, and physically plausible 3D interactive scenes, making it valuable\nfor virtual content creation, simulation environments, and embodied AI\nresearch."}
{"id": "2505.01456", "pdf": "https://arxiv.org/pdf/2505.01456", "abs": "https://arxiv.org/abs/2505.01456", "authors": ["Vaidehi Patil", "Yi-Lin Sung", "Peter Hase", "Jie Peng", "Tianlong Chen", "Mohit Bansal"], "title": "Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "The dataset and code are publicly available at\n  https://github.com/Vaidehi99/UnLOK-VQA", "summary": "LLMs trained on massive datasets may inadvertently acquire sensitive\ninformation such as personal details and potentially harmful content. This risk\nis further heightened in multimodal LLMs as they integrate information from\nmultiple modalities (image and text). Adversaries can exploit this knowledge\nthrough multimodal prompts to extract sensitive details. Evaluating how\neffectively MLLMs can forget such information (targeted unlearning)\nnecessitates the creation of high-quality, well-annotated image-text pairs.\nWhile prior work on unlearning has focused on text, multimodal unlearning\nremains underexplored. To address this gap, we first introduce a multimodal\nunlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as\nan attack-and-defense framework to evaluate methods for deleting specific\nmultimodal knowledge from MLLMs. We extend a visual question-answering dataset\nusing an automated pipeline that generates varying-proximity samples for\ntesting generalization and specificity, followed by manual filtering for\nmaintaining high quality. We then evaluate six defense objectives against seven\nattacks (four whitebox, three blackbox), including a novel whitebox method\nleveraging interpretability of hidden states. Our results show multimodal\nattacks outperform text- or image-only ones, and that the most effective\ndefense removes answer information from internal model states. Additionally,\nlarger models exhibit greater post-editing robustness, suggesting that scale\nenhances safety. UnLOK-VQA provides a rigorous benchmark for advancing\nunlearning in MLLMs."}
{"id": "2505.01457", "pdf": "https://arxiv.org/pdf/2505.01457", "abs": "https://arxiv.org/abs/2505.01457", "authors": ["Mingjun Xu", "Zehui Wang", "Hengxing Cai", "Renxin Zhong"], "title": "A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems have predominantly focused on\ntext-based retrieval, limiting their effectiveness in handling visually-rich\ndocuments that encompass text, images, tables, and charts. To bridge this gap,\nwe propose a unified multi-granularity multimodal retrieval framework tailored\nfor two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical\nencoding strategies, modality-aware retrieval mechanisms, and reranking modules\nto effectively capture and utilize the complex interdependencies between\ntextual and visual modalities. By leveraging off-the-shelf vision-language\nmodels and implementing a training-free hybridretrieval strategy, our framework\ndemonstrates robust performance without the need for task-specific fine-tuning.\nExperimental evaluations reveal that incorporating layout-aware search and\nreranking modules significantly enhances retrieval accuracy, achieving a top\nperformance score of 65.56. This work underscores the potential of scalable and\nreproducible solutions in advancing multimodal document retrieval systems."}
{"id": "2505.01476", "pdf": "https://arxiv.org/pdf/2505.01476", "abs": "https://arxiv.org/abs/2505.01476", "authors": ["Zhe Zhang", "Mingxiu Cai", "Hanxiao Wang", "Gaochang Wu", "Tianyou Chai", "Xiatian Zhu"], "title": "CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures, 10 tables, accepted by Forty-Second\n  International Conference on Machine Learning ( ICML 2025 )", "summary": "Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an\ninput image with respect to normal samples. Either by reconstructing normal\ncounterparts (reconstruction-based) or by learning an image feature embedding\nspace (embedding-based), existing approaches fundamentally rely on image-level\nor feature-level matching to derive anomaly scores. Often, such a matching\nprocess is inaccurate yet overlooked, leading to sub-optimal detection. To\naddress this issue, we introduce the concept of cost filtering, borrowed from\nclassical matching tasks, such as depth and flow estimation, into the UAD\nproblem. We call this approach {\\em CostFilter-AD}. Specifically, we first\nconstruct a matching cost volume between the input and normal samples,\ncomprising two spatial dimensions and one matching dimension that encodes\npotential matches. To refine this, we propose a cost volume filtering network,\nguided by the input observation as an attention query across multiple feature\nlayers, which effectively suppresses matching noise while preserving edge\nstructures and capturing subtle anomalies. Designed as a generic\npost-processing plug-in, CostFilter-AD can be integrated with either\nreconstruction-based or embedding-based methods. Extensive experiments on\nMVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for\nboth single- and multi-class UAD tasks. Code and models will be released at\nhttps://github.com/ZHE-SAPI/CostFilter-AD."}
{"id": "2505.01638", "pdf": "https://arxiv.org/pdf/2505.01638", "abs": "https://arxiv.org/abs/2505.01638", "authors": ["Michael Marinaccio", "Fatemeh Afghah"], "title": "Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4.6; I.4.8"], "comment": "7 pages, 4 figures, 4 tables", "summary": "High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs)\ntypically requires multimodal sensing - especially RGB and thermal imagery -\nwhich increases hardware cost and power consumption. This paper introduces\nSAM-TIFF, a novel teacher-student distillation framework for pixel-level\nwildfire temperature prediction and segmentation using RGB input only. A\nmultimodal teacher network trained on paired RGB-Thermal imagery and\nradiometric TIFF ground truth distills knowledge to a unimodal RGB student\nnetwork, enabling thermal-sensor-free inference. Segmentation supervision is\ngenerated using a hybrid approach of segment anything (SAM)-guided mask\ngeneration, and selection via TOPSIS, along with Canny edge detection and\nOtsu's thresholding pipeline for automatic point prompt selection. Our method\nis the first to perform per-pixel temperature regression from RGB UAV data,\ndemonstrating strong generalization on the recent FLAME 3 dataset. This work\nlays the foundation for lightweight, cost-effective UAV-based wildfire\nmonitoring systems without thermal sensors."}
{"id": "2505.01644", "pdf": "https://arxiv.org/pdf/2505.01644", "abs": "https://arxiv.org/abs/2505.01644", "authors": ["Jun Li", "Yijue Zhang", "Haibo Shi", "Minhong Li", "Qiwei Li", "Xiaohua Qian"], "title": "A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": "accept by IEEE Transactions on Medical Imaging (TMI) 2025", "summary": "Pancreatic cancer, characterized by its notable prevalence and mortality\nrates, demands accurate lesion delineation for effective diagnosis and\ntherapeutic interventions. The generalizability of extant methods is frequently\ncompromised due to the pronounced variability in imaging and the heterogeneous\ncharacteristics of pancreatic lesions, which may mimic normal tissues and\nexhibit significant inter-patient variability. Thus, we propose a\ngeneralization framework that synergizes pixel-level classification and\nregression tasks, to accurately delineate lesions and improve model stability.\nThis framework not only seeks to align segmentation contours with actual\nlesions but also uses regression to elucidate spatial relationships between\ndiseased and normal tissues, thereby improving tumor localization and\nmorphological characterization. Enhanced by the reciprocal transformation of\ntask outputs, our approach integrates additional regression supervision within\nthe segmentation context, bolstering the model's generalization ability from a\ndual-task perspective. Besides, dual self-supervised learning in feature spaces\nand output spaces augments the model's representational capability and\nstability across different imaging views. Experiments on 594 samples composed\nof three datasets with significant imaging differences demonstrate that our\ngeneralized pancreas segmentation results comparable to mainstream in-domain\nvalidation performance (Dice: 84.07%). More importantly, it successfully\nimproves the results of the highly challenging cross-lesion generalized\npancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a\nresilient and efficient foundational technological support for pancreatic\ndisease management and wider medical applications. The codes will be released\nat https://github.com/SJTUBME-QianLab/Dual-Task-Seg."}
{"id": "2505.01657", "pdf": "https://arxiv.org/pdf/2505.01657", "abs": "https://arxiv.org/abs/2505.01657", "authors": ["Run Ling", "Wenji Wang", "Yuting Liu", "Guibing Guo", "Linying Jiang", "Xingwei Wang"], "title": "RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Personalized image generation is crucial for improving the user experience,\nas it renders reference images into preferred ones according to user visual\npreferences. Although effective, existing methods face two main issues. First,\nexisting methods treat all items in the user historical sequence equally when\nextracting user preferences, overlooking the varying semantic similarities\nbetween historical items and the reference item. Disproportionately high\nweights for low-similarity items distort users' visual preferences for the\nreference item. Second, existing methods heavily rely on consistency between\ngenerated and reference images to optimize the generation, which leads to\nunderfitting user preferences and hinders personalization. To address these\nissues, we propose Retrieval Augment Personalized Image GenerAtion guided by\nRecommendation (RAGAR). Our approach uses a retrieval mechanism to assign\ndifferent weights to historical items according to their similarities to the\nreference item, thereby extracting more refined users' visual preferences for\nthe reference item. Then we introduce a novel rank task based on the\nmulti-modal ranking model to optimize the personalization of the generated\nimages instead of forcing depend on consistency. Extensive experiments and\nhuman evaluations on three real-world datasets demonstrate that RAGAR achieves\nsignificant improvements in both personalization and semantic metrics compared\nto five baselines."}
{"id": "2505.01670", "pdf": "https://arxiv.org/pdf/2505.01670", "abs": "https://arxiv.org/abs/2505.01670", "authors": ["Christos Zangos", "Danish Ebadulla", "Thomas Christopher Sprague", "Ambuj Singh"], "title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "This work introduces a novel approach to fMRI-based visual image\nreconstruction using a subject-agnostic common representation space. We show\nthat the brain signals of the subjects can be aligned in this common space\nduring training to form a semantically aligned common brain. This is leveraged\nto demonstrate that aligning subject-specific lightweight modules to a\nreference subject is significantly more efficient than traditional end-to-end\ntraining methods. Our approach excels in low-data scenarios. We evaluate our\nmethods on different datasets, demonstrating that the common space is subject\nand dataset-agnostic."}
{"id": "2505.01709", "pdf": "https://arxiv.org/pdf/2505.01709", "abs": "https://arxiv.org/abs/2505.01709", "authors": ["Kaidong Zhang", "Rongtao Xu", "Pengzhen Ren", "Junfan Lin", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "project page: https://abliao.github.io/RoBridge/", "summary": "Operating robots in open-ended scenarios with diverse tasks is a crucial\nresearch and application direction in robotics. While recent progress in\nnatural language processing and large multimodal models has enhanced robots'\nability to understand complex instructions, robot manipulation still faces the\nprocedural skill dilemma and the declarative skill dilemma in open\nenvironments. Existing methods often compromise cognitive and executive\ncapabilities. To address these challenges, in this paper, we propose RoBridge,\na hierarchical intelligent architecture for general robotic manipulation. It\nconsists of a high-level cognitive planner (HCP) based on a large-scale\npre-trained vision-language model (VLM), an invariant operable representation\n(IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA).\nRoBridge maintains the declarative skill of VLM and unleashes the procedural\nskill of reinforcement learning, effectively bridging the gap between cognition\nand execution. RoBridge demonstrates significant performance improvements over\nexisting baselines, achieving a 75% success rate on new tasks and an 83%\naverage success rate in sim-to-real generalization using only five real-world\ndata samples per task. This work represents a significant step towards\nintegrating cognitive reasoning with physical execution in robotic systems,\noffering a new paradigm for general robotic manipulation."}
{"id": "2505.01741", "pdf": "https://arxiv.org/pdf/2505.01741", "abs": "https://arxiv.org/abs/2505.01741", "authors": ["Asmaa Abbas", "Mohamed Gaber", "Mohammed M. Abdelsamea"], "title": "CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification", "categories": ["eess.IV", "cs.CV"], "comment": "Published in: IEEE Transactions on Emerging Topics in Computing", "summary": "Curriculum learning strategies have been proven to be effective in various\napplications and have gained significant interest in the field of machine\nlearning. It has the ability to improve the final model's performance and\naccelerate the training process. However, in the medical imaging domain, data\nirregularities can make the recognition task more challenging and usually\nresult in misclassification between the different classes in the dataset.\nClass-decomposition approaches have shown promising results in solving such a\nproblem by learning the boundaries within the classes of the data set. In this\npaper, we present a novel convolutional neural network (CNN) training method\nbased on the curriculum learning strategy and the class decomposition approach,\nwhich we call CLOG-CD, to improve the performance of medical image\nclassification. We evaluated our method on four different imbalanced medical\nimage datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray,\nand histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights\nfrom the decomposition granularity of the classes, and the training is\naccomplished from descending to ascending order (i.e., anti-curriculum\ntechnique). We also investigated the classification performance of our proposed\nmethod based on different acceleration factors and pace function curricula. We\nused two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for\nCLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to\nimprove classification performance with an accuracy of 96.08% for the CXR\ndataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee\nX-ray, and 99.17% for the CRC dataset, compared to other training strategies.\nIn addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%,\nand 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets,\nrespectively"}
{"id": "2505.01755", "pdf": "https://arxiv.org/pdf/2505.01755", "abs": "https://arxiv.org/abs/2505.01755", "authors": ["Jiesong Bai", "Yuhao Yin", "Yihang Dong", "Xiaofeng Zhang", "Chi-Man Pun", "Xuhang Chen"], "title": "LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Lensless imaging stands out as a promising alternative to conventional\nlens-based systems, particularly in scenarios demanding ultracompact form\nfactors and cost-effective architectures. However, such systems are\nfundamentally governed by the Point Spread Function (PSF), which dictates how a\npoint source contributes to the final captured signal. Traditional lensless\ntechniques often require explicit calibrations and extensive pre-processing,\nrelying on static or approximate PSF models. These rigid strategies can result\nin limited adaptability to real-world challenges, including noise, system\nimperfections, and dynamic scene variations, thus impeding high-fidelity\nreconstruction. In this paper, we propose LensNet, an end-to-end deep learning\nframework that integrates spatial-domain and frequency-domain representations\nin a unified pipeline. Central to our approach is a learnable Coded Mask\nSimulator (CMS) that enables dynamic, data-driven estimation of the PSF during\ntraining, effectively mitigating the shortcomings of fixed or sparsely\ncalibrated kernels. By embedding a Wiener filtering component, LensNet refines\nglobal structure and restores fine-scale details, thus alleviating the\ndependency on multiple handcrafted pre-processing steps. Extensive experiments\ndemonstrate LensNet's robust performance and superior reconstruction quality\ncompared to state-of-the-art methods, particularly in preserving high-frequency\ndetails and attenuating noise. The proposed framework establishes a novel\nconvergence between physics-based modeling and data-driven learning, paving the\nway for more accurate, flexible, and practical lensless imaging solutions for\napplications ranging from miniature sensors to medical diagnostics. The link of\ncode is https://github.com/baijiesong/Lensnet."}
{"id": "2505.01768", "pdf": "https://arxiv.org/pdf/2505.01768", "abs": "https://arxiv.org/abs/2505.01768", "authors": ["Hui Lin", "Dong Zeng", "Qi Xie", "Zerui Mao", "Jianhua Ma", "Deyu Meng"], "title": "Continuous Filtered Backprojection by Learnable Interpolation Network", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages, 10 figures", "summary": "Accurate reconstruction of computed tomography (CT) images is crucial in\nmedical imaging field. However, there are unavoidable interpolation errors in\nthe backprojection step of the conventional reconstruction methods, i.e.,\nfiltered-back-projection based methods, which are detrimental to the accurate\nreconstruction. In this study, to address this issue, we propose a novel deep\nlearning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to\nenhance the reconstructed CT image quality, which achieves learnable\ninterpolation in the backprojection step of filtered backprojection (FBP) and\nalleviates the interpolation errors. Specifically, in the proposed LInFBP, we\nformulate every local piece of the latent continuous function of discrete\nsinogram data as a linear combination of selected basis functions, and learn\nthis continuous function by exploiting a deep network to predict the linear\ncombination coefficients. Then, the learned latent continuous function is\nexploited for interpolation in backprojection step, which first time takes the\nadvantage of deep learning for the interpolation in FBP. Extensive experiments,\nwhich encompass diverse CT scenarios, demonstrate the effectiveness of the\nproposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play\nability and generalization capability."}
{"id": "2505.01831", "pdf": "https://arxiv.org/pdf/2505.01831", "abs": "https://arxiv.org/abs/2505.01831", "authors": ["Haofan Wu", "Yin Huang", "Yuqing Wu", "Qiuyu Yang", "Bingfang Wang", "Li Zhang", "Muhammad Fahadullah Khan", "Ali Zia", "M. Saleh Memon", "Syed Sohail Bukhari", "Abdul Fattah Memon", "Daizong Ji", "Ya Zhang", "Ghulam Mustafa", "Yin Fang"], "title": "Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": "Under review at Neural Networks", "summary": "High-quality fundus images provide essential anatomical information for\nclinical screening and ophthalmic disease diagnosis. Yet, due to hardware\nlimitations, operational variability, and patient compliance, fundus images\noften suffer from low resolution and signal-to-noise ratio. Recent years have\nwitnessed promising progress in fundus image enhancement. However, existing\nworks usually focus on restoring structural details or global characteristics\nof fundus images, lacking a unified image enhancement framework to recover\ncomprehensive multi-scale information. Moreover, few methods pinpoint the\ntarget of image enhancement, e.g., lesions, which is crucial for medical\nimage-based diagnosis. To address these challenges, we propose a multi-scale\ntarget-aware representation learning framework (MTRL-FIE) for efficient fundus\nimage enhancement. Specifically, we propose a multi-scale feature encoder (MFE)\nthat employs wavelet decomposition to embed both low-frequency structural\ninformation and high-frequency details. Next, we design a structure-preserving\nhierarchical decoder (SHD) to fuse multi-scale feature embeddings for real\nfundus image restoration. SHD integrates hierarchical fusion and group\nattention mechanisms to achieve adaptive feature fusion while retaining local\nstructural smoothness. Meanwhile, a target-aware feature aggregation (TFA)\nmodule is used to enhance pathological regions and reduce artifacts.\nExperimental results on multiple fundus image datasets demonstrate the\neffectiveness and generalizability of MTRL-FIE for fundus image enhancement.\nCompared to state-of-the-art methods, MTRL-FIE achieves superior enhancement\nperformance with a more lightweight architecture. Furthermore, our approach\ngeneralizes to other ophthalmic image processing tasks without supervised\nfine-tuning, highlighting its potential for clinical applications."}
{"id": "2505.01854", "pdf": "https://arxiv.org/pdf/2505.01854", "abs": "https://arxiv.org/abs/2505.01854", "authors": ["Yuwen Chen", "Zafer Yildiz", "Qihang Li", "Yaqian Chen", "Haoyu Dong", "Hanxue Gu", "Nicholas Konz", "Maciej A. Mazurowski"], "title": "Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Manual annotation of volumetric medical images, such as magnetic resonance\nimaging (MRI) and computed tomography (CT), is a labor-intensive and\ntime-consuming process. Recent advancements in foundation models for video\nobject segmentation, such as Segment Anything Model 2 (SAM 2), offer a\npotential opportunity to significantly speed up the annotation process by\nmanually annotating one or a few slices and then propagating target masks\nacross the entire volume. However, the performance of SAM 2 in this context\nvaries. Our experiments show that relying on a single memory bank and attention\nmodule is prone to error propagation, particularly at boundary regions where\nthe target is present in the previous slice but absent in the current one. To\naddress this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel\narchitecture that integrates distinct short-term and long-term memory banks\nwith separate attention modules to improve segmentation accuracy. We evaluate\nSLM-SAM 2 on three public datasets covering organs, bones, and muscles across\nMRI and CT modalities. We show that the proposed method markedly outperforms\nthe default SAM 2, achieving average Dice Similarity Coefficient improvement of\n0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for\nthe initial adaptation, respectively. SLM-SAM 2 also exhibits stronger\nresistance to over-propagation, making a notable step toward more accurate\nautomated annotation of medical images for segmentation model development."}
{"id": "2505.01880", "pdf": "https://arxiv.org/pdf/2505.01880", "abs": "https://arxiv.org/abs/2505.01880", "authors": ["Junyan Wu", "Wenbo Xu", "Wei Lu", "Xiangyang Luo", "Rui Yang", "Shize Guo"], "title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "comment": "9pages, 5figures. This paper has been accepted for IJCAI2025", "summary": "Audio temporal forgery localization (ATFL) aims to find the precise forgery\nregions of the partial spoof audio that is purposefully modified. Existing ATFL\nmethods rely on training efficient networks using fine-grained annotations,\nwhich are obtained costly and challenging in real-world scenarios. To meet this\nchallenge, in this paper, we propose a progressive audio-language co-learning\nnetwork (LOCO) that adopts co-learning and self-supervision manners to prompt\nlocalization performance under weak supervision scenarios. Specifically, an\naudio-language co-learning module is first designed to capture forgery\nconsensus features by aligning semantics from temporal and global perspectives.\nIn this module, forgery-aware prompts are constructed by using utterance-level\nannotations together with learnable prompts, which can incorporate semantic\npriors into temporal content features dynamically. In addition, a forgery\nlocalization module is applied to produce forgery proposals based on fused\nforgery-class activation sequences. Finally, a progressive refinement strategy\nis introduced to generate pseudo frame-level labels and leverage supervised\nsemantic contrastive learning to amplify the semantic distinction between real\nand fake content, thereby continuously optimizing forgery-aware features.\nExtensive experiments show that the proposed LOCO achieves SOTA performance on\nthree public benchmarks."}
{"id": "2505.01884", "pdf": "https://arxiv.org/pdf/2505.01884", "abs": "https://arxiv.org/abs/2505.01884", "authors": ["Siddharth Kothari", "Srinivasan Murali", "Sankalp Kothari", "Ujjwal Verma", "Jaya Sreevalsan-Nair"], "title": "Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "21 pages, 15 figures, 2 tables", "summary": "Inland water body segmentation from Synthetic Aperture Radar (SAR) images is\nan important task needed for several applications, such as flood mapping. While\nSAR sensors capture data in all-weather conditions as high-resolution images,\ndifferentiating water and water-like surfaces from SAR images is not\nstraightforward. Inland water bodies, such as large river basins, have complex\ngeometry, which adds to the challenge of segmentation. U-Net is a widely used\ndeep learning model for land-water segmentation of SAR images. In practice,\nmanual annotation is often used to generate the corresponding water masks as\nground truth. Manual annotation of the images is prone to label noise owing to\ndata poisoning attacks, especially due to complex geometry. In this work, we\nsimulate manual errors in the form of adversarial attacks on the U-Net model\nand study the robustness of the model to human errors in annotation. Our\nresults indicate that U-Net can tolerate a certain level of corruption before\nits performance drops significantly. This finding highlights the crucial role\nthat the quality of manual annotations plays in determining the effectiveness\nof the segmentation model. The code and the new dataset, along with adversarial\nexamples for robust training, are publicly available. (Github link -\nhttps://github.com/GVCL/IWSeg-SAR-Poison.git)"}
{"id": "2505.01932", "pdf": "https://arxiv.org/pdf/2505.01932", "abs": "https://arxiv.org/abs/2505.01932", "authors": ["Xinmu Wang", "Xiang Gao", "Xiyun Song", "Heather Yu", "Zongfang Lin", "Liang Peng", "Xianfeng Gu"], "title": "OT-Talk: Animating 3D Talking Head with Optimal Transportation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Animating 3D head meshes using audio inputs has significant applications in\nAR/VR, gaming, and entertainment through 3D avatars. However, bridging the\nmodality gap between speech signals and facial dynamics remains a challenge,\noften resulting in incorrect lip syncing and unnatural facial movements. To\naddress this, we propose OT-Talk, the first approach to leverage optimal\ntransportation to optimize the learning model in talking head animation.\nBuilding on existing learning frameworks, we utilize a pre-trained Hubert model\nto extract audio features and a transformer model to process temporal\nsequences. Unlike previous methods that focus solely on vertex coordinates or\ndisplacements, we introduce Chebyshev Graph Convolution to extract geometric\nfeatures from triangulated meshes. To measure mesh dissimilarities, we go\nbeyond traditional mesh reconstruction errors and velocity differences between\nadjacent frames. Instead, we represent meshes as probability measures and\napproximate their surfaces. This allows us to leverage the sliced Wasserstein\ndistance for modeling mesh variations. This approach facilitates the learning\nof smooth and accurate facial motions, resulting in coherent and natural facial\nanimations. Our experiments on two public audio-mesh datasets demonstrate that\nour method outperforms state-of-the-art techniques both quantitatively and\nqualitatively in terms of mesh reconstruction accuracy and temporal alignment.\nIn addition, we conducted a user perception study with 20 volunteers to further\nassess the effectiveness of our approach."}
{"id": "2505.01996", "pdf": "https://arxiv.org/pdf/2505.01996", "abs": "https://arxiv.org/abs/2505.01996", "authors": ["Yiping Ji", "Hemanth Saratchandran", "Peyman Moghaddam", "Simon Lucey"], "title": "Always Skip Attention", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We highlight a curious empirical result within modern Vision Transformers\n(ViTs). Specifically, self-attention catastrophically fails to train unless it\nis used in conjunction with a skip connection. This is in contrast to other\nelements of a ViT that continue to exhibit good performance (albeit suboptimal)\nwhen skip connections are removed. Further, we show that this critical\ndependence on skip connections is a relatively new phenomenon, with previous\ndeep architectures (\\eg, CNNs) exhibiting good performance in their absence. In\nthis paper, we theoretically characterize that the self-attention mechanism is\nfundamentally ill-conditioned and is, therefore, uniquely dependent on skip\nconnections for regularization. Additionally, we propose Token Graying -- a\nsimple yet effective complement (to skip connections) that further improves the\ncondition of input tokens. We validate our approach in both supervised and\nself-supervised training methods."}
{"id": "2505.02001", "pdf": "https://arxiv.org/pdf/2505.02001", "abs": "https://arxiv.org/abs/2505.02001", "authors": ["Vineesh Kumar Reddy Mondem"], "title": "Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework", "categories": ["eess.IV", "cs.CV", "94A08", "I.4.0; I.4.9; I.2.10"], "comment": "19 pages,2 figures,2 tables and biblography with similar papers with\n  some valid information", "summary": "Traditional image quality assessment metrics like Mean Squared Error and\nStructural Similarity Index often fail to reflect perceptual quality under\ncomplex distortions. We propose the Hybrid Image Resolution Quality Metric\n(HIRQM), integrating statistical, multi-scale, and deep learning-based methods\nfor a comprehensive quality evaluation. HIRQM combines three components:\nProbability Density Function for local pixel distribution analysis, Multi-scale\nFeature Similarity for structural integrity across resolutions, and\nHierarchical Deep Image Features using a pre-trained VGG16 network for semantic\nalignment with human perception. A dynamic weighting mechanism adapts component\ncontributions based on image characteristics like brightness and variance,\nenhancing flexibility across distortion types. Our contributions include a\nunified metric and dynamic weighting for better perceptual alignment. Evaluated\non TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations\nof 0.92 and 0.90, outperforming traditional metrics. It excels in handling\nnoise, blur, and compression artifacts, making it valuable for image processing\napplications like compression and restoration."}
{"id": "2505.02052", "pdf": "https://arxiv.org/pdf/2505.02052", "abs": "https://arxiv.org/abs/2505.02052", "authors": ["Lala Shakti Swarup Ray", "Lars Krupp", "Vitor Fortes Rey", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Sensor-based human activity recognition (HAR) has predominantly focused on\nInertial Measurement Units and vision data, often overlooking the capabilities\nunique to pressure sensors, which capture subtle body dynamics and shifts in\nthe center of mass. Despite their potential for postural and balance-based\nactivities, pressure sensors remain underutilized in the HAR domain due to\nlimited datasets. To bridge this gap, we propose to exploit generative\nfoundation models with pressure-specific HAR techniques. Specifically, we\npresent a bidirectional Text$\\times$Pressure model that uses generative\nfoundation models to interpret pressure data as natural language. TxP\naccomplishes two tasks: (1) Text2Pressure, converting activity text\ndescriptions into pressure sequences, and (2) Pressure2Text, generating\nactivity descriptions and classifications from dynamic pressure maps.\nLeveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on\nour synthetic PressLang dataset, containing over 81,100 text-pressure pairs.\nValidated on real-world data for activities such as yoga and daily tasks, TxP\nprovides novel approaches to data augmentation and classification grounded in\natomic actions. This consequently improved HAR performance by up to 12.4\\% in\nmacro F1 score compared to the state-of-the-art, advancing pressure-based HAR\nwith broader applications and deeper insights into human movement."}
{"id": "2505.02094", "pdf": "https://arxiv.org/pdf/2505.02094", "abs": "https://arxiv.org/abs/2505.02094", "authors": ["Runyi Yu", "Yinhuai Wang", "Qihan Zhao", "Hok Wai Tsui", "Jingbo Wang", "Ping Tan", "Qifeng Chen"], "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We address a fundamental challenge in Reinforcement Learning from Interaction\nDemonstration (RLID): demonstration noise and coverage limitations. While\nexisting data collection approaches provide valuable interaction\ndemonstrations, they often yield sparse, disconnected, and noisy trajectories\nthat fail to capture the full spectrum of possible skill variations and\ntransitions. Our key insight is that despite noisy and sparse demonstrations,\nthere exist infinite physically feasible trajectories that naturally bridge\nbetween demonstrated skills or emerge from their neighboring states, forming a\ncontinuous space of possible skill variations and transitions. Building upon\nthis insight, we present two data augmentation techniques: a Stitched\nTrajectory Graph (STG) that discovers potential transitions between\ndemonstration skills, and a State Transition Field (STF) that establishes\nunique connections for arbitrary states within the demonstration neighborhood.\nTo enable effective RLID with augmented data, we develop an Adaptive Trajectory\nSampling (ATS) strategy for dynamic curriculum generation and a historical\nencoding mechanism for memory-dependent skill learning. Our approach enables\nrobust skill acquisition that significantly generalizes beyond the reference\ndemonstrations. Extensive experiments across diverse interaction tasks\ndemonstrate substantial improvements over state-of-the-art methods in terms of\nconvergence stability, generalization capability, and recovery robustness."}
{"id": "2505.02147", "pdf": "https://arxiv.org/pdf/2505.02147", "abs": "https://arxiv.org/abs/2505.02147", "authors": ["Prajwal Thapa", "Mridul Sharma", "Jinu Nyachhyon", "Yagya Raj Pandeya"], "title": "Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora", "categories": ["cs.LG", "cs.CV", "I.4.9"], "comment": "12 pages, 6 figures, 5 tables", "summary": "Herb classification presents a critical challenge in botanical research,\nparticularly in regions with rich biodiversity such as Nepal. This study\nintroduces a novel deep learning approach for classifying 60 different herb\nspecies using Convolutional Neural Networks (CNNs) and transfer learning\ntechniques. Using a manually curated dataset of 12,000 herb images, we\ndeveloped a robust machine learning model that addresses existing limitations\nin herb recognition methodologies. Our research employed multiple model\narchitectures, including DenseNet121, 50-layer Residual Network (ResNet50),\n16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2,\nand Vision Transformer (VIT), with DenseNet121 ultimately demonstrating\nsuperior performance. Data augmentation and regularization techniques were\napplied to mitigate overfitting and enhance the generalizability of the model.\nThis work advances herb classification techniques, preserving traditional\nbotanical knowledge and promoting sustainable herb utilization."}
{"id": "2505.02211", "pdf": "https://arxiv.org/pdf/2505.02211", "abs": "https://arxiv.org/abs/2505.02211", "authors": ["Peiqi Li", "Yincheng Gao", "Renxing Li", "Haojie Yang", "Yunyun Liu", "Boji Liu", "Jiahui Ni", "Ying Zhang", "Yulu Wu", "Xiaowei Fang", "Lehang Guo", "Liping Sun", "Jiangang Chen"], "title": "CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages, 10 figures, 4 tables", "summary": "Heterogeneous morphological features and data imbalance pose significant\nchallenges in rare thyroid carcinoma classification using ultrasound imaging.\nTo address this issue, we propose a novel multitask learning framework,\nChannel-Spatial Attention Synergy Network (CSASN), which integrates a\ndual-branch feature extractor - combining EfficientNet for local spatial\nencoding and ViT for global semantic modeling, with a cascaded channel-spatial\nattention refinement module. A residual multiscale classifier and dynamically\nweighted loss function further enhance classification stability and accuracy.\nTrained on a multicenter dataset comprising more than 2000 patients from four\nclinical institutions, our framework leverages a residual multiscale classifier\nand dynamically weighted loss function to enhance classification stability and\naccuracy. Extensive ablation studies demonstrate that each module contributes\nsignificantly to model performance, particularly in recognizing rare subtypes\nsuch as FTC and MTC carcinomas. Experimental results show that CSASN\noutperforms existing single-stream CNN or Transformer-based models, achieving a\nsuperior balance between precision and recall under class-imbalanced\nconditions. This framework provides a promising strategy for AI-assisted\nthyroid cancer diagnosis."}
{"id": "2505.02304", "pdf": "https://arxiv.org/pdf/2505.02304", "abs": "https://arxiv.org/abs/2505.02304", "authors": ["Siyu Liang", "Yunan Li", "Wentian Xin", "Huizhou Chen", "Xujie Liu", "Kang Liu", "Qiguang Miao"], "title": "Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition", "categories": ["cs.CL", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Sign language recognition (SLR) faces fundamental challenges in creating\naccurate annotations due to the inherent complexity of simultaneous manual and\nnon-manual signals. To the best of our knowledge, this is the first work to\nintegrate generative large language models (LLMs) into SLR tasks. We propose a\nnovel Generative Sign-description Prompts Multi-positive Contrastive learning\n(GSP-MC) method that leverages retrieval-augmented generation (RAG) with\ndomain-specific LLMs, incorporating multi-step prompt engineering and\nexpert-validated sign language corpora to produce precise multipart\ndescriptions. The GSP-MC method also employs a dual-encoder architecture to\nbidirectionally align hierarchical skeleton features with multiple text\ndescriptions (global, synonym, and part level) through probabilistic matching.\nOur approach combines global and part-level losses, optimizing KL divergence to\nensure robust alignment across all relevant text-skeleton pairs while capturing\nboth sign-level semantics and detailed part dynamics. Experiments demonstrate\nstate-of-the-art performance against existing methods on the Chinese SLR500\n(reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's\ncross-lingual effectiveness highlight its potential for developing inclusive\ncommunication technologies."}
{"id": "2505.02350", "pdf": "https://arxiv.org/pdf/2505.02350", "abs": "https://arxiv.org/abs/2505.02350", "authors": ["Bobo Lian", "Dandan Wang", "Chenjian Wu", "Minxin Chen"], "title": "Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Point cloud surface representation is a fundamental problem in computer\ngraphics and vision. This paper presents a machine learning approach for\napproximating the signed distance function (SDF) of a point cloud using sparse\nellipsoidal radial basis function networks, enabling a compact and accurate\nsurface representation. Given the SDF values defined on the grid points\nconstructed from the point cloud, our method approximates the SDF accurately\nwith as few ellipsoidal radial basis functions (ERBFs) as possible, i.e.,\nrepresent the SDF of a point cloud by sparse ERBFs. To balance sparsity and\napproximation precision, a dynamic multi-objective optimization strategy is\nintroduced, which adaptively adds the regularization terms and jointly\noptimizes the weights, centers, shapes, and orientations of ERBFs. To improve\ncomputational efficiency, a nearest-neighbor-based data structure is employed,\nrestricting function calculations to points near each Gaussian kernel center.\nThe computations for each kernel are further parallelized on CUDA, which\nsignificantly improves the optimization speed. Additionally, a hierarchical\noctree-based refinement strategy is designed for training. Specifically, the\ninitialization and optimization of network parameters are conducted using\ncoarse grid points in the octree lattice structure. Subsequently, fine lattice\npoints are progressively incorporated to accelerate model convergence and\nenhance training efficiency. Extensive experiments on multiple benchmark\ndatasets demonstrate that our method outperforms previous sparse representation\napproaches in terms of accuracy, robustness, and computational efficiency. The\ncorresponding code is publicly available at\nhttps://github.com/lianbobo/SE-RBFNet.git."}
{"id": "2505.02369", "pdf": "https://arxiv.org/pdf/2505.02369", "abs": "https://arxiv.org/abs/2505.02369", "authors": ["Juyoung Yun"], "title": "Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IT", "cs.NE", "math.IT"], "comment": null, "summary": "Generalizing well in deep neural networks remains a core challenge,\nparticularly due to their tendency to converge to sharp minima that degrade\nrobustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking\nflatter minima but perturbs parameters using the full gradient, which can\ninclude statistically insignificant directions. We propose ZSharp, a simple yet\neffective extension to SAM that applies layer-wise Z-score normalization\nfollowed by percentile-based filtering to retain only statistically significant\ngradient components. This selective perturbation aligns updates with\ncurvature-sensitive directions, enhancing generalization without requiring\narchitectural changes. ZSharp introduces only one additional hyperparameter,\nthe percentile threshold, and remains fully compatible with existing SAM\nvariants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet,\nVGG, and Vision Transformers show that ZSharp consistently outperforms SAM and\nits variants in test accuracy, particularly on deeper and transformer-based\nmodels. These results demonstrate that ZSharp is a principled and lightweight\nimprovement for sharpness-aware optimization."}
{"id": "2505.02385", "pdf": "https://arxiv.org/pdf/2505.02385", "abs": "https://arxiv.org/abs/2505.02385", "authors": ["Lei Xie", "Huajun Zhou", "Junxiong Huang", "Jiahao Huang", "Qingrun Zeng", "Jianzhong He", "Jiawei Zhang", "Baohua Fan", "Mingchu Li", "Guoqiang Xie", "Hao Chen", "Yuanjing Feng"], "title": "An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The segmentation of cranial nerves (CNs) tract provides a valuable\nquantitative tool for the analysis of the morphology and trajectory of\nindividual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which\ncombine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have\nachieved promising segmentation performance. However, it is laborious or even\ninfeasible to collect complete multimodal data in clinical practice due to\nlimitations in equipment, user privacy, and working conditions. In this work,\nwe propose a novel arbitrary-modal fusion network for volumetric CNs tract\nsegmentation, called CNTSeg-v2, which trains one model to handle different\ncombinations of available modalities. Instead of directly combining all the\nmodalities, we select T1-weighted (T1w) images as the primary modality due to\nits simplicity in data acquisition and contribution most to the results, which\nsupervises the information selection of other auxiliary modalities. Our model\nencompasses an Arbitrary-Modal Collaboration Module (ACM) designed to\neffectively extract informative features from other auxiliary modalities,\nguided by the supervision of T1w images. Meanwhile, we construct a Deep\nDistance-guided Multi-stage (DDM) decoder to correct small errors and\ndiscontinuities through signed distance maps to improve segmentation accuracy.\nWe evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the\nclinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental\nresults show that our CNTSeg-v2 achieves state-of-the-art segmentation\nperformance, outperforming all competing methods."}
{"id": "2505.02396", "pdf": "https://arxiv.org/pdf/2505.02396", "abs": "https://arxiv.org/abs/2505.02396", "authors": ["Kennard Norbert Sudiardjo", "Islam Nur Alam", "Wilson Wijaya", "Lili Ayu Wulandhari"], "title": "Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pneumonia Diagnosis, though it is crucial for an effective treatment, it can\nbe hampered by uncertainty. This uncertainty starts to arise due to some\nfactors like atypical presentations, limitations of diagnostic tools such as\nchest X-rays, and the presence of co-existing respiratory conditions. This\nresearch proposes one of the supervised learning methods, CNN. Using\nMobileNetV2 as the pre-trained one with ResNet101V2 architecture and using\nKeras API as the built from scratch model, for identifying lung diseases\nespecially pneumonia. The datasets used in this research were obtained from the\nwebsite through Kaggle. The result shows that by implementing CNN MobileNetV2\nand CNN from scratch the result is promising. While validating data,\nMobileNetV2 performs with stability and minimal overfitting, while the training\naccuracy increased to 84.87% later it slightly decreased to 78.95%, with\nincreasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is\nmore stable. Although it takes more time to train each epoch. Meanwhile, after\nthe 10th epoch, the Scratch model displayed more instability and overfitting\ndespite having higher validation accuracy, training accuracy decreased\nsignificantly to 78.12% and the validation loss increased from 0.5698 to\n1.1809. With these results, ResNet101V2 offers stability, and the Scratch model\noffers high accuracy."}
{"id": "2505.02405", "pdf": "https://arxiv.org/pdf/2505.02405", "abs": "https://arxiv.org/abs/2505.02405", "authors": ["Mario A. V. Saucedo", "Vignesh Kottayam Viswanathan", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "Estimating Commonsense Scene Composition on Belief Scene Graphs", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at ICRA25", "summary": "This work establishes the concept of commonsense scene composition, with a\nfocus on extending Belief Scene Graphs by estimating the spatial distribution\nof unseen objects. Specifically, the commonsense scene composition capability\nrefers to the understanding of the spatial relationships among related objects\nin the scene, which in this article is modeled as a joint probability\ndistribution for all possible locations of the semantic object class. The\nproposed framework includes two variants of a Correlation Information (CECI)\nmodel for learning probability distributions: (i) a baseline approach based on\na Graph Convolutional Network, and (ii) a neuro-symbolic extension that\nintegrates a spatial ontology based on Large Language Models (LLMs).\nFurthermore, this article provides a detailed description of the dataset\ngeneration process for such tasks. Finally, the framework has been validated\nthrough multiple runs on simulated data, as well as in a real-world indoor\nenvironment, demonstrating its ability to spatially interpret scenes across\ndifferent room types."}
{"id": "2505.02476", "pdf": "https://arxiv.org/pdf/2505.02476", "abs": "https://arxiv.org/abs/2505.02476", "authors": ["Hubert Padusinski", "Christian Steinhauser", "Christian Scherl", "Julian Gaal", "Jacob Langner"], "title": "Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation", "categories": ["cs.RO", "cs.CV", "eess.IV"], "comment": "Pre-print for IEEE IAVVC 2025", "summary": "The validation of LiDAR-based perception of intelligent mobile systems\noperating in open-world applications remains a challenge due to the variability\nof real environmental conditions. Virtual simulations allow the generation of\narbitrary scenes under controlled conditions but lack physical sensor\ncharacteristics, such as intensity responses or material-dependent effects. In\ncontrast, real-world data offers true sensor realism but provides less control\nover influencing factors, hindering sufficient validation. Existing approaches\naddress this problem with augmentation of real-world point cloud data by\ntransferring objects between scenes. However, these methods do not consider\nvalidation and remain limited in controllability because they rely on empirical\ndata. We solve these limitations by proposing Point Cloud Recombination, which\nsystematically augments captured point cloud scenes by integrating point clouds\nacquired from physical target objects measured in controlled laboratory\nenvironments. Thus enabling the creation of vast amounts and varieties of\nrepeatable, physically accurate test scenes with respect to phenomena-aware\nocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we\ndemonstrate the augmentation of real-world urban and rural scenes with humanoid\ntargets featuring varied clothing and poses, for repeatable positioning. We\nshow that the recombined scenes closely match real sensor outputs, enabling\ntargeted testing, scalable failure analysis, and improved system safety. By\nproviding controlled yet sensor-realistic data, our method enables trustworthy\nconclusions about the limitations of specific sensors in compound with their\nalgorithms, e.g., object detection."}
{"id": "2505.02628", "pdf": "https://arxiv.org/pdf/2505.02628", "abs": "https://arxiv.org/abs/2505.02628", "authors": ["Yiqun Lin", "Hualiang Wang", "Jixiang Chen", "Jiewen Yang", "Jiarong Guo", "Xiaomeng Li"], "title": "DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in\nthe medical field, while the high radiation exposure required for high-quality\nimaging raises significant concerns, particularly for vulnerable populations.\nSparse-view reconstruction reduces radiation by using fewer X-ray projections\nwhile maintaining image quality, yet existing methods face challenges such as\nhigh computational demands and poor generalizability to different datasets. To\novercome these limitations, we propose DeepSparse, the first foundation model\nfor sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional\nCross-Scale Embedding), a novel network that integrates multi-view 2D features\nand multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View\nSampling Pretraining) framework, which pretrains the model on large datasets\nwith both sparse-view and dense-view projections, and a two-step finetuning\nstrategy to adapt and refine the model for new datasets. Extensive experiments\nand ablation studies demonstrate that our proposed DeepSparse achieves superior\nreconstruction quality compared to state-of-the-art methods, paving the way for\nsafer and more efficient CBCT imaging."}
{"id": "2505.02664", "pdf": "https://arxiv.org/pdf/2505.02664", "abs": "https://arxiv.org/abs/2505.02664", "authors": ["Ali Rashidi Moghadam", "Sayedmohammadreza Rastegari", "Mehdi Tale Masouleh", "Ahmad Kalhor"], "title": "Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "9 Pages, 6 figures", "summary": "Grasp pose detection in cluttered, real-world environments remains a\nsignificant challenge due to noisy and incomplete sensory data combined with\ncomplex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0)\nmethod, a lightweight yet highly effective hypothesis-and-test robotics\ngrasping framework which leverages an ensemble of Graph Neural Networks for\nefficient geometric reasoning from point cloud data. Building on the success of\nGtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp\ndetection but was limited by assumptions of complete, noise-free point clouds\nand 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to\nefficiently produce 7-Dof grasp candidates. Candidates are assessed with an\nensemble Graph Neural Network model which includes points within the gripper\njaws (inside points) and surrounding contextual points (outside points). This\nimproved representation boosts grasp detection performance over previous\nmethods using the same generator. GtG 2.0 shows up to a 35% improvement in\nAverage Precision on the GraspNet-1Billion benchmark compared to\nhypothesis-and-test and Graph Neural Network-based methods, ranking it among\nthe top three frameworks. Experiments with a 3-Dof Delta Parallel robot and\nKinect-v1 camera show a success rate of 91% and a clutter completion rate of\n100%, demonstrating its flexibility and reliability."}
{"id": "2505.02705", "pdf": "https://arxiv.org/pdf/2505.02705", "abs": "https://arxiv.org/abs/2505.02705", "authors": ["Binghong Chen", "Tingting Chai", "Wei Jiang", "Yuanrong Xu", "Guanglu Zhou", "Xiangqian Wu"], "title": "Multi-View Learning with Context-Guided Receptance for Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IJCAI 2025, code will be available at\n  https://github.com/Seeker98/CRWKV", "summary": "Image denoising is essential in low-level vision applications such as\nphotography and automated driving. Existing methods struggle with\ndistinguishing complex noise patterns in real-world scenes and consume\nsignificant computational resources due to reliance on Transformer-based\nmodels. In this work, the Context-guided Receptance Weighted Key-Value (\\M)\nmodel is proposed, combining enhanced multi-view feature integration with\nefficient sequence modeling. Our approach introduces the Context-guided Token\nShift (CTS) paradigm, which effectively captures local spatial dependencies and\nenhance the model's ability to model real-world noise distributions.\nAdditionally, the Frequency Mix (FMix) module extracting frequency-domain\nfeatures is designed to isolate noise in high-frequency spectra, and is\nintegrated with spatial representations through a multi-view learning process.\nTo improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is\nadopted, enabling full pixel-sequence interaction with linear complexity while\novercoming the causal selection constraints. The model is validated on multiple\nreal-world image denoising datasets, outperforming the existing\nstate-of-the-art methods quantitatively and reducing inference time up to 40\\%.\nQualitative results further demonstrate the ability of our model to restore\nfine details in various scenes."}
{"id": "2505.02833", "pdf": "https://arxiv.org/pdf/2505.02833", "abs": "https://arxiv.org/abs/2505.02833", "authors": ["Yanjie Ze", "Zixuan Chen", "JoÃ£o Pedro AraÃºjo", "Zi-ang Cao", "Xue Bin Peng", "Jiajun Wu", "C. Karen Liu"], "title": "TWIST: Teleoperated Whole-Body Imitation System", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Project website: https://humanoid-teleop.github.io", "summary": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step\ntoward developing general-purpose robotic intelligence, with human motion\nproviding an ideal interface for controlling all degrees of freedom. Yet, most\ncurrent humanoid teleoperation systems fall short of enabling coordinated\nwhole-body behavior, typically limiting themselves to isolated locomotion or\nmanipulation tasks. We present the Teleoperated Whole-Body Imitation System\n(TWIST), a system for humanoid teleoperation through whole-body motion\nimitation. We first generate reference motion clips by retargeting human motion\ncapture data to the humanoid robot. We then develop a robust, adaptive, and\nresponsive whole-body controller using a combination of reinforcement learning\nand behavior cloning (RL+BC). Through systematic analysis, we demonstrate how\nincorporating privileged future motion frames and real-world motion capture\n(MoCap) data improves tracking accuracy. TWIST enables real-world humanoid\nrobots to achieve unprecedented, versatile, and coordinated whole-body motor\nskills--spanning whole-body manipulation, legged manipulation, locomotion, and\nexpressive movement--using a single unified neural network controller. Our\nproject website: https://humanoid-teleop.github.io"}
