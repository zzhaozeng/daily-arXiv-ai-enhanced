<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.AI](#cs.AI) [Total: 41]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [What Kind of Reasoning (if any) is an LLM actually doing? On the Stochastic Nature and Abductive Appearance of Large Language Models](https://arxiv.org/abs/2512.10080)
*Luciano Floridi,Jessica Morley,Claudio Novelli,David Watson*

Main category: cs.CL

TL;DR: 本文探讨了当前基于token-completion方法的大型语言模型（LLMs）的推理机制，指出其本质是随机的文本生成，而非真正的溯因推理，强调了对其输出需批判性评估。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs的推理方式，明确其与溯因推理的异同，并评估其实际应用中的限制。

Method: 通过分析LLMs的随机性及其与人类溯因推理的比较，结合实例说明其生成文本的方式。

Result: LLMs可以生成看似合理的解释和常识推理，但其输出缺乏真实性、语义基础、验证和理解。

Conclusion: LLMs在辅助生成想法和支持人类思考方面具有潜力，但其输出必须经过批判性评估，因为它们无法识别或验证真相。

Abstract: This article looks at how reasoning works in current Large Language Models (LLMs) that function using the token-completion method. It examines their stochastic nature and their similarity to human abductive reasoning. The argument is that these LLMs create text based on learned patterns rather than performing actual abductive reasoning. When their output seems abductive, this is largely because they are trained on human-generated texts that include reasoning structures. Examples are used to show how LLMs can produce plausible ideas, mimic commonsense reasoning, and give explanatory answers without being grounded in truth, semantics, verification, or understanding, and without performing any real abductive reasoning. This dual nature, where the models have a stochastic base but appear abductive in use, has important consequences for how LLMs are evaluated and applied. They can assist with generating ideas and supporting human thinking, but their outputs must be critically assessed because they cannot identify truth or verify their explanations. The article concludes by addressing five objections to these points, noting some limitations in the analysis, and offering an overall evaluation.

</details>


### [2] [Generate-Then-Validate: A Novel Question Generation Approach Using Small Language Models](https://arxiv.org/abs/2512.10110)
*Yumou Wei,John Stamper,Paulo F. Carvalho*

Main category: cs.CL

TL;DR: 本文探讨了小型语言模型在自动问题生成中的应用，提出了一种结合文本生成和概率推理的新方法，并通过生成-验证策略生成高质量问题。


<details>
  <summary>Details</summary>
Motivation: 当前学习分析研究中，大规模语言模型占据主导地位，但小型语言模型在资源消耗和特定任务适应性上具有潜在优势，因此探索其在问题生成中的有效性。

Method: 提出一种新颖的问题生成流程，利用小型语言模型的文本生成和概率推理能力，采用“生成-验证”策略，通过扩展生成和基于概率推理的选择性验证来优化候选问题。

Result: 通过人类专家和大型语言模型的两项评估研究，结果表明生成的问题具有清晰的答案，并且与学习目标高度一致，验证了方法的有效性。

Conclusion: 在良好设计的流程引导下，小型语言模型能够有效生成高质量问题，展示了其在学习分析研究中的潜力。

Abstract: We explore the use of small language models (SLMs) for automatic question generation as a complement to the prevalent use of their large counterparts in learning analytics research. We present a novel question generation pipeline that leverages both the text generation and the probabilistic reasoning abilities of SLMs to generate high-quality questions. Adopting a "generate-then-validate" strategy, our pipeline first performs expansive generation to create an abundance of candidate questions and refine them through selective validation based on novel probabilistic reasoning. We conducted two evaluation studies, one with seven human experts and the other with a large language model (LLM), to assess the quality of the generated questions. Most judges (humans or LLMs) agreed that the generated questions had clear answers and generally aligned well with the intended learning objectives. Our findings suggest that an SLM can effectively generate high-quality questions when guided by a well-designed pipeline that leverages its strengths.

</details>


### [3] [Workflow is All You Need: Escaping the "Statistical Smoothing Trap" via High-Entropy Information Foraging and Adversarial Pacing](https://arxiv.org/abs/2512.10121)
*Zhongjie Jiang*

Main category: cs.CL

TL;DR: 论文提出DeepNews框架解决长文本生成中的“不可能三角”，通过模拟专业记者认知过程，实现低幻觉、逻辑连贯和个性化表达。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在垂直领域长文本生成中面临低幻觉、深度逻辑连贯和个性化表达三者难以兼得的“不可能三角”问题，根源在于统计平滑陷阱，忽视专家级写作所需的高熵信息获取和结构化认知过程。

Method: 提出DeepNews框架，包含三大模块：1）基于信息觅食理论的双粒度检索机制；2）利用领域知识库和原子块的模式引导战略规划；3）采用Rhythm Break和Logic Fog等策略的对抗性约束提示。

Result: 实验发现深度金融报道存在知识悬崖：检索内容低于15,000字符时内容真实性骤降，超过30,000字符高冗余输入可使无幻觉率稳定在85%以上。在中文科技媒体盲测中，DeepNews投稿接受率达25%，显著优于GPT-5零样本生成的0%。

Conclusion: DeepNews框架通过模拟专家认知过程，有效解决了垂直领域长文本生成的核心瓶颈，在保持逻辑严谨的同时实现了低幻觉和个性化表达，为专业级文本生成提供了新范式。

Abstract: Central to long-form text generation in vertical domains is the "impossible trinity" confronting current large language models (LLMs): the simultaneous achievement of low hallucination, deep logical coherence, and personalized expression. This study establishes that this bottleneck arises from existing generative paradigms succumbing to the Statistical Smoothing Trap, a phenomenon that overlooks the high-entropy information acquisition and structured cognitive processes integral to expert-level writing. To address this limitation, we propose the DeepNews Framework, an agentic workflow that explicitly models the implicit cognitive processes of seasoned financial journalists. The framework integrates three core modules: first, a dual-granularity retrieval mechanism grounded in information foraging theory, which enforces a 10:1 saturated information input ratio to mitigate hallucinatory outputs; second, schema-guided strategic planning, a process leveraging domain expert knowledge bases (narrative schemas) and Atomic Blocks to forge a robust logical skeleton; third, adversarial constraint prompting, a technique deploying tactics including Rhythm Break and Logic Fog to disrupt the probabilistic smoothness inherent in model-generated text. Experiments delineate a salient Knowledge Cliff in deep financial reporting: content truthfulness collapses when retrieved context falls below 15,000 characters, while a high-redundancy input exceeding 30,000 characters stabilizes the Hallucination-Free Rate (HFR) above 85%. In an ecological validity blind test conducted with a top-tier Chinese technology media outlet, the DeepNews system--built on a previous-generation model (DeepSeek-V3-0324)-achieved a 25% submission acceptance rate, significantly outperforming the 0% acceptance rate of zero-shot generation by a state-of-the-art (SOTA) model (GPT-5).

</details>


### [4] [PARAN: Persona-Augmented Review ANswering system on Food Delivery Review Dataset](https://arxiv.org/abs/2512.10148)
*Moonsoo Park,Jeongseok Yun,Bohyung Kim*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段提示框架，通过从短评中推断用户画像（包括显性和隐性特征），并融合到生成提示中，以生成个性化评论回复。


<details>
  <summary>Details</summary>
Motivation: 个性化回复在用户信息有限的场景下（如外卖平台）尤为重要，传统大模型因缺乏上下文信息常生成泛泛回复，导致效果不佳。

Method: 采用两阶段提示框架：首先从评论中推断用户画像（显性如偏好，隐性如风格），再将这些特征融入生成提示，并通过调节解码温度提升多样性与准确性。

Result: 在真实韩国外卖数据集上评估，该方法在精确度、多样性和语义一致性方面表现良好。

Conclusion: 该方法无需微调即可有效提升自动化回复的相关性和个性化，具有实际应用潜力。

Abstract: Personalized review response generation presents a significant challenge in domains where user information is limited, such as food delivery platforms. While large language models (LLMs) offer powerful text generation capabilities, they often produce generic responses when lacking contextual user data, reducing engagement and effectiveness. In this work, we propose a two-stage prompting framework that infers both explicit (e.g., user-stated preferences) and implicit (e.g., demographic or stylistic cues) personas directly from short review texts. These inferred persona attributes are then incorporated into the response generation prompt to produce user-tailored replies. To encourage diverse yet faithful generations, we adjust decoding temperature during inference. We evaluate our method using a real-world dataset collected from a Korean food delivery app, and assess its impact on precision, diversity, and semantic consistency. Our findings highlight the effectiveness of persona-augmented prompting in enhancing the relevance and personalization of automated responses without requiring model fine-tuning.

</details>


### [5] [AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding](https://arxiv.org/abs/2512.10195)
*Gyutaek Oh,Sangjoon Park,Byung-Hoon Kim*

Main category: cs.CL

TL;DR: 本文介绍了AutoMedic，一种多智能体模拟框架，用于自动评估大型语言模型在临床医学对话中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在医学领域的安全和可信应用，尤其是在动态和交互式临床多轮对话情境中的有效性，仍存在许多未探索的方面。

Method: 提出AutoMedic框架，将静态QA数据集转化为虚拟病人档案，实现基于LLM智能体的多轮临床对话，并使用CARE指标评估表现。

Result: AutoMedic被验证为有效的自动化评估框架，其评估结果得到了人类专家的认可。

Conclusion: AutoMedic提供了多角度的评估标准，为在对话医疗应用中的LLM开发提供了实用指南。

Abstract: Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.

</details>


### [6] [Multilingual VLM Training: Adapting an English-Trained VLM to French](https://arxiv.org/abs/2512.10336)
*Jules Lahmi,Alexis Roger*

Main category: cs.CL

TL;DR: 本文探讨了将英语训练的视觉-语言模型（VLM）适配到不同语言的挑战，并比较了不同方法的性能和计算成本。


<details>
  <summary>Details</summary>
Motivation: 人工智能，尤其是视觉-语言模型（VLMs）的发展，主要局限于英语，限制了非英语使用者的访问。因此，有必要将这些能力扩展到更广泛的语言。

Method: 研究考虑了基于翻译的流程、LoRA微调和分离视觉适应与语言适应的两阶段微调策略，并使用翻译后的多模态基准和本地专家的评估来评价这些方法。

Result: 数据集翻译仍然是多语言VLM性能的主要瓶颈，数据质量限制了训练和评估的效果。

Conclusion: 未来的努力应集中在母语数据集的收集和翻译策略的改进上。

Abstract: Artificial intelligence has made great progress in recent years, particularly in the development of Vision--Language Models (VLMs) that understand both visual and textual data. However, these advancements remain largely limited to English, reducing their accessibility for non--English speakers. It is essential to extend these capabilities to a broader range of languages. This paper explores the challenges of adapting an English-trained VLM to different languages. To this end, we will explore and compare different methods for their performance and computational cost. We consider a translation-based pipeline, LoRA finetuning, and a two-stage finetuning strategy that separates vision adaptation from language adaptation. To evaluate these methods, we use a combination of standard multimodal benchmarks translated into the target language and manual assessments by native experts. The results reveal that dataset translation remains a major bottleneck in multilingual VLM performance, with data quality limiting the effectiveness of training and evaluation. These findings suggest that future efforts should focus on native-language dataset collection and improved translation strategies.

</details>


### [7] [Sliding Window Attention Adaptation](https://arxiv.org/abs/2512.10411)
*Yijiong Yu,Jiale Liu,Qingyun Wu,Huazheng Wang,Ji Pei*

Main category: cs.CL

TL;DR: 本文研究了如何在不进行预训练的情况下，将全注意力（FA）预训练的大语言模型适配到滑动窗口注意力（SWA）。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的自注意力机制在输入长度上呈二次方增长，导致长文本推理成本高昂。SWA将复杂度降至线性，但与FA预训练的模型在推理时存在训练-推理不匹配问题，导致性能下降。

Method: 提出了滑动窗口注意力适配（SWAA），结合五种方法：仅在预填充期间应用SWA、保留“sink”标记、交替FA/SWA层、思维链（CoT）和微调。

Result: 实验表明，SWA适配是可行的但非简单：单一方法不足，特定组合能有效恢复原始长文本性能。

Conclusion: 分析了不同SWAA配置的性能-效率权衡，并提供了针对各种场景的推荐方案。

Abstract: The self-attention mechanism in Transformer-based Large Language Models (LLMs) scales quadratically with input length, making long-context inference expensive. Sliding window attention (SWA) reduces this cost to linear complexity, but naively enabling complete SWA at inference-time for models pretrained with full attention (FA) causes severe long-context performance degradation due to training-inference mismatch. This makes us wonder: Can FA-pretrained LLMs be well adapted to SWA without pretraining? We investigate this by proposing Sliding Window Attention Adaptation (SWAA), a set of practical recipes that combine five methods for better adaptation: (1) applying SWA only during prefilling; (2) preserving "sink" tokens; (3) interleaving FA/SWA layers; (4) chain-of-thought (CoT); and (5) fine-tuning. Our experiments show that SWA adaptation is feasible while non-trivial: no single method suffices, yet specific synergistic combinations effectively recover the original long-context performance. We further analyze the performance-efficiency trade-offs of different SWAA configurations and provide recommended recipes for diverse scenarios. Our code is available at https://github.com/yuyijiong/sliding-window-attention-adaptation

</details>


### [8] [Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers](https://arxiv.org/abs/2512.10422)
*Youmin Ko,Sungjong Seo,Hyunjoon Kim*

Main category: cs.CL

TL;DR: 提出了一种新的检索增强生成框架CoopRAG，旨在通过检索器和语言模型之间的协作，提高问题回答的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成输出时容易出现事实性错误，而现有的检索增强生成方法在简单和多跳问题回答中仍容易出现不正确的检索和幻觉。

Method: CoopRAG框架通过以下步骤实现：(i)将问题分解为子问题并生成包含不确定位置的推理链，(ii)检索与问题相关的文档，(iii)通过对比检索器的不同层对文档进行重排序，(iv)利用语言模型重构推理链。

Result: 在三个多跳问题回答数据集和一个简单问题回答数据集上的实验表明，CoopRAG在检索和问题回答性能上均优于现有的最先进方法。

Conclusion: CoopRAG通过检索器和语言模型的协作，以及检索器内部各层之间的协作，有效提高了问题回答的准确性，为解决大型语言模型的事实性错误问题提供了新的思路。

Abstract: Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available.\footnote{https://github.com/meaningful96/CoopRAG}

</details>


### [9] [Semantic Reconstruction of Adversarial Plagiarism: A Context-Aware Framework for Detecting and Restoring "Tortured Phrases" in Scientific Literature](https://arxiv.org/abs/2512.10435)
*Agniva Maiti,Prajwal Panth,Suresh Chandra Satapathy*

Main category: cs.CL

TL;DR: 提出SRAP框架，用于检测和还原科学文献中被对抗性改写工具生成的“tortured phrases”，并恢复原始术语。


<details>
  <summary>Details</summary>
Motivation: 自动改写工具通过生成“tortured phrases”来掩盖抄袭，现有检测方法因依赖静态词表或通用语言模型而效果有限。

Method: 采用两阶段架构：1) 使用SciBERT进行统计异常检测；2) 使用FAISS和SBERT进行基于源文本的语义还原。

Result: 在对抗性科学文本平行语料库上，SRAP实现了23.67%的还原准确率，显著优于零样本基线方法。

Conclusion: SRAP通过静态决策边界和向量检索有效检测并还原对抗性改写内容，支持科学文献的溯源分析。

Abstract: The integrity and reliability of scientific literature is facing a serious threat by adversarial text generation techniques, specifically from the use of automated paraphrasing tools to mask plagiarism. These tools generate "tortured phrases", statistically improbable synonyms (e.g. "counterfeit consciousness" for "artificial intelligence"), that preserve the local grammar while obscuring the original source. Most existing detection methods depend heavily on static blocklists or general-domain language models, which suffer from high false-negative rates for novel obfuscations and cannot determine the source of the plagiarized content. In this paper, we propose Semantic Reconstruction of Adversarial Plagiarism (SRAP), a framework designed not only to detect these anomalies but to mathematically recover the original terminology. We use a two-stage architecture: (1) statistical anomaly detection with a domain-specific masked language model (SciBERT) using token-level pseudo-perplexity, and (2) source-based semantic reconstruction using dense vector retrieval (FAISS) and sentence-level alignment (SBERT). Experiments on a parallel corpus of adversarial scientific text show that while zero-shot baselines fail completely (0.00 percent restoration accuracy), our retrieval-augmented approach achieves 23.67 percent restoration accuracy, significantly outperforming baseline methods. We also show that static decision boundaries are necessary for robust detection in jargon-heavy scientific text, since dynamic thresholding fails under high variance. SRAP enables forensic analysis by linking obfuscated expressions back to their most probable source documents.

</details>


### [10] [Enhancing Next-Generation Language Models with Knowledge Graphs: Extending Claude, Mistral IA, and GPT-4 via KG-BERT](https://arxiv.org/abs/2512.10440)
*Nour El Houda Ben Chaabene,Hamza Hammami*

Main category: cs.CL

TL;DR: 通过KG-BERT整合知识图谱（KGs）以增强大型语言模型（LLMs）的事实一致性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如Claude, Mistral IA, GPT-4）在自然语言处理（NLP）中表现出色，但缺乏结构化知识，导致事实不一致。

Method: 通过KG-BERT将知识图谱（KGs）集成到大型语言模型中，以提高其基础能力和推理能力。

Result: 在问答和实体链接等知识密集型任务中，实验显示出显著的性能提升。

Conclusion: 该方法提高了大型语言模型的事实可靠性，并为更具备上下文感知能力的下一代大型语言模型铺平了道路。

Abstract: Large language models (LLMs) like Claude, Mistral IA, and GPT-4 excel in NLP but lack structured knowledge, leading to factual inconsistencies. We address this by integrating Knowledge Graphs (KGs) via KG-BERT to enhance grounding and reasoning. Experiments show significant gains in knowledge-intensive tasks such as question answering and entity linking. This approach improves factual reliability and enables more context-aware next-generation LLMs.

</details>


### [11] [Grammaticality Judgments in Humans and Language Models: Revisiting Generative Grammar with LLMs](https://arxiv.org/abs/2512.10453)
*Lars G. B. Johnsen*

Main category: cs.CL

TL;DR: 本文探讨大型语言模型是否通过表面形式的训练数据，表现出对语法结构的敏感性。


<details>
  <summary>Details</summary>
Motivation: 检验仅通过表面形式训练的LLMs是否能展示出内部层级语法的证据，以探讨语法结构的本质。

Method: 通过评估GPT-4和LLaMA-3等模型在特定句法结构（如主语-助动词倒装和寄生空位许可）上的可接受性评分，检验其对语法和非语法变体的区分能力。

Result: LLMs能够可靠地区分两种句法结构的语法和非语法变体，表现出对结构的敏感性，而不仅仅是对线性顺序的敏感。

Conclusion: 通过预测性训练，LLMs能够产生对语法的敏感性，表明结构概括可以在没有显式编码的情况下，从表面形式的训练中产生。

Abstract: What counts as evidence for syntactic structure? In traditional generative grammar, systematic contrasts in grammaticality such as subject-auxiliary inversion and the licensing of parasitic gaps are taken as evidence for an internal, hierarchical grammar. In this paper, we test whether large language models (LLMs), trained only on surface forms, reproduce these contrasts in ways that imply an underlying structural representation.
  We focus on two classic constructions: subject-auxiliary inversion (testing recognition of the subject boundary) and parasitic gap licensing (testing abstract dependency structure). We evaluate models including GPT-4 and LLaMA-3 using prompts eliciting acceptability ratings. Results show that LLMs reliably distinguish between grammatical and ungrammatical variants in both constructions, and as such support that they are sensitive to structure and not just linear order. Structural generalizations, distinct from cognitive knowledge, emerge from predictive training on surface forms, suggesting functional sensitivity to syntax without explicit encoding.

</details>


### [12] [Causal Reasoning Favors Encoders: On The Limits of Decoder-Only Models](https://arxiv.org/abs/2512.10561)
*Amartya Roy,Elamparithy M,Kripabandhu Ghosh,Ponnurangam Kumaraguru,Adrian de Wynter*

Main category: cs.CL

TL;DR: 本文探讨了不同架构在因果推理中的表现，发现仅靠上下文学习不足以进行可靠的因果推理，编码器和编码器-解码器架构在小样本情况下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 因果推理需要多跳组合和严格的联合控制，而输入的虚假词汇关系可能导致误导性结果，因此研究不同模型架构在因果推理中的表现非常重要。

Method: 通过比较自然语言和非自然语言场景下，经过微调的编码器和编码器-解码器架构与仅有解码器架构在零样本和少样本上下文学习中的表现。

Result: 仅有解码器模型对分布变化明显脆弱，而经过微调的编码器和编码器-解码器模型在测试中更具泛化能力，尤其是在非自然语言分割中。

Conclusion: 对于成本效益高、短视稳健的因果推理，带目标微调的编码器或编码器-解码器架构更优。

Abstract: In context learning (ICL) underpins recent advances in large language models (LLMs), although its role and performance in causal reasoning remains unclear. Causal reasoning demands multihop composition and strict conjunctive control, and reliance on spurious lexical relations of the input could provide misleading results. We hypothesize that, due to their ability to project the input into a latent space, encoder and encoder decoder architectures are better suited for said multihop conjunctive reasoning versus decoder only models. To do this, we compare fine-tuned versions of all the aforementioned architectures with zero and few shot ICL in both natural language and non natural language scenarios. We find that ICL alone is insufficient for reliable causal reasoning, often overfocusing on irrelevant input features. In particular, decoder only models are noticeably brittle to distributional shifts, while finetuned encoder and encoder decoder models can generalize more robustly across our tests, including the non natural language split. Both architectures are only matched or surpassed by decoder only architectures at large scales. We conclude by noting that for cost effective, short horizon robust causal reasoning, encoder or encoder decoder architectures with targeted finetuning are preferable.

</details>


### [13] [RoleRMBench & RoleRM: Towards Reward Modeling for Profile-Based Role Play in Dialogue Systems](https://arxiv.org/abs/2512.10575)
*Hang Ding,Qiming Feng,Dongqi Liu,Qi Zhao,Tao Yao,Shuo Wang,Dongsheng Chen,Jian Li,Zhenye Gan,Jiangning Zhang,Chengjie Wang,Yabiao Wang*

Main category: cs.CL

TL;DR: 本文介绍了一种新的奖励模型RoleRM，旨在提升大型语言模型在角色扮演对话中的人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在角色扮演等主观和开放领域表现不佳，难以捕捉细微和基于角色的人类判断，导致与人类判断存在差距。

Method: 引入了RoleRMBench，这是首个用于角色扮演对话奖励建模的系统性基准，并提出使用连续隐含偏好（CIP）训练的奖励模型RoleRM。

Result: RoleRM在基准测试中超过强开源和闭源奖励模型24%，在叙事一致性和风格保真度方面表现突出。

Conclusion: 连续偏好表示和标注一致性在主观对齐中至关重要，为以人为中心的对话系统奠定了基础。

Abstract: Reward modeling has become a cornerstone of aligning large language models (LLMs) with human preferences. Yet, when extended to subjective and open-ended domains such as role play, existing reward models exhibit severe degradation, struggling to capture nuanced and persona-grounded human judgments. To address this gap, we introduce RoleRMBench, the first systematic benchmark for reward modeling in role-playing dialogue, covering seven fine-grained capabilities from narrative management to role consistency and engagement. Evaluation on RoleRMBench reveals large and consistent gaps between general-purpose reward models and human judgment, particularly in narrative and stylistic dimensions. We further propose RoleRM, a reward model trained with Continuous Implicit Preferences (CIP), which reformulates subjective evaluation as continuous consistent pairwise supervision under multiple structuring strategies. Comprehensive experiments show that RoleRM surpasses strong open- and closed-source reward models by over 24% on average, demonstrating substantial gains in narrative coherence and stylistic fidelity. Our findings highlight the importance of continuous preference representation and annotation consistency, establishing a foundation for subjective alignment in human-centered dialogue systems.

</details>


### [14] [AgriGPT-Omni: A Unified Speech-Vision-Text Framework for Multilingual Agricultural Intelligence](https://arxiv.org/abs/2512.10624)
*Bo Yang,Lanfei Feng,Yunkui Chen,Yu Zhang,Jianyu Zhang,Xiao Xu,Nueraili Aierken,Shijian Li*

Main category: cs.CL

TL;DR: 提出AgriGPT-Omni，一个统一的农业多模态框架，集成语音、视觉和文本，解决多语言语音数据缺乏、统一多模态架构和综合评估基准的问题。


<details>
  <summary>Details</summary>
Motivation: 农业应用受到多语言语音数据缺乏、统一多模态架构和综合评估基准的限制，导致多模态大型语言模型在农业领域应用受限。

Method: 1. 构建可扩展的数据合成和收集管道，生成最大的农业语音数据集（492K合成和1.4K真实样本，涵盖六种语言）；2. 采用三阶段范式训练农业全模态模型：文本知识注入、渐进式多模态对齐和基于GRPO的强化学习；3. 提出AgriBench-Omni-2K，首个农业三模态基准，覆盖多语言和多模态任务，具有标准化协议和可重复工具。

Result: AgriGPT-Omni在多语言和多模态推理以及真实语音理解方面显著优于通用基线模型。

Conclusion: AgriGPT-Omni框架通过统一的多模态架构和全面评估基准，推动可重复研究、包容性农业智能和可持续AI发展，尤其适用于低资源地区。

Abstract: Despite rapid advances in multimodal large language models, agricultural applications remain constrained by the lack of multilingual speech data, unified multimodal architectures, and comprehensive evaluation benchmarks. To address these challenges, we present AgriGPT-Omni, an agricultural omni-framework that integrates speech, vision, and text in a unified framework. First, we construct a scalable data synthesis and collection pipeline that converts agricultural texts and images into training data, resulting in the largest agricultural speech dataset to date, including 492K synthetic and 1.4K real speech samples across six languages. Second, based on this, we train the first agricultural omni-model via a three-stage paradigm: textual knowledge injection, progressive multimodal alignment, and GRPO-based reinforcement learning, enabling unified reasoning across languages and modalities. Third, we propose AgriBench-Omni-2K, the first tri-modal benchmark for agriculture, covering diverse speech-vision-text tasks and multilingual slices, with standardized protocols and reproducible tools. Experiments show that AgriGPT-Omni significantly outperforms general-purpose baselines on multilingual and multimodal reasoning as well as real-world speech understanding. All models, data, benchmarks, and code will be released to promote reproducible research, inclusive agricultural intelligence, and sustainable AI development for low-resource regions.

</details>


### [15] [From Data Scarcity to Data Care: Reimagining Language Technologies for Serbian and other Low-Resource Languages](https://arxiv.org/abs/2512.10630)
*Smiljana Antonijevic Ubois*

Main category: cs.CL

TL;DR: 该研究探讨了AI时代低资源语言（以塞尔维亚语为例）的语言技术发展，指出了结构性、历史性和社会技术因素带来的挑战，并提出了一个基于CARE原则的Data Care框架以解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常以英语等主导语言进行训练，导致低资源语言的表示存在文化和语言偏见。本研究旨在分析这些偏见并提出解决方案。

Method: 通过对十位学者和从业者的半结构化访谈，包括语言学家、数字人文主义者和AI开发者，研究分析了塞尔维亚语言技术的开发挑战。

Result: 研究发现，塞尔维亚语言技术开发面临历史文本遗产的破坏、表层转写、对英语训练模型的依赖、数据偏见和缺乏文化特定性的数据集管理等挑战。

Conclusion: 研究提出Data Care框架，将偏见缓解重新定义为语料库设计、注释和治理的重要组成部分，为建立包容性、可持续性和文化基础的语言技术提供可复制的模型。

Abstract: Large language models are commonly trained on dominant languages like English, and their representation of low resource languages typically reflects cultural and linguistic biases present in the source language materials. Using the Serbian language as a case, this study examines the structural, historical, and sociotechnical factors shaping language technology development for low resource languages in the AI age. Drawing on semi structured interviews with ten scholars and practitioners, including linguists, digital humanists, and AI developers, it traces challenges rooted in historical destruction of Serbian textual heritage, intensified by contemporary issues that drive reductive, engineering first approaches prioritizing functionality over linguistic nuance. These include superficial transliteration, reliance on English-trained models, data bias, and dataset curation lacking cultural specificity. To address these challenges, the study proposes Data Care, a framework grounded in CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics), that reframes bias mitigation from a post hoc technical fix to an integral component of corpus design, annotation, and governance, and positions Data Care as a replicable model for building inclusive, sustainable, and culturally grounded language technologies in contexts where traditional LLM development reproduces existing power imbalances and cultural blind spots.

</details>


### [16] [Textual Data Bias Detection and Mitigation - An Extensible Pipeline with Experimental Evaluation](https://arxiv.org/abs/2512.10734)
*Rebekka Görge,Sujan Sai Gannamaneni,Tabea Naeven,Hammam Abdelwahab,Héctor Allende-Cid,Armin B. Cremers,Lennard Helmer,Michael Mock,Anna Schmitz,Songkai Xue,Elif Yildirir,Maximilian Poretschkin,Stefan Wrobel*

Main category: cs.CL

TL;DR: 提出了一种全面的数据偏见检测和缓解流程，包含四个组件，以解决表示偏见和显性刻板印象。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型（LLMs）的文本数据表现出多方面的偏见，法规要求识别和缓解针对受保护群体的偏见，但缺乏实际操作指导。

Method: 通过LLM生成的词表检测相关群体标签，使用人口表示分数量化表示偏见，采用社会语言学信息过滤检测和缓解刻板印象，通过语法和上下文感知的反事实数据增强补偿表示偏见。

Result: 成功减少了文本数据集中的表示偏见和显性刻板印象，但使用去偏见数据微调的LLMs在偏见基准测试中并未持续表现出性能提升。

Conclusion: 当前评估方法存在关键差距，需要更有针对性的数据操作以解决模型偏见问题。

Abstract: Textual data used to train large language models (LLMs) exhibits multifaceted bias manifestations encompassing harmful language and skewed demographic distributions. Regulations such as the European AI Act require identifying and mitigating biases against protected groups in data, with the ultimate goal of preventing unfair model outputs. However, practical guidance and operationalization are lacking. We propose a comprehensive data bias detection and mitigation pipeline comprising four components that address two data bias types, namely representation bias and (explicit) stereotypes for a configurable sensitive attribute. First, we leverage LLM-generated word lists created based on quality criteria to detect relevant group labels. Second, representation bias is quantified using the Demographic Representation Score. Third, we detect and mitigate stereotypes using sociolinguistically informed filtering. Finally, we compensate representation bias through Grammar- and Context-Aware Counterfactual Data Augmentation. We conduct a two-fold evaluation using the examples of gender, religion and age. First, the effectiveness of each individual component on data debiasing is evaluated through human validation and baseline comparison. The findings demonstrate that we successfully reduce representation bias and (explicit) stereotypes in a text dataset. Second, the effect of data debiasing on model bias reduction is evaluated by bias benchmarking of several models (0.6B-8B parameters), fine-tuned on the debiased text dataset. This evaluation reveals that LLMs fine-tuned on debiased data do not consistently show improved performance on bias benchmarks, exposing critical gaps in current evaluation methodologies and highlighting the need for targeted data manipulation to address manifested model bias.

</details>


### [17] [Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving](https://arxiv.org/abs/2512.10739)
*Songyang Gao,Yuzhe Gu,Zijian Wu,Lingkai Kong,Wenwei Zhang,Zhongrui Cai,Fan Zheng,Tianyou Ma,Junhao Shen,Haiteng Zhao,Duanyang Zhang,Huilun Zhang,Kuikun Liu,Chengqi Lyu,Yanhui Duan,Chiyu Chen,Ningsheng Ma,Jianfei Gao,Han Lyu,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出了一种新型验证器OPV，通过结合结果和过程验证，以较低标注成本实现复杂长推理链的高效、准确验证和大规模标注。


<details>
  <summary>Details</summary>
Motivation: 现有结果验证器（OVs）无法检查长推理链中的不可靠中间步骤，而过程验证器（PVs）因高质量标注稀缺难以可靠检测复杂长推理链中的错误。

Method: 提出OPV，通过迭代主动学习框架和专家标注，以较少的标注成本逐步提升验证能力，并使用Rejection Fine-Tuning（RFT）和RLVR训练模型。

Result: OPV在held-out 	extsc{	hisbench}上实现了新的最先进性能，F1分数为83.1，优于Qwen3-Max-Preview的76.3，并能有效检测合成数据集中的假阳性。

Conclusion: OPV在验证复杂长推理链方面表现出色，具有广泛的应用性，并能在与策略模型协作时持续提升性能。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the \textbf{O}utcome-based \textbf{P}rocess \textbf{V}erifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out \textsc{\thisbench}, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2\% to 73.3\% on AIME2025 as the compute budget scales.

</details>


### [18] [TRIDENT: A Redundant Architecture for Caribbean-Accented Emergency Speech Triage](https://arxiv.org/abs/2512.10741)
*Elroy Galbraith,Chadwick Sutherland,Donahue Morgan*

Main category: cs.CL

TL;DR: 提出TRIDENT系统解决加勒比地区非标准英语紧急语音识别问题，通过三层架构支持调度员应用分诊协议。


<details>
  <summary>Details</summary>
Motivation: 现有紧急语音识别系统对非标准英语变体性能下降，导致加勒比地区服务存在关键缺口，需要确保口音差异不影响紧急服务获取。

Method: 设计三层调度员支持架构：1)加勒比口音优化的ASR；2)基于LLM的本地实体提取；3)生物声学痛苦检测。系统结合转录置信度、结构化临床实体和声音压力指标提供决策支持。

Result: 提出了将低ASR置信度转化为队列优先级信号的创新方法，并开发了能捕捉无声音压力但包含临床关键信息的实体提取层。系统可在灾难场景离线运行。

Conclusion: 建立了口音弹性紧急AI框架，确保加勒比地区用户平等获取国家分诊协议，但实证验证仍需未来研究。

Abstract: Emergency speech recognition systems exhibit systematic performance degradation on non-standard English varieties, creating a critical gap in services for Caribbean populations. We present TRIDENT (Transcription and Routing Intelligence for Dispatcher-Empowered National Triage), a three-layer dispatcher-support architecture designed to structure emergency call inputs for human application of established triage protocols (the ESI for routine operations and START for mass casualty events), even when automatic speech recognition fails.
  The system combines Caribbean-accent-tuned ASR, local entity extraction via large language models, and bio-acoustic distress detection to provide dispatchers with three complementary signals: transcription confidence, structured clinical entities, and vocal stress indicators. Our key insight is that low ASR confidence, rather than representing system failure, serves as a valuable queue prioritization signal -- particularly when combined with elevated vocal distress markers indicating a caller in crisis whose speech may have shifted toward basilectal registers. A complementary insight drives the entity extraction layer: trained responders and composed bystanders may report life-threatening emergencies without elevated vocal stress, requiring semantic analysis to capture clinical indicators that paralinguistic features miss.
  We describe the architectural design, theoretical grounding in psycholinguistic research on stress-induced code-switching, and deployment considerations for offline operation during disaster scenarios. This work establishes a framework for accent-resilient emergency AI that ensures Caribbean voices receive equitable access to established national triage protocols. Empirical validation on Caribbean emergency calls remains future work.

</details>


### [19] [OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification](https://arxiv.org/abs/2512.10756)
*Zijian Wu,Lingkai Kong,Wenwei Zhang,Songyang Gao,Yuzhe Gu,Zhongrui Cai,Tianyou Ma,Yuhong Liu,Zhi Wang,Runyuan Ma,Guangyu Wang,Wei Li,Conghui He,Dahua Lin,Kai Chen*

Main category: cs.CL

TL;DR: 提出了一种新型Outcome-based Process Verifier (OPV)，结合主动学习和专家标注，通过迭代优化实现高效、准确的复杂推理链验证，并在多项基准测试中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有成果型验证器（OVs）无法检查长推理链中的不可靠中间步骤，而过程型验证器（PVs）受限于高质量标注的稀缺性。需解决复杂长推理链的可靠验证与大规模标注成本问题。

Method: 提出OPV，通过总结长推理链的结论并验证其合理性，结合迭代主动学习框架和专家标注，利用拒绝微调（RFT）和RLVR逐步优化验证能力。

Result: 在OPV-Bench上达到83.1的F1分数，超越Qwen3-Max-Preview（76.3）；能检测合成数据中的假阳性，与专家评估高度一致；与策略模型协作时，将DeepSeek-R1-Distill-Qwen-32B在AIME2025上的准确率从55.2%提升至73.3%。

Conclusion: OPV在验证性能、泛化能力和成本效率上表现优异，为复杂推理任务提供了更可靠的自动化验证方案。

Abstract: Large language models (LLMs) have achieved significant progress in solving complex reasoning tasks by Reinforcement Learning with Verifiable Rewards (RLVR). This advancement is also inseparable from the oversight automated by reliable verifiers. However, current outcome-based verifiers (OVs) are unable to inspect the unreliable intermediate steps in the long reasoning chains of thought (CoTs). Meanwhile, current process-based verifiers (PVs) have difficulties in reliably detecting errors in the complex long CoTs, limited by the scarcity of high-quality annotations due to the prohibitive costs of human annotations. Therefore, we propose the Outcome-based Process Verifier (OPV), which verifies the rationale process of summarized outcomes from long CoTs to achieve both accurate and efficient verification and enable large-scale annotation. To empower the proposed verifier, we adopt an iterative active learning framework with expert annotations to progressively improve the verification capability of OPV with fewer annotation costs. Specifically, in each iteration, the most uncertain cases of the current best OPV are annotated and then subsequently used to train a new OPV through Rejection Fine-Tuning (RFT) and RLVR for the next round. Extensive experiments demonstrate OPV's superior performance and broad applicability. It achieves new state-of-the-art results on our held-out OPV-Bench, outperforming much larger open-source models such as Qwen3-Max-Preview with an F1 score of 83.1 compared to 76.3. Furthermore, OPV effectively detects false positives within synthetic dataset, closely align with expert assessment. When collaborating with policy models, OPV consistently yields performance gains, e.g., raising the accuracy of DeepSeek-R1-Distill-Qwen-32B from 55.2% to 73.3% on AIME2025 as the compute budget scales.

</details>


### [20] [Grow Up and Merge: Scaling Strategies for Efficient Language Adaptation](https://arxiv.org/abs/2512.10772)
*Kevin Glocker,Kätriin Kukk,Romina Oji,Marcel Bollmann,Marco Kuhlmann,Jenny Kunz*

Main category: cs.CL

TL;DR: 本文研究了通过扩展（scaling）预训练英语模型以适应新目标语言的效率，发现扩展模型在数据效率、性能匹配和减少灾难性遗忘方面优于标准持续预训练。


<details>
  <summary>Details</summary>
Motivation: 多语言模型在中低资源语言上性能不佳，尤其是在较小模型规模下。需要找到一种高效适应新语言的方法。

Method: 通过综合扩展消融实验，使用FLOP匹配模型，比较扩展英语基础模型与标准持续预训练的效果。

Result: 扩展模型在足够的目标语言数据下，可以匹配或超越持续预训练的小模型，且有助于保持英语能力。合并扩展模型构建多语言系统时，性能优于小模型合并。

Conclusion: 扩展是实现资源高效语言适应的有效策略，有助于减少灾难性遗忘，并能在合并时提升多语言系统的性能。

Abstract: Achieving high-performing language models which include medium- and lower-resource languages remains a challenge. Massively multilingual models still underperform compared to language-specific adaptations, especially at smaller model scales. In this work, we investigate scaling as an efficient strategy for adapting pretrained models to new target languages. Through comprehensive scaling ablations with approximately FLOP-matched models, we test whether upscaling an English base model enables more effective and resource-efficient adaptation than standard continued pretraining. We find that, once exposed to sufficient target-language data, larger upscaled models can match or surpass the performance of smaller models continually pretrained on much more data, demonstrating the benefits of scaling for data efficiency. Scaling also helps preserve the base model's capabilities in English, thus reducing catastrophic forgetting. Finally, we explore whether such scaled, language-specific models can be merged to construct modular and flexible multilingual systems. We find that while merging remains less effective than joint multilingual training, upscaled merges perform better than smaller ones. We observe large performance differences across merging methods, suggesting potential for improvement through merging approaches specialized for language-level integration.

</details>


### [21] [Script Gap: Evaluating LLM Triage on Indian Languages in Native vs Roman Scripts in a Real World Setting](https://arxiv.org/abs/2512.10780)
*Manurag Khullar,Utkarsh Desai,Poorva Malviya,Aman Dalmia,Zheyuan Ryan Shi*

Main category: cs.CL

TL;DR: 该论文研究了印度语言罗马化对大型语言模型在孕产妇和新生儿医疗分诊中性能的影响，发现罗马化文本会导致模型F1分数显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在印度高利害临床环境中的应用增加，了解罗马化文本对模型可靠性的影响变得至关重要，尤其是在真实世界数据中很少评估这种正字法变化。

Method: 研究使用了一个涵盖五种印度语言和尼泊尔语的真实世界用户生成查询数据集，对领先的大型语言模型进行了基准测试，比较了罗马化文本和本地文字的性能。

Result: 罗马化消息的性能一致下降，F1分数比本地文字低5-12分，在印度的孕产妇健康组织中可能导致近200万次分诊错误。

Conclusion: 大型语言模型在罗马化输入下表现出可靠性问题，尽管能正确推断语义意图，但在最终分类输出上仍然脆弱，揭示了LLM健康系统中的一个关键安全盲点。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes clinical applications in India. In many such settings, speakers of Indian languages frequently communicate using romanized text rather than native scripts, yet existing research rarely evaluates this orthographic variation using real-world data. We investigate how romanization impacts the reliability of LLMs in a critical domain: maternal and newborn healthcare triage. We benchmark leading LLMs on a real-world dataset of user-generated queries spanning five Indian languages and Nepali. Our results reveal consistent degradation in performance for romanized messages, with F1 scores trailing those of native scripts by 5-12 points. At our partner maternal health organization in India, this gap could cause nearly 2 million excess errors in triage. Crucially, this performance gap by scripts is not due to a failure in clinical reasoning. We demonstrate that LLMs often correctly infer the semantic intent of romanized queries. Nevertheless, their final classification outputs remain brittle in the presence of orthographic noise in romanized inputs. Our findings highlight a critical safety blind spot in LLM-based health systems: models that appear to understand romanized input may still fail to act on it reliably.

</details>


### [22] [The FACTS Leaderboard: A Comprehensive Benchmark for Large Language Model Factuality](https://arxiv.org/abs/2512.10791)
*Aileen Cheng,Alon Jacovi,Amir Globerson,Ben Golan,Charles Kwong,Chris Alberti,Connie Tao,Eyal Ben-David,Gaurav Singh Tomar,Lukas Haas,Yonatan Bitton,Adam Bloniarz,Aijun Bai,Andrew Wang,Anfal Siddiqui,Arturo Bajuelos Castillo,Aviel Atias,Chang Liu,Corey Fry,Daniel Balle,Deepanway Ghosal,Doron Kukliansky,Dror Marcus,Elena Gribovskaya,Eran Ofek,Honglei Zhuang,Itay Laish,Jan Ackermann,Lily Wang,Meg Risdal,Megan Barnes,Michael Fink,Mohamed Amin,Moran Ambar,Natan Potikha,Nikita Gupta,Nitzan Katz,Noam Velan,Ofir Roval,Ori Ram,Polina Zablotskaia,Prathamesh Bang,Priyanka Agrawal,Rakesh Ghiya,Sanjay Ganapathy,Simon Baumgartner,Sofia Erell,Sushant Prakash,Thibault Sellam,Vikram Rao,Xuanhui Wang,Yaroslav Akulov,Yulong Yang,Zhen Yang,Zhixin Lai,Zhongru Wu,Anca Dragan,Avinatan Hassidim,Fernando Pereira,Slav Petrov,Srinivasan Venkatachary,Tulsee Doshi,Yossi Matias,Sasha Goldshtein,Dipanjan Das*

Main category: cs.CL

TL;DR: 介绍 FACTS Leaderboard，一个综合评估语言模型在不同场景下生成事实准确文本能力的在线排行榜套件。


<details>
  <summary>Details</summary>
Motivation: 全面评估语言模型在生成文本时的事实准确性，通过多个子排行榜提供整体度量。

Method: 通过四个子排行榜（FACTS Multimodal, FACTS Parametric, FACTS Search, FACTS Grounding (v2)）分别评估模型在图像问答、世界知识、信息检索和文档基础上的长文本生成的事实准确性。

Result: 每个子排行榜使用自动评判模型对模型响应进行评分，最终得分是四个部分的平均值，以提供对模型整体事实性的稳健评估。

Conclusion: FACTS Leaderboard 将积极维护，包含公共和私有部分，以确保外部参与和排行榜的完整性。

Abstract: We introduce The FACTS Leaderboard, an online leaderboard suite and associated set of benchmarks that comprehensively evaluates the ability of language models to generate factually accurate text across diverse scenarios. The suite provides a holistic measure of factuality by aggregating the performance of models on four distinct sub-leaderboards: (1) FACTS Multimodal, which measures the factuality of responses to image-based questions; (2) FACTS Parametric, which assesses models' world knowledge by answering closed-book factoid questions from internal parameters; (3) FACTS Search, which evaluates factuality in information-seeking scenarios, where the model must use a search API; and (4) FACTS Grounding (v2), which evaluates whether long-form responses are grounded in provided documents, featuring significantly improved judge models. Each sub-leaderboard employs automated judge models to score model responses, and the final suite score is an average of the four components, designed to provide a robust and balanced assessment of a model's overall factuality. The FACTS Leaderboard Suite will be actively maintained, containing both public and private splits to allow for external participation while guarding its integrity. It can be found at https://www.kaggle.com/benchmarks/google/facts .

</details>


### [23] [LabelFusion: Learning to Fuse LLMs and Transformer Classifiers for Robust Text Classification](https://arxiv.org/abs/2512.10793)
*Michael Schlee,Christoph Weisser,Timo Kivimäki,Melchizedek Mashiku,Benjamin Saefken*

Main category: cs.CL

TL;DR: LabelFusion是一种集成学习方法，通过结合传统基于Transformer的分类器（如RoBERTa）和大型语言模型（LLMs），实现准确且成本可控的多类和多标签文本分类。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在结合LLM推理和传统transformer-based分类器的互补优势，以在保持高性能的同时，优化准确率、延迟和成本之间的权衡。

Method: LabelFusion通过将ML主干模型的嵌入与通过结构化提示工程策略获得的LLM每类得分进行拼接，并将该联合表示送入一个紧凑的多层感知器（FusionMLP）来生成最终预测。

Result: 该方法在AG News上达到92.4%的准确率，在10类Reuters 21578主题分类上达到92.3%的准确率。

Conclusion: LabelFusion提供了一种有效的方法来融合传统分类器和LLM的优势，实现了跨领域的稳健性能，同时允许在准确率、延迟和成本之间进行实际权衡。

Abstract: LabelFusion is a fusion ensemble for text classification that learns to combine a traditional transformer-based classifier (e.g., RoBERTa) with one or more Large Language Models (LLMs such as OpenAI GPT, Google Gemini, or DeepSeek) to deliver accurate and cost-aware predictions across multi-class and multi-label tasks. The package provides a simple high-level interface (AutoFusionClassifier) that trains the full pipeline end-to-end with minimal configuration, and a flexible API for advanced users. Under the hood, LabelFusion integrates vector signals from both sources by concatenating the ML backbone's embeddings with the LLM-derived per-class scores -- obtained through structured prompt-engineering strategies -- and feeds this joint representation into a compact multi-layer perceptron (FusionMLP) that produces the final prediction. This learned fusion approach captures complementary strengths of LLM reasoning and traditional transformer-based classifiers, yielding robust performance across domains -- achieving 92.4% accuracy on AG News and 92.3% on 10-class Reuters 21578 topic classification -- while enabling practical trade-offs between accuracy, latency, and cost.

</details>


### [24] [Unsupervised Acquisition of Discrete Grammatical Categories](https://arxiv.org/abs/2503.18702)
*David Ph. Shakouri,Crit Cremers,Niels O. Schiller*

Main category: cs.CL

TL;DR: 本文介绍了一种用于语言习得实验的计算实验室环境，通过多智能体系统演示了如何获得抽象语法知识。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过模拟成人和儿童语言模型之间的互动，探索语言习得过程中抽象语法知识的获取方法。

Method: 实现了一个由成人语言模型和女儿语言模型组成的多智能体系统，其中女儿模型只能访问由母亲模型生成的语言样本，并运用统计分析和层次聚类分析来提取语法规则。

Result: 通过层次聚类分析，成功提取了离散的语法规则，并验证了系统参数配置的有效性，在测试集中也获得了非平凡的语法类别。

Conclusion: 该系统能有效获取类似自然语言中语法类别的结构，从而证明可以获得非平凡的语法知识。

Abstract: This article presents experiments performed using a computational laboratory environment for language acquisition experiments. It implements a multi-agent system consisting of two agents: an adult language model and a daughter language model that aims to learn the mother language. Crucially, the daughter agent does not have access to the internal knowledge of the mother language model but only to the language exemplars the mother agent generates. These experiments illustrate how this system can be used to acquire abstract grammatical knowledge. We demonstrate how statistical analyses of patterns in the input data corresponding to grammatical categories yield discrete grammatical rules. These rules are subsequently added to the grammatical knowledge of the daughter language model. To this end, hierarchical agglomerative cluster analysis was applied to the utterances consecutively generated by the mother language model. It is argued that this procedure can be used to acquire structures resembling grammatical categories proposed by linguists for natural languages. Thus, it is established that non-trivial grammatical knowledge has been acquired. Moreover, the parameter configuration of this computational laboratory environment determined using training data generated by the mother language model is validated in a second experiment with a test set similarly resulting in the acquisition of non-trivial categories.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [ExaCraft: Dynamic Learning Context Adaptation for Personalized Educational Examples](https://arxiv.org/abs/2512.09931)
*Akaash Chatterjee,Suman Kundu*

Main category: cs.AI

TL;DR: ExaCraft是一个利用Google Gemini AI和Python Flask API，通过Chrome扩展程序提供个性化学习例子的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前的教育AI工具缺乏生成与学习者相关的例子和适应学习者不断变化的需求的能力。

Method: ExaCraft结合了用户定义的个人资料（如地点、教育、职业和复杂性偏好）和对学习者行为的实时分析，生成个性化例子。

Result: 系统能够根据五个关键方面的学习情境进行调整：困难指标、掌握模式、主题进展历史、会话边界和学习进展信号。

Conclusion: ExaCraft的演示展示了其例子如何从基础概念发展到高级技术实现，响应不同用例中的主题重复、再生请求和主题进展模式。

Abstract: Learning is most effective when it's connected to relevant, relatable examples that resonate with learners on a personal level. However, existing educational AI tools don't focus on generating examples or adapting to learners' changing understanding, struggles, or growing skills. We've developed ExaCraft, an AI system that generates personalized examples by adapting to the learner's dynamic context. Through the Google Gemini AI and Python Flask API, accessible via a Chrome extension, ExaCraft combines user-defined profiles (including location, education, profession, and complexity preferences) with real-time analysis of learner behavior. This ensures examples are both culturally relevant and tailored to individual learning needs. The system's core innovation is its ability to adapt to five key aspects of the learning context: indicators of struggle, mastery patterns, topic progression history, session boundaries, and learning progression signals. Our demonstration will show how ExaCraft's examples evolve from basic concepts to advanced technical implementations, responding to topic repetition, regeneration requests, and topic progression patterns in different use cases.

</details>


### [26] [Exploring Health Misinformation Detection with Multi-Agent Debate](https://arxiv.org/abs/2512.09935)
*Chih-Han Chen,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.AI

TL;DR: 提出了一个两阶段的健康 misinformation 检测框架：先通过LLM评估文章并计算一致分数，如共识不足则进入多智能体辩论阶段。


<details>
  <summary>Details</summary>
Motivation: 健康相关的错误信息在网上迅速传播，有效的验证需要高质量的证据检索和严格的推理过程。

Method: 第一阶段使用大型语言模型（LLM）独立评估检索到的文章并计算一致分数。如果分数低于预定义阈值，进入第二阶段，多个智能体进行结构化辩论，综合冲突证据并生成合理的结论。

Result: 实验结果表明，该两阶段方法比基线方法性能更优，尤其在复杂验证任务中。

Conclusion: 结合自动化评分与协作推理的两阶段框架在健康 misinformation 检测中具有重要价值。

Abstract: Fact-checking health-related claims has become increasingly critical as misinformation proliferates online. Effective verification requires both the retrieval of high-quality evidence and rigorous reasoning processes. In this paper, we propose a two-stage framework for health misinformation detection: Agreement Score Prediction followed by Multi-Agent Debate. In the first stage, we employ large language models (LLMs) to independently evaluate retrieved articles and compute an aggregated agreement score that reflects the overall evidence stance. When this score indicates insufficient consensus-falling below a predefined threshold-the system proceeds to a second stage. Multiple agents engage in structured debate to synthesize conflicting evidence and generate well-reasoned verdicts with explicit justifications. Experimental results demonstrate that our two-stage approach achieves superior performance compared to baseline methods, highlighting the value of combining automated scoring with collaborative reasoning for complex verification tasks.

</details>


### [27] [Echo-CoPilot: A Multi-View, Multi-Task Agent for Echocardiography Interpretation and Reporting](https://arxiv.org/abs/2512.09944)
*Moein Heidari,Mohammad Amin Roohi,Armin Khosravi,Ilker Hacihaliloglu*

Main category: cs.AI

TL;DR: Echo-CoPilot是一个利用大型语言模型协调多种专用超声心动图工具的多视角、多任务智能体，能够在React风格的循环中分解临床医生的查询，调用工具进行视图识别、心脏结构分割、测量和疾病预测，并生成符合指南的答案和叙述性总结。


<details>
  <summary>Details</summary>
Motivation: 当前超声心动图全研究解读仍依赖人工，认知负担重，尽管已有针对特定任务的模型，但它们通常独立运行，缺乏统一的临床连贯性评估。

Method: 引入Echo-CoPilot，该智能体使用大型语言模型，在ReAct风格的循环中分解临床医生的查询，并调用多种工具进行视图识别、心脏结构分割、测量和疾病预测，最终整合输出为指南感知的答案和叙述性总结。

Result: 在公共MIMIC-EchoQA基准测试中，Echo-CoPilot的准确率达到50.8%，优于通用和生物医学视频视觉语言模型。定性分析表明，该智能体能利用定量测量和生理背景解决接近临床决策阈值的挑战性病例。

Conclusion: Echo-CoPilot通过整合多种专用工具，提供了一种统一且临床连贯的超声心动图解读方案，在性能上超越了现有模型，并展示了在复杂病例中的临床实用性。

Abstract: Echocardiography is central to contemporary cardiovascular care, but full-study interpretation remains a cognitively demanding, multi-view task that is still performed manually. While recent foundation models for echocardiography can achieve strong performance on individual perceptual subtasks such as view classification, segmentation, or disease prediction, they typically operate in isolation and do not provide a unified, clinically coherent assessment. In this work, we introduce Echo-CoPilot, a multi-view, multi-task agent that uses a large language model to orchestrate a suite of specialized echocardiography tools. Within a ReAct-style loop, the agent decomposes clinician queries, invokes tools for view recognition, cardiac structure segmentation, measurement and disease prediction, and report synthesis, and integrates their outputs into guideline-aware answers and narrative summaries. We evaluate Echo-CoPilot on the public MIMIC-EchoQA benchmark, where it achieves an accuracy of 50.8\%, outperforming both general-purpose and biomedical video vision-language models. Qualitative analyses further show that the agent leverages quantitative measurements and physiologic context to resolve challenging cases near clinical decision thresholds, such as borderline left ventricular hypertrophy or pericardial effusion severity. The code will be released upon acceptance of the paper.

</details>


### [28] [Fuzzy Hierarchical Multiplex](https://arxiv.org/abs/2512.09976)
*Alexis Kafantaris*

Main category: cs.AI

TL;DR: 提出了一种扩展FCM因果关系的新型模糊优化框架，用于服务流程设计中的信息传输优化。


<details>
  <summary>Details</summary>
Motivation: 优化服务流程设计中的信息传输，通过动态映射数据和利用多路复用技术，分析概念间的逻辑隐含关系和层次结构。

Method: 引入模糊优化框架，扩展FCM因果关系，通过逻辑和数学分析构建框架，并对FHM进行详细分析。

Result: 成功提出并分析了一种新的模糊优化框架，阐明了其主要目标和方向，并通过逻辑步骤对FHM进行了分析。

Conclusion: 该框架为服务流程设计中的信息传输优化提供了一个理论支持，并展示了其逻辑和数学基础。

Abstract: A new fuzzy optimization framework that extends FCM causality is proposed. This model utilizes the dynamics to map data into metrics and create a framework that examines logical implication and hierarchy of concepts using a multiplex. Moreover, this is a white-theoretical paper introducing the framework and analyzing the logic and math behind it. Upon this extension the main objectives and the orientation of this framework is expounded and exemplified; this framework is meant for service optimization of information transmission in service process design. Lastly, a thorough analysis of the FHM is included which is done following the logical steps in a simple and elegant manner.

</details>


### [29] [Exploring LLMs for Scientific Information Extraction Using The SciEx Framework](https://arxiv.org/abs/2512.10004)
*Sha Li,Ayush Sadekar,Nathan Self,Yiqi Su,Lars Andersland,Mira Chaplin,Annabel Zhang,Hyoju Yang,James B Henderson,Krista Wigginton,Linsey Marr,T. M. Murali,Naren Ramakrishnan*

Main category: cs.AI

TL;DR: SciEx是一个模块化且可组合的框架，用于解决科学文献自动化信息提取中的长上下文、多模态、细粒度信息不一致及数据模式快速变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法和工具在处理科学文献时存在诸多挑战，如长文档、多模态内容、不同文献间信息不一致、数据模式快速变化等，需要一种灵活且可扩展的解决方案。

Method: SciEx框架将PDF解析、多模态检索、信息提取和聚合等关键组件解耦，支持按需数据提取，并允许新模型、提示策略和推理机制的灵活集成。

Result: 在涵盖三个科学主题的数据集上评估，SciEx能够准确且一致地提取细粒度信息，并揭示了当前基于大语言模型流程的优势和局限性。

Conclusion: SciEx提供了一种有效的方法，用于科学文献的信息提取，具有良好的扩展性和灵活性，能够应对不断变化的数据模式和需求。

Abstract: Large language models (LLMs) are increasingly touted as powerful tools for automating scientific information extraction. However, existing methods and tools often struggle with the realities of scientific literature: long-context documents, multi-modal content, and reconciling varied and inconsistent fine-grained information across multiple publications into standardized formats. These challenges are further compounded when the desired data schema or extraction ontology changes rapidly, making it difficult to re-architect or fine-tune existing systems. We present SciEx, a modular and composable framework that decouples key components including PDF parsing, multi-modal retrieval, extraction, and aggregation. This design streamlines on-demand data extraction while enabling extensibility and flexible integration of new models, prompting strategies, and reasoning mechanisms. We evaluate SciEx on datasets spanning three scientific topics for its ability to extract fine-grained information accurately and consistently. Our findings provide practical insights into both the strengths and limitations of current LLM-based pipelines.

</details>


### [30] [SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration](https://arxiv.org/abs/2512.10046)
*Yan Zhuang,Jiawei Ren,Xiaokang Ye,Jianzhi Shen,Ruixuan Zhang,Tianai Yue,Muhammad Faayez,Xuhong He,Ziqiao Ma,Lianhui Qin,Zhiting Hu,Tianmin Shu*

Main category: cs.AI

TL;DR: 本文介绍了一个用于城市环境中具身人工智能的新模拟平台SimWorld-Robotics (SWR)，以及两个挑战性机器人基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型研究主要集中在室内场景，缺乏在城市环境中进行多样化任务的研究平台。本文旨在填补这一空白。

Method: 利用Unreal Engine 5构建SWR平台，生成逼真的城市场景，支持多机器人控制和通信。设计了两个基准测试：(1) 多模态指令跟随任务，(2) 多智能体搜索任务。

Result: SWR平台超越了现有的城市模拟，在真实感、复杂性和可扩展性方面表现突出。实验表明，当前最先进的模型在SWR任务中表现不佳，缺乏城市环境所需的感知、推理和规划能力。

Conclusion: SWR平台为评估和提升机器人在复杂城市环境中的能力提供了新基准，同时揭示了现有模型在现实城市任务中的局限性。

Abstract: Recent advances in foundation models have shown promising results in developing generalist robotics that can perform diverse tasks in open-ended scenarios given multimodal inputs. However, current work has been mainly focused on indoor, household scenarios. In this work, we present SimWorld-Robotics~(SWR), a simulation platform for embodied AI in large-scale, photorealistic urban environments. Built on Unreal Engine 5, SWR procedurally generates unlimited photorealistic urban scenes populated with dynamic elements such as pedestrians and traffic systems, surpassing prior urban simulations in realism, complexity, and scalability. It also supports multi-robot control and communication. With these key features, we build two challenging robot benchmarks: (1) a multimodal instruction-following task, where a robot must follow vision-language navigation instructions to reach a destination in the presence of pedestrians and traffic; and (2) a multi-agent search task, where two robots must communicate to cooperatively locate and meet each other. Unlike existing benchmarks, these two new benchmarks comprehensively evaluate a wide range of critical robot capacities in realistic scenarios, including (1) multimodal instructions grounding, (2) 3D spatial reasoning in large environments, (3) safe, long-range navigation with people and traffic, (4) multi-robot collaboration, and (5) grounded communication. Our experimental results demonstrate that state-of-the-art models, including vision-language models (VLMs), struggle with our tasks, lacking robust perception, reasoning, and planning abilities necessary for urban environments.

</details>


### [31] [Parallel Decoder Transformer: Model-Internal Parallel Decoding with Speculative Invariance via Note Conditioning](https://arxiv.org/abs/2512.10054)
*Logan Robbins*

Main category: cs.AI

TL;DR: 提出了一种名为Parallel Decoder Transformer（PDT）的参数高效架构，通过引入轻量级Speculative Note Conditioning（SNC）适配器，在不修改预训练模型权重的前提下，实现并行解码并解决连贯性漂移问题。


<details>
  <summary>Details</summary>
Motivation: 自回归解码在大型语言模型中是串行的，导致延迟瓶颈；现有的“分解与填充”方法因缺乏流间通信而出现连贯性漂移。因此，需要一种新的方法来并行化解码并解决连贯性问题。

Method: PDT架构通过在冻结的预训练模型中嵌入协调原语，引入SNC适配器，实现并行解码流通过共享动态潜在空间进行同步。协调被形式化为一个推测性共识问题，兄弟流通过全局总线广播语义“注释”，并由学习验证头进行门控。

Result: 在一个冻结的20B参数模型上，通过50,000步课程验证，PDT在覆盖预测中实现了77.8%的精确度，并恢复了近似的串行语义，无需修改主干权重。

Conclusion: PDT提供了一种可扩展且高效的替代方案，用于结构化并行生成，避免了对整个模型进行微调的需求。

Abstract: Autoregressive decoding in Large Language Models (LLMs) is inherently sequential, creating a latency bottleneck that scales linearly with output length. While ``Decomposition-and-Fill'' methods like Skeleton-of-Thought attempt to parallelize generation via external orchestration, they suffer from \textit{coherence drift} due to the lack of cross-stream communication. In this work, we introduce the \textbf{Parallel Decoder Transformer (PDT)}, a parameter-efficient architecture that embeds coordination primitives directly into the inference process of a frozen pre-trained model.
  Instead of retraining the base model, PDT injects lightweight \textit{Speculative Note Conditioning (SNC)} adapters that allow parallel decoding streams to synchronize via a shared, dynamic latent space. We formulate coordination as a \textit{speculative consensus} problem, where sibling streams broadcast semantic ``notes'' to a global bus, gated by a learned verification head. We validate our approach on a 50,000-step curriculum using a frozen 20B-parameter backbone. Our results demonstrate that PDT achieves effective self-correction, reaching \textbf{77.8\% precision} in coverage prediction and recovering approximate serial semantics without modifying the trunk weights. This establishes PDT as a scalable, efficient alternative to full model fine-tuning for structured parallel generation.

</details>


### [32] [Linear socio-demographic representations emerge in Large Language Models from indirect cues](https://arxiv.org/abs/2512.10065)
*Paul Bouchaud,Pedro Ramaciotti*

Main category: cs.AI

TL;DR: 研究大型语言模型如何通过间接线索推断对话者的社会人口学属性，并展示了模型在激活空间中形成的线性表示。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何从人类对话者的姓名和职业等间接线索中推断社会人口学属性，以及这些推断如何影响下游行为。

Method: 使用四个开放式基于Transformer的LLMs，通过提示具有明确社会人口学属性的对话，分析其残差流中的激活空间表示，并应用线性探测技术。

Result: 模型在激活空间中形成了可解释的线性表示，能够从姓名和职业等间接线索中预测社会人口学属性，且这些表示影响下游行为。

Conclusion: 即使通过偏见基准测试的模型也可能存在并利用隐含偏见，这对大规模应用时的公平性有影响。

Abstract: We investigate how LLMs encode sociodemographic attributes of human conversational partners inferred from indirect cues such as names and occupations. We show that LLMs develop linear representations of user demographics within activation space, wherein stereotypically associated attributes are encoded along interpretable geometric directions. We first probe residual streams across layers of four open transformer-based LLMs (Magistral 24B, Qwen3 14B, GPT-OSS 20B, OLMo2-1B) prompted with explicit demographic disclosure. We show that the same probes predict demographics from implicit cues: names activate census-aligned gender and race representations, while occupations trigger representations correlated with real-world workforce statistics. These linear representations allow us to explain demographic inferences implicitly formed by LLMs during conversation. We demonstrate that these implicit demographic representations actively shape downstream behavior, such as career recommendations. Our study further highlights that models that pass bias benchmark tests may still harbor and leverage implicit biases, with implications for fairness when applied at scale.

</details>


### [33] [Interpretable Embeddings with Sparse Autoencoders: A Data Analysis Toolkit](https://arxiv.org/abs/2512.10092)
*Nick Jiang,Xiaoqing Sun,Lisa Dunlap,Lewis Smith,Neel Nanda*

Main category: cs.AI

TL;DR: 提出使用稀疏自动编码器（SAEs）创建可解释性概念映射的SAE嵌入，用于大规模文本语料库分析，展示了其在成本效益、可靠性和可控性上的优势。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖昂贵的基于LLM的技术或密集嵌入模型，缺乏对兴趣属性的控制。该论文旨在解决这一问题，通过SAEs提供一种更具成本效益和可控性的替代方案。

Method: 采用稀疏自动编码器（SAEs）创建SAE嵌入，通过四个数据分析任务验证其有效性：(1) 比较数据集语义差异，(2) 发现文档中意外的概念相关性，(3) 过滤概念以进行兴趣轴聚类，(4) 基于属性的检索。

Result: SAE嵌入在成本上比LLMs低2-8倍，能更可靠地识别偏见，并且比密集嵌入更具可控性。实验还揭示了Grok-4在澄清模糊性方面优于其他模型，以及通过案例研究发现OpenAI模型行为的变化和Tulu-3的“触发”短语。

Conclusion: SAEs作为非结构化数据分析的通用工具，通过数据解释模型的重要性不容忽视。

Abstract: Analyzing large-scale text corpora is a core challenge in machine learning, crucial for tasks like identifying undesirable model behaviors or biases in training data. Current methods often rely on costly LLM-based techniques (e.g. annotating dataset differences) or dense embedding models (e.g. for clustering), which lack control over the properties of interest. We propose using sparse autoencoders (SAEs) to create SAE embeddings: representations whose dimensions map to interpretable concepts. Through four data analysis tasks, we show that SAE embeddings are more cost-effective and reliable than LLMs and more controllable than dense embeddings. Using the large hypothesis space of SAEs, we can uncover insights such as (1) semantic differences between datasets and (2) unexpected concept correlations in documents. For instance, by comparing model responses, we find that Grok-4 clarifies ambiguities more often than nine other frontier models. Relative to LLMs, SAE embeddings uncover bigger differences at 2-8x lower cost and identify biases more reliably. Additionally, SAE embeddings are controllable: by filtering concepts, we can (3) cluster documents along axes of interest and (4) outperform dense embeddings on property-based retrieval. Using SAE embeddings, we study model behavior with two case studies: investigating how OpenAI model behavior has changed over time and finding "trigger" phrases learned by Tulu-3 (Lambert et al., 2024) from its training data. These results position SAEs as a versatile tool for unstructured data analysis and highlight the neglected importance of interpreting models through their data.

</details>


### [34] [Robust AI Security and Alignment: A Sisyphean Endeavor?](https://arxiv.org/abs/2512.10100)
*Apostol Vassilev*

Main category: cs.AI

TL;DR: 本文通过将哥德尔不完备定理扩展到人工智能（AI），确立了AI安全和对齐稳健性的信息理论限制。


<details>
  <summary>Details</summary>
Motivation: 了解这些限制并为它们带来的挑战做好准备，对于负责任地采用AI技术至关重要。

Method: 扩展哥德尔不完备定理到AI，并证明AI系统的认知推理限制。

Result: 提供了应对这些挑战的实用方法。

Conclusion: AI系统的认知推理存在更广泛的影响和限制。

Abstract: This manuscript establishes information-theoretic limitations for robustness of AI security and alignment by extending Gödel's incompleteness theorem to AI. Knowing these limitations and preparing for the challenges they bring is critically important for the responsible adoption of the AI technology. Practical approaches to dealing with these challenges are provided as well. Broader implications for cognitive reasoning limitations of AI systems are also proven.

</details>


### [35] [Modeling Narrative Archetypes in Conspiratorial Narratives: Insights from Singapore-Based Telegram Groups](https://arxiv.org/abs/2512.10105)
*Soorya Ram Shimgekar,Abhay Goyal,Lam Yin Cheung,Roy Ka-Wei Lee,Koustuv Saha,Pi Zonooz,Navin Kumar*

Main category: cs.AI

TL;DR: 本文提出了一个两阶段计算框架，用于分析新加坡Telegram群组中的阴谋叙事，发现阴谋内容融入日常讨论而非孤立存在。


<details>
  <summary>Details</summary>
Motivation: 研究数字交流生态系统中阴谋话语的结构和传播，挑战有关在线激进化的常见假设。

Method: 第一阶段微调RoBERTa-large分类信息，第二阶段构建带符号的信念图并使用Signed Belief Graph Neural Network (SiBeGNN)进行嵌入学习。

Result: 在553,648条消息中识别出七种叙事原型，SiBeGNN的聚类质量优于基线方法，cDBI为8.38。

Conclusion: 阴谋话语不仅存在于怀疑和不信任的聚类中，还渗透在日常事务的讨论中，表明其运作在普通社交互动中，为计算方法和内容管理政策提供了新应用。

Abstract: Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.

</details>


### [36] [AgriRegion: Region-Aware Retrieval for High-Fidelity Agricultural Advice](https://arxiv.org/abs/2512.10114)
*Mesafint Fanuel,Mahmoud Nabil Mahmoud,Crystal Cook Marshal,Vishal Lakhotia,Biswanath Dari,Kaushik Roy,Shaohu Zhang*

Main category: cs.AI

TL;DR: AgriRegion是一个为农业咨询设计的高保真、区域感知的RAG框架，通过地理空间元数据注入和区域优先重排序机制，确保农业建议的本地准确性。


<details>
  <summary>Details</summary>
Motivation: 通用语言模型在农业领域常因环境差异导致建议不准确，AgriRegion旨在解决这一问题。

Method: AgriRegion采用地理空间元数据注入层和区域优先重排序机制，结合本地农业推广服务的知识库，确保检索时的地理空间约束。

Result: AgriRegion在160个农业特定问题的新基准数据集AgriRegion-Eval上，幻觉率降低了10-20%，并显著提高了信任度评分。

Conclusion: AgriRegion通过区域感知和地理空间约束，显著提升了农业建议的准确性和可靠性。

Abstract: Large Language Models (LLMs) have demonstrated significant potential in democratizing access to information. However, in the domain of agriculture, general-purpose models frequently suffer from contextual hallucination, which provides non-factual advice or answers are scientifically sound in one region but disastrous in another due to variations in soil, climate, and local regulations. We introduce AgriRegion, a Retrieval-Augmented Generation (RAG) framework designed specifically for high-fidelity, region-aware agricultural advisory. Unlike standard RAG approaches that rely solely on semantic similarity, AgriRegion incorporates a geospatial metadata injection layer and a region-prioritized re-ranking mechanism. By restricting the knowledge base to verified local agricultural extension services and enforcing geo-spatial constraints during retrieval, AgriRegion ensures that the advice regarding planting schedules, pest control, and fertilization is locally accurate. We create a novel benchmark dataset, AgriRegion-Eval, which comprises 160 domain-specific questions across 12 agricultural subfields. Experiments demonstrate that AgriRegion reduces hallucinations by 10-20% compared to state-of-the-art LLMs systems and significantly improves trust scores according to a comprehensive evaluation.

</details>


### [37] [The 2025 Foundation Model Transparency Index](https://arxiv.org/abs/2512.10169)
*Alexander Wan,Kevin Klyman,Sayash Kapoor,Nestor Maslej,Shayne Longpre,Betty Xiong,Percy Liang,Rishi Bommasani*

Main category: cs.AI

TL;DR: 2025年基础模型透明度指数显示，基础模型开发者的平均透明度得分下降，其中IBM表现突出，而xAI和Midjourney得分最低。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型开发者变得越来越重要，了解其透明度实践的变化至关重要，尤其是在政策制定者日益要求透明度的背景下。

Method: 通过引入与数据获取、使用数据、监控相关的新指标，评估包括首次参与的公司如Alibaba、DeepSeek和xAI在内的基础模型开发者的透明度。

Result: 平均得分从2024年的58降至2025年的40，IBM以95分成为亮点，xAI和Midjourney仅为14分。

Conclusion: 尽管整体透明度下降，政策制定者可以通过更积极的干预措施来改善信息缺陷，特别是在训练数据和部署后影响方面。

Abstract: Foundation model developers are among the world's most important companies. As these companies become increasingly consequential, how do their transparency practices evolve? The 2025 Foundation Model Transparency Index is the third edition of an annual effort to characterize and quantify the transparency of foundation model developers. The 2025 FMTI introduces new indicators related to data acquisition, usage data, and monitoring and evaluates companies like Alibaba, DeepSeek, and xAI for the first time. The 2024 FMTI reported that transparency was improving, but the 2025 FMTI finds this progress has deteriorated: the average score out of 100 fell from 58 in 2024 to 40 in 2025. Companies are most opaque about their training data and training compute as well as the post-deployment usage and impact of their flagship models. In spite of this general trend, IBM stands out as a positive outlier, scoring 95, in contrast to the lowest scorers, xAI and Midjourney, at just 14. The five members of the Frontier Model Forum we score end up in the middle of the Index: we posit that these companies avoid reputational harms from low scores but lack incentives to be transparency leaders. As policymakers around the world increasingly mandate certain types of transparency, this work reveals the current state of transparency for foundation model developers, how it may change given newly enacted policy, and where more aggressive policy interventions are necessary to address critical information deficits.

</details>


### [38] [An exploration for higher efficiency in multi objective optimisation with reinforcement learning](https://arxiv.org/abs/2512.10208)
*Mehmet Emin Aydin*

Main category: cs.AI

TL;DR: 本文提出了一种基于多目标强化学习的方法，通过概括经验和利用操作符池来提升优化和搜索过程的效率。


<details>
  <summary>Details</summary>
Motivation: 优化和搜索过程的效率仍然是影响优化算法性能和使用的主要挑战之一。尽管单目标优化方面已有许多工作，但多目标优化的相关研究仍较少。

Method: 提出了一种基于多目标强化学习的方法，通过概括经验和利用操作符池，来寻找最优或接近最优的操作符序列。

Result: 论文概述了所提出的方法，并展示了部分已完成的阶段和待完成的阶段，旨在证明多目标强化学习方法的效率。

Conclusion: 多目标强化学习方法在提升优化和搜索过程效率方面具有良好前景，并需要进一步研究以完善其各阶段。

Abstract: Efficiency in optimisation and search processes persists to be one of the challenges, which affects the performance and use of optimisation algorithms. Utilising a pool of operators instead of a single operator to handle move operations within a neighbourhood remains promising, but an optimum or near optimum sequence of operators necessitates further investigation. One of the promising ideas is to generalise experiences and seek how to utilise it. Although numerous works are done around this issue for single objective optimisation, multi-objective cases have not much been touched in this regard. A generalised approach based on multi-objective reinforcement learning approach seems to create remedy for this issue and offer good solutions. This paper overviews a generalisation approach proposed with certain stages completed and phases outstanding that is aimed to help demonstrate the efficiency of using multi-objective reinforcement learning.

</details>


### [39] [ID-PaS : Identity-Aware Predict-and-Search for General Mixed-Integer Linear Programs](https://arxiv.org/abs/2512.10211)
*Junyang Cai,El Mehdi Er Raqabi,Pascal Van Hentenryck,Bistra Dilkina*

Main category: cs.AI

TL;DR: 本文扩展了Predict-and-Search（PaS）框架至参数化混合整数线性规划（MIP），并引入ID-PaS，一个能更有效地处理异构变量的身份感知学习框架。


<details>
  <summary>Details</summary>
Motivation: Predict-and-Search方法通过预测模型估计有前景的变量分配，并引导搜索过程寻找高质量解。然而，现有的方法局限于二元问题，且忽略了实际设置中常见的固定变量。因此，本文旨在扩展PaS框架以处理更广泛的MIP问题。

Method: 本文引入了ID-PaS框架，通过身份感知学习，使ML模型能更好地处理异构变量。实验在多个现实世界的大规模问题中进行。

Result: 实验表明，ID-PaS在与最先进求解器Gurobi和PaS的比较中，始终表现出更优的性能。

Conclusion: ID-PaS框架在处理实际世界中的复杂MIP问题上具有显著优势，通过引入身份感知学习，提高了ML模型在PaS框架中的有效性。

Abstract: Mixed-Integer Linear Programs (MIPs) are powerful and flexible tools for modeling a wide range of real-world combinatorial optimization problems. Predict-and-Search methods operate by using a predictive model to estimate promising variable assignments and then guiding a search procedure toward high-quality solutions. Recent research has demonstrated that incorporating machine learning (ML) into the Predict-and-Search framework significantly enhances its performance. Still, it is restricted to binary problems and overlooks the presence of fixed variables that commonly arise in practical settings. This work extends the Predict-and-Search (PaS) framework to parametric MIPs and introduces ID-PaS, an identity-aware learning framework that enables the ML model to handle heterogeneous variables more effectively. Experiments on several real-world large-scale problems demonstrate that ID-PaS consistently achieves superior performance compared to the state-of-the-art solver Gurobi and PaS.

</details>


### [40] [Reverse Thinking Enhances Missing Information Detection in Large Language Models](https://arxiv.org/abs/2512.10273)
*Yuxin Liu,Chaojie Gu,Yihang Zhang,Bin Qian,Shibo He*

Main category: cs.AI

TL;DR: 本文提出了一种利用反向思维框架增强大语言模型在缺失信息检测任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理涉及缺失信息的任务时，常常表现出不完整的回答、事实错误和幻觉等问题。尽管前向推理方法（如思维链和思维树）在结构化问题解决方面取得了成功，但它们往往无法系统地识别和恢复被忽略的信息。

Method: 受最近关于反向推理工作的启发，作者提出了一种新颖的框架，通过反向思维引导大语言模型识别必要条件并定位缺失元素，将缺失信息识别这一具有挑战性的任务转化为更易处理的反向推理问题。

Result: 实验结果表明，与传统的前向推理方法相比，反向思维方法在缺失信息检测任务中取得了显著的性能提升。

Conclusion: 该反向思维框架为提高大语言模型的逻辑完整性和推理鲁棒性提供了一个有前景的方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks, yet they often struggle with problems involving missing information, exhibiting issues such as incomplete responses, factual errors, and hallucinations. While forward reasoning approaches like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) have shown success in structured problem-solving, they frequently fail to systematically identify and recover omitted information. In this paper, we explore the potential of reverse thinking methodologies to enhance LLMs' performance on missing information detection tasks. Drawing inspiration from recent work on backward reasoning, we propose a novel framework that guides LLMs through reverse thinking to identify necessary conditions and pinpoint missing elements. Our approach transforms the challenging task of missing information identification into a more manageable backward reasoning problem, significantly improving model accuracy. Experimental results demonstrate that our reverse thinking approach achieves substantial performance gains compared to traditional forward reasoning methods, providing a promising direction for enhancing LLMs' logical completeness and reasoning robustness.

</details>


### [41] [Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules](https://arxiv.org/abs/2512.10300)
*Yanbei Jiang,Xueqi Ma,Shu Liu,Sarah Monazam Erfani,Tongliang Liu,James Bailey,Jey Han Lau,Krista A. Ehinger*

Main category: cs.AI

TL;DR: 本文提出了一种新的可解释性框架，以系统分析视觉-语言模型（VLMs）的内部机制，专注于注意力头在 multimodal reasoning 中的功能角色。


<details>
  <summary>Details</summary>
Motivation: 尽管 VLMs 在多模态基准测试中表现优异，但其内部工作机制仍不透明，需要进一步研究其认知组织方式。

Method: 引入了 CogVision 数据集，将复杂的多模态问题分解为逐步的子问题，通过探测方法识别专门执行特定认知功能的注意力头。

Result: 分析发现这些功能头普遍稀疏，数量和在功能间的分布各不相同，并在多模态推理中起中介作用；干预实验显示去除功能头会导致性能下降。

Conclusion: 这些发现揭示了 VLMs 的认知组织新见解，并为设计更符合人类感知和推理能力的模型提供了方向。

Abstract: Despite excelling on multimodal benchmarks, vision-language models (VLMs) largely remain a black box. In this paper, we propose a novel interpretability framework to systematically analyze the internal mechanisms of VLMs, focusing on the functional roles of attention heads in multimodal reasoning. To this end, we introduce CogVision, a dataset that decomposes complex multimodal questions into step-by-step subquestions designed to simulate human reasoning through a chain-of-thought paradigm, with each subquestion associated with specific receptive or cognitive functions such as high-level visual reception and inference. Using a probing-based methodology, we identify attention heads that specialize in these functions and characterize them as functional heads. Our analysis across diverse VLM families reveals that these functional heads are universally sparse, vary in number and distribution across functions, and mediate interactions and hierarchical organization. Furthermore, intervention experiments demonstrate their critical role in multimodal reasoning: removing functional heads leads to performance degradation, while emphasizing them enhances accuracy. These findings provide new insights into the cognitive organization of VLMs and suggest promising directions for designing models with more human-aligned perceptual and reasoning abilities.

</details>


### [42] [Trustworthy Orchestration Artificial Intelligence by the Ten Criteria with Control-Plane Governance](https://arxiv.org/abs/2512.10304)
*Byeong Ho Kang,Wenli Yang,Muhammad Bilal Amin*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Ten Criteria for Trustworthy Orchestration AI”的综合保证框架，旨在将治理嵌入AI系统的执行结构中，以确保其可信度。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在重要决策中扮演越来越重要的角色，技术能力和制度责任之间出现了差距。仅靠伦理指导无法解决这一挑战，需要架构将治理嵌入到生态系统的执行结构中。

Method: 提出了一个集成人类输入、语义一致性、审计和来源完整性到一个统一的控制面板架构中的综合保证框架。

Result: 该框架借鉴国际标准和澳大利亚国家AI保证框架倡议，展示了可信度可以通过工程方式系统地纳入AI系统。

Conclusion: 该框架确保AI系统的执行结构保持可验证、透明、可重现，并在有意义的人类控制之下。

Abstract: As Artificial Intelligence (AI) systems increasingly assume consequential decision-making roles, a widening gap has emerged between technical capabilities and institutional accountability. Ethical guidance alone is insufficient to counter this challenge; it demands architectures that embed governance into the execution fabric of the ecosystem. This paper presents the Ten Criteria for Trustworthy Orchestration AI, a comprehensive assurance framework that integrates human input, semantic coherence, audit and provenance integrity into a unified Control-Panel architecture. Unlike conventional agentic AI initiatives that primarily focus on AI-to-AI coordination, the proposed framework provides an umbrella of governance to the entire AI components, their consumers and human participants. By taking aspiration from international standards and Australia's National Framework for AI Assurance initiative, this work demonstrates that trustworthiness can be systematically incorporated (by engineering) into AI systems, ensuring the execution fabric remains verifiable, transparent, reproducible and under meaningful human control.

</details>


### [43] [EpiPlanAgent: Agentic Automated Epidemic Response Planning](https://arxiv.org/abs/2512.10313)
*Kangkun Mao,Fang Xu,Jinru Ding,Yidong Jiang,Yujun Yao,Yirong Chen,Junming Liu,Xiaoqin Wu,Qian Wu,Xiaoyan Huang,Jie Xu*

Main category: cs.AI

TL;DR: 设计并评估EpiPlanAgent，这是一个基于大型语言模型（LLMs）的智能体系统，用于自动化生成和验证数字应急响应计划。


<details>
  <summary>Details</summary>
Motivation: 传统的疫情响应规划依赖劳动密集型的人工方法，需要设计一个自动化系统来提高效率和准确性。

Method: 采用多智能体框架，集成任务分解、知识基础和模拟模块，由公共卫生专业人员使用真实疫情场景进行测试。

Result: EpiPlanAgent显著提高了计划的完整性和指南符合性，同时大幅减少了开发时间，专家评估确认AI生成内容与人工创作内容高度一致。

Conclusion: EpiPlanAgent为智能疫情响应规划提供了有效且可扩展的解决方案，展示了智能体AI在改善公共卫生准备中的潜力。

Abstract: Epidemic response planning is essential yet traditionally reliant on labor-intensive manual methods. This study aimed to design and evaluate EpiPlanAgent, an agent-based system using large language models (LLMs) to automate the generation and validation of digital emergency response plans. The multi-agent framework integrated task decomposition, knowledge grounding, and simulation modules. Public health professionals tested the system using real-world outbreak scenarios in a controlled evaluation. Results demonstrated that EpiPlanAgent significantly improved the completeness and guideline alignment of plans while drastically reducing development time compared to manual workflows. Expert evaluation confirmed high consistency between AI-generated and human-authored content. User feedback indicated strong perceived utility. In conclusion, EpiPlanAgent provides an effective, scalable solution for intelligent epidemic response planning, demonstrating the potential of agentic AI to transform public health preparedness.

</details>


### [44] [User-Feedback-Driven Continual Adaptation for Vision-and-Language Navigation](https://arxiv.org/abs/2512.10322)
*Yongqiang Yu,Xuhui Li,Hazza Mahmood,Jinxing Zhou,Haodong Hong,Longtao Jiang,Zhiqiang Xu,Qi Wu,Xiaojun Chang*

Main category: cs.AI

TL;DR: 本文提出了一种基于用户反馈的GSA-VLN框架，通过将用户反馈转化为高质量训练数据，并结合记忆库预热机制，提升导航智能体在复杂环境中的持续适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前的GSA-VLN框架忽略了用户反馈，仅依赖无监督适应。然而，用户反馈在实际应用中提供了宝贵且有针对性的监督信息，可以显著提升适应质量。

Method: 引入用户反馈驱动的适应框架，将用户反馈（导航指令和纠正信号）转化为高质量、环境对齐的训练数据；采用记忆库预热机制，重用先前获取的环境知识，以缓解冷启动问题并确保稳定的重新部署。

Result: 在GSA-R2R基准测试中，所提方法持续超越如GR-DUET等强基线，提高了导航成功率和路径效率；记忆库预热机制稳定了早期导航并减少了更新后的性能下降。

Conclusion: 该方法在连续和混合适应设置下均表现出稳健性和通用性，在各种部署条件下实现了持续的改进，为VLN的实际应用提供了有效解决方案。

Abstract: Vision-and-Language Navigation (VLN) requires agents to navigate complex environments by following natural-language instructions. General Scene Adaptation for VLN (GSA-VLN) shifts the focus from zero-shot generalization to continual, environment-specific adaptation, narrowing the gap between static benchmarks and real-world deployment. However, current GSA-VLN frameworks exclude user feedback, relying solely on unsupervised adaptation from repeated environmental exposure. In practice, user feedback offers natural and valuable supervision that can significantly enhance adaptation quality. We introduce a user-feedback-driven adaptation framework that extends GSA-VLN by systematically integrating human interactions into continual learning. Our approach converts user feedback-navigation instructions and corrective signals-into high-quality, environment-aligned training data, enabling efficient and realistic adaptation. A memory-bank warm-start mechanism further reuses previously acquired environmental knowledge, mitigating cold-start degradation and ensuring stable redeployment. Experiments on the GSA-R2R benchmark show that our method consistently surpasses strong baselines such as GR-DUET, improving navigation success and path efficiency. The memory-bank warm start stabilizes early navigation and reduces performance drops after updates. Results under both continual and hybrid adaptation settings confirm the robustness and generality of our framework, demonstrating sustained improvement across diverse deployment conditions.

</details>


### [45] [On the Collapse of Generative Paths: A Criterion and Correction for Diffusion Steering](https://arxiv.org/abs/2512.10339)
*Ziseok Lee,Minyeong Hwang,Sanghyun Jo,Wooyeol Lee,Jihyung Ko,Young Bin Park,Jae-Mun Choi,Eunho Yang,Kyungsu Kim*

Main category: cs.AI

TL;DR: 该论文提出了一种新的方法ACE，通过引入时间变化指数的自适应路径校正，解决了在推理时组合不同噪声调度或数据集的扩散模型时出现的“边缘路径崩溃”问题。


<details>
  <summary>Details</summary>
Motivation: 在无需重新训练的情况下，使预训练的扩散/流模型适应新任务是重要的研究方向。然而，现有的密度比方法在组合不同模型时容易出现边缘路径崩溃，导致中间密度无法正常化，这限制了方法的应用范围。

Method: 论文提出了一种简单路径存在性判据，可以预测崩溃发生的时机，并引入了自适应路径校正与指数（ACE）方法，扩展了Feynman-Kac引导以处理时间变化指数，从而保证有效的概率路径。

Result: 在合成2D基准测试和灵活姿态支架装饰任务中，ACE方法消除了崩溃问题，并实现了高质量的组合生成，其性能优于恒定指数基线及专门的支架装饰模型。

Conclusion: 该工作将原本不稳定的密度比方法转变为可控生成的可靠工具，为组合不同模型进行推理时提供了稳定的解决方案。

Abstract: Inference-time steering enables pretrained diffusion/flow models to be adapted to new tasks without retraining. A widely used approach is the ratio-of-densities method, which defines a time-indexed target path by reweighting probability-density trajectories from multiple models with positive, or in some cases, negative exponents. This construction, however, harbors a critical and previously unformalized failure mode: Marginal Path Collapse, where intermediate densities become non-normalizable even though endpoints remain valid. Collapse arises systematically when composing heterogeneous models trained on different noise schedules or datasets, including a common setting in molecular design where de-novo, conformer, and pocket-conditioned models must be combined for tasks such as flexible-pose scaffold decoration. We provide a novel and complete solution for the problem. First, we derive a simple path existence criterion that predicts exactly when collapse occurs from noise schedules and exponents alone. Second, we introduce Adaptive path Correction with Exponents (ACE), which extends Feynman-Kac steering to time-varying exponents and guarantees a valid probability path. On a synthetic 2D benchmark and on flexible-pose scaffold decoration, ACE eliminates collapse and enables high-guidance compositional generation, improving distributional and docking metrics over constant-exponent baselines and even specialized task-specific scaffold decoration models. Our work turns ratio-of-densities steering with heterogeneous experts from an unstable heuristic into a reliable tool for controllable generation.

</details>


### [46] [REMISVFU: Vertical Federated Unlearning via Representation Misdirection for Intermediate Output Feature](https://arxiv.org/abs/2512.10348)
*Wenhan Wu,Zhili He,Huanghuang Liang,Yili Gong,Jiawei Jiang,Chuang Hu,Dazhao Cheng*

Main category: cs.AI

TL;DR: 提出了一种用于splitVFL系统的快速、客户端级别的遗忘框架REMISVFU。


<details>
  <summary>Details</summary>
Motivation: 现有的遗忘技术主要集中在水平联邦学习（HFL），而在垂直联邦学习（VFL）中，由于特征分区架构，这些方法无效。

Method: 提出了一个即插即用的表示误导框架REMISVFU，通过在收到删除请求时，将遗忘方的编码器输出坍缩为单位球上的随机采样锚点，并通过正交投影来对齐保持损失和遗忘损失的梯度，从而消除破坏性干扰。

Result: 在公共基准测试中，REMISVFU将后门攻击成功率抑制到自然类别先验水平，仅牺牲约2.5%的干净准确率，优于最先进的基线方法。

Conclusion: REMISVFU是一个有效的解决方案，能够在splitVFL系统中实现快速、客户端级别的遗忘，同时保持其他参与方的模型效用。

Abstract: Data-protection regulations such as the GDPR grant every participant in a federated system a right to be forgotten. Federated unlearning has therefore emerged as a research frontier, aiming to remove a specific party's contribution from the learned model while preserving the utility of the remaining parties. However, most unlearning techniques focus on Horizontal Federated Learning (HFL), where data are partitioned by samples. In contrast, Vertical Federated Learning (VFL) allows organizations that possess complementary feature spaces to train a joint model without sharing raw data. The resulting feature-partitioned architecture renders HFL-oriented unlearning methods ineffective. In this paper, we propose REMISVFU, a plug-and-play representation misdirection framework that enables fast, client-level unlearning in splitVFL systems. When a deletion request arrives, the forgetting party collapses its encoder output to a randomly sampled anchor on the unit sphere, severing the statistical link between its features and the global model. To maintain utility for the remaining parties, the server jointly optimizes a retention loss and a forgetting loss, aligning their gradients via orthogonal projection to eliminate destructive interference. Evaluations on public benchmarks show that REMISVFU suppresses back-door attack success to the natural class-prior level and sacrifices only about 2.5% points of clean accuracy, outperforming state-of-the-art baselines.

</details>


### [47] [LLM-Empowered Representation Learning for Emerging Item Recommendation](https://arxiv.org/abs/2512.10370)
*Ziying Zhang,Quanming Yao,Yaqing Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为EmerFlow的LLM驱动的新兴物品推荐框架，能够生成具有独特性的嵌入表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了新兴物品随时间累积互动的动态过程，并且假设新兴物品几乎没有历史互动，这种假设过于简化。

Method: EmerFlow利用LLM推理丰富新兴物品的特征，并将其与现有推荐模型的嵌入空间对齐，通过元学习整合新的互动以优化嵌入。

Result: 在多个领域（包括电影和医药）的广泛实验中，EmerFlow持续优于现有方法。

Conclusion: EmerFlow通过LLM推理和元学习，能够从有限的互动中为新兴物品学习富有表现力的嵌入。

Abstract: In this work, we tackle the challenge of recommending emerging items, whose interactions gradually accumulate over time. Existing methods often overlook this dynamic process, typically assuming that emerging items have few or even no historical interactions. Such an assumption oversimplifies the problem, as a good model must preserve the uniqueness of emerging items while leveraging their shared patterns with established ones. To address this challenge, we propose EmerFlow, a novel LLM-empowered representation learning framework that generates distinctive embeddings for emerging items. It first enriches the raw features of emerging items through LLM reasoning, then aligns these representations with the embedding space of the existing recommendation model. Finally, new interactions are incorporated through meta-learning to refine the embeddings. This enables EmerFlow to learn expressive embeddings for emerging items from only limited interactions. Extensive experiments across diverse domains, including movies and pharmaceuticals, show that EmerFlow consistently outperforms existing methods.

</details>


### [48] [Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention](https://arxiv.org/abs/2512.10414)
*Yang Yu,Zhuangzhuang Chen,Siqi Wang,Lanqing Li,Xiaomeng Li*

Main category: cs.AI

TL;DR: 提出了一种新的方法SaEI，通过选择性对抗性熵干预来提升视觉语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的RL微调方法忽略了RL采样过程中的熵干预，因此作者提出在采样过程中通过对抗性目标来增强策略熵，以提高响应的多样性。

Method: 提出SaEI方法，包含熵引导对抗采样（EgAS）和选择性熵计算（TsEC），通过扰乱视觉输入和选择性地干预熵来增强策略探索。

Result: 在域内和域外数据集上的广泛实验表明，所提出的方法能显著提升策略探索，增强推理能力。

Conclusion: 通过熵干预，SaEI方法在提高视觉语言模型的推理能力方面具有显著效果，代码将在论文被接受后发布。

Abstract: Recently, reinforcement learning (RL) has become a common choice in enhancing the reasoning capabilities of vision-language models (VLMs). Considering existing RL- based finetuning methods, entropy intervention turns out to be an effective way to benefit exploratory ability, thereby improving policy performance. Notably, most existing stud- ies intervene in entropy by simply controlling the update of specific tokens during policy optimization of RL. They ig- nore the entropy intervention during the RL sampling that can boost the performance of GRPO by improving the di- versity of responses. In this paper, we propose Selective- adversarial Entropy Intervention, namely SaEI, which en- hances policy entropy by distorting the visual input with the token-selective adversarial objective coming from the en- tropy of sampled responses. Specifically, we first propose entropy-guided adversarial sampling (EgAS) that formu- lates the entropy of sampled responses as an adversarial ob- jective. Then, the corresponding adversarial gradient can be used to attack the visual input for producing adversarial samples, allowing the policy model to explore a larger an- swer space during RL sampling. Then, we propose token- selective entropy computation (TsEC) to maximize the ef- fectiveness of adversarial attack in EgAS without distorting factual knowledge within VLMs. Extensive experiments on both in-domain and out-of-domain datasets show that our proposed method can greatly improve policy exploration via entropy intervention, to boost reasoning capabilities. Code will be released once the paper is accepted.

</details>


### [49] [Representation of the structure of graphs by sequences of instructions](https://arxiv.org/abs/2512.10429)
*Ezequiel Lopez-Rubio*

Main category: cs.AI

TL;DR: 提出了一种将图的邻接矩阵表示为指令字符串的新方法，以便深度学习模型处理。


<details>
  <summary>Details</summary>
Motivation: 当前图的表示方法不适合深度学习语言模型处理，需要一种新的表示方式。

Method: 将图的邻接矩阵通过一系列简单指令逐步构建，形成字符串表示，且该过程可逆。

Result: 该表示方法紧凑，并保留了图的局部结构特征，初步计算实验结果良好。

Conclusion: 这种新方法有望提升深度学习模型对图数据的处理能力。

Abstract: The representation of graphs is commonly based on the adjacency matrix concept. This formulation is the foundation of most algebraic and computational approaches to graph processing. The advent of deep learning language models offers a wide range of powerful computational models that are specialized in the processing of text. However, current procedures to represent graphs are not amenable to processing by these models. In this work, a new method to represent graphs is proposed. It represents the adjacency matrix of a graph by a string of simple instructions. The instructions build the adjacency matrix step by step. The transformation is reversible, i.e. given a graph the string can be produced and vice versa. The proposed representation is compact and it maintains the local structural patterns of the graph. Therefore, it is envisaged that it could be useful to boost the processing of graphs by deep learning models. A tentative computational experiment is reported, with favorable results.

</details>


### [50] [Targeted Data Protection for Diffusion Model by Matching Training Trajectory](https://arxiv.org/abs/2512.10433)
*Hojun Lee,Mijin Koo,Yeji Song,Nojun Kwak*

Main category: cs.AI

TL;DR: TAFAP是首个通过控制整个训练轨迹实现有效目标数据保护(TDP)的方法，解决了现有TDP方法可控性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型个性化微调存在数据滥用和隐私风险，当前保护方法只能被动降质而无法稳定控制，TDP虽有前景但受制于快照匹配方法的控制力不足。

Method: 提出TAFAP框架，采用基于数据集蒸馏的轨迹匹配方法，通过对抗性扰动在整个微调过程中强制实施持续可验证的转换。

Result: 实验验证了TAFAP在扩散模型上首次实现成功目标转换，可同时控制身份和视觉特征，显著优于现有TDP方法且保持高图像质量。

Conclusion: TAFAP实现了可验证的数据保护，为控制扩散模型输出修改提供了新框架，解决了训练轨迹控制的关键难题。

Abstract: Recent advancements in diffusion models have made fine-tuning text-to-image models for personalization increasingly accessible, but have also raised significant concerns regarding unauthorized data usage and privacy infringement. Current protection methods are limited to passively degrading image quality, failing to achieve stable control. While Targeted Data Protection (TDP) offers a promising paradigm for active redirection toward user-specified target concepts, existing TDP attempts suffer from poor controllability due to snapshot-matching approaches that fail to account for complete learning dynamics. We introduce TAFAP (Trajectory Alignment via Fine-tuning with Adversarial Perturbations), the first method to successfully achieve effective TDP by controlling the entire training trajectory. Unlike snapshot-based methods whose protective influence is easily diluted as training progresses, TAFAP employs trajectory-matching inspired by dataset distillation to enforce persistent, verifiable transformations throughout fine-tuning. We validate our method through extensive experiments, demonstrating the first successful targeted transformation in diffusion models with simultaneous control over both identity and visual patterns. TAFAP significantly outperforms existing TDP attempts, achieving robust redirection toward target concepts while maintaining high image quality. This work enables verifiable safeguards and provides a new framework for controlling and tracing alterations in diffusion model outputs.

</details>


### [51] [When Reject Turns into Accept: Quantifying the Vulnerability of LLM-Based Scientific Reviewers to Indirect Prompt Injection](https://arxiv.org/abs/2512.10449)
*Devanshu Sahoo,Manish Prasad,Vasudev Majhi,Jahnvi Singh,Vinay Chamola,Yash Sinha,Murari Mandal,Dhruv Kumar*

Main category: cs.AI

TL;DR: 研究调查了“LLM-as-a-Judge”系统在面对恶意PDF操控时的稳健性，并提出了一种新的评估指标WAVS。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在同行评审中的广泛应用，出现了滥用风险，特别是在操控评审结果方面。

Method: 构建200篇科学论文的数据集，并针对13种语言模型，采用15种领域特定的攻击策略进行评估。

Result: 发现像“Maximum Mark Magyk”这样的混淆策略能有效操控评分，甚至在大模型中也能实现高的决策翻转率。

Conclusion: 该研究揭示了当前LLM评审系统的脆弱性，并计划发布数据集和注入框架以促进进一步研究。

Abstract: The landscape of scientific peer review is rapidly evolving with the integration of Large Language Models (LLMs). This shift is driven by two parallel trends: the widespread individual adoption of LLMs by reviewers to manage workload (the "Lazy Reviewer" hypothesis) and the formal institutional deployment of AI-powered assessment systems by conferences like AAAI and Stanford's Agents4Science. This study investigates the robustness of these "LLM-as-a-Judge" systems (both illicit and sanctioned) to adversarial PDF manipulation. Unlike general jailbreaks, we focus on a distinct incentive: flipping "Reject" decisions to "Accept," for which we develop a novel evaluation metric which we term as WAVS (Weighted Adversarial Vulnerability Score). We curated a dataset of 200 scientific papers and adapted 15 domain-specific attack strategies to this task, evaluating them across 13 Language Models, including GPT-5, Claude Haiku, and DeepSeek. Our results demonstrate that obfuscation strategies like "Maximum Mark Magyk" successfully manipulate scores, achieving alarming decision flip rates even in large-scale models. We will release our complete dataset and injection framework to facilitate more research on this topic.

</details>


### [52] [Planning, Living and Judging: A Multi-agent LLM-based Framework for Cyclical Urban Planning](https://arxiv.org/abs/2412.20505)
*Hang Ni,Yuzhi Wang,Hao Liu*

Main category: cs.AI

TL;DR: 提出了一种利用大语言模型的城市再生规划方法，通过循环生成、评估和优化城市规划方案。


<details>
  <summary>Details</summary>
Motivation: 城市化进程中的城市再生面临重大挑战，需要适应不断变化的需求，因此需要一个动态且响应迅速的规划方法。

Method: 提出了一种基于多智能体大语言模型的循环城市规划（CUP）框架，包括规划、模拟和评估三个关键组件。

Result: 在真实数据集上的实验证明了该框架作为一种连续且自适应规划过程的有效性。

Conclusion: CUP框架利用大语言模型的能力，实现了动态和响应迅速的城市规划，为城市再生提供了一种新的范式。

Abstract: Urban regeneration presents significant challenges within the context of urbanization, requiring adaptive approaches to tackle evolving needs. Leveraging advancements in large language models (LLMs), we propose Cyclical Urban Planning (CUP), a new paradigm that continuously generates, evaluates, and refines urban plans in a closed-loop. Specifically, our multi-agent LLM-based framework consists of three key components: (1) Planning, where LLM agents generate and refine urban plans based on contextual data; (2) Living, where agents simulate the behaviors and interactions of residents, modeling life in the urban environment; and (3) Judging, which involves evaluating plan effectiveness and providing iterative feedback for improvement. The cyclical process enables a dynamic and responsive planning approach. Experiments on the real-world dataset demonstrate the effectiveness of our framework as a continuous and adaptive planning process.

</details>


### [53] [Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning](https://arxiv.org/abs/2512.10534)
*Haiteng Zhao,Junhao Shen,Yiming Zhang,Songyang Gao,Kuikun Liu,Tianyou Ma,Fan Zheng,Dahua Lin,Wenwei Zhang,Kai Chen*

Main category: cs.AI

TL;DR: 本文提出了InternGeometry，一种能够解决国际数学奥林匹克（IMO）几何问题的大模型智能体，其通过迭代提出命题和辅助构造、验证和反馈来克服几何启发式的局限性。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解中，由于辅助构造的启发式较弱，AI 主要依赖专家模型，并且需要大量的数据合成和搜索，这限制了大模型智能体在几何方面的表现。因此，本文旨在构建一个能够克服这些限制的 medalist-level 大模型智能体。

Method: InternGeometry 通过迭代提出命题和辅助构造，并使用符号引擎进行验证，根据引擎的反馈指导后续提议。其动态记忆机制允许每个问题超过两百次与符号引擎的交互。此外，引入复杂度提升强化学习（CBRL）来加速学习过程。

Result: 基于 InternThinker-32B，InternGeometry 解决了 50 个 IMO 几何问题中的 44 个（2000-2024），超过平均金牌得主得分（40.9），仅使用了 13K 个训练样本，是 AlphaGeometry 2 所用数据的 0.004%。

Conclusion: InternGeometry 展示了大模型智能体在专家级几何任务上的潜力，能够提出人类解决方案中未出现的新型辅助构造。该模型、数据和符号引擎的发布将支持未来研究。

Abstract: Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.

</details>


### [54] [NormCode: A Semi-Formal Language for Context-Isolated AI Planning](https://arxiv.org/abs/2512.10563)
*Xin Guan*

Main category: cs.AI

TL;DR: NormCode是一种半形式化语言，通过结构化分解和多步骤数据隔离设计，解决了LLM多步工作流中的上下文污染问题。


<details>
  <summary>Details</summary>
Motivation: 多步工作流中的信息累积导致模型产生幻觉、混淆中间输出和丢失任务约束。需要一种方法来确保步骤间的信息隔离和任务透明性。

Method: NormCode通过严格分离语义操作（LLM驱动的推理）和句法操作（确定性数据重组），并采用三种同构格式（.ncds, .ncd, .ncn）支持从草图到生产的过程。

Result: 验证包括：(1) 实现100%准确率的任意长度X加法算法；(2) 自托管执行NormCode的五阶段编译器管道。

Conclusion: NormCode通过依赖驱动的调度、SQLite支持的检查点和循环管理，使AI工作流可审计，满足法律推理、医疗决策和金融分析等高要求领域的透明性需求。

Abstract: Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.

</details>


### [55] [Phythesis: Physics-Guided Evolutionary Scene Synthesis for Energy-Efficient Data Center Design via LLMs](https://arxiv.org/abs/2512.10611)
*Minghao LI,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.AI

TL;DR: 提出Phythesis，一个结合大语言模型（LLMs）和物理引导进化优化的新型框架，用于自动化设计节能数据中心。


<details>
  <summary>Details</summary>
Motivation: 传统数据中心设计方法在系统复杂性增加时扩展性差，且现有生成人工智能方法不考虑基础物理，不适合数据中心设计。

Method: Phythesis采用迭代双层优化架构，LLM驱动优化生成三维布局并自我批评优化，物理引导优化确定最佳资产参数和组合。

Result: 在三个生成尺度上，Phythesis相比纯LLM方案生成成功率提高57.3%，功耗效率提高11.5%。

Conclusion: Phythesis框架有效结合LLMs和物理引导优化，能自动化生成节能且物理上可行的数据中心设计。

Abstract: Data center (DC) infrastructure serves as the backbone to support the escalating demand for computing capacity. Traditional design methodologies that blend human expertise with specialized simulation tools scale poorly with the increasing system complexity. Recent studies adopt generative artificial intelligence to design plausible human-centric indoor layouts. However, they do not consider the underlying physics, making them unsuitable for the DC design that sets quantifiable operational objectives and strict physical constraints. To bridge the gap, we propose Phythesis, a novel framework that synergizes large language models (LLMs) and physics-guided evolutionary optimization to automate simulation-ready (SimReady) scene synthesis for energy-efficient DC design. Phythesis employs an iterative bi-level optimization architecture, where (i) the LLM-driven optimization level generates physically plausible three-dimensional layouts and self-criticizes them to refine the scene topology, and (ii) the physics-informed optimization level identifies the optimal asset parameters and selects the best asset combination. Experiments on three generation scales show that Phythesis achieves 57.3% generation success rate increase and 11.5% power usage effectiveness (PUE) improvement, compared with the vanilla LLM-based solution.

</details>


### [56] [Refinement Contrastive Learning of Cell-Gene Associations for Unsupervised Cell Type Identification](https://arxiv.org/abs/2512.10640)
*Liang Peng,Haopeng Liu,Yixuan Ye,Cheng Liu,Wenjun Shen,Si Wu,Hau-San Wong*

Main category: cs.AI

TL;DR: 提出了一种新的无监督细胞类型识别框架scRCL，通过引入细胞-基因相互作用来提升细胞类型识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的聚类方法只关注细胞内部结构，忽略了细胞-基因关联的关键作用，因此难以区分密切相关的细胞类型。

Method: 引入了一种Refinement Contrastive Learning框架（scRCL），包括两个对比分布对齐组件和一个整合基因相关性结构学习模块，以增强细胞嵌入表示。

Result: 在多个单细胞RNA-seq和空间转录组数据集上的实验表明，该方法在细胞类型识别准确性上优于现有最先进方法，并展现出有意义的基因表达特征。

Conclusion: 通过有效利用细胞-基因相互作用，scRCL能够更准确地识别细胞类型，并提供具有生物学意义的细胞表示。

Abstract: Unsupervised cell type identification is crucial for uncovering and characterizing heterogeneous populations in single cell omics studies. Although a range of clustering methods have been developed, most focus exclusively on intrinsic cellular structure and ignore the pivotal role of cell-gene associations, which limits their ability to distinguish closely related cell types. To this end, we propose a Refinement Contrastive Learning framework (scRCL) that explicitly incorporates cell-gene interactions to derive more informative representations. Specifically, we introduce two contrastive distribution alignment components that reveal reliable intrinsic cellular structures by effectively exploiting cell-cell structural relationships. Additionally, we develop a refinement module that integrates gene-correlation structure learning to enhance cell embeddings by capturing underlying cell-gene associations. This module strengthens connections between cells and their associated genes, refining the representation learning to exploiting biologically meaningful relationships. Extensive experiments on several single-cell RNA-seq and spatial transcriptomics benchmark datasets demonstrate that our method consistently outperforms state-of-the-art baselines in cell-type identification accuracy. Moreover, downstream biological analyses confirm that the recovered cell populations exhibit coherent gene-expression signatures, further validating the biological relevance of our approach. The code is available at https://github.com/THPengL/scRCL.

</details>


### [57] [Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly](https://arxiv.org/abs/2512.10787)
*Moshe Lahmy,Roi Yozevitch*

Main category: cs.AI

TL;DR: 提出SEAL-RAG，通过替换而非扩展上下文来解决多跳查询中的上下文稀释问题。


<details>
  <summary>Details</summary>
Motivation: RAG系统在处理多跳查询时，初始检索可能遗漏关键信息，而现有方法通过增加或修剪上下文来解决，但容易导致上下文稀释。

Method: SEAL-RAG采用“替换，不扩展”的策略，通过搜索、提取、评估和循环（SEAL）的周期，动态替换不相关内容，填补信息空缺。

Result: 在HotpotQA和2WikiMultiHopQA上，SEAL-RAG在答案正确性和证据精确性上均优于现有方法，统计显著性明显。

Conclusion: SEAL-RAG通过固定k替换策略，优化了top-k的精确性，避免了上下文稀释，同时保持成本的可预测性。

Abstract: Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \textbf{context dilution}, where distractors crowd out relevant information. We propose \textbf{SEAL-RAG}, a training-free controller that adopts a \textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\textbf{S}earch $\rightarrow$ \textbf{E}xtract $\rightarrow$ \textbf{A}ssess $\rightarrow$ \textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \textbf{HotpotQA} and \textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \textbf{+3--13 pp} and evidence precision by \textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \textbf{+8.0 pp} in accuracy and maintains \textbf{96\%} evidence precision compared to 22\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.

</details>


### [58] [CAPTAIN: Semantic Feature Injection for Memorization Mitigation in Text-to-Image Diffusion Models](https://arxiv.org/abs/2512.10655)
*Tong Zhang,Carlos Hinojosa,Bernard Ghanem*

Main category: cs.AI

TL;DR: CAPTAIN是一个无需训练的框架，通过在去噪过程中直接修改潜在特征来减轻扩散模型中的记忆问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能会无意中复制训练数据，引发隐私和版权问题，尤其是在大规模部署时。现有的减少记忆的方法通常会影响提示的匹配度，因此需要一种新方法。

Method: CAPTAIN采用频率基础噪声初始化、识别最优去噪时间步骤、定位记忆区域，并从非记忆参考图像注入语义对齐的特征。

Result: 实验表明，与基于CFG的基线相比，CAPTAIN在减少记忆的同时，保持了对提示的良好匹配和视觉质量。

Conclusion: CAPTAIN通过直接在去噪过程中修改潜在特征，有效地减轻了扩散模型中的记忆问题，同时保持了提示匹配和视觉质量。

Abstract: Diffusion models can unintentionally reproduce training examples, raising privacy and copyright concerns as these systems are increasingly deployed at scale. Existing inference-time mitigation methods typically manipulate classifier-free guidance (CFG) or perturb prompt embeddings; however, they often struggle to reduce memorization without compromising alignment with the conditioning prompt. We introduce CAPTAIN, a training-free framework that mitigates memorization by directly modifying latent features during denoising. CAPTAIN first applies frequency-based noise initialization to reduce the tendency to replicate memorized patterns early in the denoising process. It then identifies the optimal denoising timesteps for feature injection and localizes memorized regions. Finally, CAPTAIN injects semantically aligned features from non-memorized reference images into localized latent regions, suppressing memorization while preserving prompt fidelity and visual quality. Our experiments show that CAPTAIN achieves substantial reductions in memorization compared to CFG-based baselines while maintaining strong alignment with the intended prompt.

</details>


### [59] [On the Dynamics of Multi-Agent LLM Communities Driven by Value Diversity](https://arxiv.org/abs/2512.10665)
*Muhua Huang,Qinlin Zhao,Xiaoyuan Yi,Xing Xie*

Main category: cs.AI

TL;DR: 本文探讨了多智能体系统中价值观多样性对AI群体行为的影响，发现多样性提升了价值稳定性、涌现行为和创造性，但极端异质性会导致不稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大型语言模型的多智能体系统日益普及，研究价值观多样性如何影响AI群体的集体行为成为一个基础性问题。

Method: 采用基于Schwartz基本人类价值理论的自然主义价值引导方法，构建了多智能体模拟，让不同规模的群体进行开放式互动和宪法制定。

Result: 价值观多样性增强了价值稳定性，促进了涌现行为，使智能体自主产生更具创造性的原则，但这些效应存在递减回报，极端异质性会导致不稳定性。

Conclusion: 价值观多样性是未来AI能力的新维度，连接了AI能力与社会学中的制度涌现研究。

Abstract: As Large Language Models (LLM) based multi-agent systems become increasingly prevalent, the collective behaviors, e.g., collective intelligence, of such artificial communities have drawn growing attention. This work aims to answer a fundamental question: How does diversity of values shape the collective behavior of AI communities? Using naturalistic value elicitation grounded in the prevalent Schwartz's Theory of Basic Human Values, we constructed multi-agent simulations where communities with varying numbers of agents engaged in open-ended interactions and constitution formation. The results show that value diversity enhances value stability, fosters emergent behaviors, and brings more creative principles developed by the agents themselves without external guidance. However, these effects also show diminishing returns: extreme heterogeneity induces instability. This work positions value diversity as a new axis of future AI capability, bridging AI ability and sociological studies of institutional emergence.

</details>


### [60] [Challenges of Evaluating LLM Safety for User Welfare](https://arxiv.org/abs/2512.10687)
*Manon Kempermann,Sai Suresh Macharla Vasu,Mahalakshmi Raveenthiran,Theo Farrell,Ingmar Weber*

Main category: cs.AI

TL;DR: 本文探讨了大语言模型在提供高风险个性化建议（如金融和健康）时，现有普适性安全评估的不足，并提出需结合用户具体情境进行安全评估，通过实验展示了情境信息对评估结果的关键影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的安全评估多关注普遍风险，而忽视了针对高风险个性化建议的情境依赖性危害，用户福利安全评估框架尚不完善。

Method: 通过评估GPT-5、Claude Sonnet 4和Gemini 2.5 Pro在不同脆弱性用户情境下的金融和健康建议，比较了无情境评估与有时境评估的差异，并测试了用户真实情境提示的效果。

Result: 实验发现，无情境评估者比有时境评估者显著高估安全性，高脆弱性用户的安全评分从5/7降至3/7；且仅依靠用户真实情境提示无法有效改善评估效果。

Conclusion: 有效的用户福利安全评估需考虑多样化的用户情境，仅靠情境提示不足以保证评估准确性，应建立区别于普适性风险评估的情境感知评估方法。

Abstract: Safety evaluations of large language models (LLMs) typically focus on universal risks like dangerous capabilities or undesirable propensities. However, millions use LLMs for personal advice on high-stakes topics like finance and health, where harms are context-dependent rather than universal. While frameworks like the OECD's AI classification recognize the need to assess individual risks, user-welfare safety evaluations remain underdeveloped. We argue that developing such evaluations is non-trivial due to fundamental questions about accounting for user context in evaluation design. In this exploratory study, we evaluated advice on finance and health from GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro across user profiles of varying vulnerability. First, we demonstrate that evaluators must have access to rich user context: identical LLM responses were rated significantly safer by context-blind evaluators than by those aware of user circumstances, with safety scores for high-vulnerability users dropping from safe (5/7) to somewhat unsafe (3/7). One might assume this gap could be addressed by creating realistic user prompts containing key contextual information. However, our second study challenges this: we rerun the evaluation on prompts containing context users report they would disclose, finding no significant improvement. Our work establishes that effective user-welfare safety evaluation requires evaluators to assess responses against diverse user profiles, as realistic user context disclosure alone proves insufficient, particularly for vulnerable populations. By demonstrating a methodology for context-aware evaluation, this study provides both a starting point for such assessments and foundational evidence that evaluating individual welfare demands approaches distinct from existing universal-risk frameworks. We publish our code and dataset to aid future developments.

</details>


### [61] [Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning](https://arxiv.org/abs/2512.10691)
*Benjamin Gundersen,Nicolas Deperrois,Samuel Ruiperez-Campillo,Thomas M. Sutter,Julia E. Vogt,Michael Moor,Farhad Nooralahzadeh,Michael Krauthammer*

Main category: cs.AI

TL;DR: 本文研究了在医学视觉语言模型中结合强化学习和显式推理对胸部X光报告生成和视觉定位的影响。


<details>
  <summary>Details</summary>
Motivation: 许多医学视觉语言模型仅依赖于监督微调（SFT），而强化学习（RL）可以结合特定任务的反馈，从而提高模型性能。

Method: 首先，基于Qwen3-VL进行大规模SFT构建RadVLM，然后引入冷启动SFT阶段赋予模型基本推理能力。接着，使用带临床依据的任务特定奖励进行GRPO优化，并在特定领域和通用领域Qwen3-VL变体上运行RL实验。

Result: 尽管强大的SFT对基础性能至关重要，但RL在两项任务上均提供了额外增益，而显式推理未带来进一步改进。

Conclusion: 在统一的评估流程下，RL优化的RadVLM模型在报告生成和视觉定位方面均优于基线模型，并达到了最先进的性能，突出了临床对齐的RL作为SFT的有力补充。

Abstract: Recent advances in vision-language models (VLMs) have improved Chest X-ray (CXR) interpretation in multiple aspects. However, many medical VLMs rely solely on supervised fine-tuning (SFT), which optimizes next-token prediction without evaluating answer quality. In contrast, reinforcement learning (RL) can incorporate task-specific feedback, and its combination with explicit intermediate reasoning ("thinking") has demonstrated substantial gains on verifiable math and coding tasks. To investigate the effects of RL and thinking in a CXR VLM, we perform large-scale SFT on CXR data to build an updated RadVLM based on Qwen3-VL, followed by a cold-start SFT stage that equips the model with basic thinking ability. We then apply Group Relative Policy Optimization (GRPO) with clinically grounded, task-specific rewards for report generation and visual grounding, and run matched RL experiments on both domain-specific and general-domain Qwen3-VL variants, with and without thinking. Across these settings, we find that while strong SFT remains crucial for high base performance, RL provides additional gains on both tasks, whereas explicit thinking does not appear to further improve results. Under a unified evaluation pipeline, the RL-optimized RadVLM models outperform their baseline counterparts and reach state-of-the-art performance on both report generation and grounding, highlighting clinically aligned RL as a powerful complement to SFT for medical VLMs.

</details>


### [62] [COMPARE: Clinical Optimization with Modular Planning and Assessment via RAG-Enhanced AI-OCT: Superior Decision Support for Percutaneous Coronary Intervention Compared to ChatGPT-5 and Junior Operators](https://arxiv.org/abs/2512.10702)
*Wei Fang,Chiyao Wang,Wenshuai Ma,Hui Liu,Jianqiang Hu,Xiaona Niu,Yi Chu,Mingming Zhang,Jingxiao Yang,Dongwei Zhang,Zelin Li,Pengyun Liu,Jiawei Zheng,Pengke Zhang,Chaoshi Qin,Wangang Guo,Bin Wang,Yugang Xue,Wei Zhang,Zikuan Wang,Rui Zhu,Yihui Cao,Quanmao Lu,Rui Meng,Yan Li*

Main category: cs.AI

TL;DR: CA-GPT在OCT引导的PCI规划和评估中优于通用ChatGPT-5和初级医师。


<details>
  <summary>Details</summary>
Motivation: 血管内成像（如OCT）在PCI中结果依赖于操作员的解读能力，通用AI缺乏特定领域的可靠性，因此需要更专业的AI模型。

Method: 在96例接受OCT引导PCI的患者中，比较了CA-GPT、ChatGPT-5和初级医师的手术决策与专家记录的吻合度，并使用十项预先指定指标进行评估。

Result: 在PCI规划阶段，CA-GPT的中位数吻合度评分显著高于ChatGPT-5和初级医师。在PCI后评估阶段，CA-GPT的整体吻合度也显著高于其他两组。

Conclusion: CA-GPT为基础的AI-OCT系统在PCI规划和评估中表现出更优的决策一致性，为血管内成像解读提供了一种标准化和可靠的方法，显著增强了操作员的专业知识。

Abstract: Background: While intravascular imaging, particularly optical coherence tomography (OCT), improves percutaneous coronary intervention (PCI) outcomes, its interpretation is operator-dependent. General-purpose artificial intelligence (AI) shows promise but lacks domain-specific reliability. We evaluated the performance of CA-GPT, a novel large model deployed on an AI-OCT system, against that of the general-purpose ChatGPT-5 and junior physicians for OCT-guided PCI planning and assessment.
  Methods: In this single-center analysis of 96 patients who underwent OCT-guided PCI, the procedural decisions generated by the CA-GPT, ChatGPT-5, and junior physicians were compared with an expert-derived procedural record. Agreement was assessed using ten pre-specified metrics across pre-PCI and post-PCI phases.
  Results: For pre-PCI planning, CA-GPT demonstrated significantly higher median agreement scores (5[IQR 3.75-5]) compared to both ChatGPT-5 (3[2-4], P<0.001) and junior physicians (4[3-4], P<0.001). CA-GPT significantly outperformed ChatGPT-5 across all individual pre-PCI metrics and showed superior performance to junior physicians in stent diameter (90.3% vs. 72.2%, P<0.05) and length selection (80.6% vs. 52.8%, P<0.01). In post-PCI assessment, CA-GPT maintained excellent overall agreement (5[4.75-5]), significantly higher than both ChatGPT-5 (4[4-5], P<0.001) and junior physicians (5[4-5], P<0.05). Subgroup analysis confirmed CA-GPT's robust performance advantage in complex scenarios.
  Conclusion: The CA-GPT-based AI-OCT system achieved superior decision-making agreement versus a general-purpose large language model and junior physicians across both PCI planning and assessment phases. This approach provides a standardized and reliable method for intravascular imaging interpretation, demonstrating significant potential to augment operator expertise and optimize OCT-guided PCI.

</details>


### [63] [HAROOD: A Benchmark for Out-of-distribution Generalization in Sensor-based Human Activity Recognition](https://arxiv.org/abs/2512.10807)
*Wang Lu,Yao Zhu,Jindong Wang*

Main category: cs.AI

TL;DR: 提出HAROOD，一个针对传感器人类活动识别（HAR）的综合性OOD算法基准测试，定义了4种OOD场景，并基于6个数据集和16种方法进行了广泛实验。


<details>
  <summary>Details</summary>
Motivation: 由于个体、设备、环境和时间的变化，HAR面临显著的数据分布偏移问题。现有的OOD算法只在特定场景下应用，缺乏系统性比较。

Method: 定义了4种OOD场景：跨人、跨位置、跨数据集和跨时间，并搭建了一个包含6个数据集、16种方法和两个模型选择协议的测试平台。

Result: 实验表明，没有一种方法在所有场景下都能持续优于其他方法，显示出在该领域有巨大的提升空间。

Conclusion: 提出的HAROOD基准测试平台为未来OOD在HAR中的研究提供了重要工具和参考，代码库高度模块化，便于扩展。

Abstract: Sensor-based human activity recognition (HAR) mines activity patterns from the time-series sensory data. In realistic scenarios, variations across individuals, devices, environments, and time introduce significant distributional shifts for the same activities. Recent efforts attempt to solve this challenge by applying or adapting existing out-of-distribution (OOD) algorithms, but only in certain distribution shift scenarios (e.g., cross-device or cross-position), lacking comprehensive insights on the effectiveness of these algorithms. For instance, is OOD necessary to HAR? Which OOD algorithm performs the best? In this paper, we fill this gap by proposing HAROOD, a comprehensive benchmark for HAR in OOD settings. We define 4 OOD scenarios: cross-person, cross-position, cross-dataset, and cross-time, and build a testbed covering 6 datasets, 16 comparative methods (implemented with CNN-based and Transformer-based architectures), and two model selection protocols. Then, we conduct extensive experiments and present several findings for future research, e.g., no single method consistently outperforms others, highlighting substantial opportunity for advancement. Our codebase is highly modular and easy to extend for new datasets, algorithms, comparisons, and analysis, with the hope to facilitate the research in OOD-based HAR. Our implementation is released and can be found at https://github.com/AIFrontierLab/HAROOD.

</details>


### [64] [Agile Deliberation: Concept Deliberation for Subjective Visual Classification](https://arxiv.org/abs/2512.10821)
*Leijie Wang,Otilia Stretcu,Wei Qiao,Thomas Denby,Krishnamurthy Viswanathan,Enming Luo,Chun-Ta Lu,Tushar Dogra,Ranjay Krishna,Ariel Fuxman*

Main category: cs.AI

TL;DR: 本文介绍了一种名为“Agile Deliberation”的人机协同框架，通过概念界定和概念迭代两个阶段，帮助用户逐步明确主观概念并训练图像分类器。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉分类器在人机协同中假设用户一开始就有清晰和稳定的概念理解，但现实中用户通常从一个模糊的想法开始，需要反复修正。

Method: 通过结构化访谈内容审核专家，将实际审核员常用的策略操作化为“Agile Deliberation”框架，包括概念界定（分解概念为子概念）和概念迭代（展示边界案例供用户反馈）。

Result: 通过18个用户会话评估，每个会话1.5小时，Agile Deliberation比自动化分解基线高7.5%的F1分数，比手动审议高3%，用户报告概念理解更清晰且认知负担更低。

Conclusion: Agile Deliberation框架有效支持用户明确和迭代概念，提高了分类器的性能，并减少了用户的认知负担。

Abstract: From content moderation to content curation, applications requiring vision classifiers for visual concepts are rapidly expanding. Existing human-in-the-loop approaches typically assume users begin with a clear, stable concept understanding to be able to provide high-quality supervision. In reality, users often start with a vague idea and must iteratively refine it through "concept deliberation", a practice we uncovered through structured interviews with content moderation experts. We operationalize the common strategies in deliberation used by real content moderators into a human-in-the-loop framework called "Agile Deliberation" that explicitly supports evolving and subjective concepts. The system supports users in defining the concept for themselves by exposing them to borderline cases. The system does this with two deliberation stages: (1) concept scoping, which decomposes the initial concept into a structured hierarchy of sub-concepts, and (2) concept iteration, which surfaces semantically borderline examples for user reflection and feedback to iteratively align an image classifier with the user's evolving intent. Since concept deliberation is inherently subjective and interactive, we painstakingly evaluate the framework through 18 user sessions, each 1.5h long, rather than standard benchmarking datasets. We find that Agile Deliberation achieves 7.5% higher F1 scores than automated decomposition baselines and more than 3% higher than manual deliberation, while participants reported clearer conceptual understanding and lower cognitive effort.

</details>


### [65] [V-OCBF: Learning Safety Filters from Offline Data via Value-Guided Offline Control Barrier Functions](https://arxiv.org/abs/2512.10822)
*Mumuksh Tayal,Manan Tayal,Aditya Singh,Shishir Kolathaya,Ravi Prakash*

Main category: cs.AI

TL;DR: 本文提出了一种无需在线交互和专家设计屏障函数的安全离线强化学习框架V-OCBF，通过离线数据学习神经控制屏障函数，确保自主系统的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全离线强化学习方法仅执行软性期望成本约束，无法保证前向不变性；而CBFs虽能提供严格安全保证，但依赖专家设计或系统动态知识。因此，亟需一种无需系统模型或人工设计屏障的安全控制方法。

Method: 引入V-OCBF框架，通过递归有限差分屏障更新从离线示范中学习神经CBF，结合expectile-based目标避免分布外动作，并用二次规划(QP)合成实时安全控制。

Result: 在多个案例中，V-OCBF相比基线方法显著减少安全违规，同时保持任务性能，无需在线交互或手工设计屏障。

Conclusion: V-OCBF是一种可扩展的、适用于安全关键控制的离线框架，有效解决了现有方法在安全保证和模型依赖上的局限。

Abstract: Ensuring safety in autonomous systems requires controllers that satisfy hard, state-wise constraints without relying on online interaction. While existing Safe Offline RL methods typically enforce soft expected-cost constraints, they do not guarantee forward invariance. Conversely, Control Barrier Functions (CBFs) provide rigorous safety guarantees but usually depend on expert-designed barrier functions or full knowledge of the system dynamics. We introduce Value-Guided Offline Control Barrier Functions (V-OCBF), a framework that learns a neural CBF entirely from offline demonstrations. Unlike prior approaches, V-OCBF does not assume access to the dynamics model; instead, it derives a recursive finite-difference barrier update, enabling model-free learning of a barrier that propagates safety information over time. Moreover, V-OCBF incorporates an expectile-based objective that avoids querying the barrier on out-of-distribution actions and restricts updates to the dataset-supported action set. The learned barrier is then used with a Quadratic Program (QP) formulation to synthesize real-time safe control. Across multiple case studies, V-OCBF yields substantially fewer safety violations than baseline methods while maintaining strong task performance, highlighting its scalability for offline synthesis of safety-critical controllers without online interaction or hand-engineered barriers.

</details>
