{"id": "2504.11460", "pdf": "https://arxiv.org/pdf/2504.11460", "abs": "https://arxiv.org/abs/2504.11460", "authors": ["Tobias Hallmen", "Robin-Nico Kampa", "Fabian Deuser", "Norbert Oswald", "Elisabeth André"], "title": "Semantic Matters: Multimodal Features for Affective Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this study, we present our methodology for two tasks: the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry\nIntensity (EMI) Estimation Challenge, both conducted as part of the 8th\nWorkshop and Competition on Affective & Behavior Analysis in-the-wild. Building\non previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast\ndataset to extract various audio features, capturing both linguistic and\nparalinguistic information. Our approach incorporates a\nvalence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like\nencoder, and a vision transformer (ViT) with predictions subsequently processed\nthrough a long short-term memory (LSTM) architecture for temporal modeling. In\nthis iteration, we integrate the textual and visual modality into our analysis,\nrecognizing that semantic content provides valuable contextual cues and\nunderscoring that the meaning of speech often conveys more critical insights\nthan its acoustic counterpart alone. Fusing in the vision modality helps in\nsome cases to interpret the textual modality more precisely. This combined\napproach yields significant performance improvements over baseline methods."}
{"id": "2504.11467", "pdf": "https://arxiv.org/pdf/2504.11467", "abs": "https://arxiv.org/abs/2504.11467", "authors": ["Qianxue Zhang", "Eiman Kanjo"], "title": "MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 10 figures", "summary": "The advancement of technology has revolutionised the agricultural industry,\ntransitioning it from labour-intensive farming practices to automated,\nAI-powered management systems. In recent years, more intelligent livestock\nmonitoring solutions have been proposed to enhance farming efficiency and\nproductivity. This work presents a novel approach to animal activity\nrecognition and movement tracking, leveraging tiny machine learning (TinyML)\ntechniques, wireless communication framework, and microcontroller platforms to\ndevelop an efficient, cost-effective livestock sensing system. It collects and\nfuses accelerometer data and vision inputs to build a multi-modal network for\nthree tasks: image classification, object detection, and behaviour recognition.\nThe system is deployed and evaluated on commercial microcontrollers for\nreal-time inference using embedded applications, demonstrating up to\n270$\\times$ model size reduction, less than 80ms response latency, and on-par\nperformance comparable to existing methods. The incorporation of the TinyML\ntechnique allows for seamless data transmission between devices, benefiting use\ncases in remote locations with poor Internet connectivity. This work delivers a\nrobust, scalable IoT-edge livestock monitoring solution adaptable to diverse\nfarming needs, offering flexibility for future extensions."}
{"id": "2504.11470", "pdf": "https://arxiv.org/pdf/2504.11470", "abs": "https://arxiv.org/abs/2504.11470", "authors": ["Huaxiang Zhang", "Hao Zhang", "Aoran Mei", "Zhongxue Gan", "Guo-Niu Zhu"], "title": "SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detection Transformer-based methods have achieved significant advancements in\ngeneral object detection. However, challenges remain in effectively detecting\nsmall objects. One key difficulty is that existing encoders struggle to\nefficiently fuse low-level features. Additionally, the query selection\nstrategies are not effectively tailored for small objects. To address these\nchallenges, this paper proposes an efficient model, Small Object Detection\nTransformer (SO-DETR). The model comprises three key components: a dual-domain\nhybrid encoder, an enhanced query selection mechanism, and a knowledge\ndistillation strategy. The dual-domain hybrid encoder integrates spatial and\nfrequency domains to fuse multi-scale features effectively. This approach\nenhances the representation of high-resolution features while maintaining\nrelatively low computational overhead. The enhanced query selection mechanism\noptimizes query initialization by dynamically selecting high-scoring anchor\nboxes using expanded IoU, thereby improving the allocation of query resources.\nFurthermore, by incorporating a lightweight backbone network and implementing a\nknowledge distillation strategy, we develop an efficient detector for small\nobjects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets\ndemonstrate that SO-DETR outperforms existing methods with similar\ncomputational demands. The project page is available at\nhttps://github.com/ValiantDiligent/SO_DETR."}
{"id": "2504.11472", "pdf": "https://arxiv.org/pdf/2504.11472", "abs": "https://arxiv.org/abs/2504.11472", "authors": ["Kebin Contreras", "Brayan Monroy", "Jorge Bacca"], "title": "High Dynamic Range Modulo Imaging for Robust Object Detection in Autonomous Driving", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Object detection precision is crucial for ensuring the safety and efficacy of\nautonomous driving systems. The quality of acquired images directly influences\nthe ability of autonomous driving systems to correctly recognize and respond to\nother vehicles, pedestrians, and obstacles in real-time. However, real\nenvironments present extreme variations in lighting, causing saturation\nproblems and resulting in the loss of crucial details for detection.\nTraditionally, High Dynamic Range (HDR) images have been preferred for their\nability to capture a broad spectrum of light intensities, but the need for\nmultiple captures to construct HDR images is inefficient for real-time\napplications in autonomous vehicles. To address these issues, this work\nintroduces the use of modulo sensors for robust object detection. The modulo\nsensor allows pixels to `reset/wrap' upon reaching saturation level by\nacquiring an irradiance encoding image which can then be recovered using\nunwrapping algorithms. The applied reconstruction techniques enable HDR\nrecovery of color intensity and image details, ensuring better visual quality\neven under extreme lighting conditions at the cost of extra time. Experiments\nwith the YOLOv10 model demonstrate that images processed using modulo images\nachieve performance comparable to HDR images and significantly surpass\nsaturated images in terms of object detection accuracy. Moreover, the proposed\nmodulo imaging step combined with HDR image reconstruction is shorter than the\ntime required for conventional HDR image acquisition."}
{"id": "2504.11468", "pdf": "https://arxiv.org/pdf/2504.11468", "abs": "https://arxiv.org/abs/2504.11468", "authors": ["Hardy Chen", "Haoqin Tu", "Fali Wang", "Hui Liu", "Xianfeng Tang", "Xinya Du", "Yuyin Zhou", "Cihang Xie"], "title": "SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This work revisits the dominant supervised fine-tuning (SFT) then\nreinforcement learning (RL) paradigm for training Large Vision-Language Models\n(LVLMs), and reveals a key finding: SFT can significantly undermine subsequent\nRL by inducing ``pseudo reasoning paths'' imitated from expert models. While\nthese paths may resemble the native reasoning paths of RL models, they often\ninvolve prolonged, hesitant, less informative steps, and incorrect reasoning.\nTo systematically study this effect, we introduce VLAA-Thinking, a new\nmultimodal dataset designed to support reasoning in LVLMs. Constructed via a\nsix-step pipeline involving captioning, reasoning distillation, answer rewrite\nand verification, VLAA-Thinking comprises high-quality, step-by-step visual\nreasoning traces for SFT, along with a more challenging RL split from the same\ndata source. Using this dataset, we conduct extensive experiments comparing\nSFT, RL and their combinations. Results show that while SFT helps models learn\nreasoning formats, it often locks aligned models into imitative, rigid\nreasoning modes that impede further learning. In contrast, building on the\nGroup Relative Policy Optimization (GRPO) with a novel mixed reward module\nintegrating both perception and cognition signals, our RL approach fosters more\ngenuine, adaptive reasoning behavior. Notably, our model VLAA-Thinker, based on\nQwen2.5VL 3B, achieves top-1 performance on Open LMM Reasoning Leaderboard\n(https://huggingface.co/spaces/opencompass/Open_LMM_Reasoning_Leaderboard)\namong 4B scale LVLMs, surpassing the previous state-of-the-art by 1.8%. We hope\nour findings provide valuable insights in developing reasoning-capable LVLMs\nand can inform future research in this area."}
{"id": "2504.11473", "pdf": "https://arxiv.org/pdf/2504.11473", "abs": "https://arxiv.org/abs/2504.11473", "authors": ["Warren Zhu", "Aida Ramezani", "Yang Xu"], "title": "Visual moral inference and communication", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Humans can make moral inferences from multiple sources of input. In contrast,\nautomated moral inference in artificial intelligence typically relies on\nlanguage models with textual input. However, morality is conveyed through\nmodalities beyond language. We present a computational framework that supports\nmoral inference from natural images, demonstrated in two related tasks: 1)\ninferring human moral judgment toward visual images and 2) analyzing patterns\nin moral content communicated via images from public news. We find that models\nbased on text alone cannot capture the fine-grained human moral judgment toward\nvisual stimuli, but language-vision fusion models offer better precision in\nvisual moral inference. Furthermore, applications of our framework to news data\nreveal implicit biases in news categories and geopolitical discussions. Our\nwork creates avenues for automating visual moral inference and discovering\npatterns of visual moral communication in public media."}
{"id": "2504.11536", "pdf": "https://arxiv.org/pdf/2504.11536", "abs": "https://arxiv.org/abs/2504.11536", "authors": ["Jiazhan Feng", "Shijue Huang", "Xingwei Qu", "Ge Zhang", "Yujia Qin", "Baoquan Zhong", "Chengquan Jiang", "Jinxin Chi", "Wanjun Zhong"], "title": "ReTool: Reinforcement Learning for Strategic Tool Use in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While reasoning models (e.g., DeepSeek R1) trained with reinforcement\nlearning (RL), excel in textual reasoning, they struggle in scenarios requiring\nstructured problem-solving, such as geometric reasoning, concise computation,\nor complex equation solving-areas where computational tools like code\ninterpreters (CI) demonstrate distinct advantages. To bridge this gap, we\npropose ReTool, which enhances long-form reasoning with tool-integrated\nlearning, including two key features: (1) dynamic interleaving of real-time\ncode execution within natural language reasoning processes, and (2) an\nautomated RL paradigm that allows policy rollouts with multi-turn real-time\ncode execution and teaches the model in learning when and how to invoke tools\nbased on outcome feedback. ReTool employs a systematic training framework,\nbeginning with synthetic cold-start data generation to produce code-augmented\nlong-form reasoning traces for fine-tuning base models. Subsequent RL training\nleverages task outcomes as rewards to iteratively refine the model's tool use\nstrategy, enabling autonomous discovery of optimal tool invocation patterns\nwithout human priors. Experiments on the challenging MATH Olympiad benchmark\nAIME demonstrate ReTool's superiority: Our 32B model achieves 67% accuracy with\n400 training steps, outperforming text-based RL baseline (40% accuracy, 1080\nsteps) in efficiency and performance. Remarkably, ReTool-32B attains 72.5%\naccuracy in extended settings, surpassing OpenAI's o1-preview by 27.9%. Further\nanalysis reveals emergent behaviors such as code self-correction, signaling an\n''aha moment'' in which the model autonomously masters adaptive tool use. These\nfindings highlight the promise of outcome-driven tool integration for advancing\ncomplex mathematical reasoning and offer new insights into hybrid\nneuro-symbolic systems."}
{"id": "2504.11477", "pdf": "https://arxiv.org/pdf/2504.11477", "abs": "https://arxiv.org/abs/2504.11477", "authors": ["Yunkai Zhang", "Shiyin Wei", "Yong Huang", "Yawu Su", "Shanshan Lu", "Hui Li"], "title": "SDIGLM: Leveraging Large Language Models and Multi-Modal Chain of Thought for Structural Damage Identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing computer vision(CV)-based structural damage identification models\ndemonstrate notable accuracy in categorizing and localizing damage. However,\nthese models present several critical limitations that hinder their practical\napplication in civil engineering(CE). Primarily, their ability to recognize\ndamage types remains constrained, preventing comprehensive analysis of the\nhighly varied and complex conditions encountered in real-world CE structures.\nSecond, these models lack linguistic capabilities, rendering them unable to\narticulate structural damage characteristics through natural language\ndescriptions. With the continuous advancement of artificial intelligence(AI),\nlarge multi-modal models(LMMs) have emerged as a transformative solution,\nenabling the unified encoding and alignment of textual and visual data. These\nmodels can autonomously generate detailed descriptive narratives of structural\ndamage while demonstrating robust generalization across diverse scenarios and\ntasks. This study introduces SDIGLM, an innovative LMM for structural damage\nidentification, developed based on the open-source VisualGLM-6B architecture.\nTo address the challenge of adapting LMMs to the intricate and varied operating\nconditions in CE, this work integrates a U-Net-based semantic segmentation\nmodule to generate defect segmentation maps as visual Chain of Thought(CoT).\nAdditionally, a multi-round dialogue fine-tuning dataset is constructed to\nenhance logical reasoning, complemented by a language CoT formed through prompt\nengineering. By leveraging this multi-modal CoT, SDIGLM surpasses\ngeneral-purpose LMMs in structural damage identification, achieving an accuracy\nof 95.24% across various infrastructure types. Moreover, the model effectively\ndescribes damage characteristics such as hole size, crack direction, and\ncorrosion severity."}
{"id": "2504.11582", "pdf": "https://arxiv.org/pdf/2504.11582", "abs": "https://arxiv.org/abs/2504.11582", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "title": "AskQE: Question Answering as Automatic Evaluation for Machine Translation", "categories": ["cs.CL"], "comment": "38 pages, 7 figures", "summary": "How can a monolingual English speaker determine whether an automatic\ntranslation in French is good enough to be shared? Existing MT error detection\nand quality estimation (QE) techniques do not address this practical scenario.\nWe introduce AskQE, a question generation and answering framework designed to\ndetect critical MT errors and provide actionable feedback, helping users decide\nwhether to accept or reject MT outputs even without the knowledge of the target\nlanguage. Using ContraTICO, a dataset of contrastive synthetic MT errors in the\nCOVID-19 domain, we explore design choices for AskQE and develop an optimized\nversion relying on LLaMA-3 70B and entailed facts to guide question generation.\nWe evaluate the resulting system on the BioMQM dataset of naturally occurring\nMT errors, where AskQE has higher Kendall's Tau correlation and decision\naccuracy with human ratings compared to other QE metrics."}
{"id": "2504.11478", "pdf": "https://arxiv.org/pdf/2504.11478", "abs": "https://arxiv.org/abs/2504.11478", "authors": ["Hao Kang", "Stathi Fotiadis", "Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Min Jin Chong", "Xin Lu"], "title": "Flux Already Knows - Activating Subject-Driven Image Generation without Training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a simple yet effective zero-shot framework for subject-driven\nimage generation using a vanilla Flux model. By framing the task as grid-based\nimage completion and simply replicating the subject image(s) in a mosaic\nlayout, we activate strong identity-preserving capabilities without any\nadditional data, training, or inference-time fine-tuning. This \"free lunch\"\napproach is further strengthened by a novel cascade attention design and meta\nprompting technique, boosting fidelity and versatility. Experimental results\nshow that our method outperforms baselines across multiple key metrics in\nbenchmarks and human preference studies, with trade-offs in certain aspects.\nAdditionally, it supports diverse edits, including logo insertion, virtual\ntry-on, and subject replacement or insertion. These results demonstrate that a\npre-trained foundational text-to-image model can enable high-quality,\nresource-efficient subject-driven generation, opening new possibilities for\nlightweight customization in downstream applications."}
{"id": "2504.11626", "pdf": "https://arxiv.org/pdf/2504.11626", "abs": "https://arxiv.org/abs/2504.11626", "authors": ["Ozan İrsoy", "Pengxiang Cheng", "Jennifer L. Chen", "Daniel Preoţiuc-Pietro", "Shiyue Zhang", "Duccio Pappadopulo"], "title": "Improving Instruct Models for Free: A Study on Partial Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "Author ordering chosen at random", "summary": "Instruct models, obtained from various instruction tuning or post-training\nsteps, are commonly deemed superior and more usable than their base\ncounterpart. While the model gains instruction following ability, instruction\ntuning may lead to forgetting the knowledge from pre-training or it may\nencourage the model being overly conversational or verbose. This, in turn, can\nlead to degradation of in-context few-shot learning performance. In this work,\nwe study the performance trajectory between base and instruct models by scaling\ndown the strength of instruction-tuning via the partial adaption method. We\nshow that, across several model families and model sizes, reducing the strength\nof instruction-tuning results in material improvement on a few-shot in-context\nlearning benchmark covering a variety of classic natural language tasks. This\ncomes at the cost of losing some degree of instruction following ability as\nmeasured by AlpacaEval. Our study shines light on the potential trade-off\nbetween in-context learning and instruction following abilities that is worth\nconsidering in practice."}
{"id": "2504.11482", "pdf": "https://arxiv.org/pdf/2504.11482", "abs": "https://arxiv.org/abs/2504.11482", "authors": ["Vidya Sudevan", "Fakhreddine Zayer", "Rizwana Kausar", "Sajid Javed", "Hamad Karki", "Giulia De Masi", "Jorge Dias"], "title": "snnTrans-DHZ: A Lightweight Spiking Neural Network Architecture for Underwater Image Dehazing", "categories": ["cs.CV", "cs.AI", "cs.PF", "cs.RO", "eess.IV"], "comment": null, "summary": "Underwater image dehazing is critical for vision-based marine operations\nbecause light scattering and absorption can severely reduce visibility. This\npaper introduces snnTrans-DHZ, a lightweight Spiking Neural Network (SNN)\nspecifically designed for underwater dehazing. By leveraging the temporal\ndynamics of SNNs, snnTrans-DHZ efficiently processes time-dependent raw image\nsequences while maintaining low power consumption. Static underwater images are\nfirst converted into time-dependent sequences by repeatedly inputting the same\nimage over user-defined timesteps. These RGB sequences are then transformed\ninto LAB color space representations and processed concurrently. The\narchitecture features three key modules: (i) a K estimator that extracts\nfeatures from multiple color space representations; (ii) a Background Light\nEstimator that jointly infers the background light component from the RGB-LAB\nimages; and (iii) a soft image reconstruction module that produces haze-free,\nvisibility-enhanced outputs. The snnTrans-DHZ model is directly trained using a\nsurrogate gradient-based backpropagation through time (BPTT) strategy alongside\na novel combined loss function. Evaluated on the UIEB benchmark, snnTrans-DHZ\nachieves a PSNR of 21.68 dB and an SSIM of 0.8795, and on the EUVP dataset, it\nyields a PSNR of 23.46 dB and an SSIM of 0.8439. With only 0.5670 million\nnetwork parameters, and requiring just 7.42 GSOPs and 0.0151 J of energy, the\nalgorithm significantly outperforms existing state-of-the-art methods in terms\nof efficiency. These features make snnTrans-DHZ highly suitable for deployment\nin underwater robotics, marine exploration, and environmental monitoring."}
{"id": "2504.11673", "pdf": "https://arxiv.org/pdf/2504.11673", "abs": "https://arxiv.org/abs/2504.11673", "authors": ["Minwoo Kang", "Suhong Moon", "Seung Hyeong Lee", "Ayush Raj", "Joseph Suh", "David M. Chan"], "title": "Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly capable of simulating human\nbehavior, offering cost-effective ways to estimate user responses during the\nearly phases of survey design. While previous studies have examined whether\nmodels can reflect individual opinions or attitudes, we argue that a\n\\emph{higher-order} binding of virtual personas requires successfully\napproximating not only the opinions of a user as an identified member of a\ngroup, but also the nuanced ways in which that user perceives and evaluates\nthose outside the group. In particular, faithfully simulating how humans\nperceive different social groups is critical for applying LLMs to various\npolitical science studies, including timely topics on polarization dynamics,\ninter-group conflict, and democratic backsliding. To this end, we propose a\nnovel methodology for constructing virtual personas with synthetic user\n``backstories\" generated as extended, multi-turn interview transcripts. Our\ngenerated backstories are longer, rich in detail, and consistent in\nauthentically describing a singular individual, compared to previous methods.\nWe show that virtual personas conditioned on our backstories closely replicate\nhuman response distributions (up to an 87\\% improvement as measured by\nWasserstein Distance) and produce effect sizes that closely match those\nobserved in the original studies. Altogether, our work extends the\napplicability of LLMs beyond estimating individual self-opinions, enabling\ntheir use in a broader range of human studies."}
{"id": "2504.11489", "pdf": "https://arxiv.org/pdf/2504.11489", "abs": "https://arxiv.org/abs/2504.11489", "authors": ["Matthew Bozoukov"], "title": "Uncovering Branch specialization in InceptionV1 using k sparse autoencoders", "categories": ["cs.CV"], "comment": "Accepted to CVPR MIV workshop. 9 pages with an appendix", "summary": "Sparse Autoencoders (SAEs) have shown to find interpretable features in\nneural networks from polysemantic neurons caused by superposition. Previous\nwork has shown SAEs are an effective tool to extract interpretable features\nfrom the early layers of InceptionV1. Since then, there have been many\nimprovements to SAEs but branch specialization is still an enigma in the later\nlayers of InceptionV1. We show various examples of branch specialization\noccuring in each layer of the mixed4a-4e branch, in the 5x5 branch and in one\n1x1 branch. We also provide evidence to claim that branch specialization seems\nto be consistent across layers, similar features across the model will be\nlocalized in the same convolution size branches in their respective layer."}
{"id": "2504.11770", "pdf": "https://arxiv.org/pdf/2504.11770", "abs": "https://arxiv.org/abs/2504.11770", "authors": ["Takashi Morita", "Timothy J. O'Donnell"], "title": "Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters", "categories": ["cs.CL"], "comment": null, "summary": "Cross-linguistically, native words and loanwords follow different\nphonological rules. In English, for example, words of Germanic and Latinate\norigin exhibit different stress patterns, and a certain syntactic structure is\nexclusive to Germanic verbs. When seeing them as a cognitive model, however,\nsuch etymology-based generalizations face challenges in terms of learnability,\nsince the historical origins of words are presumably inaccessible information\nfor general language learners. In this study, we present computational evidence\nindicating that the Germanic-Latinate distinction in the English lexicon is\nlearnable from the phonotactic information of individual words. Specifically,\nwe performed an unsupervised clustering on corpus-extracted words, and the\nresulting word clusters largely aligned with the etymological distinction. The\nmodel-discovered clusters also recovered various linguistic generalizations\ndocumented in the previous literature regarding the corresponding etymological\nclasses. Moreover, our findings also uncovered previously unrecognized features\nof the quasi-etymological clusters, offering novel hypotheses for future\nexperimental studies."}
{"id": "2504.11500", "pdf": "https://arxiv.org/pdf/2504.11500", "abs": "https://arxiv.org/abs/2504.11500", "authors": ["Kaicong Huang", "Talha Azfar", "Jack Reilly", "Ruimin Ke"], "title": "TransitReID: Transit OD Data Collection with Occlusion-Resistant Dynamic Passenger Re-Identification", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Transit Origin-Destination (OD) data are essential for transit planning,\nparticularly in route optimization and demand-responsive paratransit systems.\nTraditional methods, such as manual surveys, are costly and inefficient, while\nBluetooth and WiFi-based approaches require passengers to carry specific\ndevices, limiting data coverage. On the other hand, most transit vehicles are\nequipped with onboard cameras for surveillance, offering an opportunity to\nrepurpose them for edge-based OD data collection through visual person\nre-identification (ReID). However, such approaches face significant challenges,\nincluding severe occlusion and viewpoint variations in transit environments,\nwhich greatly reduce matching accuracy and hinder their adoption. Moreover,\ndesigning effective algorithms that can operate efficiently on edge devices\nremains an open challenge. To address these challenges, we propose TransitReID,\na novel framework for individual-level transit OD data collection. TransitReID\nconsists of two key components: (1) An occlusion-robust ReID algorithm\nfeaturing a variational autoencoder guided region-attention mechanism that\nadaptively focuses on visible body regions through reconstruction\nloss-optimized weight allocation; and (2) a Hierarchical Storage and Dynamic\nMatching (HSDM) mechanism specifically designed for efficient and robust\ntransit OD matching which balances storage, speed, and accuracy. Additionally,\na multi-threaded design supports near real-time operation on edge devices,\nwhich also ensuring privacy protection. We also introduce a ReID dataset\ntailored for complex bus environments to address the lack of relevant training\ndata. Experimental results demonstrate that TransitReID achieves\nstate-of-the-art performance in ReID tasks, with an accuracy of approximately\n90\\% in bus route simulations."}
{"id": "2504.11788", "pdf": "https://arxiv.org/pdf/2504.11788", "abs": "https://arxiv.org/abs/2504.11788", "authors": ["Zhisong Zhang", "Tianqing Fang", "Kaixin Ma", "Wenhao Yu", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Enhancing Web Agents with Explicit Rollback Mechanisms", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With recent advancements in large language models, web agents have been\ngreatly improved. However, dealing with complex and dynamic web environments\nrequires more advanced planning and search abilities. Previous studies usually\nadopt a greedy one-way search strategy, which may struggle to recover from\nerroneous states. In this work, we enhance web agents with an explicit rollback\nmechanism, enabling the agent to revert back to a previous state in its\nnavigation trajectory. This mechanism gives the model the flexibility to\ndirectly control the search process, leading to an effective and efficient web\nnavigation method. We conduct experiments on two live web navigation benchmarks\nwith zero-shot and fine-tuning settings. The results demonstrate the\neffectiveness of our proposed approach."}
{"id": "2504.11515", "pdf": "https://arxiv.org/pdf/2504.11515", "abs": "https://arxiv.org/abs/2504.11515", "authors": ["Kangsheng Wang", "Chengwei Ye", "Huanzhen Zhang", "Linuo Xu", "Shuyan Liu"], "title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance."}
{"id": "2504.11793", "pdf": "https://arxiv.org/pdf/2504.11793", "abs": "https://arxiv.org/abs/2504.11793", "authors": ["Yue Li", "Lihong Zhang"], "title": "Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) faces major challenges regarding communication\noverhead and model privacy when training large language models (LLMs),\nespecially in healthcare applications. To address these, we introduce Selective\nAttention Federated Learning (SAFL), a novel approach that dynamically\nfine-tunes only those transformer layers identified as attention-critical. By\nemploying attention patterns to determine layer importance, SAFL significantly\nreduces communication bandwidth and enhances differential privacy resilience.\nEvaluations on clinical NLP benchmarks (i2b2 Clinical Concept Extraction and\nMIMIC-III discharge summaries) demonstrate that SAFL achieves competitive\nperformance with centralized models while substantially improving communication\nefficiency and privacy preservation."}
{"id": "2504.11517", "pdf": "https://arxiv.org/pdf/2504.11517", "abs": "https://arxiv.org/abs/2504.11517", "authors": ["Riad Ibadulla", "Thomas M. Chen", "Constantino Carlos Reyes-Aldasoro"], "title": "ConvShareViT: Enhancing Vision Transformers with Convolutional Attention Mechanisms for Free-Space Optical Accelerators", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper introduces ConvShareViT, a novel deep learning architecture that\nadapts Vision Transformers (ViTs) to the 4f free-space optical system.\nConvShareViT replaces linear layers in multi-head self-attention (MHSA) and\nMultilayer Perceptrons (MLPs) with a depthwise convolutional layer with shared\nweights across input channels. Through the development of ConvShareViT, the\nbehaviour of convolutions within MHSA and their effectiveness in learning the\nattention mechanism were analysed systematically. Experimental results\ndemonstrate that certain configurations, particularly those using valid-padded\nshared convolutions, can successfully learn attention, achieving comparable\nattention scores to those obtained with standard ViTs. However, other\nconfigurations, such as those using same-padded convolutions, show limitations\nin attention learning and operate like regular CNNs rather than transformer\nmodels. ConvShareViT architectures are specifically optimised for the 4f\noptical system, which takes advantage of the parallelism and high-resolution\ncapabilities of optical systems. Results demonstrate that ConvShareViT can\ntheoretically achieve up to 3.04 times faster inference than GPU-based systems.\nThis potential acceleration makes ConvShareViT an attractive candidate for\nfuture optical deep learning applications and proves that our ViT\n(ConvShareViT) can be employed using only the convolution operation, via the\nnecessary optimisation of the ViT to balance performance and complexity."}
{"id": "2504.11809", "pdf": "https://arxiv.org/pdf/2504.11809", "abs": "https://arxiv.org/abs/2504.11809", "authors": ["Biao Fu", "Donglei Yu", "Minpeng Liao", "Chengxi Li", "Yidong Chen", "Kai Fan", "Xiaodong Shi"], "title": "Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture", "categories": ["cs.CL"], "comment": null, "summary": "Simultaneous speech translation (SimulST) produces translations incrementally\nwhile processing partial speech input. Although large language models (LLMs)\nhave showcased strong capabilities in offline translation tasks, applying them\nto SimulST poses notable challenges. Existing LLM-based SimulST approaches\neither incur significant computational overhead due to repeated encoding of\nbidirectional speech encoder, or they depend on a fixed read/write policy,\nlimiting the efficiency and performance. In this work, we introduce Efficient\nand Adaptive Simultaneous Speech Translation (EASiST) with fully unidirectional\narchitecture, including both speech encoder and LLM. EASiST includes a\nmulti-latency data curation strategy to generate semantically aligned SimulST\ntraining samples and redefines SimulST as an interleaved generation task with\nexplicit read/write tokens. To facilitate adaptive inference, we incorporate a\nlightweight policy head that dynamically predicts read/write actions.\nAdditionally, we employ a multi-stage training strategy to align speech-text\nmodalities and optimize both translation and policy behavior. Experiments on\nthe MuST-C En$\\rightarrow$De and En$\\rightarrow$Es datasets demonstrate that\nEASiST offers superior latency-quality trade-offs compared to several strong\nbaselines."}
{"id": "2504.11588", "pdf": "https://arxiv.org/pdf/2504.11588", "abs": "https://arxiv.org/abs/2504.11588", "authors": ["Siteng Ma", "Honghui Du", "Yu An", "Jing Wang", "Qinqin Wang", "Haochang Wu", "Aonghus Lawlor", "Ruihai Dong"], "title": "Deep Learning Approaches for Medical Imaging Under Varying Degrees of Label Availability: A Comprehensive Survey", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 92C50, 92C55", "I.2.10; I.4.5; I.4.6; I.4.9; J.3"], "comment": "33 pages, 10 figures, 8 tables. Will be submit to Medical Image\n  Analysis", "summary": "Deep learning has achieved significant breakthroughs in medical imaging, but\nthese advancements are often dependent on large, well-annotated datasets.\nHowever, obtaining such datasets poses a significant challenge, as it requires\ntime-consuming and labor-intensive annotations from medical experts.\nConsequently, there is growing interest in learning paradigms such as\nincomplete, inexact, and absent supervision, which are designed to operate\nunder limited, inexact, or missing labels. This survey categorizes and reviews\nthe evolving research in these areas, analyzing around 600 notable\ncontributions since 2018. It covers tasks such as image classification,\nsegmentation, and detection across various medical application areas, including\nbut not limited to brain, chest, and cardiac imaging. We attempt to establish\nthe relationships among existing research studies in related areas. We provide\nformal definitions of different learning paradigms and offer a comprehensive\nsummary and interpretation of various learning mechanisms and strategies,\naiding readers in better understanding the current research landscape and\nideas. We also discuss potential future research challenges."}
{"id": "2504.11814", "pdf": "https://arxiv.org/pdf/2504.11814", "abs": "https://arxiv.org/abs/2504.11814", "authors": ["Kirill Chirkunov", "Bashar Alhafni", "Chatrine Qwaider", "Nizar Habash", "Ted Briscoe"], "title": "ARWI: Arabic Write and Improve", "categories": ["cs.CL"], "comment": null, "summary": "Although Arabic is spoken by over 400 million people, advanced Arabic writing\nassistance tools remain limited. To address this gap, we present ARWI, a new\nwriting assistant that helps learners improve essay writing in Modern Standard\nArabic. ARWI is the first publicly available Arabic writing assistant to\ninclude a prompt database for different proficiency levels, an Arabic text\neditor, state-of-the-art grammatical error detection and correction, and\nautomated essay scoring aligned with the Common European Framework of Reference\nstandards for language attainment. Moreover, ARWI can be used to gather a\ngrowing auto-annotated corpus, facilitating further research on Arabic grammar\ncorrection and essay scoring, as well as profiling patterns of errors made by\nnative speakers and non-native learners. A preliminary user study shows that\nARWI provides actionable feedback, helping learners identify grammatical gaps,\nassess language proficiency, and guide improvement."}
{"id": "2504.11637", "pdf": "https://arxiv.org/pdf/2504.11637", "abs": "https://arxiv.org/abs/2504.11637", "authors": ["Yiming Xiao", "Ali Mostafavi"], "title": "DamageCAT: A Deep Learning Transformer Framework for Typology-Based Post-Disaster Building Damage Categorization", "categories": ["cs.CV"], "comment": "23 pages, 6 figures", "summary": "Natural disasters increasingly threaten communities worldwide, creating an\nurgent need for rapid, reliable building damage assessment to guide emergency\nresponse and recovery efforts. Current methods typically classify damage in\nbinary (damaged/undamaged) or ordinal severity terms, limiting their practical\nutility. In fact, the determination of damage typology is crucial for response\nand recovery efforts. To address this important gap, this paper introduces\nDamageCAT, a novel framework that provides typology-based categorical damage\ndescriptions rather than simple severity ratings. Accordingly, this study\npresents two key contributions: (1) the BD-TypoSAT dataset containing satellite\nimage triplets (pre-disaster, post-disaster, and damage masks) from Hurricane\nIda with four damage categories (partial roof damage, total roof damage,\npartial structural collapse, and total structural collapse), and (2) a\nhierarchical U-Net-based transformer architecture that effectively processes\npre-post disaster image pairs to identify and categorize building damage.\nDespite significant class imbalances in the training data, our model achieved\nrobust performance with overall metrics of 0.7921 Intersection over Union (IoU)\nand 0.8835 F1 scores across all categories. The model's capability to recognize\nintricate damage typology in less common categories is especially remarkable.\nThe DamageCAT framework advances automated damage assessment by providing\nactionable, typological information that better supports disaster response\ndecision-making and resource allocation compared to traditional severity-based\napproaches."}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829", "abs": "https://arxiv.org/abs/2504.11829", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "title": "Déjà Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development."}
{"id": "2504.11662", "pdf": "https://arxiv.org/pdf/2504.11662", "abs": "https://arxiv.org/abs/2504.11662", "authors": ["Marcos Mendes", "Gonçalo Perna", "Pedro Rito", "Duarte Raposo", "Susana Sargento"], "title": "Real-time Object and Event Detection Service through Computer Vision and Edge Computing", "categories": ["cs.CV", "68T45"], "comment": "30th ITS World Congress, Dubai, UAE, 16-20 September 2024", "summary": "The World Health Organization suggests that road traffic crashes cost\napproximately 518 billion dollars globally each year, which accounts for 3% of\nthe gross domestic product for most countries. Most fatal road accidents in\nurban areas involve Vulnerable Road Users (VRUs). Smart cities environments\npresent innovative approaches to combat accidents involving cutting-edge\ntechnologies, that include advanced sensors, extensive datasets, Machine\nLearning (ML) models, communication systems, and edge computing. This paper\nproposes a strategy and an implementation of a system for road monitoring and\nsafety for smart cities, based on Computer Vision (CV) and edge computing.\nPromising results were obtained by implementing vision algorithms and tracking\nusing surveillance cameras, that are part of a Smart City testbed, the Aveiro\nTech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars,\npedestrians, and bicycles, while predicting the road state, the distance\nbetween moving objects, and inferring on collision events to prevent\ncollisions, in near real-time."}
{"id": "2504.11833", "pdf": "https://arxiv.org/pdf/2504.11833", "abs": "https://arxiv.org/abs/2504.11833", "authors": ["Changjiang Gao", "Xu Huang", "Wenhao Zhu", "Shujian Huang", "Lei Li", "Fei Yuan"], "title": "Could Thinking Multilingually Empower LLM Reasoning?", "categories": ["cs.CL"], "comment": null, "summary": "Previous work indicates that large language models exhibit a significant\n\"English bias\", i.e. they often perform better when tasks are presented in\nEnglish. Interestingly, we have observed that using certain other languages in\nreasoning tasks can yield better performance than English. However, this\nphenomenon remains under-explored. In this paper, we explore the upper bound of\nharnessing multilingualism in reasoning tasks, suggesting that multilingual\nreasoning promises significantly (by nearly 10 Acc@$k$ points) and robustly\n(tolerance for variations in translation quality and language choice) higher\nupper bounds than English-only reasoning. Besides analyzing the reason behind\nthe upper bound and challenges in reaching it, we also find that common answer\nselection methods cannot achieve this upper bound, due to their limitations and\nbiases. These insights could pave the way for future research aimed at fully\nharnessing the potential of multilingual reasoning in LLMs."}
{"id": "2504.11669", "pdf": "https://arxiv.org/pdf/2504.11669", "abs": "https://arxiv.org/abs/2504.11669", "authors": ["Amirhossein Dadashzadeh", "Parsa Esmati", "Majid Mirmehdi"], "title": "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Source-Free Unsupervised Video Domain Adaptation (SFUVDA)\nleverage vision-language models to enhance pseudo-label generation. However,\nchallenges such as noisy pseudo-labels and over-confident predictions limit\ntheir effectiveness in adapting well across domains. We propose Co-STAR, a\nnovel framework that integrates curriculum learning with collaborative\nself-training between a source-trained teacher and a contrastive\nvision-language model (CLIP). Our curriculum learning approach employs a\nreliability-based weight function that measures bidirectional prediction\nalignment between the teacher and CLIP, balancing between confident and\nuncertain predictions. This function preserves uncertainty for difficult\nsamples, while prioritizing reliable pseudo-labels when the predictions from\nboth models closely align. To further improve adaptation, we propose Adaptive\nCurriculum Regularization, which modifies the learning priority of samples in a\nprobabilistic, adaptive manner based on their confidence scores and prediction\nstability, mitigating overfitting to noisy and over-confident samples.\nExtensive experiments across multiple video domain adaptation benchmarks\ndemonstrate that Co-STAR consistently outperforms state-of-the-art SFUVDA\nmethods. Code is available at: https://github.com/Plrbear/Co-Star"}
{"id": "2504.11837", "pdf": "https://arxiv.org/pdf/2504.11837", "abs": "https://arxiv.org/abs/2504.11837", "authors": ["Yue Zhao", "Qingqing Gu", "Xiaoyu Wang", "Teng Chen", "Zhonglin Jiang", "Yong Chen", "Luo Ji"], "title": "FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "accepted by CMCL", "summary": "Emotional support conversation (ESC) aims to alleviate the emotional distress\nof individuals through effective conversations. Although large language models\n(LLMs) have obtained remarkable progress on ESC, most of these studies might\nnot define the diagram from the state model perspective, therefore providing a\nsuboptimal solution for long-term satisfaction. To address such an issue, we\nleverage the Finite State Machine (FSM) on LLMs, and propose a framework called\nFiSMiness. Our framework allows a single LLM to bootstrap the planning during\nESC, and self-reason the seeker's emotion, support strategy and the final\nresponse upon each conversational turn. Substantial experiments on ESC datasets\nsuggest that FiSMiness outperforms many baselines, including direct inference,\nself-refine, chain of thought, finetuning, and external-assisted methods, even\nthose with many more parameters."}
{"id": "2504.11686", "pdf": "https://arxiv.org/pdf/2504.11686", "abs": "https://arxiv.org/abs/2504.11686", "authors": ["Yiran He", "Yun Cao", "Bowen Yang", "Zeyu Zhang"], "title": "Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 11 figures, 13IHMMSec2025", "summary": "The rapid development of generative AI facilitates content creation and makes\nimage manipulation easier and more difficult to detect. While multimodal Large\nLanguage Models (LLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating AI-generated Content (AIGC) and struggle to\ncomprehend local forgery details. In this work, we investigate the application\nof multimodal LLMs in forgery detection. We propose a framework capable of\nevaluating image authenticity, localizing tampered regions, providing evidence,\nand tracing generation methods based on semantic tampering clues. Our method\ndemonstrates that the potential of LLMs in forgery analysis can be effectively\nunlocked through meticulous prompt engineering and the application of few-shot\nlearning techniques. We conduct qualitative and quantitative experiments and\nshow that GPT4V can achieve an accuracy of 92.1% in Autosplice and 86.3% in\nLaMa, which is competitive with state-of-the-art AIGC detection methods. We\nfurther discuss the limitations of multimodal LLMs in such tasks and propose\npotential improvements."}
{"id": "2504.11900", "pdf": "https://arxiv.org/pdf/2504.11900", "abs": "https://arxiv.org/abs/2504.11900", "authors": ["Kabir Ahuja", "Melanie Sclar", "Yulia Tsvetkov"], "title": "Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Stories are a fundamental aspect of human experience. Engaging deeply with\nstories and spotting plot holes -- inconsistencies in a storyline that break\nthe internal logic or rules of a story's world -- requires nuanced reasoning\nskills, including tracking entities and events and their interplay, abstract\nthinking, pragmatic narrative understanding, commonsense and social reasoning,\nand theory of mind. As Large Language Models (LLMs) increasingly generate,\ninterpret, and modify text, rigorously assessing their narrative consistency\nand deeper language understanding becomes critical. However, existing\nbenchmarks focus mainly on surface-level comprehension. In this work, we\npropose plot hole detection in stories as a proxy to evaluate language\nunderstanding and reasoning in LLMs. We introduce FlawedFictionsMaker, a novel\nalgorithm to controllably and carefully synthesize plot holes in human-written\nstories. Using this algorithm, we construct a benchmark to evaluate LLMs' plot\nhole detection abilities in stories -- FlawedFictions -- , which is robust to\ncontamination, with human filtering ensuring high quality. We find that\nstate-of-the-art LLMs struggle in accurately solving FlawedFictions regardless\nof the reasoning effort allowed, with performance significantly degrading as\nstory length increases. Finally, we show that LLM-based story summarization and\nstory generation are prone to introducing plot holes, with more than 50% and\n100% increases in plot hole detection rates with respect to human-written\noriginals."}
{"id": "2504.11695", "pdf": "https://arxiv.org/pdf/2504.11695", "abs": "https://arxiv.org/abs/2504.11695", "authors": ["Isabel Papadimitriou", "Huangyuan Su", "Thomas Fel", "Naomi Saphra", "Sham Kakade", "Stephanie Gil"], "title": "Interpreting the Linear Structure of Vision-language Model Embedding Spaces", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Vision-language models encode images and text in a joint space, minimizing\nthe distance between corresponding image and text pairs. How are language and\nimages organized in this joint space, and how do the models encode meaning and\nmodality? To investigate this, we train and release sparse autoencoders (SAEs)\non the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,\nand AIMv2). SAEs approximate model embeddings as sparse linear combinations of\nlearned directions, or \"concepts\". We find that, compared to other methods of\nlinear feature learning, SAEs are better at reconstructing the real embeddings,\nwhile also able to retain the most sparsity. Retraining SAEs with different\nseeds or different data diet leads to two findings: the rare, specific concepts\ncaptured by the SAEs are liable to change drastically, but we also show that\nthe key commonly-activating concepts extracted by SAEs are remarkably stable\nacross runs. Interestingly, while most concepts are strongly unimodal in\nactivation, we find they are not merely encoding modality per se. Many lie\nclose to - but not entirely within - the subspace defining modality, suggesting\nthat they encode cross-modal semantics despite their unimodal usage. To\nquantify this bridging behavior, we introduce the Bridge Score, a metric that\nidentifies concept pairs which are both co-activated across aligned image-text\ninputs and geometrically aligned in the shared space. This reveals that even\nunimodal concepts can collaborate to support cross-modal integration. We\nrelease interactive demos of the SAEs for all models, allowing researchers to\nexplore the organization of the concept spaces. Overall, our findings uncover a\nsparse linear structure within VLM embedding spaces that is shaped by modality,\nyet stitched together through latent bridges-offering new insight into how\nmultimodal meaning is constructed."}
{"id": "2504.11934", "pdf": "https://arxiv.org/pdf/2504.11934", "abs": "https://arxiv.org/abs/2504.11934", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025", "summary": "Gender-neutral translation (GNT) aims to avoid expressing the gender of human\nreferents when the source text lacks explicit cues about the gender of those\nreferents. Evaluating GNT automatically is particularly challenging, with\ncurrent solutions being limited to monolingual classifiers. Such solutions are\nnot ideal because they do not factor in the source sentence and require\ndedicated data and fine-tuning to scale to new languages. In this work, we\naddress such limitations by investigating the use of large language models\n(LLMs) as evaluators of GNT. Specifically, we explore two prompting approaches:\none in which LLMs generate sentence-level assessments only, and another, akin\nto a chain-of-thought approach, where they first produce detailed phrase-level\nannotations before a sentence-level judgment. Through extensive experiments on\nmultiple languages with five models, both open and proprietary, we show that\nLLMs can serve as evaluators of GNT. Moreover, we find that prompting for\nphrase-level annotations before sentence-level assessments consistently\nimproves the accuracy of all models, providing a better and more scalable\nalternative to current solutions."}
{"id": "2504.11701", "pdf": "https://arxiv.org/pdf/2504.11701", "abs": "https://arxiv.org/abs/2504.11701", "authors": ["Yaohui Fang", "Xingce Wang"], "title": "Non-uniform Point Cloud Upsampling via Local Manifold Distribution", "categories": ["cs.CV", "math.DG"], "comment": null, "summary": "Existing learning-based point cloud upsampling methods often overlook the\nintrinsic data distribution charac?teristics of point clouds, leading to\nsuboptimal results when handling sparse and non-uniform point clouds. We\npropose a novel approach to point cloud upsampling by imposing constraints from\nthe perspective of manifold distributions. Leveraging the strong fitting\ncapability of Gaussian functions, our method employs a network to iteratively\noptimize Gaussian components and their weights, accurately representing local\nmanifolds. By utilizing the probabilistic distribution properties of Gaussian\nfunctions, we construct a unified statistical manifold to impose distribution\nconstraints on the point cloud. Experimental results on multiple datasets\ndemonstrate that our method generates higher-quality and more uniformly\ndistributed dense point clouds when processing sparse and non-uniform inputs,\noutperforming state-of-the-art point cloud upsampling techniques."}
{"id": "2504.11952", "pdf": "https://arxiv.org/pdf/2504.11952", "abs": "https://arxiv.org/abs/2504.11952", "authors": ["Ram Mohan Rao Kadiyala", "Siddartha Pullakhandam", "Kanwal Mehreen", "Drishti Sharma", "Siddhant Gupta", "Jebish Purbey", "Ashay Srivastava", "Subhasya TippaReddy", "Arvind Reddy Bobbili", "Suraj Telugara Chandrashekhar", "Modabbir Adeeb", "Srinadh Vura", "Hamza Farooq"], "title": "Robust and Fine-Grained Detection of AI Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Feb ARR Submission", "summary": "An ideal detection system for machine generated content is supposed to work\nwell on any generator as many more advanced LLMs come into existence day by\nday. Existing systems often struggle with accurately identifying AI-generated\ncontent over shorter texts. Further, not all texts might be entirely authored\nby a human or LLM, hence we focused more over partial cases i.e human-LLM\nco-authored texts. Our paper introduces a set of models built for the task of\ntoken classification which are trained on an extensive collection of\nhuman-machine co-authored texts, which performed well over texts of unseen\ndomains, unseen generators, texts by non-native speakers and those with\nadversarial inputs. We also introduce a new dataset of over 2.4M such texts\nmostly co-authored by several popular proprietary LLMs over 23 languages. We\nalso present findings of our models' performance over each texts of each domain\nand generator. Additional findings include comparison of performance against\neach adversarial method, length of input texts and characteristics of generated\ntexts compared to the original human authored texts."}
{"id": "2504.11705", "pdf": "https://arxiv.org/pdf/2504.11705", "abs": "https://arxiv.org/abs/2504.11705", "authors": ["Adriano D'Alessandro", "Ali Mahdavi-Amiri", "Ghassan Hamarneh"], "title": "Learning What NOT to Count", "categories": ["cs.CV"], "comment": null, "summary": "Few/zero-shot object counting methods reduce the need for extensive\nannotations but often struggle to distinguish between fine-grained categories,\nespecially when multiple similar objects appear in the same scene. To address\nthis limitation, we propose an annotation-free approach that enables the\nseamless integration of new fine-grained categories into existing few/zero-shot\ncounting models. By leveraging latent generative models, we synthesize\nhigh-quality, category-specific crowded scenes, providing a rich training\nsource for adapting to new categories without manual labeling. Our approach\nintroduces an attention prediction network that identifies fine-grained\ncategory boundaries trained using only synthetic pseudo-annotated data. At\ninference, these fine-grained attention estimates refine the output of existing\nfew/zero-shot counting networks. To benchmark our method, we further introduce\nthe FGTC dataset, a taxonomy-specific fine-grained object counting dataset for\nnatural images. Our method substantially enhances pre-trained state-of-the-art\nmodels on fine-grained taxon counting tasks, while using only synthetic data.\nCode and data to be released upon acceptance."}
{"id": "2504.11972", "pdf": "https://arxiv.org/pdf/2504.11972", "abs": "https://arxiv.org/abs/2504.11972", "authors": ["Xanh Ho", "Jiahao Huang", "Florian Boudin", "Akiko Aizawa"], "title": "LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA", "categories": ["cs.CL"], "comment": "17 pages; code and data are available at\n  https://github.com/Alab-NII/llm-judge-extract-qa", "summary": "Extractive reading comprehension question answering (QA) datasets are\ntypically evaluated using Exact Match (EM) and F1-score, but these metrics\noften fail to fully capture model performance. With the success of large\nlanguage models (LLMs), they have been employed in various tasks, including\nserving as judges (LLM-as-a-judge). In this paper, we reassess the performance\nof QA models using LLM-as-a-judge across four reading comprehension QA\ndatasets. We examine different families of LLMs and various answer types to\nevaluate the effectiveness of LLM-as-a-judge in these tasks. Our results show\nthat LLM-as-a-judge is highly correlated with human judgments and can replace\ntraditional EM/F1 metrics. By using LLM-as-a-judge, the correlation with human\njudgments improves significantly, from 0.17 (EM) and 0.36 (F1-score) to 0.85.\nThese findings confirm that EM and F1 metrics underestimate the true\nperformance of the QA models. While LLM-as-a-judge is not perfect for more\ndifficult answer types (e.g., job), it still outperforms EM/F1, and we observe\nno bias issues, such as self-preference, when the same model is used for both\nthe QA and judgment tasks."}
{"id": "2504.11707", "pdf": "https://arxiv.org/pdf/2504.11707", "abs": "https://arxiv.org/abs/2504.11707", "authors": ["Muhammad Shahid Muneer", "Simon S. Woo"], "title": "Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Short Paper The Web Conference", "summary": "In the past years, we have witnessed the remarkable success of Text-to-Image\n(T2I) models and their widespread use on the web. Extensive research in making\nT2I models produce hyper-realistic images has led to new concerns, such as\ngenerating Not-Safe-For-Work (NSFW) web content and polluting the web society.\nTo help prevent misuse of T2I models and create a safer web environment for\nusers features like NSFW filters and post-hoc security checks are used in these\nmodels. However, recent work unveiled how these methods can easily fail to\nprevent misuse. In particular, adversarial attacks on text and image modalities\ncan easily outplay defensive measures. %Exploiting such leads to the growing\nconcern of preventing adversarial attacks on text and image modalities.\nMoreover, there is currently no robust multimodal NSFW dataset that includes\nboth prompt and image pairs and adversarial examples. This work proposes a\nmillion-scale prompt and image dataset generated using open-source diffusion\nmodels. Second, we develop a multimodal defense to distinguish safe and NSFW\ntext and images, which is robust against adversarial attacks and directly\nalleviates current challenges. Our extensive experiments show that our model\nperforms well against existing SOTA NSFW detection methods in terms of accuracy\nand recall, drastically reducing the Attack Success Rate (ASR) in multimodal\nadversarial attack scenarios. Code:\nhttps://github.com/shahidmuneer/multimodal-nsfw-defense."}
{"id": "2504.11975", "pdf": "https://arxiv.org/pdf/2504.11975", "abs": "https://arxiv.org/abs/2504.11975", "authors": ["Raúl Vázquez", "Timothee Mickus", "Elaine Zosa", "Teemu Vahtola", "Jörg Tiedemann", "Aman Sinha", "Vincent Segonne", "Fernando Sánchez-Vega", "Alessandro Raganato", "Jindřich Libovický", "Jussi Karlgren", "Shaoxiong Ji", "Jindřich Helcl", "Liane Guillou", "Ona de Gibert", "Jaione Bengoetxea", "Joseph Attieh", "Marianna Apidianaki"], "title": "SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes", "categories": ["cs.CL"], "comment": "Mu-SHROOM is part of SemEval-2025 (Task 3). TBP: Proceedings of the\n  19th International Workshop on Semantic Evaluation (SemEval-2025)", "summary": "We present the Mu-SHROOM shared task which is focused on detecting\nhallucinations and other overgeneration mistakes in the output of\ninstruction-tuned large language models (LLMs). Mu-SHROOM addresses\ngeneral-purpose LLMs in 14 languages, and frames the hallucination detection\nproblem as a span-labeling task. We received 2,618 submissions from 43\nparticipating teams employing diverse methodologies. The large number of\nsubmissions underscores the interest of the community in hallucination\ndetection. We present the results of the participating systems and conduct an\nempirical analysis to identify key factors contributing to strong performance\nin this task. We also emphasize relevant current challenges, notably the\nvarying degree of hallucinations across languages and the high annotator\ndisagreement when labeling hallucination spans."}
{"id": "2504.11732", "pdf": "https://arxiv.org/pdf/2504.11732", "abs": "https://arxiv.org/abs/2504.11732", "authors": ["Jilan Xu", "Yifei Huang", "Baoqi Pei", "Junlin Hou", "Qingqiu Li", "Guo Chen", "Yuejie Zhang", "Rui Feng", "Weidi Xie"], "title": "EgoExo-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos", "categories": ["cs.CV"], "comment": "ICLR 2025", "summary": "Generating videos in the first-person perspective has broad application\nprospects in the field of augmented reality and embodied intelligence. In this\nwork, we explore the cross-view video prediction task, where given an\nexo-centric video, the first frame of the corresponding ego-centric video, and\ntextual instructions, the goal is to generate futur frames of the ego-centric\nvideo. Inspired by the notion that hand-object interactions (HOI) in\nego-centric videos represent the primary intentions and actions of the current\nactor, we present EgoExo-Gen that explicitly models the hand-object dynamics\nfor cross-view video prediction. EgoExo-Gen consists of two stages. First, we\ndesign a cross-view HOI mask prediction model that anticipates the HOI masks in\nfuture ego-frames by modeling the spatio-temporal ego-exo correspondence. Next,\nwe employ a video diffusion model to predict future ego-frames using the first\nego-frame and textual instructions, while incorporating the HOI masks as\nstructural guidance to enhance prediction quality. To facilitate training, we\ndevelop an automated pipeline to generate pseudo HOI masks for both ego- and\nexo-videos by exploiting vision foundation models. Extensive experiments\ndemonstrate that our proposed EgoExo-Gen achieves better prediction performance\ncompared to previous video prediction models on the Ego-Exo4D and H2O benchmark\ndatasets, with the HOI masks significantly improving the generation of hands\nand interactive objects in the ego-centric videos."}
{"id": "2504.11986", "pdf": "https://arxiv.org/pdf/2504.11986", "abs": "https://arxiv.org/abs/2504.11986", "authors": ["Jose Manuel Guevara-Vela"], "title": "Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This essay proposes an analogy between large language models (LLMs) and\nquasicrystals: systems that exhibit global coherence without periodic\nrepetition and that are generated through local constraints. While LLMs are\noften evaluated in terms of predictive accuracy, factuality, or alignment, this\nstructural perspective suggests that their most characteristic behavior is the\nproduction of internally resonant linguistic patterns. Just as quasicrystals\nforced a redefinition of order in physical systems, viewing LLMs as generators\nof quasi-structured language opens new paths for evaluation and design:\nprivileging propagation of constraint over token-level accuracy, and coherence\nof form over fixed meaning. LLM outputs should be read not only for what they\nsay, but for the patterns of constraint and coherence that organize them. This\nshift reframes generative language as a space of emergent patterning: LLMs are\nneither fully random nor strictly rule-based, but defined by a logic of\nconstraint, resonance, and structural depth."}
{"id": "2504.11733", "pdf": "https://arxiv.org/pdf/2504.11733", "abs": "https://arxiv.org/abs/2504.11733", "authors": ["Li Yu", "Situo Wang", "Wei Zhou", "Moncef Gabbouj"], "title": "DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "Inspired by the dual-stream theory of the human visual system (HVS) - where\nthe ventral stream is responsible for object recognition and detail analysis,\nwhile the dorsal stream focuses on spatial relationships and motion perception\n- an increasing number of video quality assessment (VQA) works built upon this\nframework are proposed. Recent advancements in large multi-modal models,\nnotably Contrastive Language-Image Pretraining (CLIP), have motivated\nresearchers to incorporate CLIP into dual-stream-based VQA methods. This\nintegration aims to harness the model's superior semantic understanding\ncapabilities to replicate the object recognition and detail analysis in ventral\nstream, as well as spatial relationship analysis in dorsal stream. However,\nCLIP is originally designed for images and lacks the ability to capture\ntemporal and motion information inherent in videos. %Furthermore, existing\nfeature fusion strategies in no-reference video quality assessment (NR-VQA)\noften rely on fixed weighting schemes, which fail to adaptively adjust feature\nimportance. To address the limitation, this paper propose a Decoupled\nVision-Language Modeling with Text-Guided Adaptation for Blind Video Quality\nAssessment (DVLTA-VQA), which decouples CLIP's visual and textual components,\nand integrates them into different stages of the NR-VQA pipeline."}
{"id": "2504.12052", "pdf": "https://arxiv.org/pdf/2504.12052", "abs": "https://arxiv.org/abs/2504.12052", "authors": ["François Haguinet", "Jeffery L Painter", "Gregory E Powell", "Andrea Callegaro", "Andrew Bate"], "title": "Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": "30 pages, 7 figures, 5 supplementary figures", "summary": "We present a Bayesian dynamic borrowing (BDB) approach to enhance the\nquantitative identification of adverse events (AEs) in spontaneous reporting\nsystems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior\nwithin a Bayesian hierarchical model and incorporates semantic similarity\nmeasures (SSMs) to enable weighted information sharing from MedDRA Preferred\nTerms (PTs) that are clinical similar to the target PT. This continuous\nsimilarity-based borrowing addresses limitation of rigid hierarchical grouping\nin current disproportionality analysis (DPA).\n  Using data from the FDA Adverse Event Reporting System (FAERS) between 2015\nand 2019, we evalute this approach - termed IC SSM - against standard\nInformation Component (IC) analysis and IC with borrowing at the MedDRA\nhigh-level group term (HLGT) level. A novel references set (PVLens), derived\nfrom FDA product label updates, enabled prospective evaluation of method\nperformance in identifying AEs prior to official labeling.\n  The IC SSM approach demonstrated improved sensitivity compared to both\ntraditional IC and HLGT-based borrowing, with minor trade-offs in F1 scores and\nYouden's index. IC SSM consistently identified more true positives and detected\nsignals over 5 months sooner than traditional IC. Despite a marginally lower\naggregate Youden's index, IC SSM showed higher performance in the early\npost-marketing period, providing more stable and relevant estimates than\nHLGT-based borrowing and traditional IC.\n  These findings support the use of SSM-informed Bayesian borrowing as a\nscalable and context-aware enhancement to traditional DPA methods. Future\nresearch should validate this approach across other datasets and explore\nadditional similarity metrics and Bayesian inference strategies using\ncase-level data."}
{"id": "2504.11739", "pdf": "https://arxiv.org/pdf/2504.11739", "abs": "https://arxiv.org/abs/2504.11739", "authors": ["Bingjie Gao", "Xinyu Gao", "Xiaoxue Wu", "Yujie Zhou", "Yu Qiao", "Li Niu", "Xinyuan Chen", "Yaohui Wang"], "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "categories": ["cs.CV", "cs.CL"], "comment": "accepted by CVPR2025", "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}."}
{"id": "2504.12082", "pdf": "https://arxiv.org/pdf/2504.12082", "abs": "https://arxiv.org/abs/2504.12082", "authors": ["Yumin Kim", "Hwanhee Lee"], "title": "Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection is a crucial area of research in natural language\nprocessing, essential for ensuring online community safety. However, detecting\nimplicit hate speech, where harmful intent is conveyed in subtle or indirect\nways, remains a major challenge. Unlike explicit hate speech, implicit\nexpressions often depend on context, cultural subtleties, and hidden biases,\nmaking them more challenging to identify consistently. Additionally, the\ninterpretation of such speech is influenced by external knowledge and\ndemographic biases, resulting in varied detection results across different\nlanguage models. Furthermore, Large Language Models often show heightened\nsensitivity to toxic language and references to vulnerable groups, which can\nlead to misclassifications. This over-sensitivity results in false positives\n(incorrectly identifying harmless statements as hateful) and false negatives\n(failing to detect genuinely harmful content). Addressing these issues requires\nmethods that not only improve detection precision but also reduce model biases\nand enhance robustness. To address these challenges, we propose a novel method,\nwhich utilizes in-context learning without requiring model fine-tuning. By\nadaptively retrieving demonstrations that focus on similar groups or those with\nthe highest similarity scores, our approach enhances contextual comprehension.\nExperimental results show that our method outperforms current state-of-the-art\ntechniques. Implementation details and code are available at TBD."}
{"id": "2504.11749", "pdf": "https://arxiv.org/pdf/2504.11749", "abs": "https://arxiv.org/abs/2504.11749", "authors": ["Zongye Zhang", "Wenrui Cai", "Qingjie Liu", "Yunhong Wang"], "title": "SkeletonX: Data-Efficient Skeleton-based Action Recognition via Cross-sample Feature Aggregation", "categories": ["cs.CV", "I.4.9"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM). 13 pages, 7\n  figures, 11 tables", "summary": "While current skeleton action recognition models demonstrate impressive\nperformance on large-scale datasets, their adaptation to new application\nscenarios remains challenging. These challenges are particularly pronounced\nwhen facing new action categories, diverse performers, and varied skeleton\nlayouts, leading to significant performance degeneration. Additionally, the\nhigh cost and difficulty of collecting skeleton data make large-scale data\ncollection impractical. This paper studies one-shot and limited-scale learning\nsettings to enable efficient adaptation with minimal data. Existing approaches\noften overlook the rich mutual information between labeled samples, resulting\nin sub-optimal performance in low-data scenarios. To boost the utility of\nlabeled data, we identify the variability among performers and the commonality\nwithin each action as two key attributes. We present SkeletonX, a lightweight\ntraining pipeline that integrates seamlessly with existing GCN-based skeleton\naction recognizers, promoting effective training under limited labeled data.\nFirst, we propose a tailored sample pair construction strategy on two key\nattributes to form and aggregate sample pairs. Next, we develop a concise and\neffective feature aggregation module to process these pairs. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D 120, and PKU-MMD with various\nGCN backbones, demonstrating that the pipeline effectively improves performance\nwhen trained from scratch with limited data. Moreover, it surpasses previous\nstate-of-the-art methods in the one-shot setting, with only 1/10 of the\nparameters and much fewer FLOPs. The code and data are available at:\nhttps://github.com/zzysteve/SkeletonX"}
{"id": "2504.12098", "pdf": "https://arxiv.org/pdf/2504.12098", "abs": "https://arxiv.org/abs/2504.12098", "authors": ["Adil Bahaj", "Hamed Rahimi", "Mohamed Chetouani", "Mounir Ghogho"], "title": "Gauging Overprecision in LLMs: An Empirical Study", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Recently, overconfidence in large language models (LLMs) has garnered\nconsiderable attention due to its fundamental importance in quantifying the\ntrustworthiness of LLM generation. However, existing approaches prompt the\n\\textit{black box LLMs} to produce their confidence (\\textit{verbalized\nconfidence}), which can be subject to many biases and hallucinations. Inspired\nby a different aspect of overconfidence in cognitive science called\n\\textit{overprecision}, we designed a framework for its study in black box\nLLMs. This framework contains three main phases: 1) generation, 2) refinement\nand 3) evaluation. In the generation phase we prompt the LLM to generate\nanswers to numerical questions in the form of intervals with a certain level of\nconfidence. This confidence level is imposed in the prompt and not required for\nthe LLM to generate as in previous approaches. We use various prompting\ntechniques and use the same prompt multiple times to gauge the effects of\nrandomness in the generation process. In the refinement phase, answers from the\nprevious phase are refined to generate better answers. The LLM answers are\nevaluated and studied in the evaluation phase to understand its internal\nworkings. This study allowed us to gain various insights into LLM\noverprecision: 1) LLMs are highly uncalibrated for numerical tasks 2)\n{\\color{blue}there is no correlation between the length of the interval and the\nimposed confidence level, which can be symptomatic of a a) lack of\nunderstanding of the concept of confidence or b) inability to adjust\nself-confidence by following instructions}, {\\color{blue}3)} LLM numerical\nprecision differs depending on the task, scale of answer and prompting\ntechnique {\\color{blue}4) Refinement of answers doesn't improve precision in\nmost cases}. We believe this study offers new perspectives on LLM\noverconfidence and serves as a strong baseline for overprecision in LLMs."}
{"id": "2504.11754", "pdf": "https://arxiv.org/pdf/2504.11754", "abs": "https://arxiv.org/abs/2504.11754", "authors": ["Zihui Zhang", "Yafei Yang", "Hongtao Wen", "Bo Yang"], "title": "GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "ICLR 2025 Spotlight. Code and data are available at:\n  https://github.com/vLAR-group/GrabS", "summary": "We study the hard problem of 3D object segmentation in complex point clouds\nwithout requiring human labels of 3D scenes for supervision. By relying on the\nsimilarity of pretrained 2D features or external signals such as motion to\ngroup 3D points as objects, existing unsupervised methods are usually limited\nto identifying simple objects like cars or their segmented objects are often\ninferior due to the lack of objectness in pretrained features. In this paper,\nwe propose a new two-stage pipeline called GrabS. The core concept of our\nmethod is to learn generative and discriminative object-centric priors as a\nfoundation from object datasets in the first stage, and then design an embodied\nagent to learn to discover multiple objects by querying against the pretrained\ngenerative priors in the second stage. We extensively evaluate our method on\ntwo real-world datasets and a newly created synthetic dataset, demonstrating\nremarkable segmentation performance, clearly surpassing all existing\nunsupervised methods."}
{"id": "2504.12108", "pdf": "https://arxiv.org/pdf/2504.12108", "abs": "https://arxiv.org/abs/2504.12108", "authors": ["Shizhan Cai", "Liang Ding", "Dacheng Tao"], "title": "Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Large Language Models (LLMs) has intensified\nconcerns about content traceability and potential misuse. Existing watermarking\nschemes for sampled text often face trade-offs between maintaining text quality\nand ensuring robust detection against various attacks. To address these issues,\nwe propose a novel watermarking scheme that improves both detectability and\ntext quality by introducing a cumulative watermark entropy threshold. Our\napproach is compatible with and generalizes existing sampling functions,\nenhancing adaptability. Experimental results across multiple LLMs show that our\nscheme significantly outperforms existing methods, achieving over 80\\%\nimprovements on widely-used datasets, e.g., MATH and GSM8K, while maintaining\nhigh detection accuracy."}
{"id": "2504.11763", "pdf": "https://arxiv.org/pdf/2504.11763", "abs": "https://arxiv.org/abs/2504.11763", "authors": ["Aoran Liu", "Kun Hu", "Clinton Mo", "Changyang Li", "Zhiyong Wang"], "title": "Extended Short- and Long-Range Mesh Learning for Fast and Generalized Garment Simulation", "categories": ["cs.CV"], "comment": null, "summary": "3D garment simulation is a critical component for producing cloth-based\ngraphics. Recent advancements in graph neural networks (GNNs) offer a promising\napproach for efficient garment simulation. However, GNNs require extensive\nmessage-passing to propagate information such as physical forces and maintain\ncontact awareness across the entire garment mesh, which becomes computationally\ninefficient at higher resolutions. To address this, we devise a novel GNN-based\nmesh learning framework with two key components to extend the message-passing\nrange with minimal overhead, namely the Laplacian-Smoothed Dual Message-Passing\n(LSDMP) and the Geodesic Self-Attention (GSA) modules. LSDMP enhances\nmessage-passing with a Laplacian features smoothing process, which efficiently\npropagates the impact of each vertex to nearby vertices. Concurrently, GSA\nintroduces geodesic distance embeddings to represent the spatial relationship\nbetween vertices and utilises attention mechanisms to capture global mesh\ninformation. The two modules operate in parallel to ensure both short- and\nlong-range mesh modelling. Extensive experiments demonstrate the\nstate-of-the-art performance of our method, requiring fewer layers and lower\ninference latency."}
{"id": "2504.12140", "pdf": "https://arxiv.org/pdf/2504.12140", "abs": "https://arxiv.org/abs/2504.12140", "authors": ["Miguel Moura Ramos", "Patrick Fernandes", "Sweta Agrawal", "André F. T. Martins"], "title": "Multilingual Contextualization of Large Language Models for Document-Level Machine Translation", "categories": ["cs.CL"], "comment": "9 pages, work-in-progress", "summary": "Large language models (LLMs) have demonstrated strong performance in\nsentence-level machine translation, but scaling to document-level translation\nremains challenging, particularly in modeling long-range dependencies and\ndiscourse phenomena across sentences and paragraphs. In this work, we propose a\nmethod to improve LLM-based long-document translation through targeted\nfine-tuning on high-quality document-level data, which we curate and introduce\nas DocBlocks. Our approach supports multiple translation paradigms, including\ndirect document-to-document and chunk-level translation, by integrating\ninstructions both with and without surrounding context. This enables models to\nbetter capture cross-sentence dependencies while maintaining strong\nsentence-level translation performance. Experimental results show that\nincorporating multiple translation paradigms improves document-level\ntranslation quality and inference speed compared to prompting and agent-based\nmethods."}
{"id": "2504.11773", "pdf": "https://arxiv.org/pdf/2504.11773", "abs": "https://arxiv.org/abs/2504.11773", "authors": ["Yiran Wang", "Jiaqi Li", "Chaoyi Hong", "Ruibo Li", "Liusheng Sun", "Xiao Song", "Zhe Wang", "Zhiguo Cao", "Guosheng Lin"], "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Oral Presentation)", "summary": "Radar-Camera depth estimation aims to predict dense and accurate metric depth\nby fusing input images and Radar data. Model efficiency is crucial for this\ntask in pursuit of real-time processing on autonomous vehicles and robotic\nplatforms. However, due to the sparsity of Radar returns, the prevailing\nmethods adopt multi-stage frameworks with intermediate quasi-dense depth, which\nare time-consuming and not robust. To address these challenges, we propose\nTacoDepth, an efficient and accurate Radar-Camera depth estimation model with\none-stage fusion. Specifically, the graph-based Radar structure extractor and\nthe pyramid-based Radar fusion module are designed to capture and integrate the\ngraph structures of Radar point clouds, delivering superior model efficiency\nand robustness without relying on the intermediate depth results. Moreover,\nTacoDepth can be flexible for different inference modes, providing a better\nbalance of speed and accuracy. Extensive experiments are conducted to\ndemonstrate the efficacy of our method. Compared with the previous\nstate-of-the-art approach, TacoDepth improves depth accuracy and processing\nspeed by 12.8% and 91.8%. Our work provides a new perspective on efficient\nRadar-Camera depth estimation."}
{"id": "2504.12172", "pdf": "https://arxiv.org/pdf/2504.12172", "abs": "https://arxiv.org/abs/2504.12172", "authors": ["Maged S. Al-Shaibani", "Zaid Alyafeai", "Irfan Ahmad"], "title": "Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Arabic poetry is an essential and integral part of Arabic language and\nculture. It has been used by the Arabs to spot lights on their major events\nsuch as depicting brutal battles and conflicts. They also used it, as in many\nother languages, for various purposes such as romance, pride, lamentation, etc.\nArabic poetry has received major attention from linguistics over the decades.\nOne of the main characteristics of Arabic poetry is its special rhythmic\nstructure as opposed to prose. This structure is referred to as a meter.\nMeters, along with other poetic characteristics, are intensively studied in an\nArabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a\nverse is a lengthy and complicated process. It also requires technical\nknowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of\nprocessing. Developing systems for automatic identification of poem meters for\nrecited poems need large amounts of labelled data. In this study, we propose a\nstate-of-the-art framework to identify the poem meters of recited Arabic\npoetry, where we integrate two separate high-resource systems to perform the\nlow-resource task. To ensure generalization of our proposed architecture, we\npublish a benchmark for this task for future research."}
{"id": "2504.11777", "pdf": "https://arxiv.org/pdf/2504.11777", "abs": "https://arxiv.org/abs/2504.11777", "authors": ["Yongpei Ma", "Pengyu Wang", "Adam Dunn", "Usman Naseem", "Jinman Kim"], "title": "Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets", "categories": ["cs.CV", "cs.LG"], "comment": "The first two listed authors contributed equally to this work", "summary": "Medical Visual Question Answering (MVQA) systems can interpret medical images\nin response to natural language queries. However, linguistic variability in\nquestion phrasing often undermines the consistency of these systems. To address\nthis challenge, we propose a Semantically Equivalent Question Augmentation\n(SEQA) framework, which leverages large language models (LLMs) to generate\ndiverse yet semantically equivalent rephrasings of questions. Specifically,\nthis approach enriches linguistic diversity while preserving semantic meaning.\nWe further introduce an evaluation metric, Total Agreement Rate with\nSemantically Equivalent Input and Correct Answer (TAR-SC), which assesses a\nmodel's capability to generate consistent and correct responses to semantically\nequivalent linguistic variations. In addition, we also propose three other\ndiversity metrics - average number of QA items per image (ANQI), average number\nof questions per image with the same answer (ANQA), and average number of\nopen-ended questions per image with the same semantics (ANQS). Using the SEQA\nframework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD,\nand PathVQA. As a result, all three datasets achieved significant improvements\nby incorporating more semantically equivalent questions: ANQI increased by an\naverage of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate\nthree MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and\nfine-tuning settings on the enhanced datasets. Experimental results in MVQA\ndatasets show that fine-tuned models achieve an average accuracy improvement of\n19.35%, while our proposed TAR-SC metric shows an average improvement of 11.\n61%, indicating a substantial enhancement in model consistency."}
{"id": "2504.12177", "pdf": "https://arxiv.org/pdf/2504.12177", "abs": "https://arxiv.org/abs/2504.12177", "authors": ["Victor Manuel Hernandez Lopez", "Jaime E. Cuellar"], "title": "Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube", "categories": ["cs.CL", "cs.AI"], "comment": "in Spanish language", "summary": "This article analyzes the Hamas-Israel controversy through 253,925\nSpanish-language YouTube comments posted between October 2023 and January 2024,\nfollowing the October 7 attack that escalated the conflict. Adopting an\ninterdisciplinary approach, the study combines the analysis of controversies\nfrom Science and Technology Studies (STS) with advanced computational\nmethodologies, specifically Natural Language Processing (NLP) using the BERT\n(Bidirectional Encoder Representations from Transformers) model. Using this\napproach, the comments were automatically classified into seven categories,\nreflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli\npositions, among others. The results show a predominance of pro- Palestinian\ncomments, although pro-Israeli and anti-Palestinian comments received more\n\"likes.\" This study also applies the agenda-setting theory to demonstrate how\nmedia coverage significantly influences public perception, observing a notable\nshift in public opinion, transitioning from a pro- Palestinian stance to a more\ncritical position towards Israel. This work highlights the importance of\ncombining social science perspectives with technological tools in the analysis\nof controversies, presenting a methodological innovation by integrating\ncomputational analysis with critical social theories to address complex public\nopinion phenomena and media narratives."}
{"id": "2504.11779", "pdf": "https://arxiv.org/pdf/2504.11779", "abs": "https://arxiv.org/abs/2504.11779", "authors": ["Qishun Wang", "Zhengzheng Tu", "Chenglong Li", "Bo Jiang"], "title": "Multimodal Spatio-temporal Graph Learning for Alignment-free RGBT Video Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "RGB-Thermal Video Object Detection (RGBT VOD) can address the limitation of\ntraditional RGB-based VOD in challenging lighting conditions, making it more\npractical and effective in many applications.\n  However, similar to most RGBT fusion tasks, it still mainly relies on\nmanually aligned multimodal image pairs.\n  In this paper, we propose a novel Multimodal Spatio-temporal Graph learning\nNetwork (MSGNet) for alignment-free RGBT VOD problem by leveraging the robust\ngraph representation learning model.\n  Specifically, we first design an Adaptive Partitioning Layer (APL) to\nestimate the corresponding regions of the Thermal image within the RGB image\n(high-resolution), achieving a preliminary inexact alignment.\n  Then, we introduce the Spatial Sparse Graph Learning Module (S-SGLM) which\nemploys a sparse information passing mechanism on the estimated inexact\nalignment to achieve reliable information interaction between different\nmodalities.\n  Moreover, to fully exploit the temporal cues for RGBT VOD problem, we\nintroduce Hybrid Structured Temporal Modeling (HSTM), which involves a Temporal\nSparse Graph Learning Module (T-SGLM) and Temporal Star Block (TSB). T-SGLM\naims to filter out some redundant information between adjacent frames by\nemploying the sparse aggregation mechanism on the temporal graph. Meanwhile,\nTSB is dedicated to achieving the complementary learning of local spatial\nrelationships.\n  Extensive comparative experiments conducted on both the aligned dataset\nVT-VOD50 and the unaligned dataset UVT-VOD2024 demonstrate the effectiveness\nand superiority of our proposed method. Our project will be made available on\nour website for free public access."}
{"id": "2504.12180", "pdf": "https://arxiv.org/pdf/2504.12180", "abs": "https://arxiv.org/abs/2504.12180", "authors": ["Jaime E. Cuellar", "Oscar Moreno-Martinez", "Paula Sofia Torres-Rodriguez", "Jaime Andres Pavlich-Mariscal", "Andres Felipe Mican-Castiblanco", "Juan Guillermo Torres-Hurtado"], "title": "Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification", "categories": ["cs.CL", "cs.AI"], "comment": "in Spanish language", "summary": "One fundamental question for the social sciences today is: how much can we\ntrust highly complex predictive models like ChatGPT? This study tests the\nhypothesis that subtle changes in the structure of prompts do not produce\nsignificant variations in the classification results of sentiment polarity\nanalysis generated by the Large Language Model GPT-4o mini. Using a dataset of\n100.000 comments in Spanish on four Latin American presidents, the model\nclassified the comments as positive, negative, or neutral on 10 occasions,\nvarying the prompts slightly each time. The experimental methodology included\nexploratory and confirmatory analyses to identify significant discrepancies\namong classifications.\n  The results reveal that even minor modifications to prompts such as lexical,\nsyntactic, or modal changes, or even their lack of structure impact the\nclassifications. In certain cases, the model produced inconsistent responses,\nsuch as mixing categories, providing unsolicited explanations, or using\nlanguages other than Spanish. Statistical analysis using Chi-square tests\nconfirmed significant differences in most comparisons between prompts, except\nin one case where linguistic structures were highly similar.\n  These findings challenge the robustness and trust of Large Language Models\nfor classification tasks, highlighting their vulnerability to variations in\ninstructions. Moreover, it was evident that the lack of structured grammar in\nprompts increases the frequency of hallucinations. The discussion underscores\nthat trust in Large Language Models is based not only on technical performance\nbut also on the social and institutional relationships underpinning their use."}
{"id": "2504.11781", "pdf": "https://arxiv.org/pdf/2504.11781", "abs": "https://arxiv.org/abs/2504.11781", "authors": ["Guanchun Wang", "Xiangrong Zhang", "Yifei Zhang", "Zelin Peng", "Tianyang Zhang", "Xu Tang", "Licheng Jiao"], "title": "ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figures", "summary": "Unsupervised anomaly detection in hyperspectral images (HSI), aiming to\ndetect unknown targets from backgrounds, is challenging for earth surface\nmonitoring. However, current studies are hindered by steep computational costs\ndue to the high-dimensional property of HSI and dense sampling-based training\nparadigm, constraining their rapid deployment. Our key observation is that,\nduring training, not all samples within the same homogeneous area are\nindispensable, whereas ingenious sampling can provide a powerful substitute for\nreducing costs. Motivated by this, we propose an Asymmetrical Consensus State\nSpace Model (ACMamba) to significantly reduce computational costs without\ncompromising accuracy. Specifically, we design an asymmetrical anomaly\ndetection paradigm that utilizes region-level instances as an efficient\nalternative to dense pixel-level samples. In this paradigm, a low-cost\nMamba-based module is introduced to discover global contextual attributes of\nregions that are essential for HSI reconstruction. Additionally, we develop a\nconsensus learning strategy from the optimization perspective to simultaneously\nfacilitate background reconstruction and anomaly compression, further\nalleviating the negative impact of anomaly reconstruction. Theoretical analysis\nand extensive experiments across eight benchmarks verify the superiority of\nACMamba, demonstrating a faster speed and stronger performance over the\nstate-of-the-art."}
{"id": "2504.12185", "pdf": "https://arxiv.org/pdf/2504.12185", "abs": "https://arxiv.org/abs/2504.12185", "authors": ["Suyoung Bae", "Hyojun Kim", "YunSeok Choi", "Jee-Hyong Lee"], "title": "SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL 2025 main. 15 pages, 4 figures", "summary": "In various natural language processing (NLP) tasks, fine-tuning Pre-trained\nLanguage Models (PLMs) often leads to the issue of spurious correlations, which\nnegatively impacts performance, particularly when dealing with\nout-of-distribution data. To address this problem, we propose SALAD}(Structure\nAware and LLM-driven Augmented Data), a novel approach designed to enhance\nmodel robustness and generalization by generating structure-aware and\ncounterfactually augmented data for contrastive learning. Our method leverages\na tagging-based approach to generate structure-aware positive samples and\nutilizes large language models (LLMs) to generate counterfactual negative\nsamples with diverse sentence patterns. By applying contrastive learning, SALAD\nenables the model to focus on learning the structural relationships between key\nsentence components while minimizing reliance on spurious correlations. We\nvalidate our approach through experiments on three tasks: Sentiment\nClassification, Sexism Detection, and Natural Language Inference. The results\ndemonstrate that SALAD not only improves model robustness and performance\nacross different environments but also enhances generalization to\nout-of-distribution datasets and cross-domain scenarios."}
{"id": "2504.11786", "pdf": "https://arxiv.org/pdf/2504.11786", "abs": "https://arxiv.org/abs/2504.11786", "authors": ["Sang-Jun Park", "Keun-Soo Heo", "Dong-Hee Shin", "Young-Han Son", "Ji-Hye Oh", "Tae-Eui Kam"], "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation", "categories": ["cs.CV"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "The automatic generation of radiology reports has emerged as a promising\nsolution to reduce a time-consuming task and accurately capture critical\ndisease-relevant findings in X-ray images. Previous approaches for radiology\nreport generation have shown impressive performance. However, there remains\nsignificant potential to improve accuracy by ensuring that retrieved reports\ncontain disease-relevant findings similar to those in the X-ray images and by\nrefining generated reports. In this study, we propose a Disease-aware\nimage-text Alignment and self-correcting Re-alignment for Trustworthy radiology\nreport generation (DART) framework. In the first stage, we generate initial\nreports based on image-to-text retrieval with disease-matching, embedding both\nimages and texts in a shared embedding space through contrastive learning. This\napproach ensures the retrieval of reports with similar disease-relevant\nfindings that closely align with the input X-ray images. In the second stage,\nwe further enhance the initial reports by introducing a self-correction module\nthat re-aligns them with the X-ray images. Our proposed framework achieves\nstate-of-the-art results on two widely used benchmarks, surpassing previous\napproaches in both report generation and clinical efficacy metrics, thereby\nenhancing the trustworthiness of radiology reports."}
{"id": "2504.12187", "pdf": "https://arxiv.org/pdf/2504.12187", "abs": "https://arxiv.org/abs/2504.12187", "authors": ["Céline Budding"], "title": "What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for publication in Philosophy of Science", "summary": "It is sometimes assumed that Large Language Models (LLMs) know language, or\nfor example that they know that Paris is the capital of France. But what -- if\nanything -- do LLMs actually know? In this paper, I argue that LLMs can acquire\ntacit knowledge as defined by Martin Davies (1990). Whereas Davies himself\ndenies that neural networks can acquire tacit knowledge, I demonstrate that\ncertain architectural features of LLMs satisfy the constraints of semantic\ndescription, syntactic structure, and causal systematicity. Thus, tacit\nknowledge may serve as a conceptual framework for describing, explaining, and\nintervening on LLMs and their behavior."}
{"id": "2504.11798", "pdf": "https://arxiv.org/pdf/2504.11798", "abs": "https://arxiv.org/abs/2504.11798", "authors": ["Chao Yuan", "Tianyi Zhang", "Guanglin Niu"], "title": "Neighbor-Based Feature and Index Enhancement for Person Re-Identification", "categories": ["cs.CV"], "comment": "Comment: This paper has been accepted for publication in the 2025\n  IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW)", "summary": "Person re-identification (Re-ID) aims to match the same pedestrian in a large\ngallery with different cameras and views. Enhancing the robustness of the\nextracted feature representations is a main challenge in Re-ID. Existing\nmethods usually improve feature representation by improving model architecture,\nbut most methods ignore the potential contextual information, which limits the\neffectiveness of feature representation and retrieval performance. Neighborhood\ninformation, especially the potential information of multi-order neighborhoods,\ncan effectively enrich feature expression and improve retrieval accuracy, but\nthis has not been fully explored in existing research. Therefore, we propose a\nnovel model DMON-ARO that leverages latent neighborhood information to enhance\nboth feature representation and index performance. Our approach is built on two\ncomplementary modules: Dynamic Multi-Order Neighbor Modeling (DMON) and\nAsymmetric Relationship Optimization (ARO). The DMON module dynamically\naggregates multi-order neighbor relationships, allowing it to capture richer\ncontextual information and enhance feature representation through adaptive\nneighborhood modeling. Meanwhile, ARO refines the distance matrix by optimizing\nquery-to-gallery relationships, improving the index accuracy. Extensive\nexperiments on three benchmark datasets demonstrate that our approach achieves\nperformance improvements against baseline models, which illustrate the\neffectiveness of our model. Specifically, our model demonstrates improvements\nin Rank-1 accuracy and mAP. Moreover, this method can also be directly extended\nto other re-identification tasks."}
{"id": "2504.12216", "pdf": "https://arxiv.org/pdf/2504.12216", "abs": "https://arxiv.org/abs/2504.12216", "authors": ["Siyan Zhao", "Devaansh Gupta", "Qinqing Zheng", "Aditya Grover"], "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages, project page at https://dllm-reasoning.github.io/", "summary": "Recent large language models (LLMs) have demonstrated strong reasoning\ncapabilities that benefits from online reinforcement learning (RL). These\ncapabilities have primarily been demonstrated within the left-to-right\nautoregressive (AR) generation paradigm. In contrast, non-autoregressive\nparadigms based on diffusion generate text in a coarse-to-fine manner. Although\nrecent diffusion-based large language models (dLLMs) have achieved competitive\nlanguage modeling performance compared to their AR counterparts, it remains\nunclear if dLLMs can also leverage recent advances in LLM reasoning. To this\nend, we propose d1, a framework to adapt pre-trained masked dLLMs into\nreasoning models via a combination of supervised finetuning (SFT) and RL.\nSpecifically, we develop and extend techniques to improve reasoning in\npretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge\nand instill self-improvement behavior directly from existing datasets, and (b)\nwe introduce a novel critic-free, policy-gradient based RL algorithm called\ndiffu-GRPO. Through empirical studies, we investigate the performance of\ndifferent post-training recipes on multiple mathematical and logical reasoning\nbenchmarks. We find that d1 yields the best performance and significantly\nimproves performance of a state-of-the-art dLLM."}
{"id": "2504.11820", "pdf": "https://arxiv.org/pdf/2504.11820", "abs": "https://arxiv.org/abs/2504.11820", "authors": ["Delong Suzhang", "Meng Yang"], "title": "Real-World Depth Recovery via Structure Uncertainty Modeling and Inaccurate GT Depth Fitting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The low-quality structure in raw depth maps is prevalent in real-world RGB-D\ndatasets, which makes real-world depth recovery a critical task in recent\nyears. However, the lack of paired raw-ground truth (raw-GT) data in the real\nworld poses challenges for generalized depth recovery. Existing methods\ninsufficiently consider the diversity of structure misalignment in raw depth\nmaps, which leads to poor generalization in real-world depth recovery. Notably,\nrandom structure misalignments are not limited to raw depth data but also\naffect GT depth in real-world datasets. In the proposed method, we tackle the\ngeneralization problem from both input and output perspectives. For input, we\nenrich the diversity of structure misalignment in raw depth maps by designing a\nnew raw depth generation pipeline, which helps the network avoid overfitting to\na specific condition. Furthermore, a structure uncertainty module is designed\nto explicitly identify the misaligned structure for input raw depth maps to\nbetter generalize in unseen scenarios. Notably the well-trained depth\nfoundation model (DFM) can help the structure uncertainty module estimate the\nstructure uncertainty better. For output, a robust feature alignment module is\ndesigned to precisely align with the accurate structure of RGB images avoiding\nthe interference of inaccurate GT depth. Extensive experiments on multiple\ndatasets demonstrate the proposed method achieves competitive accuracy and\ngeneralization capabilities across various challenging raw depth maps."}
{"id": "2504.12285", "pdf": "https://arxiv.org/pdf/2504.12285", "abs": "https://arxiv.org/abs/2504.12285", "authors": ["Shuming Ma", "Hongyu Wang", "Shaohan Huang", "Xingxing Zhang", "Ying Hu", "Ting Song", "Yan Xia", "Furu Wei"], "title": "BitNet b1.58 2B4T Technical Report", "categories": ["cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large\nLanguage Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4\ntrillion tokens, the model has been rigorously evaluated across benchmarks\ncovering language understanding, mathematical reasoning, coding proficiency,\nand conversational ability. Our results demonstrate that BitNet b1.58 2B4T\nachieves performance on par with leading open-weight, full-precision LLMs of\nsimilar size, while offering significant advantages in computational\nefficiency, including substantially reduced memory footprint, energy\nconsumption, and decoding latency. To facilitate further research and adoption,\nthe model weights are released via Hugging Face along with open-source\ninference implementations for both GPU and CPU architectures."}
{"id": "2504.11838", "pdf": "https://arxiv.org/pdf/2504.11838", "abs": "https://arxiv.org/abs/2504.11838", "authors": ["Bianca Lamm", "Janis Keuper"], "title": "A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification", "categories": ["cs.CV"], "comment": null, "summary": "Despite the rapid evolution of learning and computer vision algorithms,\nFine-Grained Classification (FGC) still poses an open problem in many\npractically relevant applications. In the retail domain, for example, the\nidentification of fast changing and visually highly similar products and their\nproperties are key to automated price-monitoring and product recommendation.\nThis paper presents a novel Visual RAG pipeline that combines the Retrieval\nAugmented Generation (RAG) approach and Vision Language Models (VLMs) for\nfew-shot FGC. This Visual RAG pipeline extracts product and promotion data in\nadvertisement leaflets from various retailers and simultaneously predicts\nfine-grained product ids along with price and discount information. Compared to\nprevious approaches, the key characteristic of the Visual RAG pipeline is that\nit allows the prediction of novel products without re-training, simply by\nadding a few class samples to the RAG database. Comparing several VLM back-ends\nlike GPT-4o [23], GPT-4o-mini [24], and Gemini 2.0 Flash [10], our approach\nachieves 86.8% accuracy on a diverse dataset."}
{"id": "2504.11459", "pdf": "https://arxiv.org/pdf/2504.11459", "abs": "https://arxiv.org/abs/2504.11459", "authors": ["Peter Stockinger"], "title": "From Conceptual Data Models to Multimodal Representation", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "in French language", "summary": "1) Introduction and Conceptual Framework: This document explores the concept\nof information design by dividing it into two major practices: defining the\nmeaning of a corpus of textual data and its visual or multimodal\nrepresentation. It draws on expertise in enriching textual corpora,\nparticularly audiovisual ones, and transforming them into multiple narrative\nformats. The text highlights a crucial distinction between the semantic content\nof a domain and the modalities of its graphic expression, illustrating this\napproach with concepts rooted in structural semiotics and linguistics\ntraditions.\n  2) Modeling and Conceptual Design: The article emphasizes the importance of\nsemantic modeling, often achieved through conceptual networks or graphs. These\ntools enable the structuring of knowledge within a domain by accounting for\nrelationships between concepts, contexts of use, and specific objectives.\nStockinger also highlights the constraints and challenges involved in creating\ndynamic and adaptable models, integrating elements such as thesauri or\ninteroperable ontologies to facilitate the analysis and publication of complex\ncorpora.\n  3) Applications and Multimodal Visualization: The text concludes by examining\nthe practical application of these models in work environments like OKAPI,\ndeveloped to analyze, publish, and reuse audiovisual data. It also discusses\ninnovative approaches such as visual storytelling and document reengineering,\nwhich involve transforming existing content into new resources tailored to\nvarious contexts. These methods emphasize interoperability, flexibility, and\nthe intelligence of communication systems, paving the way for richer and more\ncollaborative use of digital data. The content of this document was presented\nduring the \"Semiotics of Information Design\" Day organized by Anne\nBeyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on\nJune 21, 2018, in Bordeaux."}
{"id": "2504.11845", "pdf": "https://arxiv.org/pdf/2504.11845", "abs": "https://arxiv.org/abs/2504.11845", "authors": ["Jie Zhu", "Bo Peng", "Zhe Zhang", "Bingzheng Liu", "Jianjun Lei"], "title": "Boosting Multi-View Stereo with Depth Foundation Model in the Absence of Real-World Labels", "categories": ["cs.CV"], "comment": null, "summary": "Learning-based Multi-View Stereo (MVS) methods have made remarkable progress\nin recent years. However, how to effectively train the network without using\nreal-world labels remains a challenging problem. In this paper, driven by the\nrecent advancements of vision foundation models, a novel method termed DFM-MVS,\nis proposed to leverage the depth foundation model to generate the effective\ndepth prior, so as to boost MVS in the absence of real-world labels.\nSpecifically, a depth prior-based pseudo-supervised training mechanism is\ndeveloped to simulate realistic stereo correspondences using the generated\ndepth prior, thereby constructing effective supervision for the MVS network.\nBesides, a depth prior-guided error correction strategy is presented to\nleverage the depth prior as guidance to mitigate the error propagation problem\ninherent in the widely-used coarse-to-fine network structure. Experimental\nresults on DTU and Tanks & Temples datasets demonstrate that the proposed\nDFM-MVS significantly outperforms existing MVS methods without using real-world\nlabels."}
{"id": "2504.11460", "pdf": "https://arxiv.org/pdf/2504.11460", "abs": "https://arxiv.org/abs/2504.11460", "authors": ["Tobias Hallmen", "Robin-Nico Kampa", "Fabian Deuser", "Norbert Oswald", "Elisabeth André"], "title": "Semantic Matters: Multimodal Features for Affective Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this study, we present our methodology for two tasks: the Behavioural\nAmbivalence/Hesitancy (BAH) Recognition Challenge and the Emotional Mimicry\nIntensity (EMI) Estimation Challenge, both conducted as part of the 8th\nWorkshop and Competition on Affective & Behavior Analysis in-the-wild. Building\non previous work, we utilize a Wav2Vec 2.0 model pre-trained on a large podcast\ndataset to extract various audio features, capturing both linguistic and\nparalinguistic information. Our approach incorporates a\nvalence-arousal-dominance (VAD) module derived from Wav2Vec 2.0, a BERT-like\nencoder, and a vision transformer (ViT) with predictions subsequently processed\nthrough a long short-term memory (LSTM) architecture for temporal modeling. In\nthis iteration, we integrate the textual and visual modality into our analysis,\nrecognizing that semantic content provides valuable contextual cues and\nunderscoring that the meaning of speech often conveys more critical insights\nthan its acoustic counterpart alone. Fusing in the vision modality helps in\nsome cases to interpret the textual modality more precisely. This combined\napproach yields significant performance improvements over baseline methods."}
{"id": "2504.11850", "pdf": "https://arxiv.org/pdf/2504.11850", "abs": "https://arxiv.org/abs/2504.11850", "authors": ["Finn Carter"], "title": "ACE: Attentional Concept Erasure in Diffusion Models", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Large text-to-image diffusion models have demonstrated remarkable image\nsynthesis capabilities, but their indiscriminate training on Internet-scale\ndata has led to learned concepts that enable harmful, copyrighted, or otherwise\nundesirable content generation. We address the task of concept erasure in\ndiffusion models, i.e., removing a specified concept from a pre-trained model\nsuch that prompting the concept (or related synonyms) no longer yields its\ndepiction, while preserving the model's ability to generate other content. We\npropose a novel method, Attentional Concept Erasure (ACE), that integrates a\nclosed-form attention manipulation with lightweight fine-tuning. Theoretically,\nwe formulate concept erasure as aligning the model's conditional distribution\non the target concept with a neutral distribution. Our approach identifies and\nnullifies concept-specific latent directions in the cross-attention modules via\na gated low-rank adaptation, followed by adversarially augmented fine-tuning to\nensure thorough erasure of the concept and its synonyms. Empirically, we\ndemonstrate on multiple benchmarks, including object classes, celebrity faces,\nexplicit content, and artistic styles, that ACE achieves state-of-the-art\nconcept removal efficacy and robustness. Compared to prior methods, ACE better\nbalances generality (erasing concept and related terms) and specificity\n(preserving unrelated content), scales to dozens of concepts, and is efficient,\nrequiring only a few seconds of adaptation per concept. We will release our\ncode to facilitate safer deployment of diffusion models."}
{"id": "2504.11492", "pdf": "https://arxiv.org/pdf/2504.11492", "abs": "https://arxiv.org/abs/2504.11492", "authors": ["Mayukh Bagchi"], "title": "Language and Knowledge Representation: A Stratified Approach", "categories": ["cs.DB", "cs.CL", "cs.DL"], "comment": "Doctor of Philosophy (Ph.D) in Information Engineering and Computer\n  Science, DISI, University of Trento, Italy", "summary": "The thesis proposes the problem of representation heterogeneity to emphasize\nthe fact that heterogeneity is an intrinsic property of any representation,\nwherein, different observers encode different representations of the same\ntarget reality in a stratified manner using different concepts, language and\nknowledge (as well as data). The thesis then advances a top-down solution\napproach to the above stratified problem of representation heterogeneity in\nterms of several solution components, namely: (i) a representation formalism\nstratified into concept level, language level, knowledge level and data level\nto accommodate representation heterogeneity, (ii) a top-down language\nrepresentation using Universal Knowledge Core (UKC), UKC namespaces and domain\nlanguages to tackle the conceptual and language level heterogeneity, (iii) a\ntop-down knowledge representation using the notions of language teleontology\nand knowledge teleontology to tackle the knowledge level heterogeneity, (iv)\nthe usage and further development of the existing LiveKnowledge catalog for\nenforcing iterative reuse and sharing of language and knowledge\nrepresentations, and, (v) the kTelos methodology integrating the solution\ncomponents above to iteratively generate the language and knowledge\nrepresentations absolving representation heterogeneity. The thesis also\nincludes proof-of-concepts of the language and knowledge representations\ndeveloped for two international research projects - DataScientia (data\ncatalogs) and JIDEP (materials modelling). Finally, the thesis concludes with\nfuture lines of research."}
{"id": "2504.11856", "pdf": "https://arxiv.org/pdf/2504.11856", "abs": "https://arxiv.org/abs/2504.11856", "authors": ["Zhenhuan Zhou", "Yuchen Zhang", "Along He", "Peng Wang", "Xueshuo Xie", "Tao Li"], "title": "Cross-Frequency Collaborative Training Network and Dataset for Semi-supervised First Molar Root Canal Segmentation", "categories": ["cs.CV"], "comment": "12 pages, Initial submission time 25 December 2024, Now Under Review", "summary": "Root canal (RC) treatment is a highly delicate and technically complex\nprocedure in clinical practice, heavily influenced by the clinicians'\nexperience and subjective judgment. Deep learning has made significant\nadvancements in the field of computer-aided diagnosis (CAD) because it can\nprovide more objective and accurate diagnostic results. However, its\napplication in RC treatment is still relatively rare, mainly due to the lack of\npublic datasets in this field. To address this issue, in this paper, we\nestablished a First Molar Root Canal segmentation dataset called FMRC-2025.\nAdditionally, to alleviate the workload of manual annotation for dentists and\nfully leverage the unlabeled data, we designed a Cross-Frequency Collaborative\ntraining semi-supervised learning (SSL) Network called CFC-Net. It consists of\ntwo components: (1) Cross-Frequency Collaborative Mean Teacher (CFC-MT), which\nintroduces two specialized students (SS) and one comprehensive teacher (CT) for\ncollaborative multi-frequency training. The CT and SS are trained on different\nfrequency components while fully integrating multi-frequency knowledge through\ncross and full frequency consistency supervisions. (2) Uncertainty-guided\nCross-Frequency Mix (UCF-Mix) mechanism enables the network to generate\nhigh-confidence pseudo-labels while learning to integrate multi-frequency\ninformation and maintaining the structural integrity of the targets. Extensive\nexperiments on FMRC-2025 and three public dental datasets demonstrate that\nCFC-MT is effective for RC segmentation and can also exhibit strong\ngeneralizability on other dental segmentation tasks, outperforming\nstate-of-the-art SSL medical image segmentation methods. Codes and dataset will\nbe released."}
{"id": "2504.11515", "pdf": "https://arxiv.org/pdf/2504.11515", "abs": "https://arxiv.org/abs/2504.11515", "authors": ["Kangsheng Wang", "Chengwei Ye", "Huanzhen Zhang", "Linuo Xu", "Shuyan Liu"], "title": "Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance."}
{"id": "2504.11858", "pdf": "https://arxiv.org/pdf/2504.11858", "abs": "https://arxiv.org/abs/2504.11858", "authors": ["Joël Mathys", "Andreas Plesner", "Jorel Elmiger", "Roger Wattenhofer"], "title": "Synthetic Data for Blood Vessel Network Extraction", "categories": ["cs.CV"], "comment": "Presented at SynthData Workshop at ICLR 2025", "summary": "Blood vessel networks in the brain play a crucial role in stroke research,\nwhere understanding their topology is essential for analyzing blood flow\ndynamics. However, extracting detailed topological vessel network information\nfrom microscopy data remains a significant challenge, mainly due to the\nscarcity of labeled training data and the need for high topological accuracy.\nThis work combines synthetic data generation with deep learning to\nautomatically extract vessel networks as graphs from volumetric microscopy\ndata. To combat data scarcity, we introduce a comprehensive pipeline for\ngenerating large-scale synthetic datasets that mirror the characteristics of\nreal vessel networks. Our three-stage approach progresses from abstract graph\ngeneration through vessel mask creation to realistic medical image synthesis,\nincorporating biological constraints and imaging artifacts at each stage. Using\nthis synthetic data, we develop a two-stage deep learning pipeline of 3D\nU-Net-based models for node detection and edge prediction. Fine-tuning on real\nmicroscopy data shows promising adaptation, improving edge prediction F1 scores\nfrom 0.496 to 0.626 by training on merely 5 manually labeled samples. These\nresults suggest that automated vessel network extraction is becoming\npractically feasible, opening new possibilities for large-scale vascular\nanalysis in stroke research."}
{"id": "2504.11524", "pdf": "https://arxiv.org/pdf/2504.11524", "abs": "https://arxiv.org/abs/2504.11524", "authors": ["Haokun Liu", "Sicong Huang", "Jingyu Hu", "Yangqiaoyu Zhou", "Chenhao Tan"], "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": "29 pages, 6 figures, website link:\n  https://chicagohai.github.io/HypoBench/", "summary": "There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery."}
{"id": "2504.11872", "pdf": "https://arxiv.org/pdf/2504.11872", "abs": "https://arxiv.org/abs/2504.11872", "authors": ["Daiqi Liu", "Fuxin Fan", "Andreas Maier"], "title": "A Category-Fragment Segmentation Framework for Pelvic Fracture Segmentation in X-ray Images", "categories": ["cs.CV"], "comment": "5 pages, 2 figures, 1 table", "summary": "Pelvic fractures, often caused by high-impact trauma, frequently require\nsurgical intervention. Imaging techniques such as CT and 2D X-ray imaging are\nused to transfer the surgical plan to the operating room through image\nregistration, enabling quick intraoperative adjustments. Specifically,\nsegmenting pelvic fractures from 2D X-ray imaging can assist in accurately\npositioning bone fragments and guiding the placement of screws or metal plates.\nIn this study, we propose a novel deep learning-based category and fragment\nsegmentation (CFS) framework for the automatic segmentation of pelvic bone\nfragments in 2D X-ray images. The framework consists of three consecutive\nsteps: category segmentation, fragment segmentation, and post-processing. Our\nbest model achieves an IoU of 0.91 for anatomical structures and 0.78 for\nfracture segmentation. Results demonstrate that the CFS framework is effective\nand accurate."}
{"id": "2504.11571", "pdf": "https://arxiv.org/pdf/2504.11571", "abs": "https://arxiv.org/abs/2504.11571", "authors": ["Dayeon Ki", "Tianyi Zhou", "Marine Carpuat", "Gang Wu", "Puneet Mathur", "Viswanathan Swaminathan"], "title": "GraphicBench: A Planning Benchmark for Graphic Design with Language Agents", "categories": ["cs.AI", "cs.CL"], "comment": "41 pages, 11 figures", "summary": "Large Language Model (LLM)-powered agents have unlocked new possibilities for\nautomating human tasks. While prior work has focused on well-defined tasks with\nspecified goals, the capabilities of agents in creative design tasks with\nopen-ended goals remain underexplored. We introduce GraphicBench, a new\nplanning benchmark for graphic design that covers 1,079 user queries and input\nimages across four design types. We further present GraphicTown, an LLM agent\nframework with three design experts and 46 actions (tools) to choose from for\nexecuting each step of the planned workflows in web environments. Experiments\nwith six LLMs demonstrate their ability to generate workflows that integrate\nboth explicit design constraints from user queries and implicit commonsense\nconstraints. However, these workflows often do not lead to successful execution\noutcomes, primarily due to challenges in: (1) reasoning about spatial\nrelationships, (2) coordinating global dependencies across experts, and (3)\nretrieving the most appropriate action per step. We envision GraphicBench as a\nchallenging yet valuable testbed for advancing LLM-agent planning and execution\nin creative design tasks."}
{"id": "2504.11879", "pdf": "https://arxiv.org/pdf/2504.11879", "abs": "https://arxiv.org/abs/2504.11879", "authors": ["Yushuai Sun", "Zikun Zhou", "Dongmei Jiang", "Yaowei Wang", "Jun Yu", "Guangming Lu", "Wenjie Pei"], "title": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Asymmetric retrieval is a typical scenario in real-world retrieval systems,\nwhere compatible models of varying capacities are deployed on platforms with\ndifferent resource configurations. Existing methods generally train pre-defined\nnetworks or subnetworks with capacities specifically designed for\npre-determined platforms, using compatible learning. Nevertheless, these\nmethods suffer from limited flexibility for multi-platform deployment. For\nexample, when introducing a new platform into the retrieval systems, developers\nhave to train an additional model at an appropriate capacity that is compatible\nwith existing models via backward-compatible learning. In this paper, we\npropose a Prunable Network with self-compatibility, which allows developers to\ngenerate compatible subnetworks at any desired capacity through post-training\npruning. Thus it allows the creation of a sparse subnetwork matching the\nresources of the new platform without additional training. Specifically, we\noptimize both the architecture and weight of subnetworks at different\ncapacities within a dense network in compatible learning. We also design a\nconflict-aware gradient integration scheme to handle the gradient conflicts\nbetween the dense network and subnetworks during compatible learning. Extensive\nexperiments on diverse benchmarks and visual backbones demonstrate the\neffectiveness of our method. Our code and model are available at\nhttps://github.com/Bunny-Black/PrunNet."}
{"id": "2504.11739", "pdf": "https://arxiv.org/pdf/2504.11739", "abs": "https://arxiv.org/abs/2504.11739", "authors": ["Bingjie Gao", "Xinyu Gao", "Xiaoxue Wu", "Yujie Zhou", "Yu Qiao", "Li Niu", "Xinyuan Chen", "Yaohui Wang"], "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation", "categories": ["cs.CV", "cs.CL"], "comment": "accepted by CVPR2025", "summary": "The evolution of Text-to-video (T2V) generative models, trained on\nlarge-scale datasets, has been marked by significant progress. However, the\nsensitivity of T2V generative models to input prompts highlights the critical\nrole of prompt design in influencing generative outcomes. Prior research has\npredominantly relied on Large Language Models (LLMs) to align user-provided\nprompts with the distribution of training prompts, albeit without tailored\nguidance encompassing prompt vocabulary and sentence structure nuances. To this\nend, we introduce \\textbf{RAPO}, a novel \\textbf{R}etrieval-\\textbf{A}ugmented\n\\textbf{P}rompt \\textbf{O}ptimization framework. In order to address potential\ninaccuracies and ambiguous details generated by LLM-generated prompts. RAPO\nrefines the naive prompts through dual optimization branches, selecting the\nsuperior prompt for T2V generation. The first branch augments user prompts with\ndiverse modifiers extracted from a learned relational graph, refining them to\nalign with the format of training prompts via a fine-tuned LLM. Conversely, the\nsecond branch rewrites the naive prompt using a pre-trained LLM following a\nwell-defined instruction set. Extensive experiments demonstrate that RAPO can\neffectively enhance both the static and dynamic dimensions of generated videos,\ndemonstrating the significance of prompt optimization for user-provided\nprompts. Project website:\n\\href{https://whynothaha.github.io/Prompt_optimizer/RAPO.html}{GitHub}."}
{"id": "2504.11893", "pdf": "https://arxiv.org/pdf/2504.11893", "abs": "https://arxiv.org/abs/2504.11893", "authors": ["Wei Sun", "Yanzhao Zhou", "Jianbin Jiao", "Yuan Li"], "title": "CAGS: Open-Vocabulary 3D Scene Understanding with Context-Aware Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary 3D scene understanding is crucial for applications requiring\nnatural language-driven spatial interpretation, such as robotics and augmented\nreality. While 3D Gaussian Splatting (3DGS) offers a powerful representation\nfor scene reconstruction, integrating it with open-vocabulary frameworks\nreveals a key challenge: cross-view granularity inconsistency. This issue,\nstemming from 2D segmentation methods like SAM, results in inconsistent object\nsegmentations across views (e.g., a \"coffee set\" segmented as a single entity\nin one view but as \"cup + coffee + spoon\" in another). Existing 3DGS-based\nmethods often rely on isolated per-Gaussian feature learning, neglecting the\nspatial context needed for cohesive object reasoning, leading to fragmented\nrepresentations. We propose Context-Aware Gaussian Splatting (CAGS), a novel\nframework that addresses this challenge by incorporating spatial context into\n3DGS. CAGS constructs local graphs to propagate contextual features across\nGaussians, reducing noise from inconsistent granularity, employs mask-centric\ncontrastive learning to smooth SAM-derived features across views, and leverages\na precomputation strategy to reduce computational cost by precomputing\nneighborhood relationships, enabling efficient training in large-scale scenes.\nBy integrating spatial context, CAGS significantly improves 3D instance\nsegmentation and reduces fragmentation errors on datasets like LERF-OVS and\nScanNet, enabling robust language-guided 3D scene understanding."}
{"id": "2504.11741", "pdf": "https://arxiv.org/pdf/2504.11741", "abs": "https://arxiv.org/abs/2504.11741", "authors": ["Yiyou Sun", "Georgia Zhou", "Hao Wang", "Dacheng Li", "Nouha Dziri", "Dawn Song"], "title": "Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent supervised fine-tuning (SFT) approaches have significantly improved\nlanguage models' performance on mathematical reasoning tasks, even when models\nare trained at a small scale. However, the specific capabilities enhanced\nthrough such fine-tuning remain poorly understood. In this paper, we conduct a\ndetailed analysis of model performance on the AIME24 dataset to understand how\nreasoning capabilities evolve. We discover a ladder-like structure in problem\ndifficulty, categorize questions into four tiers (Easy, Medium, Hard, and\nExtremely Hard (Exh)), and identify the specific requirements for advancing\nbetween tiers. We find that progression from Easy to Medium tier requires\nadopting an R1 reasoning style with minimal SFT (500-1K instances), while\nHard-level questions suffer from frequent model's errors at each step of the\nreasoning chain, with accuracy plateauing at around 65% despite logarithmic\nscaling. Exh-level questions present a fundamentally different challenge; they\nrequire unconventional problem-solving skills that current models uniformly\nstruggle with. Additional findings reveal that carefully curated small-scale\ndatasets offer limited advantage-scaling dataset size proves far more\neffective. Our analysis provides a clearer roadmap for advancing language model\ncapabilities in mathematical reasoning."}
{"id": "2504.11895", "pdf": "https://arxiv.org/pdf/2504.11895", "abs": "https://arxiv.org/abs/2504.11895", "authors": ["Qishan Wang", "Jia Guo", "Shuyong Gao", "Haofen Wang", "Li Xiong", "Junjie Hu", "Hanqi Guo", "Wenqiang Zhang"], "title": "Search is All You Need for Few-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot anomaly detection (FSAD) has emerged as a crucial yet challenging\ntask in industrial inspection, where normal distribution modeling must be\naccomplished with only a few normal images. While existing approaches typically\nemploy multi-modal foundation models combining language and vision modalities\nfor prompt-guided anomaly detection, these methods often demand sophisticated\nprompt engineering and extensive manual tuning. In this paper, we demonstrate\nthat a straightforward nearest-neighbor search framework can surpass\nstate-of-the-art performance in both single-class and multi-class FSAD\nscenarios. Our proposed method, VisionAD, consists of four simple yet essential\ncomponents: (1) scalable vision foundation models that extract universal and\ndiscriminative features; (2) dual augmentation strategies - support\naugmentation to enhance feature matching adaptability and query augmentation to\naddress the oversights of single-view prediction; (3) multi-layer feature\nintegration that captures both low-frequency global context and high-frequency\nlocal details with minimal computational overhead; and (4) a class-aware visual\nmemory bank enabling efficient one-for-all multi-class detection. Extensive\nevaluations across MVTec-AD, VisA, and Real-IAD benchmarks demonstrate\nVisionAD's exceptional performance. Using only 1 normal images as support, our\nmethod achieves remarkable image-level AUROC scores of 97.4%, 94.8%, and 70.8%\nrespectively, outperforming current state-of-the-art approaches by significant\nmargins (+1.6%, +3.2%, and +1.4%). The training-free nature and superior\nfew-shot capabilities of VisionAD make it particularly appealing for real-world\napplications where samples are scarce or expensive to obtain. Code is available\nat https://github.com/Qiqigeww/VisionAD."}
{"id": "2504.11844", "pdf": "https://arxiv.org/pdf/2504.11844", "abs": "https://arxiv.org/abs/2504.11844", "authors": ["Tom Everitt", "Cristina Garbacea", "Alexis Bellot", "Jonathan Richens", "Henry Papadatos", "Siméon Campos", "Rohin Shah"], "title": "Evaluating the Goal-Directedness of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "To what extent do LLMs use their capabilities towards their given goal? We\ntake this as a measure of their goal-directedness. We evaluate\ngoal-directedness on tasks that require information gathering, cognitive\neffort, and plan execution, where we use subtasks to infer each model's\nrelevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI,\nand Anthropic show that goal-directedness is relatively consistent across\ntasks, differs from task performance, and is only moderately sensitive to\nmotivational prompts. Notably, most models are not fully goal-directed. We hope\nour goal-directedness evaluations will enable better monitoring of LLM\nprogress, and enable more deliberate design choices of agentic properties in\nLLMs."}
{"id": "2504.11896", "pdf": "https://arxiv.org/pdf/2504.11896", "abs": "https://arxiv.org/abs/2504.11896", "authors": ["Xingxing Yang", "Jie Chen", "Zaifeng Yang"], "title": "Learning Physics-Informed Color-Aware Transforms for Low-Light Image Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICME 2025", "summary": "Image decomposition offers deep insights into the imaging factors of visual\ndata and significantly enhances various advanced computer vision tasks. In this\nwork, we introduce a novel approach to low-light image enhancement based on\ndecomposed physics-informed priors. Existing methods that directly map\nlow-light to normal-light images in the sRGB color space suffer from\ninconsistent color predictions and high sensitivity to spectral power\ndistribution (SPD) variations, resulting in unstable performance under diverse\nlighting conditions. To address these challenges, we introduce a\nPhysics-informed Color-aware Transform (PiCat), a learning-based framework that\nconverts low-light images from the sRGB color space into deep\nillumination-invariant descriptors via our proposed Color-aware Transform\n(CAT). This transformation enables robust handling of complex lighting and SPD\nvariations. Complementing this, we propose the Content-Noise Decomposition\nNetwork (CNDN), which refines the descriptor distributions to better align with\nwell-lit conditions by mitigating noise and other distortions, thereby\neffectively restoring content representations to low-light images. The CAT and\nthe CNDN collectively act as a physical prior, guiding the transformation\nprocess from low-light to normal-light domains. Our proposed PiCat framework\ndemonstrates superior performance compared to state-of-the-art methods across\nfive benchmark datasets."}
{"id": "2504.11889", "pdf": "https://arxiv.org/pdf/2504.11889", "abs": "https://arxiv.org/abs/2504.11889", "authors": ["Donghee Han", "Hwanjun Song", "Mun Yong Yi"], "title": "Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models."}
{"id": "2504.11914", "pdf": "https://arxiv.org/pdf/2504.11914", "abs": "https://arxiv.org/abs/2504.11914", "authors": ["Yuhao Chao", "Jie Liu", "Jie Tang", "Gangshan Wu"], "title": "AnomalyR1: A GRPO-based End-to-end MLLM for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Industrial Anomaly Detection (IAD) poses a formidable challenge due to the\nscarcity of defective samples, making it imperative to deploy models capable of\nrobust generalization to detect unseen anomalies effectively. Traditional\napproaches, often constrained by hand-crafted features or domain-specific\nexpert models, struggle to address this limitation, underscoring the need for a\nparadigm shift. We introduce AnomalyR1, a pioneering framework that leverages\nVLM-R1, a Multimodal Large Language Model (MLLM) renowned for its exceptional\ngeneralization and interpretability, to revolutionize IAD. By integrating MLLM\nwith Group Relative Policy Optimization (GRPO), enhanced by our novel Reasoned\nOutcome Alignment Metric (ROAM), AnomalyR1 achieves a fully end-to-end solution\nthat autonomously processes inputs of image and domain knowledge, reasons\nthrough analysis, and generates precise anomaly localizations and masks. Based\non the latest multimodal IAD benchmark, our compact 3-billion-parameter model\noutperforms existing methods, establishing state-of-the-art results. As MLLM\ncapabilities continue to advance, this study is the first to deliver an\nend-to-end VLM-based IAD solution that demonstrates the transformative\npotential of ROAM-enhanced GRPO, positioning our framework as a forward-looking\ncornerstone for next-generation intelligent anomaly detection systems in\nindustrial applications with limited defective data."}
{"id": "2504.11942", "pdf": "https://arxiv.org/pdf/2504.11942", "abs": "https://arxiv.org/abs/2504.11942", "authors": ["Nada Shahin", "Leila Ismail"], "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "comment": null, "summary": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure."}
{"id": "2504.11922", "pdf": "https://arxiv.org/pdf/2504.11922", "abs": "https://arxiv.org/abs/2504.11922", "authors": ["Lvpan Cai", "Haowei Wang", "Jiayi Ji", "YanShu ZhouMen", "Yiwei Ma", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach", "categories": ["cs.CV"], "comment": null, "summary": "The rise of AI-generated image editing tools has made localized forgeries\nincreasingly realistic, posing challenges for visual content integrity.\nAlthough recent efforts have explored localized AIGC detection, existing\ndatasets predominantly focus on object-level forgeries while overlooking\nbroader scene edits in regions such as sky or ground. To address these\nlimitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000\nlocally forged images with diverse scene-aware annotations, which are based on\nsemantic calibration to ensure high-quality samples. BR-Gen is constructed\nthrough a fully automated Perception-Creation-Evaluation pipeline to ensure\nsemantic coherence and visual realism. In addition, we further propose\n\\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that\nenhances the detection of localized forgeries by amplifying forgery-related\nfeatures across the entire image. NFA-ViT mines heterogeneous regions in\nimages, \\emph{i.e.}, potential edited areas, by noise fingerprints.\nSubsequently, attention mechanism is introduced to compel the interaction\nbetween normal and abnormal features, thereby propagating the generalization\ntraces throughout the entire image, allowing subtle forgeries to influence a\nbroader context and improving overall detection robustness. Extensive\nexperiments demonstrate that BR-Gen constructs entirely new scenarios that are\nnot covered by existing methods. Take a step further, NFA-ViT outperforms\nexisting methods on BR-Gen and generalizes well across current benchmarks. All\ndata and codes are available at https://github.com/clpbc/BR-Gen."}
{"id": "2504.12137", "pdf": "https://arxiv.org/pdf/2504.12137", "abs": "https://arxiv.org/abs/2504.12137", "authors": ["Laura Fieback", "Nishilkumar Balar", "Jakob Spiegelberg", "Hanno Gottschalk"], "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time."}
{"id": "2504.11930", "pdf": "https://arxiv.org/pdf/2504.11930", "abs": "https://arxiv.org/abs/2504.11930", "authors": ["Hairui Ren", "Fan Tang", "He Zhao", "Zixuan Wang", "Dandan Guo", "Yi Chang"], "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning", "categories": ["cs.CV"], "comment": null, "summary": "Fine-tuning vision-language models (VLMs) with large amounts of unlabeled\ndata has recently garnered significant interest. However, a key challenge\nremains the lack of high-quality pseudo-labeled data. Current pseudo-labeling\nstrategies often struggle with mismatches between semantic and visual\ninformation, leading to sub-optimal performance of unsupervised prompt learning\n(UPL) methods. In this paper, we introduce a simple yet effective approach\ncalled \\textbf{A}ugmenting D\\textbf{i}scriminative \\textbf{R}ichness via\nDiffusions (AiR), toward learning a richer discriminating way to represent the\nclass comprehensively and thus facilitate classification. Specifically, our\napproach includes a pseudo-label generation module that leverages high-fidelity\nsynthetic samples to create an auxiliary classifier, which captures richer\nvisual variation, bridging text-image-pair classification to a more robust\nimage-image-pair classification. Additionally, we exploit the diversity of\ndiffusion-based synthetic samples to enhance prompt learning, providing greater\ninformation for semantic-visual alignment. Extensive experiments on five public\nbenchmarks, including RESISC45 and Flowers102, and across three learning\nparadigms-UL, SSL, and TRZSL-demonstrate that AiR achieves substantial and\nconsistent performance improvements over state-of-the-art unsupervised prompt\nlearning methods."}
{"id": "2504.12229", "pdf": "https://arxiv.org/pdf/2504.12229", "abs": "https://arxiv.org/abs/2504.12229", "authors": ["David Khachaturov", "Robert Mullins", "Ilia Shumailov", "Sumanth Dathathri"], "title": "Watermarking Needs Input Repetition Masking", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) raised concerns over\npotential misuse, such as for spreading misinformation. In response two counter\nmeasures emerged: machine learning-based detectors that predict if text is\nsynthetic, and LLM watermarking, which subtly marks generated text for\nidentification and attribution. Meanwhile, humans are known to adjust language\nto their conversational partners both syntactically and lexically. By\nimplication, it is possible that humans or unwatermarked LLMs could\nunintentionally mimic properties of LLM generated text, making counter measures\nunreliable. In this work we investigate the extent to which such conversational\nadaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that\nboth humans and LLMs end up mimicking, including the watermarking signal even\nin seemingly improbable settings. This challenges current academic assumptions\nand suggests that for long-term watermarking to be reliable, the likelihood of\nfalse positives needs to be significantly lower, while longer word sequences\nshould be used for seeding watermarking mechanisms."}
{"id": "2504.11946", "pdf": "https://arxiv.org/pdf/2504.11946", "abs": "https://arxiv.org/abs/2504.11946", "authors": ["Haoyang Wang", "Liming Liu", "Peiheng Wang", "Junlin Hao", "Jiangkai Wu", "Xinggong Zhang"], "title": "R-Meshfusion: Reinforcement Learning Powered Sparse-View Mesh Reconstruction with Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Mesh reconstruction from multi-view images is a fundamental problem in\ncomputer vision, but its performance degrades significantly under sparse-view\nconditions, especially in unseen regions where no ground-truth observations are\navailable. While recent advances in diffusion models have demonstrated strong\ncapabilities in synthesizing novel views from limited inputs, their outputs\noften suffer from visual artifacts and lack 3D consistency, posing challenges\nfor reliable mesh optimization. In this paper, we propose a novel framework\nthat leverages diffusion models to enhance sparse-view mesh reconstruction in a\nprincipled and reliable manner. To address the instability of diffusion\noutputs, we propose a Consensus Diffusion Module that filters unreliable\ngenerations via interquartile range (IQR) analysis and performs variance-aware\nimage fusion to produce robust pseudo-supervision. Building on this, we design\nan online reinforcement learning strategy based on the Upper Confidence Bound\n(UCB) to adaptively select the most informative viewpoints for enhancement,\nguided by diffusion loss. Finally, the fused images are used to jointly\nsupervise a NeRF-based model alongside sparse-view ground truth, ensuring\nconsistency across both geometry and appearance. Extensive experiments\ndemonstrate that our method achieves significant improvements in both geometric\nquality and rendering quality."}
{"id": "2504.12254", "pdf": "https://arxiv.org/pdf/2504.12254", "abs": "https://arxiv.org/abs/2504.12254", "authors": ["Mahmoud Salhab", "Marwan Elghitany", "Shameed Sait", "Syed Sibghat Ullah", "Mohammad Abusheikh", "Hasan Abusheikh"], "title": "Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach attains state-of-the-art (SOTA)\nperformance, exceeding all previous efforts in the field of Arabic ASR on the\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings."}
{"id": "2504.11949", "pdf": "https://arxiv.org/pdf/2504.11949", "abs": "https://arxiv.org/abs/2504.11949", "authors": ["Jie Wang", "Chen Ye Gan", "Caoqi Wei", "Jiangtao Wen", "Yuxing Han"], "title": "Flow Intelligence: Robust Feature Matching via Temporal Signature Correlation", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching across video streams remains a cornerstone challenge in\ncomputer vision. Increasingly, robust multimodal matching has garnered interest\nin robotics, surveillance, remote sensing, and medical imaging. While\ntraditional rely on detecting and matching spatial features, they break down\nwhen faced with noisy, misaligned, or cross-modal data. Recent deep learning\nmethods have improved robustness through learned representations, but remain\nconstrained by their dependence on extensive training data and computational\ndemands. We present Flow Intelligence, a paradigm-shifting approach that moves\nbeyond spatial features by focusing on temporal motion patterns exclusively.\nInstead of detecting traditional keypoints, our method extracts motion\nsignatures from pixel blocks across consecutive frames and extract temporal\nmotion signatures between videos. These motion-based descriptors achieve\nnatural invariance to translation, rotation, and scale variations while\nremaining robust across different imaging modalities. This novel approach also\nrequires no pretraining data, eliminates the need for spatial feature\ndetection, enables cross-modal matching using only temporal motion, and it\noutperforms existing methods in challenging scenarios where traditional\napproaches fail. By leveraging motion rather than appearance, Flow Intelligence\nenables robust, real-time video feature matching in diverse environments."}
{"id": "2504.12279", "pdf": "https://arxiv.org/pdf/2504.12279", "abs": "https://arxiv.org/abs/2504.12279", "authors": ["Mikhail Osipov"], "title": "Dysarthria Normalization via Local Lie Group Transformations for Robust ASR", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": "Preprint. 11 pages, 3 figures, 2 tables, 8 appendices. Code and data\n  available upon request", "summary": "We present a geometry-driven method for normalizing dysarthric speech using\nlocal Lie group transformations of spectrograms. Time, frequency, and amplitude\ndistortions are modeled as smooth, invertible deformations, parameterized by\nscalar fields and applied via exponential maps. A neural network is trained to\ninfer these fields from synthetic distortions of typical speech-without using\nany pathological data. At test time, the model applies an approximate inverse\nto real dysarthric inputs. Despite zero-shot generalization, we observe\nsubstantial ASR gains, including up to 16 percentage points WER reduction on\nchallenging TORGO samples, with no degradation on clean speech. This work\nintroduces a principled, interpretable approach for robust speech recognition\nunder motor speech disorders"}
{"id": "2504.11966", "pdf": "https://arxiv.org/pdf/2504.11966", "abs": "https://arxiv.org/abs/2504.11966", "authors": ["Linjuan Fan", "Di Wen", "Kunyu Peng", "Kailun Yang", "Jiaming Zhang", "Ruiping Liu", "Yufan Chen", "Junwei Zheng", "Jiamin Wu", "Xudong Han", "Rainer Stiefelhagen"], "title": "Exploring Video-Based Driver Activity Recognition under Noisy Labels", "categories": ["cs.CV", "cs.LG", "cs.RO", "eess.IV"], "comment": "The source code is available at\n  https://github.com/ilonafan/DAR-noisy-labels", "summary": "As an open research topic in the field of deep learning, learning with noisy\nlabels has attracted much attention and grown rapidly over the past ten years.\nLearning with label noise is crucial for driver distraction behavior\nrecognition, as real-world video data often contains mislabeled samples,\nimpacting model reliability and performance. However, label noise learning is\nbarely explored in the driver activity recognition field. In this paper, we\npropose the first label noise learning approach for the driver activity\nrecognition task. Based on the cluster assumption, we initially enable the\nmodel to learn clustering-friendly low-dimensional representations from given\nvideos and assign the resultant embeddings into clusters. We subsequently\nperform co-refinement within each cluster to smooth the classifier outputs.\nFurthermore, we propose a flexible sample selection strategy that combines two\nselection criteria without relying on any hyperparameters to filter clean\nsamples from the training dataset. We also incorporate a self-adaptive\nparameter into the sample selection process to enforce balancing across\nclasses. A comprehensive variety of experiments on the public Drive&Act dataset\nfor all granularity levels demonstrates the superior performance of our method\nin comparison with other label-denoising methods derived from the image\nclassification field. The source code is available at\nhttps://github.com/ilonafan/DAR-noisy-labels."}
{"id": "2504.11967", "pdf": "https://arxiv.org/pdf/2504.11967", "abs": "https://arxiv.org/abs/2504.11967", "authors": ["Yifei Dong", "Fengyi Wu", "Sanjian Zhang", "Guangyu Chen", "Yuzhi Hu", "Masumi Yano", "Jingdong Sun", "Siyu Huang", "Feng Liu", "Qi Dai", "Zhi-Qi Cheng"], "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Accepted at CVPR Workshop Anti-UAV 2025. 15 pages", "summary": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure\ninspection, surveillance, and related tasks, yet they also introduce critical\nsecurity challenges. This survey provides a wide-ranging examination of the\nanti-UAV domain, centering on three core objectives-classification, detection,\nand tracking-while detailing emerging methodologies such as diffusion-based\ndata synthesis, multi-modal fusion, vision-language modeling, self-supervised\nlearning, and reinforcement learning. We systematically evaluate\nstate-of-the-art solutions across both single-modality and multi-sensor\npipelines (spanning RGB, infrared, audio, radar, and RF) and discuss\nlarge-scale as well as adversarially oriented benchmarks. Our analysis reveals\npersistent gaps in real-time performance, stealth detection, and swarm-based\nscenarios, underscoring pressing needs for robust, adaptive anti-UAV systems.\nBy highlighting open research directions, we aim to foster innovation and guide\nthe development of next-generation defense strategies in an era marked by the\nextensive use of UAVs."}
{"id": "2504.11995", "pdf": "https://arxiv.org/pdf/2504.11995", "abs": "https://arxiv.org/abs/2504.11995", "authors": ["Rahima Khanam", "Muhammad Hussain"], "title": "A Review of YOLOv12: Attention-Based Enhancements vs. Previous Versions", "categories": ["cs.CV"], "comment": null, "summary": "The YOLO (You Only Look Once) series has been a leading framework in\nreal-time object detection, consistently improving the balance between speed\nand accuracy. However, integrating attention mechanisms into YOLO has been\nchallenging due to their high computational overhead. YOLOv12 introduces a\nnovel approach that successfully incorporates attention-based enhancements\nwhile preserving real-time performance. This paper provides a comprehensive\nreview of YOLOv12's architectural innovations, including Area Attention for\ncomputationally efficient self-attention, Residual Efficient Layer Aggregation\nNetworks for improved feature aggregation, and FlashAttention for optimized\nmemory access. Additionally, we benchmark YOLOv12 against prior YOLO versions\nand competing object detectors, analyzing its improvements in accuracy,\ninference speed, and computational efficiency. Through this analysis, we\ndemonstrate how YOLOv12 advances real-time object detection by refining the\nlatency-accuracy trade-off and optimizing computational resources."}
{"id": "2504.11999", "pdf": "https://arxiv.org/pdf/2504.11999", "abs": "https://arxiv.org/abs/2504.11999", "authors": ["Mengyu Wang", "Hanbo Bi", "Yingchao Feng", "Linlin Xin", "Shuo Gong", "Tianqi Wang", "Zhiyuan Yan", "Peijin Wang", "Wenhui Diao", "Xian Sun"], "title": "A Complex-valued SAR Foundation Model Based on Physically Inspired Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Vision foundation models in remote sensing have been extensively studied due\nto their superior generalization on various downstream tasks. Synthetic\nAperture Radar (SAR) offers all-day, all-weather imaging capabilities,\nproviding significant advantages for Earth observation. However, establishing a\nfoundation model for SAR image interpretation inevitably encounters the\nchallenges of insufficient information utilization and poor interpretability.\nIn this paper, we propose a remote sensing foundation model based on\ncomplex-valued SAR data, which simulates the polarimetric decomposition process\nfor pre-training, i.e., characterizing pixel scattering intensity as a weighted\ncombination of scattering bases and scattering coefficients, thereby endowing\nthe foundation model with physical interpretability. Specifically, we construct\na series of scattering queries, each representing an independent and meaningful\nscattering basis, which interact with SAR features in the scattering query\ndecoder and output the corresponding scattering coefficient. To guide the\npre-training process, polarimetric decomposition loss and power\nself-supervision loss are constructed. The former aligns the predicted\ncoefficients with Yamaguchi coefficients, while the latter reconstructs power\nfrom the predicted coefficients and compares it to the input image's power. The\nperformance of our foundation model is validated on six typical downstream\ntasks, achieving state-of-the-art results. Notably, the foundation model can\nextract stable feature representations and exhibits strong generalization, even\nin data-scarce conditions."}
{"id": "2504.12018", "pdf": "https://arxiv.org/pdf/2504.12018", "abs": "https://arxiv.org/abs/2504.12018", "authors": ["Xinli Yue", "JianHui Sun", "Junda Lu", "Liangchao Yao", "Fan Xia", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Yuetang Deng"], "title": "Instruction-augmented Multimodal Alignment for Image-Text and Element Matching", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 Workshop", "summary": "With the rapid advancement of text-to-image (T2I) generation models,\nassessing the semantic alignment between generated images and text descriptions\nhas become a significant research challenge. Current methods, including those\nbased on Visual Question Answering (VQA), still struggle with fine-grained\nassessments and precise quantification of image-text alignment. This paper\npresents an improved evaluation method named Instruction-augmented Multimodal\nAlignment for Image-Text and Element Matching (iMatch), which evaluates\nimage-text semantic alignment by fine-tuning multimodal large language models.\nWe introduce four innovative augmentation strategies: First, the QAlign\nstrategy creates a precise probabilistic mapping to convert discrete scores\nfrom multimodal large language models into continuous matching scores. Second,\na validation set augmentation strategy uses pseudo-labels from model\npredictions to expand training data, boosting the model's generalization\nperformance. Third, an element augmentation strategy integrates element\ncategory labels to refine the model's understanding of image-text matching.\nFourth, an image augmentation strategy employs techniques like random lighting\nto increase the model's robustness. Additionally, we propose prompt type\naugmentation and score perturbation strategies to further enhance the accuracy\nof element assessments. Our experimental results show that the iMatch method\nsignificantly surpasses existing methods, confirming its effectiveness and\npractical value. Furthermore, our iMatch won first place in the CVPR NTIRE 2025\nText to Image Generation Model Quality Assessment - Track 1 Image-Text\nAlignment."}
{"id": "2504.12020", "pdf": "https://arxiv.org/pdf/2504.12020", "abs": "https://arxiv.org/abs/2504.12020", "authors": ["Shiwei Gan", "Yafeng Yin", "Zhiwei Jiang", "Hongkai Wen", "Lei Xie", "Sanglu Lu"], "title": "MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (T-PAMI). This is a regular paper\n  submission", "summary": "Recent advances in sign language research have benefited from CNN-based\nbackbones, which are primarily transferred from traditional computer vision\ntasks (\\eg object identification, image recognition). However, these CNN-based\nbackbones usually excel at extracting features like contours and texture, but\nmay struggle with capturing sign-related features. In fact, sign language tasks\nrequire focusing on sign-related regions, including the collaboration between\ndifferent regions (\\eg left hand region and right hand region) and the\neffective content in a single region. To capture such region-related features,\nwe introduce MixSignGraph, which represents sign sequences as a group of mixed\ngraphs and designs the following three graph modules for feature extraction,\n\\ie Local Sign Graph (LSG) module, Temporal Sign Graph (TSG) module and\nHierarchical Sign Graph (HSG) module. Specifically, the LSG module learns the\ncorrelation of intra-frame cross-region features within one frame, \\ie focusing\non spatial features. The TSG module tracks the interaction of inter-frame\ncross-region features among adjacent frames, \\ie focusing on temporal features.\nThe HSG module aggregates the same-region features from different-granularity\nfeature maps of a frame, \\ie focusing on hierarchical features. In addition, to\nfurther improve the performance of sign language tasks without gloss\nannotations, we propose a simple yet counter-intuitive Text-driven CTC\nPre-training (TCP) method, which generates pseudo gloss labels from text labels\nfor model pre-training. Extensive experiments conducted on current five public\nsign language datasets demonstrate the superior performance of the proposed\nmodel. Notably, our model surpasses the SOTA models on multiple sign language\ntasks across several datasets, without relying on any additional cues."}
{"id": "2504.12021", "pdf": "https://arxiv.org/pdf/2504.12021", "abs": "https://arxiv.org/abs/2504.12021", "authors": ["Mohamad Dalal", "Artur Xarles", "Anthony Cioppa", "Silvio Giancola", "Marc Van Droogenbroeck", "Bernard Ghanem", "Albert Clapés", "Sergio Escalera", "Thomas B. Moeslund"], "title": "Action Anticipation from SoccerNet Football Video Broadcasts", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "15 pages, 14 figures. To be published in the CVSports CVPR workshop", "summary": "Artificial intelligence has revolutionized the way we analyze sports videos,\nwhether to understand the actions of games in long untrimmed videos or to\nanticipate the player's motion in future frames. Despite these efforts, little\nattention has been given to anticipating game actions before they occur. In\nthis work, we introduce the task of action anticipation for football broadcast\nvideos, which consists in predicting future actions in unobserved future\nframes, within a five- or ten-second anticipation window. To benchmark this\ntask, we release a new dataset, namely the SoccerNet Ball Action Anticipation\ndataset, based on SoccerNet Ball Action Spotting. Additionally, we propose a\nFootball Action ANticipation TRAnsformer (FAANTRA), a baseline method that\nadapts FUTR, a state-of-the-art action anticipation model, to predict\nball-related actions. To evaluate action anticipation, we introduce new\nmetrics, including mAP@$\\delta$, which evaluates the temporal precision of\npredicted future actions, as well as mAP@$\\infty$, which evaluates their\noccurrence within the anticipation window. We also conduct extensive ablation\nstudies to examine the impact of various task settings, input configurations,\nand model architectures. Experimental results highlight both the feasibility\nand challenges of action anticipation in football videos, providing valuable\ninsights into the design of predictive models for sports analytics. By\nforecasting actions before they unfold, our work will enable applications in\nautomated broadcasting, tactical analysis, and player decision-making. Our\ndataset and code are publicly available at\nhttps://github.com/MohamadDalal/FAANTRA."}
{"id": "2504.12027", "pdf": "https://arxiv.org/pdf/2504.12027", "abs": "https://arxiv.org/abs/2504.12027", "authors": ["Bingyan Liu", "Chengyu Wang", "Tongtong Su", "Huan Ten", "Jun Huang", "Kailing Guo", "Kui Jia"], "title": "Understanding Attention Mechanism in Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video (T2V) synthesis models, such as OpenAI's Sora, have garnered\nsignificant attention due to their ability to generate high-quality videos from\na text prompt. In diffusion-based T2V models, the attention mechanism is a\ncritical component. However, it remains unclear what intermediate features are\nlearned and how attention blocks in T2V models affect various aspects of video\nsynthesis, such as image quality and temporal consistency. In this paper, we\nconduct an in-depth perturbation analysis of the spatial and temporal attention\nblocks of T2V models using an information-theoretic approach. Our results\nindicate that temporal and spatial attention maps affect not only the timing\nand layout of the videos but also the complexity of spatiotemporal elements and\nthe aesthetic quality of the synthesized videos. Notably, high-entropy\nattention maps are often key elements linked to superior video quality, whereas\nlow-entropy attention maps are associated with the video's intra-frame\nstructure. Based on our findings, we propose two novel methods to enhance video\nquality and enable text-guided video editing. These methods rely entirely on\nlightweight manipulation of the attention matrices in T2V models. The efficacy\nand effectiveness of our methods are further validated through experimental\nevaluation across multiple datasets."}
{"id": "2504.12029", "pdf": "https://arxiv.org/pdf/2504.12029", "abs": "https://arxiv.org/abs/2504.12029", "authors": ["Bingjie Gao", "Bo Zhang", "Li Niu"], "title": "Object Placement for Anything", "categories": ["cs.CV"], "comment": "accepted by ICME 2025", "summary": "Object placement aims to determine the appropriate placement (\\emph{e.g.},\nlocation and size) of a foreground object when placing it on the background\nimage. Most previous works are limited by small-scale labeled dataset, which\nhinders the real-world application of object placement. In this work, we devise\na semi-supervised framework which can exploit large-scale unlabeled dataset to\npromote the generalization ability of discriminative object placement models.\nThe discriminative models predict the rationality label for each foreground\nplacement given a foreground-background pair. To better leverage the labeled\ndata, under the semi-supervised framework, we further propose to transfer the\nknowledge of rationality variation, \\emph{i.e.}, whether the change of\nforeground placement would result in the change of rationality label, from\nlabeled data to unlabeled data. Extensive experiments demonstrate that our\nframework can effectively enhance the generalization ability of discriminative\nobject placement models."}
{"id": "2504.12039", "pdf": "https://arxiv.org/pdf/2504.12039", "abs": "https://arxiv.org/abs/2504.12039", "authors": ["Yizhuo Wu", "Francesco Fioranelli", "Chang Gao"], "title": "RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Radar-based HAR has emerged as a promising alternative to conventional\nmonitoring approaches, such as wearable devices and camera-based systems, due\nto its unique privacy preservation and robustness advantages. However, existing\nsolutions based on convolutional and recurrent neural networks, although\neffective, are computationally demanding during deployment. This limits their\napplicability in scenarios with constrained resources or those requiring\nmultiple sensors. Advanced architectures, such as ViT and SSM architectures,\noffer improved modeling capabilities and have made efforts toward lightweight\ndesigns. However, their computational complexity remains relatively high. To\nleverage the strengths of transformer architectures while simultaneously\nenhancing accuracy and reducing computational complexity, this paper introduces\nRadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM\nspecifically tailored for radar-based HAR. Across three diverse datasets,\nRadMamba matches the top-performing previous model's 99.8% classification\naccuracy on Dataset DIAT with only 1/400 of its parameters and equals the\nleading models' 92.0% accuracy on Dataset CI4R with merely 1/10 of their\nparameters. In scenarios with continuous sequences of actions evaluated on\nDataset UoG2020, RadMamba surpasses other models with significantly higher\nparameter counts by at least 3%, achieving this with only 6.7k parameters. Our\ncode is available at: https://github.com/lab-emi/AIRHAR."}
{"id": "2504.12045", "pdf": "https://arxiv.org/pdf/2504.12045", "abs": "https://arxiv.org/abs/2504.12045", "authors": ["Jonas Myhre Schiøtt", "Viktor Sebastian Petersen", "Dimitrios P. Papadopoulos"], "title": "pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild", "categories": ["cs.CV", "cs.LG", "I.2.1; I.4.6"], "comment": "15 pages, 7 figures, to be published in SCIA 2025", "summary": "Computer vision models have seen increased usage in sports, and reinforcement\nlearning (RL) is famous for beating humans in strategic games such as Chess and\nGo. In this paper, we are interested in building upon these advances and\nexamining the game of classic 8-ball pool. We introduce pix2pockets, a\nfoundation for an RL-assisted pool coach. Given a single image of a pool table,\nwe first aim to detect the table and the balls and then propose the optimal\nshot suggestion. For the first task, we build a dataset with 195 diverse images\nwhere we manually annotate all balls and table dots, leading to 5748 object\nsegmentation masks. For the second task, we build a standardized RL environment\nthat allows easy development and benchmarking of any RL algorithm. Our object\ndetection model yields an AP50 of 91.2 while our ball location pipeline obtains\nan error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set\na baseline for the shot suggestion task and we show that all of them fail to\npocket all balls without making a foul move. We also present a simple baseline\nthat achieves a per-shot success rate of 94.7% and clears a full game in a\nsingle turn 30% of the time."}
{"id": "2504.12048", "pdf": "https://arxiv.org/pdf/2504.12048", "abs": "https://arxiv.org/abs/2504.12048", "authors": ["Zirui Pan", "Xin Wang", "Yipeng Zhang", "Hong Chen", "Kwan Man Cheng", "Yaofei Wu", "Wenwu Zhu"], "title": "Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM", "categories": ["cs.CV"], "comment": "AAAI 2025 Poster", "summary": "Text-to-Video generation, which utilizes the provided text prompt to generate\nhigh-quality videos, has drawn increasing attention and achieved great success\ndue to the development of diffusion models recently. Existing methods mainly\nrely on a pre-trained text encoder to capture the semantic information and\nperform cross attention with the encoded text prompt to guide the generation of\nvideo. However, when it comes to complex prompts that contain dynamic scenes\nand multiple camera-view transformations, these methods can not decompose the\noverall information into separate scenes, as well as fail to smoothly change\nscenes based on the corresponding camera-views. To solve these problems, we\npropose a novel method, i.e., Modular-Cam. Specifically, to better understand a\ngiven complex prompt, we utilize a large language model to analyze user\ninstructions and decouple them into multiple scenes together with transition\nactions. To generate a video containing dynamic scenes that match the given\ncamera-views, we incorporate the widely-used temporal transformer into the\ndiffusion model to ensure continuity within a single scene and propose\nCamOperator, a modular network based module that well controls the camera\nmovements. Moreover, we propose AdaControlNet, which utilizes ControlNet to\nensure consistency across scenes and adaptively adjusts the color tone of the\ngenerated video. Extensive qualitative and quantitative experiments prove our\nproposed Modular-Cam's strong capability of generating multi-scene videos\ntogether with its ability to achieve fine-grained control of camera movements.\nGenerated results are available at https://modular-cam.github.io."}
{"id": "2504.12078", "pdf": "https://arxiv.org/pdf/2504.12078", "abs": "https://arxiv.org/abs/2504.12078", "authors": ["Trina De", "Adrian Urbanski", "Artur Yakimovich"], "title": "Single-shot Star-convex Polygon-based Instance Segmentation for Spatially-correlated Biomedical Objects", "categories": ["cs.CV", "q-bio.QM", "J.3; I.4"], "comment": "12 pages, 8 figures", "summary": "Biomedical images often contain objects known to be spatially correlated or\nnested due to their inherent properties, leading to semantic relations.\nExamples include cell nuclei being nested within eukaryotic cells and colonies\ngrowing exclusively within their culture dishes. While these semantic relations\nbear key importance, detection tasks are often formulated independently,\nrequiring multi-shot analysis pipelines. Importantly, spatial correlation could\nconstitute a fundamental prior facilitating learning of more meaningful\nrepresentations for tasks like instance segmentation. This knowledge has, thus\nfar, not been utilised by the biomedical computer vision community. We argue\nthat the instance segmentation of two or more categories of objects can be\nachieved in parallel. We achieve this via two architectures HydraStarDist (HSD)\nand the novel (HSD-WBR) based on the widely-used StarDist (SD), to take\nadvantage of the star-convexity of our target objects. HSD and HSD-WBR are\nconstructed to be capable of incorporating their interactions as constraints\ninto account. HSD implicitly incorporates spatial correlation priors based on\nobject interaction through a joint encoder. HSD-WBR further enforces the prior\nin a regularisation layer with the penalty we proposed named Within Boundary\nRegularisation Penalty (WBR). Both architectures achieve nested instance\nsegmentation in a single shot. We demonstrate their competitiveness based on\n$IoU_R$ and AP and superiority in a new, task-relevant criteria, Joint TP rate\n(JTPR) compared to their baseline SD and Cellpose. Our approach can be further\nmodified to capture partial-inclusion/-exclusion in multi-object interactions\nin fluorescent or brightfield microscopy or digital imaging. Finally, our\nstrategy suggests gains by making this learning single-shot and computationally\nefficient."}
{"id": "2504.12080", "pdf": "https://arxiv.org/pdf/2504.12080", "abs": "https://arxiv.org/abs/2504.12080", "authors": ["Mengshi Qi", "Pengfei Zhu", "Xiangtai Li", "Xiaoyang Bi", "Lu Qi", "Huadong Ma", "Ming-Hsuan Yang"], "title": "DC-SAM: In-Context Segment Anything in Images and Videos via Dual Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Given a single labeled example, in-context segmentation aims to segment\ncorresponding objects. This setting, known as one-shot segmentation in few-shot\nlearning, explores the segmentation model's generalization ability and has been\napplied to various vision tasks, including scene understanding and image/video\nediting. While recent Segment Anything Models have achieved state-of-the-art\nresults in interactive segmentation, these approaches are not directly\napplicable to in-context segmentation. In this work, we propose the Dual\nConsistency SAM (DC-SAM) method based on prompt-tuning to adapt SAM and SAM2\nfor in-context segmentation of both images and videos. Our key insights are to\nenhance the features of the SAM's prompt encoder in segmentation by providing\nhigh-quality visual prompts. When generating a mask prior, we fuse the SAM\nfeatures to better align the prompt encoder. Then, we design a cycle-consistent\ncross-attention on fused features and initial visual prompts. Next, a\ndual-branch design is provided by using the discriminative positive and\nnegative prompts in the prompt encoder. Furthermore, we design a simple\nmask-tube training strategy to adopt our proposed dual consistency method into\nthe mask tube. Although the proposed DC-SAM is primarily designed for images,\nit can be seamlessly extended to the video domain with the support of SAM2.\nGiven the absence of in-context segmentation in the video domain, we manually\ncurate and construct the first benchmark from existing video segmentation\ndatasets, named In-Context Video Object Segmentation (IC-VOS), to better assess\nthe in-context capability of the model. Extensive experiments demonstrate that\nour method achieves 55.5 (+1.4) mIoU on COCO-20i, 73.0 (+1.1) mIoU on\nPASCAL-5i, and a J&F score of 71.52 on the proposed IC-VOS benchmark. Our\nsource code and benchmark are available at https://github.com/zaplm/DC-SAM."}
{"id": "2504.12083", "pdf": "https://arxiv.org/pdf/2504.12083", "abs": "https://arxiv.org/abs/2504.12083", "authors": ["Pritam Sarkar", "Ali Etemad"], "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in Large Video Language Models (LVLMs), they still\nstruggle with fine-grained temporal understanding, hallucinate, and often make\nsimple mistakes on even simple video question-answering tasks, all of which\npose significant challenges to their safe and reliable deployment in real-world\napplications. To address these limitations, we propose a self-alignment\nframework that enables LVLMs to learn from their own errors. Our proposed\nframework first obtains a training set of preferred and non-preferred response\npairs, where non-preferred responses are generated by incorporating common\nerror patterns that often occur due to inadequate spatio-temporal\nunderstanding, spurious correlations between co-occurring concepts, and\nover-reliance on linguistic cues while neglecting the vision modality, among\nothers. To facilitate self-alignment of LVLMs with the constructed preferred\nand non-preferred response pairs, we introduce Refined Regularized Preference\nOptimization (RRPO), a novel preference optimization method that utilizes\nsub-sequence-level refined rewards and token-wise KL regularization to address\nthe limitations of Direct Preference Optimization (DPO). We demonstrate that\nRRPO achieves more precise alignment and more stable training compared to DPO.\nOur experiments and analysis validate the effectiveness of our approach across\ndiverse video tasks, including video hallucination, short- and long-video\nunderstanding, and fine-grained temporal reasoning."}
{"id": "2504.12088", "pdf": "https://arxiv.org/pdf/2504.12088", "abs": "https://arxiv.org/abs/2504.12088", "authors": ["Mirza Samad Ahmed Baig", "Syeda Anshrah Gillani", "Abdul Akbar Khan", "Shahid Munir Shah"], "title": "AttentionDrop: A Novel Regularization Method for Transformer Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "26 pages", "summary": "Transformer-based architectures achieve state-of-the-art performance across a\nwide range of tasks in natural language processing, computer vision, and\nspeech. However, their immense capacity often leads to overfitting, especially\nwhen training data is limited or noisy. We propose AttentionDrop, a unified\nfamily of stochastic regularization techniques that operate directly on the\nself-attention distributions. We introduces three variants: 1. Hard Attention\nMasking: randomly zeroes out top-k attention logits per query to encourage\ndiverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic\nGaussian convolution over attention logits to diffuse overly peaked\ndistributions. 3. Consistency-Regularized AttentionDrop: enforces output\nstability under multiple independent AttentionDrop perturbations via a KL-based\nconsistency loss."}
{"id": "2504.12100", "pdf": "https://arxiv.org/pdf/2504.12100", "abs": "https://arxiv.org/abs/2504.12100", "authors": ["Kaifeng Gao", "Siqi Chen", "Hanwang Zhang", "Jun Xiao", "Yueting Zhuang", "Qianru Sun"], "title": "Generalized Visual Relation Detection with Diffusion Models", "categories": ["cs.CV"], "comment": "Under review at IEEE TCSVT. The Appendix is provided additionally", "summary": "Visual relation detection (VRD) aims to identify relationships (or\ninteractions) between object pairs in an image. Although recent VRD models have\nachieved impressive performance, they are all restricted to pre-defined\nrelation categories, while failing to consider the semantic ambiguity\ncharacteristic of visual relations. Unlike objects, the appearance of visual\nrelations is always subtle and can be described by multiple predicate words\nfrom different perspectives, e.g., ``ride'' can be depicted as ``race'' and\n``sit on'', from the sports and spatial position views, respectively. To this\nend, we propose to model visual relations as continuous embeddings, and design\ndiffusion models to achieve generalized VRD in a conditional generative manner,\ntermed Diff-VRD. We model the diffusion process in a latent space and generate\nall possible relations in the image as an embedding sequence. During the\ngeneration, the visual and text embeddings of subject-object pairs serve as\nconditional signals and are injected via cross-attention. After the generation,\nwe design a subsequent matching stage to assign the relation words to\nsubject-object pairs by considering their semantic similarities. Benefiting\nfrom the diffusion-based generative process, our Diff-VRD is able to generate\nvisual relations beyond the pre-defined category labels of datasets. To\nproperly evaluate this generalized VRD task, we introduce two evaluation\nmetrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image\ncaptioning. Extensive experiments in both human-object interaction (HOI)\ndetection and scene graph generation (SGG) benchmarks attest to the superiority\nand effectiveness of Diff-VRD."}
{"id": "2504.12103", "pdf": "https://arxiv.org/pdf/2504.12103", "abs": "https://arxiv.org/abs/2504.12103", "authors": ["Tao Wen", "Jiepeng Wang", "Yabo Chen", "Shugong Xu", "Chi Zhang", "Xuelong Li"], "title": "Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image", "categories": ["cs.CV"], "comment": "Our project page: https://tele-ai.github.io/MetricSolver/", "summary": "Accurate and generalizable metric depth estimation is crucial for various\ncomputer vision applications but remains challenging due to the diverse depth\nscales encountered in indoor and outdoor environments. In this paper, we\nintroduce Metric-Solver, a novel sliding anchor-based metric depth estimation\nmethod that dynamically adapts to varying scene scales. Our approach leverages\nan anchor-based representation, where a reference depth serves as an anchor to\nseparate and normalize the scene depth into two components: scaled near-field\ndepth and tapered far-field depth. The anchor acts as a normalization factor,\nenabling the near-field depth to be normalized within a consistent range while\nmapping far-field depth smoothly toward zero. Through this approach, any depth\nfrom zero to infinity in the scene can be represented within a unified\nrepresentation, effectively eliminating the need to manually account for scene\nscale variations. More importantly, for the same scene, the anchor can slide\nalong the depth axis, dynamically adjusting to different depth scales. A\nsmaller anchor provides higher resolution in the near-field, improving depth\nprecision for closer objects while a larger anchor improves depth estimation in\nfar regions. This adaptability enables the model to handle depth predictions at\nvarying distances and ensure strong generalization across datasets. Our design\nenables a unified and adaptive depth representation across diverse\nenvironments. Extensive experiments demonstrate that Metric-Solver outperforms\nexisting methods in both accuracy and cross-dataset generalization."}
{"id": "2504.12104", "pdf": "https://arxiv.org/pdf/2504.12104", "abs": "https://arxiv.org/abs/2504.12104", "authors": ["Shuo Li", "Fang Liu", "Zehua Hao", "Xinyi Wang", "Lingling Li", "Xu Liu", "Puhua Chen", "Wenping Ma"], "title": "Logits DeConfusion with CLIP for Few-Shot Learning", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "With its powerful visual-language alignment capability, CLIP performs well in\nzero-shot and few-shot learning tasks. However, we found in experiments that\nCLIP's logits suffer from serious inter-class confusion problems in downstream\ntasks, and the ambiguity between categories seriously affects the accuracy. To\naddress this challenge, we propose a novel method called Logits DeConfusion,\nwhich effectively learns and eliminates inter-class confusion in logits by\ncombining our Multi-level Adapter Fusion (MAF) module with our Inter-Class\nDeconfusion (ICD) module. Our MAF extracts features from different levels and\nfuses them uniformly to enhance feature representation. Our ICD learnably\neliminates inter-class confusion in logits with a residual structure.\nExperimental results show that our method can significantly improve the\nclassification performance and alleviate the inter-class confusion problem. The\ncode is available at https://github.com/LiShuo1001/LDC."}
{"id": "2504.12112", "pdf": "https://arxiv.org/pdf/2504.12112", "abs": "https://arxiv.org/abs/2504.12112", "authors": ["Zhenyu Yu", "Mohd Yamani Inda Idris", "Pei Wang"], "title": "A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Remote sensing imagery is essential for environmental monitoring,\nagricultural management, and disaster response. However, data loss due to cloud\ncover, sensor failures, or incomplete acquisition-especially in high-resolution\nand high-frequency tasks-severely limits satellite imagery's effectiveness.\nTraditional interpolation methods struggle with large missing areas and complex\nstructures. Remote sensing imagery consists of multiple bands, each with\ndistinct meanings, and ensuring consistency across bands is critical to avoid\nanomalies in the combined images. This paper proposes SatelliteMaker, a\ndiffusion-based method that reconstructs missing data across varying levels of\ndata loss while maintaining spatial, spectral, and temporal consistency. We\nalso propose Digital Elevation Model (DEM) as a conditioning input and use\ntailored prompts to generate realistic images, making diffusion models\napplicable to quantitative remote sensing tasks. Additionally, we propose a\nVGG-Adapter module based on Distribution Loss, which reduces distribution\ndiscrepancy and ensures style consistency. Extensive experiments show that\nSatelliteMaker achieves state-of-the-art performance across multiple tasks."}
{"id": "2504.12121", "pdf": "https://arxiv.org/pdf/2504.12121", "abs": "https://arxiv.org/abs/2504.12121", "authors": ["Jose Francisco Diez-Pastor", "Francisco Javier Gonzalez-Moya", "Pedro Latorre-Carmona", "Francisco Javier Perez-Barbería", "Ludmila I. Kuncheva", "Antonio Canepa-Oneto", "Alvar Arnaiz-González", "Cesar Garcia-Osorio"], "title": "Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals", "categories": ["cs.CV"], "comment": "24 pages, 6 figures. Submitted to Computers and Geosciences", "summary": "Detection of spatial areas where biodiversity is at risk is of paramount\nimportance for the conservation and monitoring of ecosystems. Large terrestrial\nmammalian herbivores are keystone species as their activity not only has deep\neffects on soils, plants, and animals, but also shapes landscapes, as large\nherbivores act as allogenic ecosystem engineers. One key landscape feature that\nindicates intense herbivore activity and potentially impacts biodiversity is\nthe formation of grazing trails. Grazing trails are formed by the continuous\ntrampling activity of large herbivores that can produce complex networks of\ntracks of bare soil. Here, we evaluated different algorithms based on machine\nlearning techniques to identify grazing trails. Our goal is to automatically\ndetect potential areas with intense herbivory activity, which might be\nbeneficial for conservation and management plans.\n  We have applied five semantic segmentation methods combined with fourteen\nencoders aimed at mapping grazing trails on aerial images. Our results indicate\nthat in most cases the chosen methodology successfully mapped the trails,\nalthough there were a few instances where the actual trail structure was\nunderestimated. The UNet architecture with the MambaOut encoder was the best\narchitecture for mapping trails. The proposed approach could be applied to\ndevelop tools for mapping and monitoring temporal changes in these landscape\nstructures to support habitat conservation and land management programs. This\nis the first time, to the best of our knowledge, that competitive image\nsegmentation results are obtained for the detection and delineation of trails\nof large herbivorous mammals."}
{"id": "2504.12129", "pdf": "https://arxiv.org/pdf/2504.12129", "abs": "https://arxiv.org/abs/2504.12129", "authors": ["Songping Wang", "Yueming Lyu", "Shiqi Liu", "Ning Li", "Tong Tong", "Hao Sun", "Caifeng Shan"], "title": "Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The rise of customized diffusion models has spurred a boom in personalized\nvisual content creation, but also poses risks of malicious misuse, severely\nthreatening personal privacy and copyright protection. Some studies show that\nthe aesthetic properties of images are highly positively correlated with human\nperception of image quality. Inspired by this, we approach the problem from a\nnovel and intriguing aesthetic perspective to degrade the generation quality of\nmaliciously customized models, thereby achieving better protection of facial\nidentity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA)\nframework to fully explore aesthetic cues, which consists of two key branches:\n1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward\nmechanism and a global anti-aesthetic loss, it can degrade the overall\naesthetics of the generated content; 2) Local Anti-Aesthetics: A local\nanti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to\nguide adversarial perturbations to disrupt local facial identity. By seamlessly\nintegrating both branches, our HAA effectively achieves the goal of\nanti-aesthetics from a global to a local level during customized generation.\nExtensive experiments show that HAA outperforms existing SOTA methods largely\nin identity removal, providing a powerful tool for protecting facial privacy\nand copyright."}
{"id": "2504.12132", "pdf": "https://arxiv.org/pdf/2504.12132", "abs": "https://arxiv.org/abs/2504.12132", "authors": ["Linhao Qu", "Shiman Li", "Xiaoyuan Luo", "Shaolei Liu", "Qinhao Guo", "Manning Wang", "Zhijian Song"], "title": "Weakly Semi-supervised Whole Slide Image Classification by Two-level Cross Consistency Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Computer-aided Whole Slide Image (WSI) classification has the potential to\nenhance the accuracy and efficiency of clinical pathological diagnosis. It is\ncommonly formulated as a Multiple Instance Learning (MIL) problem, where each\nWSI is treated as a bag and the small patches extracted from the WSI are\nconsidered instances within that bag. However, obtaining labels for a large\nnumber of bags is a costly and time-consuming process, particularly when\nutilizing existing WSIs for new classification tasks. This limitation renders\nmost existing WSI classification methods ineffective. To address this issue, we\npropose a novel WSI classification problem setting, more aligned with clinical\npractice, termed Weakly Semi-supervised Whole slide image Classification\n(WSWC). In WSWC, a small number of bags are labeled, while a significant number\nof bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the\nabsence of patch labels, distinguishes it from typical semi-supervised image\nclassification problems, making existing algorithms for natural images\nunsuitable for directly solving the WSWC problem. In this paper, we present a\nconcise and efficient framework, named CroCo, to tackle the WSWC problem\nthrough two-level Cross Consistency supervision. CroCo comprises two\nheterogeneous classifier branches capable of performing both instance\nclassification and bag classification. The fundamental idea is to establish\ncross-consistency supervision at both the bag-level and instance-level between\nthe two branches during training. Extensive experiments conducted on four\ndatasets demonstrate that CroCo achieves superior bag classification and\ninstance classification performance compared to other comparative methods when\nlimited WSIs with bag labels are available. To the best of our knowledge, this\npaper presents for the first time the WSWC problem and gives a successful\nresolution."}
{"id": "2504.12137", "pdf": "https://arxiv.org/pdf/2504.12137", "abs": "https://arxiv.org/abs/2504.12137", "authors": ["Laura Fieback", "Nishilkumar Balar", "Jakob Spiegelberg", "Hanno Gottschalk"], "title": "Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Despite recent advances in Large Vision Language Models (LVLMs), these models\nstill suffer from generating hallucinatory responses that do not align with the\nvisual input provided. To mitigate such hallucinations, we introduce Efficient\nContrastive Decoding (ECD), a simple method that leverages probabilistic\nhallucination detection to shift the output distribution towards contextually\naccurate answers at inference time. By contrasting token probabilities and\nhallucination scores, ECD subtracts hallucinated concepts from the original\ndistribution, effectively suppressing hallucinations. Notably, our proposed\nmethod can be applied to any open-source LVLM and does not require additional\nLVLM training. We evaluate our method on several benchmark datasets and across\ndifferent LVLMs. Our experiments show that ECD effectively mitigates\nhallucinations, outperforming state-of-the-art methods with respect to\nperformance on LVLM benchmarks and computation time."}
{"id": "2504.12157", "pdf": "https://arxiv.org/pdf/2504.12157", "abs": "https://arxiv.org/abs/2504.12157", "authors": ["Xiaojun Ye", "Chun Wang", "Yiren Song", "Sheng Zhou", "Liangcheng Li", "Jiajun Bu"], "title": "FocusedAD: Character-centric Movie Audio Description", "categories": ["cs.CV", "I.2.10"], "comment": "Code and Demo link: https://github.com/Thorin215/FocusedAD", "summary": "Movie Audio Description (AD) aims to narrate visual content during\ndialogue-free segments, particularly benefiting blind and visually impaired\n(BVI) audiences. Compared with general video captioning, AD demands\nplot-relevant narration with explicit character name references, posing unique\nchallenges in movie understanding.To identify active main characters and focus\non storyline-relevant regions, we propose FocusedAD, a novel framework that\ndelivers character-centric movie audio descriptions. It includes: (i) a\nCharacter Perception Module(CPM) for tracking character regions and linking\nthem to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues\nfrom prior ADs and subtitles via learnable soft prompts; and (iii) a Focused\nCaption Module(FCM) that generates narrations enriched with plot-relevant\ndetails and named characters. To overcome limitations in character\nidentification, we also introduce an automated pipeline for building character\nquery banks. FocusedAD achieves state-of-the-art performance on multiple\nbenchmarks, including strong zero-shot results on MAD-eval-Named and our newly\nproposed Cinepile-AD dataset. Code and data will be released at\nhttps://github.com/Thorin215/FocusedAD ."}
{"id": "2504.12165", "pdf": "https://arxiv.org/pdf/2504.12165", "abs": "https://arxiv.org/abs/2504.12165", "authors": ["Yike Liu", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "CodingHomo: Bootstrapping Deep Homography With Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "Homography estimation is a fundamental task in computer vision with\napplications in diverse fields. Recent advances in deep learning have improved\nhomography estimation, particularly with unsupervised learning approaches,\noffering increased robustness and generalizability. However, accurately\npredicting homography, especially in complex motions, remains a challenge. In\nresponse, this work introduces a novel method leveraging video coding,\nparticularly by harnessing inherent motion vectors (MVs) present in videos. We\npresent CodingHomo, an unsupervised framework for homography estimation. Our\nframework features a Mask-Guided Fusion (MGF) module that identifies and\nutilizes beneficial features among the MVs, thereby enhancing the accuracy of\nhomography prediction. Additionally, the Mask-Guided Homography Estimation\n(MGHE) module is presented for eliminating undesired features in the\ncoarse-to-fine homography refinement process. CodingHomo outperforms existing\nstate-of-the-art unsupervised methods, delivering good robustness and\ngeneralizability. The code and dataset are available at:\n\\href{github}{https://github.com/liuyike422/CodingHomo"}
{"id": "2504.12167", "pdf": "https://arxiv.org/pdf/2504.12167", "abs": "https://arxiv.org/abs/2504.12167", "authors": ["Yuan Luo", "Rudolf Hoffmann", "Yan Xia", "Olaf Wysocki", "Benedikt Schwab", "Thomas H. Kolbe", "Daniel Cremers"], "title": "RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning", "categories": ["cs.CV", "cs.LG"], "comment": "The paper accepted for CVPRW '25 (PBVS 2025 - the Perception Beyond\n  the Visible Spectrum)", "summary": "Semantic 3D city models are worldwide easy-accessible, providing accurate,\nobject-oriented, and semantic-rich 3D priors. To date, their potential to\nmitigate the noise impact on radar object detection remains under-explored. In\nthis paper, we first introduce a unique dataset, RadarCity, comprising 54K\nsynchronized radar-image pairs and semantic 3D city models. Moreover, we\npropose a novel neural network, RADLER, leveraging the effectiveness of\ncontrastive self-supervised learning (SSL) and semantic 3D city models to\nenhance radar object detection of pedestrians, cyclists, and cars.\nSpecifically, we first obtain the robust radar features via a SSL network in\nthe radar-image pretext task. We then use a simple yet effective feature fusion\nstrategy to incorporate semantic-depth features from semantic 3D city models.\nHaving prior 3D information as guidance, RADLER obtains more fine-grained\ndetails to enhance radar object detection. We extensively evaluate RADLER on\nthe collected RadarCity dataset and demonstrate average improvements of 5.46%\nin mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over\nprevious radar object detection methods. We believe this work will foster\nfurther research on semantic-guided and map-supported radar object detection.\nOur project page is publicly available\nathttps://gpp-communication.github.io/RADLER ."}
{"id": "2504.12169", "pdf": "https://arxiv.org/pdf/2504.12169", "abs": "https://arxiv.org/abs/2504.12169", "authors": ["Joanne Lin", "Crispian Morris", "Ruirui Lin", "Fan Zhang", "David Bull", "Nantheera Anantrasirichai"], "title": "Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video Pipeline", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Low-light conditions pose significant challenges for both human and machine\nannotation. This in turn has led to a lack of research into machine\nunderstanding for low-light images and (in particular) videos. A common\napproach is to apply annotations obtained from high quality datasets to\nsynthetically created low light versions. In addition, these approaches are\noften limited through the use of unrealistic noise models. In this paper, we\npropose a new Degradation Estimation Network (DEN), which synthetically\ngenerates realistic standard RGB (sRGB) noise without the requirement for\ncamera metadata. This is achieved by estimating the parameters of\nphysics-informed noise distributions, trained in a self-supervised manner. This\nzero-shot approach allows our method to generate synthetic noisy content with a\ndiverse range of realistic noise characteristics, unlike other methods which\nfocus on recreating the noise characteristics of the training data. We evaluate\nour proposed synthetic pipeline using various methods trained on its synthetic\ndata for typical low-light tasks including synthetic noise replication, video\nenhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\%\nLPIPS, and 62\\% AP$_{50-95}$, respectively."}
{"id": "2504.12186", "pdf": "https://arxiv.org/pdf/2504.12186", "abs": "https://arxiv.org/abs/2504.12186", "authors": ["Alejandro Newell", "Peiyun Hu", "Lahav Lipson", "Stephan R. Richter", "Vladlen Koltun"], "title": "CoMotion: Concurrent Multi-person 3D Motion", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025, for code and weights go to\n  https://github.com/apple/ml-comotion", "summary": "We introduce an approach for detecting and tracking detailed 3D poses of\nmultiple people from a single monocular camera stream. Our system maintains\ntemporally coherent predictions in crowded scenes filled with difficult poses\nand occlusions. Our model performs both strong per-frame detection and a\nlearned pose update to track people from frame to frame. Rather than match\ndetections across time, poses are updated directly from a new input image,\nwhich enables online tracking through occlusion. We train on numerous image and\nvideo datasets leveraging pseudo-labeled annotations to produce a model that\nmatches state-of-the-art systems in 3D pose estimation accuracy while being\nfaster and more accurate in tracking multiple people through time. Code and\nweights are provided at https://github.com/apple/ml-comotion"}
{"id": "2504.12197", "pdf": "https://arxiv.org/pdf/2504.12197", "abs": "https://arxiv.org/abs/2504.12197", "authors": ["Mahdi Alehdaghi", "Rajarshi Bhattacharya", "Pourya Shamsolmoali", "Rafael M. O. Cruz", "Maguelonne Heritier", "Eric Granger"], "title": "Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning has provided considerable advancements for multimedia systems,\nyet the interpretability of deep models remains a challenge. State-of-the-art\npost-hoc explainability methods, such as GradCAM, provide visual interpretation\nbased on heatmaps but lack conceptual clarity. Prototype-based approaches, like\nProtoPNet and PIPNet, offer a more structured explanation but rely on fixed\npatches, limiting their robustness and semantic consistency.\n  To address these limitations, a part-prototypical concept mining network\n(PCMNet) is proposed that dynamically learns interpretable prototypes from\nmeaningful regions. PCMNet clusters prototypes into concept groups, creating\nsemantically grounded explanations without requiring additional annotations.\nThrough a joint process of unsupervised part discovery and concept activation\nvector extraction, PCMNet effectively captures discriminative concepts and\nmakes interpretable classification decisions.\n  Our extensive experiments comparing PCMNet against state-of-the-art methods\non multiple datasets show that it can provide a high level of interpretability,\nstability, and robustness under clean and occluded scenarios."}
{"id": "2504.12204", "pdf": "https://arxiv.org/pdf/2504.12204", "abs": "https://arxiv.org/abs/2504.12204", "authors": ["Zhihua Wang", "Yu Long", "Qinghua Lin", "Kai Zhang", "Yazhu Zhang", "Yuming Fang", "Li Liu", "Xiaochun Cao"], "title": "Towards Realistic Low-Light Image Enhancement via ISP Driven Data Modeling", "categories": ["cs.CV", "cs.MM"], "comment": "17 pages, 11 tables, 10 figures", "summary": "Deep neural networks (DNNs) have recently become the leading method for\nlow-light image enhancement (LLIE). However, despite significant progress,\ntheir outputs may still exhibit issues such as amplified noise, incorrect white\nbalance, or unnatural enhancements when deployed in real world applications. A\nkey challenge is the lack of diverse, large scale training data that captures\nthe complexities of low-light conditions and imaging pipelines. In this paper,\nwe propose a novel image signal processing (ISP) driven data synthesis pipeline\nthat addresses these challenges by generating unlimited paired training data.\nSpecifically, our pipeline begins with easily collected high-quality\nnormal-light images, which are first unprocessed into the RAW format using a\nreverse ISP. We then synthesize low-light degradations directly in the RAW\ndomain. The resulting data is subsequently processed through a series of ISP\nstages, including white balance adjustment, color space conversion, tone\nmapping, and gamma correction, with controlled variations introduced at each\nstage. This broadens the degradation space and enhances the diversity of the\ntraining data, enabling the generated data to capture a wide range of\ndegradations and the complexities inherent in the ISP pipeline. To demonstrate\nthe effectiveness of our synthetic pipeline, we conduct extensive experiments\nusing a vanilla UNet model consisting solely of convolutional layers, group\nnormalization, GeLU activation, and convolutional block attention modules\n(CBAMs). Extensive testing across multiple datasets reveals that the vanilla\nUNet model trained with our data synthesis pipeline delivers high fidelity,\nvisually appealing enhancement results, surpassing state-of-the-art (SOTA)\nmethods both quantitatively and qualitatively."}
{"id": "2504.12215", "pdf": "https://arxiv.org/pdf/2504.12215", "abs": "https://arxiv.org/abs/2504.12215", "authors": ["Ilkin Sevgi Isler", "David Mohaisen", "Curtis Lisle", "Damla Turgut", "Ulas Bagci"], "title": "Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 2 figures, to appear in IEEE ADSCA 2025", "summary": "Reliable tumor segmentation in thoracic computed tomography (CT) remains\nchallenging due to boundary ambiguity, class imbalance, and anatomical\nvariability. We propose an uncertainty-guided, coarse-to-fine segmentation\nframework that combines full-volume tumor localization with refined\nregion-of-interest (ROI) segmentation, enhanced by anatomically aware\npost-processing. The first-stage model generates a coarse prediction, followed\nby anatomically informed filtering based on lung overlap, proximity to lung\nsurfaces, and component size. The resulting ROIs are segmented by a\nsecond-stage model trained with uncertainty-aware loss functions to improve\naccuracy and boundary calibration in ambiguous regions. Experiments on private\nand public datasets demonstrate improvements in Dice and Hausdorff scores, with\nfewer false positives and enhanced spatial interpretability. These results\nhighlight the value of combining uncertainty modeling and anatomical priors in\ncascaded segmentation pipelines for robust and clinically meaningful tumor\ndelineation. On the Orlando dataset, our framework improved Swin UNETR Dice\nfrom 0.4690 to 0.6447. Reduction in spurious components was strongly correlated\nwith segmentation gains, underscoring the value of anatomically informed\npost-processing."}
{"id": "2504.12222", "pdf": "https://arxiv.org/pdf/2504.12222", "abs": "https://arxiv.org/abs/2504.12222", "authors": ["Yike Liu", "Jianhui Zhang", "Haipeng Li", "Shuaicheng Liu", "Bing Zeng"], "title": "Coding-Prior Guided Diffusion Network for Video Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "While recent video deblurring methods have advanced significantly, they often\noverlook two valuable prior information: (1) motion vectors (MVs) and coding\nresiduals (CRs) from video codecs, which provide efficient inter-frame\nalignment cues, and (2) the rich real-world knowledge embedded in pre-trained\ndiffusion generative models. We present CPGDNet, a novel two-stage framework\nthat effectively leverages both coding priors and generative diffusion priors\nfor high-quality deblurring. First, our coding-prior feature propagation (CPFP)\nmodule utilizes MVs for efficient frame alignment and CRs to generate attention\nmasks, addressing motion inaccuracies and texture variations. Second, a\ncoding-prior controlled generation (CPC) module network integrates coding\npriors into a pretrained diffusion model, guiding it to enhance critical\nregions and synthesize realistic details. Experiments demonstrate our method\nachieves state-of-the-art perceptual quality with up to 30% improvement in IQA\nmetrics. Both the code and the codingprior-augmented dataset will be\nopen-sourced."}
{"id": "2504.12240", "pdf": "https://arxiv.org/pdf/2504.12240", "abs": "https://arxiv.org/abs/2504.12240", "authors": ["Junhao Zhuang", "Lingen Li", "Xuan Ju", "Zhaoyang Zhang", "Chun Yuan", "Ying Shan"], "title": "Cobra: Efficient Line Art COlorization with BRoAder References", "categories": ["cs.CV"], "comment": "Project page with code: https://zhuang2002.github.io/Cobra/", "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."}
{"id": "2504.12245", "pdf": "https://arxiv.org/pdf/2504.12245", "abs": "https://arxiv.org/abs/2504.12245", "authors": ["Xia Wang", "Haiyang Sun", "Tiantian Cao", "Yueying Sun", "Min Feng"], "title": "SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction", "categories": ["cs.CV", "eess.IV"], "comment": "21 pages, 13 figures", "summary": "Moir\\'e patterns, resulting from aliasing between object light signals and\ncamera sampling frequencies, often degrade image quality during capture.\nTraditional demoir\\'eing methods have generally treated images as a whole for\nprocessing and training, neglecting the unique signal characteristics of\ndifferent color channels. Moreover, the randomness and variability of moir\\'e\npattern generation pose challenges to the robustness of existing methods when\napplied to real-world data. To address these issues, this paper presents SIDME\n(Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction),\na novel model designed to generate high-quality visual images by effectively\nprocessing moir\\'e patterns. SIDME combines a masked encoder-decoder\narchitecture with self-supervised learning, allowing the model to reconstruct\nimages using the inherent properties of camera sampling frequencies. A key\ninnovation is the random masked image reconstructor, which utilizes an\nencoder-decoder structure to handle the reconstruction task. Furthermore, since\nthe green channel in camera sampling has a higher sampling frequency compared\nto red and blue channels, a specialized self-supervised loss function is\ndesigned to improve the training efficiency and effectiveness. To ensure the\ngeneralization ability of the model, a self-supervised moir\\'e image generation\nmethod has been developed to produce a dataset that closely mimics real-world\nconditions. Extensive experiments demonstrate that SIDME outperforms existing\nmethods in processing real moir\\'e pattern data, showing its superior\ngeneralization performance and robustness."}
{"id": "2504.12255", "pdf": "https://arxiv.org/pdf/2504.12255", "abs": "https://arxiv.org/abs/2504.12255", "authors": ["Samuel Räber", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Human Aligned Compression for Robust Models", "categories": ["cs.CV", "eess.IV"], "comment": "Presented at the Workshop AdvML at CVPR 2025", "summary": "Adversarial attacks on image models threaten system robustness by introducing\nimperceptible perturbations that cause incorrect predictions. We investigate\nhuman-aligned learned lossy compression as a defense mechanism, comparing two\nlearned models (HiFiC and ELIC) against traditional JPEG across various quality\nlevels. Our experiments on ImageNet subsets demonstrate that learned\ncompression methods outperform JPEG, particularly for Vision Transformer\narchitectures, by preserving semantically meaningful content while removing\nadversarial noise. Even in white-box settings where attackers can access the\ndefense, these methods maintain substantial effectiveness. We also show that\nsequential compression--applying rounds of\ncompression/decompression--significantly enhances defense efficacy while\nmaintaining classification performance. Our findings reveal that human-aligned\ncompression provides an effective, computationally efficient defense that\nprotects the image features most relevant to human and machine understanding.\nIt offers a practical approach to improving model robustness against\nadversarial threats."}
{"id": "2504.12256", "pdf": "https://arxiv.org/pdf/2504.12256", "abs": "https://arxiv.org/abs/2504.12256", "authors": ["Andreas Plesner", "Turlan Kuzhagaliyev", "Roger Wattenhofer"], "title": "FLIP Reasoning Challenge", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Published at First Workshop on Open Science for Foundation Models at\n  ICLR 2025", "summary": "Over the past years, advances in artificial intelligence (AI) have\ndemonstrated how AI can solve many perception and generation tasks, such as\nimage classification and text writing, yet reasoning remains a challenge. This\npaper introduces the FLIP dataset, a benchmark for evaluating AI reasoning\ncapabilities based on human verification tasks on the Idena blockchain. FLIP\nchallenges present users with two orderings of 4 images, requiring them to\nidentify the logically coherent one. By emphasizing sequential reasoning,\nvisual storytelling, and common sense, FLIP provides a unique testbed for\nmultimodal AI systems. Our experiments evaluate state-of-the-art models,\nleveraging both vision-language models (VLMs) and large language models (LLMs).\nResults reveal that even the best open-sourced and closed-sourced models\nachieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot\nsettings, compared to human performance of 95.3%. Captioning models aid\nreasoning models by providing text descriptions of images, yielding better\nresults than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5\nPro. Combining the predictions from 15 models in an ensemble increases the\naccuracy to 85.2%. These findings highlight the limitations of existing\nreasoning models and the need for robust multimodal benchmarks like FLIP. The\nfull codebase and dataset will be available at\nhttps://github.com/aplesner/FLIP-Reasoning-Challenge."}
{"id": "2504.12259", "pdf": "https://arxiv.org/pdf/2504.12259", "abs": "https://arxiv.org/abs/2504.12259", "authors": ["Zhihang Yuan", "Rui Xie", "Yuzhang Shang", "Hanling Zhang", "Siyuan Wang", "Shengen Yan", "Guohao Dai", "Yu Wang"], "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformer(DiT)-based generation models have achieved remarkable\nsuccess in video generation. However, their inherent computational demands pose\nsignificant efficiency challenges. In this paper, we exploit the inherent\ntemporal non-uniformity of real-world videos and observe that videos exhibit\ndynamic information density, with high-motion segments demanding greater detail\npreservation than static scenes. Inspired by this temporal non-uniformity, we\npropose VGDFR, a training-free approach for Diffusion-based Video Generation\nwith Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements\nin latent space based on the motion frequency of the latent space content,\nusing fewer tokens for low-frequency segments while preserving detail in\nhigh-frequency segments. Specifically, our key contributions are: (1) A dynamic\nframe rate scheduler for DiT video generation that adaptively assigns frame\nrates for video segments. (2) A novel latent-space frame merging method to\nalign latent representations with their denoised counterparts before merging\nthose redundant in low-resolution space. (3) A preference analysis of Rotary\nPositional Embeddings (RoPE) across DiT layers, informing a tailored RoPE\nstrategy optimized for semantic and local information capture. Experiments show\nthat VGDFR can achieve a speedup up to 3x for video generation with minimal\nquality degradation."}
{"id": "2504.12264", "pdf": "https://arxiv.org/pdf/2504.12264", "abs": "https://arxiv.org/abs/2504.12264", "authors": ["Ayca Takmaz", "Cristiano Saltori", "Neehar Peri", "Tim Meinhardt", "Riccardo de Lutio", "Laura Leal-Taixé", "Aljoša Ošep"], "title": "Towards Learning to Complete Anything in Lidar", "categories": ["cs.CV"], "comment": null, "summary": "We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion\nin-the-wild. This is closely related to Lidar-based semantic/panoptic scene\ncompletion. However, contemporary methods can only complete and recognize\nobjects from a closed vocabulary labeled in existing Lidar datasets. Different\nto that, our zero-shot approach leverages the temporal context from multi-modal\nsensor sequences to mine object shapes and semantic features of observed\nobjects. These are then distilled into a Lidar-only instance-level completion\nand recognition model. Although we only mine partial shape completions, we find\nthat our distilled model learns to infer full object shapes from multiple such\npartial observations across the dataset. We show that our model can be prompted\non standard benchmarks for Semantic and Panoptic Scene Completion, localize\nobjects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class\nvocabularies. Our project page is\nhttps://research.nvidia.com/labs/dvl/projects/complete-anything-lidar"}
{"id": "2504.12273", "pdf": "https://arxiv.org/pdf/2504.12273", "abs": "https://arxiv.org/abs/2504.12273", "authors": ["Zhuo He", "Paul Henderson", "Nicolas Pugeault"], "title": "Beyond Reconstruction: A Physics Based Neural Deferred Shader for Photo-realistic Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning based rendering has demonstrated major improvements for\nphoto-realistic image synthesis, applicable to various applications including\nvisual effects in movies and photo-realistic scene building in video games.\nHowever, a significant limitation is the difficulty of decomposing the\nillumination and material parameters, which limits such methods to reconstruct\nan input scene, without any possibility to control these parameters. This paper\nintroduces a novel physics based neural deferred shading pipeline to decompose\nthe data-driven rendering process, learn a generalizable shading function to\nproduce photo-realistic results for shading and relighting tasks, we also\nprovide a shadow estimator to efficiently mimic shadowing effect. Our model\nachieves improved performance compared to classical models and a state-of-art\nneural shading model, and enables generalizable photo-realistic shading from\narbitrary illumination input."}
{"id": "2504.12276", "pdf": "https://arxiv.org/pdf/2504.12276", "abs": "https://arxiv.org/abs/2504.12276", "authors": ["Lei Sun", "Hang Guo", "Bin Ren", "Luc Van Gool", "Radu Timofte", "Yawei Li", "Xiangyu Kong", "Hyunhee Park", "Xiaoxuan Yu", "Suejin Han", "Hakjae Jeon", "Jia Li", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Jingyu Ma", "Zhijuan Huang", "Huiyuan Fu", "Hongyuan Yu", "Boqi Zhang", "Jiawei Shi", "Heng Zhang", "Huadong Ma", "Deepak Kumar Tyagi", "Aman Kukretti", "Gajender Sharma", "Sriharsha Koundinya", "Asim Manna", "Jun Cheng", "Shan Tan", "Jun Liu", "Jiangwei Hao", "Jianping Luo", "Jie Lu", "Satya Narayan Tazi", "Arnim Gautam", "Aditi Pawar", "Aishwarya Joshi", "Akshay Dudhane", "Praful Hambadre", "Sachin Chaudhary", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Jiachen Tu", "Nikhil Akalwadi", "Vijayalaxmi Ashok Aralikatti", "Dheeraj Damodar Hegde", "G Gyaneshwar Rao", "Jatin Kalal", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Zhenyuan Lin", "Yubo Dong", "Weikun Li", "Anqi Li", "Ang Gao", "Weijun Yuan", "Zhan Li", "Ruting Deng", "Yihang Chen", "Yifan Deng", "Zhanglu Chen", "Boyang Yao", "Shuling Zheng", "Feng Zhang", "Zhiheng Fu", "Anas M. Ali", "Bilel Benjdira", "Wadii Boulila", "Jan Seny", "Pei Zhou", "Jianhua Hu", "K. L. Eddie Law", "Jaeho Lee", "M. J. Aashik Rasool", "Abdur Rehman", "SMA Sharif", "Seongwan Kim", "Alexandru Brateanu", "Raul Balmez", "Ciprian Orhei", "Cosmin Ancuti", "Zeyu Xiao", "Zhuoyuan Li", "Ziqi Wang", "Yanyan Wei", "Fei Wang", "Kun Li", "Shengeng Tang", "Yunkai Zhang", "Weirun Zhou", "Haoxuan Lu"], "title": "The Tenth NTIRE 2025 Image Denoising Challenge Report", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an overview of the NTIRE 2025 Image Denoising Challenge\n({\\sigma} = 50), highlighting the proposed methodologies and corresponding\nresults. The primary objective is to develop a network architecture capable of\nachieving high-quality denoising performance, quantitatively evaluated using\nPSNR, without constraints on computational complexity or model size. The task\nassumes independent additive white Gaussian noise (AWGN) with a fixed noise\nlevel of 50. A total of 290 participants registered for the challenge, with 20\nteams successfully submitting valid results, providing insights into the\ncurrent state-of-the-art in image denoising."}
{"id": "2504.12284", "pdf": "https://arxiv.org/pdf/2504.12284", "abs": "https://arxiv.org/abs/2504.12284", "authors": ["Aditya Prakash", "Benjamin Lundell", "Dmitry Andreychuk", "David Forsyth", "Saurabh Gupta", "Harpreet Sawhney"], "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025, Project page:\n  https://ap229997.github.io/projects/latentact", "summary": "We tackle the novel problem of predicting 3D hand motion and contact maps (or\nInteraction Trajectories) given a single RGB view, action text, and a 3D\ncontact point on the object as input. Our approach consists of (1) Interaction\nCodebook: a VQVAE model to learn a latent codebook of hand poses and contact\npoints, effectively tokenizing interaction trajectories, (2) Interaction\nPredictor: a transformer-decoder module to predict the interaction trajectory\nfrom test time inputs by using an indexer module to retrieve a latent\naffordance from the learned codebook. To train our model, we develop a data\nengine that extracts 3D hand poses and contact trajectories from the diverse\nHoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger\nthan existing works, in terms of diversity of objects and interactions\nobserved, and test for generalization of the model across object categories,\naction categories, tasks, and scenes. Experimental results show the\neffectiveness of our approach over transformer & diffusion baselines across all\nsettings."}
{"id": "2504.12292", "pdf": "https://arxiv.org/pdf/2504.12292", "abs": "https://arxiv.org/abs/2504.12292", "authors": ["Liam Schoneveld", "Zhe Chen", "Davide Davoli", "Jiapeng Tang", "Saimon Terazawa", "Ko Nishino", "Matthias Nießner"], "title": "SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "For video demonstrations and additional materials please see\n  https://nlml.github.io/sheap/", "summary": "Accurate, real-time 3D reconstruction of human heads from monocular images\nand videos underlies numerous visual applications. As 3D ground truth data is\nhard to come by at scale, previous methods have sought to learn from abundant\n2D videos in a self-supervised manner. Typically, this involves the use of\ndifferentiable mesh rendering, which is effective but faces limitations. To\nimprove on this, we propose SHeaP (Self-supervised Head Geometry Predictor\nLearned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a\nset of Gaussians that are rigged to this mesh. We then reanimate this rigged\nhead avatar to match a target frame, and backpropagate photometric losses to\nboth the 3DMM and Gaussian prediction networks. We find that using Gaussians\nfor rendering substantially improves the effectiveness of this self-supervised\napproach. Training solely on 2D data, our method surpasses existing\nself-supervised approaches in geometric evaluations on the NoW benchmark for\nneutral faces and a new benchmark for non-neutral expressions. Our method also\nproduces highly expressive meshes, outperforming state-of-the-art in emotion\nclassification."}
{"id": "2504.11469", "pdf": "https://arxiv.org/pdf/2504.11469", "abs": "https://arxiv.org/abs/2504.11469", "authors": ["Guillaume Garret", "Antoine Vacavant", "Carole Frindel"], "title": "Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Open access version of an article submitted to Medical Image\n  Understanding and Analysis (MIUA) 2025", "summary": "Deep learning models have achieved impressive performance in medical image\nsegmentation, yet their black-box nature limits clinical adoption. In vascular\napplications, trustworthy segmentation should rely on both local image cues and\nglobal anatomical structures, such as vessel connectivity or branching.\nHowever, the extent to which models leverage such global context remains\nunclear. We present a novel explainability pipeline for 3D vessel segmentation,\ncombining gradient-based attribution with graph-guided point selection and a\nblob-based analysis of Saliency maps. Using vascular graphs extracted from\nground truth, we define anatomically meaningful points of interest (POIs) and\nassess the contribution of input voxels via Saliency maps. These are analyzed\nat both global and local scales using a custom blob detector. Applied to IRCAD\nand Bullitt datasets, our analysis shows that model decisions are dominated by\nhighly localized attribution blobs centered near POIs. Attribution features\nshow little correlation with vessel-level properties such as thickness,\ntubularity, or connectivity -- suggesting limited use of global anatomical\nreasoning. Our results underline the importance of structured explainability\ntools and highlight the current limitations of segmentation models in capturing\nglobal vascular context."}
{"id": "2504.11474", "pdf": "https://arxiv.org/pdf/2504.11474", "abs": "https://arxiv.org/abs/2504.11474", "authors": ["Byunggun Kim", "Younghun Kwon"], "title": "Local Temporal Feature Enhanced Transformer with ROI-rank Based Masking for Diagnosis of ADHD", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In modern society, Attention-Deficit/Hyperactivity Disorder (ADHD) is one of\nthe common mental diseases discovered not only in children but also in adults.\nIn this context, we propose a ADHD diagnosis transformer model that can\neffectively simultaneously find important brain spatiotemporal biomarkers from\nresting-state functional magnetic resonance (rs-fMRI). This model not only\nlearns spatiotemporal individual features but also learns the correlation with\nfull attention structures specialized in ADHD diagnosis. In particular, it\nfocuses on learning local blood oxygenation level dependent (BOLD) signals and\ndistinguishing important regions of interest (ROI) in the brain. Specifically,\nthe three proposed methods for ADHD diagnosis transformer are as follows.\nFirst, we design a CNN-based embedding block to obtain more expressive\nembedding features in brain region attention. It is reconstructed based on the\npreviously CNN-based ADHD diagnosis models for the transformer. Next, for\nindividual spatiotemporal feature attention, we change the attention method to\nlocal temporal attention and ROI-rank based masking. For the temporal features\nof fMRI, the local temporal attention enables to learn local BOLD signal\nfeatures with only simple window masking. For the spatial feature of fMRI,\nROI-rank based masking can distinguish ROIs with high correlation in ROI\nrelationships based on attention scores, thereby providing a more specific\nbiomarker for ADHD diagnosis. The experiment was conducted with various types\nof transformer models. To evaluate these models, we collected the data from 939\nindividuals from all sites provided by the ADHD-200 competition. Through this,\nthe spatiotemporal enhanced transformer for ADHD diagnosis outperforms the\nperformance of other different types of transformer variants. (77.78ACC\n76.60SPE 79.22SEN 79.30AUC)"}
{"id": "2504.11485", "pdf": "https://arxiv.org/pdf/2504.11485", "abs": "https://arxiv.org/abs/2504.11485", "authors": ["Sonia Foschiatti", "Axel Kittenberger", "Otmar Scherzer"], "title": "Deciphering scrolls with tomography: A training experiment", "categories": ["eess.IV", "cs.CV", "97M10, 44A12"], "comment": null, "summary": "The recovery of severely damaged ancient written documents has proven to be a\nmajor challenge for many scientists, mainly due to the impracticality of\nphysical unwrapping them. Non-destructive techniques, such as X-ray computed\ntomography (CT), combined with computer vision algorithms, have emerged as a\nmeans of facilitating the virtual reading of the hidden contents of the damaged\ndocuments. This paper proposes an educational laboratory aimed at simulating\nthe entire process of acquisition and virtual recovery of the ancient works. We\nhave developed an experimental setup that uses visible light to replace the\ndetrimental X-rays, and a didactic software pipeline that allows students to\nvirtually reconstruct a transparent rolled sheet with printed text on it, the\nwrapped scroll."}
{"id": "2504.11491", "pdf": "https://arxiv.org/pdf/2504.11491", "abs": "https://arxiv.org/abs/2504.11491", "authors": ["Mansoor Hayat", "Supavadee Aramvith", "Subrata Bhattacharjee", "Nouman Ahmad"], "title": "Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images", "categories": ["eess.IV", "cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted for presentation in the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "Accurate segmentation of abdominal adipose tissue, including subcutaneous\n(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is\nessential for understanding body composition and associated health risks such\nas type 2 diabetes and cardiovascular disease. This study proposes Attention\nGhostUNet++, a novel deep learning model incorporating Channel, Spatial, and\nDepth Attention mechanisms into the Ghost UNet++ bottleneck for automated,\nprecise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model\nachieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for\nliver segmentation, surpassing baseline models. Despite minor limitations in\nboundary detail segmentation, the proposed model significantly enhances feature\nrefinement, contextual understanding, and computational efficiency, offering a\nrobust solution for body composition analysis. The implementation of the\nproposed Attention GhostUNet++ model is available\nat:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus."}
{"id": "2504.11493", "pdf": "https://arxiv.org/pdf/2504.11493", "abs": "https://arxiv.org/abs/2504.11493", "authors": ["Azizul Zahid", "Jie Fan", "Farong Wang", "Ashton Dy", "Sai Swaminathan", "Fei Liu"], "title": "Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "ICRA'25 Workshop: Human-Centered Robot Learning in the Era of Big\n  Data and Large Models", "summary": "Understanding action correspondence between humans and robots is essential\nfor evaluating alignment in decision-making, particularly in human-robot\ncollaboration and imitation learning within unstructured environments. We\npropose a multimodal demonstration learning framework that explicitly models\nhuman demonstrations from RGB video with robot demonstrations in voxelized\nRGB-D space. Focusing on the \"pick and place\" task from the RH20T dataset, we\nutilize data from 5 users across 10 diverse scenes. Our approach combines\nResNet-based visual encoding for human intention modeling and a Perceiver\nTransformer for voxel-based robot action prediction. After 2000 training\nepochs, the human model reaches 71.67% accuracy, and the robot model achieves\n71.8% accuracy, demonstrating the framework's potential for aligning complex,\nmultimodal human and robot behaviors in manipulation tasks."}
{"id": "2504.11495", "pdf": "https://arxiv.org/pdf/2504.11495", "abs": "https://arxiv.org/abs/2504.11495", "authors": ["Yiting Wang", "Yunxin Fan", "Fei Liu"], "title": "Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Submitted to ICRA'25 Workshop of 3rd Robot-Assisted Medical Imaging", "summary": "Accurate modeling of tool-tissue interactions in robotic surgery requires\nprecise tracking of deformable tissues and integration of surgical domain\nknowledge. Traditional methods rely on labor-intensive annotations or rigid\nassumptions, limiting flexibility. We propose a framework combining sparse\nkeypoint tracking and probabilistic modeling that propagates expert-annotated\nlandmarks across endoscopic frames, even with large tissue deformations.\nClustered tissue keypoints enable dynamic local transformation construction via\nPCA, and tool poses, tracked similarly, are expressed relative to these frames.\nEmbedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM)\nintegrates data-driven observations with labeled clinical expertise,\neffectively predicting relative tool-tissue poses and enhancing visual\nunderstanding of robotic surgical motions directly from video data."}
{"id": "2504.11509", "pdf": "https://arxiv.org/pdf/2504.11509", "abs": "https://arxiv.org/abs/2504.11509", "authors": ["Wenyi Zhang", "Ju Jia", "Xiaojun Jia", "Yihao Huang", "Xinfeng Li", "Cong Wu", "Lina Wang"], "title": "PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines."}
{"id": "2504.11519", "pdf": "https://arxiv.org/pdf/2504.11519", "abs": "https://arxiv.org/abs/2504.11519", "authors": ["Mohammad Farahmand", "Amoon Jamzad", "Fahimeh Fooladgar", "Laura Connolly", "Martin Kaufmann", "Kevin Yi Mi Ren", "John Rudan", "Doug McKay", "Gabor Fichtinger", "Parvin Mousavi"], "title": "FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry", "categories": ["physics.med-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "Purpose: Accurately classifying tissue margins during cancer surgeries is\ncrucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass\nSpectrometry (REIMS), a tool for real-time intraoperative margin assessment,\ngenerates spectra that require machine learning models to support clinical\ndecision-making. However, the scarcity of labeled data in surgical contexts\npresents a significant challenge. This study is the first to develop a\nfoundation model tailored specifically for REIMS data, addressing this\nlimitation and advancing real-time surgical margin assessment. Methods: We\npropose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is\nan adaptation of a foundation model originally designed for text-audio\nassociation, pretrained using our proposed supervised contrastive approach\nbased on triplet loss. An ablation study is performed to compare our proposed\nmodel against other models and pretraining methods. Results: Our proposed model\nsignificantly improves the classification performance, achieving\nstate-of-the-art performance with an AUROC of $82.4\\% \\pm 0.8$. The results\ndemonstrate the advantage of our proposed pretraining method and selected\nbackbone over the self-supervised and semi-supervised baselines and alternative\nmodels. Conclusion: Our findings demonstrate that foundation models, adapted\nand pretrained using our novel approach, can effectively classify REIMS data\neven with limited labeled examples. This highlights the viability of foundation\nmodels for enhancing real-time surgical margin assessment, particularly in\ndata-scarce clinical environments."}
{"id": "2504.11674", "pdf": "https://arxiv.org/pdf/2504.11674", "abs": "https://arxiv.org/abs/2504.11674", "authors": ["Sicong Pan", "Liren Jin", "Xuying Huang", "Cyrill Stachniss", "Marija Popović", "Maren Bennewitz"], "title": "DM-OSVP++: One-Shot View Planning Using 3D Diffusion Models for Active RGB-Based Object Reconstruction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Active object reconstruction is crucial for many robotic applications. A key\naspect in these scenarios is generating object-specific view configurations to\nobtain informative measurements for reconstruction. One-shot view planning\nenables efficient data collection by predicting all views at once, eliminating\nthe need for time-consuming online replanning. Our primary insight is to\nleverage the generative power of 3D diffusion models as valuable prior\ninformation. By conditioning on initial multi-view images, we exploit the\npriors from the 3D diffusion model to generate an approximate object model,\nserving as the foundation for our view planning. Our novel approach integrates\nthe geometric and textural distributions of the object model into the view\nplanning process, generating views that focus on the complex parts of the\nobject to be reconstructed. We validate the proposed active object\nreconstruction system through both simulation and real-world experiments,\ndemonstrating the effectiveness of using 3D diffusion priors for one-shot view\nplanning."}
{"id": "2504.11698", "pdf": "https://arxiv.org/pdf/2504.11698", "abs": "https://arxiv.org/abs/2504.11698", "authors": ["Xingwu Ji", "Haochen Niu", "Dexin Duan", "Rendong Ying", "Fei Wen", "Peilin Liu"], "title": "An Online Adaptation Method for Robust Depth Estimation and Visual Odometry in the Open World", "categories": ["cs.RO", "cs.CV"], "comment": "11 pages, 14 figures", "summary": "Recently, learning-based robotic navigation systems have gained extensive\nresearch attention and made significant progress. However, the diversity of\nopen-world scenarios poses a major challenge for the generalization of such\nsystems to practical scenarios. Specifically, learned systems for scene\nmeasurement and state estimation tend to degrade when the application scenarios\ndeviate from the training data, resulting to unreliable depth and pose\nestimation. Toward addressing this problem, this work aims to develop a visual\nodometry system that can fast adapt to diverse novel environments in an online\nmanner. To this end, we construct a self-supervised online adaptation framework\nfor monocular visual odometry aided by an online-updated depth estimation\nmodule. Firstly, we design a monocular depth estimation network with\nlightweight refiner modules, which enables efficient online adaptation. Then,\nwe construct an objective for self-supervised learning of the depth estimation\nmodule based on the output of the visual odometry system and the contextual\nsemantic information of the scene. Specifically, a sparse depth densification\nmodule and a dynamic consistency enhancement module are proposed to leverage\ncamera poses and contextual semantics to generate pseudo-depths and valid masks\nfor the online adaptation. Finally, we demonstrate the robustness and\ngeneralization capability of the proposed method in comparison with\nstate-of-the-art learning-based approaches on urban, in-house datasets and a\nrobot platform. Code is publicly available at:\nhttps://github.com/jixingwu/SOL-SLAM."}
{"id": "2504.11734", "pdf": "https://arxiv.org/pdf/2504.11734", "abs": "https://arxiv.org/abs/2504.11734", "authors": ["Xiang Tang", "Ruotong Li", "Xiaopeng Fan"], "title": "Recent Advance in 3D Object and Scene Generation: A Survey", "categories": ["cs.GR", "cs.CV"], "comment": "34 pages, 6 figures", "summary": "In recent years, the demand for 3D content has grown exponentially with\nintelligent upgrading of interactive media, extended reality (XR), and\nMetaverse industries. In order to overcome the limitation of traditional manual\nmodeling approaches, such as labor-intensive workflows and prolonged production\ncycles, revolutionary advances have been achieved through the convergence of\nnovel 3D representation paradigms and artificial intelligence generative\ntechnologies. In this survey, we conduct a systematically review of the\ncutting-edge achievements in static 3D object and scene generation, as well as\nestablish a comprehensive technical framework through systematic\ncategorization. Specifically, we initiate our analysis with mainstream 3D\nobject representations, followed by in-depth exploration of two principal\ntechnical pathways in object generation: data-driven supervised learning\nmethods and deep generative model-based approaches. Regarding scene generation,\nwe focus on three dominant paradigms: layout-guided compositional synthesis, 2D\nprior-based scene generation, and rule-driven modeling. Finally, we critically\nexamine persistent challenges in 3D generation and propose potential research\ndirections for future investigation. This survey aims to provide readers with a\nstructured understanding of state-of-the-art 3D generation technologies while\ninspiring researchers to undertake more exploration in this domain."}
{"id": "2504.11825", "pdf": "https://arxiv.org/pdf/2504.11825", "abs": "https://arxiv.org/abs/2504.11825", "authors": ["Kangbo Ma"], "title": "TextDiffSeg: Text-guided Latent Diffusion Model for 3d Medical Images Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Diffusion Probabilistic Models (DPMs) have demonstrated significant potential\nin 3D medical image segmentation tasks. However, their high computational cost\nand inability to fully capture global 3D contextual information limit their\npractical applications. To address these challenges, we propose a novel\ntext-guided diffusion model framework, TextDiffSeg. This method leverages a\nconditional diffusion framework that integrates 3D volumetric data with natural\nlanguage descriptions, enabling cross-modal embedding and establishing a shared\nsemantic space between visual and textual modalities. By enhancing the model's\nability to recognize complex anatomical structures, TextDiffSeg incorporates\ninnovative label embedding techniques and cross-modal attention mechanisms,\neffectively reducing computational complexity while preserving global 3D\ncontextual integrity. Experimental results demonstrate that TextDiffSeg\nconsistently outperforms existing methods in segmentation tasks involving\nkidney and pancreas tumors, as well as multi-organ segmentation scenarios.\nAblation studies further validate the effectiveness of key components,\nhighlighting the synergistic interaction between text fusion, image feature\nextractor, and label encoder. TextDiffSeg provides an efficient and accurate\nsolution for 3D medical image segmentation, showcasing its broad applicability\nin clinical diagnosis and treatment planning."}
{"id": "2504.11923", "pdf": "https://arxiv.org/pdf/2504.11923", "abs": "https://arxiv.org/abs/2504.11923", "authors": ["Zeyu Dai", "Shengcai Liu", "Rui He", "Jiahao Wu", "Ning Lu", "Wenqi Fan", "Qing Li", "Ke Tang"], "title": "SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Unrestricted adversarial examples (UAEs), allow the attacker to create\nnon-constrained adversarial examples without given clean samples, posing a\nsevere threat to the safety of deep learning models. Recent works utilize\ndiffusion models to generate UAEs. However, these UAEs often lack naturalness\nand imperceptibility due to simply optimizing in intermediate latent noises. In\nlight of this, we propose SemDiff, a novel unrestricted adversarial attack that\nexplores the semantic latent space of diffusion models for meaningful\nattributes, and devises a multi-attributes optimization approach to ensure\nattack success while maintaining the naturalness and imperceptibility of\ngenerated UAEs. We perform extensive experiments on four tasks on three\nhigh-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results\ndemonstrate that SemDiff outperforms state-of-the-art methods in terms of\nattack success rate and imperceptibility. The generated UAEs are natural and\nexhibit semantically meaningful changes, in accord with the attributes'\nweights. In addition, SemDiff is found capable of evading different defenses,\nwhich further validates its effectiveness and threatening."}
{"id": "2504.11942", "pdf": "https://arxiv.org/pdf/2504.11942", "abs": "https://arxiv.org/abs/2504.11942", "authors": ["Nada Shahin", "Leila Ismail"], "title": "ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation", "categories": ["cs.AI", "cs.CL", "cs.CV", "I.2.6; I.2.7; I.2.10; I.4.8; I.4.9; I.4.10"], "comment": null, "summary": "Current sign language machine translation systems rely on recognizing hand\nmovements, facial expressions and body postures, and natural language\nprocessing, to convert signs into text. Recent approaches use Transformer\narchitectures to model long-range dependencies via positional encoding.\nHowever, they lack accuracy in recognizing fine-grained, short-range temporal\ndependencies between gestures captured at high frame rates. Moreover, their\nhigh computational complexity leads to inefficient training. To mitigate these\nissues, we propose an Adaptive Transformer (ADAT), which incorporates\ncomponents for enhanced feature extraction and adaptive feature weighting\nthrough a gating mechanism to emphasize contextually relevant features while\nreducing training overhead and maintaining translation accuracy. To evaluate\nADAT, we introduce MedASL, the first public medical American Sign Language\ndataset. In sign-to-gloss-to-text experiments, ADAT outperforms the\nencoder-decoder transformer, improving BLEU-4 accuracy by 0.1% while reducing\ntraining time by 14.33% on PHOENIX14T and 3.24% on MedASL. In sign-to-text\nexperiments, it improves accuracy by 8.7% and reduces training time by 2.8% on\nPHOENIX14T and achieves 4.7% higher accuracy and 7.17% faster training on\nMedASL. Compared to encoder-only and decoder-only baselines in sign-to-text,\nADAT is at least 6.8% more accurate despite being up to 12.1% slower due to its\ndual-stream structure."}
{"id": "2504.11953", "pdf": "https://arxiv.org/pdf/2504.11953", "abs": "https://arxiv.org/abs/2504.11953", "authors": ["Daiqi Liu", "Fuxin Fan", "Andreas Maier"], "title": "Novel-view X-ray Projection Synthesis through Geometry-Integrated Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 3 figures, 1 table", "summary": "X-ray imaging plays a crucial role in the medical field, providing essential\ninsights into the internal anatomy of patients for diagnostics, image-guided\nprocedures, and clinical decision-making. Traditional techniques often require\nmultiple X-ray projections from various angles to obtain a comprehensive view,\nleading to increased radiation exposure and more complex clinical processes.\nThis paper explores an innovative approach using the DL-GIPS model, which\nsynthesizes X-ray projections from new viewpoints by leveraging a single\nexisting projection. The model strategically manipulates geometry and texture\nfeatures extracted from an initial projection to match new viewing angles. It\nthen synthesizes the final projection by merging these modified geometry\nfeatures with consistent texture information through an advanced image\ngeneration process. We demonstrate the effectiveness and broad applicability of\nthe DL-GIPS framework through lung imaging examples, highlighting its potential\nto revolutionize stereoscopic and volumetric imaging by minimizing the need for\nextensive data acquisition."}
{"id": "2504.11992", "pdf": "https://arxiv.org/pdf/2504.11992", "abs": "https://arxiv.org/abs/2504.11992", "authors": ["Pascal Schlachter", "Jonathan Fuss", "Bin Yang"], "title": "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "Submitted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025)", "summary": "A domain (distribution) shift between training and test data often hinders\nthe real-world performance of deep neural networks, necessitating unsupervised\ndomain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged\nas a solution for practical scenarios where access to source data is restricted\nand target data is received as a continuous stream. However, the open-world\nnature of many real-world applications additionally introduces category shifts\nmeaning that the source and target label spaces may differ. Online source-free\nuniversal domain adaptation (SF-UniDA) addresses this challenge. Existing\nmethods mainly rely on self-training with pseudo-labels, yet the relationship\nbetween pseudo-labeling and adaptation outcomes has not been studied yet. To\nbridge this gap, we conduct a systematic analysis through controlled\nexperiments with simulated pseudo-labeling, offering valuable insights into\npseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap\nbetween the current state-of-the-art and the upper bound of adaptation achieved\nwith perfect pseudo-labeling. Moreover, we show that a contrastive loss enables\neffective adaptation even with moderate pseudo-label accuracy, while a\ncross-entropy loss, though less robust to pseudo-label errors, achieves\nsuperior results when pseudo-labeling approaches perfection. Lastly, our\nfindings indicate that pseudo-label accuracy is in general more crucial than\nquantity, suggesting that prioritizing fewer but high-confidence pseudo-labels\nis beneficial. Overall, our study highlights the critical role of\npseudo-labeling in (online) SF-UniDA and provides actionable insights to drive\nfuture advancements in the field. Our code is available at\nhttps://github.com/pascalschlachter/PLAnalysis."}
{"id": "2504.12203", "pdf": "https://arxiv.org/pdf/2504.12203", "abs": "https://arxiv.org/abs/2504.12203", "authors": ["Levente Lippenszky", "István Megyeri", "Krisztian Koos", "Zsófia Karancsi", "Borbála Deák-Karancsi", "András Frontó", "Árpád Makk", "Attila Rádics", "Erhan Bas", "László Ruskó"], "title": "Modality-Independent Explainable Detection of Inaccurate Organ Segmentations Using Denoising Autoencoders", "categories": ["eess.IV", "cs.CV"], "comment": "Short version of this paper was accepted for poster presentation at\n  IEEE ISBI 2025", "summary": "In radiation therapy planning, inaccurate segmentations of organs at risk can\nresult in suboptimal treatment delivery, if left undetected by the clinician.\nTo address this challenge, we developed a denoising autoencoder-based method to\ndetect inaccurate organ segmentations. We applied noise to ground truth organ\nsegmentations, and the autoencoders were tasked to denoise them. Through the\napplication of our method to organ segmentations generated on both MR and CT\nscans, we demonstrated that the method is independent of imaging modality. By\nproviding reconstructions, our method offers visual information about\ninaccurate regions of the organ segmentations, leading to more explainable\ndetection of suboptimal segmentations. We compared our method to existing\napproaches in the literature and demonstrated that it achieved superior\nperformance for the majority of organs."}
{"id": "2504.12249", "pdf": "https://arxiv.org/pdf/2504.12249", "abs": "https://arxiv.org/abs/2504.12249", "authors": ["Zhijin He", "Alan B. McMillan"], "title": "Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "The application of artificial intelligence (AI) in medical imaging has\nrevolutionized diagnostic practices, enabling advanced analysis and\ninterpretation of radiological data. This study presents a comprehensive\nevaluation of radiomics-based and deep learning-based approaches for disease\ndetection in chest radiography, focusing on COVID-19, lung opacity, and viral\npneumonia. While deep learning models, particularly convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), learn directly from image data,\nradiomics-based models extract and analyze quantitative features, potentially\nproviding advantages in data-limited scenarios. This study systematically\ncompares the diagnostic accuracy and robustness of various AI models, including\nDecision Trees, Gradient Boosting, Random Forests, Support Vector Machines\n(SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against\nstate-of-the-art computer vision deep learning architectures. Performance\nmetrics across varying sample sizes reveal insights into each model's efficacy,\nhighlighting the contexts in which specific AI approaches may offer enhanced\ndiagnostic capabilities. The results aim to inform the integration of AI-driven\ndiagnostic tools in clinical practice, particularly in automated and\nhigh-throughput environments where timely, reliable diagnosis is critical. This\ncomparative study addresses an essential gap, establishing guidance for the\nselection of AI models based on clinical and operational needs."}
{"id": "2504.12299", "pdf": "https://arxiv.org/pdf/2504.12299", "abs": "https://arxiv.org/abs/2504.12299", "authors": ["Marko Tot", "Shu Ishida", "Abdelhak Lemkhenter", "David Bignell", "Pallavi Choudhury", "Chris Lovett", "Luis França", "Matheus Ribeiro Furtado de Mendonça", "Tarun Gupta", "Darren Gehring", "Sam Devlin", "Sergio Valcarcel Macua", "Raluca Georgescu"], "title": "Adapting a World Model for Trajectory Following in a 3D Game", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Imitation learning is a powerful tool for training agents by leveraging\nexpert knowledge, and being able to replicate a given trajectory is an integral\npart of it. In complex environments, like modern 3D video games, distribution\nshift and stochasticity necessitate robust approaches beyond simple action\nreplay. In this study, we apply Inverse Dynamics Models (IDM) with different\nencoders and policy heads to trajectory following in a modern 3D video game --\nBleeding Edge. Additionally, we investigate several future alignment strategies\nthat address the distribution shift caused by the aleatoric uncertainty and\nimperfections of the agent. We measure both the trajectory deviation distance\nand the first significant deviation point between the reference and the agent's\ntrajectory and show that the optimal configuration depends on the chosen\nsetting. Our results show that in a diverse data setting, a GPT-style policy\nhead with an encoder trained from scratch performs the best, DINOv2 encoder\nwith the GPT-style policy head gives the best results in the low data regime,\nand both GPT-style and MLP-style policy heads had comparable results when\npre-trained on a diverse setting and fine-tuned for a specific behaviour\nsetting."}
