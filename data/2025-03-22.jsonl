{"id": "2503.15620", "pdf": "https://arxiv.org/pdf/2503.15620", "abs": "https://arxiv.org/abs/2503.15620", "authors": ["Austin Xu", "Srijan Bansal", "Yifei Ming", "Semih Yavuz", "Shafiq Joty"], "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 13 figures, 6 tables", "summary": "The large language model (LLM)-as-judge paradigm has been used to meet the\ndemand for a cheap, reliable, and fast evaluation of model outputs during AI\nsystem development and post-deployment monitoring. While judge models -- LLMs\nfinetuned to specialize in assessing and critiquing model outputs -- have been\ntouted as general purpose evaluators, they are typically evaluated only on\nnon-contextual scenarios, such as instruction following. The omission of\ncontextual settings -- those where external information is used as context to\ngenerate an output -- is surprising given the increasing prevalence of\nretrieval-augmented generation (RAG) and summarization use cases. Contextual\nassessment is uniquely challenging, as evaluation often depends on practitioner\npriorities, leading to conditional evaluation criteria (e.g., comparing\nresponses based on factuality and then considering completeness if they are\nequally factual). To address the gap, we propose ContextualJudgeBench, a judge\nbenchmark with 2,000 challenging response pairs across eight splits inspired by\nreal-world contextual evaluation scenarios. We build our benchmark with a\nmulti-pronged data construction pipeline that leverages both existing human\nannotations and model-based perturbations. Our comprehensive study across 11\njudge models and 9 general purpose models, reveals that the contextual\ninformation and its assessment criteria present a significant challenge to even\nstate-of-the-art models. For example, OpenAI's o1, the best-performing model,\nbarely reaches 55% consistent accuracy."}
{"id": "2503.15664", "pdf": "https://arxiv.org/pdf/2503.15664", "abs": "https://arxiv.org/abs/2503.15664", "authors": ["Hisashi Johno", "Yuki Johno", "Akitomo Amakawa", "Junichi Sato", "Ryota Tozuka", "Atsushi Komaba", "Hiroaki Watanabe", "Hiroki Watanabe", "Chihiro Goto", "Hiroyuki Morisaka", "Hiroshi Onishi", "Kazunori Nakamoto"], "title": "Enhancing Pancreatic Cancer Staging with Large Language Models: The Role of Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "11 pages, 6 figures, 2 tables, 6 supplementary files", "summary": "Purpose: Retrieval-augmented generation (RAG) is a technology to enhance the\nfunctionality and reliability of large language models (LLMs) by retrieving\nrelevant information from reliable external knowledge (REK). RAG has gained\ninterest in radiology, and we previously reported the utility of NotebookLM, an\nLLM with RAG (RAG-LLM), for lung cancer staging. However, since the comparator\nLLM differed from NotebookLM's internal model, it remained unclear whether its\nadvantage stemmed from RAG or inherent model differences. To better isolate\nRAG's impact and assess its utility across different cancers, we compared\nNotebookLM with its internal LLM, Gemini 2.0 Flash, in a pancreatic cancer\nstaging experiment.\n  Materials and Methods: A summary of Japan's pancreatic cancer staging\nguidelines was used as REK. We compared three groups - REK+/RAG+ (NotebookLM\nwith REK), REK+/RAG- (Gemini 2.0 Flash with REK), and REK-/RAG- (Gemini 2.0\nFlash without REK) - in staging 100 fictional pancreatic cancer cases based on\nCT findings. Staging criteria included TNM classification, local invasion\nfactors, and resectability classification. In REK+/RAG+, retrieval accuracy was\nquantified based on the sufficiency of retrieved REK excerpts.\n  Results: REK+/RAG+ achieved a staging accuracy of 70%, outperforming\nREK+/RAG- (38%) and REK-/RAG- (35%). For TNM classification, REK+/RAG+ attained\n80% accuracy, exceeding REK+/RAG- (55%) and REK-/RAG- (50%). Additionally,\nREK+/RAG+ explicitly presented retrieved REK excerpts, achieving a retrieval\naccuracy of 92%.\n  Conclusion: NotebookLM, a RAG-LLM, outperformed its internal LLM, Gemini 2.0\nFlash, in a pancreatic cancer staging experiment, suggesting that RAG may\nimprove LLM's staging accuracy. Furthermore, its ability to retrieve and\npresent REK excerpts provides transparency for physicians, highlighting its\napplicability for clinical diagnosis and classification."}
{"id": "2503.15718", "pdf": "https://arxiv.org/pdf/2503.15718", "abs": "https://arxiv.org/abs/2503.15718", "authors": ["Mathilde Aguiar", "Pierre Zweigenbaum", "Nona Naderi"], "title": "Am I eligible? Natural Language Inference for Clinical Trial Patient Recruitment: the Patient's Point of View", "categories": ["cs.CL"], "comment": null, "summary": "Recruiting patients to participate in clinical trials can be challenging and\ntime-consuming. Usually, participation in a clinical trial is initiated by a\nhealthcare professional and proposed to the patient. Promoting clinical trials\ndirectly to patients via online recruitment might help to reach them more\nefficiently. In this study, we address the case where a patient is initiating\ntheir own recruitment process and wants to determine whether they are eligible\nfor a given clinical trial, using their own language to describe their medical\nprofile. To study whether this creates difficulties in the patient trial\nmatching process, we design a new dataset and task, Natural Language Inference\nfor Patient Recruitment (NLI4PR), in which patient language profiles must be\nmatched to clinical trials. We create it by adapting the TREC 2022 Clinical\nTrial Track dataset, which provides patients' medical profiles, and rephrasing\nthem manually using patient language. We also use the associated clinical trial\nreports where the patients are either eligible or excluded. We prompt several\nopen-source Large Language Models on our task and achieve from 56.5 to 71.8 of\nF1 score using patient language, against 64.7 to 73.1 for the same task using\nmedical language. When using patient language, we observe only a small loss in\nperformance for the best model, suggesting that having the patient as a\nstarting point could be adopted to help recruit patients for clinical trials.\nThe corpus and code bases are all freely available on our Github and\nHuggingFace repositories."}
{"id": "2503.15737", "pdf": "https://arxiv.org/pdf/2503.15737", "abs": "https://arxiv.org/abs/2503.15737", "authors": ["Heming Zhang", "Wenyu Li", "Di Huang", "Yinjie Tang", "Yixin Chen", "Philip Payne", "Fuhai Li"], "title": "KoGNER: A Novel Framework for Knowledge Graph Distillation on Biomedical Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental task in Natural Language\nProcessing (NLP) that plays a crucial role in information extraction, question\nanswering, and knowledge-based systems. Traditional deep learning-based NER\nmodels often struggle with domain-specific generalization and suffer from data\nsparsity issues. In this work, we introduce Knowledge Graph distilled for Named\nEntity Recognition (KoGNER), a novel approach that integrates Knowledge Graph\n(KG) distillation into NER models to enhance entity recognition performance.\nOur framework leverages structured knowledge representations from KGs to enrich\ncontextual embeddings, thereby improving entity classification and reducing\nambiguity in entity detection. KoGNER employs a two-step process: (1) Knowledge\nDistillation, where external knowledge sources are distilled into a lightweight\nrepresentation for seamless integration with NER models, and (2) Entity-Aware\nAugmentation, which integrates contextual embeddings that have been enriched\nwith knowledge graph information directly into GNN, thereby improving the\nmodel's ability to understand and represent entity relationships. Experimental\nresults on benchmark datasets demonstrate that KoGNER achieves state-of-the-art\nperformance, outperforming finetuned NER models and LLMs by a significant\nmargin. These findings suggest that leveraging knowledge graphs as auxiliary\ninformation can significantly improve NER accuracy, making KoGNER a promising\ndirection for future research in knowledge-aware NLP."}
{"id": "2503.15617", "pdf": "https://arxiv.org/pdf/2503.15617", "abs": "https://arxiv.org/abs/2503.15617", "authors": ["Masud Ahmed", "Zahid Hasan", "Syed Arefinul Haque", "Abu Zaher Md Faridee", "Sanjay Purushotham", "Suya You", "Nirmalya Roy"], "title": "CAM-Seg: A Continuous-valued Embedding Approach for Semantic Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional transformer-based semantic segmentation relies on quantized\nembeddings. However, our analysis reveals that autoencoder accuracy on\nsegmentation mask using quantized embeddings (e.g. VQ-VAE) is 8% lower than\ncontinuous-valued embeddings (e.g. KL-VAE). Motivated by this, we propose a\ncontinuous-valued embedding framework for semantic segmentation. By\nreformulating semantic mask generation as a continuous image-to-embedding\ndiffusion process, our approach eliminates the need for discrete latent\nrepresentations while preserving fine-grained spatial and semantic details. Our\nkey contribution includes a diffusion-guided autoregressive transformer that\nlearns a continuous semantic embedding space by modeling long-range\ndependencies in image features. Our framework contains a unified architecture\ncombining a VAE encoder for continuous feature extraction, a diffusion-guided\ntransformer for conditioned embedding generation, and a VAE decoder for\nsemantic mask reconstruction. Our setting facilitates zero-shot domain\nadaptation capabilities enabled by the continuity of the embedding space.\nExperiments across diverse datasets (e.g., Cityscapes and domain-shifted\nvariants) demonstrate state-of-the-art robustness to distribution shifts,\nincluding adverse weather (e.g., fog, snow) and viewpoint variations. Our model\nalso exhibits strong noise resilience, achieving robust performance ($\\approx$\n95% AP compared to baseline) under gaussian noise, moderate motion blur, and\nmoderate brightness/contrast variations, while experiencing only a moderate\nimpact ($\\approx$ 90% AP compared to baseline) from 50% salt and pepper noise,\nsaturation and hue shifts. Code available:\nhttps://github.com/mahmed10/CAMSS.git"}
{"id": "2503.15768", "pdf": "https://arxiv.org/pdf/2503.15768", "abs": "https://arxiv.org/abs/2503.15768", "authors": ["Alexandra DeLucia", "Mark Dredze"], "title": "Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Abstractive multi-document summarization (MDS) is the task of automatically\nsummarizing information in multiple documents, from news articles to\nconversations with multiple speakers. The training approaches for current MDS\nmodels can be grouped into four approaches: end-to-end with special\npre-training (\"direct\"), chunk-then-summarize, extract-then-summarize, and\ninference with GPT-style models. In this work, we evaluate MDS models across\ntraining approaches, domains, and dimensions (reference similarity, quality,\nand factuality), to analyze how and why models trained on one domain can fail\nto summarize documents from another (News, Science, and Conversation) in the\nzero-shot domain transfer setting. We define domain-transfer \"failure\" as a\ndecrease in factuality, higher deviation from the target, and a general\ndecrease in summary quality. In addition to exploring domain transfer for MDS\nmodels, we examine potential issues with applying popular summarization metrics\nout-of-the-box."}
{"id": "2503.15621", "pdf": "https://arxiv.org/pdf/2503.15621", "abs": "https://arxiv.org/abs/2503.15621", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE."}
{"id": "2503.15783", "pdf": "https://arxiv.org/pdf/2503.15783", "abs": "https://arxiv.org/abs/2503.15783", "authors": ["Tsunehiko Tanaka", "Edgar Simo-Serra"], "title": "Grammar and Gameplay-aligned RL for Game Description Generation with LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Game Description Generation (GDG) is the task of generating a game\ndescription written in a Game Description Language (GDL) from natural language\ntext. Previous studies have explored generation methods leveraging the\ncontextual understanding capabilities of Large Language Models (LLMs); however,\naccurately reproducing the game features of the game descriptions remains a\nchallenge. In this paper, we propose reinforcement learning-based fine-tuning\nof LLMs for GDG (RLGDG). Our training method simultaneously improves\ngrammatical correctness and fidelity to game concepts by introducing both\ngrammar rewards and concept rewards. Furthermore, we adopt a two-stage training\nstrategy where Reinforcement Learning (RL) is applied following Supervised\nFine-Tuning (SFT). Experimental results demonstrate that our proposed method\nsignificantly outperforms baseline methods using SFT alone."}
{"id": "2503.15625", "pdf": "https://arxiv.org/pdf/2503.15625", "abs": "https://arxiv.org/abs/2503.15625", "authors": ["Matthew Massey", "Abdullah-Al-Zubaer Imran"], "title": "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Surficial geologic mapping is essential for understanding Earth surface\nprocesses, addressing modern challenges such as climate change and national\nsecurity, and supporting common applications in engineering and resource\nmanagement. However, traditional mapping methods are labor-intensive, limiting\nspatial coverage and introducing potential biases. To address these\nlimitations, we introduce EarthScape, a novel, AI-ready multimodal dataset\nspecifically designed for surficial geologic mapping and Earth surface\nanalysis. EarthScape integrates high-resolution aerial RGB and near-infrared\n(NIR) imagery, digital elevation models (DEM), multi-scale DEM-derived terrain\nfeatures, and hydrologic and infrastructure vector data. The dataset provides\ndetailed annotations for seven distinct surficial geologic classes encompassing\nvarious geological processes. We present a comprehensive data processing\npipeline using open-sourced raw data and establish baseline benchmarks using\ndifferent spatial modalities to demonstrate the utility of EarthScape. As a\nliving dataset with a vision for expansion, EarthScape bridges the gap between\ncomputer vision and Earth sciences, offering a valuable resource for advancing\nresearch in multimodal learning, geospatial analysis, and geological mapping.\nOur code is available at https://github.com/masseygeo/earthscape."}
{"id": "2503.15837", "pdf": "https://arxiv.org/pdf/2503.15837", "abs": "https://arxiv.org/abs/2503.15837", "authors": ["Shangqing Zhao", "Yuhao Zhou", "Yupei Ren", "Zhe Chen", "Chenghao Jia", "Fang Zhe", "Zhaogaung Long", "Shu Liu", "Man Lan"], "title": "Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation", "categories": ["cs.CL", "cs.AI"], "comment": "working in progress", "summary": "Ancient Chinese text processing presents unique challenges for large language\nmodels (LLMs) due to its distinct linguistic features, complex structural\nconstraints, and rich cultural context. While existing benchmarks have\nprimarily focused on evaluating comprehension through multiple-choice\nquestions, there remains a critical gap in assessing models' generative\ncapabilities in classical Chinese. We introduce F\\`ux\\`i, a comprehensive\nbenchmark that evaluates both understanding and generation capabilities across\n21 diverse tasks. Our benchmark distinguishes itself through three key\ncontributions: (1) balanced coverage of both comprehension and generation\ntasks, including novel tasks like poetry composition and couplet completion,\n(2) specialized evaluation metrics designed specifically for classical Chinese\ntext generation, combining rule-based verification with fine-tuned LLM\nevaluators, and (3) a systematic assessment framework that considers both\nlinguistic accuracy and cultural authenticity. Through extensive evaluation of\nstate-of-the-art LLMs, we reveal significant performance gaps between\nunderstanding and generation tasks, with models achieving promising results in\ncomprehension but struggling considerably in generation tasks, particularly\nthose requiring deep cultural knowledge and adherence to classical formats. Our\nfindings highlight the current limitations in ancient Chinese text processing\nand provide insights for future model development. The benchmark, evaluation\ntoolkit, and baseline results are publicly available to facilitate research in\nthis domain."}
{"id": "2503.15633", "pdf": "https://arxiv.org/pdf/2503.15633", "abs": "https://arxiv.org/abs/2503.15633", "authors": ["Amélie Royer", "Moritz Böhle", "Gabriel de Marmiesse", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez", "Patrick Pérez"], "title": "Vision-Speech Models: Teaching Speech Models to Converse about Images", "categories": ["cs.CV"], "comment": null, "summary": "The recent successes of Vision-Language models raise the question of how to\nequivalently imbue a pretrained speech model with vision understanding, an\nimportant milestone towards building a multimodal speech model able to freely\nconverse about images. Building such a conversational Vision-Speech model\nbrings its unique challenges: (i) paired image-speech datasets are much scarcer\nthan their image-text counterparts, (ii) ensuring real-time latency at\ninference is crucial thus bringing compute and memory constraints, and (iii)\nthe model should preserve prosodic features (e.g., speaker tone) which cannot\nbe inferred from text alone. In this work, we introduce MoshiVis, augmenting a\nrecent dialogue speech LLM, Moshi, with visual inputs through lightweight\nadaptation modules. An additional dynamic gating mechanism enables the model to\nmore easily switch between the visual inputs and unrelated conversation topics.\nTo reduce training costs, we design a simple one-stage, parameter-efficient\nfine-tuning pipeline in which we leverage a mixture of image-text (i.e.,\n\"speechless\") and image-speech samples. We evaluate the model on downstream\nvisual understanding tasks with both audio and text prompts, and report\nqualitative samples of interactions with MoshiVis. Our inference code will be\nmade available, as well as the image-speech data used for audio evaluation."}
{"id": "2503.15850", "pdf": "https://arxiv.org/pdf/2503.15850", "abs": "https://arxiv.org/abs/2503.15850", "authors": ["Xiaoou Liu", "Tiejin Chen", "Longchao Da", "Chacha Chen", "Zhen Lin", "Hua Wei"], "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in text generation, reasoning, and\ndecision-making, enabling their adoption in high-stakes domains such as\nhealthcare, law, and transportation. However, their reliability is a major\nconcern, as they often produce plausible but incorrect responses. Uncertainty\nquantification (UQ) enhances trustworthiness by estimating confidence in\noutputs, enabling risk mitigation and selective prediction. However,\ntraditional UQ methods struggle with LLMs due to computational constraints and\ndecoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources,\nsuch as input ambiguity, reasoning path divergence, and decoding stochasticity,\nthat extend beyond classical aleatoric and epistemic uncertainty. To address\nthis, we introduce a new taxonomy that categorizes UQ methods based on\ncomputational efficiency and uncertainty dimensions (input, reasoning,\nparameter, and prediction uncertainty). We evaluate existing techniques, assess\ntheir real-world applicability, and identify open challenges, emphasizing the\nneed for scalable, interpretable, and robust UQ approaches to enhance LLM\nreliability."}
{"id": "2503.15639", "pdf": "https://arxiv.org/pdf/2503.15639", "abs": "https://arxiv.org/abs/2503.15639", "authors": ["Ritabrata Chakraborty", "Shivakumara Palaiahnakote", "Umapada Pal", "Cheng-Lin Liu"], "title": "A Context-Driven Training-Free Network for Lightweight Scene Text Segmentation and Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modern scene text recognition systems often depend on large end-to-end\narchitectures that require extensive training and are prohibitively expensive\nfor real-time scenarios. In such cases, the deployment of heavy models becomes\nimpractical due to constraints on memory, computational resources, and latency.\nTo address these challenges, we propose a novel, training-free plug-and-play\nframework that leverages the strengths of pre-trained text recognizers while\nminimizing redundant computations. Our approach uses context-based\nunderstanding and introduces an attention-based segmentation stage, which\nrefines candidate text regions at the pixel level, improving downstream\nrecognition. Instead of performing traditional text detection that follows a\nblock-level comparison between feature map and source image and harnesses\ncontextual information using pretrained captioners, allowing the framework to\ngenerate word predictions directly from scene context.Candidate texts are\nsemantically and lexically evaluated to get a final score. Predictions that\nmeet or exceed a pre-defined confidence threshold bypass the heavier process of\nend-to-end text STR profiling, ensuring faster inference and cutting down on\nunnecessary computations. Experiments on public benchmarks demonstrate that our\nparadigm achieves performance on par with state-of-the-art systems, yet\nrequires substantially fewer resources."}
{"id": "2503.15879", "pdf": "https://arxiv.org/pdf/2503.15879", "abs": "https://arxiv.org/abs/2503.15879", "authors": ["DongGeon Lee", "Ahjeong Park", "Hyeri Lee", "Hyeonseo Nam", "Yunho Maeng"], "title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to NAACL 2025 SRW", "summary": "Non-factoid question-answering (NFQA) poses a significant challenge due to\nits open-ended nature, diverse intents, and the need for multi-aspect\nreasoning, which renders conventional factoid QA approaches, including\nretrieval-augmented generation (RAG), inadequate. Unlike factoid questions,\nnon-factoid questions (NFQs) lack definitive answers and require synthesizing\ninformation from multiple sources across various reasoning dimensions. To\naddress these limitations, we introduce Typed-RAG, a type-aware multi-aspect\ndecomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies\nNFQs into distinct types -- such as debate, experience, and comparison -- and\napplies aspect-based decomposition to refine retrieval and generation\nstrategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and\naggregating the results, Typed-RAG generates more informative and contextually\nrelevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark\ndataset covering diverse NFQ types. Experimental results demonstrate that\nTyped-RAG outperforms baselines, thereby highlighting the importance of\ntype-aware decomposition for effective retrieval and generation in NFQA. Our\ncode and dataset are available at\n\\href{https://github.com/TeamNLP/Typed-RAG}{https://github.com/TeamNLP/Typed-RAG}."}
{"id": "2503.15647", "pdf": "https://arxiv.org/pdf/2503.15647", "abs": "https://arxiv.org/abs/2503.15647", "authors": ["Jumanh Atoum", "Garrison L. H. Johnston", "Nabil Simaan", "Jie Ying Wu"], "title": "Multi-Modal Gesture Recognition from Video and Surgical Tool Pose Information via Motion Invariants", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recognizing surgical gestures in real-time is a stepping stone towards\nautomated activity recognition, skill assessment, intra-operative assistance,\nand eventually surgical automation. The current robotic surgical systems\nprovide us with rich multi-modal data such as video and kinematics. While some\nrecent works in multi-modal neural networks learn the relationships between\nvision and kinematics data, current approaches treat kinematics information as\nindependent signals, with no underlying relation between tool-tip poses.\nHowever, instrument poses are geometrically related, and the underlying\ngeometry can aid neural networks in learning gesture representation. Therefore,\nwe propose combining motion invariant measures (curvature and torsion) with\nvision and kinematics data using a relational graph network to capture the\nunderlying relations between different data streams. We show that gesture\nrecognition improves when combining invariant signals with tool position,\nachieving 90.3\\% frame-wise accuracy on the JIGSAWS suturing dataset. Our\nresults show that motion invariant signals coupled with position are better\nrepresentations of gesture motion compared to traditional position and\nquaternion representations. Our results highlight the need for geometric-aware\nmodeling of kinematics for gesture recognition."}
{"id": "2503.15888", "pdf": "https://arxiv.org/pdf/2503.15888", "abs": "https://arxiv.org/abs/2503.15888", "authors": ["Baolong Bi", "Shenghua Liu", "Yiwei Wang", "Yilong Xu", "Junfeng Fang", "Lingrui Mei", "Xueqi Cheng"], "title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucinations in Large\nLanguage Models (LLMs) by integrating external knowledge. However, conflicts\nbetween parametric knowledge and retrieved context pose challenges,\nparticularly when retrieved information is unreliable or the model's internal\nknowledge is outdated. In such cases, LLMs struggle to determine whether to\nrely more on their own parameters or the conflicted context. To address this,\nwe propose **CK-PLUG**, a plug-and-play method for controlling LLMs' reliance\non parametric and contextual knowledge. We introduce a novel knowledge\nconsistency metric, Confidence Gain, which detects knowledge conflicts by\nmeasuring entropy shifts in token probability distributions after context\ninsertion. CK-PLUG then enables fine-grained control over knowledge preference\nby adjusting the probability distribution of tokens with negative confidence\ngain through a single tuning parameter. Experiments demonstrate CK-PLUG's\nability to significantly regulate knowledge reliance in counterfactual RAG\nscenarios while maintaining generation fluency and knowledge accuracy. For\ninstance, on Llama3-8B, memory recall (MR) of RAG response can be adjusted\nwithin a broad range (9.9%-71.9%), compared to the baseline of 42.1%. Moreover,\nCK-PLUG supports adaptive control based on the model's confidence in both\ninternal and external knowledge, achieving consistent performance improvements\nacross various general RAG tasks. Our code is available at:\n$\\href{https://github.com/byronBBL/CK-PLUG}{\\text{this https URL}}$."}
{"id": "2503.15653", "pdf": "https://arxiv.org/pdf/2503.15653", "abs": "https://arxiv.org/abs/2503.15653", "authors": ["Miguel Ureña Pliego", "Rubén Martínez Marín", "Nianfang Shi", "Takeru Shibayama", "Ulrich Leth", "Miguel Marchamalo Sacristán"], "title": "Transport-Related Surface Detection with Machine Learning: Analyzing Temporal Trends in Madrid and Vienna", "categories": ["cs.CV"], "comment": "Preprint", "summary": "This study explores the integration of machine learning into urban aerial\nimage analysis, with a focus on identifying infrastructure surfaces for cars\nand pedestrians and analyzing historical trends. It emphasizes the transition\nfrom convolutional architectures to transformer-based pre-trained models,\nunderscoring their potential in global geospatial analysis. A workflow is\npresented for automatically generating geospatial datasets, enabling the\ncreation of semantic segmentation datasets from various sources, including\nWMS/WMTS links, vectorial cartography, and OpenStreetMap (OSM) overpass-turbo\nrequests. The developed code allows a fast dataset generation process for\ntraining machine learning models using openly available data without manual\nlabelling. Using aerial imagery and vectorial data from the respective\ngeographical offices of Madrid and Vienna, two datasets were generated for car\nand pedestrian surface detection. A transformer-based model was trained and\nevaluated for each city, demonstrating good accuracy values. The historical\ntrend analysis involved applying the trained model to earlier images predating\nthe availability of vectorial data 10 to 20 years, successfully identifying\ntemporal trends in infrastructure for pedestrians and cars across different\ncity areas. This technique is applicable for municipal governments to gather\nvaluable data at a minimal cost."}
{"id": "2503.15904", "pdf": "https://arxiv.org/pdf/2503.15904", "abs": "https://arxiv.org/abs/2503.15904", "authors": ["Evan Chen", "Run-Jun Zhan", "Yan-Bai Lin", "Hung-Hsuan Chen"], "title": "From Structured Prompts to Open Narratives: Measuring Gender Bias in LLMs Through Open-Ended Storytelling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet concerns persist regarding their tendency to reflect or amplify social\nbiases present in their training data. This study introduces a novel evaluation\nframework to uncover gender biases in LLMs, focusing on their occupational\nnarratives. Unlike previous methods relying on structured scenarios or\ncarefully crafted prompts, our approach leverages free-form storytelling to\nreveal biases embedded in the models. Systematic analyses show an\noverrepresentation of female characters across occupations in six widely used\nLLMs. Additionally, our findings reveal that LLM-generated occupational gender\nrankings align more closely with human stereotypes than actual labor\nstatistics. These insights underscore the need for balanced mitigation\nstrategies to ensure fairness while avoiding the reinforcement of new\nstereotypes."}
{"id": "2503.15661", "pdf": "https://arxiv.org/pdf/2503.15661", "abs": "https://arxiv.org/abs/2503.15661", "authors": ["Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Juan A. Rodriguez", "Montek Kalsi", "Rabiul Awal", "Nicolas Chapados", "M. Tamer Özsu", "Aishwarya Agrawal", "David Vazquez", "Christopher Pal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks."}
{"id": "2503.15924", "pdf": "https://arxiv.org/pdf/2503.15924", "abs": "https://arxiv.org/abs/2503.15924", "authors": ["Peiyi Lin", "Fukai Zhang", "Kai Niu", "Hao Fu"], "title": "Towards Automatic Continual Learning: A Self-Adaptive Framework for Continual Instruction Tuning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Continual instruction tuning enables large language models (LLMs) to learn\nincrementally while retaining past knowledge, whereas existing methods\nprimarily focus on how to retain old knowledge rather than on selecting which\nnew knowledge to learn. In domain-specific contexts, maintaining data quality\nand managing system constraints remain key challenges. To address these issues,\nwe propose an automated continual instruction tuning framework that dynamically\nfilters incoming data, which identify and reduce redundant data across\nsuccessive updates. Our approach utilizes a small proxy model for efficient\nperplexity-based filtering, and updates the proxy to ensure that the filtering\ncriteria remain aligned with the evolving state of the deployed model. Compared\nto existing static data selection methods, our framework can effectively handle\nincrementally acquired data and shifting distributions. Additionally, it\naddresses practical deployment challenges by enabling seamless model updates,\nsupporting version rollback and incorporating automatic checkpoint evaluation.\nWe evaluated the system in real-world medical scenarios. It reduced\ncomputational costs by 66.7% and improved model performance, and achieved\nautonomous updates, thus demonstrating its effectiveness for automatic\ncontinual instruction tuning."}
{"id": "2503.15666", "pdf": "https://arxiv.org/pdf/2503.15666", "abs": "https://arxiv.org/abs/2503.15666", "authors": ["Kyle Vedder"], "title": "Toward Scalable, Flexible Scene Flow for Point Clouds", "categories": ["cs.CV"], "comment": "PhD Thesis", "summary": "Scene flow estimation is the task of describing 3D motion between temporally\nsuccessive observations. This thesis aims to build the foundation for building\nscene flow estimators with two important properties: they are scalable, i.e.\nthey improve with access to more data and computation, and they are flexible,\ni.e. they work out-of-the-box in a variety of domains and on a variety of\nmotion patterns without requiring significant hyperparameter tuning.\n  In this dissertation we present several concrete contributions towards this.\nIn Chapter 1 we contextualize scene flow and its prior methods. In Chapter 2 we\npresent a blueprint to build and scale feedforward scene flow estimators\nwithout requiring expensive human annotations via large scale distillation from\npseudolabels provided by strong unsupervised test-time optimization methods. In\nChapter 3 we introduce a benchmark to better measure estimate quality across\ndiverse object types, better bringing into focus what we care about and expect\nfrom scene flow estimators, and use this benchmark to host a public challenge\nthat produced significant progress. In Chapter 4 we present a state-of-the-art\nunsupervised scene flow estimator that introduces a new, full sequence problem\nformulation and exhibits great promise in adjacent domains like 3D point\ntracking. Finally, in Chapter 5 I philosophize about what's next for scene flow\nand its potential future broader impacts."}
{"id": "2503.15944", "pdf": "https://arxiv.org/pdf/2503.15944", "abs": "https://arxiv.org/abs/2503.15944", "authors": ["Jinyi Liu", "Yan Zheng", "Rong Cheng", "Qiyu Wu", "Wei Guo", "Fei Ni", "Hebin Liang", "Yifu Yuan", "Hangyu Mao", "Fuzheng Zhang", "Jianye Hao"], "title": "From Chaos to Order: The Atomic Reasoner Framework for Fine-grained Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have shown remarkable\nprogress, yet their capacity for logical ``slow-thinking'' reasoning persists\nas a critical research frontier. Current inference scaling paradigms suffer\nfrom two fundamental constraints: fragmented thought flows compromising logical\ncoherence, and intensively computational complexity that escalates with search\nspace dimensions. To overcome these limitations, we present \\textbf{Atomic\nReasoner} (\\textbf{AR}), a cognitive inference strategy that enables\nfine-grained reasoning through systematic atomic-level operations. AR\ndecomposes the reasoning process into atomic cognitive units, employing a\ncognitive routing mechanism to dynamically construct reasoning representations\nand orchestrate inference pathways. This systematic methodology implements\nstepwise, structured cognition, which ensures logical coherence while\nsignificantly reducing cognitive load, effectively simulating the cognitive\npatterns observed in human deep thinking processes. Extensive experimental\nresults demonstrate AR's superior reasoning capabilities without the\ncomputational burden of exhaustive solution searches, particularly excelling in\nlinguistic logic puzzles. These findings substantiate AR's effectiveness in\nenhancing LLMs' capacity for robust, long-sequence logical reasoning and\ndeliberation."}
{"id": "2503.15667", "pdf": "https://arxiv.org/pdf/2503.15667", "abs": "https://arxiv.org/abs/2503.15667", "authors": ["Yuming Gu", "Phong Tran", "Yujian Zheng", "Hongyi Xu", "Heyuan Li", "Adilbek Karmanov", "Hao Li"], "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis", "categories": ["cs.CV"], "comment": "Page:https://freedomgu.github.io/DiffPortrait360\n  Code:https://github.com/FreedomGu/DiffPortrait360/", "summary": "Generating high-quality 360-degree views of human heads from single-view\nimages is essential for enabling accessible immersive telepresence applications\nand scalable personalized content creation. While cutting-edge methods for full\nhead generation are limited to modeling realistic human heads, the latest\ndiffusion-based approaches for style-omniscient head synthesis can produce only\nfrontal views and struggle with view consistency, preventing their conversion\ninto true 3D models for rendering from arbitrary angles. We introduce a novel\napproach that generates fully consistent 360-degree head views, accommodating\nhuman, stylized, and anthropomorphic forms, including accessories like glasses\nand hats. Our method builds on the DiffPortrait3D framework, incorporating a\ncustom ControlNet for back-of-head detail generation and a dual appearance\nmodule to ensure global front-back consistency. By training on continuous view\nsequences and integrating a back reference image, our approach achieves robust,\nlocally continuous view synthesis. Our model can be used to produce\nhigh-quality neural radiance fields (NeRFs) for real-time, free-viewpoint\nrendering, outperforming state-of-the-art methods in object synthesis and\n360-degree head generation for very challenging input portraits."}
{"id": "2503.15952", "pdf": "https://arxiv.org/pdf/2503.15952", "abs": "https://arxiv.org/abs/2503.15952", "authors": ["Chen Li", "Nazhou Liu", "Kai Yang"], "title": "Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning", "categories": ["cs.CL"], "comment": "This is an unfinished version and will be updated. We aim to share\n  some findings", "summary": "Since DeepSeek-R1 popularized, Group Relative Policy Optimization (GRPO) has\nbecome the core part of Reasoning LLMs training. However, we find some\ndeficiency that influences RL stability and inference efficiency. Thus, we\npropose Adaptive Group Policy Optimization (AGPO) which contains two simple but\neffective modifications: a revised advantage estimation method to mitigate\nzero-variance situations; a length-based reward, incentivizing the model to\navoid overthinking. The experiments demonstrate our methods achieve more stable\ntraining and comparable or superior performance with significantly fewer tokens\nin reasoning steps."}
{"id": "2503.15671", "pdf": "https://arxiv.org/pdf/2503.15671", "abs": "https://arxiv.org/abs/2503.15671", "authors": ["Arindam Dutta", "Meng Zheng", "Zhongpai Gao", "Benjamin Planche", "Anwesha Choudhuri", "Terrence Chen", "Amit K. Roy-Chowdhury", "Ziyan Wu"], "title": "CHROME: Clothed Human Reconstruction with Occlusion-Resilience and Multiview-Consistency from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing clothed humans from a single image is a fundamental task in\ncomputer vision with wide-ranging applications. Although existing monocular\nclothed human reconstruction solutions have shown promising results, they often\nrely on the assumption that the human subject is in an occlusion-free\nenvironment. Thus, when encountering in-the-wild occluded images, these\nalgorithms produce multiview inconsistent and fragmented reconstructions.\nAdditionally, most algorithms for monocular 3D human reconstruction leverage\ngeometric priors such as SMPL annotations for training and inference, which are\nextremely challenging to acquire in real-world applications. To address these\nlimitations, we propose CHROME: Clothed Human Reconstruction with\nOcclusion-Resilience and Multiview-ConsistEncy from a Single Image, a novel\npipeline designed to reconstruct occlusion-resilient 3D humans with multiview\nconsistency from a single occluded image, without requiring either ground-truth\ngeometric prior annotations or 3D supervision. Specifically, CHROME leverages a\nmultiview diffusion model to first synthesize occlusion-free human images from\nthe occluded input, compatible with off-the-shelf pose control to explicitly\nenforce cross-view consistency during synthesis. A 3D reconstruction model is\nthen trained to predict a set of 3D Gaussians conditioned on both the occluded\ninput and synthesized views, aligning cross-view details to produce a cohesive\nand accurate 3D representation. CHROME achieves significant improvements in\nterms of both novel view synthesis (upto 3 db PSNR) and geometric\nreconstruction under challenging conditions."}
{"id": "2503.15979", "pdf": "https://arxiv.org/pdf/2503.15979", "abs": "https://arxiv.org/abs/2503.15979", "authors": ["Navneet Agarwal", "Kairit Sirts"], "title": "Exploratory Study into Relations between Cognitive Distortions and Emotional Appraisals", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, there has been growing interest in studying cognitive\ndistortions and emotional appraisals from both computational and psychological\nperspectives. Despite considerable similarities between emotional reappraisal\nand cognitive reframing as emotion regulation techniques, these concepts have\nlargely been examined in isolation. This research explores the relationship\nbetween cognitive distortions and emotional appraisal dimensions, examining\ntheir potential connections and relevance for future interdisciplinary studies.\nUnder this pretext, we conduct an exploratory computational study, aimed at\ninvestigating the relationship between cognitive distortion and emotional\nappraisals. We show that the patterns of statistically significant\nrelationships between cognitive distortions and appraisal dimensions vary\nacross different distortion categories, giving rise to distinct appraisal\nprofiles for individual distortion classes. Additionally, we analyze the impact\nof cognitive restructuring on appraisal dimensions, exemplifying the emotion\nregulation aspect of cognitive restructuring."}
{"id": "2503.15672", "pdf": "https://arxiv.org/pdf/2503.15672", "abs": "https://arxiv.org/abs/2503.15672", "authors": ["William Ljungbergh", "Adam Lilja", "Adam Tonderski. Arvid Laveno Ling", "Carl Lindström", "Willem Verbeke", "Junsheng Fu", "Christoffer Petersson", "Lars Hammarstrand", "Michael Felsberg"], "title": "GASP: Unifying Geometric and Semantic Self-Supervised Pre-training for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Self-supervised pre-training based on next-token prediction has enabled large\nlanguage models to capture the underlying structure of text, and has led to\nunprecedented performance on a large array of tasks when applied at scale.\nSimilarly, autonomous driving generates vast amounts of spatiotemporal data,\nalluding to the possibility of harnessing scale to learn the underlying\ngeometric and semantic structure of the environment and its evolution over\ntime. In this direction, we propose a geometric and semantic self-supervised\npre-training method, GASP, that learns a unified representation by predicting,\nat any queried future point in spacetime, (1) general occupancy, capturing the\nevolving structure of the 3D scene; (2) ego occupancy, modeling the ego vehicle\npath through the environment; and (3) distilled high-level features from a\nvision foundation model. By modeling geometric and semantic 4D occupancy fields\ninstead of raw sensor measurements, the model learns a structured,\ngeneralizable representation of the environment and its evolution through time.\nWe validate GASP on multiple autonomous driving benchmarks, demonstrating\nsignificant improvements in semantic occupancy forecasting, online mapping, and\nego trajectory prediction. Our results demonstrate that continuous 4D geometric\nand semantic occupancy prediction provides a scalable and effective\npre-training paradigm for autonomous driving. For code and additional\nvisualizations, see \\href{https://research.zenseact.com/publications/gasp/."}
{"id": "2503.15983", "pdf": "https://arxiv.org/pdf/2503.15983", "abs": "https://arxiv.org/abs/2503.15983", "authors": ["Tony Zhang", "Rickard Brännvall"], "title": "InhibiDistilbert: Knowledge Distillation for a ReLU and Addition-based Transformer", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68Q32 (Secondary)", "I.2.6; I.2.7; I.5.1"], "comment": "7 pages, 2 tables", "summary": "This work explores optimizing transformer-based language models by\nintegrating model compression techniques with inhibitor attention, a novel\nalternative attention mechanism. Inhibitor attention employs Manhattan\ndistances and ReLU activations instead of the matrix multiplications and\nsoftmax activation of the conventional scaled dot-product attention. This shift\noffers potential computational and energy savings while maintaining model\neffectiveness. We propose further adjustments to improve the inhibitor\nmechanism's training efficiency and evaluate its performance on the DistilBERT\narchitecture. Our knowledge distillation experiments indicate that the modified\ninhibitor transformer model can achieve competitive performance on standard NLP\nbenchmarks, including General Language Understanding Evaluation (GLUE) and\nsentiment analysis tasks."}
{"id": "2503.15676", "pdf": "https://arxiv.org/pdf/2503.15676", "abs": "https://arxiv.org/abs/2503.15676", "authors": ["Cédric Vincent", "Taehyoung Kim", "Henri Meeß"], "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation from RGB cameras is essential to the perception of\nautonomous flying vehicles. The stability of predictions through the captured\nvideos is paramount to their reliability and, by extension, to the\ntrustworthiness of the agents. In this paper, we propose a lightweight video\nsemantic segmentation approach-suited to onboard real-time inference-achieving\nhigh temporal consistency on aerial data through Semantic Similarity\nPropagation across frames. SSP temporally propagates the predictions of an\nefficient image segmentation model with global registration alignment to\ncompensate for camera movements. It combines the current estimation and the\nprior prediction with linear interpolation using weights computed from the\nfeatures similarities of the two frames. Because data availability is a\nchallenge in this domain, we propose a consistency-aware Knowledge Distillation\ntraining procedure for sparsely labeled datasets with few annotations. Using a\nlarge image segmentation model as a teacher to train the efficient SSP, we\nleverage the strong correlations between labeled and unlabeled frames in the\nsame training videos to obtain high-quality supervision on all frames. KD-SSP\nobtains a significant temporal consistency increase over the base image\nsegmentation model of 12.5% and 6.7% TC on UAVid and RuralScapes respectively,\nwith higher accuracy and comparable inference speed. On these aerial datasets,\nKD-SSP provides a superior segmentation quality and inference speed trade-off\nthan other video methods proposed for general applications and shows\nconsiderably higher consistency. The code will be made publicly available upon\nacceptance."}
{"id": "2503.15990", "pdf": "https://arxiv.org/pdf/2503.15990", "abs": "https://arxiv.org/abs/2503.15990", "authors": ["Langming Liu", "Haibin Chen", "Yuhao Wang", "Yujin Yuan", "Shilei Liu", "Wenbo Su", "Xiangyu Zhao", "Bo Zheng"], "title": "ECKGBench: Benchmarking Large Language Models in E-commerce Leveraging Knowledge Graph", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their capabilities across\nvarious NLP tasks. Their potential in e-commerce is also substantial, evidenced\nby practical implementations such as platform search, personalized\nrecommendations, and customer service. One primary concern associated with LLMs\nis their factuality (e.g., hallucination), which is urgent in e-commerce due to\nits significant impact on user experience and revenue. Despite some methods\nproposed to evaluate LLMs' factuality, issues such as lack of reliability, high\nconsumption, and lack of domain expertise leave a gap between effective\nassessment in e-commerce. To bridge the evaluation gap, we propose ECKGBench, a\ndataset specifically designed to evaluate the capacities of LLMs in e-commerce\nknowledge. Specifically, we adopt a standardized workflow to automatically\ngenerate questions based on a large-scale knowledge graph, guaranteeing\nsufficient reliability. We employ the simple question-answering paradigm,\nsubstantially improving the evaluation efficiency by the least input and output\ntokens. Furthermore, we inject abundant e-commerce expertise in each evaluation\nstage, including human annotation, prompt design, negative sampling, and\nverification. Besides, we explore the LLMs' knowledge boundaries in e-commerce\nfrom a novel perspective. Through comprehensive evaluations of several advanced\nLLMs on ECKGBench, we provide meticulous analysis and insights into leveraging\nLLMs for e-commerce."}
{"id": "2503.15683", "pdf": "https://arxiv.org/pdf/2503.15683", "abs": "https://arxiv.org/abs/2503.15683", "authors": ["Benidir Yanis", "Gonthier Nicolas", "Mallet Clement"], "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generation", "categories": ["cs.CV"], "comment": null, "summary": "Bi-temporal change detection at scale based on Very High Resolution (VHR)\nimages is crucial for Earth monitoring. This remains poorly addressed so far:\nmethods either require large volumes of annotated data (semantic case), or are\nlimited to restricted datasets (binary set-ups). Most approaches do not exhibit\nthe versatility required for temporal and spatial adaptation: simplicity in\narchitecture design and pretraining on realistic and comprehensive datasets.\nSynthetic datasets are the key solution but still fail to handle complex and\ndiverse scenes. In this paper, we present HySCDG a generative pipeline for\ncreating a large hybrid semantic change detection dataset that contains both\nreal VHR images and inpainted ones, along with land cover semantic map at both\ndates and the change map. Being semantically and spatially guided, HySCDG\ngenerates realistic images, leading to a comprehensive and hybrid\ntransfer-proof dataset FSC-180k. We evaluate FSC-180k on five change detection\ncases (binary and semantic), from zero-shot to mixed and sequential training,\nand also under low data regime training. Experiments demonstrate that\npretraining on our hybrid dataset leads to a significant performance boost,\noutperforming SyntheWorld, a fully synthetic dataset, in every configuration.\nAll codes, models, and data are available here:\n$\\href{https://yb23.github.io/projects/cywd/}{https://yb23.github.io/projects/cywd/}$."}
{"id": "2503.16022", "pdf": "https://arxiv.org/pdf/2503.16022", "abs": "https://arxiv.org/abs/2503.16022", "authors": ["Mario Sanz-Guerrero", "Katharina von der Wense"], "title": "Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to the 6th Workshop on Insights from Negative Results in NLP\n  at NAACL 2025", "summary": "In-context learning (ICL) has transformed the use of large language models\n(LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled\nexamples without finetuning. Despite its effectiveness, ICL is prone to errors,\nespecially for challenging examples. With the goal of improving the performance\nof ICL, we propose corrective in-context learning (CICL), an approach that\nincorporates a model's incorrect predictions alongside ground truth corrections\ninto the prompt, aiming to enhance classification accuracy through\nself-correction. However, contrary to our hypothesis, extensive experiments on\ntext classification tasks demonstrate that CICL consistently underperforms\nstandard ICL, with performance degrading as the proportion of corrections in\nthe prompt increases. Our findings indicate that CICL introduces confusion by\ndisrupting the model's task understanding, rather than refining its\npredictions. Additionally, we observe that presenting harder examples in\nstandard ICL does not improve performance, suggesting that example difficulty\nalone may not be a reliable criterion for effective selection. By presenting\nthese negative results, we provide important insights into the limitations of\nself-corrective mechanisms in LLMs and offer directions for future research."}
{"id": "2503.15686", "pdf": "https://arxiv.org/pdf/2503.15686", "abs": "https://arxiv.org/abs/2503.15686", "authors": ["Jiaqi Liu", "Jichao Zahng", "Paolo Rota", "Nicu Sebe"], "title": "Multi-focal Conditioned Latent Diffusion for Person Image Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025 Accepted", "summary": "The Latent Diffusion Model (LDM) has demonstrated strong capabilities in\nhigh-resolution image generation and has been widely employed for Pose-Guided\nPerson Image Synthesis (PGPIS), yielding promising results. However, the\ncompression process of LDM often results in the deterioration of details,\nparticularly in sensitive areas such as facial features and clothing textures.\nIn this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD)\nmethod to address these limitations by conditioning the model on disentangled,\npose-invariant features from these sensitive regions. Our approach utilizes a\nmulti-focal condition aggregation module, which effectively integrates facial\nidentity and texture-specific information, enhancing the model's ability to\nproduce appearance realistic and identity-consistent images. Our method\ndemonstrates consistent identity and appearance generation on the DeepFashion\ndataset and enables flexible person image editing due to its generation\nconsistency. The code is available at https://github.com/jqliu09/mcld."}
{"id": "2503.16024", "pdf": "https://arxiv.org/pdf/2503.16024", "abs": "https://arxiv.org/abs/2503.16024", "authors": ["Ruihan Yang", "Fanghua Ye", "Jian Li", "Siyu Yuan", "Yikai Zhang", "Zhaopeng Tu", "Xiaolong Li", "Deqing Yang"], "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have recently transformed from text-based\nassistants to autonomous agents capable of planning, reasoning, and iteratively\nimproving their actions. While numerical reward signals and verifiers can\neffectively rank candidate actions, they often provide limited contextual\nguidance. In contrast, natural language feedback better aligns with the\ngenerative capabilities of LLMs, providing richer and more actionable\nsuggestions. However, parsing and implementing this feedback effectively can be\nchallenging for LLM-based agents. In this work, we introduce Critique-Guided\nImprovement (CGI), a novel two-player framework, comprising an actor model that\nexplores an environment and a critic model that generates detailed nature\nlanguage feedback. By training the critic to produce fine-grained assessments\nand actionable revisions, and the actor to utilize these critiques, our\napproach promotes more robust exploration of alternative strategies while\navoiding local optima. Experiments in three interactive environments show that\nCGI outperforms existing baselines by a substantial margin. Notably, even a\nsmall critic model surpasses GPT-4 in feedback quality. The resulting actor\nachieves state-of-the-art performance, demonstrating the power of explicit\niterative guidance to enhance decision-making in LLM-based agents."}
{"id": "2503.15697", "pdf": "https://arxiv.org/pdf/2503.15697", "abs": "https://arxiv.org/abs/2503.15697", "authors": ["Panagiota Moraiti", "Efstathios Karypidis"], "title": "Technical Report for the 5th CLVision Challenge at CVPR: Addressing the Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution", "categories": ["cs.CV"], "comment": null, "summary": "This paper outlines our approach to the 5th CLVision challenge at CVPR, which\naddresses the Class-Incremental with Repetition (CIR) scenario. In contrast to\ntraditional class incremental learning, this novel setting introduces unique\nchallenges and research opportunities, particularly through the integration of\nunlabeled data into the training process. In the CIR scenario, encountered\nclasses may reappear in later learning experiences, and each experience may\ninvolve only a subset of the overall class distribution. Additionally, the\nunlabeled data provided during training may include instances of unseen\nclasses, or irrelevant classes which should be ignored. Our approach focuses on\nretaining previously learned knowledge by utilizing knowledge distillation and\npseudo-labeling techniques. The key characteristic of our method is the\nexploitation of unlabeled data during training, in order to maintain optimal\nperformance on instances of previously encountered categories and reduce the\ndetrimental effects of catastrophic forgetting. Our method achieves an average\naccuracy of 16.68\\% during the pre-selection phase and 21.19% during the final\nevaluation phase, outperforming the baseline accuracy of 9.39%. We provide the\nimplementation code at\nhttps://github.com/panagiotamoraiti/continual-learning-challenge-2024 ."}
{"id": "2503.16031", "pdf": "https://arxiv.org/pdf/2503.16031", "abs": "https://arxiv.org/abs/2503.16031", "authors": ["Sai Kartheek Reddy Kasu", "Shankar Biradar", "Sunil Saumya"], "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content", "categories": ["cs.CL"], "comment": "15 Pages, 4 figures, 8 tables", "summary": "This paper presents the Deceptive Humor Dataset (DHD), a novel resource for\nstudying humor derived from fabricated claims and misinformation. In an era of\nrampant misinformation, understanding how humor intertwines with deception is\nessential. DHD consists of humor-infused comments generated from false\nnarratives, incorporating fabricated claims and manipulated information using\nthe ChatGPT-4o model. Each instance is labeled with a Satire Level, ranging\nfrom 1 for subtle satire to 3 for high-level satire and classified into five\ndistinct Humor Categories: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans multiple languages including English, Telugu,\nHindi, Kannada, Tamil, and their code-mixed variants (Te-En, Hi-En, Ka-En,\nTa-En), making it a valuable multilingual benchmark. By introducing DHD, we\nestablish a structured foundation for analyzing humor in deceptive contexts,\npaving the way for a new research direction that explores how humor not only\ninteracts with misinformation but also influences its perception and spread. We\nestablish strong baselines for the proposed dataset, providing a foundation for\nfuture research to benchmark and advance deceptive humor detection models."}
{"id": "2503.15699", "pdf": "https://arxiv.org/pdf/2503.15699", "abs": "https://arxiv.org/abs/2503.15699", "authors": ["Neehar Kondapaneni", "Oisin Mac Aodha", "Pietro Perona"], "title": "Representational Similarity via Interpretable Visual Concepts", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "32 pages, 5 Figures, 16 Supplemental Figures, ICLR 2025", "summary": "How do two deep neural networks differ in how they arrive at a decision?\nMeasuring the similarity of deep networks has been a long-standing open\nquestion. Most existing methods provide a single number to measure the\nsimilarity of two networks at a given layer, but give no insight into what\nmakes them similar or dissimilar. We introduce an interpretable\nrepresentational similarity method (RSVC) to compare two networks. We use RSVC\nto discover shared and unique visual concepts between two models. We show that\nsome aspects of model differences can be attributed to unique concepts\ndiscovered by one model that are not well represented in the other. Finally, we\nconduct extensive evaluation across different vision model architectures and\ntraining protocols to demonstrate its effectiveness."}
{"id": "2503.16040", "pdf": "https://arxiv.org/pdf/2503.16040", "abs": "https://arxiv.org/abs/2503.16040", "authors": ["Yaoyao Yu", "Leilei Gan", "Yinghao Hu", "Bin Wei", "Kun Kuang", "Fei Wu"], "title": "Evaluating Test-Time Scaling LLMs for Legal Reasoning: OpenAI o1, DeepSeek-R1, and Beyond", "categories": ["cs.CL"], "comment": null, "summary": "Recently, Test-Time Scaling Large Language Models (LLMs), such as DeepSeek-R1\nand OpenAI o1, have demonstrated exceptional capabilities across various\ndomains and tasks, particularly in reasoning. While these models have shown\nimpressive performance on general language tasks, their effectiveness in\nspecialized fields like legal remains unclear. To address this, we present a\npreliminary evaluation of LLMs in various legal scenarios, covering both\nChinese and English legal tasks. Our analysis includes 9 LLMs and 17 legal\ntasks, with a focus on newly published and more complex challenges such as\nmulti-defendant legal judgments and legal argument reasoning. Our findings\nindicate that, despite DeepSeek-R1 and OpenAI o1 being among the most powerful\nmodels, their legal reasoning capabilities are still lacking. Specifically,\nthese models score below 80\\% on seven Chinese legal reasoning tasks and below\n80\\% on two English legal reasoning tasks. This suggests that, even among the\nmost advanced reasoning models, legal reasoning abilities remain\nunderdeveloped."}
{"id": "2503.15708", "pdf": "https://arxiv.org/pdf/2503.15708", "abs": "https://arxiv.org/abs/2503.15708", "authors": ["Sam Narimani", "Solveig Roth Hoff", "Kathinka Dahli Kurz", "Kjell-Inge Gjesdal", "Jurgen Geisler", "Endre Grovik"], "title": "Sustainable Deep Learning-Based Breast Lesion Segmentation: Impact of Breast Region Segmentation on Performance", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Purpose: Segmentation of the breast lesion in dynamic contrast-enhanced\nmagnetic resonance imaging (DCE-MRI) is an essential step to accurately\ndiagnose and plan treatment and monitor progress. This study aims to highlight\nthe impact of breast region segmentation (BRS) on deep learning-based breast\nlesion segmentation (BLS) in breast DCE-MRI.\n  Methods Using the Stavanger Dataset containing primarily 59 DCE-MRI scans and\nUNet++ as deep learning models, four different process were conducted to\ncompare effect of BRS on BLS. These four approaches included the whole volume\nwithout BRS and with BRS, BRS with the selected lesion slices and lastly\noptimal volume with BRS. Preprocessing methods like augmentation and\noversampling were used to enhance the small dataset, data shape uniformity and\nimprove model performance. Optimal volume size were investigated by a precise\nprocess to ensure that all lesions existed in slices. To evaluate the model, a\nhybrid loss function including dice, focal and cross entropy along with 5-fold\ncross validation method were used and lastly a test dataset which was randomly\nsplit used to evaluate the model performance on unseen data for each of four\nmentioned approaches.\n  Results Results demonstrate that using BRS considerably improved model\nperformance and validation. Significant improvement in last approach -- optimal\nvolume with BRS -- compared to the approach without BRS counting around 50\npercent demonstrating how effective BRS has been in BLS. Moreover, huge\nimprovement in energy consumption, decreasing up to 450 percent, introduces a\ngreen solution toward a more environmentally sustainable approach for future\nwork on large dataset."}
{"id": "2503.16043", "pdf": "https://arxiv.org/pdf/2503.16043", "abs": "https://arxiv.org/abs/2503.16043", "authors": ["Zhiyu Cao", "Peifeng Li", "Yaxin Fan", "Qiaoming Zhu"], "title": "Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although existing fashionable generation methods on Incomplete Utterance\nRewriting (IUR) can generate coherent utterances, they often result in the\ninclusion of irrelevant and redundant tokens in rewritten utterances due to\ntheir inability to focus on critical tokens in dialogue context. Furthermore,\nthe limited size of the training datasets also contributes to the insufficient\ntraining of the IUR model. To address the first issue, we propose a multi-task\nlearning framework EO-IUR (Editing Operation-guided Incomplete Utterance\nRewriting) that introduces the editing operation labels generated by sequence\nlabeling module to guide generation model to focus on critical tokens.\nFurthermore, we introduce a token-level heterogeneous graph to represent\ndialogues. To address the second issue, we propose a two-dimensional utterance\naugmentation strategy, namely editing operation-based incomplete utterance\naugmentation and LLM-based historical utterance augmentation. The experimental\nresults on three datasets demonstrate that our EO-IUR outperforms previous\nstate-of-the-art (SOTA) baselines in both open-domain and task-oriented\ndialogue. The code will be available at https://github.com/Dewset/EO-IUR."}
{"id": "2503.15712", "pdf": "https://arxiv.org/pdf/2503.15712", "abs": "https://arxiv.org/abs/2503.15712", "authors": ["Weiwen Hu", "Niccolò Parodi", "Marcus Zepp", "Ingo Feldmann", "Oliver Schreer", "Peter Eisert"], "title": "SPNeRF: Open Vocabulary 3D Neural Scene Segmentation with Superpoints", "categories": ["cs.CV"], "comment": "In Proceedings of the 20th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications (2025)", "summary": "Open-vocabulary segmentation, powered by large visual-language models like\nCLIP, has expanded 2D segmentation capabilities beyond fixed classes predefined\nby the dataset, enabling zero-shot understanding across diverse scenes.\nExtending these capabilities to 3D segmentation introduces challenges, as\nCLIP's image-based embeddings often lack the geometric detail necessary for 3D\nscene segmentation. Recent methods tend to address this by introducing\nadditional segmentation models or replacing CLIP with variations trained on\nsegmentation data, which lead to redundancy or loss on CLIP's general language\ncapabilities. To overcome this limitation, we introduce SPNeRF, a NeRF based\nzero-shot 3D segmentation approach that leverages geometric priors. We\nintegrate geometric primitives derived from the 3D scene into NeRF training to\nproduce primitive-wise CLIP features, avoiding the ambiguity of point-wise\nfeatures. Additionally, we propose a primitive-based merging mechanism enhanced\nwith affinity scores. Without relying on additional segmentation models, our\nmethod further explores CLIP's capability for 3D segmentation and achieves\nnotable improvements over original LERF."}
{"id": "2503.16048", "pdf": "https://arxiv.org/pdf/2503.16048", "abs": "https://arxiv.org/abs/2503.16048", "authors": ["Michael Goodale", "Salvador Mascarenhas", "Yair Lakretz"], "title": "Meta-Learning Neural Mechanisms rather than Bayesian Priors", "categories": ["cs.CL"], "comment": null, "summary": "Children acquire language despite being exposed to several orders of\nmagnitude less data than large language models require. Meta-learning has been\nproposed as a way to integrate human-like learning biases into neural-network\narchitectures, combining both the structured generalizations of symbolic models\nwith the scalability of neural-network models. But what does meta-learning\nexactly imbue the model with? We investigate the meta-learning of formal\nlanguages and find that, contrary to previous claims, meta-trained models are\nnot learning simplicity-based priors when meta-trained on datasets organised\naround simplicity. Rather, we find evidence that meta-training imprints neural\nmechanisms (such as counters) into the model, which function like cognitive\nprimitives for the network on downstream tasks. Most surprisingly, we find that\nmeta-training on a single formal language can provide as much improvement to a\nmodel as meta-training on 5000 different formal languages, provided that the\nformal language incentivizes the learning of useful neural mechanisms. Taken\ntogether, our findings provide practical implications for efficient\nmeta-learning paradigms and new theoretical insights into linking symbolic\ntheories and neural mechanisms."}
{"id": "2503.15731", "pdf": "https://arxiv.org/pdf/2503.15731", "abs": "https://arxiv.org/abs/2503.15731", "authors": ["Yuqing Zhang", "Qi Han", "Ligeng Wang", "Kai Cheng", "Bo Wang", "Kun Zhan"], "title": "Graph-Weighted Contrastive Learning for Semi-Supervised Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "Journal of Electronic Imaging, 2025", "summary": "Most existing graph-based semi-supervised hyperspectral image classification\nmethods rely on superpixel partitioning techniques. However, they suffer from\nmisclassification of certain pixels due to inaccuracies in superpixel\nboundaries, \\ie, the initial inaccuracies in superpixel partitioning limit\noverall classification performance. In this paper, we propose a novel\ngraph-weighted contrastive learning approach that avoids the use of superpixel\npartitioning and directly employs neural networks to learn hyperspectral image\nrepresentation. Furthermore, while many approaches require all graph nodes to\nbe available during training, our approach supports mini-batch training by\nprocessing only a subset of nodes at a time, reducing computational complexity\nand improving generalization to unseen nodes. Experimental results on three\nwidely-used datasets demonstrate the effectiveness of the proposed approach\ncompared to baselines relying on superpixel partitioning."}
{"id": "2503.16063", "pdf": "https://arxiv.org/pdf/2503.16063", "abs": "https://arxiv.org/abs/2503.16063", "authors": ["Zhiyu Cao", "Peifeng Li", "Qiaoming Zhu", "Yaxin Fan"], "title": "Two-stage Incomplete Utterance Rewriting on Editing Operation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Previous work on Incomplete Utterance Rewriting (IUR) has primarily focused\non generating rewritten utterances based solely on dialogue context, ignoring\nthe widespread phenomenon of coreference and ellipsis in dialogues. To address\nthis issue, we propose a novel framework called TEO (\\emph{Two-stage approach\non Editing Operation}) for IUR, in which the first stage generates editing\noperations and the second stage rewrites incomplete utterances utilizing the\ngenerated editing operations and the dialogue context. Furthermore, an\nadversarial perturbation strategy is proposed to mitigate cascading errors and\nexposure bias caused by the inconsistency between training and inference in the\nsecond stage. Experimental results on three IUR datasets show that our TEO\noutperforms the SOTA models significantly."}
{"id": "2503.15742", "pdf": "https://arxiv.org/pdf/2503.15742", "abs": "https://arxiv.org/abs/2503.15742", "authors": ["Sarosij Bose", "Arindam Dutta", "Sayak Nag", "Junge Zhang", "Jiachen Li", "Konstantinos Karydis", "Amit K. Roy Chowdhury"], "title": "Uncertainty-Aware Diffusion Guided Refinement of 3D Scenes", "categories": ["cs.CV"], "comment": "13 pages, 7 figures", "summary": "Reconstructing 3D scenes from a single image is a fundamentally ill-posed\ntask due to the severely under-constrained nature of the problem. Consequently,\nwhen the scene is rendered from novel camera views, existing single image to 3D\nreconstruction methods render incoherent and blurry views. This problem is\nexacerbated when the unseen regions are far away from the input camera. In this\nwork, we address these inherent limitations in existing single image-to-3D\nscene feedforward networks. To alleviate the poor performance due to\ninsufficient information beyond the input image's view, we leverage a strong\ngenerative prior in the form of a pre-trained latent video diffusion model, for\niterative refinement of a coarse scene represented by optimizable Gaussian\nparameters. To ensure that the style and texture of the generated images align\nwith that of the input image, we incorporate on-the-fly Fourier-style transfer\nbetween the generated images and the input image. Additionally, we design a\nsemantic uncertainty quantification module that calculates the per-pixel\nentropy and yields uncertainty maps used to guide the refinement process from\nthe most confident pixels while discarding the remaining highly uncertain ones.\nWe conduct extensive experiments on real-world scene datasets, including\nin-domain RealEstate-10K and out-of-domain KITTI-v2, showing that our approach\ncan provide more realistic and high-fidelity novel view synthesis results\ncompared to existing state-of-the-art methods."}
{"id": "2503.16071", "pdf": "https://arxiv.org/pdf/2503.16071", "abs": "https://arxiv.org/abs/2503.16071", "authors": ["Jiale Wei", "Shuchi Wu", "Ruochen Liu", "Xiang Ying", "Jingbo Shang", "Fangbo Tao"], "title": "Tuning LLMs by RAG Principles: Towards LLM-native Memory", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Memory, additional information beyond the training of large language models\n(LLMs), is crucial to various real-world applications, such as personal\nassistant. The two mainstream solutions to incorporate memory into the\ngeneration process are long-context LLMs and retrieval-augmented generation\n(RAG). In this paper, we first systematically compare these two types of\nsolutions on three renovated/new datasets and show that (1) long-context\nsolutions, although more expensive, shall be easier to capture the big picture\nand better answer queries which require considering the memory as a whole; and\n(2) when the queries concern specific information, RAG solutions shall be more\ncompetitive especially when the keywords can be explicitly matched. Therefore,\nwe propose a novel method RAG-Tuned-LLM which fine-tunes a relative small\n(e.g., 7B) LLM using the data generated following the RAG principles, so it can\ncombine the advantages of both solutions. Extensive experiments on three\ndatasets demonstrate that RAG-Tuned-LLM can beat long-context LLMs and RAG\nmethods across a wide range of query types."}
{"id": "2503.15761", "pdf": "https://arxiv.org/pdf/2503.15761", "abs": "https://arxiv.org/abs/2503.15761", "authors": ["Mir Mohammad Khaleghi", "Mehran Safayani", "Abdolreza Mirzaei"], "title": "GraPLUS: Graph-based Placement Using Semantics for Image Composition", "categories": ["cs.CV"], "comment": "17 pages, 3 figures, 6 tables", "summary": "We present GraPLUS (Graph-based Placement Using Semantics), a novel framework\nfor plausible object placement in images that leverages scene graphs and large\nlanguage models. Our approach uniquely combines graph-structured scene\nrepresentation with semantic understanding to determine contextually\nappropriate object positions. The framework employs GPT-2 to transform\ncategorical node and edge labels into rich semantic embeddings that capture\nboth definitional characteristics and typical spatial contexts, enabling\nnuanced understanding of object relationships and placement patterns. GraPLUS\nachieves placement accuracy of 92.1% and an FID score of 28.83 on the OPA\ndataset, outperforming state-of-the-art methods by 8.1% while maintaining\ncompetitive visual quality. In human evaluation studies involving 964 samples\nassessed by 19 participants, our method was preferred in 52.1% of cases,\nsignificantly outperforming previous approaches. The framework's key\ninnovations include: (i) leveraging pre-trained scene graph models that\ntransfer knowledge from other domains, (ii) edge-aware graph neural networks\nthat process scene semantics through structured relationships, (iii) a\ncross-modal attention mechanism that aligns categorical embeddings with\nenhanced scene features, and (iv) a multiobjective training strategy\nincorporating semantic consistency constraints."}
{"id": "2503.16094", "pdf": "https://arxiv.org/pdf/2503.16094", "abs": "https://arxiv.org/abs/2503.16094", "authors": ["Reem I. Masoud", "Martin Ferianc", "Philip Treleaven", "Miguel Rodrigues"], "title": "Cultural Alignment in Large Language Models Using Soft Prompt Tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) alignment conventionally relies on supervised\nfine-tuning or reinforcement learning based alignment frameworks. These methods\ntypically require labeled or preference datasets and involve updating model\nweights to align the LLM with the training objective or reward model.\nMeanwhile, in social sciences such as cross-cultural studies, factor analysis\nis widely used to uncover underlying dimensions or latent variables that\nexplain observed patterns in survey data. The non-differentiable nature of\nthese measurements deriving from survey data renders the former alignment\nmethods infeasible for alignment with cultural dimensions. To overcome this, we\npropose a parameter efficient strategy that combines soft prompt tuning, which\nfreezes the model parameters while modifying the input prompt embeddings, with\nDifferential Evolution (DE), a black-box optimization method for cases where a\ndifferentiable objective is unattainable. This strategy ensures alignment\nconsistency without the need for preference data or model parameter updates,\nsignificantly enhancing efficiency and mitigating overfitting. Our method\ndemonstrates significant improvements in LLama-3-8B-Instruct's cultural\ndimensions across multiple regions, outperforming both the Naive LLM and the\nIn-context Learning (ICL) baseline, and effectively bridges computational\nmodels with human cultural nuances."}
{"id": "2503.15763", "pdf": "https://arxiv.org/pdf/2503.15763", "abs": "https://arxiv.org/abs/2503.15763", "authors": ["Huan Lei"], "title": "OffsetOPT: Explicit Surface Reconstruction without Normals", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Neural surface reconstruction has been dominated by implicit representations\nwith marching cubes for explicit surface extraction. However, those methods\ntypically require high-quality normals for accurate reconstruction. We propose\nOffsetOPT, a method that reconstructs explicit surfaces directly from 3D point\nclouds and eliminates the need for point normals. The approach comprises two\nstages: first, we train a neural network to predict surface triangles based on\nlocal point geometry, given uniformly distributed training point clouds. Next,\nwe apply the frozen network to reconstruct surfaces from unseen point clouds by\noptimizing a per-point offset to maximize the accuracy of triangle predictions.\nCompared to state-of-the-art methods, OffsetOPT not only excels at\nreconstructing overall surfaces but also significantly preserves sharp surface\nfeatures. We demonstrate its accuracy on popular benchmarks, including\nsmall-scale shapes and large-scale open surfaces."}
{"id": "2503.16131", "pdf": "https://arxiv.org/pdf/2503.16131", "abs": "https://arxiv.org/abs/2503.16131", "authors": ["Feiyang Li", "Yingjian Chen", "Haoran Liu", "Rui Yang", "Han Yuan", "Yuang Jiang", "Tianxiao Li", "Edison Marrese Taylor", "Hossein Rouhizadeh", "Yusuke Iwasawa", "Douglas Teodoro", "Yutaka Matsuo", "Irene Li"], "title": "MKG-Rank: Enhancing Large Language Models with Knowledge Graph for Multilingual Medical Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable progress in medical\nquestion answering (QA), yet their effectiveness remains predominantly limited\nto English due to imbalanced multilingual training data and scarce medical\nresources for low-resource languages. To address this critical language gap in\nmedical QA, we propose Multilingual Knowledge Graph-based Retrieval Ranking\n(MKG-Rank), a knowledge graph-enhanced framework that enables English-centric\nLLMs to perform multilingual medical QA. Through a word-level translation\nmechanism, our framework efficiently integrates comprehensive English-centric\nmedical knowledge graphs into LLM reasoning at a low cost, mitigating\ncross-lingual semantic distortion and achieving precise medical QA across\nlanguage barriers. To enhance efficiency, we introduce caching and multi-angle\nranking strategies to optimize the retrieval process, significantly reducing\nresponse times and prioritizing relevant medical knowledge. Extensive\nevaluations on multilingual medical QA benchmarks across Chinese, Japanese,\nKorean, and Swahili demonstrate that MKG-Rank consistently outperforms\nzero-shot LLMs, achieving maximum 33.89% increase in accuracy, while\nmaintaining an average retrieval time of only 0.0009 seconds."}
{"id": "2503.15778", "pdf": "https://arxiv.org/pdf/2503.15778", "abs": "https://arxiv.org/abs/2503.15778", "authors": ["Boshra Khalili", "Andrew W. Smyth"], "title": "AutoDrive-QA- Automated Generation of Multiple-Choice Questions for Autonomous Driving Datasets Using Large Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In autonomous driving, open-ended question answering often suffers from\nunreliable evaluations because freeform responses require either complex\nmetrics or subjective human judgment. To address this challenge, we introduce\nAutoDrive-QA, an automatic pipeline that converts existing driving QA datasets\n(including DriveLM, NuScenes-QA, and LingoQA) into a structured multiple-choice\nquestion (MCQ) format. This benchmark systematically assesses perception,\nprediction, and planning tasks, providing a standardized and objective\nevaluation framework. AutoDrive-QA employs an automated pipeline that leverages\nlarge language models (LLMs) to generate high-quality, contextually relevant\ndistractors based on domain-specific error patterns commonly found in\nautonomous driving scenarios. To evaluate both general capabilities and\ngeneralization performance, we test the benchmark on three public datasets and\nconduct zero-shot experiments on an unseen dataset. The zero-shot evaluations\nreveal that GPT-4V leads with 69.57% accuracy -- achieving 74.94% in\nPerception, 65.33% in Prediction, and 68.45% in Planning -- demonstrating that\nwhile all models excel in Perception, they struggle in Prediction.\nConsequently, AutoDrive-QA establishes a rigorous, unbiased standard for\nintegrating and evaluating different vision-language models across various\nautonomous driving datasets, thereby improving generalization in this field. We\nrelease all the codes in the AutoDrive-QA GitHub Repository."}
{"id": "2503.16158", "pdf": "https://arxiv.org/pdf/2503.16158", "abs": "https://arxiv.org/abs/2503.16158", "authors": ["Shenbin Qian", "Constantin Orăsan", "Diptesh Kanojia", "Félix do Carmo"], "title": "Automatically Generating Chinese Homophone Words to Probe Machine Translation Estimation Systems", "categories": ["cs.CL"], "comment": "Accepted to the 10th Workshop on Noisy and User-generated Text at\n  NAACL 2025", "summary": "Evaluating machine translation (MT) of user-generated content (UGC) involves\nunique challenges such as checking whether the nuance of emotions from the\nsource are preserved in the target text. Recent studies have proposed\nemotion-related datasets, frameworks and models to automatically evaluate MT\nquality of Chinese UGC, without relying on reference translations. However,\nwhether these models are robust to the challenge of preserving emotional\nnuances has been left largely unexplored. To address this gap, we introduce a\nnovel method inspired by information theory which generates challenging Chinese\nhomophone words related to emotions, by leveraging the concept of\nself-information. Our approach generates homophones that were observed to cause\ntranslation errors in emotion preservation, and exposes vulnerabilities in MT\nsystems and their evaluation methods when tackling emotional UGC. We evaluate\nthe efficacy of our method using human evaluation for the quality of these\ngenerated homophones, and compare it with an existing one, showing that our\nmethod achieves higher correlation with human judgments. The generated Chinese\nhomophones, along with their manual translations, are utilized to generate\nperturbations and to probe the robustness of existing quality evaluation\nmodels, including models trained using multi-task learning, fine-tuned variants\nof multilingual language models, as well as large language models (LLMs). Our\nresults indicate that LLMs with larger size exhibit higher stability and\nrobustness to such perturbations. We release our data and code for\nreproducibility and further research."}
{"id": "2503.15784", "pdf": "https://arxiv.org/pdf/2503.15784", "abs": "https://arxiv.org/abs/2503.15784", "authors": ["Parham Saremi", "Amar Kumar", "Mohammed Mohammed", "Zahra TehraniNasab", "Tal Arbel"], "title": "RL4Med-DDPO: Reinforcement Learning for Controlled Guidance Towards Diverse Medical Image Generation using Vision-Language Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Foundation Models (VLFM) have shown a tremendous increase in\nperformance in terms of generating high-resolution, photorealistic natural\nimages. While VLFMs show a rich understanding of semantic content across\nmodalities, they often struggle with fine-grained alignment tasks that require\nprecise correspondence between image regions and textual descriptions a\nlimitation in medical imaging, where accurate localization and detection of\nclinical features are essential for diagnosis and analysis. To address this\nissue, we propose a multi-stage architecture where a pre-trained VLFM provides\na cursory semantic understanding, while a reinforcement learning (RL) algorithm\nrefines the alignment through an iterative process that optimizes for\nunderstanding semantic context. The reward signal is designed to align the\nsemantic information of the text with synthesized images. We demonstrate the\neffectiveness of our method on a medical imaging skin dataset where the\ngenerated images exhibit improved generation quality and alignment with prompt\nover the fine-tuned Stable Diffusion. We also show that the synthesized samples\ncould be used to improve disease classifier performance for underrepresented\nsubgroups through augmentation."}
{"id": "2503.16161", "pdf": "https://arxiv.org/pdf/2503.16161", "abs": "https://arxiv.org/abs/2503.16161", "authors": ["Alex-Razvan Ispas", "Charles-Elie Simon", "Fabien Caspani", "Vincent Guigue"], "title": "Towards Lighter and Robust Evaluation for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI", "62-08", "I.2.7"], "comment": "17 pages, 5 figures, published at 1st workshop of Quantify\n  Uncertainty and Hallucination in Foundation Models: The Next Frontier in\n  Reliable AI at ICLR 25", "summary": "Large Language Models are prompting us to view more NLP tasks from a\ngenerative perspective. At the same time, they offer a new way of accessing\ninformation, mainly through the RAG framework. While there have been notable\nimprovements for the autoregressive models, overcoming hallucination in the\ngenerated answers remains a continuous problem. A standard solution is to use\ncommercial LLMs, such as GPT4, to evaluate these algorithms. However, such\nframeworks are expensive and not very transparent. Therefore, we propose a\nstudy which demonstrates the interest of open-weight models for evaluating RAG\nhallucination. We develop a lightweight approach using smaller, quantized LLMs\nto provide an accessible and interpretable metric that gives continuous scores\nfor the generated answer with respect to their correctness and faithfulness.\nThis score allows us to question decisions' reliability and explore thresholds\nto develop a new AUC metric as an alternative to correlation with human\njudgment."}
{"id": "2503.15800", "pdf": "https://arxiv.org/pdf/2503.15800", "abs": "https://arxiv.org/abs/2503.15800", "authors": ["Jingyun Liu", "Daiqin Yang", "Zhenzhong Chen"], "title": "Frequency Enhancement for Image Demosaicking", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Recovering high-frequency textures in image demosaicking remains a\nchallenging issue. While existing methods introduced elaborate spatial learning\nmethods, they still exhibit limited performance. To address this issue, a\nfrequency enhancement approach is proposed. Based on the frequency analysis of\ncolor filter array (CFA)/demosaicked/ground truth images, we propose Dual-path\nFrequency Enhancement Network (DFENet), which reconstructs RGB images in a\ndivide-and-conquer manner through fourier-domain frequency selection. In\nDFENet, two frequency selectors are employed, each selecting a set of frequency\ncomponents for processing along separate paths. One path focuses on generating\nmissing information through detail refinement in spatial domain, while the\nother aims at suppressing undesirable frequencies with the guidance of CFA\nimages in frequency domain. Multi-level frequency supervision with a stagewise\ntraining strategy is employed to further improve the reconstruction\nperformance. With these designs, the proposed DFENet outperforms other\nstate-of-the-art algorithms on different datasets and demonstrates significant\nadvantages on hard cases. Moreover, to better assess algorithms' ability to\nreconstruct high-frequency textures, a new dataset, LineSet37, is contributed,\nwhich consists of 37 artificially designed and generated images. These images\nfeature complex line patterns and are prone to severe visual artifacts like\ncolor moir\\'e after demosaicking. Experiments on LineSet37 offer a more\ntargeted evaluation of performance on challenging cases. The code and dataset\nare available at https://github.com/VelvetReverie/DFENet-demosaicking."}
{"id": "2503.16163", "pdf": "https://arxiv.org/pdf/2503.16163", "abs": "https://arxiv.org/abs/2503.16163", "authors": ["Shibo Jie", "Yehui Tang", "Kai Han", "Zhi-Hong Deng", "Jing Han"], "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."}
{"id": "2503.15816", "pdf": "https://arxiv.org/pdf/2503.15816", "abs": "https://arxiv.org/abs/2503.15816", "authors": ["Abduljaleel Adejumo", "Faegheh Yeganli", "Clifford Broni-bediako", "Aoran Xiao", "Naoto Yokoya", "Mennatullah Siam"], "title": "A Vision Centric Remote Sensing Benchmark", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "6 PAGES, 7 figures, CVPR", "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision-language tasks but their remote sensing (RS) counterpart are relatively\nunder explored. Unlike natural images, RS imagery presents unique challenges\nthat current MLLMs struggle to handle, particularly in visual grounding and\nspatial reasoning. This study investigates the limitations of CLIP-based MLLMs\nin RS, highlighting their failure to differentiate visually distinct yet\nsemantically similar RS images. To address this, we introduce a remote sensing\nmultimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs\nin RS tasks by identifying the CLIP-blind pairs, where CLIP-based models\nincorrectly assign high similarity scores to visually distinct RS images.\nThrough a visual question answering (VQA) evaluation, we analyze the\nperformance of state-of-the-art MLLMs, revealing significant limitations in RS\nspecific representation learning. The results provide valuable insights into\nthe weaknesses of CLIP-based visual encoding and offer a foundation for future\nresearch to develop more effective MLLMs tailored for remote sensing\napplications."}
{"id": "2503.16212", "pdf": "https://arxiv.org/pdf/2503.16212", "abs": "https://arxiv.org/abs/2503.16212", "authors": ["Qizhi Pei", "Lijun Wu", "Zhuoshi Pan", "Yu Li", "Honglin Lin", "Chenlin Ming", "Xin Gao", "Conghui He", "Rui Yan"], "title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have shown impressive progress in mathematical\nreasoning. While data augmentation is promising to enhance mathematical\nproblem-solving ability, current approaches are predominantly limited to\ninstance-level modifications-such as rephrasing or generating syntactic\nvariations-which fail to capture and leverage the intrinsic relational\nstructures inherent in mathematical knowledge. Inspired by human learning\nprocesses, where mathematical proficiency develops through systematic exposure\nto interconnected concepts, we introduce MathFusion, a novel framework that\nenhances mathematical reasoning through cross-problem instruction synthesis.\nMathFusion implements this through three fusion strategies: (1) sequential\nfusion, which chains related problems to model solution dependencies; (2)\nparallel fusion, which combines analogous problems to reinforce conceptual\nunderstanding; and (3) conditional fusion, which creates context-aware\nselective problems to enhance reasoning flexibility. By applying these\nstrategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by\nfine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental\nresults demonstrate that MathFusion achieves substantial improvements in\nmathematical reasoning while maintaining high data efficiency, boosting\nperformance by 18.0 points in accuracy across diverse benchmarks while\nrequiring only 45K additional synthetic instructions, representing a\nsubstantial improvement over traditional single-instruction approaches. Our\ndatasets, models, and code are publicly available at\nhttps://github.com/QizhiPei/mathfusion."}
{"id": "2503.15818", "pdf": "https://arxiv.org/pdf/2503.15818", "abs": "https://arxiv.org/abs/2503.15818", "authors": ["Haotian Ma", "Lin Gu", "Siyi Wu", "Yingying Zhu"], "title": "Computation-Efficient and Recognition-Friendly 3D Point Cloud Privacy Protection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D point cloud has been widely used in applications such as self-driving\ncars, robotics, CAD models, etc. To the best of our knowledge, these\napplications raised the issue of privacy leakage in 3D point clouds, which has\nnot been studied well. Different from the 2D image privacy, which is related to\ntexture and 2D geometric structure, the 3D point cloud is texture-less and only\nrelevant to 3D geometric structure. In this work, we defined the 3D point cloud\nprivacy problem and proposed an efficient privacy-preserving framework named\nPointFlowGMM that can support downstream classification and segmentation tasks\nwithout seeing the original data. Using a flow-based generative model, the\npoint cloud is projected into a latent Gaussian mixture distributed subspace.\nWe further designed a novel angular similarity loss to obfuscate the original\ngeometric structure and reduce the model size from 767MB to 120MB without a\ndecrease in recognition performance. The projected point cloud in the latent\nspace is orthogonally rotated randomly to further protect the original\ngeometric structure, the class-to-class relationship is preserved after\nrotation, thus, the protected point cloud can support the recognition task. We\nevaluated our model on multiple datasets and achieved comparable recognition\nresults on encrypted point clouds compared to the original point clouds."}
{"id": "2503.16252", "pdf": "https://arxiv.org/pdf/2503.16252", "abs": "https://arxiv.org/abs/2503.16252", "authors": ["Zhaowei Liu", "Xin Guo", "Fangqi Lou", "Lingfeng Zeng", "Jinyi Niu", "Zixuan Wang", "Jiajie Xu", "Weige Cai", "Ziwei Yang", "Xueqian Zhao", "Chao Li", "Sheng Xu", "Dezhi Chen", "Yun Chen", "Zuo Bai", "Liwen Zhang"], "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models are rapidly evolving across various domains.\nHowever, their capabilities in handling complex financial tasks still require\nin-depth exploration. In this paper, we introduce Fin-R1, a reasoning large\nlanguage model specifically designed for the financial sector. Fin-R1 is built\nusing a two-stage architecture, leveraging a financial reasoning dataset\ndistilled and processed based on DeepSeek-R1. Through supervised fine-tuning\n(SFT) and reinforcement learning (RL) training, it demonstrates performance\nclose to DeepSeek-R1 with a parameter size of 7 billion across a range of\nfinancial reasoning tasks. It achieves the state-of-the-art (SOTA) in the FinQA\nand ConvFinQA tasks between those LLMs in our evaluation, surpassing larger\nmodels in other tasks as well. Fin-R1 showcases strong reasoning and\ndecision-making capabilities, providing solutions to various problems\nencountered in the financial domain. Our code is available at\nhttps://github.com/SUFE-AIFLM-Lab/Fin-R1."}
{"id": "2503.15831", "pdf": "https://arxiv.org/pdf/2503.15831", "abs": "https://arxiv.org/abs/2503.15831", "authors": ["Zihao Zhang", "Haoran Chen", "Haoyu Zhao", "Guansong Lu", "Yanwei Fu", "Hang Xu", "Zuxuan Wu"], "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Handling complex or nonlinear motion patterns has long posed challenges for\nvideo frame interpolation. Although recent advances in diffusion-based methods\noffer improvements over traditional optical flow-based approaches, they still\nstruggle to generate sharp, temporally consistent frames in scenarios with\nlarge motion. To address this limitation, we introduce EDEN, an Enhanced\nDiffusion for high-quality large-motion vidEo frame iNterpolation. Our approach\nfirst utilizes a transformer-based tokenizer to produce refined latent\nrepresentations of the intermediate frames for diffusion models. We then\nenhance the diffusion transformer with temporal attention across the process\nand incorporate a start-end frame difference embedding to guide the generation\nof dynamic motion. Extensive experiments demonstrate that EDEN achieves\nstate-of-the-art results across popular benchmarks, including nearly a 10%\nLPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD."}
{"id": "2503.16334", "pdf": "https://arxiv.org/pdf/2503.16334", "abs": "https://arxiv.org/abs/2503.16334", "authors": ["Ying Shen", "Lifu Huang"], "title": "LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates", "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "Recent findings reveal that much of the knowledge in a Transformer-based\nLarge Language Model (LLM) is encoded in its feed-forward (FFN) layers, where\neach FNN layer can be interpreted as the summation of sub-updates, each\ncorresponding to a weighted column vector from the FFN's value parameter matrix\nthat often encodes human-interpretable concepts. In light of this, we\nhypothesize that model performance and behaviors can be further enhanced and\ncontrolled by modulating the contributions of these sub-updates based on their\nrelevance to the input or target output style, and propose LLMBRACES, a novel\nand efficient method that computes relevance scores associated with value\nvectors in FFN layers and leverages these scores to dynamically adjust the\ncontribution of sub-updates. By optimizing sub-update contributions, LLMBRACES\nrefines the prediction process, leading to more accurate and reliable outputs,\nmuch like a 'brace' providing support and stability. Moreover, LLMBRACES can be\nextended to support conditional control over generation characteristics, such\nas sentiment, thereby offering fine-grained steering of LLM outputs. Extensive\nexperiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and\nLlama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both\nfine-tuning and zero-shot settings while requiring significantly fewer tunable\nparameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in\nsentiment-controlled generation and toxicity reduction, highlighting its\npotential for flexible, controlled text generation across applications."}
{"id": "2503.15835", "pdf": "https://arxiv.org/pdf/2503.15835", "abs": "https://arxiv.org/abs/2503.15835", "authors": ["Yiren Lu", "Yunlai Zhou", "Disheng Liu", "Tuo Liang", "Yu Yin"], "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR2025. Project page at https://vulab-ai.github.io/BARD-GS/", "summary": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene\nreconstruction, and recent advancements have extended its application to\ndynamic scenes. However, the quality of reconstructions depends heavily on\nhigh-quality input images and precise camera poses, which are not that trivial\nto fulfill in real-world scenarios. Capturing dynamic scenes with handheld\nmonocular cameras, for instance, typically involves simultaneous movement of\nboth the camera and objects within a single exposure. This combined motion\nfrequently results in image blur that existing methods cannot adequately\nhandle. To address these challenges, we introduce BARD-GS, a novel approach for\nrobust dynamic scene reconstruction that effectively handles blurry inputs and\nimprecise camera poses. Our method comprises two main components: 1) camera\nmotion deblurring and 2) object motion deblurring. By explicitly decomposing\nmotion blur into camera motion blur and object motion blur and modeling them\nseparately, we achieve significantly improved rendering results in dynamic\nregions. In addition, we collect a real-world motion blur dataset of dynamic\nscenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS\neffectively reconstructs high-quality dynamic scenes under realistic\nconditions, significantly outperforming existing methods."}
{"id": "2503.16356", "pdf": "https://arxiv.org/pdf/2503.16356", "abs": "https://arxiv.org/abs/2503.16356", "authors": ["Yunzhi Yao", "Jizhan Fang", "Jia-Chen Gu", "Ningyu Zhang", "Shumin Deng", "Huajun Chen", "Nanyun Peng"], "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."}
{"id": "2503.15846", "pdf": "https://arxiv.org/pdf/2503.15846", "abs": "https://arxiv.org/abs/2503.15846", "authors": ["Xuanming Cui", "Jaiminkumar Ashokbhai Bhoi", "Chionh Wei Peng", "Adriel Kuek", "Ser Nam Lim"], "title": "What can Off-the-Shelves Large Multi-Modal Models do for Dynamic Scene Graph Generation?", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic Scene Graph Generation (DSGG) for videos is a challenging task in\ncomputer vision. While existing approaches often focus on sophisticated\narchitectural design and solely use recall during evaluation, we take a closer\nlook at their predicted scene graphs and discover three critical issues with\nexisting DSGG methods: severe precision-recall trade-off, lack of awareness on\ntriplet importance, and inappropriate evaluation protocols. On the other hand,\nrecent advances of Large Multimodal Models (LMMs) have shown great capabilities\nin video understanding, yet they have not been tested on fine-grained,\nframe-wise understanding tasks like DSGG. In this work, we conduct the first\nsystematic analysis of Video LMMs for performing DSGG. Without relying on\nsophisticated architectural design, we show that LMMs with simple decoder-only\nstructure can be turned into State-of-the-Art scene graph generators that\neffectively overcome the aforementioned issues, while requiring little\nfinetuning (5-10% training data)."}
{"id": "2503.16419", "pdf": "https://arxiv.org/pdf/2503.16419", "abs": "https://arxiv.org/abs/2503.16419", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", "Shaochen", "Zhong", "Hanjie Chen", "Xia Hu"], "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "categories": ["cs.CL"], "comment": "Project Website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking."}
{"id": "2503.15851", "pdf": "https://arxiv.org/pdf/2503.15851", "abs": "https://arxiv.org/abs/2503.15851", "authors": ["Zhou Zhenglin", "Ma Fan", "Fan Hehe", "Chua Tat-Seng"], "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, project page:\n  https://zhenglinzhou.github.io/zero-1-to-a", "summary": "Animatable head avatar generation typically requires extensive data for\ntraining. To reduce the data requirements, a natural solution is to leverage\nexisting data-free static avatar generation methods, such as pre-trained\ndiffusion models with score distillation sampling (SDS), which align avatars\nwith pseudo ground-truth outputs from the diffusion model. However, directly\ndistilling 4D avatars from video diffusion often leads to over-smooth results\ndue to spatial and temporal inconsistencies in the generated video. To address\nthis issue, we propose Zero-1-to-A, a robust method that synthesizes a spatial\nand temporal consistency dataset for 4D avatar reconstruction using the video\ndiffusion model. Specifically, Zero-1-to-A iteratively constructs video\ndatasets and optimizes animatable avatars in a progressive manner, ensuring\nthat avatar quality increases smoothly and consistently throughout the learning\nprocess. This progressive learning involves two stages: (1) Spatial Consistency\nLearning fixes expressions and learns from front-to-side views, and (2)\nTemporal Consistency Learning fixes views and learns from relaxed to\nexaggerated expressions, generating 4D avatars in a simple-to-complex manner.\nExtensive experiments demonstrate that Zero-1-to-A improves fidelity, animation\nquality, and rendering speed compared to existing diffusion-based methods,\nproviding a solution for lifelike avatar creation. Code is publicly available\nat: https://github.com/ZhenglinZhou/Zero-1-to-A."}
{"id": "2503.16428", "pdf": "https://arxiv.org/pdf/2503.16428", "abs": "https://arxiv.org/abs/2503.16428", "authors": ["Ruyi Xu", "Guangxuan Xiao", "Haofeng Huang", "Junxian Guo", "Song Han"], "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring", "categories": ["cs.CL", "cs.CV"], "comment": "The first two authors contributed equally to this work", "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention."}
{"id": "2503.15855", "pdf": "https://arxiv.org/pdf/2503.15855", "abs": "https://arxiv.org/abs/2503.15855", "authors": ["Hyojun Go", "Byeongjun Park", "Hyelin Nam", "Byung-Hoon Kim", "Hyungjin Chung", "Changick Kim"], "title": "VideoRFSplat: Direct Scene-Level Text-to-3D Gaussian Splatting Generation with Flexible Pose and Multi-View Joint Modeling", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://gohyojun15.github.io/VideoRFSplat/", "summary": "We propose VideoRFSplat, a direct text-to-3D model leveraging a video\ngeneration model to generate realistic 3D Gaussian Splatting (3DGS) for\nunbounded real-world scenes. To generate diverse camera poses and unbounded\nspatial extent of real-world scenes, while ensuring generalization to arbitrary\ntext prompts, previous methods fine-tune 2D generative models to jointly model\ncamera poses and multi-view images. However, these methods suffer from\ninstability when extending 2D generative models to joint modeling due to the\nmodality gap, which necessitates additional models to stabilize training and\ninference. In this work, we propose an architecture and a sampling strategy to\njointly model multi-view images and camera poses when fine-tuning a video\ngeneration model. Our core idea is a dual-stream architecture that attaches a\ndedicated pose generation model alongside a pre-trained video generation model\nvia communication blocks, generating multi-view images and camera poses through\nseparate streams. This design reduces interference between the pose and image\nmodalities. Additionally, we propose an asynchronous sampling strategy that\ndenoises camera poses faster than multi-view images, allowing rapidly denoised\nposes to condition multi-view generation, reducing mutual ambiguity and\nenhancing cross-modal consistency. Trained on multiple large-scale real-world\ndatasets (RealEstate10K, MVImgNet, DL3DV-10K, ACID), VideoRFSplat outperforms\nexisting text-to-3D direct generation methods that heavily depend on post-hoc\nrefinement via score distillation sampling, achieving superior results without\nsuch refinement."}
{"id": "2503.15491", "pdf": "https://arxiv.org/pdf/2503.15491", "abs": "https://arxiv.org/abs/2503.15491", "authors": ["Kazuhiro Sasabuchi", "Naoki Wake", "Atsushi Kanehira", "Jun Takamatsu", "Katsushi Ikeuchi"], "title": "Agreeing to Interact in Human-Robot Interaction using Large Language Models and Vision Language Models", "categories": ["cs.HC", "cs.CL", "cs.LG", "cs.RO"], "comment": null, "summary": "In human-robot interaction (HRI), the beginning of an interaction is often\ncomplex. Whether the robot should communicate with the human is dependent on\nseveral situational factors (e.g., the current human's activity, urgency of the\ninteraction, etc.). We test whether large language models (LLM) and vision\nlanguage models (VLM) can provide solutions to this problem. We compare four\ndifferent system-design patterns using LLMs and VLMs, and test on a test set\ncontaining 84 human-robot situations. The test set mixes several publicly\navailable datasets and also includes situations where the appropriate action to\ntake is open-ended. Our results using the GPT-4o and Phi-3 Vision model\nindicate that LLMs and VLMs are capable of handling interaction beginnings when\nthe desired actions are clear, however, challenge remains in the open-ended\nsituations where the model must balance between the human and robot situation."}
{"id": "2503.15867", "pdf": "https://arxiv.org/pdf/2503.15867", "abs": "https://arxiv.org/abs/2503.15867", "authors": ["Rohit Kundu", "Athula Balachandran", "Amit K. Roy-Chowdhury"], "title": "TruthLens: Explainable DeepFake Detection for Face Manipulated and Fully Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting DeepFakes has become a crucial research area as the widespread use\nof AI image generators enables the effortless creation of face-manipulated and\nfully synthetic content, yet existing methods are often limited to binary\nclassification (real vs. fake) and lack interpretability. To address these\nchallenges, we propose TruthLens, a novel and highly generalizable framework\nfor DeepFake detection that not only determines whether an image is real or\nfake but also provides detailed textual reasoning for its predictions. Unlike\ntraditional methods, TruthLens effectively handles both face-manipulated\nDeepFakes and fully AI-generated content while addressing fine-grained queries\nsuch as \"Does the eyes/nose/mouth look real or fake?\"\n  The architecture of TruthLens combines the global contextual understanding of\nmultimodal large language models like PaliGemma2 with the localized feature\nextraction capabilities of vision-only models like DINOv2. This hybrid design\nleverages the complementary strengths of both models, enabling robust detection\nof subtle manipulations while maintaining interpretability. Extensive\nexperiments on diverse datasets demonstrate that TruthLens outperforms\nstate-of-the-art methods in detection accuracy (by 2-14%) and explainability,\nin both in-domain and cross-data settings, generalizing effectively across\ntraditional and emerging manipulation techniques."}
{"id": "2503.15509", "pdf": "https://arxiv.org/pdf/2503.15509", "abs": "https://arxiv.org/abs/2503.15509", "authors": ["Amandine M. Caut", "Amy Rouillard", "Beimnet Zenebe", "Matthias Green", "Ágúst Pálmason Morthens", "David J. T. Sumpter"], "title": "Representing data in words", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "An important part of data science is the use of visualisations to display\ndata in a way that is easy to digest. Visualisations often rely on underlying\nstatistical or machine learning models -- ranging from basic calculations like\ncategory means to advanced methods such as principal component analysis of\nmultidimensional datasets -- to convey insights. We introduce an analogous\nconcept for word descriptions of data, which we call wordalisations.\nWordalisations describe data in easy to digest words, without necessarily\nreporting numerical values from the data. We show how to create wordalisations\nusing large language models, through prompt templates engineered according to a\ntask-agnostic structure which can be used to automatically generate prompts\nfrom data. We show how to produce reliable and engaging texts on three\napplication areas: scouting football players, personality tests, and\ninternational survey data. Using the model cards framework, we emphasise the\nimportance of clearly stating the model we are imposing on the data when\ncreating the wordalisation, detailing how numerical values are translated into\nwords, incorporating background information into prompts for the large language\nmodel, and documenting the limitations of the wordalisations. We argue that our\nmodel cards approach is a more appropriate framework for setting best practices\nin wordalisation of data than performance tests on benchmark datasets."}
{"id": "2503.15868", "pdf": "https://arxiv.org/pdf/2503.15868", "abs": "https://arxiv.org/abs/2503.15868", "authors": ["Debabrata Mandal", "Soumitri Chattopadhyay", "Guansen Tong", "Praneeth Chakravarthula"], "title": "UniCoRN: Latent Diffusion-based Unified Controllable Image Restoration Network across Multiple Degradations", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration is essential for enhancing degraded images across computer\nvision tasks. However, most existing methods address only a single type of\ndegradation (e.g., blur, noise, or haze) at a time, limiting their real-world\napplicability where multiple degradations often occur simultaneously. In this\npaper, we propose UniCoRN, a unified image restoration approach capable of\nhandling multiple degradation types simultaneously using a multi-head diffusion\nmodel. Specifically, we uncover the potential of low-level visual cues\nextracted from images in guiding a controllable diffusion model for real-world\nimage restoration and we design a multi-head control network adaptable via a\nmixture-of-experts strategy. We train our model without any prior assumption of\nspecific degradations, through a smartly designed curriculum learning recipe.\nAdditionally, we also introduce MetaRestore, a metalens imaging benchmark\ncontaining images with multiple degradations and artifacts. Extensive\nevaluations on several challenging datasets, including our benchmark,\ndemonstrate that our method achieves significant performance gains and can\nrobustly restore images with severe degradations. Project page:\nhttps://codejaeger.github.io/unicorn-gh"}
{"id": "2503.15514", "pdf": "https://arxiv.org/pdf/2503.15514", "abs": "https://arxiv.org/abs/2503.15514", "authors": ["Jaymari Chua", "Chen Wang", "Lina Yao"], "title": "Superhuman AI Disclosure: Impacts on Toxicity, Fairness, and Trust Vary by Expertise and Persona Attributes", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.ET", "I.2.7; K.4.1"], "comment": null, "summary": "As artificial intelligence demonstrates surpassing human performance across\nreal-world tasks, disclosing superhuman capabilities poses challenges for\nfairness, accountability, and trust. To investigate how transparency impacts\nattitudes and perceptions, we introduce a grounded and validated set of\nsynthetic personas reflecting diverse fairness concerns and technology\nacceptance levels. Then we evaluate responses in two contrasting domains: (1) a\ncompetitive player in StarCraft II, where strategy and high-skill gameplay\noften elicit toxic interactions, and (2) a cooperative personal-assistant in\nproviding information. Across numerous interactions spanning persona profiles,\nwe test non-disclosure versus explicit superhuman labelling under controlled\ngame outcomes and usage contexts. Our findings reveal sharp domain-specific\neffects: in StarCraft II, explicitly labelling AI as superhuman, novice\npersonas who learned of it reported lower toxicity and higher\nfairness-attributing defeat to advanced skill rather than hidden\ncheating-whereas expert personas found the disclosure statements irksome but\nstill less deceptive than non-disclosure. Conversely, in the LLM as\npersonal-assistant setting, disclosure of superhuman capabilities improved\nperceived trustworthiness, though it risked AI overreliance among certain\npersona segments. We release Dataset X-containing persona cards-including\nprofile attributes, disclosure prompts, and detailed interaction logs,\naccompanied by reproducible protocols and disclaimers for adapting them to\ndiverse tasks. Our results demonstrate that transparency is not a cure-all:\nwhile it reduces suspicion and enhances trust in cooperative contexts, it may\ninflame resistance or disappointment in competitive domains."}
{"id": "2503.15871", "pdf": "https://arxiv.org/pdf/2503.15871", "abs": "https://arxiv.org/abs/2503.15871", "authors": ["Kyungho Bae", "Jinhyung Kim", "Sihaeng Lee", "Soonyoung Lee", "Gunhee Lee", "Jinwoo Choi"], "title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations", "categories": ["cs.CV"], "comment": "Accepted for CVPR 2025", "summary": "In this work, we tackle action-scene hallucination in Video Large Language\nModels (Video-LLMs), where models incorrectly predict actions based on the\nscene context or scenes based on observed actions. We observe that existing\nVideo-LLMs often suffer from action-scene hallucination due to two main\nfactors. First, existing Video-LLMs intermingle spatial and temporal features\nby applying an attention operation across all tokens. Second, they use the\nstandard Rotary Position Embedding (RoPE), which causes the text tokens to\noveremphasize certain types of tokens depending on their sequential orders. To\naddress these issues, we introduce MASH-VLM, Mitigating Action-Scene\nHallucination in Video-LLMs through disentangled spatial-temporal\nrepresentations. Our approach includes two key innovations: (1) DST-attention,\na novel attention mechanism that disentangles the spatial and temporal tokens\nwithin the LLM by using masked attention to restrict direct interactions\nbetween the spatial and temporal tokens; (2) Harmonic-RoPE, which extends the\ndimensionality of the positional IDs, allowing the spatial and temporal tokens\nto maintain balanced positions relative to the text tokens. To evaluate the\naction-scene hallucination in Video-LLMs, we introduce the UNSCENE benchmark\nwith 1,320 videos and 4,078 QA pairs. Extensive experiments demonstrate that\nMASH-VLM achieves state-of-the-art results on the UNSCENE benchmark, as well as\non existing video understanding benchmarks."}
{"id": "2503.15521", "pdf": "https://arxiv.org/pdf/2503.15521", "abs": "https://arxiv.org/abs/2503.15521", "authors": ["Loukas Triantafyllopoulos", "Dimitris Kalles"], "title": "From Divergence to Consensus: Evaluating the Role of Large Language Models in Facilitating Agreement through Adaptive Strategies", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "32 pages, 5 figures, 4 tables", "summary": "Achieving consensus in group decision-making often involves overcoming\nsignificant challenges, particularly in reconciling diverse perspectives and\nmitigating biases that hinder agreement. Traditional methods relying on human\nfacilitators are often constrained by scalability and efficiency, especially in\nlarge-scale, fast-paced discussions. To address these challenges, this study\nproposes a novel framework employing large language models (LLMs) as automated\nfacilitators within a custom-built multi-user chat system. Leveraging cosine\nsimilarity as a core metric, this approach evaluates the ability of three\nstate-of-the-art LLMs- ChatGPT 4.0, Mistral Large 2, and AI21 Jamba Instruct-\nto synthesize consensus proposals that align with participants' viewpoints.\nUnlike conventional techniques, the system integrates adaptive facilitation\nstrategies, including clarifying misunderstandings, summarizing discussions,\nand proposing compromises, enabling the LLMs to iteratively refine consensus\nproposals based on user feedback. Experimental results demonstrate the\nsuperiority of ChatGPT 4.0, which achieves higher alignment with participant\nopinions, requiring fewer iterations to reach consensus compared to its\ncounterparts. Moreover, analysis reveals the nuanced performance of the models\nacross various sustainability-focused discussion topics, such as climate\naction, quality education, good health and well-being, and access to clean\nwater and sanitation. These findings highlight the transformative potential of\nLLM-driven facilitation for improving collective decision-making processes and\nunderscore the importance of advancing evaluation metrics and cross-cultural\nadaptability in future research."}
{"id": "2503.15875", "pdf": "https://arxiv.org/pdf/2503.15875", "abs": "https://arxiv.org/abs/2503.15875", "authors": ["Haiguang Wang", "Daqi Liu", "Hongwei Xie", "Haisong Liu", "Enhui Ma", "Kaicheng Yu", "Limin Wang", "Bing Wang"], "title": "MiLA: Multi-view Intensive-fidelity Long-term Video Generation World Model for Autonomous Driving", "categories": ["cs.CV"], "comment": "project website: https://github.com/xiaomi-mlab/mila.github.io", "summary": "In recent years, data-driven techniques have greatly advanced autonomous\ndriving systems, but the need for rare and diverse training data remains a\nchallenge, requiring significant investment in equipment and labor. World\nmodels, which predict and generate future environmental states, offer a\npromising solution by synthesizing annotated video data for training. However,\nexisting methods struggle to generate long, consistent videos without\naccumulating errors, especially in dynamic scenes. To address this, we propose\nMiLA, a novel framework for generating high-fidelity, long-duration videos up\nto one minute. MiLA utilizes a Coarse-to-Re(fine) approach to both stabilize\nvideo generation and correct distortion of dynamic objects. Additionally, we\nintroduce a Temporal Progressive Denoising Scheduler and Joint Denoising and\nCorrecting Flow modules to improve the quality of generated videos. Extensive\nexperiments on the nuScenes dataset show that MiLA achieves state-of-the-art\nperformance in video generation quality. For more information, visit the\nproject website: https://github.com/xiaomi-mlab/mila.github.io."}
{"id": "2503.15552", "pdf": "https://arxiv.org/pdf/2503.15552", "abs": "https://arxiv.org/abs/2503.15552", "authors": ["Tharindu Kumarage", "Cameron Johnson", "Jadie Adams", "Lin Ai", "Matthias Kirchner", "Anthony Hoogs", "Joshua Garland", "Julia Hirschberg", "Arslan Basharat", "Huan Liu"], "title": "Personalized Attacks of Social Engineering in Multi-turn Conversations -- LLM Agents for Simulation and Detection", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "The rapid advancement of conversational agents, particularly chatbots powered\nby Large Language Models (LLMs), poses a significant risk of social engineering\n(SE) attacks on social media platforms. SE detection in multi-turn, chat-based\ninteractions is considerably more complex than single-instance detection due to\nthe dynamic nature of these conversations. A critical factor in mitigating this\nthreat is understanding the mechanisms through which SE attacks operate,\nspecifically how attackers exploit vulnerabilities and how victims' personality\ntraits contribute to their susceptibility. In this work, we propose an\nLLM-agentic framework, SE-VSim, to simulate SE attack mechanisms by generating\nmulti-turn conversations. We model victim agents with varying personality\ntraits to assess how psychological profiles influence susceptibility to\nmanipulation. Using a dataset of over 1000 simulated conversations, we examine\nattack scenarios in which adversaries, posing as recruiters, funding agencies,\nand journalists, attempt to extract sensitive information. Based on this\nanalysis, we present a proof of concept, SE-OmniGuard, to offer personalized\nprotection to users by leveraging prior knowledge of the victims personality,\nevaluating attack strategies, and monitoring information exchanges in\nconversations to identify potential SE attempts."}
{"id": "2503.15877", "pdf": "https://arxiv.org/pdf/2503.15877", "abs": "https://arxiv.org/abs/2503.15877", "authors": ["Tiange Xiang", "Kai Li", "Chengjiang Long", "Christian Häne", "Peihong Guo", "Scott Delp", "Ehsan Adeli", "Li Fei-Fei"], "title": "Repurposing 2D Diffusion Models with Gaussian Atlas for 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image diffusion models have been driven by the\nincreasing availability of paired 2D data. However, the development of 3D\ndiffusion models has been hindered by the scarcity of high-quality 3D data,\nresulting in less competitive performance compared to their 2D counterparts. To\naddress this challenge, we propose repurposing pre-trained 2D diffusion models\nfor 3D object generation. We introduce Gaussian Atlas, a novel representation\nthat utilizes dense 2D grids, enabling the fine-tuning of 2D diffusion models\nto generate 3D Gaussians. Our approach demonstrates successful transfer\nlearning from a pre-trained 2D diffusion model to a 2D manifold flattened from\n3D structures. To support model training, we compile GaussianVerse, a\nlarge-scale dataset comprising 205K high-quality 3D Gaussian fittings of\nvarious 3D objects. Our experimental results show that text-to-image diffusion\nmodels can be effectively adapted for 3D content generation, bridging the gap\nbetween 2D and 3D modeling."}
{"id": "2503.15621", "pdf": "https://arxiv.org/pdf/2503.15621", "abs": "https://arxiv.org/abs/2503.15621", "authors": ["Federico Cocchi", "Nicholas Moratelli", "Davide Caffagni", "Sara Sarto", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "title": "LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Recent progress in Multimodal Large Language Models (MLLMs) has highlighted\nthe critical roles of both the visual backbone and the underlying language\nmodel. While prior work has primarily focused on scaling these components to\nbillions of parameters, the trade-offs between model size, architecture, and\nperformance remain underexplored. Additionally, inconsistencies in training\ndata and evaluation protocols have hindered direct comparisons, making it\ndifficult to derive optimal design choices. In this paper, we introduce\nLLaVA-MORE, a new family of MLLMs that integrates recent language models with\ndiverse visual backbones. To ensure fair comparisons, we employ a unified\ntraining protocol applied consistently across all architectures. Our analysis\nsystematically explores both small- and medium-scale LLMs -- including Phi-4,\nLLaMA-3.1, and Gemma-2 -- to evaluate multimodal reasoning, generation, and\ninstruction following, while examining the relationship between model size and\nperformance. Beyond evaluating the LLM impact on final results, we conduct a\ncomprehensive study of various visual encoders, ranging from CLIP-based\narchitectures to alternatives such as DINOv2, SigLIP, and SigLIP2. Additional\nexperiments investigate the effects of increased image resolution and\nvariations in pre-training datasets. Overall, our results provide insights into\nthe design of more effective MLLMs, offering a reproducible evaluation\nframework that facilitates direct comparisons and can guide future model\ndevelopment. Our source code and trained models are publicly available at:\nhttps://github.com/aimagelab/LLaVA-MORE."}
{"id": "2503.15886", "pdf": "https://arxiv.org/pdf/2503.15886", "abs": "https://arxiv.org/abs/2503.15886", "authors": ["Hui Liu", "Wenya Wang", "Kecheng Chen", "Jie Liu", "Yibing Liu", "Tiexin Qin", "Peisong He", "Xinghao Jiang", "Haoliang Li"], "title": "Enhancing Zero-Shot Image Recognition in Vision-Language Models through Human-like Concept Guidance", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 7 figures 7 tables", "summary": "In zero-shot image recognition tasks, humans demonstrate remarkable\nflexibility in classifying unseen categories by composing known simpler\nconcepts. However, existing vision-language models (VLMs), despite achieving\nsignificant progress through large-scale natural language supervision, often\nunderperform in real-world applications because of sub-optimal prompt\nengineering and the inability to adapt effectively to target classes. To\naddress these issues, we propose a Concept-guided Human-like Bayesian Reasoning\n(CHBR) framework. Grounded in Bayes' theorem, CHBR models the concept used in\nhuman image recognition as latent variables and formulates this task by summing\nacross potential concepts, weighted by a prior distribution and a likelihood\nfunction. To tackle the intractable computation over an infinite concept space,\nwe introduce an importance sampling algorithm that iteratively prompts large\nlanguage models (LLMs) to generate discriminative concepts, emphasizing\ninter-class differences. We further propose three heuristic approaches\ninvolving Average Likelihood, Confidence Likelihood, and Test Time Augmentation\n(TTA) Likelihood, which dynamically refine the combination of concepts based on\nthe test image. Extensive evaluations across fifteen datasets demonstrate that\nCHBR consistently outperforms existing state-of-the-art zero-shot\ngeneralization methods."}
{"id": "2503.15661", "pdf": "https://arxiv.org/pdf/2503.15661", "abs": "https://arxiv.org/abs/2503.15661", "authors": ["Shravan Nayak", "Xiangru Jian", "Kevin Qinghong Lin", "Juan A. Rodriguez", "Montek Kalsi", "Rabiul Awal", "Nicolas Chapados", "M. Tamer Özsu", "Aishwarya Agrawal", "David Vazquez", "Christopher Pal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar"], "title": "UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate\ntasks like document editing and file management can greatly enhance computer\nworkflows. While existing research focuses on online settings, desktop\nenvironments, critical for many professional and everyday tasks, remain\nunderexplored due to data collection challenges and licensing issues. We\nintroduce UI-Vision, the first comprehensive, license-permissive benchmark for\noffline, fine-grained evaluation of computer use agents in real-world desktop\nenvironments. Unlike online benchmarks, UI-Vision provides: (i) dense,\nhigh-quality annotations of human demonstrations, including bounding boxes, UI\nlabels, and action trajectories (clicks, drags, and keyboard inputs) across 83\nsoftware applications, and (ii) three fine-to-coarse grained tasks-Element\nGrounding, Layout Grounding, and Action Prediction-with well-defined metrics to\nrigorously evaluate agents' performance in desktop environments. Our evaluation\nreveals critical limitations in state-of-the-art models like UI-TARS-72B,\nincluding issues with understanding professional software, spatial reasoning,\nand complex actions like drag-and-drop. These findings highlight the challenges\nin developing fully autonomous computer use agents. By releasing UI-Vision as\nopen-source, we aim to advance the development of more capable agents for\nreal-world desktop tasks."}
{"id": "2503.15887", "pdf": "https://arxiv.org/pdf/2503.15887", "abs": "https://arxiv.org/abs/2503.15887", "authors": ["Haochen Wang", "Kai Hu", "Liangcai Gao"], "title": "DocVideoQA: Towards Comprehensive Understanding of Document-Centric Videos through Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Remote work and online courses have become important methods of knowledge\ndissemination, leading to a large number of document-based instructional\nvideos. Unlike traditional video datasets, these videos mainly feature\nrich-text images and audio that are densely packed with information closely\ntied to the visual content, requiring advanced multimodal understanding\ncapabilities. However, this domain remains underexplored due to dataset\navailability and its inherent complexity. In this paper, we introduce the\nDocVideoQA task and dataset for the first time, comprising 1454 videos across\n23 categories with a total duration of about 828 hours. The dataset is\nannotated with 154k question-answer pairs generated manually and via GPT,\nassessing models' comprehension, temporal awareness, and modality integration\ncapabilities. Initially, we establish a baseline using open-source MLLMs.\nRecognizing the challenges in modality comprehension for document-centric\nvideos, we present DV-LLaMA, a robust video MLLM baseline. Our method enhances\nunimodal feature extraction with diverse instruction-tuning data and employs\ncontrastive learning to strengthen modality integration. Through fine-tuning,\nthe LLM is equipped with audio-visual capabilities, leading to significant\nimprovements in document-centric video understanding. Extensive testing on the\nDocVideoQA dataset shows that DV-LLaMA significantly outperforms existing\nmodels. We'll release the code and dataset to facilitate future research."}
{"id": "2503.15798", "pdf": "https://arxiv.org/pdf/2503.15798", "abs": "https://arxiv.org/abs/2503.15798", "authors": ["Shibo Jie", "Yehui Tang", "Kai Han", "Yitong Li", "Duyu Tang", "Zhi-Hong Deng", "Yunhe Wang"], "title": "Mixture of Lookup Experts", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mixture-of-Experts (MoE) activates only a subset of experts during inference,\nallowing the model to maintain low inference FLOPs and latency even as the\nparameter count scales up. However, since MoE dynamically selects the experts,\nall the experts need to be loaded into VRAM. Their large parameter size still\nlimits deployment, and offloading, which load experts into VRAM only when\nneeded, significantly increase inference latency. To address this, we propose\nMixture of Lookup Experts (MoLE), a new MoE architecture that is efficient in\nboth communication and VRAM usage. In MoLE, the experts are Feed-Forward\nNetworks (FFNs) during training, taking the output of the embedding layer as\ninput. Before inference, these experts can be re-parameterized as lookup tables\n(LUTs) that retrieves expert outputs based on input ids, and offloaded to\nstorage devices. Therefore, we do not need to perform expert computations\nduring inference. Instead, we directly retrieve the expert's computation\nresults based on input ids and load them into VRAM, and thus the resulting\ncommunication overhead is negligible. Experiments show that, with the same\nFLOPs and VRAM usage, MoLE achieves inference speeds comparable to dense models\nand significantly faster than MoE with experts offloading, while maintaining\nperformance on par with MoE."}
{"id": "2503.15892", "pdf": "https://arxiv.org/pdf/2503.15892", "abs": "https://arxiv.org/abs/2503.15892", "authors": ["Haiyang Yu", "Siyang Yi", "Ke Niu", "Minghan Zhuo", "Bin Li"], "title": "UMIT: Unifying Medical Imaging Tasks via Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of deep learning, particularly in the field of\nmedical image analysis, an increasing number of Vision-Language Models (VLMs)\nare being widely applied to solve complex health and biomedical challenges.\nHowever, existing research has primarily focused on specific tasks or single\nmodalities, which limits their applicability and generalization across diverse\nmedical scenarios. To address this challenge, we propose UMIT, a unified\nmulti-modal, multi-task VLM designed specifically for medical imaging tasks.\nUMIT is able to solve various tasks, including visual question answering,\ndisease detection, and medical report generation. In addition, it is applicable\nto multiple imaging modalities (e.g., X-ray, CT and PET), covering a wide range\nof applications from basic diagnostics to complex lesion analysis. Moreover,\nUMIT supports both English and Chinese, expanding its applicability globally\nand ensuring accessibility to healthcare services in different linguistic\ncontexts. To enhance the model's adaptability and task-handling capability, we\ndesign a unique two-stage training strategy and fine-tune UMIT with designed\ninstruction templates. Through extensive empirical evaluation, UMIT outperforms\nprevious methods in five tasks across multiple datasets. The performance of\nUMIT indicates that it can significantly enhance diagnostic accuracy and\nworkflow efficiency, thus providing effective solutions for medical imaging\napplications."}
{"id": "2503.15808", "pdf": "https://arxiv.org/pdf/2503.15808", "abs": "https://arxiv.org/abs/2503.15808", "authors": ["Katie Seaborn"], "title": "ChatGPT and U(X): A Rapid Review on Measuring the User Experience", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "ChatGPT, powered by a large language model (LLM), has revolutionized everyday\nhuman-computer interaction (HCI) since its 2022 release. While now used by\nmillions around the world, a coherent pathway for evaluating the user\nexperience (UX) ChatGPT offers remains missing. In this rapid review (N = 58),\nI explored how ChatGPT UX has been approached quantitatively so far. I focused\non the independent variables (IVs) manipulated, the dependent variables (DVs)\nmeasured, and the methods used for measurement. Findings reveal trends, gaps,\nand emerging consensus in UX assessments. This work offers a first step towards\nsynthesizing existing approaches to measuring ChatGPT UX, urgent trajectories\nto advance standardization and breadth, and two preliminary frameworks aimed at\nguiding future research and tool development. I seek to elevate the field of\nChatGPT UX by empowering researchers and practitioners in optimizing user\ninteractions with ChatGPT and similar LLM-based systems."}
{"id": "2503.15893", "pdf": "https://arxiv.org/pdf/2503.15893", "abs": "https://arxiv.org/abs/2503.15893", "authors": ["Jiawei Wang", "Kai Hu", "Qiang Huo"], "title": "UniHDSA: A Unified Relation Prediction Approach for Hierarchical Document Structure Analysis", "categories": ["cs.CV"], "comment": "Accepted by Pattern Recognition. arXiv admin note: substantial text\n  overlap with arXiv:2405.11757", "summary": "Document structure analysis, aka document layout analysis, is crucial for\nunderstanding both the physical layout and logical structure of documents,\nserving information retrieval, document summarization, knowledge extraction,\netc. Hierarchical Document Structure Analysis (HDSA) specifically aims to\nrestore the hierarchical structure of documents created using authoring\nsoftware with hierarchical schemas. Previous research has primarily followed\ntwo approaches: one focuses on tackling specific subtasks of HDSA in isolation,\nsuch as table detection or reading order prediction, while the other adopts a\nunified framework that uses multiple branches or modules, each designed to\naddress a distinct task. In this work, we propose a unified relation prediction\napproach for HDSA, called UniHDSA, which treats various HDSA sub-tasks as\nrelation prediction problems and consolidates relation prediction labels into a\nunified label space. This allows a single relation prediction module to handle\nmultiple tasks simultaneously, whether at a page-level or document-level\nstructure analysis. To validate the effectiveness of UniHDSA, we develop a\nmultimodal end-to-end system based on Transformer architectures. Extensive\nexperimental results demonstrate that our approach achieves state-of-the-art\nperformance on a hierarchical document structure analysis benchmark,\nComp-HRDoc, and competitive results on a large-scale document layout analysis\ndataset, DocLayNet, effectively illustrating the superiority of our method\nacross all sub-tasks."}
{"id": "2503.15848", "pdf": "https://arxiv.org/pdf/2503.15848", "abs": "https://arxiv.org/abs/2503.15848", "authors": ["Jinghan Zhang", "Xiting Wang", "Fengran Mo", "Yeyang Zhou", "Wanfu Gao", "Kunpeng Liu"], "title": "Entropy-based Exploration Conduction for Multi-step Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In large language model (LLM) reasoning, multi-step processes have proven\neffective for solving complex tasks. However, the depth of exploration can\nsignificantly affect the reasoning performance. Existing methods to\nautomatically decide the depth often bring high costs and lack flexibility, and\nthus undermine the model's reasoning accuracy. To address these issues, we\npropose Entropy-based Exploration Depth Conduction (Entro-duction), a novel\nmethod that dynamically adjusts the exploration depth during multi-step\nreasoning by monitoring LLM's output entropy and variance entropy. We employ\nthese two metrics to capture the model's current uncertainty and the\nfluctuation of uncertainty across consecutive reasoning steps. Based on the\nobserved changes, the LLM selects whether to deepen, expand or stop exploration\naccording to the probability. In this way, we balance the reasoning accuracy\nand exploration effectiveness. Experimental results across four benchmark\ndatasets demonstrate the efficacy of Entro-duction. We further conduct\nexperiments and analysis on the components of Entro-duction to discuss their\ncontributions to reasoning performance."}
{"id": "2503.15897", "pdf": "https://arxiv.org/pdf/2503.15897", "abs": "https://arxiv.org/abs/2503.15897", "authors": ["Junho Kim", "Gwangtak Bae", "Eun Sun Lee", "Young Min Kim"], "title": "Learning 3D Scene Analogies with Neural Contextual Scene Maps", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Understanding scene contexts is crucial for machines to perform tasks and\nadapt prior knowledge in unseen or noisy 3D environments. As data-driven\nlearning is intractable to comprehensively encapsulate diverse ranges of\nlayouts and open spaces, we propose teaching machines to identify relational\ncommonalities in 3D spaces. Instead of focusing on point-wise or object-wise\nrepresentations, we introduce 3D scene analogies, which are smooth maps between\n3D scene regions that align spatial relationships. Unlike well-studied single\ninstance-level maps, these scene-level maps smoothly link large scene regions,\npotentially enabling unique applications in trajectory transfer in AR/VR, long\ndemonstration transfer for imitation learning, and context-aware object\nrearrangement. To find 3D scene analogies, we propose neural contextual scene\nmaps, which extract descriptor fields summarizing semantic and geometric\ncontexts, and holistically align them in a coarse-to-fine manner for map\nestimation. This approach reduces reliance on individual feature points, making\nit robust to input noise or shape variations. Experiments demonstrate the\neffectiveness of our approach in identifying scene analogies and transferring\ntrajectories or object placements in diverse indoor scenes, indicating its\npotential for robotics and AR/VR applications."}
{"id": "2503.15880", "pdf": "https://arxiv.org/pdf/2503.15880", "abs": "https://arxiv.org/abs/2503.15880", "authors": ["Yunan Wang", "Jijie Li", "Bo-Wen Zhang", "Liangdong Wang", "Guang Liu"], "title": "InCo-DPO: Balancing Distribution Shift and Data Quality for Enhanced Preference Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Direct Preference Optimization (DPO) optimizes language models to align with\nhuman preferences. Utilizing on-policy samples, generated directly by the\npolicy model, typically results in better performance due to its distribution\nconsistency with the model compared to off-policy samples. This paper\nidentifies the quality of candidate preference samples as another critical\nfactor. While the quality of on-policy data is inherently constrained by the\ncapabilities of the policy model, off-policy data, which can be derived from\ndiverse sources, offers greater potential for quality despite experiencing\ndistribution shifts. However, current research mostly relies on on-policy data\nand neglects the value of off-policy data in terms of data quality, due to the\nchallenge posed by distribution shift. In this paper, we propose InCo-DPO, an\nefficient method for synthesizing preference data by integrating on-policy and\noff-policy data, allowing dynamic adjustments to balance distribution shifts\nand data quality, thus finding an optimal trade-off. Consequently, InCo-DPO\novercomes the limitations of distribution shifts in off-policy data and the\nquality constraints of on-policy data. We evaluated InCo-DPO with the\nAlpaca-Eval 2.0 and Arena-Hard benchmarks. Experimental results demonstrate\nthat our approach not only outperforms both on-policy and off-policy data but\nalso achieves a state-of-the-art win rate of 60.8 on Arena-Hard with the\nvanilla DPO using Gemma-2 model."}
{"id": "2503.15898", "pdf": "https://arxiv.org/pdf/2503.15898", "abs": "https://arxiv.org/abs/2503.15898", "authors": ["Boran Wen", "Dingbang Huang", "Zichen Zhang", "Jiahong Zhou", "Jianbin Deng", "Jingyu Gong", "Yulong Chen", "Lizhuang Ma", "Yong-Lu Li"], "title": "Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Reconstructing human-object interactions (HOI) from single images is\nfundamental in computer vision. Existing methods are primarily trained and\ntested on indoor scenes due to the lack of 3D data, particularly constrained by\nthe object variety, making it challenging to generalize to real-world scenes\nwith a wide range of objects. The limitations of previous 3D HOI datasets were\nprimarily due to the difficulty in acquiring 3D object assets. However, with\nthe development of 3D reconstruction from single images, recently it has become\npossible to reconstruct various objects from 2D HOI images. We therefore\npropose a pipeline for annotating fine-grained 3D humans, objects, and their\ninteractions from single images. We annotated 2.5k+ 3D HOI assets from existing\n2D HOI datasets and built the first open-vocabulary in-the-wild 3D HOI dataset\nOpen3DHOI, to serve as a future test set. Moreover, we design a novel\nGaussian-HOI optimizer, which efficiently reconstructs the spatial interactions\nbetween humans and objects while learning the contact regions. Besides the 3D\nHOI reconstruction, we also propose several new tasks for 3D HOI understanding\nto pave the way for future work. Data and code will be publicly available at\nhttps://wenboran2002.github.io/3dhoi."}
{"id": "2503.15948", "pdf": "https://arxiv.org/pdf/2503.15948", "abs": "https://arxiv.org/abs/2503.15948", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Alexander Panchenko", "Vasily Konovalov"], "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025", "summary": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset."}
{"id": "2503.15905", "pdf": "https://arxiv.org/pdf/2503.15905", "abs": "https://arxiv.org/abs/2503.15905", "authors": ["Jiyuan Wang", "Chunyu Lin", "Cheng Guan", "Lang Nie", "Jing He", "Haodong Li", "Kang Liao", "Yao Zhao"], "title": "Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose Jasmine, the first Stable Diffusion (SD)-based\nself-supervised framework for monocular depth estimation, which effectively\nharnesses SD's visual priors to enhance the sharpness and generalization of\nunsupervised prediction. Previous SD-based methods are all supervised since\nadapting diffusion models for dense prediction requires high-precision\nsupervision. In contrast, self-supervised reprojection suffers from inherent\nchallenges (e.g., occlusions, texture-less regions, illumination variance), and\nthe predictions exhibit blurs and artifacts that severely compromise SD's\nlatent priors. To resolve this, we construct a novel surrogate task of hybrid\nimage reconstruction. Without any additional supervision, it preserves the\ndetail priors of SD models by reconstructing the images themselves while\npreventing depth estimation from degradation. Furthermore, to address the\ninherent misalignment between SD's scale and shift invariant estimation and\nself-supervised scale-invariant depth estimation, we build the Scale-Shift GRU.\nIt not only bridges this distribution gap but also isolates the fine-grained\ntexture of SD output against the interference of reprojection loss. Extensive\nexperiments demonstrate that Jasmine achieves SoTA performance on the KITTI\nbenchmark and exhibits superior zero-shot generalization across multiple\ndatasets."}
{"id": "2503.16021", "pdf": "https://arxiv.org/pdf/2503.16021", "abs": "https://arxiv.org/abs/2503.16021", "authors": ["Emil Bakkensen Johansen", "Oliver Baumann"], "title": "Autonomous AI imitators increase diversity in homogeneous information ecosystems", "categories": ["cs.CY", "cs.AI", "cs.CL", "J.4"], "comment": "35 pages, 10 figures, 4 tables", "summary": "Recent breakthroughs in large language models (LLMs) have facilitated\nautonomous AI agents capable of imitating human-generated content. This\ntechnological advancement raises fundamental questions about AI's potential\nimpact on the diversity and democratic value of information ecosystems. Here,\nwe introduce a large-scale simulation framework to examine AI-based imitation\nin news, a context critically influential for public discourse. By\nsystematically testing two distinct imitation strategies across a range of\ninformation environments varying in initial diversity, we demonstrate that\nAI-generated articles do not uniformly homogenize content. Instead, AI's\ninfluence is strongly context-dependent: AI-generated articles can introduce\nvaluable diversity in originally homogeneous news environments, while\npotentially diminishing diversity in contexts that initially display high\nheterogeneity. These results illustrate that the baseline diversity of an\ninformation space critically shapes AI's impact, challenging assumptions that\nAI-driven imitation uniformly threatens information diversity. Instead, when\ninformation is initially homogeneous, AI-driven imitation can expand\nperspectives, styles, and topics. This is especially important in news\ncontexts, where information diversity fosters richer public debate by exposing\ncitizens to alternative viewpoints, challenging biases, and preventing\nnarrative monopolies, which is essential for a resilient democracy."}
{"id": "2503.15908", "pdf": "https://arxiv.org/pdf/2503.15908", "abs": "https://arxiv.org/abs/2503.15908", "authors": ["Jiatong Xia", "Libo Sun", "Lingqiao Liu"], "title": "Enhancing Close-up Novel View Synthesis via Pseudo-labeling", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by AAAI 2025", "summary": "Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian\nSplatting (3DGS), have demonstrated remarkable capabilities in novel view\nsynthesis. However, despite their success in producing high-quality images for\nviewpoints similar to those seen during training, they struggle when generating\ndetailed images from viewpoints that significantly deviate from the training\nset, particularly in close-up views. The primary challenge stems from the lack\nof specific training data for close-up views, leading to the inability of\ncurrent methods to render these views accurately. To address this issue, we\nintroduce a novel pseudo-label-based learning strategy. This approach leverages\npseudo-labels derived from existing training data to provide targeted\nsupervision across a wide range of close-up viewpoints. Recognizing the absence\nof benchmarks for this specific challenge, we also present a new dataset\ndesigned to assess the effectiveness of both current and future methods in this\narea. Our extensive experiments demonstrate the efficacy of our approach."}
{"id": "2503.16036", "pdf": "https://arxiv.org/pdf/2503.16036", "abs": "https://arxiv.org/abs/2503.16036", "authors": ["Zhihang Liu", "Chen-Wei Xie", "Pandeng Li", "Liming Zhao", "Longxiang Tang", "Yun Zheng", "Chuanbin Liu", "Hongtao Xie"], "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPR2025", "summary": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom."}
{"id": "2503.15910", "pdf": "https://arxiv.org/pdf/2503.15910", "abs": "https://arxiv.org/abs/2503.15910", "authors": ["Junsung Park", "Hwijeong Lee", "Inha Kang", "Hyunjung Shim"], "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, CVPR 2025", "summary": "Existing domain generalization methods for LiDAR semantic segmentation under\nadverse weather struggle to accurately predict \"things\" categories compared to\n\"stuff\" categories. In typical driving scenes, \"things\" categories can be\ndynamic and associated with higher collision risks, making them crucial for\nsafe navigation and planning. Recognizing the importance of \"things\"\ncategories, we identify their performance drop as a serious bottleneck in\nexisting approaches. We observed that adverse weather induces degradation of\nsemantic-level features and both corruption of local features, leading to a\nmisprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggest\nour method, NTN - segmeNt Things for No-accident. To address semantic-level\nfeature corruption, we bind each point feature to its superclass, preventing\nthe misprediction of things classes into visually dissimilar categories.\nAdditionally, to enhance robustness against local corruption caused by adverse\nweather, we define each LiDAR beam as a local region and propose a\nregularization term that aligns the clean data with its corrupted counterpart\nin feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU\ngain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the\nSemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9\nmIoU improvement on \"things\" classes, respectively, highlighting its\neffectiveness."}
{"id": "2503.16072", "pdf": "https://arxiv.org/pdf/2503.16072", "abs": "https://arxiv.org/abs/2503.16072", "authors": ["Sergey Berezin", "Reza Farahbakhsh", "Noel Crespi"], "title": "Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The fundamental problem of toxicity detection lies in the fact that the term\n\"toxicity\" is ill-defined. Such uncertainty causes researchers to rely on\nsubjective and vague data during model training, which leads to non-robust and\ninaccurate results, following the 'garbage in - garbage out' paradigm. This\nstudy introduces a novel, objective, and context-aware framework for toxicity\ndetection, leveraging stress levels as a key determinant of toxicity. We\npropose new definition, metric and training approach as a parts of our\nframework and demonstrate it's effectiveness using a dataset we collected."}
{"id": "2503.15914", "pdf": "https://arxiv.org/pdf/2503.15914", "abs": "https://arxiv.org/abs/2503.15914", "authors": ["Jiayi He", "Xu Wang", "Ruobei Zhang", "Shengeng Tang", "Yaxiong Wang", "Lechao Cheng"], "title": "Text-Driven Diffusion Model for Sign Language Production", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "We introduce the hfut-lmc team's solution to the SLRTP Sign Production\nChallenge. The challenge aims to generate semantically aligned sign language\npose sequences from text inputs. To this end, we propose a Text-driven\nDiffusion Model (TDM) framework. During the training phase, TDM utilizes an\nencoder to encode text sequences and incorporates them into the diffusion model\nas conditional input to generate sign pose sequences. To guarantee the high\nquality and accuracy of the generated pose sequences, we utilize two key loss\nfunctions. The joint loss function L_{joint} is used to precisely measure and\nminimize the differences between the joint positions of the generated pose\nsequences and those of the ground truth. Similarly, the bone orientation loss\nfunction L_{bone} is instrumental in ensuring that the orientation of the bones\nin the generated poses aligns with the actual, correct orientations. In the\ninference stage, the TDM framework takes on a different yet equally important\ntask. It starts with noisy sequences and, under the strict constraints of the\ntext conditions, gradually refines and generates semantically consistent sign\nlanguage pose sequences. Our carefully designed framework performs well on the\nsign language production task, and our solution achieves a BLEU-1 score of\n20.17, placing second in the challenge."}
{"id": "2503.16148", "pdf": "https://arxiv.org/pdf/2503.16148", "abs": "https://arxiv.org/abs/2503.16148", "authors": ["Mats Faulborn", "Indira Sen", "Max Pellert", "Andreas Spitz", "David Garcia"], "title": "Only a Little to the Left: A Theory-grounded Measure of Political Bias in Large Language Models", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Prompt-based language models like GPT4 and LLaMa have been used for a wide\nvariety of use cases such as simulating agents, searching for information, or\nfor content analysis. For all of these applications and others, political\nbiases in these models can affect their performance. Several researchers have\nattempted to study political bias in language models using evaluation suites\nbased on surveys, such as the Political Compass Test (PCT), often finding a\nparticular leaning favored by these models. However, there is some variation in\nthe exact prompting techniques, leading to diverging findings and most research\nrelies on constrained-answer settings to extract model responses. Moreover, the\nPolitical Compass Test is not a scientifically valid survey instrument. In this\nwork, we contribute a political bias measured informed by political science\ntheory, building on survey design principles to test a wide variety of input\nprompts, while taking into account prompt sensitivity. We then prompt 11\ndifferent open and commercial models, differentiating between instruction-tuned\nand non-instruction-tuned models, and automatically classify their political\nstances from 88,110 responses. Leveraging this dataset, we compute political\nbias profiles across different prompt variations and find that while PCT\nexaggerates bias in certain models like GPT3.5, measures of political bias are\noften unstable, but generally more left-leaning for instruction-tuned models."}
{"id": "2503.15917", "pdf": "https://arxiv.org/pdf/2503.15917", "abs": "https://arxiv.org/abs/2503.15917", "authors": ["Beilei Cui", "Long Bai", "Mobarakol Islam", "An Wang", "Zhiqi Ma", "Yiming Huang", "Feng Li", "Zhen Chen", "Zhongliang Jiang", "Nassir Navab", "Hongliang Ren"], "title": "Learning to Efficiently Adapt Foundation Models for Self-Supervised Endoscopic 3D Scene Reconstruction from Any Cameras", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D scene reconstruction is essential for numerous medical tasks.\nGiven the challenges in obtaining ground truth data, there has been an\nincreasing focus on self-supervised learning (SSL) for endoscopic depth\nestimation as a basis for scene reconstruction. While foundation models have\nshown remarkable progress in visual tasks, their direct application to the\nmedical domain often leads to suboptimal results. However, the visual features\nfrom these models can still enhance endoscopic tasks, emphasizing the need for\nefficient adaptation strategies, which still lack exploration currently. In\nthis paper, we introduce Endo3DAC, a unified framework for endoscopic scene\nreconstruction that efficiently adapts foundation models. We design an\nintegrated network capable of simultaneously estimating depth maps, relative\nposes, and camera intrinsic parameters. By freezing the backbone foundation\nmodel and training only the specially designed Gated Dynamic Vector-Based\nLow-Rank Adaptation (GDV-LoRA) with separate decoder heads, Endo3DAC achieves\nsuperior depth and pose estimation while maintaining training efficiency.\nAdditionally, we propose a 3D scene reconstruction pipeline that optimizes\ndepth maps' scales, shifts, and a few parameters based on our integrated\nnetwork. Extensive experiments across four endoscopic datasets demonstrate that\nEndo3DAC significantly outperforms other state-of-the-art methods while\nrequiring fewer trainable parameters. To our knowledge, we are the first to\nutilize a single network that only requires surgical videos to perform both SSL\ndepth estimation and scene reconstruction tasks. The code will be released upon\nacceptance."}
{"id": "2503.16167", "pdf": "https://arxiv.org/pdf/2503.16167", "abs": "https://arxiv.org/abs/2503.16167", "authors": ["Hong Yi Lin", "Chunhua Liu", "Haoyu Gao", "Patanamon Thongtanunam", "Christoph Treude"], "title": "CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "State-of-the-art large language models (LLMs) have demonstrated impressive\ncode generation capabilities but struggle with real-world software engineering\ntasks, such as revising source code to address code reviews, hindering their\npractical use. Code review comments are often implicit, ambiguous, and\ncolloquial, requiring models to grasp both code and human intent. This\nchallenge calls for evaluating large language models' ability to bridge both\ntechnical and conversational contexts. While existing work has employed the\nautomated code refinement (ACR) task to resolve these comments, current\nevaluation methods fall short, relying on text matching metrics that provide\nlimited insight into model failures and remain susceptible to training data\ncontamination. To address these limitations, we introduce a novel evaluation\nbenchmark, $\\textbf{CodeReviewQA}$ that enables us to conduct fine-grained\nassessment of model capabilities and mitigate data contamination risks. In\nCodeReviewQA, we decompose the generation task of code refinement into\n$\\textbf{three essential reasoning steps}$: $\\textit{change type recognition}$\n(CTR), $\\textit{change localisation}$ (CL), and $\\textit{solution\nidentification}$ (SI). Each step is reformulated as multiple-choice questions\nwith varied difficulty levels, enabling precise assessment of model\ncapabilities, while mitigating data contamination risks. Our comprehensive\nevaluation spans 72 recently released large language models on $\\textbf{900\nmanually curated, high-quality examples}$ across nine programming languages.\nOur results show that CodeReviewQA is able to expose specific model weaknesses\nin code review comprehension, disentangled from their generative automated code\nrefinement results."}
{"id": "2503.15927", "pdf": "https://arxiv.org/pdf/2503.15927", "abs": "https://arxiv.org/abs/2503.15927", "authors": ["Hui Zhang", "Tingwei Gao", "Jie Shao", "Zuxuan Wu"], "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Diffusion models have demonstrated impressive generation capabilities,\nparticularly with recent advancements leveraging transformer architectures to\nimprove both visual and artistic quality. However, Diffusion Transformers\n(DiTs) continue to encounter challenges related to low inference speed,\nprimarily due to the iterative denoising process. To address this issue, we\npropose BlockDance, a training-free approach that explores feature similarities\nat adjacent time steps to accelerate DiTs. Unlike previous feature-reuse\nmethods that lack tailored reuse strategies for features at different scales,\nBlockDance prioritizes the identification of the most structurally similar\nfeatures, referred to as Structurally Similar Spatio-Temporal (STSS) features.\nThese features are primarily located within the structure-focused blocks of the\ntransformer during the later stages of denoising. BlockDance caches and reuses\nthese highly similar features to mitigate redundant computation, thereby\naccelerating DiTs while maximizing consistency with the generated results of\nthe original model. Furthermore, considering the diversity of generated content\nand the varying distributions of redundant features, we introduce\nBlockDance-Ada, a lightweight decision-making network tailored for\ninstance-specific acceleration. BlockDance-Ada dynamically allocates resources\nand provides superior content quality. Both BlockDance and BlockDance-Ada have\nproven effective across various generation tasks and models, achieving\naccelerations between 25% and 50% while maintaining generation quality."}
{"id": "2503.16184", "pdf": "https://arxiv.org/pdf/2503.16184", "abs": "https://arxiv.org/abs/2503.16184", "authors": ["Andrea Maracani", "Savas Ozkan", "Sijun Cho", "Hyowon Kim", "Eunchung Noh", "Jeongwon Min", "Cho Jung Min", "Dookun Park", "Mete Ozay"], "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs."}
{"id": "2503.15931", "pdf": "https://arxiv.org/pdf/2503.15931", "abs": "https://arxiv.org/abs/2503.15931", "authors": ["Sidi Yang", "Binxiao Huang", "Yulun Zhang", "Dahai Yu", "Yujiu Yang", "Ngai Wong"], "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "While deep neural networks have revolutionized image denoising capabilities,\ntheir deployment on edge devices remains challenging due to substantial\ncomputational and memory requirements. To this end, we present DnLUT, an\nultra-efficient lookup table-based framework that achieves high-quality color\nimage denoising with minimal resource consumption. Our key innovation lies in\ntwo complementary components: a Pairwise Channel Mixer (PCM) that effectively\ncaptures inter-channel correlations and spatial dependencies in parallel, and a\nnovel L-shaped convolution design that maximizes receptive field coverage while\nminimizing storage overhead. By converting these components into optimized\nlookup tables post-training, DnLUT achieves remarkable efficiency - requiring\nonly 500KB storage and 0.1% energy consumption compared to its CNN contestant\nDnCNN, while delivering 20X faster inference. Extensive experiments demonstrate\nthat DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR,\nestablishing a new state-of-the-art in resource-efficient color image\ndenoising. The project is available at https://github.com/Stephen0808/DnLUT."}
{"id": "2503.16219", "pdf": "https://arxiv.org/pdf/2503.16219", "abs": "https://arxiv.org/abs/2503.16219", "authors": ["Quy-Anh Dang", "Chris Ngo"], "title": "Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Enhancing the reasoning capabilities of large language models (LLMs)\ntypically relies on massive computational resources and extensive datasets,\nlimiting accessibility for resource-constrained settings. Our study\ninvestigates the potential of reinforcement learning (RL) to improve reasoning\nin small LLMs, focusing on a 1.5-billion-parameter model,\nDeepSeek-R1-Distill-Qwen-1.5B, under strict constraints: training on 4 NVIDIA\nA40 GPUs (48 GB VRAM each) within 24 hours. Adapting the Group Relative Policy\nOptimization (GRPO) algorithm and curating a compact, high-quality mathematical\nreasoning dataset, we conducted three experiments to explore model behavior and\nperformance. Our results demonstrate rapid reasoning gains - e.g., AMC23\naccuracy rising from 63% to 80% and AIME24 reaching 46.7%, surpassing\no1-preview - using only 7,000 samples and a $42 training cost, compared to\nthousands of dollars for baseline models. However, challenges such as\noptimization instability and length constraints emerged with prolonged\ntraining. These findings highlight the efficacy of RL-based fine-tuning for\nsmall LLMs, offering a cost-effective alternative to large-scale approaches. We\nrelease our code and datasets as open-source resources, providing insights into\ntrade-offs and laying a foundation for scalable, reasoning-capable LLMs in\nresource-limited environments. All are available at\nhttps://github.com/knoveleng/open-rs."}
{"id": "2503.15934", "pdf": "https://arxiv.org/pdf/2503.15934", "abs": "https://arxiv.org/abs/2503.15934", "authors": ["Hongda Liu", "Longguang Wang", "Ye Zhang", "Ziru Yu", "Yulan Guo"], "title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer", "categories": ["cs.CV"], "comment": "11 pages, 10 figures, 2 tables", "summary": "Global effective receptive field plays a crucial role for image style\ntransfer (ST) to obtain high-quality stylized results. However, existing ST\nbackbones (e.g., CNNs and Transformers) suffer huge computational complexity to\nachieve global receptive fields. Recently, the State Space Model (SSM),\nespecially the improved variant Mamba, has shown great potential for long-range\ndependency modeling with linear complexity, which offers a approach to resolve\nthe above dilemma. In this paper, we develop a Mamba-based style transfer\nframework, termed SaMam. Specifically, a mamba encoder is designed to\nefficiently extract content and style information. In addition, a style-aware\nmamba decoder is developed to flexibly adapt to various styles. Moreover, to\naddress the problems of local pixel forgetting, channel redundancy and spatial\ndiscontinuity of existing SSMs, we introduce both local enhancement and zigzag\nscan. Qualitative and quantitative results demonstrate that our SaMam\noutperforms state-of-the-art methods in terms of both accuracy and efficiency."}
{"id": "2503.16394", "pdf": "https://arxiv.org/pdf/2503.16394", "abs": "https://arxiv.org/abs/2503.16394", "authors": ["Akhil Perincherry", "Jacob Krantz", "Stefan Lee"], "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/."}
{"id": "2503.15940", "pdf": "https://arxiv.org/pdf/2503.15940", "abs": "https://arxiv.org/abs/2503.15940", "authors": ["Yaxiong Chen", "Chuang Du", "Chunlei Li", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "UniCrossAdapter: Multimodal Adaptation of CLIP for Radiology Report Generation", "categories": ["cs.CV"], "comment": "MICCAI 2024 Workshop", "summary": "Automated radiology report generation aims to expedite the tedious and\nerror-prone reporting process for radiologists. While recent works have made\nprogress, learning to align medical images and textual findings remains\nchallenging due to the relative scarcity of labeled medical data. For example,\ndatasets for this task are much smaller than those used for image captioning in\ncomputer vision. In this work, we propose to transfer representations from\nCLIP, a large-scale pre-trained vision-language model, to better capture\ncross-modal semantics between images and texts. However, directly applying CLIP\nis suboptimal due to the domain gap between natural images and radiology. To\nenable efficient adaptation, we introduce UniCrossAdapter, lightweight adapter\nmodules that are incorporated into CLIP and fine-tuned on the target task while\nkeeping base parameters fixed. The adapters are distributed across modalities\nand their interaction to enhance vision-language alignment. Experiments on two\npublic datasets demonstrate the effectiveness of our approach, advancing\nstate-of-the-art in radiology report generation. The proposed transfer learning\nframework provides a means of harnessing semantic knowledge from large-scale\npre-trained models to tackle data-scarce medical vision-language tasks. Code is\navailable at https://github.com/chauncey-tow/MRG-CLIP."}
{"id": "2503.16402", "pdf": "https://arxiv.org/pdf/2503.16402", "abs": "https://arxiv.org/abs/2503.16402", "authors": ["Yifan Sun", "Han Wang", "Dongbai Li", "Gang Wang", "Huan Zhang"], "title": "The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "Benchmark Data Contamination (BDC)-the inclusion of benchmark testing samples\nin the training set-has raised increasing concerns in Large Language Model\n(LLM) evaluation, leading to falsely inflated performance estimates and\nundermining evaluation reliability. To address this, researchers have proposed\nvarious mitigation strategies to update existing benchmarks, including\nmodifying original questions or generating new ones based on them. However, a\nrigorous examination of the effectiveness of these mitigation strategies\nremains lacking. In this paper, we design a systematic and controlled pipeline\nalong with two novel metrics-fidelity and contamination resistance-to provide a\nfine-grained and comprehensive assessment of existing BDC mitigation\nstrategies. Previous assessment methods, such as accuracy drop and accuracy\nmatching, focus solely on aggregate accuracy, often leading to incomplete or\nmisleading conclusions. Our metrics address this limitation by emphasizing\nquestion-level evaluation result matching. Extensive experiments with 10 LLMs,\n5 benchmarks, 20 BDC mitigation strategies, and 2 contamination scenarios\nreveal that no existing strategy significantly improves resistance over the\nvanilla case (i.e., no benchmark update) across all benchmarks, and none\neffectively balances fidelity and contamination resistance. These findings\nunderscore the urgent need for designing more effective BDC mitigation\nstrategies. Our code repository is available at\nhttps://github.com/ASTRAL-Group/BDC_mitigation_assessment."}
{"id": "2503.15948", "pdf": "https://arxiv.org/pdf/2503.15948", "abs": "https://arxiv.org/abs/2503.15948", "authors": ["Elisei Rykov", "Kseniia Petrushina", "Kseniia Titova", "Alexander Panchenko", "Vasily Konovalov"], "title": "Don't Fight Hallucinations, Use Them: Estimating Image Realism using NLI over Atomic Facts", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Proceedings of De-Factify 4: 4nd Workshop on Multimodal Fact Checking\n  and Hate Speech Detection, co-located with AAAI-2025", "summary": "Quantifying the realism of images remains a challenging problem in the field\nof artificial intelligence. For example, an image of Albert Einstein holding a\nsmartphone violates common-sense because modern smartphone were invented after\nEinstein's death. We introduce a novel method for assessing image realism using\nLarge Vision-Language Models (LVLMs) and Natural Language Inference (NLI). Our\napproach is based on the premise that LVLMs may generate hallucinations when\nconfronted with images that defy common sense. Using LVLM to extract atomic\nfacts from these images, we obtain a mix of accurate facts and erroneous\nhallucinations. We proceed by calculating pairwise entailment scores among\nthese facts, subsequently aggregating these values to yield a singular reality\nscore. This process serves to identify contradictions between genuine facts and\nhallucinatory elements, signaling the presence of images that violate common\nsense. Our approach has achieved a new state-of-the-art performance in\nzero-shot mode on the WHOOPS! dataset."}
{"id": "2503.16416", "pdf": "https://arxiv.org/pdf/2503.16416", "abs": "https://arxiv.org/abs/2503.16416", "authors": ["Asaf Yehudai", "Lilach Eden", "Alan Li", "Guy Uziel", "Yilun Zhao", "Roy Bar-Haim", "Arman Cohan", "Michal Shmueli-Scheuer"], "title": "Survey on Evaluation of LLM-based Agents", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling\nautonomous systems to plan, reason, use tools, and maintain memory while\ninteracting with dynamic environments. This paper provides the first\ncomprehensive survey of evaluation methodologies for these increasingly capable\nagents. We systematically analyze evaluation benchmarks and frameworks across\nfour critical dimensions: (1) fundamental agent capabilities, including\nplanning, tool use, self-reflection, and memory; (2) application-specific\nbenchmarks for web, software engineering, scientific, and conversational\nagents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating\nagents. Our analysis reveals emerging trends, including a shift toward more\nrealistic, challenging evaluations with continuously updated benchmarks. We\nalso identify critical gaps that future research must address-particularly in\nassessing cost-efficiency, safety, and robustness, and in developing\nfine-grained, and scalable evaluation methods. This survey maps the rapidly\nevolving landscape of agent evaluation, reveals the emerging trends in the\nfield, identifies current limitations, and proposes directions for future\nresearch."}
{"id": "2503.15949", "pdf": "https://arxiv.org/pdf/2503.15949", "abs": "https://arxiv.org/abs/2503.15949", "authors": ["Yaxiong Chen", "Minghong Wei", "Zixuan Zheng", "Jingliang Hu", "Yilei Shi", "Shengwu Xiong", "Xiao Xiang Zhu", "Lichao Mou"], "title": "CausalCLIPSeg: Unlocking CLIP's Potential in Referring Medical Image Segmentation with Causal Intervention", "categories": ["cs.CV"], "comment": "MICCAI 2024", "summary": "Referring medical image segmentation targets delineating lesions indicated by\ntextual descriptions. Aligning visual and textual cues is challenging due to\ntheir distinct data properties. Inspired by large-scale pre-trained\nvision-language models, we propose CausalCLIPSeg, an end-to-end framework for\nreferring medical image segmentation that leverages CLIP. Despite not being\ntrained on medical data, we enforce CLIP's rich semantic space onto the medical\ndomain by a tailored cross-modal decoding method to achieve text-to-pixel\nalignment. Furthermore, to mitigate confounding bias that may cause the model\nto learn spurious correlations instead of meaningful causal relationships,\nCausalCLIPSeg introduces a causal intervention module which self-annotates\nconfounders and excavates causal features from inputs for segmentation\njudgments. We also devise an adversarial min-max game to optimize causal\nfeatures while penalizing confounding ones. Extensive experiments demonstrate\nthe state-of-the-art performance of our proposed method. Code is available at\nhttps://github.com/WUTCM-Lab/CausalCLIPSeg."}
{"id": "2503.15969", "pdf": "https://arxiv.org/pdf/2503.15969", "abs": "https://arxiv.org/abs/2503.15969", "authors": ["Clive Tinashe Marimo", "Benedikt Blumenstiel", "Maximilian Nitsche", "Johannes Jakubik", "Thomas Brunschwiler"], "title": "Beyond the Visible: Multispectral Vision-Language Learning for Earth Observation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models for Earth observation (EO) typically rely on the\nvisual spectrum of data as the only model input, thus failing to leverage the\nrich spectral information available in the multispectral channels recorded by\nsatellites. Therefore, in this paper, we introduce Llama3-MS-CLIP, the first\nvision-language model pre-trained with contrastive learning on a large-scale\nmultispectral dataset and report on the performance gains due to the extended\nspectral range. Furthermore, we present the largest-to-date image-caption\ndataset for multispectral data, consisting of one million Sentinel-2 samples\nand corresponding textual descriptions generated with Llama3-LLaVA-Next and\nOverture Maps data. We develop a scalable captioning pipeline, which is\nvalidated by domain experts. We evaluate Llama3-MS-CLIP on multispectral\nzero-shot image classification and retrieval using three datasets of varying\ncomplexity. Our results demonstrate that Llama3-MS-CLIP significantly\noutperforms other RGB-based approaches, improving classification accuracy by\n6.77% on average and retrieval performance by 4.63% mAP compared to the\nsecond-best model. Our results emphasize the relevance of multispectral\nvision-language learning. We release the image-caption dataset, code, and model\nweights under an open-source license."}
{"id": "2503.15970", "pdf": "https://arxiv.org/pdf/2503.15970", "abs": "https://arxiv.org/abs/2503.15970", "authors": ["JunGyu Lee", "Kunyoung Lee", "Haesol Park", "Ig-Jae Kim", "Gi Pyo Nam"], "title": "V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Facial Expression Recognition (FER) plays a crucial role in human affective\nanalysis and has been widely applied in computer vision tasks such as\nhuman-computer interaction and psychological assessment. The 8th Affective\nBehavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions\nusing the video-based Aff-Wild2 dataset. This challenge includes various tasks,\nincluding the video-based EXPR recognition track, which is our primary focus.\nIn this paper, we demonstrate that addressing label ambiguity and class\nimbalance, which are known to cause performance degradation, can lead to\nmeaningful performance improvements. Specifically, we propose Video-based\nNoise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to\neach frame in a clip to address label ambiguity and effectively capture\ntemporal variations in facial expressions. Furthermore, we introduce a simple\nand effective augmentation strategy to reduce redundancy between consecutive\nframes, which is a primary cause of overfitting. Through extensive experiments,\nwe validate the effectiveness of our approach, demonstrating significant\nimprovements in video-based FER performance."}
{"id": "2503.15973", "pdf": "https://arxiv.org/pdf/2503.15973", "abs": "https://arxiv.org/abs/2503.15973", "authors": ["Zichen Liu", "Kunlun Xu", "Bing Su", "Xu Zou", "Yuxin Peng", "Jiahuan Zhou"], "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP\nhave demonstrated promising zero-shot generalization across numerous\nimage-based tasks. However, extending these capabilities to video tasks remains\nchallenging due to limited labeled video data and high training costs. Recent\nvideo prompting methods attempt to adapt CLIP for video tasks by introducing\nlearnable prompts, but they typically rely on a single static prompt for all\nvideo sequences, overlooking the diverse temporal dynamics and spatial\nvariations that exist across frames. This limitation significantly hinders the\nmodel's ability to capture essential temporal information for effective video\nunderstanding. To address this, we propose an integrated Spatial-TempOral\ndynamic Prompting (STOP) model which consists of two complementary modules, the\nintra-frame spatial prompting and inter-frame temporal prompting. Our\nintra-frame spatial prompts are designed to adaptively highlight discriminative\nregions within each frame by leveraging intra-frame attention and temporal\nvariation, allowing the model to focus on areas with substantial temporal\ndynamics and capture fine-grained spatial details. Additionally, to highlight\nthe varying importance of frames for video understanding, we further introduce\ninter-frame temporal prompts, dynamically inserting prompts between frames with\nhigh temporal variance as measured by frame similarity. This enables the model\nto prioritize key frames and enhances its capacity to understand temporal\ndependencies across sequences. Extensive experiments on various video\nbenchmarks demonstrate that STOP consistently achieves superior performance\nagainst state-of-the-art methods. The code is available at\nhttps://github.com/zhoujiahuan1991/CVPR2025-STOP."}
{"id": "2503.15975", "pdf": "https://arxiv.org/pdf/2503.15975", "abs": "https://arxiv.org/abs/2503.15975", "authors": ["Kendong Liu", "Zhiyu Zhu", "Hui Liu", "Junhui Hou"], "title": "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We present Acc3D to tackle the challenge of accelerating the diffusion\nprocess to generate 3D models from single images. To derive high-quality\nreconstructions through few-step inferences, we emphasize the critical issue of\nregularizing the learning of score function in states of random noise. To this\nend, we propose edge consistency, i.e., consistent predictions across the high\nsignal-to-noise ratio region, to enhance a pre-trained diffusion model,\nenabling a distillation-based refinement of the endpoint score function.\nBuilding on those distilled diffusion models, we propose an adversarial\naugmentation strategy to further enrich the generation detail and boost overall\ngeneration quality. The two modules complement each other, mutually reinforcing\nto elevate generative performance. Extensive experiments demonstrate that our\nAcc3D not only achieves over a $20\\times$ increase in computational efficiency\nbut also yields notable quality improvements, compared to the\nstate-of-the-arts."}
{"id": "2503.15978", "pdf": "https://arxiv.org/pdf/2503.15978", "abs": "https://arxiv.org/abs/2503.15978", "authors": ["Pengyu Liu", "Guohua Dong", "Dan Guo", "Kun Li", "Fengling Li", "Xun Yang", "Meng Wang", "Xiaomin Ying"], "title": "A Survey on fMRI-based Brain Decoding for Reconstructing Multimodal Stimuli", "categories": ["cs.CV"], "comment": "31 pages, 6 figures", "summary": "In daily life, we encounter diverse external stimuli, such as images, sounds,\nand videos. As research in multimodal stimuli and neuroscience advances,\nfMRI-based brain decoding has become a key tool for understanding brain\nperception and its complex cognitive processes. Decoding brain signals to\nreconstruct stimuli not only reveals intricate neural mechanisms but also\ndrives progress in AI, disease treatment, and brain-computer interfaces. Recent\nadvancements in neuroimaging and image generation models have significantly\nimproved fMRI-based decoding. While fMRI offers high spatial resolution for\nprecise brain activity mapping, its low temporal resolution and signal noise\npose challenges. Meanwhile, techniques like GANs, VAEs, and Diffusion Models\nhave enhanced reconstructed image quality, and multimodal pre-trained models\nhave boosted cross-modal decoding tasks. This survey systematically reviews\nrecent progress in fMRI-based brain decoding, focusing on stimulus\nreconstruction from passive brain signals. It summarizes datasets, relevant\nbrain regions, and categorizes existing methods by model structure.\nAdditionally, it evaluates model performance and discusses their effectiveness.\nFinally, it identifies key challenges and proposes future research directions,\noffering valuable insights for the field. For more information and resources\nrelated to this survey, visit https://github.com/LpyNow/BrainDecodingImage."}
{"id": "2503.15984", "pdf": "https://arxiv.org/pdf/2503.15984", "abs": "https://arxiv.org/abs/2503.15984", "authors": ["Suraj Singh", "Anastasia Batsheva", "Oleg Y. Rogov", "Ahmed Bouridane"], "title": "DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration", "categories": ["cs.CV", "astro-ph.IM", "cs.AI", "eess.IV"], "comment": "10 pages, 7 figures, 2 tables", "summary": "Contemporary image restoration and super-resolution techniques effectively\nharness deep neural networks, markedly outperforming traditional methods.\nHowever, astrophotography presents unique challenges for deep learning due to\nlimited training data. This work explores hybrid strategies, such as the Deep\nImage Prior (DIP) model, which facilitates blind training but is susceptible to\noverfitting, artifact generation, and instability when handling noisy images.\nWe propose enhancements to the DIP model's baseline performance through several\nadvanced techniques. First, we refine the model to process multiple frames\nconcurrently, employing the Back Projection method and the TVNet model. Next,\nwe adopt a Markov approach incorporating Monte Carlo estimation, Langevin\ndynamics, and a variational input technique to achieve unbiased estimates with\nminimal variance and counteract overfitting effectively. Collectively, these\nmodifications reduce the likelihood of noise learning and mitigate loss\nfunction fluctuations during training, enhancing result stability. We validated\nour algorithm across multiple image sets of astronomical and celestial objects,\nachieving performance that not only mitigates limitations of Lucky Imaging, a\nclassical computer vision technique that remains a standard in astronomical\nimage reconstruction but surpasses the original DIP model, state of the art\ntransformer- and diffusion-based models, underscoring the significance of our\nimprovements."}
{"id": "2503.15997", "pdf": "https://arxiv.org/pdf/2503.15997", "abs": "https://arxiv.org/abs/2503.15997", "authors": ["P. Schulz", "T. Hempel", "A. Al-Hamadi"], "title": "Automating 3D Dataset Generation with Neural Radiance Fields", "categories": ["cs.CV"], "comment": "Accepted and presented at ROBOVIS 2025 (5th International Conference\n  on Robotics, Computer Vision and Intelligent Systems)", "summary": "3D detection is a critical task to understand spatial characteristics of the\nenvironment and is used in a variety of applications including robotics,\naugmented reality, and image retrieval. Training performant detection models\nrequire diverse, precisely annotated, and large scale datasets that involve\ncomplex and expensive creation processes. Hence, there are only few public 3D\ndatasets that are additionally limited in their range of classes. In this work,\nwe propose a pipeline for automatic generation of 3D datasets for arbitrary\nobjects. By utilizing the universal 3D representation and rendering\ncapabilities of Radiance Fields, our pipeline generates high quality 3D models\nfor arbitrary objects. These 3D models serve as input for a synthetic dataset\ngenerator. Our pipeline is fast, easy to use and has a high degree of\nautomation. Our experiments demonstrate, that 3D pose estimation networks,\ntrained with our generated datasets, archive strong performance in typical\napplication scenarios."}
{"id": "2503.16000", "pdf": "https://arxiv.org/pdf/2503.16000", "abs": "https://arxiv.org/abs/2503.16000", "authors": ["Haojia Gao", "Haohua Que", "Hoiian Au", "Weihao Shan", "Mingkai Liu", "Yusen Qin", "Lei Mu", "Rong Zhao", "Xinghua Yang", "Qi Wei", "Fei Qiao"], "title": "SenseExpo: Efficient Autonomous Exploration with Prediction Information from Lightweight Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes SenseExpo, an efficient autonomous exploration framework\nbased on a lightweight prediction network, which addresses the limitations of\ntraditional methods in computational overhead and environmental generalization.\nBy integrating Generative Adversarial Networks (GANs), Transformer, and Fast\nFourier Convolution (FFC), we designed a lightweight prediction model with\nmerely 709k parameters. Our smallest model achieves better performance on the\nKTH dataset than U-net (24.5M) and LaMa (51M), delivering PSNR 9.026 and SSIM\n0.718, particularly representing a 38.7% PSNR improvement over the\n51M-parameter LaMa model. Cross-domain testing demonstrates its strong\ngeneralization capability, with an FID score of 161.55 on the HouseExpo\ndataset, significantly outperforming comparable methods. Regarding exploration\nefficiency, on the KTH dataset,SenseExpo demonstrates approximately a 67.9%\ntime reduction in exploration time compared to MapEx. On the MRPB 1.0 dataset,\nSenseExpo achieves 77.1% time reduction roughly compared to MapEx. Deployed as\na plug-and-play ROS node, the framework seamlessly integrates with existing\nnavigation systems, providing an efficient solution for resource-constrained\ndevices."}
{"id": "2503.16012", "pdf": "https://arxiv.org/pdf/2503.16012", "abs": "https://arxiv.org/abs/2503.16012", "authors": ["Stijn Groenen", "Marzieh Hassanshahi Varposhti", "Mahyar Shahsavari"], "title": "GazeSCRNN: Event-based Near-eye Gaze Tracking using a Spiking Neural Network", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "This work introduces GazeSCRNN, a novel spiking convolutional recurrent\nneural network designed for event-based near-eye gaze tracking. Leveraging the\nhigh temporal resolution, energy efficiency, and compatibility of Dynamic\nVision Sensor (DVS) cameras with event-based systems, GazeSCRNN uses a spiking\nneural network (SNN) to address the limitations of traditional gaze-tracking\nsystems in capturing dynamic movements. The proposed model processes event\nstreams from DVS cameras using Adaptive Leaky-Integrate-and-Fire (ALIF) neurons\nand a hybrid architecture optimized for spatio-temporal data. Extensive\nevaluations on the EV-Eye dataset demonstrate the model's accuracy in\npredicting gaze vectors. In addition, we conducted ablation studies to reveal\nthe importance of the ALIF neurons, dynamic event framing, and training\ntechniques, such as Forward-Propagation-Through-Time, in enhancing overall\nsystem performance. The most accurate model achieved a Mean Angle Error (MAE)\nof 6.034{\\deg} and a Mean Pupil Error (MPE) of 2.094 mm. Consequently, this\nwork is pioneering in demonstrating the feasibility of using SNNs for\nevent-based gaze tracking, while shedding light on critical challenges and\nopportunities for further improvement."}
{"id": "2503.16025", "pdf": "https://arxiv.org/pdf/2503.16025", "abs": "https://arxiv.org/abs/2503.16025", "authors": ["Yair Shpitzer", "Gal Chechik", "Idan Schwartz"], "title": "Single Image Iterative Subject-driven Generation and Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Project page is at https://siso-paper.github.io/", "summary": "Personalizing image generation and editing is particularly challenging when\nwe only have a few images of the subject, or even a single image. A common\napproach to personalization is concept learning, which can integrate the\nsubject into existing models relatively quickly, but produces images whose\nquality tends to deteriorate quickly when the number of subject images is\nsmall. Quality can be improved by pre-training an encoder, but training\nrestricts generation to the training distribution, and is time consuming. It is\nstill an open hard challenge to personalize image generation and editing from a\nsingle image without training. Here, we present SISO, a novel, training-free\napproach based on optimizing a similarity score with an input subject image.\nMore specifically, SISO iteratively generates images and optimizes the model\nbased on loss of similarity with the given subject image until a satisfactory\nlevel of similarity is achieved, allowing plug-and-play optimization to any\nimage generator. We evaluated SISO in two tasks, image editing and image\ngeneration, using a diverse data set of personal subjects, and demonstrate\nsignificant improvements over existing methods in image quality, subject\nfidelity, and background preservation."}
{"id": "2503.16032", "pdf": "https://arxiv.org/pdf/2503.16032", "abs": "https://arxiv.org/abs/2503.16032", "authors": ["Sunqi Fan", "Meng-Hao Guo", "Shuojin Yang"], "title": "Agentic Keyframe Search for Video Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Video question answering (VideoQA) enables machines to extract and comprehend\nkey information from videos through natural language interaction, which is a\ncritical step towards achieving intelligence. However, the demand for a\nthorough understanding of videos and high computational costs still limit the\nwidespread applications of VideoQA. To address it, we propose Agentic Keyframe\nSearch (AKeyS), a simple yet powerful algorithm for identifying keyframes in\nthe VideoQA task. It can effectively distinguish key information from\nredundant, irrelevant content by leveraging modern language agents to direct\nclassical search algorithms. Specifically, we first segment the video and\norganize it as a tree structure. Then, AKeyS uses a language agent to estimate\nheuristics and movement costs while dynamically expanding nodes. Finally, the\nagent determines if sufficient keyframes have been collected based on\ntermination conditions and provides answers. Extensive experiments on the\nEgoSchema and NExT-QA datasets show that AKeyS outperforms all previous methods\nwith the highest keyframe searching efficiency, which means it can accurately\nidentify key information and conduct effective visual reasoning with minimal\ncomputational overhead. For example, on the EgoSchema subset, it achieves 1.8%\nhigher accuracy while processing only 43.5% of the frames compared to\nVideoTree. We believe that AKeyS represents a significant step towards building\nintelligent agents for video understanding. The code is publicly available at\nhttps://github.com/fansunqi/AKeyS."}
{"id": "2503.16036", "pdf": "https://arxiv.org/pdf/2503.16036", "abs": "https://arxiv.org/abs/2503.16036", "authors": ["Zhihang Liu", "Chen-Wei Xie", "Pandeng Li", "Liming Zhao", "Longxiang Tang", "Yun Zheng", "Chuanbin Liu", "Hongtao Xie"], "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted to CVPR2025", "summary": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the\ncomputational overhead resulting from massive video frames, often alleviated\nthrough compression strategies. However, the visual content is not equally\ncontributed to user instructions, existing strategies (\\eg, average pool)\ninevitably lead to the loss of potentially useful information. To tackle this,\nwe propose the Hybrid-level Instruction Injection Strategy for Conditional\nToken Compression in MLLMs (HICom), utilizing the instruction as a condition to\nguide the compression from both local and global levels. This encourages the\ncompression to retain the maximum amount of user-focused information while\nreducing visual tokens to minimize computational burden. Specifically, the\ninstruction condition is injected into the grouped visual tokens at the local\nlevel and the learnable tokens at the global level, and we conduct the\nattention mechanism to complete the conditional compression. From the\nhybrid-level compression, the instruction-relevant visual parts are highlighted\nwhile the temporal-spatial structure is also preserved for easier understanding\nof LLMs. To further unleash the potential of HICom, we introduce a new\nconditional pre-training stage with our proposed dataset HICom-248K.\nExperiments show that our HICom can obtain distinguished video understanding\nability with fewer tokens, increasing the performance by 2.43\\% average on\nthree multiple-choice QA benchmarks and saving 78.8\\% tokens compared with the\nSOTA method. The code is available at https://github.com/lntzm/HICom."}
{"id": "2503.16051", "pdf": "https://arxiv.org/pdf/2503.16051", "abs": "https://arxiv.org/abs/2503.16051", "authors": ["Andrei Jelea", "Ahmed Nabil Belbachir", "Marius Leordeanu"], "title": "Closer to Ground Truth: Realistic Shape and Appearance Labeled Data Generation for Unsupervised Underwater Image Segmentation", "categories": ["cs.CV"], "comment": "Proceedings of ECCVW 2024", "summary": "Solving fish segmentation in underwater videos, a real-world problem of great\npractical value in marine and aquaculture industry, is a challenging task due\nto the difficulty of the filming environment, poor visibility and limited\nexisting annotated underwater fish data. In order to overcome these obstacles,\nwe introduce a novel two stage unsupervised segmentation approach that requires\nno human annotations and combines artificially created and real images. Our\nmethod generates challenging synthetic training data, by placing virtual fish\nin real-world underwater habitats, after performing fish transformations such\nas Thin Plate Spline shape warping and color Histogram Matching, which\nrealistically integrate synthetic fish into the backgrounds, making the\ngenerated images increasingly closer to the real world data with every stage of\nour approach. While we validate our unsupervised method on the popular DeepFish\ndataset, obtaining a performance close to a fully-supervised SoTA model, we\nfurther show its effectiveness on the specific case of salmon segmentation in\nunderwater videos, for which we introduce DeepSalmon, the largest dataset of\nits kind in the literature (30 GB). Moreover, on both datasets we prove the\ncapability of our approach to boost the performance of the fully-supervised\nSoTA model."}
{"id": "2503.16056", "pdf": "https://arxiv.org/pdf/2503.16056", "abs": "https://arxiv.org/abs/2503.16056", "authors": ["Wanshu Fan", "Yue Wang", "Cong Wang", "Yunzhe Zhang", "Wei Wang", "Dongsheng Zhou"], "title": "Semantic-Guided Global-Local Collaborative Networks for Lightweight Image Super-Resolution", "categories": ["cs.CV"], "comment": "14 pages,13 figures, 9 tables", "summary": "Single-Image Super-Resolution (SISR) plays a pivotal role in enhancing the\naccuracy and reliability of measurement systems, which are integral to various\nvision-based instrumentation and measurement applications. These systems often\nrequire clear and detailed images for precise object detection and recognition.\nHowever, images captured by visual measurement tools frequently suffer from\ndegradation, including blurring and loss of detail, which can impede\nmeasurement accuracy.As a potential remedy, we in this paper propose a\nSemantic-Guided Global-Local Collaborative Network (SGGLC-Net) for lightweight\nSISR. Our SGGLC-Net leverages semantic priors extracted from a pre-trained\nmodel to guide the super-resolution process, enhancing image detail quality\neffectively. Specifically,we propose a Semantic Guidance Module that seamlessly\nintegrates the semantic priors into the super-resolution network, enabling the\nnetwork to more adeptly capture and utilize semantic priors, thereby enhancing\nimage details. To further explore both local and non-local interactions for\nimproved detail rendition,we propose a Global-Local Collaborative Module, which\nfeatures three Global and Local Detail Enhancement Modules, as well as a Hybrid\nAttention Mechanism to work together to efficiently learn more useful features.\nOur extensive experiments show that SGGLC-Net achieves competitive PSNR and\nSSIM values across multiple benchmark datasets, demonstrating higher\nperformance with the multi-adds reduction of 12.81G compared to\nstate-of-the-art lightweight super-resolution approaches. These improvements\nunderscore the potential of our approach to enhance the precision and\neffectiveness of visual measurement systems. Codes are at\nhttps://github.com/fanamber831/SGGLC-Net."}
{"id": "2503.16057", "pdf": "https://arxiv.org/pdf/2503.16057", "abs": "https://arxiv.org/abs/2503.16057", "authors": ["Yike Yuan", "Ziyu Wang", "Zihao Huang", "Defa Zhu", "Xun Zhou", "Jingyi Yu", "Qiyang Min"], "title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Diffusion models have emerged as mainstream framework in visual generation.\nBuilding upon this success, the integration of Mixture of Experts (MoE) methods\nhas shown promise in enhancing model scalability and performance. In this\npaper, we introduce Race-DiT, a novel MoE model for diffusion transformers with\na flexible routing strategy, Expert Race. By allowing tokens and experts to\ncompete together and select the top candidates, the model learns to dynamically\nassign experts to critical tokens. Additionally, we propose per-layer\nregularization to address challenges in shallow layer learning, and router\nsimilarity loss to prevent mode collapse, ensuring better expert utilization.\nExtensive experiments on ImageNet validate the effectiveness of our approach,\nshowcasing significant performance gains while promising scaling properties."}
{"id": "2503.16058", "pdf": "https://arxiv.org/pdf/2503.16058", "abs": "https://arxiv.org/abs/2503.16058", "authors": ["Xu He", "Zhen Huang", "Qingsong Yao", "Xiaoqian Zhou", "S. Kevin Zhou"], "title": "Landmarks Are Alike Yet Distinct: Harnessing Similarity and Individuality for One-Shot Medical Landmark Detection", "categories": ["cs.CV"], "comment": null, "summary": "Landmark detection plays a crucial role in medical imaging applications such\nas disease diagnosis, bone age estimation, and therapy planning. However,\ntraining models for detecting multiple landmarks simultaneously often\nencounters the \"seesaw phenomenon\", where improvements in detecting certain\nlandmarks lead to declines in detecting others. Yet, training a separate model\nfor each landmark increases memory usage and computational overhead. To address\nthese challenges, we propose a novel approach based on the belief that\n\"landmarks are distinct\" by training models with pseudo-labels and template\ndata updated continuously during the training process, where each model is\ndedicated to detecting a single landmark to achieve high accuracy. Furthermore,\ngrounded on the belief that \"landmarks are also alike\", we introduce an\nadapter-based fusion model, combining shared weights with landmark-specific\nweights, to efficiently share model parameters while allowing flexible\nadaptation to individual landmarks. This approach not only significantly\nreduces memory and computational resource requirements but also effectively\nmitigates the seesaw phenomenon in multi-landmark training. Experimental\nresults on publicly available medical image datasets demonstrate that the\nsingle-landmark models significantly outperform traditional multi-point joint\ntraining models in detecting individual landmarks. Although our adapter-based\nfusion model shows slightly lower performance compared to the combined results\nof all single-landmark models, it still surpasses the current state-of-the-art\nmethods while achieving a notable improvement in resource efficiency."}
{"id": "2503.16064", "pdf": "https://arxiv.org/pdf/2503.16064", "abs": "https://arxiv.org/abs/2503.16064", "authors": ["Qiang Zou", "Shuli Cheng", "Jiayi Chen"], "title": "PromptHash: Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.MM"], "comment": "Accepted by CVPR2025", "summary": "Cross-modal hashing is a promising approach for efficient data retrieval and\nstorage optimization. However, contemporary methods exhibit significant\nlimitations in semantic preservation, contextual integrity, and information\nredundancy, which constrains retrieval efficacy. We present PromptHash, an\ninnovative framework leveraging affinity prompt-aware collaborative learning\nfor adaptive cross-modal hashing. We propose an end-to-end framework for\naffinity-prompted collaborative hashing, with the following fundamental\ntechnical contributions: (i) a text affinity prompt learning mechanism that\npreserves contextual information while maintaining parameter efficiency, (ii)\nan adaptive gated selection fusion architecture that synthesizes State Space\nModel with Transformer network for precise cross-modal feature integration, and\n(iii) a prompt affinity alignment strategy that bridges modal heterogeneity\nthrough hierarchical contrastive learning. To the best of our knowledge, this\nstudy presents the first investigation into affinity prompt awareness within\ncollaborative cross-modal adaptive hash learning, establishing a paradigm for\nenhanced semantic consistency across modalities. Through comprehensive\nevaluation on three benchmark multi-label datasets, PromptHash demonstrates\nsubstantial performance improvements over existing approaches. Notably, on the\nNUS-WIDE dataset, our method achieves significant gains of 18.22% and 18.65% in\nimage-to-text and text-to-image retrieval tasks, respectively. The code is\npublicly available at https://github.com/ShiShuMo/PromptHash."}
{"id": "2503.16065", "pdf": "https://arxiv.org/pdf/2503.16065", "abs": "https://arxiv.org/abs/2503.16065", "authors": ["Yingmao Miao", "Zhanpeng Huang", "Rui Han", "Zibin Wang", "Chenhao Lin", "Chao Shen"], "title": "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "While virtual try-on for clothes and shoes with diffusion models has gained\nattraction, virtual try-on for ornaments, such as bracelets, rings, earrings,\nand necklaces, remains largely unexplored. Due to the intricate tiny patterns\nand repeated geometric sub-structures in most ornaments, it is much more\ndifficult to guarantee identity and appearance consistency under large pose and\nscale variances between ornaments and models. This paper proposes the task of\nvirtual try-on for ornaments and presents a method to improve the geometric and\nappearance preservation of ornament virtual try-ons. Specifically, we estimate\nan accurate wearing mask to improve the alignments between ornaments and models\nin an iterative scheme alongside the denoising process. To preserve structure\ndetails, we further regularize attention layers to map the reference ornament\nmask to the wearing mask in an implicit way. Experimental results demonstrate\nthat our method successfully wears ornaments from reference images onto target\nmodels, handling substantial differences in scale and pose while preserving\nidentity and achieving realistic visual effects."}
{"id": "2503.16067", "pdf": "https://arxiv.org/pdf/2503.16067", "abs": "https://arxiv.org/abs/2503.16067", "authors": ["Tim Seizinger", "Florin-Alexandru Vasluianu", "Marcos V. Conde", "Radu Timofte"], "title": "Bokehlicious: Photorealistic Bokeh Rendering with Controllable Apertures", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "Bokeh rendering methods play a key role in creating the visually appealing,\nsoftly blurred backgrounds seen in professional photography. While recent\nlearning-based approaches show promising results, generating realistic Bokeh\nwith variable strength remains challenging. Existing methods require additional\ninputs and suffer from unrealistic Bokeh reproduction due to reliance on\nsynthetic data. In this work, we propose Bokehlicious, a highly efficient\nnetwork that provides intuitive control over Bokeh strength through an\nAperture-Aware Attention mechanism, mimicking the physical lens aperture. To\nfurther address the lack of high-quality real-world data, we present RealBokeh,\na novel dataset featuring 23,000 high-resolution (24-MP) images captured by\nprofessional photographers, covering diverse scenes with varied aperture and\nfocal length settings. Evaluations on both our new RealBokeh and established\nBokeh rendering benchmarks show that Bokehlicious consistently outperforms SOTA\nmethods while significantly reducing computational cost and exhibiting strong\nzero-shot generalization. Our method and dataset further extend to defocus\ndeblurring, achieving competitive results on the RealDOF benchmark. Our code\nand data can be found at https://github.com/TimSeizinger/Bokehlicious"}
{"id": "2503.16068", "pdf": "https://arxiv.org/pdf/2503.16068", "abs": "https://arxiv.org/abs/2503.16068", "authors": ["Longbin Ji", "Lei Zhong", "Pengfei Wei", "Changjian Li"], "title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion", "categories": ["cs.CV"], "comment": "Code, data and project page: https://robingg1.github.io/Pose-Traj/", "summary": "Recent advancements in trajectory-guided video generation have achieved\nnotable progress. However, existing models still face challenges in generating\nobject motions with potentially changing 6D poses under wide-range rotations,\ndue to limited 3D understanding. To address this problem, we introduce\nPoseTraj, a pose-aware video dragging model for generating 3D-aligned motion\nfrom 2D trajectories. Our method adopts a novel two-stage pose-aware\npretraining framework, improving 3D understanding across diverse trajectories.\nSpecifically, we propose a large-scale synthetic dataset PoseTraj-10K,\ncontaining 10k videos of objects following rotational trajectories, and enhance\nthe model perception of object pose changes by incorporating 3D bounding boxes\nas intermediate supervision signals. Following this, we fine-tune the\ntrajectory-controlling module on real-world videos, applying an additional\ncamera-disentanglement module to further refine motion accuracy. Experiments on\nvarious benchmark datasets demonstrate that our method not only excels in 3D\npose-aligned dragging for rotational trajectories but also outperforms existing\nbaselines in trajectory accuracy and video quality."}
{"id": "2503.16069", "pdf": "https://arxiv.org/pdf/2503.16069", "abs": "https://arxiv.org/abs/2503.16069", "authors": ["Aniek Eijpe", "Soufyan Lakbir", "Melis Erdal Cesur", "Sara P. Oliveira", "Sanne Abeln", "Wilson Silva"], "title": "Disentangled and Interpretable Multimodal Attention Fusion for Cancer Survival Prediction", "categories": ["cs.CV"], "comment": "11 pages, 1 figure, 3 tables", "summary": "To improve the prediction of cancer survival using whole-slide images and\ntranscriptomics data, it is crucial to capture both modality-shared and\nmodality-specific information. However, multimodal frameworks often entangle\nthese representations, limiting interpretability and potentially suppressing\ndiscriminative features. To address this, we propose Disentangled and\nInterpretable Multimodal Attention Fusion (DIMAF), a multimodal framework that\nseparates the intra- and inter-modal interactions within an attention-based\nfusion mechanism to learn distinct modality-specific and modality-shared\nrepresentations. We introduce a loss based on Distance Correlation to promote\ndisentanglement between these representations and integrate Shapley additive\nexplanations to assess their relative contributions to survival prediction. We\nevaluate DIMAF on four public cancer survival datasets, achieving a relative\naverage improvement of 1.85% in performance and 23.7% in disentanglement\ncompared to current state-of-the-art multimodal models. Beyond improved\nperformance, our interpretable framework enables a deeper exploration of the\nunderlying interactions between and within modalities in cancer biology."}
{"id": "2503.16086", "pdf": "https://arxiv.org/pdf/2503.16086", "abs": "https://arxiv.org/abs/2503.16086", "authors": ["Gabriela Ghimpeteanu", "Hayat Rajani", "Josep Quintana", "Rafael Garcia"], "title": "Hyperspectral Imaging for Identifying Foreign Objects on Pork Belly", "categories": ["cs.CV", "cs.LG", "I.2.6; I.2.10; J.7"], "comment": "Article under review by Computers in Industry, Elsevier", "summary": "Ensuring food safety and quality is critical in the food processing industry,\nwhere the detection of contaminants remains a persistent challenge. This study\npresents an automated solution for detecting foreign objects on pork belly meat\nusing hyperspectral imaging (HSI). A hyperspectral camera was used to capture\ndata across various bands in the near-infrared (NIR) spectrum (900-1700 nm),\nenabling accurate identification of contaminants that are often undetectable\nthrough traditional visual inspection methods. The proposed solution combines\npre-processing techniques with a segmentation approach based on a lightweight\nVision Transformer (ViT) to distinguish contaminants from meat, fat, and\nconveyor belt materials. The adopted strategy demonstrates high detection\naccuracy and training efficiency, while also addressing key industrial\nchallenges such as inherent noise, temperature variations, and spectral\nsimilarity between contaminants and pork belly. Experimental results validate\nthe effectiveness of hyperspectral imaging in enhancing food safety,\nhighlighting its potential for broad real-time applications in automated\nquality control processes."}
{"id": "2503.16096", "pdf": "https://arxiv.org/pdf/2503.16096", "abs": "https://arxiv.org/abs/2503.16096", "authors": ["Lucas Morin", "Valéry Weber", "Ahmed Nassar", "Gerhard Ingmar Meijer", "Luc Van Gool", "Yawei Li", "Peter Staar"], "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures", "categories": ["cs.CV"], "comment": null, "summary": "The automated analysis of chemical literature holds promise to accelerate\ndiscovery in fields such as material science and drug development. In\nparticular, search capabilities for chemical structures and Markush structures\n(chemical structure templates) within patent documents are valuable, e.g., for\nprior-art search. Advancements have been made in the automatic extraction of\nchemical structures from text and images, yet the Markush structures remain\nlargely unexplored due to their complex multi-modal nature. In this work, we\npresent MarkushGrapher, a multi-modal approach for recognizing Markush\nstructures in documents. Our method jointly encodes text, image, and layout\ninformation through a Vision-Text-Layout encoder and an Optical Chemical\nStructure Recognition vision encoder. These representations are merged and used\nto auto-regressively generate a sequential graph representation of the Markush\nstructure along with a table defining its variable groups. To overcome the lack\nof real-world training data, we propose a synthetic data generation pipeline\nthat produces a wide range of realistic Markush structures. Additionally, we\npresent M2S, the first annotated benchmark of real-world Markush structures, to\nadvance research on this challenging task. Extensive experiments demonstrate\nthat our approach outperforms state-of-the-art chemistry-specific and\ngeneral-purpose vision-language models in most evaluation settings. Code,\nmodels, and datasets will be available."}
{"id": "2503.16106", "pdf": "https://arxiv.org/pdf/2503.16106", "abs": "https://arxiv.org/abs/2503.16106", "authors": ["Mohamad Hassan N C", "Divyam Gupta", "Mainak Singha", "Sai Bhargav Rongali", "Ankit Jha", "Muhammad Haris Khan", "Biplab Banerjee"], "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "We introduce Low-Shot Open-Set Domain Generalization (LSOSDG), a novel\nparadigm unifying low-shot learning with open-set domain generalization (ODG).\nWhile prompt-based methods using models like CLIP have advanced DG, they falter\nin low-data regimes (e.g., 1-shot) and lack precision in detecting open-set\nsamples with fine-grained semantics related to training classes. To address\nthese challenges, we propose OSLOPROMPT, an advanced prompt-learning framework\nfor CLIP with two core innovations. First, to manage limited supervision across\nsource domains and improve DG, we introduce a domain-agnostic prompt-learning\nmechanism that integrates adaptable domain-specific cues and visually guided\nsemantic attributes through a novel cross-attention module, besides being\nsupported by learnable domain- and class-generic visual prompts to enhance\ncross-modal adaptability. Second, to improve outlier rejection during\ninference, we classify unfamiliar samples as \"unknown\" and train specialized\nprompts with systematically synthesized pseudo-open samples that maintain\nfine-grained relationships to known classes, generated through a targeted query\nstrategy with off-the-shelf foundation models. This strategy enhances feature\nlearning, enabling our model to detect open samples with varied granularity\nmore effectively. Extensive evaluations across five benchmarks demonstrate that\nOSLOPROMPT establishes a new state-of-the-art in LSOSDG, significantly\noutperforming existing methods."}
{"id": "2503.16120", "pdf": "https://arxiv.org/pdf/2503.16120", "abs": "https://arxiv.org/abs/2503.16120", "authors": ["Jiyong Rao", "Brian Nlong Zhao", "Yu Wang"], "title": "Probabilistic Prompt Distribution Learning for Animal Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Multi-species animal pose estimation has emerged as a challenging yet\ncritical task, hindered by substantial visual diversity and uncertainty. This\npaper challenges the problem by efficient prompt learning for Vision-Language\nPretrained (VLP) models, \\textit{e.g.} CLIP, aiming to resolve the\ncross-species generalization problem. At the core of the solution lies in the\nprompt designing, probabilistic prompt modeling and cross-modal adaptation,\nthereby enabling prompts to compensate for cross-modal information and\neffectively overcome large data variances under unbalanced data distribution.\nTo this end, we propose a novel probabilistic prompting approach to fully\nexplore textual descriptions, which could alleviate the diversity issues caused\nby long-tail property and increase the adaptability of prompts on unseen\ncategory instance. Specifically, we first introduce a set of learnable prompts\nand propose a diversity loss to maintain distinctiveness among prompts, thus\nrepresenting diverse image attributes. Diverse textual probabilistic\nrepresentations are sampled and used as the guidance for the pose estimation.\nSubsequently, we explore three different cross-modal fusion strategies at\nspatial level to alleviate the adverse impacts of visual uncertainty. Extensive\nexperiments on multi-species animal pose benchmarks show that our method\nachieves the state-of-the-art performance under both supervised and zero-shot\nsettings. The code is available at https://github.com/Raojiyong/PPAP."}
{"id": "2503.16125", "pdf": "https://arxiv.org/pdf/2503.16125", "abs": "https://arxiv.org/abs/2503.16125", "authors": ["Jiangyi Wang", "Na Zhao"], "title": "Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Active learning has emerged as a promising approach to reduce the substantial\nannotation burden in 3D object detection tasks, spurring several initiatives in\noutdoor environments. However, its application in indoor environments remains\nunexplored. Compared to outdoor 3D datasets, indoor datasets face significant\nchallenges, including fewer training samples per class, a greater number of\nclasses, more severe class imbalance, and more diverse scene types and\nintra-class variances. This paper presents the first study on active learning\nfor indoor 3D object detection, where we propose a novel framework tailored for\nthis task. Our method incorporates two key criteria - uncertainty and diversity\n- to actively select the most ambiguous and informative unlabeled samples for\nannotation. The uncertainty criterion accounts for both inaccurate detections\nand undetected objects, ensuring that the most ambiguous samples are\nprioritized. Meanwhile, the diversity criterion is formulated as a joint\noptimization problem that maximizes the diversity of both object class\ndistributions and scene types, using a new Class-aware Adaptive Prototype (CAP)\nbank. The CAP bank dynamically allocates representative prototypes to each\nclass, helping to capture varying intra-class diversity across different\ncategories. We evaluate our method on SUN RGB-D and ScanNetV2, where it\noutperforms baselines by a significant margin, achieving over 85% of\nfully-supervised performance with just 10% of the annotation budget."}
{"id": "2503.16128", "pdf": "https://arxiv.org/pdf/2503.16128", "abs": "https://arxiv.org/abs/2503.16128", "authors": ["Benedykt Pawlus", "Bogdan Smolka", "Jolanta Kawulok", "Michal Kawulok"], "title": "Coupling deep and handcrafted features to assess smile genuineness", "categories": ["cs.CV"], "comment": "Submitted to SPIE Defense + Commercial Sensing 2024", "summary": "Assessing smile genuineness from video sequences is a vital topic concerned\nwith recognizing facial expression and linking them with the underlying\nemotional states. There have been a number of techniques proposed underpinned\nwith handcrafted features, as well as those that rely on deep learning to\nelaborate the useful features. As both of these approaches have certain\nbenefits and limitations, in this work we propose to combine the features\nlearned by a long short-term memory network with the features handcrafted to\ncapture the dynamics of facial action units. The results of our experiments\nindicate that the proposed solution is more effective than the baseline\ntechniques and it allows for assessing the smile genuineness from video\nsequences in real-time."}
{"id": "2503.16134", "pdf": "https://arxiv.org/pdf/2503.16134", "abs": "https://arxiv.org/abs/2503.16134", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Tong Shao", "Ke Tang", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Quad Bayer demosaicing is the central challenge for enabling the widespread\napplication of Hybrid Event-based Vision Sensors (HybridEVS). Although existing\nlearning-based methods that leverage long-range dependency modeling have\nachieved promising results, their complexity severely limits deployment on\nmobile devices for real-world applications. To address these limitations, we\npropose a lightweight Mamba-based binary neural network designed for efficient\nand high-performing demosaicing of HybridEVS RAW images. First, to effectively\ncapture both global and local dependencies, we introduce a hybrid Binarized\nMamba-Transformer architecture that combines the strengths of the Mamba and\nSwin Transformer architectures. Next, to significantly reduce computational\ncomplexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all\nprojections while retaining the core Selective Scan in full precision. Bi-Mamba\nalso incorporates additional global visual information to enhance global\ncontext and mitigate precision loss. We conduct quantitative and qualitative\nexperiments to demonstrate the effectiveness of BMTNet in both performance and\ncomputational efficiency, providing a lightweight demosaicing solution suited\nfor real-world edge devices. Our codes and models are available at\nhttps://github.com/Clausy9/BMTNet."}
{"id": "2503.16153", "pdf": "https://arxiv.org/pdf/2503.16153", "abs": "https://arxiv.org/abs/2503.16153", "authors": ["Tianyi Wei", "Yifan Zhou", "Dongdong Chen", "Xingang Pan"], "title": "FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing", "categories": ["cs.CV"], "comment": "Project page: https://wtybest.github.io/projects/FreeFlux/", "summary": "The integration of Rotary Position Embedding (RoPE) in Multimodal Diffusion\nTransformer (MMDiT) has significantly enhanced text-to-image generation\nquality. However, the fundamental reliance of self-attention layers on\npositional embedding versus query-key similarity during generation remains an\nintriguing question. We present the first mechanistic analysis of RoPE-based\nMMDiT models (e.g., FLUX), introducing an automated probing strategy that\ndisentangles positional information versus content dependencies by\nstrategically manipulating RoPE during generation. Our analysis reveals\ndistinct dependency patterns that do not straightforwardly correlate with\ndepth, offering new insights into the layer-specific roles in RoPE-based MMDiT.\nBased on these findings, we propose a training-free, task-specific image\nediting framework that categorizes editing tasks into three types:\nposition-dependent editing (e.g., object addition), content\nsimilarity-dependent editing (e.g., non-rigid editing), and region-preserved\nediting (e.g., background replacement). For each type, we design tailored\nkey-value injection strategies based on the characteristics of the editing\ntask. Extensive qualitative and quantitative evaluations demonstrate that our\nmethod outperforms state-of-the-art approaches, particularly in preserving\noriginal semantic content and achieving seamless modifications."}
{"id": "2503.16165", "pdf": "https://arxiv.org/pdf/2503.16165", "abs": "https://arxiv.org/abs/2503.16165", "authors": ["Xiangyu Li", "Wanshu Fan", "Yue Shen", "Cong Wang", "Wei Wang", "Xin Yang", "Qiang Zhang", "Dongsheng Zhou"], "title": "Iterative Optimal Attention and Local Model for Single Image Rain Streak Removal", "categories": ["cs.CV", "cs.IR"], "comment": "14 pages, 14 figures, 6 tables", "summary": "High-fidelity imaging is crucial for the successful safety supervision and\nintelligent deployment of vision-based measurement systems (VBMS). It ensures\nhigh-quality imaging in VBMS, which is fundamental for reliable visual\nmeasurement and analysis. However, imaging quality can be significantly\nimpaired by adverse weather conditions, particularly rain, leading to blurred\nimages and reduced contrast. Such impairments increase the risk of inaccurate\nevaluations and misinterpretations in VBMS. To address these limitations, we\npropose an Expectation Maximization Reconstruction Transformer (EMResformer)\nfor single image rain streak removal. The EMResformer retains the key\nself-attention values for feature aggregation, enhancing local features to\nproduce superior image reconstruction. Specifically, we propose an Expectation\nMaximization Block seamlessly integrated into the single image rain streak\nremoval network, enhancing its ability to eliminate superfluous information and\nrestore a cleaner background image. Additionally, to further enhance local\ninformation for improved detail rendition, we introduce a Local Model Residual\nBlock, which integrates two local model blocks along with a sequence of\nconvolutions and activation functions. This integration synergistically\nfacilitates the extraction of more pertinent features for enhanced single image\nrain streak removal. Extensive experiments validate that our proposed\nEMResformer surpasses current state-of-the-art single image rain streak removal\nmethods on both synthetic and real-world datasets, achieving an improved\nbalance between model complexity and single image deraining performance.\nFurthermore, we evaluate the effectiveness of our method in VBMS scenarios,\ndemonstrating that high-quality imaging significantly improves the accuracy and\nreliability of VBMS tasks."}
{"id": "2503.16171", "pdf": "https://arxiv.org/pdf/2503.16171", "abs": "https://arxiv.org/abs/2503.16171", "authors": ["Soham Roy", "Abhishek Mishra", "Shirish Karande", "Murari Mandal"], "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Modern text-to-image generative models can inadvertently reproduce\ncopyrighted content memorized in their training data, raising serious concerns\nabout potential copyright infringement. We introduce Guardians of Generation, a\nmodel agnostic inference time framework for dynamic copyright shielding in AI\nimage generation. Our approach requires no retraining or modification of the\ngenerative model weights, instead integrating seamlessly with existing\ndiffusion pipelines. It augments the generation process with an adaptive\nguidance mechanism comprising three components: a detection module, a prompt\nrewriting module, and a guidance adjustment module. The detection module\nmonitors user prompts and intermediate generation steps to identify features\nindicative of copyrighted content before they manifest in the final output. If\nsuch content is detected, the prompt rewriting mechanism dynamically transforms\nthe user's prompt by sanitizing or replacing references that could trigger\ncopyrighted material while preserving the prompt's intended semantics. The\nadaptive guidance module adaptively steers the diffusion process away from\nflagged content by modulating the model's sampling trajectory. Together, these\ncomponents form a robust shield that enables a tunable balance between\npreserving creative fidelity and ensuring copyright compliance. We validate our\nmethod on a variety of generative models such as Stable Diffusion, SDXL, and\nFlux, demonstrating substantial reductions in copyrighted content generation\nwith negligible impact on output fidelity or alignment with user intent. This\nwork provides a practical, plug-and-play safeguard for generative image models,\nenabling more responsible deployment under real-world copyright constraints.\nSource code is available at: https://respailab.github.io/gog"}
{"id": "2503.16179", "pdf": "https://arxiv.org/pdf/2503.16179", "abs": "https://arxiv.org/abs/2503.16179", "authors": ["Fatemeh Amerehi", "Patrick Healy"], "title": "Narrowing Class-Wise Robustness Gaps in Adversarial Training", "categories": ["cs.CV", "cs.LG"], "comment": "4 figures, ICLR 2025 Workshop on Foundation Models in the Wild", "summary": "Efforts to address declining accuracy as a result of data shifts often\ninvolve various data-augmentation strategies. Adversarial training is one such\nmethod, designed to improve robustness to worst-case distribution shifts caused\nby adversarial examples. While this method can improve robustness, it may also\nhinder generalization to clean examples and exacerbate performance imbalances\nacross different classes. This paper explores the impact of adversarial\ntraining on both overall and class-specific performance, as well as its\nspill-over effects. We observe that enhanced labeling during training boosts\nadversarial robustness by 53.50% and mitigates class imbalances by 5.73%,\nleading to improved accuracy in both clean and adversarial settings compared to\nstandard adversarial training."}
{"id": "2503.16184", "pdf": "https://arxiv.org/pdf/2503.16184", "abs": "https://arxiv.org/abs/2503.16184", "authors": ["Andrea Maracani", "Savas Ozkan", "Sijun Cho", "Hyowon Kim", "Eunchung Noh", "Jeongwon Min", "Cho Jung Min", "Dookun Park", "Mete Ozay"], "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling architectures have been proven effective for improving Scene Text\nRecognition (STR), but the individual contribution of vision encoder and text\ndecoder scaling remain under-explored. In this work, we present an in-depth\nempirical analysis and demonstrate that, contrary to previous observations,\nscaling the decoder yields significant performance gains, always exceeding\nthose achieved by encoder scaling alone. We also identify label noise as a key\nchallenge in STR, particularly in real-world data, which can limit the\neffectiveness of STR models. To address this, we propose Cloze\nSelf-Distillation (CSD), a method that mitigates label noise by distilling a\nstudent model from context-aware soft predictions and pseudolabels generated by\na teacher model. Additionally, we enhance the decoder architecture by\nintroducing differential cross-attention for STR. Our methodology achieves\nstate-of-the-art performance on 10 out of 11 benchmarks using only real data,\nwhile significantly reducing the parameter size and computational costs."}
{"id": "2503.16185", "pdf": "https://arxiv.org/pdf/2503.16185", "abs": "https://arxiv.org/abs/2503.16185", "authors": ["Peihao Wu", "Yongxiang Yao", "Wenfei Zhang", "Dong Wei", "Yi Wan", "Yansheng Li", "Yongjun Zhang"], "title": "MapGlue: Multimodal Remote Sensing Image Matching", "categories": ["cs.CV"], "comment": "The dataset and code are available at\n  https://github.com/PeihaoWu/MapGlue", "summary": "Multimodal remote sensing image (MRSI) matching is pivotal for cross-modal\nfusion, localization, and object detection, but it faces severe challenges due\nto geometric, radiometric, and viewpoint discrepancies across imaging\nmodalities. Existing unimodal datasets lack scale and diversity, limiting deep\nlearning solutions. This paper proposes MapGlue, a universal MRSI matching\nframework, and MapData, a large-scale multimodal dataset addressing these gaps.\nOur contributions are twofold. MapData, a globally diverse dataset spanning 233\nsampling points, offers original images (7,000x5,000 to 20,000x15,000 pixels).\nAfter rigorous cleaning, it provides 121,781 aligned electronic map-visible\nimage pairs (512x512 pixels) with hybrid manual-automated ground truth,\naddressing the scarcity of scalable multimodal benchmarks. MapGlue integrates\nsemantic context with a dual graph-guided mechanism to extract cross-modal\ninvariant features. This structure enables global-to-local interaction,\nenhancing descriptor robustness against modality-specific distortions.\nExtensive evaluations on MapData and five public datasets demonstrate MapGlue's\nsuperiority in matching accuracy under complex conditions, outperforming\nstate-of-the-art methods. Notably, MapGlue generalizes effectively to unseen\nmodalities without retraining, highlighting its adaptability. This work\naddresses longstanding challenges in MRSI matching by combining scalable\ndataset construction with a robust, semantics-driven framework. Furthermore,\nMapGlue shows strong generalization capabilities on other modality matching\ntasks for which it was not specifically trained. The dataset and code are\navailable at https://github.com/PeihaoWu/MapGlue."}
{"id": "2503.16188", "pdf": "https://arxiv.org/pdf/2503.16188", "abs": "https://arxiv.org/abs/2503.16188", "authors": ["Ming Li", "Shitian Zhao", "Jike Zhong", "Yuxiang Lai", "Kaipeng Zhang"], "title": "CLS-RL: Image Classification with Rule-Based Reinforcement Learning", "categories": ["cs.CV"], "comment": "Preprint, work in progress", "summary": "Classification is a core task in machine learning. Recent research has shown\nthat although Multimodal Large Language Models (MLLMs) are initially poor at\nimage classification, fine-tuning them with an adequate amount of data can\nsignificantly enhance their performance, making them comparable to SOTA\nclassification models. However, acquiring large-scale labeled data is\nexpensive. In this paper, we explore few-shot MLLM classification fine-tuning.\nWe found that SFT can cause severe overfitting issues and may even degrade\nperformance over the zero-shot approach. To address this challenge, inspired by\nthe recent successes in rule-based reinforcement learning, we propose CLS-RL,\nwhich uses verifiable signals as reward to fine-tune MLLMs. We discovered that\nCLS-RL outperforms SFT in most datasets and has a much higher average accuracy\non both base-to-new and few-shot learning setting. Moreover, we observed a\nfree-lunch phenomenon for CLS-RL; when models are fine-tuned on a particular\ndataset, their performance on other distinct datasets may also improve over\nzero-shot models, even if those datasets differ in distribution and class\nnames. This suggests that RL-based methods effectively teach models the\nfundamentals of classification. Lastly, inspired by recent works in inference\ntime thinking, we re-examine the `thinking process' during fine-tuning, a\ncritical aspect of RL-based methods, in the context of visual classification.\nWe question whether such tasks require extensive thinking process during\nfine-tuning, proposing that this may actually detract from performance. Based\non this premise, we introduce the No-Thinking-CLS-RL method, which minimizes\nthinking processes during training by setting an equality accuracy reward. Our\nfindings indicate that, with much less fine-tuning time, No-Thinking-CLS-RL\nmethod achieves superior in-domain performance and generalization capabilities\nthan CLS-RL."}
{"id": "2503.16194", "pdf": "https://arxiv.org/pdf/2503.16194", "abs": "https://arxiv.org/abs/2503.16194", "authors": ["Ziyao Guo", "Kaipeng Zhang", "Michael Qizhe Shieh"], "title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "Autoregressive models have shown remarkable success in image generation by\nadapting sequential prediction techniques from language modeling. However,\napplying these approaches to images requires discretizing continuous pixel data\nthrough vector quantization methods like VQ-VAE. To alleviate the quantization\nerrors that existed in VQ-VAE, recent works tend to use larger codebooks.\nHowever, this will accordingly expand vocabulary size, complicating the\nautoregressive modeling task. This paper aims to find a way to enjoy the\nbenefits of large codebooks without making autoregressive modeling more\ndifficult. Through empirical investigation, we discover that tokens with\nsimilar codeword representations produce similar effects on the final generated\nimage, revealing significant redundancy in large codebooks. Based on this\ninsight, we propose to predict tokens from coarse to fine (CTF), realized by\nassigning the same coarse label for similar tokens. Our framework consists of\ntwo stages: (1) an autoregressive model that sequentially predicts coarse\nlabels for each token in the sequence, and (2) an auxiliary model that\nsimultaneously predicts fine-grained labels for all tokens conditioned on their\ncoarse labels. Experiments on ImageNet demonstrate our method's superior\nperformance, achieving an average improvement of 59 points in Inception Score\ncompared to baselines. Notably, despite adding an inference step, our approach\nachieves faster sampling speeds."}
{"id": "2503.16195", "pdf": "https://arxiv.org/pdf/2503.16195", "abs": "https://arxiv.org/abs/2503.16195", "authors": ["Chia-Yi Hsu", "Jia-You Chen", "Yu-Lin Tsai", "Chih-Hsun Lin", "Pin-Yu Chen", "Chia-Mu Yu", "Chun-Ying Huang"], "title": "VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICASSP 2025", "summary": "Differentially private (DP) synthetic data has become the de facto standard\nfor releasing sensitive data. However, many DP generative models suffer from\nthe low utility of synthetic data, especially for high-resolution images. On\nthe other hand, one of the emerging techniques in parameter efficient\nfine-tuning (PEFT) is visual prompting (VP), which allows well-trained existing\nmodels to be reused for the purpose of adapting to subsequent downstream tasks.\nIn this work, we explore such a phenomenon in constructing captivating\ngenerative models with DP constraints. We show that VP in conjunction with\nDP-NTK, a DP generator that exploits the power of the neural tangent kernel\n(NTK) in training DP generative models, achieves a significant performance\nboost, particularly for high-resolution image datasets, with accuracy improving\nfrom 0.644$\\pm$0.044 to 0.769. Lastly, we perform ablation studies on the\neffect of different parameters that influence the overall performance of\nVP-NTK. Our work demonstrates a promising step forward in improving the utility\nof DP synthetic data, particularly for high-resolution images."}
{"id": "2503.16218", "pdf": "https://arxiv.org/pdf/2503.16218", "abs": "https://arxiv.org/abs/2503.16218", "authors": ["Yu Cao", "Zengqun Zhao", "Ioannis Patras", "Shaogang Gong"], "title": "Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts", "categories": ["cs.CV"], "comment": null, "summary": "Visual artifacts remain a persistent challenge in diffusion models, even with\ntraining on massive datasets. Current solutions primarily rely on supervised\ndetectors, yet lack understanding of why these artifacts occur in the first\nplace. In our analysis, we identify three distinct phases in the diffusion\ngenerative process: Profiling, Mutation, and Refinement. Artifacts typically\nemerge during the Mutation phase, where certain regions exhibit anomalous score\ndynamics over time, causing abrupt disruptions in the normal evolution pattern.\nThis temporal nature explains why existing methods focusing only on spatial\nuncertainty of the final output fail at effective artifact localization. Based\non these insights, we propose ASCED (Abnormal Score Correction for Enhancing\nDiffusion), that detects artifacts by monitoring abnormal score dynamics during\nthe diffusion process, with a trajectory-aware on-the-fly mitigation strategy\nthat appropriate generation of noise in the detected areas. Unlike most\nexisting methods that apply post hoc corrections, \\eg, by applying a\nnoising-denoising scheme after generation, our mitigation strategy operates\nseamlessly within the existing diffusion process. Extensive experiments\ndemonstrate that our proposed approach effectively reduces artifacts across\ndiverse domains, matching or surpassing existing supervised methods without\nadditional training."}
{"id": "2503.16247", "pdf": "https://arxiv.org/pdf/2503.16247", "abs": "https://arxiv.org/abs/2503.16247", "authors": ["Max Gutbrod", "David Rauber", "Danilo Weber Nunes", "Christoph Palm"], "title": "OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The growing reliance on Artificial Intelligence (AI) in critical domains such\nas healthcare demands robust mechanisms to ensure the trustworthiness of these\nsystems, especially when faced with unexpected or anomalous inputs. This paper\nintroduces the Open Medical Imaging Benchmarks for Out-Of-Distribution\nDetection (OpenMIBOOD), a comprehensive framework for evaluating\nout-of-distribution (OOD) detection methods specifically in medical imaging\ncontexts. OpenMIBOOD includes three benchmarks from diverse medical domains,\nencompassing 14 datasets divided into covariate-shifted in-distribution,\nnear-OOD, and far-OOD categories. We evaluate 24 post-hoc methods across these\nbenchmarks, providing a standardized reference to advance the development and\nfair comparison of OOD detection methods. Results reveal that findings from\nbroad-scale OOD benchmarks in natural image domains do not translate to medical\napplications, underscoring the critical need for such benchmarks in the medical\nfield. By mitigating the risk of exposing AI models to inputs outside their\ntraining distribution, OpenMIBOOD aims to support the advancement of reliable\nand trustworthy AI systems in healthcare. The repository is available at\nhttps://github.com/remic-othr/OpenMIBOOD."}
{"id": "2503.16254", "pdf": "https://arxiv.org/pdf/2503.16254", "abs": "https://arxiv.org/abs/2503.16254", "authors": ["Markus Karmann", "Peng-Tao Jiang", "Bo Li", "Onay Urfalioglu"], "title": "M2N2V2: Multi-Modal Unsupervised and Training-free Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We present Markov Map Nearest Neighbor V2 (M2N2V2), a novel and simple, yet\neffective approach which leverages depth guidance and attention maps for\nunsupervised and training-free point-prompt-based interactive segmentation.\nFollowing recent trends in supervised multimodal approaches, we carefully\nintegrate depth as an additional modality to create novel depth-guided\nMarkov-maps. Furthermore, we observe occasional segment size fluctuations in\nM2N2 during the interactive process, which can decrease the overall mIoU's. To\nmitigate this problem, we model the prompting as a sequential process and\npropose a novel adaptive score function which considers the previous\nsegmentation and the current prompt point in order to prevent unreasonable\nsegment size changes. Using Stable Diffusion 2 and Depth Anything V2 as\nbackbones, we empirically show that our proposed M2N2V2 significantly improves\nthe Number of Clicks (NoC) and mIoU compared to M2N2 in all datasets except\nthose from the medical domain. Interestingly, our unsupervised approach\nachieves competitive results compared to supervised methods like SAM and\nSimpleClick in the more challenging DAVIS and HQSeg44K datasets in the NoC\nmetric, reducing the gap between supervised and unsupervised methods."}
{"id": "2503.16257", "pdf": "https://arxiv.org/pdf/2503.16257", "abs": "https://arxiv.org/abs/2503.16257", "authors": ["Keda Tao", "Haoxuan You", "Yang Sui", "Can Qin", "Huan Wang"], "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."}
{"id": "2503.16260", "pdf": "https://arxiv.org/pdf/2503.16260", "abs": "https://arxiv.org/abs/2503.16260", "authors": ["Zijian Li", "Jingjing Fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "title": "Chain of Functions: A Programmatic Pipeline for Fine-Grained Chart Reasoning Data", "categories": ["cs.CV"], "comment": "Under review", "summary": "Visual reasoning is crucial for multimodal large language models (MLLMs) to\naddress complex chart queries, yet high-quality rationale data remains scarce.\nExisting methods leveraged (M)LLMs for data generation, but direct prompting\noften yields limited precision and diversity. In this paper, we propose\n\\textit{Chain of Functions (CoF)}, a novel programmatic reasoning data\ngeneration pipeline that utilizes freely-explored reasoning paths as\nsupervision to ensure data precision and diversity. Specifically, it starts\nwith human-free exploration among the atomic functions (e.g., maximum data and\narithmetic operations) to generate diverse function chains, which are then\ntranslated into linguistic rationales and questions with only a moderate\nopen-sourced LLM. \\textit{CoF} provides multiple benefits: 1) Precision:\nfunction-governed generation reduces hallucinations compared to freeform\ngeneration; 2) Diversity: enumerating function chains enables varied question\ntaxonomies; 3) Explainability: function chains serve as built-in rationales,\nallowing fine-grained evaluation beyond overall accuracy; 4) Practicality:\neliminating reliance on extremely large models. Employing \\textit{CoF}, we\nconstruct the \\textit{ChartCoF} dataset, with 1.4k complex reasoning Q\\&A for\nfine-grained analysis and 50k Q\\&A for reasoning enhancement. The fine-grained\nevaluation on \\textit{ChartCoF} reveals varying performance across question\ntaxonomies for each MLLM, and the experiments also show that finetuning with\n\\textit{ChartCoF} achieves state-of-the-art performance among same-scale MLLMs\non widely used benchmarks. Furthermore, the novel paradigm of function-governed\nrationale generation in \\textit{CoF} could inspire broader applications beyond\ncharts."}
{"id": "2503.16263", "pdf": "https://arxiv.org/pdf/2503.16263", "abs": "https://arxiv.org/abs/2503.16263", "authors": ["Ayberk Acar", "Mariana Smith", "Lidia Al-Zogbi", "Tanner Watts", "Fangjie Li", "Hao Li", "Nural Yilmaz", "Paul Maria Scheikl", "Jesse F. d'Almeida", "Susheela Sharma", "Lauren Branscombe", "Tayfun Efe Ertop", "Robert J. Webster III", "Ipek Oguz", "Alan Kuntz", "Axel Krieger", "Jie Ying Wu"], "title": "From Monocular Vision to Autonomous Action: Guiding Tumor Resection via 3D Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": "7 Pages, 8 Figures, 1 Table. This work has been submitted IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS) for\n  possible publication", "summary": "Surgical automation requires precise guidance and understanding of the scene.\nCurrent methods in the literature rely on bulky depth cameras to create maps of\nthe anatomy, however this does not translate well to space-limited clinical\napplications. Monocular cameras are small and allow minimally invasive\nsurgeries in tight spaces but additional processing is required to generate 3D\nscene understanding. We propose a 3D mapping pipeline that uses only RGB images\nto create segmented point clouds of the target anatomy. To ensure the most\nprecise reconstruction, we compare different structure from motion algorithms'\nperformance on mapping the central airway obstructions, and test the pipeline\non a downstream task of tumor resection. In several metrics, including\npost-procedure tissue model evaluation, our pipeline performs comparably to\nRGB-D cameras and, in some cases, even surpasses their performance. These\npromising results demonstrate that automation guidance can be achieved in\nminimally invasive procedures with monocular cameras. This study is a step\ntoward the complete autonomy of surgical robots."}
{"id": "2503.16282", "pdf": "https://arxiv.org/pdf/2503.16282", "abs": "https://arxiv.org/abs/2503.16282", "authors": ["Zhaochong An", "Guolei Sun", "Yun Liu", "Runjia Li", "Junlin Han", "Ender Konukoglu", "Serge Belongie"], "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Generalized few-shot 3D point cloud segmentation (GFS-PCS) adapts models to\nnew classes with few support samples while retaining base class segmentation.\nExisting GFS-PCS methods enhance prototypes via interacting with support or\nquery features but remain limited by sparse knowledge from few-shot samples.\nMeanwhile, 3D vision-language models (3D VLMs), generalizing across open-world\nnovel classes, contain rich but noisy novel class knowledge. In this work, we\nintroduce a GFS-PCS framework that synergizes dense but noisy pseudo-labels\nfrom 3D VLMs with precise yet sparse few-shot samples to maximize the strengths\nof both, named GFS-VL. Specifically, we present a prototype-guided pseudo-label\nselection to filter low-quality regions, followed by an adaptive infilling\nstrategy that combines knowledge from pseudo-label contexts and few-shot\nsamples to adaptively label the filtered, unlabeled areas. Additionally, we\ndesign a novel-base mix strategy to embed few-shot samples into training\nscenes, preserving essential context for improved novel class learning.\nMoreover, recognizing the limited diversity in current GFS-PCS benchmarks, we\nintroduce two challenging benchmarks with diverse novel classes for\ncomprehensive generalization evaluation. Experiments validate the effectiveness\nof our framework across models and datasets. Our approach and benchmarks\nprovide a solid foundation for advancing GFS-PCS in the real world. The code is\nat https://github.com/ZhaochongAn/GFS-VL"}
{"id": "2503.16284", "pdf": "https://arxiv.org/pdf/2503.16284", "abs": "https://arxiv.org/abs/2503.16284", "authors": ["Sharon Peled", "Yosef E. Maruvka", "Moti Freiman"], "title": "PSA-MIL: A Probabilistic Spatial Attention-Based Multiple Instance Learning for Whole Slide Image Classification", "categories": ["cs.CV"], "comment": "8 pages, 7 figures", "summary": "Whole Slide Images (WSIs) are high-resolution digital scans widely used in\nmedical diagnostics. WSI classification is typically approached using Multiple\nInstance Learning (MIL), where the slide is partitioned into tiles treated as\ninterconnected instances. While attention-based MIL methods aim to identify the\nmost informative tiles, they often fail to fully exploit the spatial\nrelationships among them, potentially overlooking intricate tissue structures\ncrucial for accurate diagnosis. To address this limitation, we propose\nProbabilistic Spatial Attention MIL (PSA-MIL), a novel attention-based MIL\nframework that integrates spatial context into the attention mechanism through\nlearnable distance-decayed priors, formulated within a probabilistic\ninterpretation of self-attention as a posterior distribution. This formulation\nenables a dynamic inference of spatial relationships during training,\neliminating the need for predefined assumptions often imposed by previous\napproaches. Additionally, we suggest a spatial pruning strategy for the\nposterior, effectively reducing self-attention's quadratic complexity. To\nfurther enhance spatial modeling, we introduce a diversity loss that encourages\nvariation among attention heads, ensuring each captures distinct spatial\nrepresentations. Together, PSA-MIL enables a more data-driven and adaptive\nintegration of spatial context, moving beyond predefined constraints. We\nachieve state-of-the-art performance across both contextual and non-contextual\nbaselines, while significantly reducing computational costs."}
{"id": "2503.16289", "pdf": "https://arxiv.org/pdf/2503.16289", "abs": "https://arxiv.org/abs/2503.16289", "authors": ["Inwoo Hwang", "Bing Zhou", "Young Min Kim", "Jian Wang", "Chuan Guo"], "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interactions", "categories": ["cs.CV"], "comment": "15 pages, Project page: http://inwoohwang.me/SceneMI", "summary": "Modeling human-scene interactions (HSI) is essential for understanding and\nsimulating everyday human behaviors. Recent approaches utilizing generative\nmodeling have made progress in this domain; however, they are limited in\ncontrollability and flexibility for real-world applications. To address these\nchallenges, we propose reformulating the HSI modeling problem as Scene-aware\nMotion In-betweening -- a more tractable and practical task. We introduce\nSceneMI, a framework that supports several practical applications, including\nkeyframe-guided character animation in 3D scenes and enhancing the motion\nquality of imperfect HSI data. SceneMI employs dual scene descriptors to\ncomprehensively encode global and local scene context. Furthermore, our\nframework leverages the inherent denoising nature of diffusion models to\ngeneralize on noisy keyframes. Experimental results demonstrate SceneMI's\neffectiveness in scene-aware keyframe in-betweening and generalization to the\nreal-world GIMO dataset, where motions and scenes are acquired by noisy IMU\nsensors and smartphones. We further showcase SceneMI's applicability in HSI\nreconstruction from monocular videos."}
{"id": "2503.16302", "pdf": "https://arxiv.org/pdf/2503.16302", "abs": "https://arxiv.org/abs/2503.16302", "authors": ["Zeqiang Lai", "Yunfei Zhao", "Zibo Zhao", "Haolin Liu", "Fuyun Wang", "Huiwen Shi", "Xianghui Yang", "Qinxiang Lin", "Jinwei Huang", "Yuhong Liu", "Jie Jiang", "Chunchao Guo", "Xiangyu Yue"], "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Technical report", "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."}
{"id": "2503.16318", "pdf": "https://arxiv.org/pdf/2503.16318", "abs": "https://arxiv.org/abs/2503.16318", "authors": ["Edgar Sucar", "Zihang Lai", "Eldar Insafutdinov", "Andrea Vedaldi"], "title": "Dynamic Point Maps: A Versatile Representation for Dynamic 3D Reconstruction", "categories": ["cs.CV"], "comment": "Web page:\n  https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/", "summary": "DUSt3R has recently shown that one can reduce many tasks in multi-view\ngeometry, including estimating camera intrinsics and extrinsics, reconstructing\nthe scene in 3D, and establishing image correspondences, to the prediction of a\npair of viewpoint-invariant point maps, i.e., pixel-aligned point clouds\ndefined in a common reference frame. This formulation is elegant and powerful,\nbut unable to tackle dynamic scenes. To address this challenge, we introduce\nthe concept of Dynamic Point Maps (DPM), extending standard point maps to\nsupport 4D tasks such as motion segmentation, scene flow estimation, 3D object\ntracking, and 2D correspondence. Our key intuition is that, when time is\nintroduced, there are several possible spatial and time references that can be\nused to define the point maps. We identify a minimal subset of such\ncombinations that can be regressed by a network to solve the sub tasks\nmentioned above. We train a DPM predictor on a mixture of synthetic and real\ndata and evaluate it across diverse benchmarks for video depth prediction,\ndynamic point cloud reconstruction, 3D scene flow and object pose tracking,\nachieving state-of-the-art performance. Code, models and additional results are\navailable at https://www.robots.ox.ac.uk/~vgg/research/dynamic-point-maps/."}
{"id": "2503.16322", "pdf": "https://arxiv.org/pdf/2503.16322", "abs": "https://arxiv.org/abs/2503.16322", "authors": ["Ruonan Yu", "Songhua Liu", "Zhenxiong Tan", "Xinchao Wang"], "title": "Ultra-Resolution Adaptation with Ease", "categories": ["cs.CV"], "comment": "Technical Report. Codes are available\n  \\href{https://github.com/Huage001/URAE}{here}", "summary": "Text-to-image diffusion models have achieved remarkable progress in recent\nyears. However, training models for high-resolution image generation remains\nchallenging, particularly when training data and computational resources are\nlimited. In this paper, we explore this practical problem from two key\nperspectives: data and parameter efficiency, and propose a set of key\nguidelines for ultra-resolution adaptation termed \\emph{URAE}. For data\nefficiency, we theoretically and empirically demonstrate that synthetic data\ngenerated by some teacher models can significantly promote training\nconvergence. For parameter efficiency, we find that tuning minor components of\nthe weight matrices outperforms widely-used low-rank adapters when synthetic\ndata are unavailable, offering substantial performance gains while maintaining\nefficiency. Additionally, for models leveraging guidance distillation, such as\nFLUX, we show that disabling classifier-free guidance, \\textit{i.e.}, setting\nthe guidance scale to 1 during adaptation, is crucial for satisfactory\nperformance. Extensive experiments validate that URAE achieves comparable\n2K-generation performance to state-of-the-art closed-source models like FLUX1.1\n[Pro] Ultra with only 3K samples and 2K iterations, while setting new\nbenchmarks for 4K-resolution generation. Codes are available\n\\href{https://github.com/Huage001/URAE}{here}."}
{"id": "2503.16338", "pdf": "https://arxiv.org/pdf/2503.16338", "abs": "https://arxiv.org/abs/2503.16338", "authors": ["Shengjun Zhang", "Xin Fei", "Fangfu Liu", "Haixu Song", "Yueqi Duan"], "title": "Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images", "categories": ["cs.CV"], "comment": "NeurIPS 2024", "summary": "3D Gaussian Splatting (3DGS) has demonstrated impressive novel view synthesis\nperformance. While conventional methods require per-scene optimization, more\nrecently several feed-forward methods have been proposed to generate\npixel-aligned Gaussian representations with a learnable network, which are\ngeneralizable to different scenes. However, these methods simply combine\npixel-aligned Gaussians from multiple views as scene representations, thereby\nleading to artifacts and extra memory cost without fully capturing the\nrelations of Gaussians from different images. In this paper, we propose\nGaussian Graph Network (GGN) to generate efficient and generalizable Gaussian\nrepresentations. Specifically, we construct Gaussian Graphs to model the\nrelations of Gaussian groups from different views. To support message passing\nat Gaussian level, we reformulate the basic graph operations over Gaussian\nrepresentations, enabling each Gaussian to benefit from its connected Gaussian\ngroups with Gaussian feature fusion. Furthermore, we design a Gaussian pooling\nlayer to aggregate various Gaussian groups for efficient representations. We\nconduct experiments on the large-scale RealEstate10K and ACID datasets to\ndemonstrate the efficiency and generalization of our method. Compared to the\nstate-of-the-art methods, our model uses fewer Gaussians and achieves better\nimage quality with higher rendering speed."}
{"id": "2503.16357", "pdf": "https://arxiv.org/pdf/2503.16357", "abs": "https://arxiv.org/abs/2503.16357", "authors": ["Tao Feng", "Yifan Xie", "Xun Guan", "Jiyuan Song", "Zhou Liu", "Fei Ma", "Fei Yu"], "title": "UniSync: A Unified Framework for Audio-Visual Synchronization", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures, accepted by ICME 2025", "summary": "Precise audio-visual synchronization in speech videos is crucial for content\nquality and viewer comprehension. Existing methods have made significant\nstrides in addressing this challenge through rule-based approaches and\nend-to-end learning techniques. However, these methods often rely on limited\naudio-visual representations and suboptimal learning strategies, potentially\nconstraining their effectiveness in more complex scenarios. To address these\nlimitations, we present UniSync, a novel approach for evaluating audio-visual\nsynchronization using embedding similarities. UniSync offers broad\ncompatibility with various audio representations (e.g., Mel spectrograms,\nHuBERT) and visual representations (e.g., RGB images, face parsing maps, facial\nlandmarks, 3DMM), effectively handling their significant dimensional\ndifferences. We enhance the contrastive learning framework with a margin-based\nloss component and cross-speaker unsynchronized pairs, improving discriminative\ncapabilities. UniSync outperforms existing methods on standard datasets and\ndemonstrates versatility across diverse audio-visual representations. Its\nintegration into talking face generation frameworks enhances synchronization\nquality in both natural and AI-generated content."}
{"id": "2503.16365", "pdf": "https://arxiv.org/pdf/2503.16365", "abs": "https://arxiv.org/abs/2503.16365", "authors": ["Muyao Li", "Zihao Wang", "Kaichen He", "Xiaojian Ma", "Yitao Liang"], "title": "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 5 figures", "summary": "Recently, action-based decision-making in open-world environments has gained\nsignificant attention. Visual Language Action (VLA) models, pretrained on\nlarge-scale web datasets, have shown promise in decision-making tasks. However,\nprevious work has primarily focused on action post-training, often neglecting\nenhancements to the foundational model itself. In response, we introduce a\nnovel approach, Act from Visual Language Post-Training, which refines Visual\nLanguage Models (VLMs) through visual and linguistic guidance in a\nself-supervised manner. This enhancement improves the models' capabilities in\nworld knowledge, visual recognition, and spatial grounding in open-world\nenvironments. Following the above post-training paradigms, we obtain the first\nVLA models in Minecraft that can follow human instructions on over 1k different\natomic tasks, including crafting, smelting, cooking, mining, and killing. Our\nexperiments demonstrate that post-training on non-trajectory tasks leads to a\nsignificant 40% improvement over the best agent baseline on a diverse set of\natomic tasks. Furthermore, we demonstrate that our approach surpasses\ntraditional imitation learning-based policies in Minecraft, achieving\nstate-of-the-art performance. We have open-sourced the code, models, and\ndatasets to foster further research. The project page can be found in\nhttps://craftjarvis.github.io/JarvisVLA."}
{"id": "2503.16375", "pdf": "https://arxiv.org/pdf/2503.16375", "abs": "https://arxiv.org/abs/2503.16375", "authors": ["Han-Hung Lee", "Qinghong Han", "Angel X. Chang"], "title": "NuiScene: Exploring Efficient Generation of Unbounded Outdoor Scenes", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we explore the task of generating expansive outdoor scenes,\nranging from castles to high-rises. Unlike indoor scene generation, which has\nbeen a primary focus of prior work, outdoor scene generation presents unique\nchallenges, including wide variations in scene heights and the need for a\nmethod capable of rapidly producing large landscapes. To address this, we\npropose an efficient approach that encodes scene chunks as uniform vector sets,\noffering better compression and performance than the spatially structured\nlatents used in prior methods. Furthermore, we train an explicit outpainting\nmodel for unbounded generation, which improves coherence compared to prior\nresampling-based inpainting schemes while also speeding up generation by\neliminating extra diffusion steps. To facilitate this task, we curate\nNuiScene43, a small but high-quality set of scenes, preprocessed for joint\ntraining. Notably, when trained on scenes of varying styles, our model can\nblend different environments, such as rural houses and city skyscrapers, within\nthe same scene, highlighting the potential of our curation process to leverage\nheterogeneous scenes for joint training."}
{"id": "2503.16376", "pdf": "https://arxiv.org/pdf/2503.16376", "abs": "https://arxiv.org/abs/2503.16376", "authors": ["Leyang Wang", "Joice Lin"], "title": "LaPIG: Cross-Modal Generation of Paired Thermal and Visible Facial Images", "categories": ["cs.CV"], "comment": null, "summary": "The success of modern machine learning, particularly in facial translation\nnetworks, is highly dependent on the availability of high-quality, paired,\nlarge-scale datasets. However, acquiring sufficient data is often challenging\nand costly. Inspired by the recent success of diffusion models in high-quality\nimage synthesis and advancements in Large Language Models (LLMs), we propose a\nnovel framework called LLM-assisted Paired Image Generation (LaPIG). This\nframework enables the construction of comprehensive, high-quality paired\nvisible and thermal images using captions generated by LLMs. Our method\nencompasses three parts: visible image synthesis with ArcFace embedding,\nthermal image translation using Latent Diffusion Models (LDMs), and caption\ngeneration with LLMs. Our approach not only generates multi-view paired visible\nand thermal images to increase data diversity but also produces high-quality\npaired data while maintaining their identity information. We evaluate our\nmethod on public datasets by comparing it with existing methods, demonstrating\nthe superiority of LaPIG."}
{"id": "2503.16378", "pdf": "https://arxiv.org/pdf/2503.16378", "abs": "https://arxiv.org/abs/2503.16378", "authors": ["Tzu-Yun Tseng", "Alexey Nekrasov", "Malcolm Burdorf", "Bastian Leibe", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "Panoptic-CUDAL Technical Report: Rural Australia Point Cloud Dataset in Rainy Conditions", "categories": ["cs.CV"], "comment": null, "summary": "Existing autonomous driving datasets are predominantly oriented towards\nwell-structured urban settings and favorable weather conditions, leaving the\ncomplexities of rural environments and adverse weather conditions largely\nunaddressed. Although some datasets encompass variations in weather and\nlighting, bad weather scenarios do not appear often. Rainfall can significantly\nimpair sensor functionality, introducing noise and reflections in LiDAR and\ncamera data and reducing the system's capabilities for reliable environmental\nperception and safe navigation. We introduce the Panoptic-CUDAL dataset, a\nnovel dataset purpose-built for panoptic segmentation in rural areas subject to\nrain. By recording high-resolution LiDAR, camera, and pose data, Panoptic-CUDAL\noffers a diverse, information-rich dataset in a challenging scenario. We\npresent analysis of the recorded data and provide baseline results for panoptic\nand semantic segmentation methods on LiDAR point clouds. The dataset can be\nfound here:\nhttps://robotics.sydney.edu.au/our-research/intelligent-transportation-systems/"}
{"id": "2503.16394", "pdf": "https://arxiv.org/pdf/2503.16394", "abs": "https://arxiv.org/abs/2503.16394", "authors": ["Akhil Perincherry", "Jacob Krantz", "Stefan Lee"], "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an\nunseen environment using natural language instructions. In this work, we study\nif visual representations of sub-goals implied by the instructions can serve as\nnavigational cues and lead to increased navigation performance. To synthesize\nthese visual representations or imaginations, we leverage a text-to-image\ndiffusion model on landmark references contained in segmented instructions.\nThese imaginations are provided to VLN agents as an added modality to act as\nlandmark cues and an auxiliary loss is added to explicitly encourage relating\nthese with their corresponding referring expressions. Our findings reveal an\nincrease in success rate (SR) of around 1 point and up to 0.5 points in success\nscaled by inverse path length (SPL) across agents. These results suggest that\nthe proposed approach reinforces visual understanding compared to relying on\nlanguage instructions alone. Code and data for our work can be found at\nhttps://www.akhilperincherry.com/VLN-Imagine-website/."}
{"id": "2503.16396", "pdf": "https://arxiv.org/pdf/2503.16396", "abs": "https://arxiv.org/abs/2503.16396", "authors": ["Chun-Han Yao", "Yiming Xie", "Vikram Voleti", "Huaizu Jiang", "Varun Jampani"], "title": "SV4D 2.0: Enhancing Spatio-Temporal Consistency in Multi-View Video Diffusion for High-Quality 4D Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present Stable Video 4D 2.0 (SV4D 2.0), a multi-view video diffusion model\nfor dynamic 3D asset generation. Compared to its predecessor SV4D, SV4D 2.0 is\nmore robust to occlusions and large motion, generalizes better to real-world\nvideos, and produces higher-quality outputs in terms of detail sharpness and\nspatio-temporal consistency. We achieve this by introducing key improvements in\nmultiple aspects: 1) network architecture: eliminating the dependency of\nreference multi-views and designing blending mechanism for 3D and frame\nattention, 2) data: enhancing quality and quantity of training data, 3)\ntraining strategy: adopting progressive 3D-4D training for better\ngeneralization, and 4) 4D optimization: handling 3D inconsistency and large\nmotion via 2-stage refinement and progressive frame sampling. Extensive\nexperiments demonstrate significant performance gain by SV4D 2.0 both visually\nand quantitatively, achieving better detail (-14\\% LPIPS) and 4D consistency\n(-44\\% FV4D) in novel-view video synthesis and 4D optimization (-12\\% LPIPS and\n-24\\% FV4D) compared to SV4D. Project page: https://sv4d2.0.github.io."}
{"id": "2503.16397", "pdf": "https://arxiv.org/pdf/2503.16397", "abs": "https://arxiv.org/abs/2503.16397", "authors": ["Nikita Starodubcev", "Denis Kuznedelev", "Artem Babenko", "Dmitry Baranchuk"], "title": "Scale-wise Distillation of Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present SwD, a scale-wise distillation framework for diffusion models\n(DMs), which effectively employs next-scale prediction ideas for\ndiffusion-based few-step generators. In more detail, SwD is inspired by the\nrecent insights relating diffusion processes to the implicit spectral\nautoregression. We suppose that DMs can initiate generation at lower data\nresolutions and gradually upscale the samples at each denoising step without\nloss in performance while significantly reducing computational costs. SwD\nnaturally integrates this idea into existing diffusion distillation methods\nbased on distribution matching. Also, we enrich the family of distribution\nmatching approaches by introducing a novel patch loss enforcing finer-grained\nsimilarity to the target distribution. When applied to state-of-the-art\ntext-to-image diffusion models, SwD approaches the inference times of two full\nresolution steps and significantly outperforms the counterparts under the same\ncomputation budget, as evidenced by automated metrics and human preference\nstudies."}
{"id": "2503.16399", "pdf": "https://arxiv.org/pdf/2503.16399", "abs": "https://arxiv.org/abs/2503.16399", "authors": ["Chen Chen", "Zhirui Wang", "Taowei Sheng", "Yi Jiang", "Yundu Li", "Peirui Cheng", "Luning Zhang", "Kaiqiang Chen", "Yanfeng Hu", "Xue Yang", "Xian Sun"], "title": "SA-Occ: Satellite-Assisted 3D Occupancy Prediction in Real World", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Existing vision-based 3D occupancy prediction methods are inherently limited\nin accuracy due to their exclusive reliance on street-view imagery, neglecting\nthe potential benefits of incorporating satellite views. We propose SA-Occ, the\nfirst Satellite-Assisted 3D occupancy prediction model, which leverages GPS &\nIMU to integrate historical yet readily available satellite imagery into\nreal-time applications, effectively mitigating limitations of ego-vehicle\nperceptions, involving occlusions and degraded performance in distant regions.\nTo address the core challenges of cross-view perception, we propose: 1)\nDynamic-Decoupling Fusion, which resolves inconsistencies in dynamic regions\ncaused by the temporal asynchrony between satellite and street views; 2)\n3D-Proj Guidance, a module that enhances 3D feature extraction from inherently\n2D satellite imagery; and 3) Uniform Sampling Alignment, which aligns the\nsampling density between street and satellite views. Evaluated on\nOcc3D-nuScenes, SA-Occ achieves state-of-the-art performance, especially among\nsingle-frame methods, with a 39.05% mIoU (a 6.97% improvement), while incurring\nonly 6.93 ms of additional latency per frame. Our code and newly curated\ndataset are available at https://github.com/chenchen235/SA-Occ."}
{"id": "2503.16412", "pdf": "https://arxiv.org/pdf/2503.16412", "abs": "https://arxiv.org/abs/2503.16412", "authors": ["Ananta R. Bhattarai", "Xingzhe He", "Alla Sheffer", "Helge Rhodin"], "title": "DreamTexture: Shape from Virtual Texture with Analysis by Augmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://anantarb.github.io/dreamtexture/", "summary": "DreamFusion established a new paradigm for unsupervised 3D reconstruction\nfrom virtual views by combining advances in generative models and\ndifferentiable rendering. However, the underlying multi-view rendering, along\nwith supervision from large-scale generative models, is computationally\nexpensive and under-constrained. We propose DreamTexture, a novel\nShape-from-Virtual-Texture approach that leverages monocular depth cues to\nreconstruct 3D objects. Our method textures an input image by aligning a\nvirtual texture with the real depth cues in the input, exploiting the inherent\nunderstanding of monocular geometry encoded in modern diffusion models. We then\nreconstruct depth from the virtual texture deformation with a new conformal map\noptimization, which alleviates memory-intensive volumetric representations. Our\nexperiments reveal that generative models possess an understanding of monocular\nshape cues, which can be extracted by augmenting and aligning texture cues -- a\nnovel monocular reconstruction paradigm that we call Analysis by Augmentation."}
{"id": "2503.16413", "pdf": "https://arxiv.org/pdf/2503.16413", "abs": "https://arxiv.org/abs/2503.16413", "authors": ["Xueyan Zou", "Yuchen Song", "Ri-Zhao Qiu", "Xuanbin Peng", "Jianglong Ye", "Sifei Liu", "Xiaolong Wang"], "title": "M3: 3D-Spatial MultiModal Memory", "categories": ["cs.CV", "cs.RO"], "comment": "ICLR2025 homepage: https://m3-spatial-memory.github.io code:\n  https://github.com/MaureenZOU/m3-spatial", "summary": "We present 3D Spatial MultiModal Memory (M3), a multimodal memory system\ndesigned to retain information about medium-sized static scenes through video\nsources for visual perception. By integrating 3D Gaussian Splatting techniques\nwith foundation models, M3 builds a multimodal memory capable of rendering\nfeature representations across granularities, encompassing a wide range of\nknowledge. In our exploration, we identify two key challenges in previous works\non feature splatting: (1) computational constraints in storing high-dimensional\nfeatures for each Gaussian primitive, and (2) misalignment or information loss\nbetween distilled features and foundation model features. To address these\nchallenges, we propose M3 with key components of principal scene components and\nGaussian memory attention, enabling efficient training and inference. To\nvalidate M3, we conduct comprehensive quantitative evaluations of feature\nsimilarity and downstream tasks, as well as qualitative visualizations to\nhighlight the pixel trace of Gaussian memory attention. Our approach\nencompasses a diverse range of foundation models, including vision-language\nmodels (VLMs), perception models, and large multimodal and language models\n(LMMs/LLMs). Furthermore, to demonstrate real-world applicability, we deploy\nM3's feature field in indoor scenes on a quadruped robot. Notably, we claim\nthat M3 is the first work to address the core compression challenges in 3D\nfeature distillation."}
{"id": "2503.16418", "pdf": "https://arxiv.org/pdf/2503.16418", "abs": "https://arxiv.org/abs/2503.16418", "authors": ["Liming Jiang", "Qing Yan", "Yumin Jia", "Zichuan Liu", "Hao Kang", "Xin Lu"], "title": "InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://bytedance.github.io/InfiniteYou/ Code and\n  model: https://github.com/bytedance/InfiniteYou", "summary": "Achieving flexible and high-fidelity identity-preserved image generation\nremains formidable, particularly with advanced Diffusion Transformers (DiTs)\nlike FLUX. We introduce InfiniteYou (InfU), one of the earliest robust\nframeworks leveraging DiTs for this task. InfU addresses significant issues of\nexisting methods, such as insufficient identity similarity, poor text-image\nalignment, and low generation quality and aesthetics. Central to InfU is\nInfuseNet, a component that injects identity features into the DiT base model\nvia residual connections, enhancing identity similarity while maintaining\ngeneration capabilities. A multi-stage training strategy, including pretraining\nand supervised fine-tuning (SFT) with synthetic single-person-multiple-sample\n(SPMS) data, further improves text-image alignment, ameliorates image quality,\nand alleviates face copy-pasting. Extensive experiments demonstrate that InfU\nachieves state-of-the-art performance, surpassing existing baselines. In\naddition, the plug-and-play design of InfU ensures compatibility with various\nexisting methods, offering a valuable contribution to the broader community."}
{"id": "2503.16420", "pdf": "https://arxiv.org/pdf/2503.16420", "abs": "https://arxiv.org/abs/2503.16420", "authors": ["Paul Engstler", "Aleksandar Shtedritski", "Iro Laina", "Christian Rupprecht", "Andrea Vedaldi"], "title": "SynCity: Training-Free Generation of 3D Worlds", "categories": ["cs.CV"], "comment": "Project page: https://research.paulengstler.com/syncity/", "summary": "We address the challenge of generating 3D worlds from textual descriptions.\nWe propose SynCity, a training- and optimization-free approach, which leverages\nthe geometric precision of pre-trained 3D generative models and the artistic\nversatility of 2D image generators to create large, high-quality 3D spaces.\nWhile most 3D generative models are object-centric and cannot generate\nlarge-scale worlds, we show how 3D and 2D generators can be combined to\ngenerate ever-expanding scenes. Through a tile-based approach, we allow\nfine-grained control over the layout and the appearance of scenes. The world is\ngenerated tile-by-tile, and each new tile is generated within its world-context\nand then fused with the scene. SynCity generates compelling and immersive\nscenes that are rich in detail and diversity."}
{"id": "2503.16421", "pdf": "https://arxiv.org/pdf/2503.16421", "abs": "https://arxiv.org/abs/2503.16421", "authors": ["Quanhao Li", "Zhen Xing", "Rui Wang", "Hui Zhang", "Qi Dai", "Zuxuan Wu"], "title": "MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in video generation have led to remarkable improvements in\nvisual quality and temporal coherence. Upon this, trajectory-controllable video\ngeneration has emerged to enable precise object motion control through\nexplicitly defined spatial paths. However, existing methods struggle with\ncomplex object movements and multi-object motion control, resulting in\nimprecise trajectory adherence, poor object consistency, and compromised visual\nquality. Furthermore, these methods only support trajectory control in a single\nformat, limiting their applicability in diverse scenarios. Additionally, there\nis no publicly available dataset or benchmark specifically tailored for\ntrajectory-controllable video generation, hindering robust training and\nsystematic evaluation. To address these challenges, we introduce MagicMotion, a\nnovel image-to-video generation framework that enables trajectory control\nthrough three levels of conditions from dense to sparse: masks, bounding boxes,\nand sparse boxes. Given an input image and trajectories, MagicMotion seamlessly\nanimates objects along defined trajectories while maintaining object\nconsistency and visual quality. Furthermore, we present MagicData, a\nlarge-scale trajectory-controlled video dataset, along with an automated\npipeline for annotation and filtering. We also introduce MagicBench, a\ncomprehensive benchmark that assesses both video quality and trajectory control\naccuracy across different numbers of objects. Extensive experiments demonstrate\nthat MagicMotion outperforms previous methods across various metrics. Our\nproject page are publicly available at\nhttps://quanhaol.github.io/magicmotion-site."}
{"id": "2503.16422", "pdf": "https://arxiv.org/pdf/2503.16422", "abs": "https://arxiv.org/abs/2503.16422", "authors": ["Yuheng Yuan", "Qiuhong Shen", "Xingyi Yang", "Xinchao Wang"], "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "4D Gaussian Splatting (4DGS) has recently gained considerable attention as a\nmethod for reconstructing dynamic scenes. Despite achieving superior quality,\n4DGS typically requires substantial storage and suffers from slow rendering\nspeed. In this work, we delve into these issues and identify two key sources of\ntemporal redundancy. (Q1) \\textbf{Short-Lifespan Gaussians}: 4DGS uses a large\nportion of Gaussians with short temporal span to represent scene dynamics,\nleading to an excessive number of Gaussians. (Q2) \\textbf{Inactive Gaussians}:\nWhen rendering, only a small subset of Gaussians contributes to each frame.\nDespite this, all Gaussians are processed during rasterization, resulting in\nredundant computation overhead. To address these redundancies, we present\n\\textbf{4DGS-1K}, which runs at over 1000 FPS on modern GPUs. For Q1, we\nintroduce the Spatial-Temporal Variation Score, a new pruning criterion that\neffectively removes short-lifespan Gaussians while encouraging 4DGS to capture\nscene dynamics using Gaussians with longer temporal spans. For Q2, we store a\nmask for active Gaussians across consecutive frames, significantly reducing\nredundant computations in rendering. Compared to vanilla 4DGS, our method\nachieves a $41\\times$ reduction in storage and $9\\times$ faster rasterization\nspeed on complex dynamic scenes, while maintaining comparable visual quality.\nPlease see our project page at https://4DGS-1K.github.io."}
{"id": "2503.16423", "pdf": "https://arxiv.org/pdf/2503.16423", "abs": "https://arxiv.org/abs/2503.16423", "authors": ["Ron Campos", "Ashmal Vayani", "Parth Parag Kulkarni", "Rohit Gupta", "Aritra Dutta", "Mubarak Shah"], "title": "GAEA: A Geolocation Aware Conversational Model", "categories": ["cs.CV", "cs.LG", "I.4; I.2.7; I.5"], "comment": "The dataset and code used in this submission is available at:\n  https://ucf-crcv.github.io/GAEA/", "summary": "Image geolocalization, in which, traditionally, an AI model predicts the\nprecise GPS coordinates of an image is a challenging task with many downstream\napplications. However, the user cannot utilize the model to further their\nknowledge other than the GPS coordinate; the model lacks an understanding of\nthe location and the conversational ability to communicate with the user. In\nrecent days, with tremendous progress of large multimodal models (LMMs)\nproprietary and open-source researchers have attempted to geolocalize images\nvia LMMs. However, the issues remain unaddressed; beyond general tasks, for\nmore specialized downstream tasks, one of which is geolocalization, LMMs\nstruggle. In this work, we propose to solve this problem by introducing a\nconversational model GAEA that can provide information regarding the location\nof an image, as required by a user. No large-scale dataset enabling the\ntraining of such a model exists. Thus we propose a comprehensive dataset GAEA\nwith 800K images and around 1.6M question answer pairs constructed by\nleveraging OpenStreetMap (OSM) attributes and geographical context clues. For\nquantitative evaluation, we propose a diverse benchmark comprising 4K\nimage-text pairs to evaluate conversational capabilities equipped with diverse\nquestion types. We consider 11 state-of-the-art open-source and proprietary\nLMMs and demonstrate that GAEA significantly outperforms the best open-source\nmodel, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by\n8.28%. Our dataset, model and codes are available"}
{"id": "2503.16425", "pdf": "https://arxiv.org/pdf/2503.16425", "abs": "https://arxiv.org/abs/2503.16425", "authors": ["Zigang Geng", "Mengde Xu", "Han Hu", "Shuyang Gu"], "title": "Tokenize Image as a Set", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes a fundamentally new paradigm for image generation through\nset-based tokenization and distribution modeling. Unlike conventional methods\nthat serialize images into fixed-position latent codes with a uniform\ncompression ratio, we introduce an unordered token set representation to\ndynamically allocate coding capacity based on regional semantic complexity.\nThis TokenSet enhances global context aggregation and improves robustness\nagainst local perturbations. To address the critical challenge of modeling\ndiscrete sets, we devise a dual transformation mechanism that bijectively\nconverts sets into fixed-length integer sequences with summation constraints.\nFurther, we propose Fixed-Sum Discrete Diffusion--the first framework to\nsimultaneously handle discrete values, fixed sequence length, and summation\ninvariance--enabling effective set distribution modeling. Experiments\ndemonstrate our method's superiority in semantic-aware representation and\ngeneration quality. Our innovations, spanning novel representation and modeling\nstrategies, advance visual generation beyond traditional sequential token\nparadigms. Our code and models are publicly available at\nhttps://github.com/Gengzigang/TokenSet."}
{"id": "2503.16426", "pdf": "https://arxiv.org/pdf/2503.16426", "abs": "https://arxiv.org/abs/2503.16426", "authors": ["Keyan Chen", "Chenyang Liu", "Bowen Chen", "Wenyuan Li", "Zhengxia Zou", "Zhenwei Shi"], "title": "DynamicVis: An Efficient and General Visual Foundation Model for Remote Sensing Image Understanding", "categories": ["cs.CV"], "comment": null, "summary": "The advancement of remote sensing technology has improved the spatial\nresolution of satellite imagery, facilitating more detailed visual\nrepresentations for diverse interpretations. However, existing methods exhibit\nlimited generalization capabilities across varied applications. While some\ncontemporary foundation models demonstrate potential, they are hindered by\ninsufficient cross-task adaptability and primarily process low-resolution\nimagery of restricted sizes, thus failing to fully exploit high-resolution data\nor leverage comprehensive large-scene semantics. Crucially, remote sensing\nimagery differs fundamentally from natural images, as key foreground targets\n(eg., maritime objects, artificial structures) often occupy minimal spatial\nproportions (~1%) and exhibit sparse distributions. Efficiently modeling\ncross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a\nsignificant challenge yet remains critical for remote sensing image\nunderstanding. Motivated by the selective attention mechanisms inherent to the\nhuman visual system, we propose DynamicVis, a dynamic visual perception\nfoundation model for remote sensing imagery. The framework integrates a novel\ndynamic region perception backbone based on the selective state space model,\nwhich strategically balances localized detail extraction with global contextual\nintegration, enabling computationally efficient encoding of large-scale data\nwhile maintaining architectural scalability. To enhance cross-task knowledge\ntransferring, we introduce a multi-instance learning paradigm utilizing\nmeta-embedding representations, trained on million-scale region-level\nannotations. Evaluations across nine downstream tasks demonstrate the model's\nversatility. DynamicVis achieves multi-level feature modeling with exceptional\nefficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and\n833 MB GPU memory (3% of ViT's)."}
{"id": "2503.16429", "pdf": "https://arxiv.org/pdf/2503.16429", "abs": "https://arxiv.org/abs/2503.16429", "authors": ["Xiaoyang Wu", "Daniel DeTone", "Duncan Frost", "Tianwei Shen", "Chris Xie", "Nan Yang", "Jakob Engel", "Richard Newcombe", "Hengshuang Zhao", "Julian Straub"], "title": "Sonata: Self-Supervised Learning of Reliable Point Representations", "categories": ["cs.CV"], "comment": "CVPR 2025, produced by Pointcept x Meta, project page:\n  https://xywu.me/sonata/", "summary": "In this paper, we question whether we have a reliable self-supervised point\ncloud model that can be used for diverse 3D tasks via simple linear probing,\neven with limited data and minimal computation. We find that existing 3D\nself-supervised learning approaches fall short when evaluated on representation\nquality through linear probing. We hypothesize that this is due to what we term\nthe \"geometric shortcut\", which causes representations to collapse to low-level\nspatial features. This challenge is unique to 3D and arises from the sparse\nnature of point cloud data. We address it through two key strategies: obscuring\nspatial information and enhancing the reliance on input features, ultimately\ncomposing a Sonata of 140k point clouds through self-distillation. Sonata is\nsimple and intuitive, yet its learned representations are strong and reliable:\nzero-shot visualizations demonstrate semantic grouping, alongside strong\nspatial reasoning through nearest-neighbor relationships. Sonata demonstrates\nexceptional parameter and data efficiency, tripling linear probing accuracy\n(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%\nof the data compared to previous approaches. Full fine-tuning further advances\nSOTA across both 3D indoor and outdoor perception tasks."}
{"id": "2503.16430", "pdf": "https://arxiv.org/pdf/2503.16430", "abs": "https://arxiv.org/abs/2503.16430", "authors": ["Yuqing Wang", "Zhijie Lin", "Yao Teng", "Yuanzhi Zhu", "Shuhuai Ren", "Jiashi Feng", "Xihui Liu"], "title": "Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation", "categories": ["cs.CV"], "comment": "Project page: https://yuqingwang1029.github.io/TokenBridge", "summary": "Autoregressive visual generation models typically rely on tokenizers to\ncompress images into tokens that can be predicted sequentially. A fundamental\ndilemma exists in token representation: discrete tokens enable straightforward\nmodeling with standard cross-entropy loss, but suffer from information loss and\ntokenizer training instability; continuous tokens better preserve visual\ndetails, but require complex distribution modeling, complicating the generation\npipeline. In this paper, we propose TokenBridge, which bridges this gap by\nmaintaining the strong representation capacity of continuous tokens while\npreserving the modeling simplicity of discrete tokens. To achieve this, we\ndecouple discretization from the tokenizer training process through\npost-training quantization that directly obtains discrete tokens from\ncontinuous representations. Specifically, we introduce a dimension-wise\nquantization strategy that independently discretizes each feature dimension,\npaired with a lightweight autoregressive prediction mechanism that efficiently\nmodel the resulting large token space. Extensive experiments show that our\napproach achieves reconstruction and generation quality on par with continuous\nmethods while using standard categorical prediction. This work demonstrates\nthat bridging discrete and continuous paradigms can effectively harness the\nstrengths of both approaches, providing a promising direction for high-quality\nvisual generation with simple autoregressive modeling. Project page:\nhttps://yuqingwang1029.github.io/TokenBridge."}
{"id": "2503.15524", "pdf": "https://arxiv.org/pdf/2503.15524", "abs": "https://arxiv.org/abs/2503.15524", "authors": ["Matthew Wilchek", "Linhan Wang", "Sally Dickinson", "Erica Feuerbacher", "Kurt Luther", "Feras A. Batarseh"], "title": "KHAIT: K-9 Handler Artificial Intelligence Teaming for Collaborative Sensemaking", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.ET", "cs.MA", "I.2.11; I.4.8; H.5.2; H.5.3; J.7"], "comment": "13 pages, 7 figures, ACM 30th International Conference on Intelligent\n  User Interfaces (IUI 25)", "summary": "In urban search and rescue (USAR) operations, communication between handlers\nand specially trained canines is crucial but often complicated by challenging\nenvironments and the specific behaviors canines are trained to exhibit when\ndetecting a person. Since a USAR canine often works out of sight of the\nhandler, the handler lacks awareness of the canine's location and situation,\nknown as the 'sensemaking gap.' In this paper, we propose KHAIT, a novel\napproach to close the sensemaking gap and enhance USAR effectiveness by\nintegrating object detection-based Artificial Intelligence (AI) and Augmented\nReality (AR). Equipped with AI-powered cameras, edge computing, and AR\nheadsets, KHAIT enables precise and rapid object detection from a canine's\nperspective, improving survivor localization. We evaluate this approach in a\nreal-world USAR environment, demonstrating an average survival allocation time\ndecrease of 22%, enhancing the speed and accuracy of operations."}
{"id": "2503.15557", "pdf": "https://arxiv.org/pdf/2503.15557", "abs": "https://arxiv.org/abs/2503.15557", "authors": ["Inwoo Hwang", "Jinseok Bae", "Donggeun Lim", "Young Min Kim"], "title": "Motion Synthesis with Sparse and Flexible Keyjoint Control", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "11 pages, Project Page: http://inwoohwang.me/SFControl", "summary": "Creating expressive character animations is labor-intensive, requiring\nintricate manual adjustment of animators across space and time. Previous works\non controllable motion generation often rely on a predefined set of dense\nspatio-temporal specifications (e.g., dense pelvis trajectories with exact\nper-frame timing), limiting practicality for animators. To process high-level\nintent and intuitive control in diverse scenarios, we propose a practical\ncontrollable motions synthesis framework that respects sparse and flexible\nkeyjoint signals. Our approach employs a decomposed diffusion-based motion\nsynthesis framework that first synthesizes keyjoint movements from sparse input\ncontrol signals and then synthesizes full-body motion based on the completed\nkeyjoint trajectories. The low-dimensional keyjoint movements can easily adapt\nto various control signal types, such as end-effector position for diverse\ngoal-driven motion synthesis, or incorporate functional constraints on a subset\nof keyjoints. Additionally, we introduce a time-agnostic control formulation,\neliminating the need for frame-specific timing annotations and enhancing\ncontrol flexibility. Then, the shared second stage can synthesize a natural\nwhole-body motion that precisely satisfies the task requirement from dense\nkeyjoint movements. We demonstrate the effectiveness of sparse and flexible\nkeyjoint control through comprehensive experiments on diverse datasets and\nscenarios."}
{"id": "2503.15558", "pdf": "https://arxiv.org/pdf/2503.15558", "abs": "https://arxiv.org/abs/2503.15558", "authors": ["NVIDIA", ":", "Alisson Azzolini", "Hannah Brandon", "Prithvijit Chattopadhyay", "Huayu Chen", "Jinju Chu", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Francesco Ferroni", "Rama Govindaraju", "Jinwei Gu", "Siddharth Gururani", "Imad El Hanafi", "Zekun Hao", "Jacob Huffman", "Jingyi Jin", "Brendan Johnson", "Rizwan Khan", "George Kurian", "Elena Lantz", "Nayeon Lee", "Zhaoshuo Li", "Xuan Li", "Tsung-Yi Lin", "Yen-Chen Lin", "Ming-Yu Liu", "Andrew Mathau", "Yun Ni", "Lindsey Pavao", "Wei Ping", "David W. Romero", "Misha Smelyanskiy", "Shuran Song", "Lyne Tchapmi", "Andrew Z. Wang", "Boxin Wang", "Haoxiang Wang", "Fangyin Wei", "Jiashu Xu", "Yao Xu", "Xiaodong Yang", "Zhuolin Yang", "Xiaohui Zeng", "Zhe Zhang"], "title": "Cosmos-Reason1: From Physical Common Sense To Embodied Reasoning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Physical AI systems need to perceive, understand, and perform complex actions\nin the physical world. In this paper, we present the Cosmos-Reason1 models that\ncan understand the physical world and generate appropriate embodied decisions\n(e.g., next step action) in natural language through long chain-of-thought\nreasoning processes. We begin by defining key capabilities for Physical AI\nreasoning, with a focus on physical common sense and embodied reasoning. To\nrepresent physical common sense, we use a hierarchical ontology that captures\nfundamental knowledge about space, time, and physics. For embodied reasoning,\nwe rely on a two-dimensional ontology that generalizes across different\nphysical embodiments. Building on these capabilities, we develop two multimodal\nlarge language models, Cosmos-Reason1-8B and Cosmos-Reason1-56B. We curate data\nand train our models in four stages: vision pre-training, general supervised\nfine-tuning (SFT), Physical AI SFT, and Physical AI reinforcement learning (RL)\nas the post-training. To evaluate our models, we build comprehensive benchmarks\nfor physical common sense and embodied reasoning according to our ontologies.\nEvaluation results show that Physical AI SFT and reinforcement learning bring\nsignificant improvements. To facilitate the development of Physical AI, we will\nmake our code and pre-trained models available under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-reason1."}
{"id": "2503.15562", "pdf": "https://arxiv.org/pdf/2503.15562", "abs": "https://arxiv.org/abs/2503.15562", "authors": ["Nicolás Laverde", "Melissa Robles", "Johan Rodríguez"], "title": "Shap-MeD", "categories": ["cs.GR", "cs.CE", "cs.CV"], "comment": null, "summary": "We present Shap-MeD, a text-to-3D object generative model specialized in the\nbiomedical domain. The objective of this study is to develop an assistant that\nfacilitates the 3D modeling of medical objects, thereby reducing development\ntime. 3D modeling in medicine has various applications, including surgical\nprocedure simulation and planning, the design of personalized prosthetic\nimplants, medical education, the creation of anatomical models, and the\ndevelopment of research prototypes. To achieve this, we leverage Shap-e, an\nopen-source text-to-3D generative model developed by OpenAI, and fine-tune it\nusing a dataset of biomedical objects. Our model achieved a mean squared error\n(MSE) of 0.089 in latent generation on the evaluation set, compared to Shap-e's\nMSE of 0.147. Additionally, we conducted a qualitative evaluation, comparing\nour model with others in the generation of biomedical objects. Our results\nindicate that Shap-MeD demonstrates higher structural accuracy in biomedical\nobject generation."}
{"id": "2503.15576", "pdf": "https://arxiv.org/pdf/2503.15576", "abs": "https://arxiv.org/abs/2503.15576", "authors": ["Alba Márquez-Rodríguez", "Miguel Ángel Mohedano-Munoz", "Manuel J. Marín-Jiménez", "Eduardo Santamaría-García", "Giulia Bastianelli", "Pedro Jordano", "Irene Mendoza"], "title": "A Bird Song Detector for improving bird identification through Deep Learning: a case study from Doñana", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.LG", "cs.NE", "I.5.4; I.2.6; I.4.8"], "comment": "20 pages, 13 images, for associated dataset see\n  https://huggingface.co/datasets/GrunCrow/BIRDeep_AudioAnnotations , for\n  associated code see\n  https://github.com/GrunCrow/BIRDeep_BirdSongDetector_NeuralNetworks and\n  https://github.com/GrunCrow/Bird-Song-Detector", "summary": "Passive Acoustic Monitoring with automatic recorders is essential for\necosystem conservation but generates vast unsupervised audio data, posing\nchallenges for extracting meaningful information. Deep Learning techniques\noffer a promising solution. BirdNET, a widely used model for bird\nidentification, has shown success in many study systems but is limited in some\nregions due to biases in its training data. A key challenge in bird species\ndetection is that many recordings either lack target species or contain\noverlapping vocalizations. To overcome these problems, we developed a\nmulti-stage pipeline for automatic bird vocalization identification in Do\\~nana\nNational Park (SW Spain), a region facing significant conservation threats. Our\napproach included a Bird Song Detector to isolate vocalizations and custom\nclassifiers trained with BirdNET embeddings. We manually annotated 461 minutes\nof audio from three habitats across nine locations, yielding 3,749 annotations\nfor 34 classes. Spectrograms facilitated the use of image processing\ntechniques. Applying the Bird Song Detector before classification improved\nspecies identification, as all classification models performed better when\nanalyzing only the segments where birds were detected. Specifically, the\ncombination of the Bird Song Detector and fine-tuned BirdNET compared to the\nbaseline without the Bird Song Detector. Our approach demonstrated the\neffectiveness of integrating a Bird Song Detector with fine-tuned\nclassification models for bird identification at local soundscapes. These\nfindings highlight the need to adapt general-purpose tools for specific\necological challenges, as demonstrated in Do\\~nana. Automatically detecting\nbird species serves for tracking the health status of this threatened\necosystem, given the sensitivity of birds to environmental changes, and helps\nin the design of conservation measures for reducing biodiversity loss"}
{"id": "2503.15586", "pdf": "https://arxiv.org/pdf/2503.15586", "abs": "https://arxiv.org/abs/2503.15586", "authors": ["Zeqi Gu", "Difan Liu", "Timothy Langlois", "Matthew Fisher", "Abe Davis"], "title": "How to Train Your Dragon: Automatic Diffusion-Based Rigging for Characters with Diverse Topologies", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to Eurographics 2025", "summary": "Recent diffusion-based methods have achieved impressive results on animating\nimages of human subjects. However, most of that success has built on\nhuman-specific body pose representations and extensive training with labeled\nreal videos. In this work, we extend the ability of such models to animate\nimages of characters with more diverse skeletal topologies. Given a small\nnumber (3-5) of example frames showing the character in different poses with\ncorresponding skeletal information, our model quickly infers a rig for that\ncharacter that can generate images corresponding to new skeleton poses. We\npropose a procedural data generation pipeline that efficiently samples training\ndata with diverse topologies on the fly. We use it, along with a novel skeleton\nrepresentation, to train our model on articulated shapes spanning a large space\nof textures and topologies. Then during fine-tuning, our model rapidly adapts\nto unseen target characters and generalizes well to rendering new poses, both\nfor realistic and more stylized cartoon appearances. To better evaluate\nperformance on this novel and challenging task, we create the first 2D video\ndataset that contains both humanoid and non-humanoid subjects with per-frame\nkeypoint annotations. With extensive experiments, we demonstrate the superior\nquality of our results. Project page: https://traindragondiffusion.github.io/"}
{"id": "2503.15648", "pdf": "https://arxiv.org/pdf/2503.15648", "abs": "https://arxiv.org/abs/2503.15648", "authors": ["Ragendhu Sp", "Tony Thomas", "Sabu Emmanuel"], "title": "Cancelable Biometric Template Generation Using Random Feature Vector Transformations", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Cancelable biometric schemes are designed to extract an identity-preserving,\nnon-invertible as well as revocable pseudo-identifier from biometric data.\nRecognition systems need to store only this pseudo-identifier, to avoid\ntampering and/or stealing of original biometric data during the recognition\nprocess. State-of-the-art cancelable schemes generate pseudo-identifiers by\ntransforming the original template using either user-specific salting or\nmany-to-one transformations. In addition to the performance concerns, most of\nsuch schemes are modality-specific and prone to reconstruction attacks as there\nare chances for unauthorized access to security-critical transformation keys. A\nnovel, modality-independent cancelable biometric scheme is proposed to overcome\nthese limitations. In this scheme, a cancelable template (pseudo identifier) is\ngenerated as a distance vector between multiple random transformations of the\nbiometric feature vector. These transformations were done by grouping feature\nvector components based on a set of user-specific random vectors. The proposed\nscheme nullifies the possibility of template reconstruction as the generated\ncancelable template contains only the distance values between the different\nrandom transformations of the feature vector and it does not store any details\nof the biometric template. The recognition performance of the proposed scheme\nis evaluated for face and fingerprint modalities. Equal Error Rate (EER) of 1.5\nis obtained for face and 1.7 is obtained for the fingerprint in the worst case."}
{"id": "2503.15770", "pdf": "https://arxiv.org/pdf/2503.15770", "abs": "https://arxiv.org/abs/2503.15770", "authors": ["Bingxuan Li", "Jiahao Wu", "Yuan Xu", "Yunxiang Zhang", "Zezheng Zhu", "Nanfang Yu", "Qi Sun"], "title": "Nano-3D: Metasurface-Based Neural Depth Imaging", "categories": ["physics.optics", "cs.AR", "cs.CV"], "comment": null, "summary": "Depth imaging is a foundational building block for broad applications, such\nas autonomous driving and virtual/augmented reality. Traditionally, depth\ncameras have relied on time-of-flight sensors or multi-lens systems to achieve\nphysical depth measurements. However, these systems often face a trade-off\nbetween a bulky form factor and imprecise approximations, limiting their\nsuitability for spatially constrained scenarios. Inspired by the emerging\nadvancements of nano-optics, we present Nano-3D, a metasurface-based neural\ndepth imaging solution with an ultra-compact footprint. Nano-3D integrates our\ncustom-fabricated 700 nm thick TiO2 metasurface with a multi-module deep neural\nnetwork to extract precise metric depth information from monocular\nmetasurface-polarized imagery. We demonstrate the effectiveness of Nano-3D with\nboth simulated and physical experiments. We hope the exhibited success paves\nthe way for the community to bridge future graphics systems with emerging\nnanomaterial technologies through novel computational approaches."}
{"id": "2503.15781", "pdf": "https://arxiv.org/pdf/2503.15781", "abs": "https://arxiv.org/abs/2503.15781", "authors": ["Yuci Han", "Charles Toth", "Alper Yilmaz"], "title": "UAS Visual Navigation in Large and Unseen Environments via a Meta Agent", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The aim of this work is to develop an approach that enables Unmanned Aerial\nSystem (UAS) to efficiently learn to navigate in large-scale urban environments\nand transfer their acquired expertise to novel environments. To achieve this,\nwe propose a meta-curriculum training scheme. First, meta-training allows the\nagent to learn a master policy to generalize across tasks. The resulting model\nis then fine-tuned on the downstream tasks. We organize the training curriculum\nin a hierarchical manner such that the agent is guided from coarse to fine\ntowards the target task. In addition, we introduce Incremental Self-Adaptive\nReinforcement learning (ISAR), an algorithm that combines the ideas of\nincremental learning and meta-reinforcement learning (MRL). In contrast to\ntraditional reinforcement learning (RL), which focuses on acquiring a policy\nfor a specific task, MRL aims to learn a policy with fast transfer ability to\nnovel tasks. However, the MRL training process is time consuming, whereas our\nproposed ISAR algorithm achieves faster convergence than the conventional MRL\nalgorithm. We evaluate the proposed methodologies in simulated environments and\ndemonstrate that using this training philosophy in conjunction with the ISAR\nalgorithm significantly improves the convergence speed for navigation in\nlarge-scale cities and the adaptation proficiency in novel environments."}
{"id": "2503.15809", "pdf": "https://arxiv.org/pdf/2503.15809", "abs": "https://arxiv.org/abs/2503.15809", "authors": ["Xuan Gao", "Jingtao Zhou", "Dongyu Liu", "Yuqi Zhou", "Juyong Zhang"], "title": "Controlling Avatar Diffusion with Learnable Gaussian Embedding", "categories": ["cs.GR", "cs.CV"], "comment": "Project Page: https://ustc3dv.github.io/Learn2Control/", "summary": "Recent advances in diffusion models have made significant progress in digital\nhuman generation. However, most existing models still struggle to maintain 3D\nconsistency, temporal coherence, and motion accuracy. A key reason for these\nshortcomings is the limited representation ability of commonly used control\nsignals(e.g., landmarks, depth maps, etc.). In addition, the lack of diversity\nin identity and pose variations in public datasets further hinders progress in\nthis area. In this paper, we analyze the shortcomings of current control\nsignals and introduce a novel control signal representation that is\noptimizable, dense, expressive, and 3D consistent. Our method embeds a\nlearnable neural Gaussian onto a parametric head surface, which greatly\nenhances the consistency and expressiveness of diffusion-based head models.\nRegarding the dataset, we synthesize a large-scale dataset with multiple poses\nand identities. In addition, we use real/synthetic labels to effectively\ndistinguish real and synthetic data, minimizing the impact of imperfections in\nsynthetic data on the generated head images. Extensive experiments show that\nour model outperforms existing methods in terms of realism, expressiveness, and\n3D consistency. Our code, synthetic datasets, and pre-trained models will be\nreleased in our project page: https://ustc3dv.github.io/Learn2Control/"}
{"id": "2503.15861", "pdf": "https://arxiv.org/pdf/2503.15861", "abs": "https://arxiv.org/abs/2503.15861", "authors": ["Jie Gan", "Zhuonan Liang", "Jianan Fan", "Lisa Mcguire", "Caterina Watson", "Jacqueline Spurway", "Jillian Clarke", "Weidong Cai"], "title": "Sequential Spatial-Temporal Network for Interpretable Automatic Ultrasonic Assessment of Fetal Head during labor", "categories": ["eess.IV", "cs.CV"], "comment": "This work has been accepted to 2025 IEEE 22nd International Symposium\n  on Biomedical Imaging (ISBI)", "summary": "The intrapartum ultrasound guideline established by ISUOG highlights the\nAngle of Progression (AoP) and Head Symphysis Distance (HSD) as pivotal metrics\nfor assessing fetal head descent and predicting delivery outcomes. Accurate\nmeasurement of the AoP and HSD requires a structured process. This begins with\nidentifying standardized ultrasound planes, followed by the detection of\nspecific anatomical landmarks within the regions of the pubic symphysis and\nfetal head that correlate with the delivery parameters AoP and HSD. Finally,\nthese measurements are derived based on the identified anatomical landmarks.\nAddressing the clinical demands and standard operation process outlined in the\nISUOG guideline, we introduce the Sequential Spatial-Temporal Network (SSTN),\nthe first interpretable model specifically designed for the video of\nintrapartum ultrasound analysis. The SSTN operates by first identifying\nultrasound planes, then segmenting anatomical structures such as the pubic\nsymphysis and fetal head, and finally detecting key landmarks for precise\nmeasurement of HSD and AoP. Furthermore, the cohesive framework leverages\ntask-related information to improve accuracy and reliability. Experimental\nevaluations on clinical datasets demonstrate that SSTN significantly surpasses\nexisting models, reducing the mean absolute error by 18% for AoP and 22% for\nHSD."}
{"id": "2503.15986", "pdf": "https://arxiv.org/pdf/2503.15986", "abs": "https://arxiv.org/abs/2503.15986", "authors": ["Zeqi Zheng", "Yanchen Huang", "Yingchao Yu", "Zizheng Zhu", "Junfeng Tang", "Zhaofei Yu", "Yaochu Jin"], "title": "SpiLiFormer: Enhancing Spiking Transformers with Lateral Inhibition", "categories": ["cs.NE", "cs.CV"], "comment": "16 pages, 7 figures", "summary": "Spiking Neural Networks (SNNs) based on Transformers have garnered\nsignificant attention due to their superior performance and high energy\nefficiency. However, the spiking attention modules of most existing\nTransformer-based SNNs are adapted from those of analog Transformers, failing\nto fully address the issue of over-allocating attention to irrelevant contexts.\nTo fix this fundamental yet overlooked issue, we propose a Lateral\nInhibition-inspired Spiking Transformer (SpiLiFormer). It emulates the brain's\nlateral inhibition mechanism, guiding the model to enhance attention to\nrelevant tokens while suppressing attention to irrelevant ones. Our model\nachieves state-of-the-art (SOTA) performance across multiple datasets,\nincluding CIFAR-10 (+0.45%), CIFAR-100 (+0.48%), CIFAR10-DVS (+2.70%),\nN-Caltech101 (+1.94%), and ImageNet-1K (+1.6%). Notably, on the ImageNet-1K\ndataset, SpiLiFormer (69.9M parameters, 4 time steps, 384 resolution)\noutperforms E-SpikeFormer (173.0M parameters, 8 time steps, 384 resolution), a\nSOTA spiking Transformer, by 0.46% using only 39% of the parameters and half\nthe time steps. Our code and training checkpoints will be released upon\nacceptance."}
{"id": "2503.15996", "pdf": "https://arxiv.org/pdf/2503.15996", "abs": "https://arxiv.org/abs/2503.15996", "authors": ["Marc Benedí San Millán", "Angela Dai", "Matthias Nießner"], "title": "Animating the Uncaptured: Humanoid Mesh Animation with Video Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages, 10 figures", "summary": "Animation of humanoid characters is essential in various graphics\napplications, but requires significant time and cost to create realistic\nanimations. We propose an approach to synthesize 4D animated sequences of input\nstatic 3D humanoid meshes, leveraging strong generalized motion priors from\ngenerative video models -- as such video models contain powerful motion\ninformation covering a wide variety of human motions. From an input static 3D\nhumanoid mesh and a text prompt describing the desired animation, we synthesize\na corresponding video conditioned on a rendered image of the 3D mesh. We then\nemploy an underlying SMPL representation to animate the corresponding 3D mesh\naccording to the video-generated motion, based on our motion optimization. This\nenables a cost-effective and accessible solution to enable the synthesis of\ndiverse and realistic 4D animations."}
{"id": "2503.16013", "pdf": "https://arxiv.org/pdf/2503.16013", "abs": "https://arxiv.org/abs/2503.16013", "authors": ["Xiaomeng Chu", "Jiajun Deng", "Guoliang You", "Wei Liu", "Xingchen Li", "Jianmin Ji", "Yanyong Zhang"], "title": "GraspCoT: Integrating Physical Property Reasoning for 6-DoF Grasping under Flexible Language Instructions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Flexible instruction-guided 6-DoF grasping is a significant yet challenging\ntask for real-world robotic systems. Existing methods utilize the contextual\nunderstanding capabilities of the large language models (LLMs) to establish\nmappings between expressions and targets, allowing robots to comprehend users'\nintentions in the instructions. However, the LLM's knowledge about objects'\nphysical properties remains underexplored despite its tight relevance to\ngrasping. In this work, we propose GraspCoT, a 6-DoF grasp detection framework\nthat integrates a Chain-of-Thought (CoT) reasoning mechanism oriented to\nphysical properties, guided by auxiliary question-answering (QA) tasks.\nParticularly, we design a set of QA templates to enable hierarchical reasoning\nthat includes three stages: target parsing, physical property analysis, and\ngrasp action selection. Moreover, GraspCoT presents a unified multimodal LLM\narchitecture, which encodes multi-view observations of 3D scenes into 3D-aware\nvisual tokens, and then jointly embeds these visual tokens with CoT-derived\ntextual tokens within LLMs to generate grasp pose predictions. Furthermore, we\npresent IntentGrasp, a large-scale benchmark that fills the gap in public\ndatasets for multi-object grasp detection under diverse and indirect verbal\ncommands. Extensive experiments on IntentGrasp demonstrate the superiority of\nour method, with additional validation in real-world robotic applications\nconfirming its practicality. Codes and data will be released."}
{"id": "2503.16055", "pdf": "https://arxiv.org/pdf/2503.16055", "abs": "https://arxiv.org/abs/2503.16055", "authors": ["Abdelrahman Elsayed", "Sarim Hashmi", "Mohammed Elseiagy", "Hu Wang", "Mohammad Yaqub", "Ibrahim Almakky"], "title": "SALT: Singular Value Adaptation with Low-Rank Transformation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The complex nature of medical image segmentation calls for models that are\nspecifically designed to capture detailed, domain-specific features. Large\nfoundation models offer considerable flexibility, yet the cost of fine-tuning\nthese models remains a significant barrier. Parameter-Efficient Fine-Tuning\n(PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model\nweights with low-rank matrices but may suffer from underfitting when the chosen\nrank is insufficient to capture domain-specific nuances. Conversely, full-rank\nSingular Value Decomposition (SVD) based methods provide comprehensive updates\nby modifying all singular values, yet they often lack flexibility and exhibit\nvariable performance across datasets. We propose SALT (Singular Value\nAdaptation with Low-Rank Transformation), a method that selectively adapts the\nmost influential singular values using trainable scale and shift parameters\nwhile complementing this with a low-rank update for the remaining subspace.\nThis hybrid approach harnesses the advantages of both LoRA and SVD, enabling\neffective adaptation without relying on increasing model size or depth.\nEvaluated on 5 challenging medical datasets, ranging from as few as 20 samples\nto 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in\nDice with only 3.9% trainable parameters, demonstrating robust adaptation even\nin low-resource settings. The code for SALT is available at:\nhttps://github.com/BioMedIA-MBZUAI/SALT"}
{"id": "2503.16075", "pdf": "https://arxiv.org/pdf/2503.16075", "abs": "https://arxiv.org/abs/2503.16075", "authors": ["Marek Wodzinski", "Henning Müller"], "title": "3-D Image-to-Image Fusion in Lightsheet Microscopy by Two-Step Adversarial Network: Contribution to the FuseMyCells Challenge", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightsheet microscopy is a powerful 3-D imaging technique that addresses\nlimitations of traditional optical and confocal microscopy but suffers from a\nlow penetration depth and reduced image quality at greater depths. Multiview\nlightsheet microscopy improves 3-D resolution by combining multiple views but\nsimultaneously increasing the complexity and the photon budget, leading to\npotential photobleaching and phototoxicity. The FuseMyCells challenge,\norganized in conjunction with the IEEE ISBI 2025 conference, aims to benchmark\ndeep learning-based solutions for fusing high-quality 3-D volumes from single\n3-D views, potentially simplifying procedures and conserving the photon budget.\nIn this work, we propose a contribution to the FuseMyCells challenge based on a\ntwo-step procedure. The first step processes a downsampled version of the image\nto capture the entire region of interest, while the second step uses a\npatch-based approach for high-resolution inference, incorporating adversarial\nloss to enhance visual outcomes. This method addresses challenges related to\nhigh data resolution, the necessity of global context, and the preservation of\nhigh-frequency details. Experimental results demonstrate the effectiveness of\nour approach, highlighting its potential to improve 3-D image fusion quality\nand extend the capabilities of lightsheet microscopy. The average SSIM for the\nnucleus and membranes is greater than 0.85 and 0.91, respectively."}
{"id": "2503.16149", "pdf": "https://arxiv.org/pdf/2503.16149", "abs": "https://arxiv.org/abs/2503.16149", "authors": ["Dong Chen", "Boyue Zhao", "Yi Zhang", "Meng Zhao"], "title": "Selective Complementary Feature Fusion and Modal Feature Compression Interaction for Brain Tumor Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Efficient modal feature fusion strategy is the key to achieve accurate\nsegmentation of brain glioma. However, due to the specificity of different MRI\nmodes, it is difficult to carry out cross-modal fusion with large differences\nin modal features, resulting in the model ignoring rich feature information. On\nthe other hand, the problem of multi-modal feature redundancy interaction\noccurs in parallel networks due to the proliferation of feature dimensions,\nfurther increase the difficulty of multi-modal feature fusion at the bottom\nend. In order to solve the above problems, we propose a noval complementary\nfeature compression interaction network (CFCI-Net), which realizes the\ncomplementary fusion and compression interaction of multi-modal feature\ninformation with an efficient mode fusion strategy. Firstly, we propose a\nselective complementary feature fusion (SCFF) module, which adaptively fuses\nrich cross-modal feature information by complementary soft selection weights.\nSecondly, a modal feature compression interaction (MFCI) transformer is\nproposed to deal with the multi-mode fusion redundancy problem when the feature\ndimension surges. The MFCI transformer is composed of modal feature compression\n(MFC) and modal feature interaction (MFI) to realize redundancy feature\ncompression and multi-mode feature interactive learning. %In MFI, we propose a\nhierarchical interactive attention mechanism based on multi-head attention.\nEvaluations on the BraTS2019 and BraTS2020 datasets demonstrate that CFCI-Net\nachieves superior results compared to state-of-the-art models. Code:\nhttps://github.com/CDmm0/CFCI-Net"}
{"id": "2503.16177", "pdf": "https://arxiv.org/pdf/2503.16177", "abs": "https://arxiv.org/abs/2503.16177", "authors": ["Shiyong Liu", "Xiao Tang", "Zhihao Li", "Yingfan He", "Chongjie Ye", "Jianzhuang Liu", "Binxiao Huang", "Shunbo Zhou", "Xiaofei Wu"], "title": "OccluGaussian: Occlusion-Aware Gaussian Splatting for Large Scene Reconstruction and Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "Project website: https://occlugaussian.github.io", "summary": "In large-scale scene reconstruction using 3D Gaussian splatting, it is common\nto partition the scene into multiple smaller regions and reconstruct them\nindividually. However, existing division methods are occlusion-agnostic,\nmeaning that each region may contain areas with severe occlusions. As a result,\nthe cameras within those regions are less correlated, leading to a low average\ncontribution to the overall reconstruction. In this paper, we propose an\nocclusion-aware scene division strategy that clusters training cameras based on\ntheir positions and co-visibilities to acquire multiple regions. Cameras in\nsuch regions exhibit stronger correlations and a higher average contribution,\nfacilitating high-quality scene reconstruction. We further propose a\nregion-based rendering technique to accelerate large scene rendering, which\nculls Gaussians invisible to the region where the viewpoint is located. Such a\ntechnique significantly speeds up the rendering without compromising quality.\nExtensive experiments on multiple large scenes show that our method achieves\nsuperior reconstruction results with faster rendering speed compared to\nexisting state-of-the-art approaches. Project page:\nhttps://occlugaussian.github.io."}
{"id": "2503.16222", "pdf": "https://arxiv.org/pdf/2503.16222", "abs": "https://arxiv.org/abs/2503.16222", "authors": ["Teresa Klatzer", "Savvas Melidonis", "Marcelo Pereyra", "Konstantinos C. Zygalakis"], "title": "Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems", "categories": ["stat.CO", "cs.CV", "cs.NA", "math.NA", "stat.ML", "53B21, 60H35, 62F15, 65C40, 65C60, 65J22, 68U10"], "comment": "31 pages, 17 figures", "summary": "This paper introduces a novel plug-and-play (PnP) Langevin sampling\nmethodology for Bayesian inference in low-photon Poisson imaging problems, a\nchallenging class of problems with significant applications in astronomy,\nmedicine, and biology. PnP Langevin sampling algorithms offer a powerful\nframework for Bayesian image restoration, enabling accurate point estimation as\nwell as advanced inference tasks, including uncertainty quantification and\nvisualization analyses, and empirical Bayesian inference for automatic model\nparameter tuning. However, existing PnP Langevin algorithms are not well-suited\nfor low-photon Poisson imaging due to high solution uncertainty and poor\nregularity properties, such as exploding gradients and non-negativity\nconstraints. To address these challenges, we propose two strategies for\nextending Langevin PnP sampling to Poisson imaging models: (i) an accelerated\nPnP Langevin method that incorporates boundary reflections and a Poisson\nlikelihood approximation and (ii) a mirror sampling algorithm that leverages a\nRiemannian geometry to handle the constraints and the poor regularity of the\nlikelihood without approximations. The effectiveness of these approaches is\ndemonstrated through extensive numerical experiments and comparisons with\nstate-of-the-art methods."}
{"id": "2503.16251", "pdf": "https://arxiv.org/pdf/2503.16251", "abs": "https://arxiv.org/abs/2503.16251", "authors": ["Dawood Wasif", "Terrence J. Moore", "Jin-Hee Cho"], "title": "RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles", "categories": ["cs.LG", "cs.CV", "cs.DC", "cs.ET"], "comment": "Submitted to PETS 2025 (under review)", "summary": "Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to\nenhance perception models while preserving privacy. However, existing FL\nframeworks struggle to balance privacy, fairness, and robustness, leading to\nperformance disparities across demographic groups. Privacy-preserving\ntechniques like differential privacy mitigate data leakage risks but worsen\nfairness by restricting access to sensitive attributes needed for bias\ncorrection. This work explores the trade-off between privacy and fairness in\nFL-based object detection for AVs and introduces RESFL, an integrated solution\noptimizing both. RESFL incorporates adversarial privacy disentanglement and\nuncertainty-guided fairness-aware aggregation. The adversarial component uses a\ngradient reversal layer to remove sensitive attributes, reducing privacy risks\nwhile maintaining fairness. The uncertainty-aware aggregation employs an\nevidential neural network to weight client updates adaptively, prioritizing\ncontributions with lower fairness disparities and higher confidence. This\nensures robust and equitable FL model updates. We evaluate RESFL on the FACET\ndataset and CARLA simulator, assessing accuracy, fairness, privacy resilience,\nand robustness under varying conditions. RESFL improves detection accuracy,\nreduces fairness disparities, and lowers privacy attack success rates while\ndemonstrating superior robustness to adversarial conditions compared to other\napproaches."}
{"id": "2503.16264", "pdf": "https://arxiv.org/pdf/2503.16264", "abs": "https://arxiv.org/abs/2503.16264", "authors": ["Dounia Hammou", "Yancheng Cai", "Pavan Madhusudanarao", "Christos G. Bampis", "Rafał K. Mantiuk"], "title": "Do image and video quality metrics model low-level human vision?", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Image and video quality metrics, such as SSIM, LPIPS, and VMAF, are aimed to\npredict the perceived quality of the evaluated content and are often claimed to\nbe \"perceptual\". Yet, few metrics directly model human visual perception, and\nmost rely on hand-crafted formulas or training datasets to achieve alignment\nwith perceptual data. In this paper, we propose a set of tests for\nfull-reference quality metrics that examine their ability to model several\naspects of low-level human vision: contrast sensitivity, contrast masking, and\ncontrast matching. The tests are meant to provide additional scrutiny for newly\nproposed metrics. We use our tests to analyze 33 existing image and video\nquality metrics and find their strengths and weaknesses, such as the ability of\nLPIPS and MS-SSIM to predict contrast masking and poor performance of VMAF in\nthis task. We further find that the popular SSIM metric overemphasizes\ndifferences in high spatial frequencies, but its multi-scale counterpart,\nMS-SSIM, addresses this shortcoming. Such findings cannot be easily made using\nexisting evaluation protocols."}
{"id": "2503.16309", "pdf": "https://arxiv.org/pdf/2503.16309", "abs": "https://arxiv.org/abs/2503.16309", "authors": ["Vivek Gopalakrishnan", "Neel Dey", "David-Dimitris Chlorogiannis", "Andrew Abumoussa", "Anna M. Larson", "Darren B. Orbach", "Sarah Frisken", "Polina Golland"], "title": "Rapid patient-specific neural networks for intraoperative X-ray to volume registration", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": null, "summary": "The integration of artificial intelligence in image-guided interventions\nholds transformative potential, promising to extract 3D geometric and\nquantitative information from conventional 2D imaging modalities during complex\nprocedures. Achieving this requires the rapid and precise alignment of 2D\nintraoperative images (e.g., X-ray) with 3D preoperative volumes (e.g., CT,\nMRI). However, current 2D/3D registration methods fail across the broad\nspectrum of procedures dependent on X-ray guidance: traditional optimization\ntechniques require custom parameter tuning for each subject, whereas neural\nnetworks trained on small datasets do not generalize to new patients or require\nlabor-intensive manual annotations, increasing clinical burden and precluding\napplication to new anatomical targets. To address these challenges, we present\nxvr, a fully automated framework for training patient-specific neural networks\nfor 2D/3D registration. xvr uses physics-based simulation to generate abundant\nhigh-quality training data from a patient's own preoperative volumetric\nimaging, thereby overcoming the inherently limited ability of supervised models\nto generalize to new patients and procedures. Furthermore, xvr requires only 5\nminutes of training per patient, making it suitable for emergency interventions\nas well as planned procedures. We perform the largest evaluation of a 2D/3D\nregistration algorithm on real X-ray data to date and find that xvr robustly\ngeneralizes across a diverse dataset comprising multiple anatomical structures,\nimaging modalities, and hospitals. Across surgical tasks, xvr achieves\nsubmillimeter-accurate registration at intraoperative speeds, improving upon\nexisting methods by an order of magnitude. xvr is released as open-source\nsoftware freely available at https://github.com/eigenvivek/xvr."}
{"id": "2503.16356", "pdf": "https://arxiv.org/pdf/2503.16356", "abs": "https://arxiv.org/abs/2503.16356", "authors": ["Yunzhi Yao", "Jizhan Fang", "Jia-Chen Gu", "Ningyu Zhang", "Shumin Deng", "Huajun Chen", "Nanyun Peng"], "title": "CaKE: Circuit-aware Editing Enables Generalizable Knowledge Learners", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Knowledge Editing (KE) enables the modification of outdated or incorrect\ninformation in large language models (LLMs). While existing KE methods can\nupdate isolated facts, they struggle to generalize these updates to multi-hop\nreasoning tasks that depend on the modified knowledge. Through an analysis of\nreasoning circuits -- the neural pathways LLMs use for knowledge-based\ninference, we observe that current layer-localized KE approaches, such as MEMIT\nand WISE, which edit only single or a few model layers, struggle to effectively\nincorporate updated information into these reasoning pathways. To address this\nlimitation, we propose CaKE (Circuit-aware Knowledge Editing), a novel method\nthat enables more effective integration of updated knowledge in LLMs. CaKE\nleverages strategically curated data, guided by our circuits-based analysis,\nthat enforces the model to utilize the modified knowledge, stimulating the\nmodel to develop appropriate reasoning circuits for newly integrated knowledge.\nExperimental results show that CaKE enables more accurate and consistent use of\nupdated knowledge across related reasoning tasks, leading to an average of 20%\nimprovement in multi-hop reasoning accuracy on MQuAKE dataset compared to\nexisting KE methods. We release the code and data in\nhttps://github.com/zjunlp/CaKE."}
{"id": "2503.16389", "pdf": "https://arxiv.org/pdf/2503.16389", "abs": "https://arxiv.org/abs/2503.16389", "authors": ["Kristin Qi", "Xinhan Di"], "title": "Attentional Triple-Encoder Network in Spatiospectral Domains for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "IEEE Conference on Artificial Intelligence (IEEE CAI)", "summary": "Retinal Optical Coherence Tomography (OCT) segmentation is essential for\ndiagnosing pathology. Traditional methods focus on either spatial or spectral\ndomains, overlooking their combined dependencies. We propose a triple-encoder\nnetwork that integrates CNNs for spatial features, Fast Fourier Convolution\n(FFC) for spectral features, and attention mechanisms to capture global\nrelationships across both domains. Attention fusion modules integrate\nconvolution and cross-attention to further enhance features. Our method\nachieves an average Dice score improvement from 0.855 to 0.864, outperforming\nprior work."}
{"id": "2503.16406", "pdf": "https://arxiv.org/pdf/2503.16406", "abs": "https://arxiv.org/abs/2503.16406", "authors": ["SeungJu Cha", "Kwanyoung Lee", "Ye-Chan Kim", "Hyunwoo Oh", "Dong-Jin Kim"], "title": "VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Accepted at CVPR 2025, code :\n  https://github.com/SeungJuCha/VerbDiff.git", "summary": "Recent large-scale text-to-image diffusion models generate photorealistic\nimages but often struggle to accurately depict interactions between humans and\nobjects due to their limited ability to differentiate various interaction\nwords. In this work, we propose VerbDiff to address the challenge of capturing\nnuanced interactions within text-to-image diffusion models. VerbDiff is a novel\ntext-to-image generation model that weakens the bias between interaction words\nand objects, enhancing the understanding of interactions. Specifically, we\ndisentangle various interaction words from frequency-based anchor words and\nleverage localized interaction regions from generated images to help the model\nbetter capture semantics in distinctive words without extra conditions. Our\napproach enables the model to accurately understand the intended interaction\nbetween humans and objects, producing high-quality images with accurate\ninteractions aligned with specified verbs. Extensive experiments on the\nHICO-DET dataset demonstrate the effectiveness of our method compared to\nprevious approaches."}
{"id": "2503.16408", "pdf": "https://arxiv.org/pdf/2503.16408", "abs": "https://arxiv.org/abs/2503.16408", "authors": ["Yiran Qin", "Li Kang", "Xiufeng Song", "Zhenfei Yin", "Xiaohong Liu", "Xihui Liu", "Ruimao Zhang", "Lei Bai"], "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://iranqin.github.io/robofactory/", "summary": "Designing effective embodied multi-agent systems is critical for solving\ncomplex real-world tasks across domains. Due to the complexity of multi-agent\nembodied systems, existing methods fail to automatically generate safe and\nefficient training data for such systems. To this end, we propose the concept\nof compositional constraints for embodied multi-agent systems, addressing the\nchallenges arising from collaboration among embodied agents. We design various\ninterfaces tailored to different types of constraints, enabling seamless\ninteraction with the physical world. Leveraging compositional constraints and\nspecifically designed interfaces, we develop an automated data collection\nframework for embodied multi-agent systems and introduce the first benchmark\nfor embodied multi-agent manipulation, RoboFactory. Based on RoboFactory\nbenchmark, we adapt and evaluate the method of imitation learning and analyzed\nits performance in different difficulty agent tasks. Furthermore, we explore\nthe architectures and training strategies for multi-agent imitation learning,\naiming to build safe and efficient embodied multi-agent systems."}
{"id": "2503.16424", "pdf": "https://arxiv.org/pdf/2503.16424", "abs": "https://arxiv.org/abs/2503.16424", "authors": ["Xi Liu", "Chaoyi Zhou", "Nanxuan Zhao", "Siyu Huang"], "title": "Bézier Splatting for Fast and Differentiable Vector Graphics", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Differentiable vector graphics (VGs) are widely used in image vectorization\nand vector synthesis, while existing representations are costly to optimize and\nstruggle to achieve high-quality rendering results for high-resolution images.\nThis work introduces a new differentiable VG representation, dubbed B\\'ezier\nsplatting, that enables fast yet high-fidelity VG rasterization. B\\'ezier\nsplatting samples 2D Gaussians along B\\'ezier curves, which naturally provide\npositional gradients at object boundaries. Thanks to the efficient\nsplatting-based differentiable rasterizer, B\\'ezier splatting achieves over 20x\nand 150x faster per forward and backward rasterization step for open curves\ncompared to DiffVG. Additionally, we introduce an adaptive pruning and\ndensification strategy that dynamically adjusts the spatial distribution of\ncurves to escape local minima, further improving VG quality. Experimental\nresults show that B\\'ezier splatting significantly outperforms existing methods\nwith better visual fidelity and 10x faster optimization speed."}
{"id": "2503.16428", "pdf": "https://arxiv.org/pdf/2503.16428", "abs": "https://arxiv.org/abs/2503.16428", "authors": ["Ruyi Xu", "Guangxuan Xiao", "Haofeng Huang", "Junxian Guo", "Song Han"], "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring", "categories": ["cs.CL", "cs.CV"], "comment": "The first two authors contributed equally to this work", "summary": "Long-Context Transformer Models (LCTMs) are vital for real-world applications\nbut suffer high computational costs due to attention's quadratic complexity.\nBlock-sparse attention mitigates this by focusing computation on critical\nregions, yet existing methods struggle with balancing accuracy and efficiency\ndue to costly block importance measurements. In this paper, we introduce\nXAttention, a plug-and-play framework that dramatically accelerates\nlong-context inference in Transformers models using sparse attention.\nXAttention's key innovation is the insight that the sum of antidiagonal values\n(i.e., from the lower-left to upper-right) in the attention matrix provides a\npowerful proxy for block importance. This allows for precise identification and\npruning of non-essential blocks, resulting in high sparsity and dramatically\naccelerated inference. Across comprehensive evaluations on demanding\nlong-context benchmarks-including RULER and LongBench for language, VideoMME\nfor video understanding, and VBench for video generation. XAttention achieves\naccuracy comparable to full attention while delivering substantial\ncomputational gains. We demonstrate up to 13.5x acceleration in attention\ncomputation. These results underscore XAttention's ability to unlock the\npractical potential of block sparse attention, paving the way for scalable and\nefficient deployment of LCTMs in real-world applications. Code is available at\nhttps://github.com/mit-han-lab/x-attention."}
