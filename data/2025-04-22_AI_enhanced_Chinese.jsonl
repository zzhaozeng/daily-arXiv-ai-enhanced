{"id": "2504.13915", "pdf": "https://arxiv.org/pdf/2504.13915", "abs": "https://arxiv.org/abs/2504.13915", "authors": ["Dibyadip Chatterjee", "Edoardo Remelli", "Yale Song", "Bugra Tekin", "Abhay Mittal", "Bharat Bhatnagar", "Necati Cihan Camg\u00f6z", "Shreyas Hampali", "Eric Sauser", "Shugao Ma", "Angela Yao", "Fadime Sener"], "title": "Memory-efficient Streaming VideoLLMs for Real-time Procedural Video Understanding", "categories": ["cs.CV"], "comment": "13 pages, 5 figures; https://dibschat.github.io/ProVideLLM", "summary": "We introduce ProVideLLM, an end-to-end framework for real-time procedural\nvideo understanding. ProVideLLM integrates a multimodal cache configured to\nstore two types of tokens - verbalized text tokens, which provide compressed\ntextual summaries of long-term observations, and visual tokens, encoded with\nDETR-QFormer to capture fine-grained details from short-term observations. This\ndesign reduces token count by 22x over existing methods in representing one\nhour of long-term observations while effectively encoding fine-granularity of\nthe present. By interleaving these tokens in our multimodal cache, ProVideLLM\nensures sub-linear scaling of memory and compute with video length, enabling\nper-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with\na minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art\nresults on six procedural tasks across four datasets.", "AI": {"tldr": "ProVideLLM\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u7a0b\u5e8f\u5316\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7f13\u5b58\u548c\u9ad8\u6548\u4ee4\u724c\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u957f\u65f6\u89c2\u5bdf\u65f6\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0cProVideLLM\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u7f13\u5b58\u548c\u4ee4\u724c\u8bbe\u8ba1\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u96c6\u6210\u591a\u6a21\u6001\u7f13\u5b58\uff0c\u5b58\u50a8\u6587\u672c\u4ee4\u724c\u548c\u89c6\u89c9\u4ee4\u724c\uff0c\u4f7f\u7528DETR-QFormer\u7f16\u7801\u7ec6\u8282\uff0c\u5b9e\u73b0\u9ad8\u6548\u4ee4\u724c\u8868\u793a\u548c\u8ba1\u7b97\u3002", "result": "\u4ee4\u724c\u6570\u91cf\u51cf\u5c1122\u500d\uff0c\u652f\u630110 FPS\u7684\u9010\u5e27\u63a8\u7406\u548c25 FPS\u7684\u6d41\u5f0f\u5bf9\u8bdd\uff0cGPU\u5185\u5b58\u5360\u7528\u4ec52GB\uff0c\u5728\u516d\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "ProVideLLM\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u5b9e\u65f6\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7a0b\u5e8f\u5316\u4efb\u52a1\u3002"}}
{"id": "2504.13987", "pdf": "https://arxiv.org/pdf/2504.13987", "abs": "https://arxiv.org/abs/2504.13987", "authors": ["Tariq Berrada Ifriqi", "Adriana Romero-Soriano", "Michal Drozdzal", "Jakob Verbeek", "Karteek Alahari"], "title": "Entropy Rectifying Guidance for Diffusion and Flow Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Guidance techniques are commonly used in diffusion and flow models to improve\nimage quality and consistency for conditional generative tasks such as\nclass-conditional and text-to-image generation. In particular, classifier-free\nguidance (CFG) -- the most widely adopted guidance technique -- contrasts\nconditional and unconditional predictions to improve the generated images. This\nresults, however, in trade-offs across quality, diversity and consistency,\nimproving some at the expense of others. While recent work has shown that it is\npossible to disentangle these factors to some extent, such methods come with an\noverhead of requiring an additional (weaker) model, or require more forward\npasses per sampling step. In this paper, we propose Entropy Rectifying Guidance\n(ERG), a simple and effective guidance mechanism based on inference-time\nchanges in the attention mechanism of state-of-the-art diffusion transformer\narchitectures, which allows for simultaneous improvements over image quality,\ndiversity and prompt consistency. ERG is more general than CFG and similar\nguidance techniques, as it extends to unconditional sampling. ERG results in\nsignificant improvements in various generation tasks such as text-to-image,\nclass-conditional and unconditional image generation. We also show that ERG can\nbe seamlessly combined with other recent guidance methods such as CADS and APG,\nfurther boosting generation performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u71b5\u6821\u6b63\u5f15\u5bfc\uff08ERG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u63d0\u793a\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff08CFG\uff09\u65b9\u6cd5\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e14\u73b0\u6709\u6539\u8fdb\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6a21\u578b\u6216\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u71b5\u6821\u6b63\u5f15\u5bfc\uff08ERG\uff09\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u53d8\u5316\uff0c\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u751f\u6210\u8fc7\u7a0b\u3002", "result": "ERG\u5728\u6587\u672c\u5230\u56fe\u50cf\u3001\u7c7b\u6761\u4ef6\u751f\u6210\u548c\u65e0\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u4e0e\u5176\u4ed6\u5f15\u5bfc\u65b9\u6cd5\uff08\u5982CADS\u548cAPG\uff09\u7ed3\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "ERG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u589e\u52a0\u989d\u5916\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u751f\u6210\u4efb\u52a1\u7684\u6548\u679c\u3002"}}
{"id": "2504.13995", "pdf": "https://arxiv.org/pdf/2504.13995", "abs": "https://arxiv.org/abs/2504.13995", "authors": ["Andrea Amaduzzi", "Pierluigi Zama Ramirez", "Giuseppe Lisanti", "Samuele Salti", "Luigi Di Stefano"], "title": "Scaling LLaNA: Advancing NeRF-Language Understanding Through Large-Scale Training", "categories": ["cs.CV"], "comment": "Under submission. Project page at\n  https://andreamaduzzi.github.io/llana/", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\nremarkable capabilities in understanding both images and 3D data, yet these\nmodalities face inherent limitations in comprehensively representing object\ngeometry and appearance. Neural Radiance Fields (NeRFs) have emerged as a\npromising alternative, encoding both geometric and photorealistic properties\nwithin the weights of a simple Multi-Layer Perceptron (MLP). This work\ninvestigates the feasibility and effectiveness of ingesting NeRFs into an MLLM.\nWe introduce LLaNA, the first MLLM able to perform new tasks such as NeRF\ncaptioning and Q\\&A, by directly processing the weights of a NeRF's MLP.\nNotably, LLaNA is able to extract information about the represented objects\nwithout the need to render images or materialize 3D data structures. In\naddition, we build the first large-scale NeRF-language dataset, composed by\nmore than 300K NeRFs trained on ShapeNet and Objaverse, with paired textual\nannotations that enable various NeRF-language tasks. Based on this dataset, we\ndevelop a benchmark to evaluate the NeRF understanding capability of our\nmethod. Results show that directly processing NeRF weights leads to better\nperformance on NeRF-Language tasks compared to approaches that rely on either\n2D or 3D representations derived from NeRFs.", "AI": {"tldr": "LLaNA\u662f\u9996\u4e2a\u80fd\u591f\u76f4\u63a5\u5904\u7406NeRF\u6743\u91cd\u5e76\u6267\u884cNeRF\u63cf\u8ff0\u548c\u95ee\u7b54\u4efb\u52a1\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u65e0\u9700\u6e32\u67d3\u56fe\u50cf\u6216\u751f\u62103D\u6570\u636e\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u7406\u89e3\u56fe\u50cf\u548c3D\u6570\u636e\u65f6\u5b58\u5728\u51e0\u4f55\u548c\u5916\u89c2\u8868\u5f81\u7684\u5c40\u9650\u6027\uff0c\u800cNeRF\u4f5c\u4e3a\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u7f16\u7801\u51e0\u4f55\u548c\u771f\u5b9e\u611f\u5c5e\u6027\u3002", "method": "\u63d0\u51faLLaNA\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u7406NeRF\u7684MLP\u6743\u91cd\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b30\u4e07NeRF\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u76f4\u63a5\u5904\u7406NeRF\u6743\u91cd\u7684LLaNA\u5728NeRF-\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f9d\u8d562D\u62163D\u8868\u5f81\u7684\u65b9\u6cd5\u3002", "conclusion": "LLaNA\u5c55\u793a\u4e86\u76f4\u63a5\u5904\u7406NeRF\u6743\u91cd\u7684\u53ef\u884c\u6027\uff0c\u4e3aMLLM\u5728NeRF\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.14011", "pdf": "https://arxiv.org/pdf/2504.14011", "abs": "https://arxiv.org/abs/2504.14011", "authors": ["Fulvio Sanguigni", "Davide Morelli", "Marcella Cornia", "Rita Cucchiara"], "title": "Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "IJCNN 2025", "summary": "In recent years, the fashion industry has increasingly adopted AI\ntechnologies to enhance customer experience, driven by the proliferation of\ne-commerce platforms and virtual applications. Among the various tasks, virtual\ntry-on and multimodal fashion image editing -- which utilizes diverse input\nmodalities such as text, garment sketches, and body poses -- have become a key\narea of research. Diffusion models have emerged as a leading approach for such\ngenerative tasks, offering superior image quality and diversity. However, most\nexisting virtual try-on methods rely on having a specific garment input, which\nis often impractical in real-world scenarios where users may only provide\ntextual specifications. To address this limitation, in this work we introduce\nFashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that\nenables the customization of fashion items based on user preferences provided\nin textual form. Our approach retrieves multiple garments that match the input\nspecifications and generates a personalized image by incorporating attributes\nfrom the retrieved items. To achieve this, we employ textual inversion\ntechniques, where retrieved garment images are projected into the textual\nembedding space of the Stable Diffusion text encoder, allowing seamless\nintegration of retrieved elements into the generative process. Experimental\nresults on the Dress Code dataset demonstrate that Fashion-RAG outperforms\nexisting methods both qualitatively and quantitatively, effectively capturing\nfine-grained visual details from retrieved garments. To the best of our\nknowledge, this is the first work to introduce a retrieval-augmented generation\napproach specifically tailored for multimodal fashion image editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFashion-RAG\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u8f93\u5165\u5b9e\u73b0\u65f6\u5c1a\u7269\u54c1\u7684\u5b9a\u5236\u5316\u751f\u6210\uff0c\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5177\u4f53\u7684\u670d\u88c5\u8f93\u5165\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u7528\u6237\u53ef\u80fd\u4ec5\u63d0\u4f9b\u6587\u672c\u63cf\u8ff0\u3002Fashion-RAG\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u53cd\u8f6c\u6280\u672f\u5c06\u68c0\u7d22\u5230\u7684\u670d\u88c5\u56fe\u50cf\u6295\u5f71\u5230Stable Diffusion\u7684\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5728Dress Code\u6570\u636e\u96c6\u4e0a\uff0cFashion-RAG\u5728\u8d28\u91cf\u548c\u6570\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6355\u6349\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u3002", "conclusion": "Fashion-RAG\u662f\u9996\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u65f6\u5c1a\u56fe\u50cf\u7f16\u8f91\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u4e3a\u865a\u62df\u8bd5\u7a7f\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14216", "pdf": "https://arxiv.org/pdf/2504.14216", "abs": "https://arxiv.org/abs/2504.14216", "authors": ["Pierre-Alain Fayolle", "Evgenii Maltsev"], "title": "PyFRep: Shape Modeling with Differentiable Function Representation", "categories": ["cs.GR"], "comment": "21 pages, 10 figures. Code available at\n  https://github.com/fayolle/PyFRep", "summary": "We propose a framework for performing differentiable geometric modeling based\non the Function Representation (FRep). The framework is built on top of modern\nlibraries for performing automatic differentiation allowing us to obtain\nderivatives w.r.t. space or shape parameters. We demonstrate possible\napplications of this framework: Curvature estimation for shape interrogation,\nsigned distance function computation and approximation and fitting shape\nparameters of a parametric model to data. Our framework is released as\nopen-source.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u8868\u793a\uff08FRep\uff09\u7684\u53ef\u5fae\u5206\u51e0\u4f55\u5efa\u6a21\u6846\u67b6\uff0c\u652f\u6301\u81ea\u52a8\u5fae\u5206\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u66f2\u7387\u4f30\u8ba1\u3001\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u8ba1\u7b97\u548c\u5f62\u72b6\u53c2\u6570\u62df\u5408\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5728\u51e0\u4f55\u5efa\u6a21\u4e2d\u5b9e\u73b0\u53ef\u5fae\u5206\u6027\uff0c\u4fbf\u4e8e\u83b7\u53d6\u7a7a\u95f4\u6216\u5f62\u72b6\u53c2\u6570\u7684\u5bfc\u6570\uff0c\u4ece\u800c\u652f\u6301\u66f4\u9ad8\u6548\u7684\u5f62\u72b6\u5206\u6790\u548c\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u73b0\u4ee3\u81ea\u52a8\u5fae\u5206\u5e93\u6784\u5efa\u6846\u67b6\uff0c\u5229\u7528FRep\u8868\u793a\u51e0\u4f55\u5f62\u72b6\uff0c\u5b9e\u73b0\u7a7a\u95f4\u548c\u5f62\u72b6\u53c2\u6570\u7684\u53ef\u5fae\u5206\u8ba1\u7b97\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u66f2\u7387\u4f30\u8ba1\u3001\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u8ba1\u7b97\u548c\u5f62\u72b6\u53c2\u6570\u62df\u5408\uff0c\u6846\u67b6\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u51e0\u4f55\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u5fae\u5206\u7684\u5de5\u5177\uff0c\u652f\u6301\u591a\u79cd\u5e94\u7528\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2504.13914", "pdf": "https://arxiv.org/pdf/2504.13914", "abs": "https://arxiv.org/abs/2504.13914", "authors": ["ByteDance Seed", ":", "Yufeng Yuan", "Yu Yue", "Mingxuan Wang", "Xiaochen Zuo", "Jiaze Chen", "Lin Yan", "Wenyuan Xu", "Chi Zhang", "Xin Liu", "Chengyi Wang", "TianTian Fan", "Lingjun Liu", "Qiying Yu", "Xiangpeng Wei", "Zhiqi Lin", "Ruofei Zhu", "Qingping Yang", "Chengzhi Wei", "Jerry He", "Guanlin Liu", "Zheng Wu", "Xiangyu Yu", "Zhicheng Liu", "Jingjing Xu", "Jiangjie Chen", "Haojie Pan", "Shengding Hu", "Zhengyin Du", "Wenqi Wang", "Zewei Sun", "Chenwei Lou", "Bole Ma", "Zihan Wang", "Mofan Zhang", "Wang Zhang", "Gaohong Liu", "Kaihua Jiang", "Haibin Lin", "Ru Zhang", "Juncai Liu", "Li Han", "Jinxin Chi", "Wenqiang Zhang", "Jiayi Xu", "Jun Yuan", "Zhen Xiao", "Yuqiao Xian", "Jingqiao Wu", "Kai Hua", "Na Zhou", "Jianhui Duan", "Heyang Lu", "Changbao Wang", "Jinxiang Ou", "Shihang Wang", "Xiaoran Jin", "Xuesong Yao", "Chengyin Xu", "Wenchang Ma", "Zhecheng An", "Renming Pang", "Xia Xiao", "Jing Su", "Yuyu Zhang", "Tao Sun", "Kaibo Liu", "Yifan Sun", "Kai Shen", "Sijun Zhang", "Yiyuan Ma", "Xingyan Bin", "Ji Li", "Yao Luo", "Deyi Liu", "Shiyi Zhan", "Yunshui Li", "Yuan Yang", "Defa Zhu", "Ke Shen", "Chenggang Li", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "We introduce Seed-Thinking-v1.5, capable of reasoning through thinking before\nresponding, resulting in improved performance on a wide range of benchmarks.\nSeed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on\nGPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond\nreasoning tasks, the method demonstrates notable generalization across diverse\ndomains. For instance, it surpasses DeepSeek R1 by 8% in win rate on\nnon-reasoning tasks, indicating its broader applicability. Compared to other\nstate-of-the-art reasoning models, Seed-Thinking-v1.5 is a Mixture-of-Experts\n(MoE) model with a relatively small size, featuring 20B activated and 200B\ntotal parameters. As part of our effort to assess generalized reasoning, we\ndevelop two internal benchmarks, BeyondAIME and Codeforces, both of which will\nbe publicly released to support future research.", "AI": {"tldr": "Seed-Thinking-v1.5\u662f\u4e00\u79cd\u5177\u5907\u5148\u601d\u8003\u540e\u56de\u7b54\u80fd\u529b\u7684\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728STEM\u548c\u7f16\u7a0b\u9886\u57df\u3002", "motivation": "\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u6fc0\u6d3b\u53c2\u657020B\uff0c\u603b\u53c2\u6570200B\u3002", "result": "\u5728AIME 2024\u3001Codeforces\u548cGPQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u975e\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8aDeepSeek R1 8%\u3002", "conclusion": "Seed-Thinking-v1.5\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u5185\u90e8\u57fa\u51c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.14032", "pdf": "https://arxiv.org/pdf/2504.14032", "abs": "https://arxiv.org/abs/2504.14032", "authors": ["Haiwen Huang", "Anpei Chen", "Volodymyr Havrylov", "Andreas Geiger", "Dan Zhang"], "title": "LoftUp: Learning a Coordinate-Based Feature Upsampler for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Vision foundation models (VFMs) such as DINOv2 and CLIP have achieved\nimpressive results on various downstream tasks, but their limited feature\nresolution hampers performance in applications requiring pixel-level\nunderstanding. Feature upsampling offers a promising direction to address this\nchallenge. In this work, we identify two critical factors for enhancing feature\nupsampling: the upsampler architecture and the training objective. For the\nupsampler architecture, we introduce a coordinate-based cross-attention\ntransformer that integrates the high-resolution images with coordinates and\nlow-resolution VFM features to generate sharp, high-quality features. For the\ntraining objective, we propose constructing high-resolution pseudo-groundtruth\nfeatures by leveraging class-agnostic masks and self-distillation. Our approach\neffectively captures fine-grained details and adapts flexibly to various input\nand feature resolutions. Through experiments, we demonstrate that our approach\nsignificantly outperforms existing feature upsampling techniques across various\ndownstream tasks. Our code is released at https://github.com/andrehuang/loftup.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5750\u6807\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u6362\u5668\u548c\u81ea\u84b8\u998f\u8bad\u7ec3\u76ee\u6807\u7684\u7279\u5f81\u4e0a\u91c7\u6837\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u7ea7\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982DINOv2\u548cCLIP\uff09\u5728\u50cf\u7d20\u7ea7\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u7279\u5f81\u5206\u8fa8\u7387\u4e0d\u8db3\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u5f15\u5165\u5750\u6807\u4ea4\u53c9\u6ce8\u610f\u529b\u53d8\u6362\u5668\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001\u5750\u6807\u548c\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\uff1b\u63d0\u51fa\u5229\u7528\u7c7b\u65e0\u5173\u63a9\u7801\u548c\u81ea\u84b8\u998f\u6784\u5efa\u9ad8\u5206\u8fa8\u7387\u4f2a\u771f\u503c\u7279\u5f81\u7684\u8bad\u7ec3\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7279\u5f81\u4e0a\u91c7\u6837\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u7ec6\u8282\u5e76\u9002\u5e94\u4e0d\u540c\u5206\u8fa8\u7387\u8f93\u5165\uff0c\u4e3a\u50cf\u7d20\u7ea7\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14219", "pdf": "https://arxiv.org/pdf/2504.14219", "abs": "https://arxiv.org/abs/2504.14219", "authors": ["Alara Dirik", "Tuanfeng Wang", "Duygu Ceylan", "Stefanos Zafeiriou", "Anna Fr\u00fchst\u00fcck"], "title": "PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PRISM, a unified framework that enables multiple image generation\nand editing tasks in a single foundational model. Starting from a pre-trained\ntext-to-image diffusion model, PRISM proposes an effective fine-tuning strategy\nto produce RGB images along with intrinsic maps (referred to as X layers)\nsimultaneously. Unlike previous approaches, which infer intrinsic properties\nindividually or require separate models for decomposition and conditional\ngeneration, PRISM maintains consistency across modalities by generating all\nintrinsic layers jointly. It supports diverse tasks, including text-to-RGBX\ngeneration, RGB-to-X decomposition, and X-to-RGBX conditional generation.\nAdditionally, PRISM enables both global and local image editing through\nconditioning on selected intrinsic layers and text prompts. Extensive\nexperiments demonstrate the competitive performance of PRISM both for intrinsic\nimage decomposition and conditional image generation while preserving the base\nmodel's text-to-image generation capability.", "AI": {"tldr": "PRISM\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7\u8054\u5408\u751f\u6210RGB\u56fe\u50cf\u548c\u5185\u5728\u5c42\uff08X\u5c42\uff09\u5b9e\u73b0\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5355\u72ec\u63a8\u65ad\u5185\u5728\u5c5e\u6027\u6216\u4f9d\u8d56\u591a\u4e2a\u6a21\u578b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u540c\u65f6\u751f\u6210RGB\u56fe\u50cf\u548cX\u5c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePRISM\u5728\u5185\u5728\u56fe\u50cf\u5206\u89e3\u548c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "conclusion": "PRISM\u4e3a\u591a\u4efb\u52a1\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14037", "pdf": "https://arxiv.org/pdf/2504.14037", "abs": "https://arxiv.org/abs/2504.14037", "authors": ["Djamila Mohdeb", "Meriem Laifa", "Zineb Guemraoui", "Dalila Behih"], "title": "Uncovering Conspiratorial Narratives within Arabic Online Content", "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "This study investigates the spread of conspiracy theories in Arabic digital\nspaces through computational analysis of online content. By combining Named\nEntity Recognition and Topic Modeling techniques, specifically the Top2Vec\nalgorithm, we analyze data from Arabic blogs and Facebook to identify and\nclassify conspiratorial narratives. Our analysis uncovers six distinct\ncategories: gender/feminist, geopolitical, government cover-ups, apocalyptic,\nJudeo-Masonic, and geoengineering. The research highlights how these narratives\nare deeply embedded in Arabic social media discourse, shaped by regional\nhistorical, cultural, and sociopolitical contexts. By applying advanced Natural\nLanguage Processing methods to Arabic content, this study addresses a gap in\nconspiracy theory research, which has traditionally focused on English-language\ncontent or offline data. The findings provide new insights into the\nmanifestation and evolution of conspiracy theories in Arabic digital spaces,\nenhancing our understanding of their role in shaping public discourse in the\nArab world.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u5206\u6790\u963f\u62c9\u4f2f\u8bed\u5728\u7ebf\u5185\u5bb9\uff0c\u7ed3\u5408\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u4e3b\u9898\u5efa\u6a21\uff08Top2Vec\u7b97\u6cd5\uff09\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u4e86\u963f\u62c9\u4f2f\u8bed\u535a\u5ba2\u548cFacebook\u4e2d\u7684\u9634\u8c0b\u8bba\u53d9\u4e8b\uff0c\u63ed\u793a\u4e86\u516d\u79cd\u7c7b\u578b\u3002", "motivation": "\u586b\u8865\u9634\u8c0b\u8bba\u7814\u7a76\u4e2d\u963f\u62c9\u4f2f\u8bed\u5185\u5bb9\u6216\u5728\u7ebf\u6570\u636e\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e2d\u9634\u8c0b\u8bba\u7684\u5d4c\u5165\u65b9\u5f0f\u53ca\u5176\u80cc\u666f\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548cTop2Vec\u7b97\u6cd5\u5bf9\u963f\u62c9\u4f2f\u8bed\u535a\u5ba2\u548cFacebook\u6570\u636e\u8fdb\u884c\u8ba1\u7b97\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u516d\u7c7b\u9634\u8c0b\u8bba\u53d9\u4e8b\uff1a\u6027\u522b/\u5973\u6743\u4e3b\u4e49\u3001\u5730\u7f18\u653f\u6cbb\u3001\u653f\u5e9c\u63a9\u76d6\u3001\u672b\u65e5\u8bba\u3001\u72b9\u592a\u5171\u6d4e\u4f1a\u548c\u5730\u7403\u5de5\u7a0b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u963f\u62c9\u4f2f\u6570\u5b57\u7a7a\u95f4\u4e2d\u9634\u8c0b\u8bba\u7684\u8868\u73b0\u4e0e\u6f14\u53d8\uff0c\u589e\u8fdb\u4e86\u5bf9\u5176\u5728\u963f\u62c9\u4f2f\u4e16\u754c\u516c\u5171\u8bdd\u8bed\u4e2d\u4f5c\u7528\u7684\u7406\u89e3\u3002"}}
{"id": "2504.14054", "pdf": "https://arxiv.org/pdf/2504.14054", "abs": "https://arxiv.org/abs/2504.14054", "authors": ["Soroosh Baselizadeh", "Cheuk-To Yu", "Olga Veksler", "Yuri Boykov"], "title": "Occlusion-Ordered Semantic Instance Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Standard semantic instance segmentation provides useful, but inherently 2D\ninformation from a single image. To enable 3D analysis, one usually integrates\nabsolute monocular depth estimation with instance segmentation. However,\nmonocular depth is a difficult task. Instead, we leverage a simpler\nsingle-image task, occlusion-based relative depth ordering, providing coarser\nbut useful 3D information. We show that relative depth ordering works more\nreliably from occlusions than from absolute depth. We propose to solve the\njoint task of relative depth ordering and segmentation of instances based on\nocclusions. We call this task Occlusion-Ordered Semantic Instance Segmentation\n(OOSIS). We develop an approach to OOSIS that extracts instances and their\nocclusion order simultaneously from oriented occlusion boundaries and semantic\nsegmentation. Unlike popular detect-and-segment framework for instance\nsegmentation, combining occlusion ordering with instance segmentation allows a\nsimple and clean formulation of OOSIS as a labeling problem. As a part of our\nsolution for OOSIS, we develop a novel oriented occlusion boundaries approach\nthat significantly outperforms prior work. We also develop a new joint OOSIS\nmetric based both on instance mask accuracy and correctness of their occlusion\norder. We achieve better performance than strong baselines on KINS and COCOA\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOOSIS\u7684\u4efb\u52a1\uff0c\u7ed3\u5408\u76f8\u5bf9\u6df1\u5ea6\u6392\u5e8f\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u5229\u7528\u906e\u6321\u4fe1\u606f\u63d0\u4f9b3D\u5206\u6790\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u76842D\u5b9e\u4f8b\u5206\u5272\u7f3a\u4e4f3D\u4fe1\u606f\uff0c\u800c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u96be\u5ea6\u8f83\u5927\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u5229\u7528\u906e\u6321\u4fe1\u606f\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u76f8\u5bf9\u6df1\u5ea6\u6392\u5e8f\uff0c\u5e76\u7ed3\u5408\u5b9e\u4f8b\u5206\u5272\u3002", "method": "\u63d0\u51faOOSIS\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9a\u5411\u906e\u6321\u8fb9\u754c\u548c\u8bed\u4e49\u5206\u5272\u540c\u65f6\u63d0\u53d6\u5b9e\u4f8b\u53ca\u5176\u906e\u6321\u987a\u5e8f\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u6807\u7b7e\u95ee\u9898\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5b9a\u5411\u906e\u6321\u8fb9\u754c\u65b9\u6cd5\u3002", "result": "\u5728KINS\u548cCOCOA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9a\u5411\u906e\u6321\u8fb9\u754c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "OOSIS\u901a\u8fc7\u7ed3\u5408\u906e\u6321\u987a\u5e8f\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u76843D\u4fe1\u606f\u63d0\u53d6\u65b9\u6cd5\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2504.14257", "pdf": "https://arxiv.org/pdf/2504.14257", "abs": "https://arxiv.org/abs/2504.14257", "authors": ["Yilin Liu", "Duoteng Xu", "Xingyao Yu", "Xiang Xu", "Daniel Cohen-Or", "Hao Zhang", "Hui Huang"], "title": "HoLa: B-Rep Generation using a Holistic Latent Representation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We introduce a novel representation for learning and generating\nComputer-Aided Design (CAD) models in the form of $\\textit{boundary\nrepresentations}$ (B-Reps). Our representation unifies the continuous geometric\nproperties of B-Rep primitives in different orders (e.g., surfaces and curves)\nand their discrete topological relations in a $\\textit{holistic latent}$ (HoLa)\nspace. This is based on the simple observation that the topological connection\nbetween two surfaces is intrinsically tied to the geometry of their\nintersecting curve. Such a prior allows us to reformulate topology learning in\nB-Reps as a geometric reconstruction problem in Euclidean space. Specifically,\nwe eliminate the presence of curves, vertices, and all the topological\nconnections in the latent space by learning to distinguish and derive curve\ngeometries from a pair of surface primitives via a neural intersection network.\nTo this end, our holistic latent space is only defined on surfaces but encodes\na full B-Rep model, including the geometry of surfaces, curves, vertices, and\ntheir topological relations. Our compact and holistic latent space facilitates\nthe design of a first diffusion-based generator to take on a large variety of\ninputs including point clouds, single/multi-view images, 2D sketches, and text\nprompts. Our method significantly reduces ambiguities, redundancies, and\nincoherences among the generated B-Rep primitives, as well as training\ncomplexities inherent in prior multi-step B-Rep learning pipelines, while\nachieving greatly improved validity rate over current state of the art: 82% vs.\n$\\approx$50%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684CAD\u6a21\u578b\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u4e00\u7684HoLa\u7a7a\u95f4\u6574\u5408\u51e0\u4f55\u4e0e\u62d3\u6251\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfCAD\u6a21\u578b\u8868\u793a\u65b9\u6cd5\u5728\u51e0\u4f55\u4e0e\u62d3\u6251\u5173\u7cfb\u7684\u7edf\u4e00\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u751f\u6210\u6a21\u578b\u65f6\u5b58\u5728\u5197\u4f59\u548c\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u4ea4\u53c9\u7f51\u7edc\u5b66\u4e60\u66f2\u9762\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\uff0c\u5c06\u62d3\u6251\u5b66\u4e60\u8f6c\u5316\u4e3a\u51e0\u4f55\u91cd\u6784\u95ee\u9898\uff0c\u6784\u5efa\u4ec5\u57fa\u4e8e\u66f2\u9762\u7684\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u751f\u6210\u6a21\u578b\u7684\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u81f382%\uff0c\u8fdc\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ea650%\u3002", "conclusion": "HoLa\u7a7a\u95f4\u4e3aCAD\u6a21\u578b\u7684\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u548c\u8bad\u7ec3\u590d\u6742\u6027\u3002"}}
{"id": "2504.14039", "pdf": "https://arxiv.org/pdf/2504.14039", "abs": "https://arxiv.org/abs/2504.14039", "authors": ["Jaime Raldua Veuthey", "Zainab Ali Majid", "Suhas Hariharan", "Jacob Haimes"], "title": "MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) advance, their potential for widespread\nsocietal impact grows simultaneously. Hence, rigorous LLM evaluations are both\na technical necessity and social imperative. While numerous evaluation\nbenchmarks have been developed, there remains a critical gap in\nmeta-evaluation: effectively assessing benchmarks' quality. We propose MEQA, a\nframework for the meta-evaluation of question and answer (QA) benchmarks, to\nprovide standardized assessments, quantifiable scores, and enable meaningful\nintra-benchmark comparisons. We demonstrate this approach on cybersecurity\nbenchmarks, using human and LLM evaluators, highlighting the benchmarks'\nstrengths and weaknesses. We motivate our choice of test domain by AI models'\ndual nature as powerful defensive tools and security threats.", "AI": {"tldr": "MEQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u95ee\u7b54\uff08QA\uff09\u57fa\u51c6\u8d28\u91cf\u7684\u5143\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u91cf\u5316\u5206\u6570\uff0c\u5e76\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\uff0c\u5176\u793e\u4f1a\u5f71\u54cd\u65e5\u76ca\u663e\u8457\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u7684\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u73b0\u6709\u57fa\u51c6\u7684\u8d28\u91cf\u8bc4\u4f30\u5b58\u5728\u7a7a\u767d\uff0cMEQA\u586b\u8865\u4e86\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u63d0\u51faMEQA\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u91cf\u5316\u5206\u6570\u5bf9QA\u57fa\u51c6\u8fdb\u884c\u5143\u8bc4\u4f30\uff0c\u5e76\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7ed3\u5408\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u8005\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "MEQA\u6210\u529f\u8bc6\u522b\u4e86\u7f51\u7edc\u5b89\u5168\u57fa\u51c6\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MEQA\u4e3aQA\u57fa\u51c6\u7684\u5143\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5c24\u5176\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86AI\u6a21\u578b\u7684\u53cc\u91cd\u6027\uff08\u9632\u5fa1\u5de5\u5177\u4e0e\u5b89\u5168\u5a01\u80c1\uff09\u3002"}}
{"id": "2504.14075", "pdf": "https://arxiv.org/pdf/2504.14075", "abs": "https://arxiv.org/abs/2504.14075", "authors": ["Wei Dong", "Yan Min", "Han Zhou", "Jun Chen"], "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 NTIRE Workshop, Structure prior,\n  CNN-Transformer, LLIE", "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on\neither direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from\nsemantic features or illumination maps. Nonetheless, the intrinsic\nill-posedness of LLIE and the difficulty in retrieving robust semantics from\nheavily corrupted images hinder their effectiveness in extremely low-light\nenvironments. To tackle this challenge, we present SG-LLIE, a new multi-scale\nCNN-Transformer hybrid framework guided by structure priors. Different from\nemploying pre-trained models for the extraction of semantics or illumination\nmaps, we choose to extract robust structure priors based on\nillumination-invariant edge detectors. Moreover, we develop a CNN-Transformer\nHybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in\nthe UNet encoder-decoder architecture. Besides the CNN blocks which excels in\nmulti-scale feature extraction and fusion, we introduce a Structure-Guided\nTransformer Block (SGTB) in each HSGFE that incorporates structural priors to\nmodulate the enhancement process. Extensive experiments show that our method\nachieves state-of-the-art performance on several LLIE benchmarks in both\nquantitative metrics and visual quality. Our solution ranks second in the NTIRE\n2025 Low-Light Enhancement Challenge. Code is released at\nhttps://github.com/minyan8/imagine.", "AI": {"tldr": "SG-LLIE\u662f\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5148\u9a8c\u7684\u591a\u5c3a\u5ea6CNN-Transformer\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u63d0\u53d6\u5149\u7167\u4e0d\u53d8\u8fb9\u7f18\u68c0\u6d4b\u5668\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u7ed3\u5408CNN\u548cTransformer\u6a21\u5757\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u6781\u7aef\u4f4e\u5149\u73af\u5883\u4e0b\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u95ee\u9898\u7684\u75c5\u6001\u6027\u548c\u4ece\u4e25\u91cd\u635f\u574f\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e49\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51faSG-LLIE\u6846\u67b6\uff0c\u5229\u7528\u5149\u7167\u4e0d\u53d8\u8fb9\u7f18\u68c0\u6d4b\u5668\u63d0\u53d6\u7ed3\u6784\u5148\u9a8c\uff0c\u7ed3\u5408CNN-Transformer\u6df7\u5408\u6a21\u5757\uff08HSGFE\uff09\u548c\u591a\u5c3a\u5ea6UNet\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728NTIRE 2025\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "SG-LLIE\u901a\u8fc7\u7ed3\u6784\u5148\u9a8c\u548c\u591a\u5c3a\u5ea6\u6df7\u5408\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2504.14272", "pdf": "https://arxiv.org/pdf/2504.14272", "abs": "https://arxiv.org/abs/2504.14272", "authors": ["Yunha Yeo", "Daeho Um"], "title": "Can AI Recognize the Style of Art? Analyzing Aesthetics through the Lens of Style Transfer", "categories": ["cs.GR"], "comment": "Accepted to ISEA 2025", "summary": "This study investigates how artificial intelligence (AI) recognizes style\nthrough style transfer-an AI technique that generates a new image by applying\nthe style of one image to another. Despite the considerable interest that style\ntransfer has garnered among researchers, most efforts have focused on enhancing\nthe quality of output images through advanced AI algorithms. In this paper, we\napproach style transfer from an aesthetic perspective, thereby bridging AI\ntechniques and aesthetics. We analyze two style transfer algorithms: one based\non convolutional neural networks (CNNs) and the other utilizing recent\nTransformer models. By comparing the images produced by each, we explore the\nelements that constitute the style of artworks through an aesthetic analysis of\nthe style transfer results. We then elucidate the limitations of current style\ntransfer techniques. Based on these limitations, we propose potential\ndirections for future research on style transfer techniques.", "AI": {"tldr": "\u7814\u7a76\u4ece\u7f8e\u5b66\u89d2\u5ea6\u5206\u6790AI\u98ce\u683c\u8fc1\u79fb\u6280\u672f\uff0c\u6bd4\u8f83CNN\u548cTransformer\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u7f8e\u5b66\u6548\u679c\uff0c\u5e76\u63a2\u8ba8\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u591a\u6570\u7814\u7a76\u96c6\u4e2d\u4e8e\u7b97\u6cd5\u4f18\u5316\uff0c\u7f3a\u4e4f\u7f8e\u5b66\u89c6\u89d2\u7684\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5206\u6790\u57fa\u4e8eCNN\u548cTransformer\u7684\u98ce\u683c\u8fc1\u79fb\u7b97\u6cd5\uff0c\u901a\u8fc7\u7f8e\u5b66\u8bc4\u4f30\u6bd4\u8f83\u751f\u6210\u56fe\u50cf\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u6784\u6210\u827a\u672f\u4f5c\u54c1\u98ce\u683c\u7684\u5173\u952e\u5143\u7d20\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u7814\u7a76\u7684\u65b9\u5411\uff0c\u5f3a\u8c03\u7f8e\u5b66\u4e0eAI\u6280\u672f\u7684\u7ed3\u5408\u3002"}}
{"id": "2504.14066", "pdf": "https://arxiv.org/pdf/2504.14066", "abs": "https://arxiv.org/abs/2504.14066", "authors": ["Laerdon Kim"], "title": "A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to CLPsych Workshop, NAACL 2025", "summary": "We present a baseline for the CLPsych 2025 A.1 task: classifying self-states\nin mental health data taken from Reddit. We use few-shot learning with a 4-bit\nquantized Gemma 2 9B model and a data preprocessing step which first identifies\nrelevant sentences indicating self-state evidence, and then performs a binary\nclassification to determine whether the sentence is evidence of an adaptive or\nmaladaptive self-state. This system outperforms our other method which relies\non an LLM to highlight spans of variable length independently. We attribute the\nperformance of our model to the benefits of this sentence chunking step for two\nreasons: partitioning posts into sentences 1) broadly matches the granularity\nat which self-states were human-annotated and 2) simplifies the task for our\nlanguage model to a binary classification problem. Our system places third out\nof fourteen systems submitted for Task A.1, achieving a test-time recall of\n0.579.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e4\u4f4d\u91cf\u5316Gemma 2 9B\u6a21\u578b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u7c7bReddit\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u4e2d\u7684\u81ea\u6211\u72b6\u6001\uff0c\u901a\u8fc7\u9884\u5904\u7406\u6b65\u9aa4\u8bc6\u522b\u76f8\u5173\u53e5\u5b50\u5e76\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u4e2d\u81ea\u6211\u72b6\u6001\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728Reddit\u7b49\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\uff0c\u4ee5\u652f\u6301\u5fc3\u7406\u5065\u5eb7\u5206\u6790\u548c\u5e72\u9884\u3002", "method": "\u4f7f\u75284\u4f4d\u91cf\u5316Gemma 2 9B\u6a21\u578b\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u7ed3\u5408\u9884\u5904\u7406\u6b65\u9aa4\uff08\u8bc6\u522b\u76f8\u5173\u53e5\u5b50\u5e76\u4e8c\u5143\u5206\u7c7b\uff09\u3002", "result": "\u7cfb\u7edf\u572814\u4e2a\u63d0\u4ea4\u7684\u7cfb\u7edf\u4e2d\u6392\u540d\u7b2c\u4e09\uff0c\u6d4b\u8bd5\u53ec\u56de\u7387\u4e3a0.579\u3002", "conclusion": "\u53e5\u5b50\u5206\u5757\u6b65\u9aa4\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u56e0\u5176\u66f4\u63a5\u8fd1\u4eba\u5de5\u6807\u6ce8\u7684\u7c92\u5ea6\u5e76\u7b80\u5316\u4e86\u4efb\u52a1\u3002"}}
{"id": "2504.14092", "pdf": "https://arxiv.org/pdf/2504.14092", "abs": "https://arxiv.org/abs/2504.14092", "authors": ["Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen"], "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal", "categories": ["cs.CV"], "comment": "Accpeted by CVPR 2025 NTIRE Workshop, Retinex Guidance, Histogram\n  Transformer", "summary": "While deep learning methods have achieved notable progress in shadow removal,\nmany existing approaches rely on shadow masks that are difficult to obtain,\nlimiting their generalization to real-world scenes. In this work, we propose\nReHiT, an efficient mask-free shadow removal framework based on a hybrid\nCNN-Transformer architecture guided by Retinex theory. We first introduce a\ndual-branch pipeline to separately model reflectance and illumination\ncomponents, and each is restored by our developed Illumination-Guided Hybrid\nCNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are\ncapable of learning residual dense features and performing multi-scale semantic\nfusion, multi-scale semantic fusion, we develop the Illumination-Guided\nHistogram Transformer Block (IGHB) to effectively handle non-uniform\nillumination and spatially complex shadows. Extensive experiments on several\nbenchmark datasets validate the effectiveness of our approach over existing\nmask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge\ndataset, our solution delivers competitive results with one of the smallest\nparameter sizes and fastest inference speeds among top-ranked entries,\nhighlighting its applicability for real-world applications with limited\ncomputational resources. The code is available at\nhttps://github.com/dongw22/oath.", "AI": {"tldr": "ReHiT\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408CNN-Transformer\u67b6\u6784\u7684\u65e0\u63a9\u6a21\u9634\u5f71\u53bb\u9664\u6846\u67b6\uff0c\u7ed3\u5408Retinex\u7406\u8bba\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7ba1\u9053\u5206\u522b\u5efa\u6a21\u53cd\u5c04\u548c\u5149\u7167\u5206\u91cf\uff0c\u5e76\u5229\u7528IG-HCT\u6a21\u5757\u8fdb\u884c\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u9634\u5f71\u63a9\u6a21\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u5206\u652f\u7ba1\u9053\u548cIG-HCT\u6a21\u5757\uff0c\u7ed3\u5408CNN\u548cTransformer\uff0c\u5904\u7406\u975e\u5747\u5300\u5149\u7167\u548c\u590d\u6742\u9634\u5f71\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65e0\u63a9\u6a21\u65b9\u6cd5\uff0c\u53c2\u6570\u5c11\u4e14\u63a8\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "ReHiT\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u771f\u5b9e\u573a\u666f\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.14373", "pdf": "https://arxiv.org/pdf/2504.14373", "abs": "https://arxiv.org/abs/2504.14373", "authors": ["Chen Guo", "Zhuo Su", "Jian Wang", "Shuang Li", "Xu Chang", "Zhaohu Li", "Yang Zhao", "Guidong Wang", "Ruqi Huang"], "title": "SEGA: Drivable 3D Gaussian Head Avatar from a Single Image", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Creating photorealistic 3D head avatars from limited input has become\nincreasingly important for applications in virtual reality, telepresence, and\ndigital entertainment. While recent advances like neural rendering and 3D\nGaussian splatting have enabled high-quality digital human avatar creation and\nanimation, most methods rely on multiple images or multi-view inputs, limiting\ntheir practicality for real-world use. In this paper, we propose SEGA, a novel\napproach for Single-imagE-based 3D drivable Gaussian head Avatar creation that\ncombines generalized prior models with a new hierarchical UV-space Gaussian\nSplatting framework. SEGA seamlessly combines priors derived from large-scale\n2D datasets with 3D priors learned from multi-view, multi-expression, and\nmulti-ID data, achieving robust generalization to unseen identities while\nensuring 3D consistency across novel viewpoints and expressions. We further\npresent a hierarchical UV-space Gaussian Splatting framework that leverages\nFLAME-based structural priors and employs a dual-branch architecture to\ndisentangle dynamic and static facial components effectively. The dynamic\nbranch encodes expression-driven fine details, while the static branch focuses\non expression-invariant regions, enabling efficient parameter inference and\nprecomputation. This design maximizes the utility of limited 3D data and\nachieves real-time performance for animation and rendering. Additionally, SEGA\nperforms person-specific fine-tuning to further enhance the fidelity and\nrealism of the generated avatars. Experiments show our method outperforms\nstate-of-the-art approaches in generalization ability, identity preservation,\nand expression realism, advancing one-shot avatar creation for practical\napplications.", "AI": {"tldr": "SEGA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u56fe\u50cf\u76843D\u53ef\u9a71\u52a8\u9ad8\u65af\u5934\u5316\u8eab\u521b\u5efa\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5e7f\u4e49\u5148\u9a8c\u6a21\u578b\u548c\u5206\u5c42UV\u7a7a\u95f4\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u8eab\u4efd\u7684\u9c81\u68d2\u6cdb\u5316\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u56fe\u50cf\u6216\u591a\u89c6\u89d2\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u5355\u56fe\u50cf\u7684\u9ad8\u8d28\u91cf3D\u5316\u8eab\u751f\u6210\u65b9\u6cd5\u3002", "method": "SEGA\u7ed3\u5408\u4e862D\u548c3D\u5148\u9a8c\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42UV\u7a7a\u95f4\u9ad8\u65af\u6e85\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u5206\u79bb\u52a8\u6001\u548c\u9759\u6001\u9762\u90e8\u7ec4\u4ef6\uff0c\u5e76\u652f\u6301\u4e2a\u6027\u5316\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSEGA\u5728\u6cdb\u5316\u80fd\u529b\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u8868\u60c5\u771f\u5b9e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u52a8\u753b\u548c\u6e32\u67d3\u3002", "conclusion": "SEGA\u4e3a\u5355\u56fe\u50cf3D\u5316\u8eab\u521b\u5efa\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u5a31\u4e50\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14089", "pdf": "https://arxiv.org/pdf/2504.14089", "abs": "https://arxiv.org/abs/2504.14089", "authors": ["Kang He", "Kaushik Roy"], "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.", "AI": {"tldr": "LogicTree\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7b97\u6cd5\u5f15\u5bfc\u641c\u7d22\u89e3\u51b3LLMs\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u8bc1\u660e\u51c6\u786e\u7387\u3002", "motivation": "LLMs\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4e2d\u9762\u4e34\u7cfb\u7edf\u6027\u63a2\u7d22\u548c\u903b\u8f91\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5927\u524d\u63d0\u7a7a\u95f4\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51faLogicTree\u6846\u67b6\uff0c\u7ed3\u5408\u7f13\u5b58\u673a\u5236\u548c\u7ebf\u6027\u5316\u524d\u63d0\u641c\u7d22\uff0c\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u5e76\u5f15\u5165LLM-free\u542f\u53d1\u5f0f\u7b56\u7565\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cLogicTree\u5e73\u5747\u6bd4CoT\u548cToT\u5206\u522b\u63d0\u534723.6%\u548c12.5%\u7684\u51c6\u786e\u7387\uff0cGPT-4o\u8868\u73b0\u4f18\u4e8eo3-mini\u3002", "conclusion": "LogicTree\u901a\u8fc7\u7ed3\u6784\u5316\u8bc1\u660e\u63a2\u7d22\u548c\u9ad8\u6548\u524d\u63d0\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2504.14096", "pdf": "https://arxiv.org/pdf/2504.14096", "abs": "https://arxiv.org/abs/2504.14096", "authors": ["Yogesh Kulkarni", "Pooyan Fazli"], "title": "VideoPASTA: 7K Preference Pairs That Matter for Video-LLM Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Video-language models (Video-LLMs) excel at understanding video content but\nstruggle with spatial relationships, temporal ordering, and cross-frame\ncontinuity. To address these limitations, we introduce VideoPASTA (Preference\nAlignment with Spatio-Temporal-Cross Frame Adversaries), a framework that\nenhances Video-LLMs through targeted preference optimization. VideoPASTA trains\nmodels to distinguish accurate video representations from carefully generated\nadversarial examples that deliberately violate spatial, temporal, or\ncross-frame relations. By applying Direct Preference Optimization to just 7,020\npreference pairs, VideoPASTA learns robust representations that capture\nfine-grained spatial relationships and long-range temporal dynamics.\nExperiments on standard video benchmarks show significant relative performance\ngains of 3.05% on VideoMME, 1.97% on NeXTQA, and 1.31% on LongVideoBench, over\nthe baseline Qwen2.5-VL model. These results demonstrate that targeted\nalignment, rather than massive pretraining or architectural modifications,\neffectively addresses core video-language challenges. Notably, VideoPASTA\nachieves these improvements without human annotation or captioning, relying on\njust 32-frame sampling, compared to the 96-frame, multi-GPU setups of prior\nwork. This efficiency makes our approach a scalable, plug-and-play solution\nthat seamlessly integrates with existing models while preserving their\ncapabilities.", "AI": {"tldr": "VideoPASTA\u901a\u8fc7\u504f\u597d\u5bf9\u9f50\u4f18\u5316\u63d0\u5347\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8de8\u5e27\u5173\u7cfb\u7406\u89e3\u80fd\u529b\uff0c\u4ec5\u9700\u5c11\u91cf\u504f\u597d\u5bf9\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u3001\u65f6\u95f4\u987a\u5e8f\u548c\u8de8\u5e27\u8fde\u7eed\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u5f15\u5165VideoPASTA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6837\u672c\u8bad\u7ec3\u6a21\u578b\u533a\u5206\u51c6\u786e\u4e0e\u9519\u8bef\u7684\u89c6\u9891\u8868\u793a\uff0c\u5e76\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u5982VideoMME\u63d0\u53473.05%\uff09\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6216\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "VideoPASTA\u8bc1\u660e\u9488\u5bf9\u6027\u4f18\u5316\u6bd4\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6216\u67b6\u6784\u4fee\u6539\u66f4\u6709\u6548\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14590", "pdf": "https://arxiv.org/pdf/2504.14590", "abs": "https://arxiv.org/abs/2504.14590", "authors": ["Zichen Jin"], "title": "Interdisciplinary Integration of Remote Sensing -- A Review with Four Examples", "categories": ["cs.GR", "astro-ph.IM", "cs.SY", "eess.SY", "physics.geo-ph"], "comment": null, "summary": "As a high-level discipline, the development of remote sensing depends on the\ncontribution of many other basic and applied disciplines and technologies. For\nexample, due to the close relationship between remote sensing and\nphotogrammetry, remote sensing would inevitably integrate disciplines such as\noptics and color science. Also, remote sensing integrates the knowledge of\nelectronics in the conversion from optical signals to electrical signals via\nCCD (Charge-Coupled Device) or other image sensors. Moreover, when conducting\nobject identification and classification with remote sensing data, mathematical\nmorphology and other digital image processing technologies are used. These\nexamples are only the tip of the iceberg of interdisciplinary integration of\nremote sensing. This work briefly reviews the interdisciplinary integration of\nremote sensing with four examples - ecology, mathematical morphology, machine\nlearning, and electronics.", "AI": {"tldr": "\u672c\u6587\u7b80\u8981\u56de\u987e\u4e86\u9065\u611f\u4e0e\u5176\u4ed6\u5b66\u79d1\u7684\u4ea4\u53c9\u878d\u5408\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u751f\u6001\u5b66\u3001\u6570\u5b66\u5f62\u6001\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u7535\u5b50\u5b66\u56db\u4e2a\u9886\u57df\u7684\u4f8b\u5b50\u3002", "motivation": "\u63a2\u8ba8\u9065\u611f\u4f5c\u4e3a\u4e00\u95e8\u9ad8\u5c42\u6b21\u5b66\u79d1\uff0c\u5982\u4f55\u4f9d\u8d56\u5e76\u6574\u5408\u5176\u4ed6\u57fa\u7840\u548c\u5e94\u7528\u5b66\u79d1\u4e0e\u6280\u672f\uff0c\u4ee5\u63a8\u52a8\u5176\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u5177\u4f53\u4f8b\u5b50\uff08\u751f\u6001\u5b66\u3001\u6570\u5b66\u5f62\u6001\u5b66\u3001\u673a\u5668\u5b66\u4e60\u548c\u7535\u5b50\u5b66\uff09\u5206\u6790\u9065\u611f\u4e0e\u5176\u4ed6\u5b66\u79d1\u7684\u4ea4\u53c9\u878d\u5408\u3002", "result": "\u5c55\u793a\u4e86\u9065\u611f\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u6280\u672f\u6574\u5408\uff0c\u7a81\u663e\u5176\u8de8\u5b66\u79d1\u7279\u6027\u3002", "conclusion": "\u9065\u611f\u7684\u53d1\u5c55\u79bb\u4e0d\u5f00\u591a\u5b66\u79d1\u7684\u4ea4\u53c9\u878d\u5408\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u66f4\u591a\u9886\u57df\u7684\u5408\u4f5c\u4e0e\u521b\u65b0\u3002"}}
{"id": "2504.14117", "pdf": "https://arxiv.org/pdf/2504.14117", "abs": "https://arxiv.org/abs/2504.14117", "authors": ["Nusrat Jahan Prottasha", "Upama Roy Chowdhury", "Shetu Mohanto", "Tasfia Nuzhat", "Abdullah As Sami", "Md Shamol Ali", "Md Shohanur Islam Sobuj", "Hafijur Raman", "Md Kowsher", "Ozlem Ozmen Garibay"], "title": "PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models", "categories": ["cs.CL", "cs.CV"], "comment": "PEFT Survey paper", "summary": "Large models such as Large Language Models (LLMs) and Vision Language Models\n(VLMs) have transformed artificial intelligence, powering applications in\nnatural language processing, computer vision, and multimodal learning. However,\nfully fine-tuning these models remains expensive, requiring extensive\ncomputational resources, memory, and task-specific data. Parameter-Efficient\nFine-Tuning (PEFT) has emerged as a promising solution that allows adapting\nlarge models to downstream tasks by updating only a small portion of\nparameters. This survey presents a comprehensive overview of PEFT techniques,\nfocusing on their motivations, design principles, and effectiveness. We begin\nby analyzing the resource and accessibility challenges posed by traditional\nfine-tuning and highlight key issues, such as overfitting, catastrophic\nforgetting, and parameter inefficiency. We then introduce a structured taxonomy\nof PEFT methods -- grouped into additive, selective, reparameterized, hybrid,\nand unified frameworks -- and systematically compare their mechanisms and\ntrade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse\ndomains, including language, vision, and generative modeling, showing how these\ntechniques offer strong performance with lower resource costs. We also discuss\nimportant open challenges in scalability, interpretability, and robustness, and\nsuggest future directions such as federated learning, domain adaptation, and\ntheoretical grounding. Our goal is to provide a unified understanding of PEFT\nand its growing role in enabling practical, efficient, and sustainable use of\nlarge models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u52a8\u673a\u3001\u5206\u7c7b\u3001\u6548\u679c\u53ca\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u6a21\u578b\u5fae\u8c03\u7684\u8d44\u6e90\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5fae\u8c03\u5927\u6a21\u578b\uff08\u5982LLMs\u548cVLMs\uff09\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u4efb\u52a1\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5b58\u5728\u8fc7\u62df\u5408\u3001\u707e\u96be\u6027\u9057\u5fd8\u7b49\u95ee\u9898\u3002PEFT\u901a\u8fc7\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPEFT\u7684\u5206\u7c7b\u6846\u67b6\uff08\u52a0\u6cd5\u3001\u9009\u62e9\u6027\u3001\u91cd\u53c2\u6570\u5316\u3001\u6df7\u5408\u548c\u7edf\u4e00\u65b9\u6cd5\uff09\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u5176\u673a\u5236\u548c\u6743\u8861\u3002", "result": "PEFT\u5728\u8bed\u8a00\u3001\u89c6\u89c9\u548c\u751f\u6210\u5efa\u6a21\u7b49\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u8d44\u6e90\u6d88\u8017\u66f4\u4f4e\u3002", "conclusion": "PEFT\u4e3a\u5927\u6a21\u578b\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u7684\u9014\u5f84\uff0c\u672a\u6765\u9700\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7b49\u6311\u6218\u3002"}}
{"id": "2504.14108", "pdf": "https://arxiv.org/pdf/2504.14108", "abs": "https://arxiv.org/abs/2504.14108", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Pei Wang", "Yuelong Xia"], "title": "Point-Driven Interactive Text and Image Layer Editing Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "We present DanceText, a training-free framework for multilingual text editing\nin images, designed to support complex geometric transformations and achieve\nseamless foreground-background integration. While diffusion-based generative\nmodels have shown promise in text-guided image synthesis, they often lack\ncontrollability and fail to preserve layout consistency under non-trivial\nmanipulations such as rotation, translation, scaling, and warping. To address\nthese limitations, DanceText introduces a layered editing strategy that\nseparates text from the background, allowing geometric transformations to be\nperformed in a modular and controllable manner. A depth-aware module is further\nproposed to align appearance and perspective between the transformed text and\nthe reconstructed background, enhancing photorealism and spatial consistency.\nImportantly, DanceText adopts a fully training-free design by integrating\npretrained modules, allowing flexible deployment without task-specific\nfine-tuning. Extensive experiments on the AnyWord-3M benchmark demonstrate that\nour method achieves superior performance in visual quality, especially under\nlarge-scale and complex transformation scenarios.", "AI": {"tldr": "DanceText\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u56fe\u50cf\u6587\u672c\u7f16\u8f91\u6846\u67b6\uff0c\u652f\u6301\u590d\u6742\u51e0\u4f55\u53d8\u6362\u5e76\u5b9e\u73b0\u65e0\u7f1d\u7684\u524d\u666f-\u80cc\u666f\u878d\u5408\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u5408\u6210\u4e2d\u7f3a\u4e4f\u53ef\u63a7\u6027\u548c\u5e03\u5c40\u4e00\u81f4\u6027\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7f16\u8f91\u7b56\u7565\u5206\u79bb\u6587\u672c\u4e0e\u80cc\u666f\uff0c\u5f15\u5165\u6df1\u5ea6\u611f\u77e5\u6a21\u5757\u589e\u5f3a\u771f\u5b9e\u611f\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u90e8\u7f72\u3002", "result": "\u5728AnyWord-3M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u53d8\u6362\u573a\u666f\u4e0b\u89c6\u89c9\u8d28\u91cf\u4f18\u8d8a\u3002", "conclusion": "DanceText\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u591a\u8bed\u8a00\u6587\u672c\u7f16\u8f91\uff0c\u9002\u7528\u4e8e\u590d\u6742\u53d8\u6362\u9700\u6c42\u3002"}}
{"id": "2504.15028", "pdf": "https://arxiv.org/pdf/2504.15028", "abs": "https://arxiv.org/abs/2504.15028", "authors": ["Santiago Jimenez-Navarro", "Julia Guerrero-Viu", "Belen Masia"], "title": "A Controllable Appearance Representation for Flexible Transfer and Editing", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present a method that computes an interpretable representation of material\nappearance within a highly compact, disentangled latent space. This\nrepresentation is learned in a self-supervised fashion using an adapted\nFactorVAE. We train our model with a carefully designed unlabeled dataset,\navoiding possible biases induced by human-generated labels. Our model\ndemonstrates strong disentanglement and interpretability by effectively\nencoding material appearance and illumination, despite the absence of explicit\nsupervision. Then, we use our representation as guidance for training a\nlightweight IP-Adapter to condition a diffusion pipeline that transfers the\nappearance of one or more images onto a target geometry, and allows the user to\nfurther edit the resulting appearance. Our approach offers fine-grained control\nover the generated results: thanks to the well-structured compact latent space,\nusers can intuitively manipulate attributes such as hue or glossiness in image\nspace to achieve the desired final appearance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7FactorVAE\u751f\u6210\u7d27\u51d1\u4e14\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u7528\u4e8e\u6750\u6599\u5916\u89c2\u7684\u76f4\u89c2\u7f16\u8f91\u3002", "motivation": "\u907f\u514d\u4eba\u5de5\u6807\u6ce8\u5e26\u6765\u7684\u504f\u5dee\uff0c\u5b9e\u73b0\u6750\u6599\u5916\u89c2\u548c\u5149\u7167\u7684\u65e0\u76d1\u7763\u89e3\u8026\u8868\u793a\u3002", "method": "\u4f7f\u7528FactorVAE\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u7ed3\u5408\u65e0\u6807\u7b7e\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5e76\u5229\u7528IP-Adapter\u6307\u5bfc\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5916\u89c2\u8fc1\u79fb\u548c\u7f16\u8f91\u3002", "result": "\u6a21\u578b\u5728\u65e0\u663e\u5f0f\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u89e3\u8026\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u76f4\u89c2\u7684\u5c5e\u6027\u7f16\u8f91\uff08\u5982\u8272\u8c03\u3001\u5149\u6cfd\u5ea6\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u7684\u5916\u89c2\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u6750\u6599\u5916\u89c2\u7684\u8fc1\u79fb\u548c\u7f16\u8f91\u3002"}}
{"id": "2504.14150", "pdf": "https://arxiv.org/pdf/2504.14150", "abs": "https://arxiv.org/abs/2504.14150", "authors": ["Katie Matton", "Robert Osazuwa Ness", "John Guttag", "Emre K\u0131c\u0131man"], "title": "Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "61 pages, 14 figures, 36 tables", "summary": "Large language models (LLMs) are capable of generating plausible explanations\nof how they arrived at an answer to a question. However, these explanations can\nmisrepresent the model's \"reasoning\" process, i.e., they can be unfaithful.\nThis, in turn, can lead to over-trust and misuse. We introduce a new approach\nfor measuring the faithfulness of LLM explanations. First, we provide a\nrigorous definition of faithfulness. Since LLM explanations mimic human\nexplanations, they often reference high-level concepts in the input question\nthat purportedly influenced the model. We define faithfulness in terms of the\ndifference between the set of concepts that LLM explanations imply are\ninfluential and the set that truly are. Second, we present a novel method for\nestimating faithfulness that is based on: (1) using an auxiliary LLM to modify\nthe values of concepts within model inputs to create realistic counterfactuals,\nand (2) using a Bayesian hierarchical model to quantify the causal effects of\nconcepts at both the example- and dataset-level. Our experiments show that our\nmethod can be used to quantify and discover interpretable patterns of\nunfaithfulness. On a social bias task, we uncover cases where LLM explanations\nhide the influence of social bias. On a medical question answering task, we\nuncover cases where LLM explanations provide misleading claims about which\npieces of evidence influenced the model's decisions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u901a\u8fc7\u5b9a\u4e49\u5fe0\u5b9e\u6027\u5e76\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u548c\u8d1d\u53f6\u65af\u5206\u5c42\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLM\u89e3\u91ca\u4e2d\u53ef\u80fd\u9690\u85cf\u7684\u504f\u89c1\u548c\u8bef\u5bfc\u6027\u3002", "motivation": "LLM\u751f\u6210\u7684\u89e3\u91ca\u53ef\u80fd\u4e0d\u5fe0\u5b9e\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u4fe1\u4efb\u548c\u8bef\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u79cd\u4e0d\u5fe0\u5b9e\u6027\u3002", "method": "\u5b9a\u4e49\u5fe0\u5b9e\u6027\u4e3a\u89e3\u91ca\u4e2d\u6697\u793a\u7684\u6982\u5ff5\u4e0e\u771f\u5b9e\u5f71\u54cd\u6982\u5ff5\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5229\u7528\u8f85\u52a9LLM\u751f\u6210\u53cd\u4e8b\u5b9e\u8f93\u5165\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u5206\u5c42\u6a21\u578b\u91cf\u5316\u6982\u5ff5\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u91cf\u5316\u4e0d\u5fe0\u5b9e\u6027\uff0c\u5e76\u5728\u793e\u4f1a\u504f\u89c1\u548c\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d1\u73b0LLM\u89e3\u91ca\u9690\u85cf\u7684\u504f\u89c1\u548c\u8bef\u5bfc\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30LLM\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u95ee\u9898\uff0c\u6709\u52a9\u4e8e\u9632\u6b62\u8bef\u7528\u3002"}}
{"id": "2504.14113", "pdf": "https://arxiv.org/pdf/2504.14113", "abs": "https://arxiv.org/abs/2504.14113", "authors": ["Jiyong Kwag", "Alper Yilmaz", "Charles Toth"], "title": "Lightweight Road Environment Segmentation using Vector Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Road environment segmentation plays a significant role in autonomous driving.\nNumerous works based on Fully Convolutional Networks (FCNs) and Transformer\narchitectures have been proposed to leverage local and global contextual\nlearning for efficient and accurate semantic segmentation. In both\narchitectures, the encoder often relies heavily on extracting continuous\nrepresentations from the image, which limits the ability to represent\nmeaningful discrete information. To address this limitation, we propose\nsegmentation of the autonomous driving environment using vector quantization.\nVector quantization offers three primary advantages for road environment\nsegmentation. (1) Each continuous feature from the encoder is mapped to a\ndiscrete vector from the codebook, helping the model discover distinct features\nmore easily than with complex continuous features. (2) Since a discrete feature\nacts as compressed versions of the encoder's continuous features, they also\ncompress noise or outliers, enhancing the image segmentation task. (3) Vector\nquantization encourages the latent space to form coarse clusters of continuous\nfeatures, forcing the model to group similar features, making the learned\nrepresentations more structured for the decoding process. In this work, we\ncombined vector quantization with the lightweight image segmentation model\nMobileUNETR and used it as a baseline model for comparison to demonstrate its\nefficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes,\noutperforming the baseline by 2.9 % without increasing the model's initial size\nor complexity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u91cf\u5316\u7684\u9053\u8def\u73af\u5883\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408MobileUNETR\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eFCN\u548cTransformer\u7684\u7f16\u7801\u5668\u4f9d\u8d56\u8fde\u7eed\u7279\u5f81\u8868\u793a\uff0c\u96be\u4ee5\u6355\u6349\u79bb\u6563\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u8bed\u4e49\u5206\u5272\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u5411\u91cf\u91cf\u5316\u6280\u672f\uff0c\u5c06\u8fde\u7eed\u7279\u5f81\u6620\u5c04\u4e3a\u79bb\u6563\u5411\u91cf\uff0c\u7ed3\u5408MobileUNETR\u6a21\u578b\u8fdb\u884c\u5206\u5272\u3002", "result": "\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u8fbe\u523077.0% mIoU\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53472.9%\u3002", "conclusion": "\u5411\u91cf\u91cf\u5316\u80fd\u6709\u6548\u63d0\u5347\u9053\u8def\u73af\u5883\u5206\u5272\u6027\u80fd\uff0c\u4e14\u4e0d\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2504.13846", "pdf": "https://arxiv.org/pdf/2504.13846", "abs": "https://arxiv.org/abs/2504.13846", "authors": ["Antonio Strippoli"], "title": "VoxLogicA UI: Supporting Declarative Medical Image Analysis", "categories": ["cs.HC", "cs.ET", "cs.GR", "cs.SE", "q-bio.QM"], "comment": "Advisors: Vincenzo Ciancia, Fabio Gadducci, Mieke Massink", "summary": "This Master's Thesis in Computer Science dives into the design and creation\nof a user-friendly interface for VoxLogicA, an image analysis tool using\nspatial model checking with a focus on neuroimaging. The research tackles the\nproblem of existing tools being too complex, which makes them hard for medical\nprofessionals and researchers to use. By using spatial logic, the goal is to\nmake these powerful analytical tools more practical and accessible in\nreal-world clinical settings. The main objectives are to design a modern web\ninterface that's easy to use, build it with the latest web technologies (e.g.\nSvelte and Niivue), and test its effectiveness through user studies and\nreal-world case analyses.", "AI": {"tldr": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0VoxLogicA\u7684\u7528\u6237\u53cb\u597d\u754c\u9762\uff0c\u7b80\u5316\u795e\u7ecf\u5f71\u50cf\u5206\u6790\u5de5\u5177\u7684\u4f7f\u7528\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u8fc7\u4e8e\u590d\u6742\uff0c\u96be\u4ee5\u88ab\u533b\u5b66\u4e13\u4e1a\u4eba\u58eb\u548c\u7814\u7a76\u4eba\u5458\u4f7f\u7528\uff0c\u5e0c\u671b\u901a\u8fc7\u7a7a\u95f4\u903b\u8f91\u63d0\u5347\u5de5\u5177\u7684\u5b9e\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u91c7\u7528\u73b0\u4ee3Web\u6280\u672f\uff08\u5982Svelte\u548cNiivue\uff09\u8bbe\u8ba1\u754c\u9762\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u5b9e\u9645\u6848\u4f8b\u5206\u6790\u6d4b\u8bd5\u5176\u6548\u679c\u3002", "result": "\u672a\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "\u76ee\u6807\u662f\u4f7f\u5f3a\u5927\u7684\u5206\u6790\u5de5\u5177\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u66f4\u5b9e\u7528\u548c\u6613\u7528\u3002"}}
{"id": "2504.14154", "pdf": "https://arxiv.org/pdf/2504.14154", "abs": "https://arxiv.org/abs/2504.14154", "authors": ["Zhiyuan Wang", "Qingni Wang", "Yue Zhang", "Tianlong Chen", "Xiaofeng Zhu", "Xiaoshuang Shi", "Kaidi Xu"], "title": "SConU: Selective Conformal Uncertainty in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "As large language models are increasingly utilized in real-world\napplications, guarantees of task-specific metrics are essential for their\nreliable deployment. Previous studies have introduced various criteria of\nconformal uncertainty grounded in split conformal prediction, which offer\nuser-specified correctness coverage. However, existing frameworks often fail to\nidentify uncertainty data outliers that violate the exchangeability assumption,\nleading to unbounded miscoverage rates and unactionable prediction sets. In\nthis paper, we propose a novel approach termed Selective Conformal Uncertainty\n(SConU), which, for the first time, implements significance tests, by\ndeveloping two conformal p-values that are instrumental in determining whether\na given sample deviates from the uncertainty distribution of the calibration\nset at a specific manageable risk level. Our approach not only facilitates\nrigorous management of miscoverage rates across both single-domain and\ninterdisciplinary contexts, but also enhances the efficiency of predictions.\nFurthermore, we comprehensively analyze the components of the conformal\nprocedures, aiming to approximate conditional coverage, particularly in\nhigh-stakes question-answering tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSConU\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u8457\u6027\u6d4b\u8bd5\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u5f02\u5e38\u503c\uff0c\u63d0\u9ad8\u9884\u6d4b\u6548\u7387\u5e76\u63a7\u5236\u8bef\u8986\u76d6\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u6307\u6807\u7684\u4fdd\u8bc1\uff0c\u73b0\u6709\u6846\u67b6\u65e0\u6cd5\u8bc6\u522b\u8fdd\u53cd\u4ea4\u6362\u6027\u5047\u8bbe\u7684\u5f02\u5e38\u503c\uff0c\u5bfc\u81f4\u8bef\u8986\u76d6\u7387\u4e0d\u53ef\u63a7\u3002", "method": "\u5f00\u53d1\u4e86\u4e24\u79cdconformal p\u503c\uff0c\u901a\u8fc7\u663e\u8457\u6027\u6d4b\u8bd5\u5224\u65ad\u6837\u672c\u662f\u5426\u504f\u79bb\u6821\u51c6\u96c6\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u5e03\uff0c\u5e76\u7ba1\u7406\u8bef\u8986\u76d6\u7387\u3002", "result": "SConU\u65b9\u6cd5\u5728\u5355\u9886\u57df\u548c\u8de8\u5b66\u79d1\u80cc\u666f\u4e0b\u5747\u80fd\u6709\u6548\u63a7\u5236\u8bef\u8986\u76d6\u7387\uff0c\u5e76\u63d0\u5347\u9884\u6d4b\u6548\u7387\u3002", "conclusion": "SConU\u65b9\u6cd5\u4e3a\u9ad8\u98ce\u9669\u7684\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u8fd1\u4f3c\u6761\u4ef6\u8986\u76d6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14129", "pdf": "https://arxiv.org/pdf/2504.14129", "abs": "https://arxiv.org/abs/2504.14129", "authors": ["Yaning Zhang", "Jiahe Zhang", "Chunjie Ma", "Weili Guan", "Tian Gan", "Zan Gao"], "title": "BMRL: Bi-Modal Guided Multi-Perspective Representation Learning for Zero-Shot Deepfake Attribution", "categories": ["cs.CV"], "comment": null, "summary": "The challenge of tracing the source attribution of forged faces has gained\nsignificant attention due to the rapid advancement of generative models.\nHowever, existing deepfake attribution (DFA) works primarily focus on the\ninteraction among various domains in vision modality, and other modalities such\nas texts and face parsing are not fully explored. Besides, they tend to fail to\nassess the generalization performance of deepfake attributors to unseen\ngenerators in a fine-grained manner. In this paper, we propose a novel bi-modal\nguided multi-perspective representation learning (BMRL) framework for zero-shot\ndeepfake attribution (ZS-DFA), which facilitates effective traceability to\nunseen generators. Specifically, we design a multi-perspective visual encoder\n(MPVE) to explore general deepfake attribution visual characteristics across\nthree views (i.e., image, noise, and edge). We devise a novel parsing encoder\nto focus on global face attribute embeddings, enabling parsing-guided DFA\nrepresentation learning via vision-parsing matching. A language encoder is\nproposed to capture fine-grained language embeddings, facilitating\nlanguage-guided general visual forgery representation learning through\nvision-language alignment. Additionally, we present a novel deepfake\nattribution contrastive center (DFACC) loss, to pull relevant generators closer\nand push irrelevant ones away, which can be introduced into DFA models to\nenhance traceability. Experimental results demonstrate that our method\noutperforms the state-of-the-art on the ZS-DFA task through various protocols\nevaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6a21\u6001\u5f15\u5bfc\u7684\u591a\u89c6\u89d2\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff08BMRL\uff09\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\uff08ZS-DFA\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u3001\u89e3\u6790\u548c\u8bed\u8a00\u6a21\u6001\u589e\u5f3a\u5bf9\u672a\u89c1\u751f\u6210\u5668\u7684\u6eaf\u6e90\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u6a21\u6001\uff0c\u5ffd\u89c6\u5176\u4ed6\u6a21\u6001\uff08\u5982\u6587\u672c\u548c\u9762\u90e8\u89e3\u6790\uff09\uff0c\u4e14\u96be\u4ee5\u5728\u7ec6\u7c92\u5ea6\u4e0a\u8bc4\u4f30\u5bf9\u672a\u89c1\u751f\u6210\u5668\u7684\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u89c6\u89d2\u89c6\u89c9\u7f16\u7801\u5668\uff08MPVE\uff09\u63a2\u7d22\u56fe\u50cf\u3001\u566a\u58f0\u548c\u8fb9\u7f18\u4e09\u4e2a\u89c6\u89d2\u7684\u7279\u5f81\uff1b\u63d0\u51fa\u89e3\u6790\u7f16\u7801\u5668\u6355\u83b7\u5168\u5c40\u9762\u90e8\u5c5e\u6027\u5d4c\u5165\uff1b\u8bed\u8a00\u7f16\u7801\u5668\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u8a00\u5d4c\u5165\uff1b\u5e76\u5f15\u5165\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u5bf9\u6bd4\u4e2d\u5fc3\uff08DFACC\uff09\u635f\u5931\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ZS-DFA\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "BMRL\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u548c\u591a\u89c6\u89d2\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u672a\u89c1\u751f\u6210\u5668\u7684\u6eaf\u6e90\u80fd\u529b\u3002"}}
{"id": "2504.13893", "pdf": "https://arxiv.org/pdf/2504.13893", "abs": "https://arxiv.org/abs/2504.13893", "authors": ["Qiang Zou", "Shuo Liu"], "title": "Semantic Direct Modeling", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Current direct modeling systems limit users to low-level interactions with\nvertices, edges, and faces, forcing designers to manage detailed geometric\nelements rather than focusing on high-level design intent. This paper\nintroduces semantic direct modeling (SDM), a novel approach that lifts direct\nmodeling from low-level geometric modifications to high-level semantic\ninteractions. This is achieved by utilizing a large language model (LLM)\nfine-tuned with CAD-specific prompts, which can guide the LLM to reason through\ndesign intent and accurately interpret CAD commands, thereby allowing designers\nto express their intent using natural language. Additionally, SDM maps design\nintent to the corresponding geometric features in the CAD model through a new\nconditional, context-sensitive feature recognition method, which uses\ngenerative AI to dynamically assign feature labels based on design intent.\nTogether, they enable a seamless flow from high-level design intent to\nlow-level geometric modifications, bypassing tedious software interactions. The\neffectiveness of SDM has been validated through real mechanical design cases.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u76f4\u63a5\u5efa\u6a21\uff08SDM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u751f\u6210\u5f0fAI\uff0c\u5c06\u76f4\u63a5\u5efa\u6a21\u4ece\u4f4e\u7ea7\u7684\u51e0\u4f55\u64cd\u4f5c\u63d0\u5347\u4e3a\u9ad8\u7ea7\u7684\u8bed\u4e49\u4ea4\u4e92\uff0c\u7b80\u5316\u8bbe\u8ba1\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u76f4\u63a5\u5efa\u6a21\u7cfb\u7edf\u9650\u5236\u4e86\u7528\u6237\u5bf9\u9876\u70b9\u3001\u8fb9\u548c\u9762\u7684\u4f4e\u7ea7\u64cd\u4f5c\uff0c\u8bbe\u8ba1\u5e08\u9700\u5173\u6ce8\u51e0\u4f55\u7ec6\u8282\u800c\u975e\u9ad8\u5c42\u8bbe\u8ba1\u610f\u56fe\u3002", "method": "SDM\u5229\u7528\u7ecf\u8fc7CAD\u7279\u5b9a\u63d0\u793a\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u6790\u8bbe\u8ba1\u610f\u56fe\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u6027\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u7279\u5f81\u8bc6\u522b\u65b9\u6cd5\u5c06\u610f\u56fe\u6620\u5c04\u5230\u51e0\u4f55\u7279\u5f81\u3002", "result": "SDM\u80fd\u591f\u5b9e\u73b0\u4ece\u9ad8\u5c42\u8bbe\u8ba1\u610f\u56fe\u5230\u4f4e\u7ea7\u51e0\u4f55\u4fee\u6539\u7684\u65e0\u7f1d\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u673a\u68b0\u8bbe\u8ba1\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "SDM\u901a\u8fc7\u8bed\u4e49\u4ea4\u4e92\u7b80\u5316\u4e86\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u3002"}}
{"id": "2504.14165", "pdf": "https://arxiv.org/pdf/2504.14165", "abs": "https://arxiv.org/abs/2504.14165", "authors": ["Ziyan Zhang", "Yang Hou", "Chen Gong", "Zhenghua Li"], "title": "Self-Correction Makes LLMs Better Parsers", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing (NLP) tasks. However, recent studies suggest that\nthey still face challenges in performing fundamental NLP tasks essential for\ndeep language understanding, particularly syntactic parsing. In this paper, we\nconduct an in-depth analysis of LLM parsing capabilities, delving into the\nspecific shortcomings of their parsing results. We find that LLMs may stem from\nlimitations to fully leverage grammar rules in existing treebanks, which\nrestricts their capability to generate valid syntactic structures. To help LLMs\nacquire knowledge without additional training, we propose a self-correction\nmethod that leverages grammar rules from existing treebanks to guide LLMs in\ncorrecting previous errors. Specifically, we automatically detect potential\nerrors and dynamically search for relevant rules, offering hints and examples\nto guide LLMs in making corrections themselves. Experimental results on three\ndatasets with various LLMs, demonstrate that our method significantly improves\nperformance in both in-domain and cross-domain settings on the English and\nChinese datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u6811\u5e93\u7684\u8bed\u6cd5\u89c4\u5219\u6307\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53e5\u6cd5\u5206\u6790\u4efb\u52a1\u4e2d\u81ea\u6211\u4fee\u6b63\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u53e5\u6cd5\u5206\u6790\u7b49\u57fa\u7840\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u6709\u6811\u5e93\u7684\u8bed\u6cd5\u89c4\u5219\u751f\u6210\u6709\u6548\u7684\u53e5\u6cd5\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u81ea\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u6d4b\u6f5c\u5728\u9519\u8bef\u5e76\u52a8\u6001\u641c\u7d22\u76f8\u5173\u8bed\u6cd5\u89c4\u5219\uff0c\u4e3aLLMs\u63d0\u4f9b\u63d0\u793a\u548c\u793a\u4f8b\uff0c\u6307\u5bfc\u5176\u81ea\u6211\u4fee\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u82f1\u8bed\u548c\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u62ec\u9886\u57df\u5185\u548c\u8de8\u9886\u57df\u8bbe\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u73b0\u6709\u6811\u5e93\u7684\u8bed\u6cd5\u89c4\u5219\u6307\u5bfcLLMs\u81ea\u6211\u4fee\u6b63\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u5176\u5728\u53e5\u6cd5\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.14131", "pdf": "https://arxiv.org/pdf/2504.14131", "abs": "https://arxiv.org/abs/2504.14131", "authors": ["Ole-Christian Galbo Engstr\u00f8m", "Michela Albano-Gaglio", "Erik Schou Dreier", "Yamine Bouzembrak", "Maria Font-i-Furnols", "Puneet Mishra", "Kim Steenstrup Pedersen"], "title": "Transforming hyperspectral images into chemical maps: A new deep learning based approach to hyperspectral image processing", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Current approaches to chemical map generation from hyperspectral images are\nbased on models such as partial least squares (PLS) regression, generating\npixel-wise predictions that do not consider spatial context and suffer from a\nhigh degree of noise. This study proposes an end-to-end deep learning approach\nusing a modified version of U-Net and a custom loss function to directly obtain\nchemical maps from hyperspectral images, skipping all intermediate steps\nrequired for traditional pixel-wise analysis. We compare the U-Net with the\ntraditional PLS regression on a real dataset of pork belly samples with\nassociated mean fat reference values. The U-Net obtains a test set root mean\nsquared error of between 9% and 13% lower than that of PLS regression on the\ntask of mean fat prediction. At the same time, U-Net generates fine detail\nchemical maps where 99.91% of the variance is spatially correlated. Conversely,\nonly 2.53% of the variance in the PLS-generated chemical maps is spatially\ncorrelated, indicating that each pixel-wise prediction is largely independent\nof neighboring pixels. Additionally, while the PLS-generated chemical maps\ncontain predictions far beyond the physically possible range of 0-100%, U-Net\nlearns to stay inside this range. Thus, the findings of this study indicate\nthat U-Net is superior to PLS for chemical map generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbU-Net\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u7684\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u76f4\u63a5\u4ece\u9ad8\u5149\u8c31\u56fe\u50cf\u751f\u6210\u5316\u5b66\u56fe\u8c31\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684PLS\u56de\u5f52\u65b9\u6cd5\uff0cU-Net\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a7a\u95f4\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u5316\u5b66\u56fe\u8c31\u751f\u6210\u65b9\u6cd5\uff08\u5982PLS\u56de\u5f52\uff09\u5b58\u5728\u566a\u58f0\u9ad8\u4e14\u5ffd\u7565\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684U-Net\u7ed3\u6784\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\uff0c\u76f4\u63a5\u751f\u6210\u5316\u5b66\u56fe\u8c31\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u50cf\u7d20\u7ea7\u5206\u6790\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "U-Net\u5728\u732a\u8089\u6837\u672c\u6570\u636e\u96c6\u4e0a\u7684\u5747\u65b9\u6839\u8bef\u5dee\u6bd4PLS\u4f4e9%-13%\uff0c\u4e14\u751f\u6210\u7684\u5316\u5b66\u56fe\u8c3199.91%\u7684\u65b9\u5dee\u5177\u6709\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u800cPLS\u4ec5\u4e3a2.53%\u3002", "conclusion": "U-Net\u5728\u5316\u5b66\u56fe\u8c31\u751f\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8ePLS\u56de\u5f52\uff0c\u5c24\u5176\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u7a7a\u95f4\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u663e\u8457\u3002"}}
{"id": "2504.14135", "pdf": "https://arxiv.org/pdf/2504.14135", "abs": "https://arxiv.org/abs/2504.14135", "authors": ["Jonathan Embley-Riches", "Jianwei Liu", "Simon Julier", "Dimitrios Kanoulas"], "title": "Unreal Robotics Lab: A High-Fidelity Robotics Simulator with Advanced Physics and Rendering", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "High-fidelity simulation is essential for robotics research, enabling safe\nand efficient testing of perception, control, and navigation algorithms.\nHowever, achieving both photorealistic rendering and accurate physics modeling\nremains a challenge. This paper presents a novel simulation framework--the\nUnreal Robotics Lab (URL) that integrates the Unreal Engine's advanced\nrendering capabilities with MuJoCo's high-precision physics simulation. Our\napproach enables realistic robotic perception while maintaining accurate\nphysical interactions, facilitating benchmarking and dataset generation for\nvision-based robotics applications. The system supports complex environmental\neffects, such as smoke, fire, and water dynamics, which are critical for\nevaluating robotic performance under adverse conditions. We benchmark visual\nnavigation and SLAM methods within our framework, demonstrating its utility for\ntesting real-world robustness in controlled yet diverse scenarios. By bridging\nthe gap between physics accuracy and photorealistic rendering, our framework\nprovides a powerful tool for advancing robotics research and sim-to-real\ntransfer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Unreal Engine\u548cMuJoCo\u7684\u65b0\u578b\u673a\u5668\u4eba\u4eff\u771f\u6846\u67b6URL\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u6e32\u67d3\u4e0e\u7cbe\u786e\u7269\u7406\u6a21\u62df\u7684\u7ed3\u5408\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u673a\u5668\u4eba\u5e94\u7528\u7684\u6d4b\u8bd5\u4e0e\u6570\u636e\u96c6\u751f\u6210\u3002", "motivation": "\u9ad8\u4fdd\u771f\u4eff\u771f\u5bf9\u673a\u5668\u4eba\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u540c\u65f6\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\u548c\u7cbe\u786e\u7269\u7406\u6a21\u62df\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408Unreal Engine\u7684\u9ad8\u7ea7\u6e32\u67d3\u80fd\u529b\u548cMuJoCo\u7684\u9ad8\u7cbe\u5ea6\u7269\u7406\u6a21\u62df\uff0c\u6784\u5efa\u4e86Unreal Robotics Lab\uff08URL\uff09\u6846\u67b6\u3002", "result": "\u652f\u6301\u590d\u6742\u73af\u5883\u6548\u679c\uff08\u5982\u70df\u96fe\u3001\u706b\u7130\u548c\u6c34\u52a8\u529b\u5b66\uff09\uff0c\u5e76\u6210\u529f\u7528\u4e8e\u89c6\u89c9\u5bfc\u822a\u548cSLAM\u65b9\u6cd5\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u7269\u7406\u7cbe\u5ea6\u4e0e\u903c\u771f\u6e32\u67d3\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u673a\u5668\u4eba\u7814\u7a76\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2504.14175", "pdf": "https://arxiv.org/pdf/2504.14175", "abs": "https://arxiv.org/abs/2504.14175", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion", "categories": ["cs.CL", "cs.IR"], "comment": "preprint", "summary": "Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyzed whether the generated documents contained information\nentailed by ground truth evidence and assessed their impact on performance. Our\nfindings indicate that performance improvements occurred consistently only for\nclaims whose generated documents included sentences entailed by ground truth\nevidence. This suggests that knowledge leakage may be present in these\nbenchmarks, inflating the perceived performance of LLM-based query expansion\nmethods, particularly in real-world scenarios that require retrieving niche or\nnovel knowledge.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6307\u51fa\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u6e90\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u77e5\u8bc6\u6cc4\u6f0f\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1LLM\u751f\u6210\u7684\u5047\u8bbe\u6587\u6863\u662f\u5426\u56e0\u5305\u542b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u771f\u5b9e\u8bc1\u636e\u4fe1\u606f\uff08\u77e5\u8bc6\u6cc4\u6f0f\uff09\u800c\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "method": "\u4ee5\u4e8b\u5b9e\u9a8c\u8bc1\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5206\u6790\u751f\u6210\u6587\u6863\u662f\u5426\u5305\u542b\u771f\u5b9e\u8bc1\u636e\u7684\u9690\u542b\u4fe1\u606f\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u63d0\u5347\u4ec5\u53d1\u751f\u5728\u751f\u6210\u6587\u6863\u5305\u542b\u771f\u5b9e\u8bc1\u636e\u9690\u542b\u4fe1\u606f\u7684\u6848\u4f8b\u4e2d\uff0c\u8868\u660e\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u5b58\u5728\u77e5\u8bc6\u6cc4\u6f0f\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\u77e5\u8bc6\u6cc4\u6f0f\u53ef\u80fd\u5938\u5927LLM\u67e5\u8be2\u6269\u5c55\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u9700\u8981\u68c0\u7d22\u5c0f\u4f17\u6216\u65b0\u77e5\u8bc6\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u3002"}}
{"id": "2504.14132", "pdf": "https://arxiv.org/pdf/2504.14132", "abs": "https://arxiv.org/abs/2504.14132", "authors": ["Xuanhua Yin", "Dingxin Zhang", "Jianhui Yu", "Weidong Cai"], "title": "HFBRI-MAE: Handcrafted Feature Based Rotation-Invariant Masked Autoencoder for 3D Point Cloud Analysis", "categories": ["cs.CV"], "comment": "12 pages, 9 figures, accepted by IJCNN 2025", "summary": "Self-supervised learning (SSL) has demonstrated remarkable success in 3D\npoint cloud analysis, particularly through masked autoencoders (MAEs). However,\nexisting MAE-based methods lack rotation invariance, leading to significant\nperformance degradation when processing arbitrarily rotated point clouds in\nreal-world scenarios. To address this limitation, we introduce Handcrafted\nFeature-Based Rotation-Invariant Masked Autoencoder (HFBRI-MAE), a novel\nframework that refines the MAE design with rotation-invariant handcrafted\nfeatures to ensure stable feature learning across different orientations. By\nleveraging both rotation-invariant local and global features for token\nembedding and position embedding, HFBRI-MAE effectively eliminates rotational\ndependencies while preserving rich geometric structures. Additionally, we\nredefine the reconstruction target to a canonically aligned version of the\ninput, mitigating rotational ambiguities. Extensive experiments on ModelNet40,\nScanObjectNN, and ShapeNetPart demonstrate that HFBRI-MAE consistently\noutperforms existing methods in object classification, segmentation, and\nfew-shot learning, highlighting its robustness and strong generalization\nability in real-world 3D applications.", "AI": {"tldr": "HFBRI-MAE\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65cb\u8f6c\u4e0d\u53d8\u7684\u624b\u5de5\u7279\u5f81\u6539\u8fdb\u4e86MAE\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65cb\u8f6c\u70b9\u4e91\u5904\u7406\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMAE\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5904\u7406\u65cb\u8f6c\u70b9\u4e91\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "HFBRI-MAE\u7ed3\u5408\u65cb\u8f6c\u4e0d\u53d8\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u8fdb\u884c\u6807\u8bb0\u5d4c\u5165\u548c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u91cd\u5efa\u76ee\u6807\u4e3a\u89c4\u8303\u5bf9\u9f50\u7248\u672c\u3002", "result": "\u5728ModelNet40\u3001ScanObjectNN\u548cShapeNetPart\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHFBRI-MAE\u5728\u5206\u7c7b\u3001\u5206\u5272\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HFBRI-MAE\u5177\u6709\u9c81\u68d2\u6027\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u76843D\u5e94\u7528\u3002"}}
{"id": "2504.14194", "pdf": "https://arxiv.org/pdf/2504.14194", "abs": "https://arxiv.org/abs/2504.14194", "authors": ["Xinlin Zhuang", "Jiahui Peng", "Ren Ma", "Yinfan Wang", "Tianyi Bai", "Xingjian Wei", "Jiantao Qiu", "Chi Zhang", "Ying Qian", "Conghui He"], "title": "Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models", "categories": ["cs.CL"], "comment": "Under review", "summary": "The composition of pre-training datasets for large language models (LLMs)\nremains largely undisclosed, hindering transparency and efforts to optimize\ndata quality, a critical driver of model performance. Current data selection\nmethods, such as natural language quality assessments, diversity-based filters,\nand classifier-based approaches, are limited by single-dimensional evaluation\nor redundancy-focused strategies. To address these gaps, we propose PRRC to\nevaluate data quality across Professionalism, Readability, Reasoning, and\nCleanliness. We further introduce Meta-rater, a multi-dimensional data\nselection method that integrates these dimensions with existing quality metrics\nthrough learned optimal weightings. Meta-rater employs proxy models to train a\nregression model that predicts validation loss, enabling the identification of\noptimal combinations of quality scores. Experiments demonstrate that Meta-rater\ndoubles convergence speed for 1.3B parameter models and improves downstream\ntask performance by 3.23, with scalable benefits observed in 3.3B models\ntrained on 100B tokens. Additionally, we release the annotated SlimPajama-627B\ndataset, labeled across 25 quality metrics (including PRRC), to advance\nresearch in data-centric LLM development. Our work establishes that holistic,\nmulti-dimensional quality integration significantly outperforms conventional\nsingle-dimension approaches, offering a scalable paradigm for enhancing\npre-training efficiency and model capability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPRRC\u6846\u67b6\u548cMeta-rater\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u63d0\u5347LLM\u9884\u8bad\u7ec3\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5355\u7ef4\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dLLM\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u591a\u4e3a\u5355\u7ef4\u6216\u5197\u4f59\u5bfc\u5411\uff0c\u9650\u5236\u4e86\u6570\u636e\u900f\u660e\u6027\u548c\u6a21\u578b\u6027\u80fd\u4f18\u5316\u3002", "method": "\u63d0\u51faPRRC\u6846\u67b6\uff08\u4e13\u4e1a\u6027\u3001\u53ef\u8bfb\u6027\u3001\u63a8\u7406\u6027\u548c\u6e05\u6d01\u6027\uff09\u548cMeta-rater\u65b9\u6cd5\uff0c\u7ed3\u5408\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u9a8c\u8bc1\u635f\u5931\uff0c\u4f18\u5316\u6570\u636e\u9009\u62e9\u3002", "result": "Meta-rater\u4f7f1.3B\u53c2\u6570\u6a21\u578b\u6536\u655b\u901f\u5ea6\u7ffb\u500d\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u53473.23\uff0c\u4e14\u57283.3B\u6a21\u578b\u4e2d\u8868\u73b0\u53ef\u6269\u5c55\u3002", "conclusion": "\u591a\u7ef4\u8d28\u91cf\u8bc4\u4f30\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3aLLM\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u8303\u5f0f\u3002"}}
{"id": "2504.14137", "pdf": "https://arxiv.org/pdf/2504.14137", "abs": "https://arxiv.org/abs/2504.14137", "authors": ["Hangyu Liu", "Bo Peng", "Pengxiang Ding", "Donglin Wang"], "title": "Rethinking Target Label Conditioning in Adversarial Attacks: A 2D Tensor-Guided Generative Approach", "categories": ["cs.CV"], "comment": "12 pages, 4 figures", "summary": "Compared to single-target adversarial attacks, multi-target attacks have\ngarnered significant attention due to their ability to generate adversarial\nimages for multiple target classes simultaneously. Existing generative\napproaches for multi-target attacks mainly analyze the effect of the use of\ntarget labels on noise generation from a theoretical perspective, lacking\npractical validation and comprehensive summarization. To address this gap, we\nfirst identify and validate that the semantic feature quality and quantity are\ncritical factors affecting the transferability of targeted attacks: 1) Feature\nquality refers to the structural and detailed completeness of the implanted\ntarget features, as deficiencies may result in the loss of key discriminative\ninformation; 2) Feature quantity refers to the spatial sufficiency of the\nimplanted target features, as inadequacy limits the victim model's attention to\nthis feature. Based on these findings, we propose the 2D Tensor-Guided\nAdversarial Fusion (2D-TGAF) framework, which leverages the powerful generative\ncapabilities of diffusion models to encode target labels into two-dimensional\nsemantic tensors for guiding adversarial noise generation. Additionally, we\ndesign a novel masking strategy tailored for the training process, ensuring\nthat parts of the generated noise retain complete semantic information about\nthe target class. Extensive experiments on the standard ImageNet dataset\ndemonstrate that 2D-TGAF consistently surpasses state-of-the-art methods in\nattack success rates, both on normally trained models and across various\ndefense mechanisms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a2D-TGAF\u7684\u591a\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4e8c\u7ef4\u8bed\u4e49\u5f20\u91cf\u6307\u5bfc\u566a\u58f0\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u591a\u76ee\u6807\u653b\u51fb\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u8df5\u9a8c\u8bc1\u548c\u5168\u9762\u603b\u7ed3\uff0c\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9a8c\u8bc1\u8bed\u4e49\u7279\u5f81\u8d28\u91cf\u548c\u6570\u91cf\u5bf9\u653b\u51fb\u8fc1\u79fb\u6027\u7684\u5173\u952e\u5f71\u54cd\u3002", "method": "\u63d0\u51fa2D-TGAF\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5c06\u76ee\u6807\u6807\u7b7e\u7f16\u7801\u4e3a\u4e8c\u7ef4\u8bed\u4e49\u5f20\u91cf\u6307\u5bfc\u566a\u58f0\u751f\u6210\uff0c\u5e76\u8bbe\u8ba1\u63a9\u7801\u7b56\u7565\u786e\u4fdd\u566a\u58f0\u4fdd\u7559\u76ee\u6807\u7c7b\u522b\u7684\u5b8c\u6574\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c2D-TGAF\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u5bf9\u6297\u9632\u5fa1\u673a\u5236\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "2D-TGAF\u901a\u8fc7\u4f18\u5316\u8bed\u4e49\u7279\u5f81\u8d28\u91cf\u548c\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\u3002"}}
{"id": "2504.14203", "pdf": "https://arxiv.org/pdf/2504.14203", "abs": "https://arxiv.org/abs/2504.14203", "authors": ["Jian Zhang", "Tianqing Zhang", "Qi Li", "Hongwei Wang"], "title": "EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by SIGIR'2025", "summary": "In recent years, research has mainly focused on the general NER task. There\nstill have some challenges with nested NER task in the specific domains.\nSpecifically, the scenarios of low resource and class imbalance impede the wide\napplication for biomedical and industrial domains. In this study, we design a\nnovel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss\nand Multiclass loss. Our proposed method specially leverages the information of\nentity boundary and entity classification, thereby enhancing the model's\ncapacity to learn from a limited number of data samples. To validate the\nperformance of this innovative method in enhancing NER task, we conducted\nexperiments on three distinct biomedical NER datasets and one dataset\nconstructed by ourselves from industrial complex equipment maintenance\ndocuments. Comparing to strong baselines, our method demonstrates the\ncompetitive performance across all datasets. During the experimental analysis,\nour proposed method exhibits significant advancements in entity boundary\nrecognition and entity classification. Our code are available here.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u635f\u5931\u51fd\u6570EIoU-EMC\uff0c\u901a\u8fc7\u6539\u8fdbIoU\u635f\u5931\u548c\u591a\u7c7b\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u7279\u5b9a\u9886\u57df\u4e2d\u5d4c\u5957NER\u4efb\u52a1\u7684\u4f4e\u8d44\u6e90\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u7279\u5b9a\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u548c\u5de5\u4e1a\uff09\u4e2d\u7684\u5d4c\u5957NER\u4efb\u52a1\u9762\u4e34\u4f4e\u8d44\u6e90\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86EIoU-EMC\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u5b9e\u4f53\u8fb9\u754c\u548c\u5b9e\u4f53\u5206\u7c7b\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5c11\u91cf\u6570\u636e\u6837\u672c\u4e0a\u7684\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66NER\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5de5\u4e1a\u8bbe\u5907\u7ef4\u62a4\u6587\u6863\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5b9e\u4f53\u8fb9\u754c\u8bc6\u522b\u548c\u5206\u7c7b\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "EIoU-EMC\u65b9\u6cd5\u5728\u5d4c\u5957NER\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14138", "pdf": "https://arxiv.org/pdf/2504.14138", "abs": "https://arxiv.org/abs/2504.14138", "authors": ["Ghodsiyeh Rostami", "Po-Han Chen", "Mahdi S. Hosseini"], "title": "Segment Any Crack: Deep Semantic Segmentation Adaptation for Crack Detection", "categories": ["cs.CV"], "comment": null, "summary": "Image-based crack detection algorithms are increasingly in demand in\ninfrastructure monitoring, as early detection of cracks is of paramount\nimportance for timely maintenance planning. While deep learning has\nsignificantly advanced crack detection algorithms, existing models often\nrequire extensive labeled datasets and high computational costs for\nfine-tuning, limiting their adaptability across diverse conditions. This study\nintroduces an efficient selective fine-tuning strategy, focusing on tuning\nnormalization components, to enhance the adaptability of segmentation models\nfor crack detection. The proposed method is applied to the Segment Anything\nModel (SAM) and five well-established segmentation models. Experimental results\ndemonstrate that selective fine-tuning of only normalization parameters\noutperforms full fine-tuning and other common fine-tuning techniques in both\nperformance and computational efficiency, while improving generalization. The\nproposed approach yields a SAM-based model, Segment Any Crack (SAC), achieving\na 61.22\\% F1-score and 44.13\\% IoU on the OmniCrack30k benchmark dataset, along\nwith the highest performance across three zero-shot datasets and the lowest\nstandard deviation. The results highlight the effectiveness of the adaptation\napproach in improving segmentation accuracy while significantly reducing\ncomputational overhead.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565\uff0c\u4e13\u6ce8\u4e8e\u8c03\u6574\u5f52\u4e00\u5316\u7ec4\u4ef6\uff0c\u4ee5\u63d0\u9ad8\u5206\u5272\u6a21\u578b\u5728\u88c2\u7f1d\u68c0\u6d4b\u4e2d\u7684\u9002\u5e94\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5b8c\u5168\u5fae\u8c03\u548c\u5176\u4ed6\u5e38\u89c1\u5fae\u8c03\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u88c2\u7f1d\u68c0\u6d4b\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565\uff0c\u4ec5\u8c03\u6574\u5f52\u4e00\u5316\u53c2\u6570\uff0c\u5e94\u7528\u4e8eSAM\u548c\u4e94\u79cd\u5206\u5272\u6a21\u578b\u3002", "result": "\u5728OmniCrack30k\u6570\u636e\u96c6\u4e0a\uff0cSAC\u6a21\u578b\u8fbe\u523061.22% F1-score\u548c44.13% IoU\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u9009\u62e9\u6027\u5fae\u8c03\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2504.14212", "pdf": "https://arxiv.org/pdf/2504.14212", "abs": "https://arxiv.org/abs/2504.14212", "authors": ["Takuma Udagawa", "Yang Zhao", "Hiroshi Kanayama", "Bishwaranjan Bhattacharjee"], "title": "Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) acquire general linguistic knowledge from\nmassive-scale pretraining. However, pretraining data mainly comprised of\nweb-crawled texts contain undesirable social biases which can be perpetuated or\neven amplified by LLMs. In this study, we propose an efficient yet effective\nannotation pipeline to investigate social biases in the pretraining corpora.\nOur pipeline consists of protected attribute detection to identify diverse\ndemographics, followed by regard classification to analyze the language\npolarity towards each attribute. Through our experiments, we demonstrate the\neffect of our bias analysis and mitigation measures, focusing on Common Crawl\nas the most representative pretraining corpus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u7528\u4e8e\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u504f\u89c1\u5206\u6790\u548c\u7f13\u89e3\u63aa\u65bd\u7684\u6548\u679c\u3002", "motivation": "\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u53ef\u80fd\u88ab\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5ef6\u7eed\u6216\u653e\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u7f13\u89e3\u8fd9\u4e9b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6807\u6ce8\u6d41\u7a0b\uff0c\u5305\u62ec\u53d7\u4fdd\u62a4\u5c5e\u6027\u68c0\u6d4b\u548c\u8bed\u8a00\u6781\u6027\u5206\u7c7b\uff0c\u7528\u4e8e\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u504f\u89c1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u504f\u89c1\u5206\u6790\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u7f13\u89e3\u63aa\u65bd\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14139", "pdf": "https://arxiv.org/pdf/2504.14139", "abs": "https://arxiv.org/abs/2504.14139", "authors": ["Hai Pham-Ngoc", "De Nguyen-Van", "Dung Vu-Tien", "Phuong Le-Hong"], "title": "ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Background: Automated classification of thyroid fine needle aspiration biopsy\n(FNAB) images faces challenges in limited data, inter-observer variability, and\ncomputational cost. Efficient, interpretable models are crucial for clinical\nsupport. Objective: To develop and externally validate a deep learning system\nfor the multi-class classification of thyroid FNAB images into three key\ncategories that directly guide post-biopsy treatment decisions in Vietnam:\nbenign (B2), suspicious for malignancy (B5), and malignant (B6), while\nachieving high diagnostic accuracy with low computational overhead. Methods:\nOur framework features: (1) YOLOv10-based cell cluster detection for\ninformative sub-region extraction and noise reduction; (2) a curriculum\nlearning-inspired protocol sequencing localized crops to full images for\nmulti-scale feature capture; (3) adaptive lightweight EfficientNetB0 (4\nmillions parameters) selection balancing performance and efficiency; and (4) a\nTransformer-inspired module for multi-scale, multi-region analysis. External\nvalidation used 1,015 independent FNAB images. Results: ThyroidEffi Basic\nachieved a macro F1 of 89.19\\% and AUCs of 0.98 (B2), 0.95 (B5), and 0.96 (B6)\non the internal test set. External validation yielded AUCs of 0.9495 (B2),\n0.7436 (B5), and 0.8396 (B6). ThyroidEffi Premium improved macro F1 to 89.77\\%.\nGrad-CAM highlighted key diagnostic regions, confirming interpretability. The\nsystem processed 1000 cases in 30 seconds, demonstrating feasibility on widely\naccessible hardware like a 12-core CPU. Conclusions: This work demonstrates\nthat high-accuracy, interpretable thyroid FNAB image classification is\nachievable with minimal computational demands.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u7532\u72b6\u817a\u7ec6\u9488\u7a7f\u523a\u6d3b\u68c0\uff08FNAB\uff09\u56fe\u50cf\u7684\u591a\u7c7b\u5206\u7c7b\uff0c\u5e76\u5728\u8d8a\u5357\u8fdb\u884c\u4e86\u5916\u90e8\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u7532\u72b6\u817aFNAB\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u3001\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u7ed3\u5408YOLOv10\u7ec6\u80de\u7c07\u68c0\u6d4b\u3001\u8bfe\u7a0b\u5b66\u4e60\u534f\u8bae\u3001\u8f7b\u91cf\u7ea7EfficientNetB0\u548cTransformer\u6a21\u5757\uff0c\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u4e0e\u5206\u6790\u3002", "result": "\u5728\u5185\u90e8\u6d4b\u8bd5\u96c6\u4e0a\uff0cThyroidEffi Basic\u7684\u5b8fF1\u4e3a89.19%\uff0cAUC\u5206\u522b\u4e3a0.98\uff08B2\uff09\u30010.95\uff08B5\uff09\u548c0.96\uff08B6\uff09\uff1b\u5916\u90e8\u9a8c\u8bc1AUC\u4e3a0.9495\uff08B2\uff09\u30010.7436\uff08B5\uff09\u548c0.8396\uff08B6\uff09\u3002ThyroidEffi Premium\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u7cbe\u5ea6\u3001\u53ef\u89e3\u91ca\u7684\u7532\u72b6\u817aFNAB\u56fe\u50cf\u5206\u7c7b\u53ef\u4ee5\u5728\u4f4e\u8ba1\u7b97\u9700\u6c42\u4e0b\u5b9e\u73b0\u3002"}}
{"id": "2504.14218", "pdf": "https://arxiv.org/pdf/2504.14218", "abs": "https://arxiv.org/abs/2504.14218", "authors": ["Junchi Yao", "Shu Yang", "Jianhua Xu", "Lijie Hu", "Mengdi Li", "Di Wang"], "title": "Understanding the Repeat Curse in Large Language Models from a Feature Perspective", "categories": ["cs.CL"], "comment": "Submitted to ACL 2025", "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cDuplicatus Charm\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u91cd\u590d\u6587\u672c\u751f\u6210\u95ee\u9898\uff0c\u5e76\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u548c\u6291\u5236\u91cd\u590d\u7279\u5f81\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5e38\u51fa\u73b0\u91cd\u590d\u6587\u672c\u751f\u6210\u7684\u95ee\u9898\uff08\u79f0\u4e3a\u201c\u91cd\u590d\u8bc5\u5492\u201d\uff09\uff0c\u5176\u6839\u672c\u673a\u5236\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u63d0\u53d6\u5355\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u51fa\u201cDuplicatus Charm\u201d\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u5e76\u64cd\u7eb5\u91cd\u590d\u7279\u5f81\u3002", "result": "\u6784\u5efa\u4e86\u91cd\u590d\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u91cd\u590d\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u6291\u5236\u8fd9\u4e9b\u7279\u5f81\u6709\u6548\u7f13\u89e3\u4e86\u91cd\u590d\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u91cd\u590d\u751f\u6210\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2504.14151", "pdf": "https://arxiv.org/pdf/2504.14151", "abs": "https://arxiv.org/abs/2504.14151", "authors": ["Sergio Arnaud", "Paul McVay", "Ada Martin", "Arjun Majumdar", "Krishna Murthy Jatavallabhula", "Phillip Thomas", "Ruslan Partsey", "Daniel Dugas", "Abha Gejji", "Alexander Sax", "Vincent-Pierre Berges", "Mikael Henaff", "Ayush Jain", "Ang Cao", "Ishita Prasad", "Mrinal Kalakrishnan", "Michael Rabbat", "Nicolas Ballas", "Mido Assran", "Oleksandr Maksymets", "Aravind Rajeswaran", "Franziska Meier"], "title": "Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.10; I.2.6; I.2.9; I.3.7; I.4.6; I.4.8"], "comment": null, "summary": "We present LOCATE 3D, a model for localizing objects in 3D scenes from\nreferring expressions like \"the small coffee table between the sofa and the\nlamp.\" LOCATE 3D sets a new state-of-the-art on standard referential grounding\nbenchmarks and showcases robust generalization capabilities. Notably, LOCATE 3D\noperates directly on sensor observation streams (posed RGB-D frames), enabling\nreal-world deployment on robots and AR devices. Key to our approach is 3D-JEPA,\na novel self-supervised learning (SSL) algorithm applicable to sensor point\nclouds. It takes as input a 3D pointcloud featurized using 2D foundation models\n(CLIP, DINO). Subsequently, masked prediction in latent space is employed as a\npretext task to aid the self-supervised learning of contextualized pointcloud\nfeatures. Once trained, the 3D-JEPA encoder is finetuned alongside a\nlanguage-conditioned decoder to jointly predict 3D masks and bounding boxes.\nAdditionally, we introduce LOCATE 3D DATASET, a new dataset for 3D referential\ngrounding, spanning multiple capture setups with over 130K annotations. This\nenables a systematic study of generalization capabilities as well as a stronger\nmodel.", "AI": {"tldr": "LOCATE 3D\u662f\u4e00\u4e2a\u901a\u8fc7\u63cf\u8ff0\u6027\u8bed\u8a00\u5b9a\u4f4d3D\u573a\u666f\u4e2d\u7269\u4f53\u7684\u6a21\u578b\uff0c\u91c7\u75283D-JEPA\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\u548c\u63a9\u7801\u9884\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u4e2d\u57fa\u4e8e\u8bed\u8a00\u63cf\u8ff0\u7684\u7269\u4f53\u5b9a\u4f4d\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u548cAR\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u4f7f\u75283D-JEPA\u81ea\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u54082D\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u3001DINO\uff09\u5bf93D\u70b9\u4e91\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u9884\u6d4b\u4efb\u52a1\u5b66\u4e60\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u8bad\u7ec3\u540e\u7684\u7f16\u7801\u5668\u4e0e\u8bed\u8a00\u6761\u4ef6\u89e3\u7801\u5668\u8054\u5408\u9884\u6d4b3D\u63a9\u7801\u548c\u8fb9\u754c\u6846\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b13\u4e07\u6807\u6ce8\u7684\u65b0\u6570\u636e\u96c6LOCATE 3D DATASET\u3002", "conclusion": "LOCATE 3D\u57283D\u7269\u4f53\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14223", "pdf": "https://arxiv.org/pdf/2504.14223", "abs": "https://arxiv.org/abs/2504.14223", "authors": ["Michael F\u00e4rber", "Parisa Aghdam", "Kyuri Im", "Mario Tawfelis", "Hardik Ghoshal"], "title": "SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "accepted at ECIR 2025", "summary": "Text simplification is essential for making complex content accessible to\ndiverse audiences who face comprehension challenges. Yet, the limited\navailability of simplified materials creates significant barriers to personal\nand professional growth and hinders social inclusion. Although researchers have\nexplored various methods for automatic text simplification, none fully leverage\nlarge language models (LLMs) to offer tailored customization for different\ntarget groups and varying levels of simplicity. Moreover, despite its proven\nbenefits for both consumers and organizations, the well-established practice of\nplain language remains underutilized. In this paper, we\nhttps://simplifymytext.org, the first system designed to produce plain language\ncontent from multiple input formats, including typed text and file uploads,\nwith flexible customization options for diverse audiences. We employ GPT-4 and\nLlama-3 and evaluate outputs across multiple metrics. Overall, our work\ncontributes to research on automatic text simplification and highlights the\nimportance of tailored communication in promoting inclusivity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8eGPT-4\u548cLlama-3\u7684\u591a\u683c\u5f0f\u8f93\u5165\u6587\u672c\u7b80\u5316\u7cfb\u7edf\uff0c\u652f\u6301\u5b9a\u5236\u5316\u8f93\u51fa\uff0c\u65e8\u5728\u63d0\u5347\u5305\u5bb9\u6027\u3002", "motivation": "\u590d\u6742\u6587\u672c\u5bf9\u7406\u89e3\u56f0\u96be\u7fa4\u4f53\u6784\u6210\u969c\u788d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b9a\u5236\u5316\u7b80\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u652f\u6301\u591a\u683c\u5f0f\u8f93\u5165\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528GPT-4\u548cLlama-3\u751f\u6210\u7b80\u5316\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u591a\u6307\u6807\u8bc4\u4f30\u8f93\u51fa\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u751f\u6210\u5b9a\u5236\u5316\u7b80\u5316\u6587\u672c\uff0c\u9a8c\u8bc1\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u7b80\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63a8\u52a8\u4e86\u81ea\u52a8\u6587\u672c\u7b80\u5316\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u5b9a\u5236\u5316\u6c9f\u901a\u5bf9\u5305\u5bb9\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.14178", "pdf": "https://arxiv.org/pdf/2504.14178", "abs": "https://arxiv.org/abs/2504.14178", "authors": ["Yijie Li", "Hewei Wang", "Jiayi Zhang", "Jinjiang You", "Jinfeng Xu", "Puzhen Wu", "Yunzhong Xiao", "Soumyabrata Dev"], "title": "Segregation and Context Aggregation Network for Real-time Cloud Segmentation", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Cloud segmentation from intensity images is a pivotal task in atmospheric\nscience and computer vision, aiding weather forecasting and climate analysis.\nGround-based sky/cloud segmentation extracts clouds from images for further\nfeature analysis. Existing methods struggle to balance segmentation accuracy\nand computational efficiency, limiting real-world deployment on edge devices,\nso we introduce SCANet, a novel lightweight cloud segmentation model featuring\nSegregation and Context Aggregation Module (SCAM), which refines rough\nsegmentation maps into weighted sky and cloud features processed separately.\nSCANet achieves state-of-the-art performance while drastically reducing\ncomputational complexity. SCANet-large (4.29M) achieves comparable accuracy to\nstate-of-the-art methods with 70.9% fewer parameters. Meanwhile, SCANet-lite\n(90K) delivers 1390 fps in FP16, surpassing real-time standards. Additionally,\nwe propose an efficient pre-training strategy that enhances performance even\nwithout ImageNet pre-training.", "AI": {"tldr": "SCANet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e91\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7Segregation and Context Aggregation Module\uff08SCAM\uff09\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u9650\u5236\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "SCANet\u91c7\u7528SCAM\u6a21\u5757\uff0c\u5c06\u7c97\u7565\u5206\u5272\u56fe\u7ec6\u5316\u4e3a\u52a0\u6743\u7684\u5929\u7a7a\u548c\u4e91\u7279\u5f81\uff0c\u5e76\u5206\u522b\u5904\u7406\u3002", "result": "SCANet-large\u53c2\u6570\u51cf\u5c1170.9%\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff1bSCANet-lite\u8fbe\u52301390 fps\uff08FP16\uff09\uff0c\u8fdc\u8d85\u5b9e\u65f6\u6807\u51c6\u3002", "conclusion": "SCANet\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u4e91\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2504.14225", "pdf": "https://arxiv.org/pdf/2504.14225", "abs": "https://arxiv.org/abs/2504.14225", "authors": ["Bowen Jiang", "Zhuoqun Hao", "Young-Min Cho", "Bryan Li", "Yuan Yuan", "Sihao Chen", "Lyle Ungar", "Camillo J. Taylor", "Dan Roth"], "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as personalized assistants for\nusers across a wide range of tasks -- from offering writing support to\ndelivering tailored recommendations or consultations. Over time, the\ninteraction history between a user and an LLM can provide extensive information\nabout an individual's traits and preferences. However, open questions remain on\nhow well LLMs today can effectively leverage such history to (1) internalize\nthe user's inherent traits and preferences, (2) track how the user profiling\nand preferences evolve over time, and (3) generate personalized responses\naccordingly in new scenarios.\n  In this work, we introduce the PERSONAMEM benchmark. PERSONAMEM features\ncurated user profiles with over 180 simulated user-LLM interaction histories,\neach containing up to 60 sessions of multi-turn conversations across 15\nreal-world tasks that require personalization. Given an in-situ user query,\ni.e. query issued by the user from the first-person perspective, we evaluate\nLLM chatbots' ability to identify the most suitable response according to the\ncurrent state of the user's profile. We observe that current LLMs still\nstruggle to recognize the dynamic evolution in users' profiles over time\nthrough direct prompting approaches. As a consequence, LLMs often fail to\ndeliver responses that align with users' current situations and preferences,\nwith frontier models such as GPT-4.1, o4-mini, GPT-4.5, o1, or Gemini-2.0\nachieving only around 50% overall accuracy, suggesting room for improvement. We\nhope that PERSONAMEM, along with the user profile and conversation simulation\npipeline, can facilitate future research in the development of truly user-aware\nchatbots. Code and data are available at github.com/bowen-upenn/PersonaMem.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PERSONAMEM\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5982\u4f55\u5229\u7528\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u6765\u4e2a\u6027\u5316\u54cd\u5e94\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u52a8\u6001\u8ddf\u8e2a\u7528\u6237\u504f\u597d\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76LLMs\u5982\u4f55\u5229\u7528\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u6570\u636e\u6765\u66f4\u597d\u5730\u7406\u89e3\u7528\u6237\u7279\u8d28\u548c\u504f\u597d\uff0c\u5e76\u751f\u6210\u4e2a\u6027\u5316\u54cd\u5e94\u3002", "method": "\u63d0\u51faPERSONAMEM\u57fa\u51c6\uff0c\u5305\u542b180\u4e2a\u6a21\u62df\u7528\u6237-LLM\u4ea4\u4e92\u5386\u53f2\uff0c\u8bc4\u4f30LLMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u4e2a\u6027\u5316\u54cd\u5e94\u80fd\u529b\u3002", "result": "\u5f53\u524dLLMs\u5728\u52a8\u6001\u8ddf\u8e2a\u7528\u6237\u504f\u597d\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u524d\u6cbf\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u7ea650%\u3002", "conclusion": "PERSONAMEM\u57fa\u51c6\u548c\u6a21\u62df\u5de5\u5177\u53ef\u4e3a\u672a\u6765\u5f00\u53d1\u66f4\u7528\u6237\u611f\u77e5\u7684\u804a\u5929\u673a\u5668\u4eba\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2504.14200", "pdf": "https://arxiv.org/pdf/2504.14200", "abs": "https://arxiv.org/abs/2504.14200", "authors": ["Huiyi Chen", "Jiawei Peng", "Kaihua Tang", "Xin Geng", "Xu Yang"], "title": "Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "In-context learning (ICL) enables Large Vision-Language Models (LVLMs) to\nadapt to new tasks without parameter updates, using a few demonstrations from a\nlarge support set. However, selecting informative demonstrations leads to high\ncomputational and memory costs. While some methods explore selecting a small\nand representative coreset in the text classification, evaluating all support\nset samples remains costly, and discarded samples lead to unnecessary\ninformation loss. These methods may also be less effective for image\nclassification due to differences in feature spaces. Given these limitations,\nwe propose Key-based Coreset Optimization (KeCO), a novel framework that\nleverages untapped data to construct a compact and informative coreset. We\nintroduce visual features as keys within the coreset, which serve as the anchor\nfor identifying samples to be updated through different selection strategies.\nBy leveraging untapped samples from the support set, we update the keys of\nselected coreset samples, enabling the randomly initialized coreset to evolve\ninto a more informative coreset under low computational cost. Through extensive\nexperiments on coarse-grained and fine-grained image classification benchmarks,\nwe demonstrate that KeCO effectively enhances ICL performance for image\nclassification task, achieving an average improvement of more than 20\\%.\nNotably, we evaluate KeCO under a simulated online scenario, and the strong\nperformance in this scenario highlights the practical value of our framework\nfor resource-constrained real-world scenarios.", "AI": {"tldr": "KeCO\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u7684\u952e\u503c\u6838\u5fc3\u96c6\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u9009\u62e9\u6838\u5fc3\u96c6\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u4fe1\u606f\u635f\u5931\u4e25\u91cd\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faKeCO\u6846\u67b6\uff0c\u5229\u7528\u672a\u4f7f\u7528\u7684\u6570\u636e\u6784\u5efa\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u6838\u5fc3\u96c6\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u952e\u503c\u4f18\u5316\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cKeCO\u5e73\u5747\u6027\u80fd\u63d0\u5347\u8d85\u8fc720%\uff0c\u4e14\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "KeCO\u4e3a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2504.14287", "pdf": "https://arxiv.org/pdf/2504.14287", "abs": "https://arxiv.org/abs/2504.14287", "authors": ["Demetris Paschalides", "George Pallis", "Marios D. Dikaiakos"], "title": "Probing the Subtle Ideological Manipulation of Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\u5149\u8c31\u4e0a\u7684\u53ef\u64cd\u7eb5\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5de6\u53f3\u4e8c\u5206\u6cd5\uff0c\u63d0\u51fa\u4e86\u591a\u4efb\u52a1\u6570\u636e\u96c6\u5e76\u9a8c\u8bc1\u4e86\u5fae\u8c03\u5bf9\u6a21\u578b\u610f\u8bc6\u5f62\u6001\u8868\u8fbe\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22LLMs\u5728\u653f\u6cbb\u610f\u8bc6\u5f62\u6001\u4e0a\u7684\u53ef\u64cd\u7eb5\u6027\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u5de6\u53f3\u4e8c\u5206\u6cd5\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u5176\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u591a\u4efb\u52a1\u6570\u636e\u96c6\uff08\u5982\u610f\u8bc6\u5f62\u6001\u95ee\u7b54\u3001\u5ba3\u8a00\u586b\u7a7a\u7b49\uff09\uff0c\u5e76\u5bf9Phi-2\u3001Mistral\u548cLlama-3\u4e09\u79cdLLMs\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5fae\u8c03\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5bf9\u610f\u8bc6\u5f62\u6001\u7684\u7ec6\u81f4\u8868\u8fbe\uff0c\u800c\u663e\u5f0f\u63d0\u793a\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLMs\u6613\u53d7\u610f\u8bc6\u5f62\u6001\u64cd\u7eb5\uff0c\u9700\u52a0\u5f3a\u9632\u62a4\u63aa\u65bd\u3002"}}
{"id": "2504.14202", "pdf": "https://arxiv.org/pdf/2504.14202", "abs": "https://arxiv.org/abs/2504.14202", "authors": ["Zichuan Liu", "Liming Jiang", "Qing Yan", "Yumin Jia", "Hao Kang", "Xin Lu"], "title": "Learning Joint ID-Textual Representation for ID-Preserving Image Synthesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel framework for ID-preserving generation using a multi-modal\nencoding strategy rather than injecting identity features via adapters into\npre-trained models. Our method treats identity and text as a unified\nconditioning input. To achieve this, we introduce FaceCLIP, a multi-modal\nencoder that learns a joint embedding space for both identity and textual\nsemantics. Given a reference face and a text prompt, FaceCLIP produces a\nunified representation that encodes both identity and text, which conditions a\nbase diffusion model to generate images that are identity-consistent and\ntext-aligned. We also present a multi-modal alignment algorithm to train\nFaceCLIP, using a loss that aligns its joint representation with face, text,\nand image embedding spaces. We then build FaceCLIP-SDXL, an ID-preserving image\nsynthesis pipeline by integrating FaceCLIP with Stable Diffusion XL (SDXL).\nCompared to prior methods, FaceCLIP-SDXL enables photorealistic portrait\ngeneration with better identity preservation and textual relevance. Extensive\nexperiments demonstrate its quantitative and qualitative superiority.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u7f16\u7801\u7684ID\u4fdd\u7559\u751f\u6210\u6846\u67b6FaceCLIP\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u8eab\u4efd\u4e0e\u6587\u672c\u7684\u7edf\u4e00\u8f93\u5165\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u8eab\u4efd\u4e00\u81f4\u4e14\u6587\u672c\u5bf9\u9f50\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u9002\u914d\u5668\u6ce8\u5165\u8eab\u4efd\u7279\u5f81\uff0c\u9650\u5236\u4e86\u751f\u6210\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u7f16\u7801\u7b56\u7565\u63d0\u5347\u8eab\u4efd\u4fdd\u7559\u548c\u6587\u672c\u5bf9\u9f50\u80fd\u529b\u3002", "method": "\u5f15\u5165FaceCLIP\u591a\u6a21\u6001\u7f16\u7801\u5668\uff0c\u5b66\u4e60\u8eab\u4efd\u4e0e\u6587\u672c\u7684\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u8bbe\u8ba1\u591a\u6a21\u6001\u5bf9\u9f50\u7b97\u6cd5\u8bad\u7ec3FaceCLIP\u3002", "result": "FaceCLIP-SDXL\u5728\u8eab\u4efd\u4fdd\u7559\u548c\u6587\u672c\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u903c\u771f\u7684\u8096\u50cf\u56fe\u50cf\u3002", "conclusion": "FaceCLIP-SDXL\u901a\u8fc7\u591a\u6a21\u6001\u7f16\u7801\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86ID\u4fdd\u7559\u751f\u6210\u7684\u6548\u679c\uff0c\u4e3a\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14321", "pdf": "https://arxiv.org/pdf/2504.14321", "abs": "https://arxiv.org/abs/2504.14321", "authors": ["Xingyu Li", "Chen Gong", "Guohong Fu"], "title": "Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources.To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86TikTalkCoref\uff0c\u9996\u4e2a\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u591a\u6a21\u6001\u5171\u6307\u6d88\u89e3\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5171\u6307\u6d88\u89e3\uff08MCR\uff09\u5bf9\u7406\u89e3\u591a\u6a21\u6001\u5185\u5bb9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u7684\u6570\u636e\u8d44\u6e90\u3002", "method": "\u4ece\u6296\u97f3\u5e73\u53f0\u6536\u96c6\u77ed\u89c6\u9891\u4e0e\u6587\u672c\u5bf9\u8bdd\uff0c\u624b\u52a8\u6807\u6ce8\u5171\u6307\u7c07\uff0c\u5e76\u63d0\u51fa\u57fa\u51c6\u65b9\u6cd5\u3002", "result": "\u6784\u5efa\u4e86TikTalkCoref\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u51c6\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "TikTalkCoref\u5c06\u4fc3\u8fdb\u793e\u4ea4\u5a92\u4f53\u591a\u6a21\u6001\u5171\u6307\u6d88\u89e3\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2504.14221", "pdf": "https://arxiv.org/pdf/2504.14221", "abs": "https://arxiv.org/abs/2504.14221", "authors": ["Wenbing Zhu", "Lidong Wang", "Ziqing Zhou", "Chengjie Wang", "Yurui Pan", "Ruoyi Zhang", "Zhuhao Chen", "Linjie Cheng", "Bin-Bin Gao", "Jiangning Zhang", "Zhenye Gan", "Yuxie Wang", "Yulong Chen", "Shuguang Qian", "Mingmin Chi", "Bo Peng", "Lizhuang Ma"], "title": "Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": "13 pages. Dataset and code: https://realiad4ad.github.io/Real-IAD D3", "summary": "The increasing complexity of industrial anomaly detection (IAD) has\npositioned multimodal detection methods as a focal area of machine vision\nresearch. However, dedicated multimodal datasets specifically tailored for IAD\nremain limited. Pioneering datasets like MVTec 3D have laid essential\ngroundwork in multimodal IAD by incorporating RGB+3D data, but still face\nchallenges in bridging the gap with real industrial environments due to\nlimitations in scale and resolution. To address these challenges, we introduce\nReal-IAD D3, a high-precision multimodal dataset that uniquely incorporates an\nadditional pseudo3D modality generated through photometric stereo, alongside\nhigh-resolution RGB images and micrometer-level 3D point clouds. Real-IAD D3\nfeatures finer defects, diverse anomalies, and greater scale across 20\ncategories, providing a challenging benchmark for multimodal IAD Additionally,\nwe introduce an effective approach that integrates RGB, point cloud, and\npseudo-3D depth information to leverage the complementary strengths of each\nmodality, enhancing detection performance. Our experiments highlight the\nimportance of these modalities in boosting detection robustness and overall IAD\nperformance. The dataset and code are publicly accessible for research purposes\nat https://realiad4ad.github.io/Real-IAD D3", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Real-IAD D3\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7cbe\u5ea6\u591a\u6a21\u6001\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5305\u542bRGB\u56fe\u50cf\u3001\u5fae\u7c73\u7ea73D\u70b9\u4e91\u548c\u901a\u8fc7\u5149\u5ea6\u7acb\u4f53\u751f\u6210\u7684\u4f2a3D\u6a21\u6001\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\uff08\u5982MVTec 3D\uff09\u5728\u89c4\u6a21\u548c\u5206\u8fa8\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6a21\u62df\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6765\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faReal-IAD D3\u6570\u636e\u96c6\uff0c\u5305\u542bRGB\u30013D\u70b9\u4e91\u548c\u4f2a3D\u6a21\u6001\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u6a21\u6001\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "conclusion": "Real-IAD D3\u4e3a\u591a\u6a21\u6001\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u57fa\u51c6\uff0c\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366", "abs": "https://arxiv.org/abs/2504.14366", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4eceTransformer\u6559\u5e08\u6a21\u578b\u5230\u4e5d\u79cd\u5b50\u4e8c\u6b21\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\u6548\u679c\uff0c\u63a2\u8ba8\u4e86\u4e0d\u540c\u67b6\u6784\u5bf9\u84b8\u998f\u8fc7\u7a0b\u7684\u5f71\u54cd\u53ca\u521d\u59cb\u5316\u7b56\u7565\u7684\u4f5c\u7528\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u63a8\u7406\u65f6\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u6210\u4e3a\u74f6\u9888\uff0c\u4fc3\u4f7f\u63a2\u7d22\u5b50\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6848\uff08\u5982SSMs\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u5faa\u73af\u67b6\u6784\uff09\u7684\u77e5\u8bc6\u84b8\u998f\u6548\u679c\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4eceTransformer\u6559\u5e08\u6a21\u578b\u5230\u4e5d\u79cd\u5b50\u4e8c\u6b21\u5b66\u751f\u6a21\u578b\u7684\u77e5\u8bc6\u84b8\u998f\uff0c\u7814\u7a76\u4e86\u77e9\u9635\u6df7\u5408\u548cQKV\u590d\u5236\u7b49\u521d\u59cb\u5316\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2aNLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u8bc1\u7ed3\u679c\u63ed\u793a\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u6307\u51fa\u4e86\u6210\u529f\u77e5\u8bc6\u8f6c\u79fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b50\u4e8c\u6b21\u67b6\u6784\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u67b6\u6784\u9009\u62e9\u548c\u521d\u59cb\u5316\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.14224", "pdf": "https://arxiv.org/pdf/2504.14224", "abs": "https://arxiv.org/abs/2504.14224", "authors": ["Yongguang Li", "Jindong Li", "Qi Wang", "Qianli Xing", "Runliang Niu", "Shengsheng Wang", "Menglin Yang"], "title": "Revisiting CLIP for SF-OSDA: Unleashing Zero-Shot Potential with Adaptive Threshold and Training-Free Feature Filtering", "categories": ["cs.CV"], "comment": null, "summary": "Source-Free Unsupervised Open-Set Domain Adaptation (SF-OSDA) methods using\nCLIP face significant issues: (1) while heavily dependent on domain-specific\nthreshold selection, existing methods employ simple fixed thresholds,\nunderutilizing CLIP's zero-shot potential in SF-OSDA scenarios; and (2)\noverlook intrinsic class tendencies while employing complex training to enforce\nfeature separation, incurring deployment costs and feature shifts that\ncompromise CLIP's generalization ability. To address these issues, we propose\nCLIPXpert, a novel SF-OSDA approach that integrates two key components: an\nadaptive thresholding strategy and an unknown class feature filtering module.\nSpecifically, the Box-Cox GMM-Based Adaptive Thresholding (BGAT) module\ndynamically determines the optimal threshold by estimating sample score\ndistributions, balancing known class recognition and unknown class sample\ndetection. Additionally, the Singular Value Decomposition (SVD)-Based\nUnknown-Class Feature Filtering (SUFF) module reduces the tendency of unknown\nclass samples towards known classes, improving the separation between known and\nunknown classes. Experiments show that our source-free and training-free method\noutperforms state-of-the-art trained approach UOTA by 1.92% on the DomainNet\ndataset, achieves SOTA-comparable performance on datasets such as Office-Home,\nand surpasses other SF-OSDA methods. This not only validates the effectiveness\nof our proposed method but also highlights CLIP's strong zero-shot potential\nfor SF-OSDA tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCLIPXpert\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u672a\u77e5\u7c7b\u7279\u5f81\u8fc7\u6ee4\u6a21\u5757\u89e3\u51b3SF-OSDA\u4e2dCLIP\u7684\u9608\u503c\u9009\u62e9\u548c\u7279\u5f81\u5206\u79bb\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709SF-OSDA\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u9608\u503c\u4e14\u5ffd\u7565\u7c7b\u5185\u8d8b\u52bf\uff0c\u5bfc\u81f4CLIP\u6f5c\u529b\u672a\u5145\u5206\u53d1\u6325\u3002", "method": "\u63d0\u51faBGAT\u6a21\u5757\u52a8\u6001\u786e\u5b9a\u9608\u503c\uff0cSUFF\u6a21\u5757\u8fc7\u6ee4\u672a\u77e5\u7c7b\u7279\u5f81\uff0c\u63d0\u5347\u5df2\u77e5\u4e0e\u672a\u77e5\u7c7b\u7684\u5206\u79bb\u3002", "result": "\u5728DomainNet\u4e0a\u4f18\u4e8eUOTA 1.92%\uff0c\u5728Office-Home\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "CLIPXpert\u9a8c\u8bc1\u4e86CLIP\u5728SF-OSDA\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u6f5c\u529b\uff0c\u65b9\u6cd5\u6709\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u3002"}}
{"id": "2504.14367", "pdf": "https://arxiv.org/pdf/2504.14367", "abs": "https://arxiv.org/abs/2504.14367", "authors": ["Gabriel Machado Santos", "Rita Maria da Silva Julia", "Marcelo Zanchetta do Nascimento"], "title": "Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages Accepted for publication in IEEE CEC 2025", "summary": "Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff08CFG\uff09\u548cMAP-Elites\u7b97\u6cd5\u7684\u8fdb\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u63a2\u7d22\u63d0\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u63d0\u793a\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u4e0d\u540c\u4efb\u52a1\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u63d0\u793a\u5de5\u7a0b\u5bf9\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u63d0\u793a\u7ed3\u6784\u4e0e\u4efb\u52a1\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u91c7\u7528CFG\u548cMAP-Elites\u7b97\u6cd5\uff0c\u7cfb\u7edf\u63a2\u7d22\u63d0\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u591a\u6837\u5316\u4e14\u9ad8\u6027\u80fd\u7684\u63d0\u793a\uff0c\u5e76\u5206\u6790\u5176\u4e0e\u4efb\u52a1\u7684\u5339\u914d\u5ea6\u3002", "result": "\u5728\u591a\u4e2aLLM\u548c\u4e03\u9879BigBench Lite\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u63d0\u793a\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6620\u5c04\u8868\u578b\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u53d8\u5316\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u4efb\u52a1\u7279\u5b9a\u548c\u9002\u5e94\u6027\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2504.14231", "pdf": "https://arxiv.org/pdf/2504.14231", "abs": "https://arxiv.org/abs/2504.14231", "authors": ["Johannes Spoecklberger", "Wei Lin", "Pedro Hermosilla", "Sivan Doveh", "Horst Possegger", "M. Jehanzeb Mirza"], "title": "Exploring Modality Guidance to Enhance VFM-based Feature Fusion for UDA in 3D Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Vision Foundation Models (VFMs) have become a de facto choice for many\ndownstream vision tasks, like image classification, image segmentation, and\nobject localization. However, they can also provide significant utility for\ndownstream 3D tasks that can leverage the cross-modal information (e.g., from\npaired image data). In our work, we further explore the utility of VFMs for\nadapting from a labeled source to unlabeled target data for the task of\nLiDAR-based 3D semantic segmentation. Our method consumes paired 2D-3D (image\nand point cloud) data and relies on the robust (cross-domain) features from a\nVFM to train a 3D backbone on a mix of labeled source and unlabeled target\ndata. At the heart of our method lies a fusion network that is guided by both\nthe image and point cloud streams, with their relative contributions adjusted\nbased on the target domain. We extensively compare our proposed methodology\nwith different state-of-the-art methods in several settings and achieve strong\nperformance gains. For example, achieving an average improvement of 6.5 mIoU\n(over all tasks), when compared with the previous state-of-the-art.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u5728LiDAR 3D\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u878d\u54082D-3D\u6570\u636e\u63d0\u5347\u6027\u80fd\u3002", "motivation": "VFMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u5176\u57283D\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u5229\u7528VFMs\u7684\u8de8\u6a21\u6001\u4fe1\u606f\u63d0\u53473D\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u54082D\u56fe\u50cf\u548c3D\u70b9\u4e91\u6570\u636e\uff0c\u52a8\u6001\u8c03\u6574\u6a21\u6001\u8d21\u732e\uff0c\u5229\u7528VFMs\u7684\u7279\u5f81\u8bad\u7ec33D\u4e3b\u5e72\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u53476.5 mIoU\u3002", "conclusion": "VFMs\u57283D\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u8de8\u6a21\u6001\u878d\u5408\u662f\u63d0\u5347\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2504.14452", "pdf": "https://arxiv.org/pdf/2504.14452", "abs": "https://arxiv.org/abs/2504.14452", "authors": ["Tong Chen", "Faeze Brahman", "Jiacheng Liu", "Niloofar Mireshghallah", "Weijia Shi", "Pang Wei Koh", "Luke Zettlemoyer", "Hannaneh Hajishirzi"], "title": "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).", "AI": {"tldr": "ParaPO\u662f\u4e00\u79cd\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u610f\u590d\u5236\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6574\u4f53\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u5728\u975e\u5bf9\u6297\u6027\u573a\u666f\u4e0b\u8bb0\u5fc6\u5e76\u590d\u5236\u9884\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u6d89\u53ca\u7248\u6743\u3001\u6284\u88ad\u3001\u9690\u79c1\u548c\u521b\u9020\u529b\u7b49\u62c5\u5fe7\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\uff0c\u4f7f\u5176\u504f\u597d\u5bf9\u8bb0\u5fc6\u5185\u5bb9\u7684\u6539\u5199\u7248\u672c\u800c\u975e\u539f\u6587\uff0c\u5e76\u5f00\u53d1\u7cfb\u7edf\u63d0\u793a\u53d8\u4f53\u4ee5\u63a7\u5236\u590d\u5236\u884c\u4e3a\u3002", "result": "\u5728Llama3.1-8B\u548cTulu3-8B\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cParaPO\u663e\u8457\u51cf\u5c11\u590d\u5236\u884c\u4e3a\uff0c\u4e14\u7cfb\u7edf\u63d0\u793a\u53d8\u4f53\u80fd\u4fdd\u7559\u540d\u8a00\u5f15\u7528\u80fd\u529b\u3002", "conclusion": "ParaPO\u6709\u6548\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u610f\u590d\u5236\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.14238", "pdf": "https://arxiv.org/pdf/2504.14238", "abs": "https://arxiv.org/abs/2504.14238", "authors": ["Lu Pan", "Yu-Hsuan Huang", "Hongxia Xie", "Cheng Zhang", "Hongwei Zhao", "Hong-Han Shuai", "Wen-Huang Cheng"], "title": "Single Document Image Highlight Removal via A Large-Scale Real-World Dataset and A Location-Aware Network", "categories": ["cs.CV"], "comment": "main paper with 8 pages, conference", "summary": "Reflective documents often suffer from specular highlights under ambient\nlighting, severely hindering text readability and degrading overall visual\nquality. Although recent deep learning methods show promise in highlight\nremoval, they remain suboptimal for document images, primarily due to the lack\nof dedicated datasets and tailored architectural designs. To tackle these\nchallenges, we present DocHR14K, a large-scale real-world dataset comprising\n14,902 high-resolution image pairs across six document categories and various\nlighting conditions. To the best of our knowledge, this is the first\nhigh-resolution dataset for document highlight removal that captures a wide\nrange of real-world lighting conditions. Additionally, motivated by the\nobservation that the residual map between highlighted and clean images\nnaturally reveals the spatial structure of highlight regions, we propose a\nsimple yet effective Highlight Location Prior (HLP) to estimate highlight masks\nwithout human annotations. Building on this prior, we present the\nLocation-Aware Laplacian Pyramid Highlight Removal Network (L2HRNet), which\neffectively removes highlights by leveraging estimated priors and incorporates\ndiffusion module to restore details. Extensive experiments demonstrate that\nDocHR14K improves highlight removal under diverse lighting conditions. Our\nL2HRNet achieves state-of-the-art performance across three benchmark datasets,\nincluding a 5.01\\% increase in PSNR and a 13.17\\% reduction in RMSE on\nDocHR14K.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDocHR14K\u6570\u636e\u96c6\u548cL2HRNet\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u6863\u56fe\u50cf\u4e2d\u9ad8\u5149\u53bb\u9664\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u9ad8\u5149\u4e25\u91cd\u5f71\u54cd\u6587\u672c\u53ef\u8bfb\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u548c\u9488\u5bf9\u6027\u8bbe\u8ba1\u800c\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDocHR14K\u6570\u636e\u96c6\u548c\u57fa\u4e8eHighlight Location Prior (HLP)\u7684L2HRNet\u7f51\u7edc\uff0c\u5229\u7528\u6b8b\u5dee\u56fe\u4f30\u8ba1\u9ad8\u5149\u533a\u57df\uff0c\u7ed3\u5408\u62c9\u666e\u62c9\u65af\u91d1\u5b57\u5854\u548c\u6269\u6563\u6a21\u5757\u6062\u590d\u7ec6\u8282\u3002", "result": "DocHR14K\u663e\u8457\u63d0\u5347\u9ad8\u5149\u53bb\u9664\u6548\u679c\uff0cL2HRNet\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0cPSNR\u63d0\u9ad85.01%\uff0cRMSE\u964d\u4f4e13.17%\u3002", "conclusion": "DocHR14K\u548cL2HRNet\u4e3a\u6587\u6863\u9ad8\u5149\u53bb\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14462", "pdf": "https://arxiv.org/pdf/2504.14462", "abs": "https://arxiv.org/abs/2504.14462", "authors": ["Armin Toroghi", "Willis Guo", "Scott Sanner"], "title": "CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "The rise of Large Language Models (LLMs) has redefined the AI landscape,\nparticularly due to their ability to encode factual and commonsense knowledge,\nand their outstanding performance in tasks requiring reasoning. Despite these\nadvances, hallucinations and reasoning errors remain a significant barrier to\ntheir deployment in high-stakes settings. In this work, we observe that even\nthe most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning\nerrors and hallucinations on tasks requiring commonsense reasoning over\nobscure, long-tail entities. To investigate this limitation, we present a new\ndataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that\nconsists of 3,300 queries from question answering and claim verification tasks\nand covers a diverse range of commonsense reasoning skills. We remark that\nCoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset\nsince the support of knowledge required to answer its queries is present in the\nWikidata knowledge graph. However, as opposed to existing KGQA benchmarks that\nmerely focus on factoid questions, our CoLoTa queries also require commonsense\nreasoning. Our experiments with strong LLM-based KGQA methodologies indicate\ntheir severe inability to answer queries involving commonsense reasoning.\nHence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM\ncommonsense reasoning capabilities and their robustness to hallucinations on\nlong-tail entities and (ii) the commonsense reasoning capabilities of KGQA\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u6570\u636e\u96c6CoLoTa\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u5c3e\u5b9e\u4f53\u4e0a\u7684\u5e38\u8bc6\u63a8\u7406\u80fd\u529b\u53ca\u5176\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u7f16\u7801\u4e8b\u5b9e\u548c\u5e38\u8bc6\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u957f\u5c3e\u5b9e\u4f53\u4e0a\u7684\u63a8\u7406\u9519\u8bef\u548c\u5e7b\u89c9\u95ee\u9898\u963b\u788d\u4e86\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b3,300\u4e2a\u67e5\u8be2\u7684CoLoTa\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u95ee\u7b54\u548c\u58f0\u660e\u9a8c\u8bc1\u4efb\u52a1\uff0c\u5e76\u652f\u6301\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\uff08KGQA\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709LLM-based KGQA\u65b9\u6cd5\u5728\u6d89\u53ca\u5e38\u8bc6\u63a8\u7406\u7684\u67e5\u8be2\u4e0a\u8868\u73b0\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "CoLoTa\u53ef\u4f5c\u4e3a\u8bc4\u4f30LLMs\u548cKGQA\u65b9\u6cd5\u5728\u5e38\u8bc6\u63a8\u7406\u53ca\u957f\u5c3e\u5b9e\u4f53\u4e0a\u8868\u73b0\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2504.14240", "pdf": "https://arxiv.org/pdf/2504.14240", "abs": "https://arxiv.org/abs/2504.14240", "authors": ["Xie Liang", "Gao Wei", "Zhenghui Ming", "Li Ge"], "title": "ROI-Guided Point Cloud Geometry Compression Towards Human and Machine Vision", "categories": ["cs.CV", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Point cloud data is pivotal in applications like autonomous driving, virtual\nreality, and robotics. However, its substantial volume poses significant\nchallenges in storage and transmission. In order to obtain a high compression\nratio, crucial semantic details usually confront severe damage, leading to\ndifficulties in guaranteeing the accuracy of downstream tasks. To tackle this\nproblem, we are the first to introduce a novel Region of Interest (ROI)-guided\nPoint Cloud Geometry Compression (RPCGC) method for human and machine vision.\nOur framework employs a dual-branch parallel structure, where the base layer\nencodes and decodes a simplified version of the point cloud, and the\nenhancement layer refines this by focusing on geometry details. Furthermore,\nthe residual information of the enhancement layer undergoes refinement through\nan ROI prediction network. This network generates mask information, which is\nthen incorporated into the residuals, serving as a strong supervision signal.\nAdditionally, we intricately apply these mask details in the Rate-Distortion\n(RD) optimization process, with each point weighted in the distortion\ncalculation. Our loss function includes RD loss and detection loss to better\nguide point cloud encoding for the machine. Experiment results demonstrate that\nRPCGC achieves exceptional compression performance and better detection\naccuracy (10% gain) than some learning-based compression methods at high\nbitrates in ScanNet and SUN RGB-D datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eROI\u5f15\u5bfc\u7684\u70b9\u4e91\u51e0\u4f55\u538b\u7f29\u65b9\u6cd5\uff08RPCGC\uff09\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u5e76\u884c\u7ed3\u6784\u4f18\u5316\u538b\u7f29\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u5347\u673a\u5668\u89c6\u89c9\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u70b9\u4e91\u6570\u636e\u5728\u81ea\u52a8\u9a7e\u9a76\u3001\u865a\u62df\u73b0\u5b9e\u7b49\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5927\u4f53\u79ef\u5e26\u6765\u5b58\u50a8\u548c\u4f20\u8f93\u6311\u6218\u3002\u73b0\u6709\u9ad8\u538b\u7f29\u6bd4\u65b9\u6cd5\u5e38\u635f\u5bb3\u8bed\u4e49\u7ec6\u8282\uff0c\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u5e76\u884c\u7ed3\u6784\uff1a\u57fa\u7840\u5c42\u7f16\u7801\u7b80\u5316\u70b9\u4e91\uff0c\u589e\u5f3a\u5c42\u805a\u7126\u51e0\u4f55\u7ec6\u8282\uff1b\u901a\u8fc7ROI\u9884\u6d4b\u7f51\u7edc\u4f18\u5316\u6b8b\u5dee\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u63a9\u7801\u4fe1\u606f\u8fdb\u884cRD\u4f18\u5316\u3002", "result": "\u5728ScanNet\u548cSUN RGB-D\u6570\u636e\u96c6\u4e0a\uff0cRPCGC\u5728\u9ad8\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u538b\u7f29\u6027\u80fd\u548c10%\u7684\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "RPCGC\u901a\u8fc7ROI\u5f15\u5bfc\u548c\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u6709\u6548\u5e73\u8861\u4e86\u538b\u7f29\u6bd4\u4e0e\u8bed\u4e49\u7ec6\u8282\u4fdd\u7559\uff0c\u63d0\u5347\u4e86\u673a\u5668\u89c6\u89c9\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.14468", "pdf": "https://arxiv.org/pdf/2504.14468", "abs": "https://arxiv.org/abs/2504.14468", "authors": ["Yijun Liu"], "title": "sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment", "categories": ["cs.CL", "cs.LG", "eess.SP", "q-bio.NC"], "comment": "Accepted for poster presentation at the CVPR 2025 Workshop on\n  Multimodal Foundation Models (MMFM3)", "summary": "Interpreting neural activity through meaningful latent representations\nremains a complex and evolving challenge at the intersection of neuroscience\nand artificial intelligence. We investigate the potential of multimodal\nfoundation models to align invasive brain recordings with natural language. We\npresent SSENSE, a contrastive learning framework that projects single-subject\nstereo-electroencephalography (sEEG) signals into the sentence embedding space\nof a frozen CLIP model, enabling sentence-level retrieval directly from brain\nactivity. SSENSE trains a neural encoder on spectral representations of sEEG\nusing InfoNCE loss, without fine-tuning the text encoder. We evaluate our\nmethod on time-aligned sEEG and spoken transcripts from a naturalistic\nmovie-watching dataset. Despite limited data, SSENSE achieves promising\nresults, demonstrating that general-purpose language representations can serve\nas effective priors for neural decoding.", "AI": {"tldr": "SSENSE\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06sEEG\u4fe1\u53f7\u6620\u5c04\u5230CLIP\u6a21\u578b\u7684\u53e5\u5b50\u5d4c\u5165\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4ece\u8111\u6d3b\u52a8\u76f4\u63a5\u68c0\u7d22\u53e5\u5b50\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5728\u795e\u7ecf\u79d1\u5b66\u4e0e\u4eba\u5de5\u667a\u80fd\u4ea4\u53c9\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c06\u4fb5\u5165\u6027\u8111\u8bb0\u5f55\u4e0e\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528InfoNCE\u635f\u5931\u5728sEEG\u7684\u9891\u8c31\u8868\u793a\u4e0a\u8bad\u7ec3\u795e\u7ecf\u7f16\u7801\u5668\uff0c\u4e0d\u5fae\u8c03\u6587\u672c\u7f16\u7801\u5668\u3002", "result": "\u5728\u6709\u9650\u6570\u636e\u4e0b\uff0cSSENSE\u5c55\u793a\u4e86\u901a\u7528\u8bed\u8a00\u8868\u5f81\u53ef\u4f5c\u4e3a\u795e\u7ecf\u89e3\u7801\u7684\u6709\u6548\u5148\u9a8c\u3002", "conclusion": "\u901a\u7528\u8bed\u8a00\u8868\u5f81\u53ef\u7528\u4e8e\u795e\u7ecf\u89e3\u7801\uff0cSSENSE\u4e3a\u8111\u6d3b\u52a8\u4e0e\u8bed\u8a00\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.14245", "pdf": "https://arxiv.org/pdf/2504.14245", "abs": "https://arxiv.org/abs/2504.14245", "authors": ["Yikun Ji", "Yan Hong", "Jiahui Zhan", "Haoxing Chen", "jun lan", "Huijia Zhu", "Weiqiang Wang", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Explainable Fake Image Detection with Multi-Modal Large Language Models", "categories": ["cs.CV", "cs.CL", "I.2.7; I.2.10"], "comment": null, "summary": "Progress in image generation raises significant public security concerns. We\nargue that fake image detection should not operate as a \"black box\". Instead,\nan ideal approach must ensure both strong generalization and transparency.\nRecent progress in Multi-modal Large Language Models (MLLMs) offers new\nopportunities for reasoning-based AI-generated image detection. In this work,\nwe evaluate the capabilities of MLLMs in comparison to traditional detection\nmethods and human evaluators, highlighting their strengths and limitations.\nFurthermore, we design six distinct prompts and propose a framework that\nintegrates these prompts to develop a more robust, explainable, and\nreasoning-driven detection system. The code is available at\nhttps://github.com/Gennadiyev/mllm-defake.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "motivation": "\u56fe\u50cf\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u5f15\u53d1\u4e86\u516c\u5171\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6cdb\u5316\u53c8\u900f\u660e\u7684\u5047\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u516d\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u8bcd\uff0c\u5e76\u6574\u5408\u8fd9\u4e9b\u63d0\u793a\u8bcd\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u63a8\u7406\u7684\u68c0\u6d4b\u6846\u67b6\uff0c\u540c\u65f6\u5bf9\u6bd4\u4e86MLLMs\u4e0e\u4f20\u7edf\u65b9\u6cd5\u53ca\u4eba\u7c7b\u8bc4\u4f30\u8005\u7684\u8868\u73b0\u3002", "result": "MLLMs\u5728\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MLLMs\u4e3a\u5047\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.14482", "pdf": "https://arxiv.org/pdf/2504.14482", "abs": "https://arxiv.org/abs/2504.14482", "authors": ["Xiang Li", "Duyi Pan", "Hongru Xiao", "Jiale Han", "Jing Tang", "Jiabao Ma", "Wei Wang", "Bo Cheng"], "title": "DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue", "categories": ["cs.CL", "cs.SD"], "comment": "Accepted by ICME 2025. Dataset and code are publicly available:\n  [https://github.com/uirlx/DialogueAgents](https://github.com/uirlx/DialogueAgents)", "summary": "Speech synthesis is crucial for human-computer interaction, enabling natural\nand intuitive communication. However, existing datasets involve high\nconstruction costs due to manual annotation and suffer from limited character\ndiversity, contextual scenarios, and emotional expressiveness. To address these\nissues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis\nframework, which integrates three specialized agents -- a script writer, a\nspeech synthesizer, and a dialogue critic -- to collaboratively generate\ndialogues. Grounded in a diverse character pool, the framework iteratively\nrefines dialogue scripts and synthesizes speech based on speech review,\nboosting emotional expressiveness and paralinguistic features of the\nsynthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a\nbilingual, multi-party, multi-turn speech dialogue dataset covering diverse\ntopics. Extensive experiments demonstrate the effectiveness of our framework\nand the high quality of the MultiTalk dataset. We release the dataset and code\nhttps://github.com/uirlx/DialogueAgents to facilitate future research on\nadvanced speech synthesis models and customized data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u8bed\u97f3\u5408\u6210\u6846\u67b6DialogueAgents\uff0c\u901a\u8fc7\u811a\u672c\u7f16\u5199\u3001\u8bed\u97f3\u5408\u6210\u548c\u5bf9\u8bdd\u6279\u8bc4\u4e09\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u751f\u6210\u5bf9\u8bdd\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u8d28\u91cf\u7684\u53cc\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6MultiTalk\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5408\u6210\u6570\u636e\u96c6\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u60c5\u611f\u8868\u8fbe\u548c\u8bed\u5883\u591a\u6837\u6027\u3002", "method": "\u91c7\u7528\u4e09\u4e2a\u667a\u80fd\u4f53\uff08\u811a\u672c\u7f16\u5199\u3001\u8bed\u97f3\u5408\u6210\u3001\u5bf9\u8bdd\u6279\u8bc4\uff09\u534f\u4f5c\u8fed\u4ee3\u4f18\u5316\u5bf9\u8bdd\u811a\u672c\u548c\u8bed\u97f3\u5408\u6210\uff0c\u63d0\u5347\u60c5\u611f\u8868\u8fbe\u548c\u526f\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u751f\u6210\u4e86\u9ad8\u8d28\u91cf\u7684\u53cc\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6MultiTalk\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "DialogueAgents\u6846\u67b6\u548cMultiTalk\u6570\u636e\u96c6\u4e3a\u8bed\u97f3\u5408\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u8d44\u6e90\u3002"}}
{"id": "2504.14249", "pdf": "https://arxiv.org/pdf/2504.14249", "abs": "https://arxiv.org/abs/2504.14249", "authors": ["Bin Ren", "Eduard Zamfir", "Zongwei Wu", "Yawei Li", "Yidi Li", "Danda Pani Paudel", "Radu Timofte", "Ming-Hsuan Yang", "Luc Van Gool", "Nicu Sebe"], "title": "Any Image Restoration via Efficient Spatial-Frequency Degradation Adaptation", "categories": ["cs.CV"], "comment": "Efficient All in One Image Restoration", "summary": "Restoring any degraded image efficiently via just one model has become\nincreasingly significant and impactful, especially with the proliferation of\nmobile devices. Traditional solutions typically involve training dedicated\nmodels per degradation, resulting in inefficiency and redundancy. More recent\napproaches either introduce additional modules to learn visual prompts,\nsignificantly increasing model size, or incorporate cross-modal transfer from\nlarge language models trained on vast datasets, adding complexity to the system\narchitecture. In contrast, our approach, termed AnyIR, takes a unified path\nthat leverages inherent similarity across various degradations to enable both\nefficient and comprehensive restoration through a joint embedding mechanism,\nwithout scaling up the model or relying on large language models.Specifically,\nwe examine the sub-latent space of each input, identifying key components and\nreweighting them first in a gated manner. To fuse the intrinsic degradation\nawareness and the contextualized attention, a spatial-frequency parallel fusion\nstrategy is proposed for enhancing spatial-aware local-global interactions and\nenriching the restoration details from the frequency perspective. Extensive\nbenchmarking in the all-in-one restoration setting confirms AnyIR's SOTA\nperformance, reducing model complexity by around 82\\% in parameters and 85\\% in\nFLOPs. Our code will be available at our Project page\n(https://amazingren.github.io/AnyIR/)", "AI": {"tldr": "AnyIR\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5d4c\u5165\u673a\u5236\u9ad8\u6548\u6062\u590d\u591a\u79cd\u9000\u5316\u56fe\u50cf\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6216\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u4e3a\u6bcf\u79cd\u9000\u5316\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u6548\u7387\u4f4e\u4e14\u5197\u4f59\uff1b\u73b0\u6709\u65b9\u6cd5\u589e\u52a0\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u6216\u4f9d\u8d56\u8de8\u6a21\u6001\u8fc1\u79fb\u3002AnyIR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b50\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\u8f93\u5165\uff0c\u91c7\u7528\u95e8\u63a7\u91cd\u52a0\u6743\u673a\u5236\uff0c\u7ed3\u5408\u7a7a\u95f4-\u9891\u7387\u5e76\u884c\u878d\u5408\u7b56\u7565\u589e\u5f3a\u5c40\u90e8-\u5168\u5c40\u4ea4\u4e92\u548c\u9891\u7387\u7ec6\u8282\u3002", "result": "AnyIR\u5728\u5168\u80fd\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0SOTA\uff0c\u53c2\u6570\u548cFLOPs\u5206\u522b\u51cf\u5c11\u7ea682%\u548c85%\u3002", "conclusion": "AnyIR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\u3002"}}
{"id": "2504.14492", "pdf": "https://arxiv.org/pdf/2504.14492", "abs": "https://arxiv.org/abs/2504.14492", "authors": ["Yichen Li", "Zhiting Fan", "Ruizhe Chen", "Xiaotang Gai", "Luqi Gong", "Yan Zhang", "Zuozhu Liu"], "title": "FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.", "AI": {"tldr": "FairSteer\u662f\u4e00\u79cd\u65e0\u9700\u5b9a\u5236\u63d0\u793a\u6216\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u7684\u63a8\u7406\u65f6\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u504f\u6fc0\u6d3b\u3001\u8ba1\u7b97\u53bb\u504f\u8f6c\u5411\u5411\u91cf\u5e76\u52a8\u6001\u8c03\u6574\u6fc0\u6d3b\u6765\u5b9e\u73b0\u53bb\u504f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bb9\u6613\u4ece\u8bad\u7ec3\u8bed\u6599\u4e2d\u6355\u6349\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e0d\u7a33\u5b9a\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "FairSteer\u57fa\u4e8e\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u5206\u7c7b\u5668\u68c0\u6d4b\u504f\u6fc0\u6d3b\uff0c\u8ba1\u7b97\u53bb\u504f\u8f6c\u5411\u5411\u91cf\uff08DSV\uff09\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u6fc0\u6d3b\u3002", "result": "\u5728\u516d\u79cdLLMs\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cFairSteer\u5728\u95ee\u7b54\u3001\u53cd\u4e8b\u5b9e\u8f93\u5165\u8bc4\u4f30\u548c\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "FairSteer\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u63d0\u793a\u8bbe\u8ba1\u3002"}}
{"id": "2504.14253", "pdf": "https://arxiv.org/pdf/2504.14253", "abs": "https://arxiv.org/abs/2504.14253", "authors": ["Yifan Wang", "Jie Gui", "Xinli Shi", "Linqing Gui", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "ColorVein: Colorful Cancelable Vein Biometrics", "categories": ["cs.CV"], "comment": null, "summary": "Vein recognition technologies have become one of the primary solutions for\nhigh-security identification systems. However, the issue of biometric\ninformation leakage can still pose a serious threat to user privacy and\nanonymity. Currently, there is no cancelable biometric template generation\nscheme specifically designed for vein biometrics. Therefore, this paper\nproposes an innovative cancelable vein biometric generation scheme: ColorVein.\nUnlike previous cancelable template generation schemes, ColorVein does not\ndestroy the original biometric features and introduces additional color\ninformation to grayscale vein images. This method significantly enhances the\ninformation density of vein images by transforming static grayscale information\ninto dynamically controllable color representations through interactive\ncolorization. ColorVein allows users/administrators to define a controllable\npseudo-random color space for grayscale vein images by editing the position,\nnumber, and color of hint points, thereby generating protected cancelable\ntemplates. Additionally, we propose a new secure center loss to optimize the\ntraining process of the protected feature extraction model, effectively\nincreasing the feature distance between enrolled users and any potential\nimpostors. Finally, we evaluate ColorVein's performance on all types of vein\nbiometrics, including recognition performance, unlinkability, irreversibility,\nand revocability, and conduct security and privacy analyses. ColorVein achieves\ncompetitive performance compared with state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u53ef\u53d6\u6d88\u9759\u8109\u751f\u7269\u7279\u5f81\u751f\u6210\u65b9\u6848ColorVein\uff0c\u901a\u8fc7\u5f15\u5165\u989c\u8272\u4fe1\u606f\u589e\u5f3a\u9759\u8109\u56fe\u50cf\u7684\u4fe1\u606f\u5bc6\u5ea6\uff0c\u5e76\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u9759\u8109\u751f\u7269\u7279\u5f81\u7684\u53ef\u53d6\u6d88\u6a21\u677f\u751f\u6210\u65b9\u6848\uff0c\u751f\u7269\u4fe1\u606f\u6cc4\u9732\u53ef\u80fd\u5a01\u80c1\u7528\u6237\u9690\u79c1\u548c\u533f\u540d\u6027\u3002", "method": "ColorVein\u901a\u8fc7\u4ea4\u4e92\u5f0f\u7740\u8272\u5c06\u9759\u6001\u7070\u5ea6\u4fe1\u606f\u8f6c\u6362\u4e3a\u52a8\u6001\u53ef\u63a7\u7684\u989c\u8272\u8868\u793a\uff0c\u5e76\u5f15\u5165\u5b89\u5168\u4e2d\u5fc3\u635f\u5931\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u3002", "result": "ColorVein\u5728\u8bc6\u522b\u6027\u80fd\u3001\u4e0d\u53ef\u94fe\u63a5\u6027\u3001\u4e0d\u53ef\u9006\u6027\u548c\u53ef\u64a4\u9500\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b89\u5168\u6027\u548c\u9690\u79c1\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ColorVein\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5b89\u5168\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2504.14496", "pdf": "https://arxiv.org/pdf/2504.14496", "abs": "https://arxiv.org/abs/2504.14496", "authors": ["Zijian Wang", "Chang Xu"], "title": "Functional Abstraction of Knowledge Recall in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained transformer large language models (LLMs) demonstrate strong\nknowledge recall capabilities. This paper investigates the knowledge recall\nmechanism in LLMs by abstracting it into a functional structure. We propose\nthat during knowledge recall, the model's hidden activation space implicitly\nentails a function execution process where specific activation vectors align\nwith functional components (Input argument, Function body, and Return values).\nSpecifically, activation vectors of relation-related tokens define a mapping\nfunction from subjects to objects, with subject-related token activations\nserving as input arguments and object-related token activations as return\nvalues. For experimental verification, we first design a patching-based\nknowledge-scoring algorithm to identify knowledge-aware activation vectors as\nindependent functional components. Then, we conduct counter-knowledge testing\nto examine the independent functional effects of each component on knowledge\nrecall outcomes. From this functional perspective, we improve the contextual\nknowledge editing approach augmented by activation patching. By rewriting\nincoherent activations in context, we enable improved short-term memory\nretention for new knowledge prompting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u77e5\u8bc6\u53ec\u56de\u673a\u5236\uff0c\u5c06\u5176\u62bd\u8c61\u4e3a\u529f\u80fd\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u6fc0\u6d3b\u5411\u91cf\u4e0e\u529f\u80fd\u7ec4\u4ef6\uff08\u8f93\u5165\u3001\u51fd\u6570\u4f53\u3001\u8fd4\u56de\u503c\uff09\u7684\u5bf9\u9f50\u5173\u7cfb\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6539\u8fdb\u4e86\u57fa\u4e8e\u6fc0\u6d3b\u4fee\u8865\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22LLMs\u4e2d\u77e5\u8bc6\u53ec\u56de\u7684\u673a\u5236\uff0c\u5c06\u5176\u5f62\u5f0f\u5316\u4e3a\u529f\u80fd\u7ed3\u6784\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u6fc0\u6d3b\u5411\u91cf\u5b9e\u73b0\u77e5\u8bc6\u6620\u5c04\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u4fee\u8865\u7684\u77e5\u8bc6\u8bc4\u5206\u7b97\u6cd5\u8bc6\u522b\u529f\u80fd\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u53cd\u77e5\u8bc6\u6d4b\u8bd5\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u7684\u72ec\u7acb\u529f\u80fd\u6548\u5e94\u3002", "result": "\u6fc0\u6d3b\u5411\u91cf\u4e0e\u529f\u80fd\u7ec4\u4ef6\u7684\u5bf9\u9f50\u5173\u7cfb\u5f97\u5230\u9a8c\u8bc1\uff0c\u6539\u8fdb\u7684\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u63d0\u5347\u4e86\u65b0\u77e5\u8bc6\u7684\u77ed\u671f\u8bb0\u5fc6\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "\u4ece\u529f\u80fd\u89c6\u89d2\u63ed\u793a\u4e86LLMs\u7684\u77e5\u8bc6\u53ec\u56de\u673a\u5236\uff0c\u4e3a\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14254", "pdf": "https://arxiv.org/pdf/2504.14254", "abs": "https://arxiv.org/abs/2504.14254", "authors": ["Jie Wang", "Nana Yu", "Zihao Zhang", "Yahong Han"], "title": "Visual Consensus Prompting for Co-Salient Object Detection", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Existing co-salient object detection (CoSOD) methods generally employ a\nthree-stage architecture (i.e., encoding, consensus extraction & dispersion,\nand prediction) along with a typical full fine-tuning paradigm. Although they\nyield certain benefits, they exhibit two notable limitations: 1) This\narchitecture relies on encoded features to facilitate consensus extraction, but\nthe meticulously extracted consensus does not provide timely guidance to the\nencoding stage. 2) This paradigm involves globally updating all parameters of\nthe model, which is parameter-inefficient and hinders the effective\nrepresentation of knowledge within the foundation model for this task.\nTherefore, in this paper, we propose an interaction-effective and\nparameter-efficient concise architecture for the CoSOD task, addressing two key\nlimitations. It introduces, for the first time, a parameter-efficient prompt\ntuning paradigm and seamlessly embeds consensus into the prompts to formulate\ntask-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen\nfoundation model to perform better on CoSOD tasks by formulating task-specific\nvisual consensus prompts with minimized tunable parameters. Concretely, the\nprimary insight of the purposeful Consensus Prompt Generator (CPG) is to\nenforce limited tunable parameters to focus on co-salient representations and\ngenerate consensus prompts. The formulated Consensus Prompt Disperser (CPD)\nleverages consensus prompts to form task-specific visual consensus prompts,\nthereby arousing the powerful potential of pre-trained models in addressing\nCoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms\n13 cutting-edge full fine-tuning models, achieving the new state of the art\n(with 6.8% improvement in F_m metrics on the most challenging CoCA dataset).\nSource code has been available at https://github.com/WJ-CV/VCP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u89c6\u89c9\u5171\u8bc6\u63d0\u793a\uff08VCP\uff09\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CoSOD\u65b9\u6cd5\u5728\u5171\u8bc6\u63d0\u53d6\u548c\u53c2\u6570\u6548\u7387\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CoSOD\u65b9\u6cd5\u4f9d\u8d56\u7f16\u7801\u7279\u5f81\u63d0\u53d6\u5171\u8bc6\u4e14\u53c2\u6570\u66f4\u65b0\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u53c2\u6570\u9ad8\u6548\u7684\u63d0\u793a\u8c03\u4f18\u8303\u5f0f\uff0c\u901a\u8fc7\u5171\u8bc6\u63d0\u793a\u751f\u6210\u5668\uff08CPG\uff09\u548c\u5206\u6563\u5668\uff08CPD\uff09\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u89c6\u89c9\u5171\u8bc6\u63d0\u793a\uff08VCP\uff09\u3002", "result": "\u5728\u6700\u5177\u6311\u6218\u6027\u7684CoCA\u6570\u636e\u96c6\u4e0a\uff0cF_m\u6307\u6807\u63d0\u5347\u4e866.8%\uff0c\u4f18\u4e8e13\u79cd\u524d\u6cbf\u5168\u5fae\u8c03\u6a21\u578b\u3002", "conclusion": "VCP\u67b6\u6784\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u5f0f\u663e\u8457\u63d0\u5347\u4e86CoSOD\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14530", "pdf": "https://arxiv.org/pdf/2504.14530", "abs": "https://arxiv.org/abs/2504.14530", "authors": ["Zhijing Jin"], "title": "Causality for Natural Language Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "PhD Thesis 2024", "summary": "Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u7814\u7a76\u5176\u673a\u5236\u3001\u5e94\u7528\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u56e0\u679c\u63a8\u7406\u662f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u7ea7\u7406\u89e3\u548c\u51b3\u7b56\u7684\u5173\u952e\u80fd\u529b\uff0c\u7814\u7a76LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u7814\u7a76\u3001\u65b0\u6570\u636e\u96c6\u3001\u57fa\u51c6\u4efb\u52a1\u548c\u65b9\u6cd5\u6846\u67b6\uff0c\u5206\u6790LLMs\u7684\u56e0\u679c\u63a8\u7406\u6280\u80fd\u53ca\u5176\u673a\u5236\u3002", "result": "\u63ed\u793a\u4e86LLMs\u5728\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6311\u6218\u548c\u673a\u9047\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8fd9\u4e00\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14260", "pdf": "https://arxiv.org/pdf/2504.14260", "abs": "https://arxiv.org/abs/2504.14260", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "Cross-attention for State-based model RWKV-7", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention", "AI": {"tldr": "CrossWKV\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u589e\u5f3aRWKV-7\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u901a\u8fc7\u7ebf\u6027\u590d\u6742\u5ea6\u7684WKV\u67b6\u6784\u548c\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "motivation": "\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u9ad8\u6548\u7684\u5185\u5b58\u4f7f\u7528\u3002", "method": "\u7ed3\u5408RWKV-7\u7684WKV\u67b6\u6784\uff0c\u4f7f\u7528\u5e7f\u4e49delta\u89c4\u5219\u548c\u4f4e\u79e9\u9002\u5e94\u6280\u672f\uff08LoRA\uff09\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728DIR-7\u6846\u67b6\u4e0b\uff0cCrossWKV\u5728ImageNet 256x256\u4e0a\u53d6\u5f97FID 2.88\u548cCLIP\u5206\u65700.33\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CrossWKV\u5728\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u5206\u8fa8\u7387\u548c\u52a8\u6001\u72b6\u6001\u64cd\u4f5c\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.14538", "pdf": "https://arxiv.org/pdf/2504.14538", "abs": "https://arxiv.org/abs/2504.14538", "authors": ["Yiting Ran", "Xintao Wang", "Tian Qiu", "Jiaqing Liang", "Yanghua Xiao", "Deqing Yang"], "title": "BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation", "categories": ["cs.CL"], "comment": "19 pages, 4 figures", "summary": "Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BookWorld\u7cfb\u7edf\uff0c\u7528\u4e8e\u6784\u5efa\u548c\u6a21\u62df\u57fa\u4e8e\u4e66\u7c4d\u7684\u591a\u667a\u80fd\u4f53\u793e\u4f1a\uff0c\u8986\u76d6\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u6545\u4e8b\u751f\u6210\u548c\u793e\u4ea4\u6a21\u62df\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ece\u5934\u521b\u5efa\u667a\u80fd\u4f53\u793e\u4f1a\uff0c\u800c\u6a21\u62df\u5df2\u6709\u865a\u6784\u4e16\u754c\u548c\u89d2\u8272\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4f46\u5176\u5177\u6709\u91cd\u8981\u5b9e\u7528\u4ef7\u503c\u3002", "method": "\u8bbe\u8ba1\u4e86BookWorld\u7cfb\u7edf\uff0c\u6db5\u76d6\u52a8\u6001\u89d2\u8272\u3001\u865a\u6784\u4e16\u754c\u89c2\u3001\u5730\u7406\u7ea6\u675f\u7b49\u73b0\u5b9e\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBookWorld\u5728\u6545\u4e8b\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4fdd\u6301\u5bf9\u539f\u8457\u7684\u5fe0\u5b9e\u5ea6\uff0c\u80dc\u738775.36%\u3002", "conclusion": "BookWorld\u4e3a\u6269\u5c55\u548c\u63a2\u7d22\u865a\u6784\u4f5c\u54c1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14267", "pdf": "https://arxiv.org/pdf/2504.14267", "abs": "https://arxiv.org/abs/2504.14267", "authors": ["Li Yu", "Xuanzhe Sun", "Wei Zhou", "Moncef Gabbouj"], "title": "Text-Audio-Visual-conditioned Diffusion Model for Video Saliency Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Video saliency prediction is crucial for downstream applications, such as\nvideo compression and human-computer interaction. With the flourishing of\nmultimodal learning, researchers started to explore multimodal video saliency\nprediction, including audio-visual and text-visual approaches. Auditory cues\nguide the gaze of viewers to sound sources, while textual cues provide semantic\nguidance for understanding video content. Integrating these complementary cues\ncan improve the accuracy of saliency prediction. Therefore, we attempt to\nsimultaneously analyze visual, auditory, and textual modalities in this paper,\nand propose TAVDiff, a Text-Audio-Visual-conditioned Diffusion Model for video\nsaliency prediction. TAVDiff treats video saliency prediction as an image\ngeneration task conditioned on textual, audio, and visual inputs, and predicts\nsaliency maps through stepwise denoising. To effectively utilize text, a large\nmultimodal model is used to generate textual descriptions for video frames and\nintroduce a saliency-oriented image-text response (SITR) mechanism to generate\nimage-text response maps. It is used as conditional information to guide the\nmodel to localize the visual regions that are semantically related to the\ntextual description. Regarding the auditory modality, it is used as another\nconditional information for directing the model to focus on salient regions\nindicated by sounds. At the same time, since the diffusion transformer (DiT)\ndirectly concatenates the conditional information with the timestep, which may\naffect the estimation of the noise level. To achieve effective conditional\nguidance, we propose Saliency-DiT, which decouples the conditional information\nfrom the timestep. Experimental results show that TAVDiff outperforms existing\nmethods, improving 1.03\\%, 2.35\\%, 2.71\\% and 0.33\\% on SIM, CC, NSS and AUC-J\nmetrics, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTAVDiff\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u3001\u542c\u89c9\u548c\u6587\u672c\u6a21\u6001\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u9884\u6d4b\u89c6\u9891\u663e\u8457\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u663e\u8457\u6027\u9884\u6d4b\u5bf9\u4e0b\u6e38\u5e94\u7528\uff08\u5982\u89c6\u9891\u538b\u7f29\u548c\u4eba\u673a\u4ea4\u4e92\uff09\u81f3\u5173\u91cd\u8981\u3002\u591a\u6a21\u6001\u5b66\u4e60\u7684\u53d1\u5c55\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u591a\u6a21\u6001\u663e\u8457\u6027\u9884\u6d4b\uff0c\u4ee5\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faTAVDiff\u6a21\u578b\uff0c\u5c06\u663e\u8457\u6027\u9884\u6d4b\u89c6\u4e3a\u57fa\u4e8e\u6587\u672c\u3001\u97f3\u9891\u548c\u89c6\u89c9\u8f93\u5165\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u9010\u6b65\u53bb\u566a\u9884\u6d4b\u663e\u8457\u6027\u56fe\u3002\u5f15\u5165SITR\u673a\u5236\u5904\u7406\u6587\u672c\u6a21\u6001\uff0c\u5e76\u6539\u8fdbDiT\u4ee5\u89e3\u8026\u6761\u4ef6\u4fe1\u606f\u4e0e\u65f6\u95f4\u6b65\u3002", "result": "TAVDiff\u5728SIM\u3001CC\u3001NSS\u548cAUC-J\u6307\u6807\u4e0a\u5206\u522b\u63d0\u53471.03%\u30012.35%\u30012.71%\u548c0.33%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TAVDiff\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u6539\u8fdb\u7684\u6761\u4ef6\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u663e\u8457\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.14597", "pdf": "https://arxiv.org/pdf/2504.14597", "abs": "https://arxiv.org/abs/2504.14597", "authors": ["Lingrui Mei", "Shenghua Liu", "Yiwei Wang", "Baolong Bi", "Yuyao Ge", "Jun Wan", "Yurong Wu", "Xueqi Cheng"], "title": "a1: Steep Test-time Scaling Law via Environment Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,\nyet continue to struggle with hallucinations, logical errors, and inability to\nself-correct during complex multi-step tasks. Current approaches like\nchain-of-thought prompting offer limited reasoning capabilities that fail when\nprecise step validation is required. We propose Environment Augmented\nGeneration (EAG), a framework that enhances LLM reasoning through: (1)\nreal-time environmental feedback validating each reasoning step, (2) dynamic\nbranch exploration for investigating alternative solution paths when faced with\nerrors, and (3) experience-based learning from successful reasoning\ntrajectories. Unlike existing methods, EAG enables deliberate backtracking and\nstrategic replanning through tight integration of execution feedback with\nbranching exploration. Our a1-32B model achieves state-of-the-art performance\namong similar-sized models across all benchmarks, matching larger models like\no1 on competition mathematics while outperforming comparable models by up to\n24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:\ninitial token investment in environment interaction yields substantial\nlong-term performance dividends, with advantages amplifying proportionally to\ntask complexity. EAG's theoretical framework demonstrates how environment\ninteractivity and systematic branch exploration together establish a new\nparadigm for reliable machine reasoning, particularly for problems requiring\nprecise multi-step calculation and logical verification.", "AI": {"tldr": "EAG\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u73af\u5883\u53cd\u9988\u3001\u52a8\u6001\u5206\u652f\u63a2\u7d22\u548c\u7ecf\u9a8c\u5b66\u4e60\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u5f53\u524dLLM\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u548c\u903b\u8f91\u9519\u8bef\uff0c\u4e14\u65e0\u6cd5\u81ea\u6211\u7ea0\u6b63\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faEAG\u6846\u67b6\uff0c\u7ed3\u5408\u5b9e\u65f6\u73af\u5883\u53cd\u9988\u9a8c\u8bc1\u6b65\u9aa4\u3001\u52a8\u6001\u5206\u652f\u63a2\u7d22\u66ff\u4ee3\u8def\u5f84\uff0c\u4ee5\u53ca\u6210\u529f\u63a8\u7406\u8f68\u8ff9\u7684\u7ecf\u9a8c\u5b66\u4e60\u3002", "result": "a1-32B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u540c\u7c7b\u6a21\u578b\u6700\u4f73\u6027\u80fd\uff0c\u90e8\u5206\u4efb\u52a1\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u4f18\u52bf\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\u3002", "conclusion": "EAG\u901a\u8fc7\u73af\u5883\u4ea4\u4e92\u548c\u5206\u652f\u63a2\u7d22\u4e3a\u53ef\u9760\u673a\u5668\u63a8\u7406\u5efa\u7acb\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u7cbe\u786e\u591a\u6b65\u8ba1\u7b97\u548c\u903b\u8f91\u9a8c\u8bc1\u7684\u95ee\u9898\u3002"}}
{"id": "2504.14278", "pdf": "https://arxiv.org/pdf/2504.14278", "abs": "https://arxiv.org/abs/2504.14278", "authors": ["Shang Zhang", "Yuke Hou", "Guoqiang Gong", "Ruoyan Xiong", "Yue Zhang"], "title": "RAMCT: Novel Region-adaptive Multi-channel Tracker with Iterative Tikhonov Regularization for Thermal Infrared Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Correlation filter (CF)-based trackers have gained significant attention for\ntheir computational efficiency in thermal infrared (TIR) target tracking.\nHowever, ex-isting methods struggle with challenges such as low-resolution\nimagery, occlu-sion, background clutter, and target deformation, which severely\nimpact tracking performance. To overcome these limitations, we propose RAMCT, a\nregion-adaptive sparse correlation filter tracker that integrates multi-channel\nfeature opti-mization with an adaptive regularization strategy. Firstly, we\nrefine the CF learn-ing process by introducing a spatially adaptive binary\nmask, which enforces spar-sity in the target region while dynamically\nsuppressing background interference. Secondly, we introduce generalized\nsingular value decomposition (GSVD) and propose a novel GSVD-based\nregion-adaptive iterative Tikhonov regularization method. This enables flexible\nand robust optimization across multiple feature channels, improving resilience\nto occlusion and background variations. Thirdly, we propose an online\noptimization strategy with dynamic discrepancy-based pa-rameter adjustment.\nThis mechanism facilitates real time adaptation to target and background\nvariations, thereby improving tracking accuracy and robustness. Ex-tensive\nexperiments on LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR2017 benchmarks\ndemonstrate that RAMCT outperforms other state-of-the-art trackers in terms of\naccuracy and robustness.", "AI": {"tldr": "RAMCT\u662f\u4e00\u79cd\u533a\u57df\u81ea\u9002\u5e94\u7a00\u758f\u76f8\u5173\u6ee4\u6ce2\u5668\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u591a\u901a\u9053\u7279\u5f81\u4f18\u5316\u548c\u81ea\u9002\u5e94\u6b63\u5219\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u4f4e\u5206\u8fa8\u7387\u3001\u906e\u6321\u3001\u80cc\u666f\u5e72\u6270\u548c\u76ee\u6807\u53d8\u5f62\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76f8\u5173\u6ee4\u6ce2\u5668\u8ddf\u8e2a\u5668\u5728\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u4e2d\u9762\u4e34\u4f4e\u5206\u8fa8\u7387\u3001\u906e\u6321\u3001\u80cc\u666f\u5e72\u6270\u548c\u76ee\u6807\u53d8\u5f62\u7b49\u6311\u6218\uff0c\u5f71\u54cd\u8ddf\u8e2a\u6027\u80fd\u3002", "method": "1. \u5f15\u5165\u7a7a\u95f4\u81ea\u9002\u5e94\u4e8c\u8fdb\u5236\u63a9\u7801\u4f18\u5316CF\u5b66\u4e60\u8fc7\u7a0b\uff1b2. \u63d0\u51fa\u57fa\u4e8eGSVD\u7684\u533a\u57df\u81ea\u9002\u5e94\u8fed\u4ee3Tikhonov\u6b63\u5219\u5316\u65b9\u6cd5\uff1b3. \u8bbe\u8ba1\u52a8\u6001\u5dee\u5f02\u53c2\u6570\u8c03\u6574\u7684\u5728\u7ebf\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAMCT\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u8ddf\u8e2a\u5668\u3002", "conclusion": "RAMCT\u901a\u8fc7\u81ea\u9002\u5e94\u548c\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14619", "pdf": "https://arxiv.org/pdf/2504.14619", "abs": "https://arxiv.org/abs/2504.14619", "authors": ["Yuri Balashov", "Alex Balashov", "Shiho Fukuda Koski"], "title": "Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations", "categories": ["cs.CL"], "comment": "28 pages, 4 figures. Accepted at the MT Summit, University of Geneva,\n  June 2025", "summary": "This is the first in a series of papers exploring the rapidly expanding new\nopportunities arising from recent progress in language technologies for\nindividual translators and language service providers with modest resources.\nThe advent of advanced neural machine translation systems, large language\nmodels, and their integration into workflows via computer-assisted translation\ntools and translation management systems have reshaped the translation\nlandscape. These advancements enable not only translation but also quality\nevaluation, error spotting, glossary generation, and adaptation to\ndomain-specific needs, creating new technical opportunities for freelancers. In\nthis series, we aim to empower translators with actionable methods to harness\nthese advancements. Our approach emphasizes Translation Analytics, a suite of\nevaluation techniques traditionally reserved for large-scale industry\napplications but now becoming increasingly available for smaller-scale users.\nThis first paper introduces a practical framework for adapting automatic\nevaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'\nneeds. We illustrate the potential of these metrics using a trilingual corpus\nderived from a real-world project in the medical domain and provide statistical\nanalysis correlating human evaluations with automatic scores. Our findings\nemphasize the importance of proactive engagement with emerging technologies to\nnot only adapt but thrive in the evolving professional environment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6280\u672f\u8fdb\u6b65\u4e3a\u4e2a\u4f53\u8bd1\u8005\u548c\u8d44\u6e90\u6709\u9650\u7684\u8bed\u8a00\u670d\u52a1\u63d0\u4f9b\u5546\u5e26\u6765\u7684\u65b0\u673a\u9047\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u81ea\u7531\u8bd1\u8005\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5e2e\u52a9\u81ea\u7531\u8bd1\u8005\u5229\u7528\u65b0\u5174\u8bed\u8a00\u6280\u672f\uff08\u5982\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u5de5\u4f5c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u3001chrF\u3001TER\u548cCOMET\uff09\u9002\u914d\u81ea\u7531\u8bd1\u8005\u9700\u6c42\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u533b\u5b66\u9886\u57df\u7684\u4e09\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4ef7\u4e4b\u95f4\u5b58\u5728\u7edf\u8ba1\u76f8\u5173\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u81ea\u7531\u8bd1\u8005\u5e94\u4e3b\u52a8\u62e5\u62b1\u65b0\u5174\u6280\u672f\uff0c\u4ee5\u5728\u5feb\u901f\u53d8\u5316\u7684\u804c\u4e1a\u73af\u5883\u4e2d\u9002\u5e94\u5e76\u53d6\u5f97\u6210\u529f\u3002"}}
{"id": "2504.14280", "pdf": "https://arxiv.org/pdf/2504.14280", "abs": "https://arxiv.org/abs/2504.14280", "authors": ["Jindong Li", "Yongguang Li", "Yali Fu", "Jiahong Liu", "Yixin Liu", "Menglin Yang", "Irwin King"], "title": "CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As machine learning evolves, domain generalization (DG) and domain adaptation\n(DA) have become crucial for enhancing model robustness across diverse\nenvironments. Contrastive Language-Image Pretraining (CLIP) plays a significant\nrole in these tasks, offering powerful zero-shot capabilities that allow models\nto perform effectively in unseen domains. However, there remains a significant\ngap in the literature, as no comprehensive survey currently exists that\nsystematically explores the applications of CLIP in DG and DA, highlighting the\nnecessity for this review. This survey presents a comprehensive review of\nCLIP's applications in DG and DA. In DG, we categorize methods into optimizing\nprompt learning for task alignment and leveraging CLIP as a backbone for\neffective feature extraction, both enhancing model adaptability. For DA, we\nexamine both source-available methods utilizing labeled source data and\nsource-free approaches primarily based on target domain data, emphasizing\nknowledge transfer mechanisms and strategies for improved performance across\ndiverse contexts. Key challenges, including overfitting, domain diversity, and\ncomputational efficiency, are addressed, alongside future research\nopportunities to advance robustness and efficiency in practical applications.\nBy synthesizing existing literature and pinpointing critical gaps, this survey\nprovides valuable insights for researchers and practitioners, proposing\ndirections for effectively leveraging CLIP to enhance methodologies in domain\ngeneralization and adaptation. Ultimately, this work aims to foster innovation\nand collaboration in the quest for more resilient machine learning models that\ncan perform reliably across diverse real-world scenarios. A more up-to-date\nversion of the papers is maintained at:\nhttps://github.com/jindongli-Ai/Survey_on_CLIP-Powered_Domain_Generalization_and_Adaptation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86CLIP\u5728\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u548c\u9886\u57df\u9002\u5e94\uff08DA\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u5bf9CLIP\u5728DG\u548cDA\u4e2d\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002", "method": "\u5728DG\u4e2d\uff0c\u65b9\u6cd5\u5206\u4e3a\u4f18\u5316\u63d0\u793a\u5b66\u4e60\u548c\u5229\u7528CLIP\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff1b\u5728DA\u4e2d\uff0c\u5206\u4e3a\u57fa\u4e8e\u6e90\u6570\u636e\u7684\u65b9\u6cd5\u548c\u6e90\u65e0\u5173\u65b9\u6cd5\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86CLIP\u5728DG\u548cDA\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\uff08\u5982\u8fc7\u62df\u5408\u3001\u9886\u57df\u591a\u6837\u6027\uff09\u548c\u672a\u6765\u673a\u4f1a\u3002", "conclusion": "\u672c\u6587\u4e3aCLIP\u5728DG\u548cDA\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u9c81\u68d2\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14620", "pdf": "https://arxiv.org/pdf/2504.14620", "abs": "https://arxiv.org/abs/2504.14620", "authors": ["Hongming Tan", "Shaoxiong Zhan", "Fengwei Jia", "Hai-Tao Zheng", "Wai Kin Chan"], "title": "A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted novelty scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Comprehensive experiments on scientific conference paper datasets\nshow that HSPIM outperforms baseline methods in effectiveness, generalization,\nand interpretability.", "AI": {"tldr": "HSPIM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u8bba\u6587\u5206\u6bb5\u548c\u96f6\u6837\u672c\u63d0\u793a\u8bc4\u4f30\u79d1\u5b66\u8bba\u6587\u7684\u521b\u65b0\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u6355\u6349\u8bba\u6587\u521b\u65b0\u6027\u4e14\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "HSPIM\u5c06\u8bba\u6587\u5206\u89e3\u4e3a\u7ae0\u8282\uff0c\u5229\u7528\u96f6\u6837\u672cLLM\u63d0\u793a\u8fdb\u884c\u7ae0\u8282\u5206\u7c7b\u3001QA\u589e\u5f3a\u548c\u52a0\u6743\u65b0\u9896\u6027\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u95ee\u9898\u63d0\u793a\u7ec4\u5408\u3002", "result": "\u5728\u79d1\u5b66\u4f1a\u8bae\u8bba\u6587\u6570\u636e\u96c6\u4e0a\uff0cHSPIM\u5728\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HSPIM\u4e3a\u79d1\u5b66\u8bba\u6587\u521b\u65b0\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14289", "pdf": "https://arxiv.org/pdf/2504.14289", "abs": "https://arxiv.org/abs/2504.14289", "authors": ["Shang Zhang", "Yujie Cui", "Ruoyan Xiong", "Huanbin Zhang"], "title": "ISTD-YOLO: A Multi-Scale Lightweight High-Performance Infrared Small Target Detection Algorithm", "categories": ["cs.CV"], "comment": null, "summary": "Aiming at the detection difficulties of infrared images such as complex\nbackground, low signal-to-noise ratio, small target size and weak brightness, a\nlightweight infrared small target detection algorithm ISTD-YOLO based on\nimproved YOLOv7 was proposed. Firstly, the YOLOv7 network structure was\nlightweight reconstructed, and a three-scale lightweight network architecture\nwas designed. Then, the ELAN-W module of the model neck network is replaced by\nVoV-GSCSP to reduce the computational cost and the complexity of the network\nstructure. Secondly, a parameter-free attention mechanism was introduced into\nthe neck network to enhance the relevance of local con-text information.\nFinally, the Normalized Wasserstein Distance (NWD) was used to optimize the\ncommonly used IoU index to enhance the localization and detection accuracy of\nsmall targets. Experimental results show that compared with YOLOv7 and the\ncurrent mainstream algorithms, ISTD-YOLO can effectively improve the detection\neffect, and all indicators are effectively improved, which can achieve\nhigh-quality detection of infrared small targets.", "AI": {"tldr": "ISTD-YOLO\u662f\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbYOLOv7\u7684\u8f7b\u91cf\u7ea7\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u91cd\u6784\u3001\u5f15\u5165\u65e0\u53c2\u6570\u6ce8\u610f\u529b\u673a\u5236\u548c\u4f18\u5316NWD\u6307\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u9488\u5bf9\u7ea2\u5916\u56fe\u50cf\u68c0\u6d4b\u4e2d\u80cc\u666f\u590d\u6742\u3001\u4fe1\u566a\u6bd4\u4f4e\u3001\u76ee\u6807\u5c3a\u5bf8\u5c0f\u548c\u4eae\u5ea6\u5f31\u7b49\u56f0\u96be\uff0c\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u68c0\u6d4b\u7b97\u6cd5\u3002", "method": "1. \u8f7b\u91cf\u5316\u91cd\u6784YOLOv7\u7f51\u7edc\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e09\u5c3a\u5ea6\u8f7b\u91cf\u7f51\u7edc\uff1b2. \u7528VoV-GSCSP\u66ff\u6362ELAN-W\u6a21\u5757\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b3. \u5f15\u5165\u65e0\u53c2\u6570\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u5c40\u90e8\u4fe1\u606f\u76f8\u5173\u6027\uff1b4. \u4f7f\u7528NWD\u4f18\u5316IoU\u6307\u6807\u63d0\u5347\u5c0f\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cISTD-YOLO\u76f8\u6bd4YOLOv7\u548c\u4e3b\u6d41\u7b97\u6cd5\uff0c\u68c0\u6d4b\u6548\u679c\u663e\u8457\u63d0\u5347\uff0c\u5404\u9879\u6307\u6807\u5747\u6709\u6539\u5584\u3002", "conclusion": "ISTD-YOLO\u80fd\u6709\u6548\u5b9e\u73b0\u7ea2\u5916\u5c0f\u76ee\u6807\u7684\u9ad8\u8d28\u91cf\u68c0\u6d4b\u3002"}}
{"id": "2504.14630", "pdf": "https://arxiv.org/pdf/2504.14630", "abs": "https://arxiv.org/abs/2504.14630", "authors": ["Rondik Hadi Abdulrahman", "Hossein Hassani"], "title": "Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish", "categories": ["cs.CL"], "comment": "18 pages, 11 figures, 8 tables", "summary": "Extracting concise information from scientific documents aids learners,\nresearchers, and practitioners. Automatic Text Summarization (ATS), a key\nNatural Language Processing (NLP) application, automates this process. While\nATS methods exist for many languages, Kurdish remains underdeveloped due to\nlimited resources. This study develops a dataset and language model based on\n231 scientific papers in Sorani Kurdish, collected from four academic\ndepartments in two universities in the Kurdistan Region of Iraq (KRI),\naveraging 26 pages per document. Using Sentence Weighting and Term\nFrequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were\nconducted, differing in whether the conclusions were included. The average word\ncount was 5,492.3 in the first experiment and 5,266.96 in the second. Results\nwere evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L\nmetrics, with the best accuracy reaching 19.58%. Six experts conducted manual\nevaluations using three criteria, with results varying by document. This\nresearch provides valuable resources for Kurdish NLP researchers to advance ATS\nand related fields.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4e3a\u5e93\u5c14\u5fb7\u8bed\uff08Sorani\u65b9\u8a00\uff09\u5f00\u53d1\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u6587\u672c\u6458\u8981\uff08ATS\uff09\uff0c\u586b\u8865\u4e86\u8be5\u8bed\u8a00\u8d44\u6e90\u7684\u7a7a\u767d\u3002\u901a\u8fc7\u4e24\u79cd\u5b9e\u9a8c\uff08\u662f\u5426\u5305\u542b\u7ed3\u8bba\u90e8\u5206\uff09\u548c\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6700\u4f73\u51c6\u786e\u7387\u8fbe\u523019.58%\u3002", "motivation": "\u5e93\u5c14\u5fb7\u8bed\u5728\u81ea\u52a8\u6587\u672c\u6458\u8981\u9886\u57df\u8d44\u6e90\u532e\u4e4f\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5e93\u5c14\u5fb7\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8d44\u6e90\u3002", "method": "\u7814\u7a76\u57fa\u4e8e231\u7bc7\u5e93\u5c14\u5fb7\u8bed\u79d1\u5b66\u8bba\u6587\uff0c\u4f7f\u7528\u53e5\u5b50\u52a0\u6743\u548cTF-IDF\u7b97\u6cd5\u8fdb\u884c\u6458\u8981\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u624b\u52a8\u548c\u81ea\u52a8\uff08ROUGE\u6307\u6807\uff09\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u4f73\u6458\u8981\u51c6\u786e\u7387\u4e3a19.58%\uff0c\u4e13\u5bb6\u624b\u52a8\u8bc4\u4f30\u7ed3\u679c\u56e0\u6587\u6863\u800c\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5e93\u5c14\u5fb7\u8bedATS\u548c\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u5e93\u5c14\u5fb7\u8bedNLP\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14290", "pdf": "https://arxiv.org/pdf/2504.14290", "abs": "https://arxiv.org/abs/2504.14290", "authors": ["Shouwei Ruan", "Zhenyu Wu", "Yao Huang", "Ruochen Zhang", "Yitong Sun", "Caixin Kang", "Xingxing Wei"], "title": "Towards NSFW-Free Text-to-Image Generation via Safety-Constraint Direct Preference Optimization", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Ensuring the safety of generated content remains a fundamental challenge for\nText-to-Image (T2I) generation. Existing studies either fail to guarantee\ncomplete safety under potentially harmful concepts or struggle to balance\nsafety with generation quality. To address these issues, we propose\nSafety-Constrained Direct Preference Optimization (SC-DPO), a novel framework\nfor safety alignment in T2I models. SC-DPO integrates safety constraints into\nthe general human preference calibration, aiming to maximize the likelihood of\ngenerating human-preferred samples while minimizing the safety cost of the\ngenerated outputs. In SC-DPO, we introduce a safety cost model to accurately\nquantify harmful levels for images, and train it effectively using the proposed\ncontrastive learning and cost anchoring objectives. To apply SC-DPO for\neffective T2I safety alignment, we constructed SCP-10K, a safety-constrained\npreference dataset containing rich harmful concepts, which blends\nsafety-constrained preference pairs under both harmful and clean instructions,\nfurther mitigating the trade-off between safety and sample quality.\nAdditionally, we propose a Dynamic Focusing Mechanism (DFM) for SC-DPO,\npromoting the model's learning of difficult preference pair samples. Extensive\nexperiments demonstrate that SC-DPO outperforms existing methods, effectively\ndefending against various NSFW content while maintaining optimal sample quality\nand human preference alignment. Additionally, SC-DPO exhibits resilience\nagainst adversarial prompts designed to generate harmful content.", "AI": {"tldr": "SC-DPO\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b89\u5168\u7ea6\u675f\u548c\u4eba\u7c7b\u504f\u597d\u6821\u51c6\uff0c\u5e73\u8861\u5b89\u5168\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u6709\u5bb3\u6982\u5ff5\u4e0b\u7684\u5b89\u5168\u6027\u6216\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faSC-DPO\u6846\u67b6\uff0c\u5305\u62ec\u5b89\u5168\u6210\u672c\u6a21\u578b\u3001\u5bf9\u6bd4\u5b66\u4e60\u548c\u6210\u672c\u951a\u5b9a\u76ee\u6807\uff0c\u4ee5\u53ca\u52a8\u6001\u805a\u7126\u673a\u5236\uff08DFM\uff09\u3002", "result": "SC-DPO\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u5fa1NSFW\u5185\u5bb9\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "conclusion": "SC-DPO\u4e3aT2I\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5b89\u5168\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5bf9\u6297\u6027\u63d0\u793a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.14633", "pdf": "https://arxiv.org/pdf/2504.14633", "abs": "https://arxiv.org/abs/2504.14633", "authors": ["Soo-joon Choi", "Ji-jun Park"], "title": "Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance", "categories": ["cs.CL"], "comment": null, "summary": "Financial event entity extraction is a crucial task for analyzing market\ndynamics and building financial knowledge graphs, yet it presents significant\nchallenges due to the specialized language and complex structures in financial\ntexts. Traditional approaches often rely on sequence labeling models, which can\nstruggle with long-range dependencies and the inherent complexity of extracting\nmultiple, potentially overlapping entities. Motivated by the advanced language\nunderstanding and generative capabilities of Large Language Models (LLMs), we\npropose a novel method that reframes financial event entity extraction as a\ntext-to-structured-output generation task. Our approach involves fine-tuning a\npre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly\ngenerate a structured representation, such as a JSON object, containing the\nextracted entities and their precise character spans from the input text. We\nevaluate our method on the challenging CCKS 2019 Financial Event Entity\nExtraction dataset, comparing its performance against strong sequence labeling\nbaselines, including SEBERTNets and sebertNets. Experimental results\ndemonstrate that our generative LLM method achieves a new state-of-the-art F1\nscore on this benchmark, significantly outperforming previous methods. Through\ndetailed quantitative analysis across event types, entity types, and instance\ncomplexity, as well as human evaluation, we show that our approach is more\neffective at handling the nuances of financial text and extracting high-quality\nentities. This work validates the potential of applying generative LLMs\ndirectly to complex, domain-specific information extraction tasks requiring\nstructured output.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u91d1\u878d\u4e8b\u4ef6\u5b9e\u4f53\u63d0\u53d6\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u76f4\u63a5\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u6587\u672c\u8bed\u8a00\u590d\u6742\u4e14\u5b9e\u4f53\u91cd\u53e0\uff0c\u4f20\u7edf\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u96be\u4ee5\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u591a\u5b9e\u4f53\u63d0\u53d6\u95ee\u9898\u3002", "method": "\u5c06\u91d1\u878d\u4e8b\u4ef6\u5b9e\u4f53\u63d0\u53d6\u4efb\u52a1\u91cd\u6784\u4e3a\u6587\u672c\u5230\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u751f\u6210\u4efb\u52a1\uff0c\u5229\u7528PEFT\u5fae\u8c03\u9884\u8bad\u7ec3LLM\u76f4\u63a5\u751f\u6210\u5305\u542b\u5b9e\u4f53\u53ca\u5176\u5b57\u7b26\u8de8\u5ea6\u7684JSON\u5bf9\u8c61\u3002", "result": "\u5728CCKS 2019\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0\u7684SOTA F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eSEBERTNets\u548csebertNets\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u751f\u6210\u5f0fLLM\u5728\u590d\u6742\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u5904\u7406\u91d1\u878d\u6587\u672c\u7684\u590d\u6742\u6027\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u5b9e\u4f53\u3002"}}
{"id": "2504.14294", "pdf": "https://arxiv.org/pdf/2504.14294", "abs": "https://arxiv.org/abs/2504.14294", "authors": ["Pourya Shamsolmoali", "Masoumeh Zareapoor", "Huiyu Zhou", "Michael Felsberg", "Dacheng Tao", "Xuelong Li"], "title": "From Missing Pieces to Masterpieces: Image Completion with Context-Adaptive Diffusion", "categories": ["cs.CV"], "comment": "Accepted in TPAMI", "summary": "Image completion is a challenging task, particularly when ensuring that\ngenerated content seamlessly integrates with existing parts of an image. While\nrecent diffusion models have shown promise, they often struggle with\nmaintaining coherence between known and unknown (missing) regions. This issue\narises from the lack of explicit spatial and semantic alignment during the\ndiffusion process, resulting in content that does not smoothly integrate with\nthe original image. Additionally, diffusion models typically rely on global\nlearned distributions rather than localized features, leading to\ninconsistencies between the generated and existing image parts. In this work,\nwe propose ConFill, a novel framework that introduces a Context-Adaptive\nDiscrepancy (CAD) model to ensure that intermediate distributions of known and\nunknown regions are closely aligned throughout the diffusion process. By\nincorporating CAD, our model progressively reduces discrepancies between\ngenerated and original images at each diffusion step, leading to contextually\naligned completion. Moreover, ConFill uses a new Dynamic Sampling mechanism\nthat adaptively increases the sampling rate in regions with high reconstruction\ncomplexity. This approach enables precise adjustments, enhancing detail and\nintegration in restored areas. Extensive experiments demonstrate that ConFill\noutperforms current methods, setting a new benchmark in image completion.", "AI": {"tldr": "ConFill\u662f\u4e00\u79cd\u65b0\u578b\u56fe\u50cf\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5dee\u5f02\u6a21\u578b\uff08CAD\uff09\u548c\u52a8\u6001\u91c7\u6837\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u4e0e\u539f\u59cb\u56fe\u50cf\u7684\u878d\u5408\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u8865\u5168\u4e2d\u96be\u4ee5\u4fdd\u6301\u5df2\u77e5\u4e0e\u672a\u77e5\u533a\u57df\u7684\u8fde\u8d2f\u6027\uff0c\u7f3a\u4e4f\u663e\u5f0f\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u4e0e\u539f\u59cb\u56fe\u50cf\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faConFill\u6846\u67b6\uff0c\u7ed3\u5408CAD\u6a21\u578b\u9010\u6b65\u5bf9\u9f50\u5df2\u77e5\u4e0e\u672a\u77e5\u533a\u57df\u7684\u4e2d\u95f4\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u52a8\u6001\u91c7\u6837\u673a\u5236\u5728\u9ad8\u590d\u6742\u5ea6\u533a\u57df\u81ea\u9002\u5e94\u589e\u52a0\u91c7\u6837\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cConFill\u5728\u56fe\u50cf\u8865\u5168\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6210\u4e3a\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "ConFill\u901a\u8fc7\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u52a8\u6001\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8865\u5168\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.14657", "pdf": "https://arxiv.org/pdf/2504.14657", "abs": "https://arxiv.org/abs/2504.14657", "authors": ["Yihan Lin", "Zhirong Bella Yu", "Simon Lee"], "title": "A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025", "summary": "Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\uff0c\u53d1\u73b0LLM\u5728\u5c0f\u89c4\u6a21\u7279\u5f81\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u96be\u4ee5\u4fdd\u6301\u771f\u5b9e\u5206\u5e03\u548c\u76f8\u5173\u6027\u3002", "motivation": "\u5408\u6210EHR\u6570\u636e\u53ef\u4fdd\u62a4\u9690\u79c1\u5e76\u652f\u6301\u533b\u7597\u5e94\u7528\uff0c\u4f46\u9700\u89e3\u51b3\u5176\u5728\u4e0d\u540c\u533b\u9662\u95f4\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u8bc4\u4f30\u5546\u4e1aLLM\u751f\u6210\u5408\u6210EHR\u7684\u80fd\u529b\uff0c\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u591a\u4e2a\u65b9\u9762\u3002", "result": "LLM\u5728\u5c0f\u89c4\u6a21\u7279\u5f81\u96c6\u4e0a\u53ef\u9760\uff0c\u4f46\u5728\u9ad8\u7ef4\u6570\u636e\u4e2d\u96be\u4ee5\u4fdd\u6301\u771f\u5b9e\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "LLM\u5728\u751f\u6210\u5408\u6210EHR\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u4ee5\u5e94\u5bf9\u9ad8\u7ef4\u6570\u636e\u7684\u6311\u6218\u3002"}}
{"id": "2504.14301", "pdf": "https://arxiv.org/pdf/2504.14301", "abs": "https://arxiv.org/abs/2504.14301", "authors": ["Nazia Aslam", "Kamal Nasrollahi"], "title": "Balancing Privacy and Action Performance: A Penalty-Driven Approach to Image Anonymization", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to CVPRW 2025", "summary": "The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u56fe\u50cf\u533f\u540d\u5316\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u533f\u540d\u5668\u4ee5\u5e73\u8861\u9690\u79c1\u6cc4\u6f0f\u4e0e\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u76d1\u63a7\u7cfb\u7edf\u7684\u53d1\u5c55\u5f15\u53d1\u4e86\u9690\u79c1\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5982\u4f55\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4e0d\u727a\u7272\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u60e9\u7f5a\u7684\u65b9\u6848\uff0c\u4f18\u5316\u533f\u540d\u5668\u4ee5\u51cf\u5c11\u9690\u79c1\u6cc4\u6f0f\u5e76\u4fdd\u6301\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9690\u79c1\u6cc4\u6f0f\u4e00\u81f4\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u7684\u5e73\u8861\uff0c\u7b26\u5408\u6b27\u76dfAI\u6cd5\u6848\u548cGDPR\u7684\u76d1\u7ba1\u6807\u51c6\u3002"}}
{"id": "2504.14669", "pdf": "https://arxiv.org/pdf/2504.14669", "abs": "https://arxiv.org/abs/2504.14669", "authors": ["Wei Zou", "Sen Yang", "Yu Bao", "Shujian Huang", "Jiajun Chen", "Shanbo Cheng"], "title": "Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data", "categories": ["cs.CL"], "comment": "11 pages, 4 figures", "summary": "The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.", "AI": {"tldr": "TRANS-ZERO\u662f\u4e00\u4e2a\u5229\u7528\u5355\u8bed\u6570\u636e\u548cLLM\u5185\u5728\u591a\u8bed\u8a00\u77e5\u8bc6\u7684\u81ea\u535a\u5f08\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9057\u4f20\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u504f\u597d\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u4e0e\u76d1\u7763\u65b9\u6cd5\u5ab2\u7f8e\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u548c\u707e\u96be\u6027\u9057\u5fd8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTRANS-ZERO\u6846\u67b6\uff0c\u7ed3\u5408\u9057\u4f20\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08G-MCTS\uff09\u548c\u504f\u597d\u4f18\u5316\uff0c\u4ec5\u4f7f\u7528\u5355\u8bed\u6570\u636e\u548cLLM\u7684\u591a\u8bed\u8a00\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5339\u914d\u5927\u89c4\u6a21\u5e76\u884c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u5728\u975e\u82f1\u8bed\u7ffb\u8bd1\u65b9\u5411\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "G-MCTS\u901a\u8fc7\u8fed\u4ee3\u7ffb\u8bd1\u63a2\u7d22\u8bed\u4e49\u4e00\u81f4\u7684\u5019\u9009\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u6846\u67b6\u6210\u529f\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2504.14306", "pdf": "https://arxiv.org/pdf/2504.14306", "abs": "https://arxiv.org/abs/2504.14306", "authors": ["Yitao Zhao", "Sen Lei", "Nanqing Liu", "Heng-Chao Li", "Turgay Celik", "Qing Zhu"], "title": "Exploring Generalizable Pre-training for Real-world Change Detection via Geometric Estimation", "categories": ["cs.CV"], "comment": "Submitted to IEEE TGRS", "summary": "As an essential procedure in earth observation system, change detection (CD)\naims to reveal the spatial-temporal evolution of the observation regions. A key\nprerequisite for existing change detection algorithms is aligned geo-references\nbetween multi-temporal images by fine-grained registration. However, in the\nmajority of real-world scenarios, a prior manual registration is required\nbetween the original images, which significantly increases the complexity of\nthe CD workflow. In this paper, we proposed a self-supervision motivated CD\nframework with geometric estimation, called \"MatchCD\". Specifically, the\nproposed MatchCD framework utilizes the zero-shot capability to optimize the\nencoder with self-supervised contrastive representation, which is reused in the\ndownstream image registration and change detection to simultaneously handle the\nbi-temporal unalignment and object change issues. Moreover, unlike the\nconventional change detection requiring segmenting the full-frame image into\nsmall patches, our MatchCD framework can directly process the original\nlarge-scale image (e.g., 6K*4K resolutions) with promising performance. The\nperformance in multiple complex scenarios with significant geometric distortion\ndemonstrates the effectiveness of our proposed framework.", "AI": {"tldr": "MatchCD\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u9a71\u52a8\u7684\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u4f30\u8ba1\u540c\u65f6\u89e3\u51b3\u591a\u65f6\u76f8\u56fe\u50cf\u672a\u5bf9\u9f50\u548c\u76ee\u6807\u53d8\u5316\u95ee\u9898\uff0c\u65e0\u9700\u624b\u52a8\u914d\u51c6\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u5927\u5c3a\u5ea6\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u53d8\u5316\u68c0\u6d4b\u7b97\u6cd5\u9700\u8981\u624b\u52a8\u914d\u51c6\u591a\u65f6\u76f8\u56fe\u50cf\uff0c\u589e\u52a0\u4e86\u5de5\u4f5c\u6d41\u7a0b\u7684\u590d\u6742\u6027\u3002MatchCD\u65e8\u5728\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u6cd5\u7b80\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "MatchCD\u5229\u7528\u96f6\u6837\u672c\u80fd\u529b\u4f18\u5316\u7f16\u7801\u5668\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5bf9\u6bd4\u8868\u793a\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u590d\u7528\uff0c\u76f4\u63a5\u5904\u7406\u5927\u5c3a\u5ea6\u56fe\u50cf\u3002", "result": "\u5728\u51e0\u4f55\u7578\u53d8\u663e\u8457\u7684\u590d\u6742\u573a\u666f\u4e2d\uff0cMatchCD\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MatchCD\u6846\u67b6\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u51e0\u4f55\u4f30\u8ba1\uff0c\u663e\u8457\u7b80\u5316\u4e86\u53d8\u5316\u68c0\u6d4b\u6d41\u7a0b\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.14690", "pdf": "https://arxiv.org/pdf/2504.14690", "abs": "https://arxiv.org/abs/2504.14690", "authors": ["Mehrnoush Shamsfard", "Zahra Saaberi", "Mostafa Karimi manesh", "Seyed Mohammad Hossein Hashemi", "Zahra Vatankhah", "Motahareh Ramezani", "Niki Pourazin", "Tara Zare", "Maryam Azimi", "Sarina Chitsaz", "Sama Khoraminejad", "Morteza Mahdavi Mortazavi", "Mohammad Mahdi Chizari", "Sahar Maleki", "Seyed Soroush Majd", "Mostafa Masumi", "Sayed Ali Musavi Khoeini", "Amir Mohseni", "Sogol Alipour"], "title": "FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; E.0"], "comment": "24 pages, 3 figures, 3 tables", "summary": "Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86FarsEval-PKBETS\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6ce2\u65af\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u51c6\u786e\u7387\u4f4e\u4e8e50%\u3002", "motivation": "\u7814\u7a76\u6ce2\u65af\u8bed\u7b49\u8d44\u6e90\u8f83\u5c11\u8bed\u8a00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b4000\u4e2a\u95ee\u9898\u7684\u6ce2\u65af\u8bed\u57fa\u51c6FarsEval-PKBETS\uff0c\u6db5\u76d6\u591a\u9886\u57df\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e09\u4e2a\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u4e09\u4e2a\u6a21\u578b\u7684\u5e73\u5747\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u8be5\u57fa\u51c6\u3002", "conclusion": "\u6ce2\u65af\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2504.14309", "pdf": "https://arxiv.org/pdf/2504.14309", "abs": "https://arxiv.org/abs/2504.14309", "authors": ["Ruoyan Xiong", "Huanbin Zhang", "Shentao Wang", "Hui He", "Yuke Hou", "Yue Zhang", "Yujie Cui", "Huipan Guan", "Shang Zhang"], "title": "FGSGT: Saliency-Guided Siamese Network Tracker Based on Key Fine-Grained Feature Information for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) images typically lack detailed features and have low\ncontrast, making it challenging for conventional feature extraction models to\ncapture discriminative target characteristics. As a result, trackers are often\naffected by interference from visually similar objects and are susceptible to\ntracking drift. To address these challenges, we propose a novel saliency-guided\nSiamese network tracker based on key fine-grained feature infor-mation. First,\nwe introduce a fine-grained feature parallel learning convolu-tional block with\na dual-stream architecture and convolutional kernels of varying sizes. This\ndesign captures essential global features from shallow layers, enhances feature\ndiversity, and minimizes the loss of fine-grained in-formation typically\nencountered in residual connections. In addition, we propose a multi-layer\nfine-grained feature fusion module that uses bilinear matrix multiplication to\neffectively integrate features across both deep and shallow layers. Next, we\nintroduce a Siamese residual refinement block that corrects saliency map\nprediction errors using residual learning. Combined with deep supervision, this\nmechanism progressively refines predictions, ap-plying supervision at each\nrecursive step to ensure consistent improvements in accuracy. Finally, we\npresent a saliency loss function to constrain the sali-ency predictions,\ndirecting the network to focus on highly discriminative fi-ne-grained features.\nExtensive experiment results demonstrate that the pro-posed tracker achieves\nthe highest precision and success rates on the PTB-TIR and LSOTB-TIR\nbenchmarks. It also achieves a top accuracy of 0.78 on the VOT-TIR 2015\nbenchmark and 0.75 on the VOT-TIR 2017 benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u5f15\u5bfc\u7684Siamese\u7f51\u7edc\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u5e76\u884c\u5b66\u4e60\u548c\u591a\u5c42\u7279\u5f81\u878d\u5408\uff0c\u89e3\u51b3\u4e86TIR\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "TIR\u56fe\u50cf\u7279\u5f81\u7ec6\u8282\u5c11\u3001\u5bf9\u6bd4\u5ea6\u4f4e\uff0c\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u96be\u4ee5\u6355\u6349\u76ee\u6807\u7279\u5f81\uff0c\u6613\u53d7\u5e72\u6270\u548c\u8ddf\u8e2a\u6f02\u79fb\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u6d41\u67b6\u6784\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u5e76\u884c\u5b66\u4e60\u6a21\u5757\u3001\u591a\u5c42\u7ec6\u7c92\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u3001Siamese\u6b8b\u5dee\u7ec6\u5316\u5757\u548c\u663e\u8457\u6027\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728PTB-TIR\u3001LSOTB-TIR\u548cVOT-TIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u9ad8\u7cbe\u5ea6\u548c\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u663e\u8457\u6027\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86TIR\u56fe\u50cf\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2504.14692", "pdf": "https://arxiv.org/pdf/2504.14692", "abs": "https://arxiv.org/abs/2504.14692", "authors": ["Songtao Jiang", "Yuan Wang", "Sibo Song", "Yan Zhang", "Zijie Meng", "Bohan Lei", "Jian Wu", "Jimeng Sun", "Zuozhu Liu"], "title": "OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding", "categories": ["cs.CL"], "comment": null, "summary": "The practical deployment of medical vision-language models (Med-VLMs)\nnecessitates seamless integration of textual data with diverse visual\nmodalities, including 2D/3D images and videos, yet existing models typically\nemploy separate encoders for different modalities. To address this limitation,\nwe present OmniV-Med, a unified framework for multimodal medical understanding.\nOur technical contributions are threefold: First, we construct\nOmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K\ninstructional samples spanning 14 medical image modalities and 11 clinical\ntasks. Second, we devise a rotary position-adaptive encoder that processes\nmulti-resolution 2D/3D images and videos within a unified architecture,\ndiverging from conventional modality-specific encoders. Third, we introduce a\nmedical-aware token pruning mechanism that exploits spatial-temporal redundancy\nin volumetric data (e.g., consecutive CT slices) and medical videos,\neffectively reducing 60\\% of visual tokens without performance degradation.\nEmpirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art\nperformance on 7 benchmarks spanning 2D/3D medical imaging and video\nunderstanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains\ncomparable performance while requiring only 8 RTX3090 GPUs for training and\nsupporting efficient long-video inference. Data, code and model will be\nreleased.", "AI": {"tldr": "OmniV-Med\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u533b\u7597\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u7efc\u5408\u6570\u636e\u96c6\u3001\u8bbe\u8ba1\u81ea\u9002\u5e94\u7f16\u7801\u5668\u548c\u5f15\u5165\u533b\u5b66\u611f\u77e5\u7684\u4ee4\u724c\u4fee\u526a\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u57282D/3D\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u4e0a\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5bf9\u4e0d\u540c\u6a21\u6001\u4f7f\u7528\u72ec\u7acb\u7684\u7f16\u7801\u5668\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "1. \u6784\u5efa\u5305\u542b252K\u6837\u672c\u7684\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u96c6\uff1b2. \u8bbe\u8ba1\u65cb\u8f6c\u4f4d\u7f6e\u81ea\u9002\u5e94\u7f16\u7801\u5668\uff1b3. \u5f15\u5165\u533b\u5b66\u611f\u77e5\u4ee4\u724c\u4fee\u526a\u673a\u5236\u4ee5\u51cf\u5c11\u5197\u4f59\u3002", "result": "OmniV-Med-7B\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u8f7b\u91cf\u7ea7\u7248\u672c\uff08OmniV-Med-1.5B\uff09\u6027\u80fd\u76f8\u5f53\u4e14\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u4f4e\u3002", "conclusion": "OmniV-Med\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u9ad8\u6548\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u533b\u5b66\u7406\u89e3\u7684\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.14311", "pdf": "https://arxiv.org/pdf/2504.14311", "abs": "https://arxiv.org/abs/2504.14311", "authors": ["Ruoyan Xiong", "Yuke Hou", "Princess Retor Torboh", "Hui He", "Huanbin Zhang", "Yue Zhang", "Yanpin Wang", "Huipan Guan", "Shang Zhang"], "title": "DCFG: Diverse Cross-Channel Fine-Grained Feature Learning and Progressive Fusion Siamese Tracker for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenge of capturing highly discriminative features in\nther-mal infrared (TIR) tracking, we propose a novel Siamese tracker based on\ncross-channel fine-grained feature learning and progressive fusion. First, we\nintroduce a cross-channel fine-grained feature learning network that employs\nmasks and suppression coefficients to suppress dominant target features,\nen-abling the tracker to capture more detailed and subtle information. The\nnet-work employs a channel rearrangement mechanism to enhance efficient\nin-formation flow, coupled with channel equalization to reduce parameter count.\nAdditionally, we incorporate layer-by-layer combination units for ef-fective\nfeature extraction and fusion, thereby minimizing parameter redun-dancy and\ncomputational complexity. The network further employs feature redirection and\nchannel shuffling strategies to better integrate fine-grained details. Second,\nwe propose a specialized cross-channel fine-grained loss function designed to\nguide feature groups toward distinct discriminative re-gions of the target,\nthus improving overall target representation. This loss function includes an\ninter-channel loss term that promotes orthogonality be-tween channels,\nmaximizing feature diversity and facilitating finer detail capture. Extensive\nexperiments demonstrate that our proposed tracker achieves the highest\naccuracy, scoring 0.81 on the VOT-TIR 2015 and 0.78 on the VOT-TIR 2017\nbenchmark, while also outperforming other methods across all evaluation metrics\non the LSOTB-TIR and PTB-TIR benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u901a\u9053\u7ec6\u7c92\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u6e10\u8fdb\u878d\u5408\u7684\u65b0\u578bSiamese\u8ddf\u8e2a\u5668\uff0c\u7528\u4e8e\u70ed\u7ea2\u5916\uff08TIR\uff09\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u70ed\u7ea2\u5916\u8ddf\u8e2a\u4e2d\u96be\u4ee5\u6355\u6349\u9ad8\u5224\u522b\u6027\u7279\u5f81\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u8de8\u901a\u9053\u7ec6\u7c92\u5ea6\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\uff0c\u7ed3\u5408\u63a9\u7801\u548c\u6291\u5236\u7cfb\u6570\u6291\u5236\u4e3b\u5bfc\u76ee\u6807\u7279\u5f81\uff0c\u5f15\u5165\u901a\u9053\u91cd\u6392\u548c\u5747\u8861\u673a\u5236\uff0c\u4ee5\u53ca\u7279\u5f81\u91cd\u5b9a\u5411\u548c\u901a\u9053\u6df7\u6d17\u7b56\u7565\u3002\u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u8de8\u901a\u9053\u7ec6\u7c92\u5ea6\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728VOT-TIR 2015\u548c2017\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u52300.81\u548c0.78\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728LSOTB-TIR\u548cPTB-TIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u5b66\u4e60\u548c\u6e10\u8fdb\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70ed\u7ea2\u5916\u8ddf\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14707", "pdf": "https://arxiv.org/pdf/2504.14707", "abs": "https://arxiv.org/abs/2504.14707", "authors": ["Ratna Kandala", "Katie Hoemann"], "title": "Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives", "categories": ["cs.CL"], "comment": null, "summary": "This study explores BERTopic's potential for modeling open-ended Belgian\nDutch daily narratives, contrasting its performance with Latent Dirichlet\nAllocation (LDA) and KMeans. Although LDA scores well on certain automated\nmetrics, human evaluations reveal semantically irrelevant co-occurrences,\nhighlighting the limitations of purely statistic-based methods. In contrast,\nBERTopic's reliance on contextual embeddings yields culturally resonant themes,\nunderscoring the importance of hybrid evaluation frameworks that account for\nmorphologically rich languages. KMeans performed less coherently than prior\nresearch suggested, pointing to the unique challenges posed by personal\nnarratives. Our findings emphasize the need for robust generalization in NLP\nmodels, especially in underrepresented linguistic contexts.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u6bd4\u4e86BERTopic\u3001LDA\u548cKMeans\u5728\u6bd4\u5229\u65f6\u8377\u5170\u8bed\u65e5\u5e38\u53d9\u4e8b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0BERTopic\u5728\u8bed\u4e49\u76f8\u5173\u6027\u4e0a\u4f18\u4e8eLDA\u548cKMeans\u3002", "motivation": "\u63a2\u7d22BERTopic\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff08\u5982\u6bd4\u5229\u65f6\u8377\u5170\u8bed\uff09\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5bf9\u6bd4\u5176\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08LDA\u548cKMeans\uff09\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528BERTopic\u3001LDA\u548cKMeans\u5bf9\u5f00\u653e\u5f0f\u7684\u6bd4\u5229\u65f6\u8377\u5170\u8bed\u65e5\u5e38\u53d9\u4e8b\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5bf9\u6bd4\u6027\u80fd\u3002", "result": "BERTopic\u751f\u6210\u7684\u8bed\u4e49\u66f4\u76f8\u5173\u4e14\u6587\u5316\u5171\u9e23\u66f4\u5f3a\uff0c\u800cLDA\u548cKMeans\u5728\u8bed\u4e49\u76f8\u5173\u6027\u548c\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728NLP\u6a21\u578b\u4e2d\u7ed3\u5408\u4e0a\u4e0b\u6587\u5d4c\u5165\u548c\u6df7\u5408\u8bc4\u4f30\u6846\u67b6\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u3002"}}
{"id": "2504.14335", "pdf": "https://arxiv.org/pdf/2504.14335", "abs": "https://arxiv.org/abs/2504.14335", "authors": ["Zhengbo Zhang", "Yuxi Zhou", "Duo Peng", "Joo-Hwee Lim", "Zhigang Tu", "De Wen Soh", "Lin Geng Foo"], "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by cvpr2025", "summary": "One-shot controllable video editing (OCVE) is an important yet challenging\ntask, aiming to propagate user edits that are made -- using any image editing\ntool -- on the first frame of a video to all subsequent frames, while ensuring\ncontent consistency between edited frames and source frames. To achieve this,\nprior methods employ DDIM inversion to transform source frames into latent\nnoise, which is then fed into a pre-trained diffusion model, conditioned on the\nuser-edited first frame, to generate the edited video. However, the DDIM\ninversion process accumulates errors, which hinder the latent noise from\naccurately reconstructing the source frames, ultimately compromising content\nconsistency in the generated edited frames. To overcome it, our method\neliminates the need for DDIM inversion by performing OCVE through a novel\nperspective based on visual prompting. Furthermore, inspired by consistency\nmodels that can perform multi-step consistency sampling to generate a sequence\nof content-consistent images, we propose a content consistency sampling (CCS)\nto ensure content consistency between the generated edited frames and the\nsource frames. Moreover, we introduce a temporal-content consistency sampling\n(TCS) based on Stein Variational Gradient Descent to ensure temporal\nconsistency across the edited frames. Extensive experiments validate the\neffectiveness of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700DDIM\u53cd\u8f6c\u7684\u5355\u6b21\u53ef\u63a7\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u548c\u5185\u5bb9\u4e00\u81f4\u6027\u91c7\u6837\u786e\u4fdd\u7f16\u8f91\u5e27\u4e0e\u6e90\u5e27\u7684\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0DDIM\u53cd\u8f6c\u7d2f\u79ef\u8bef\u5dee\u5bfc\u81f4\u5185\u5bb9\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u63d0\u793a\u66ff\u4ee3DDIM\u53cd\u8f6c\uff0c\u5e76\u5f15\u5165\u5185\u5bb9\u4e00\u81f4\u6027\u91c7\u6837\uff08CCS\uff09\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u91c7\u6837\uff08TCS\uff09\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.14738", "pdf": "https://arxiv.org/pdf/2504.14738", "abs": "https://arxiv.org/abs/2504.14738", "authors": ["Reya Vir", "Shreya Shankar", "Harrison Chase", "Will Fu-Hinthorn", "Aditya Parameswaran"], "title": "PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main Conference", "summary": "Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.", "AI": {"tldr": "PROMPTEVALS\u662f\u4e00\u4e2a\u5305\u542b2087\u4e2aLLM\u7ba1\u9053\u63d0\u793a\u548c12623\u4e2a\u65ad\u8a00\u6807\u51c6\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u9ad8LLM\u5728\u751f\u6210\u4e2d\u7684\u53ef\u9760\u6027\u3002\u5fae\u8c03\u7684Mistral\u548cLlama 3\u6a21\u578b\u5728\u751f\u6210\u65ad\u8a00\u65b9\u9762\u6bd4GPT-4o\u8868\u73b0\u66f4\u597d\u3002", "motivation": "LLM\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5e38\u65e0\u6cd5\u6ee1\u8db3\u5f00\u53d1\u8005\u671f\u671b\uff0c\u9700\u8981\u65ad\u8a00\u6216\u62a4\u680f\u6765\u63d0\u9ad8\u53ef\u9760\u6027\u3002\u4f46\u786e\u5b9a\u9002\u5408\u4efb\u52a1\u7684\u65ad\u8a00\u6807\u51c6\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5f15\u5165PROMPTEVALS\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u96c6\u8bc4\u4f30\u95ed\u6e90\u548c\u5f00\u6e90\u6a21\u578b\u751f\u6210\u65ad\u8a00\u7684\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u7684Mistral\u548cLlama 3\u6a21\u578b\u5e73\u5747\u6bd4GPT-4o\u8868\u73b0\u597d20.93%\uff0c\u4e14\u5ef6\u8fdf\u66f4\u4f4e\u3002", "conclusion": "PROMPTEVALS\u6570\u636e\u96c6\u6709\u671b\u63a8\u52a8LLM\u53ef\u9760\u6027\u3001\u5bf9\u9f50\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.14337", "pdf": "https://arxiv.org/pdf/2504.14337", "abs": "https://arxiv.org/abs/2504.14337", "authors": ["Josef Taher", "Eric Hyypp\u00e4", "Matti Hyypp\u00e4", "Klaara Salolahti", "Xiaowei Yu", "Leena Matikainen", "Antero Kukko", "Matti Lehtom\u00e4ki", "Harri Kaartinen", "Sopitta Thurachen", "Paula Litkey", "Ville Luoma", "Markus Holopainen", "Gefei Kong", "Hongchao Fan", "Petri R\u00f6nnholm", "Antti Polvivaara", "Samuli Junttila", "Mikko Vastaranta", "Stefano Puliti", "Rasmus Astrup", "Joel Kostensalo", "Mari Myllym\u00e4ki", "Maksymilian Kulicki", "Krzysztof Stere\u0144czak", "Raul de Paula Pires", "Ruben Valbuena", "Juan Pedro Carbonell-Rivera", "Jes\u00fas Torralba", "Yi-Chen Chen", "Lukas Winiwarter", "Markus Hollaus", "Gottfried Mandlburger", "Narges Takhtkeshha", "Fabio Remondino", "Maciej Lisiewicz", "Bart\u0142omiej Kraszewski", "Xinlian Liang", "Jianchang Chen", "Eero Ahokas", "Kirsi Karila", "Eugeniu Vezeteu", "Petri Manninen", "Roope N\u00e4si", "Heikki Hyyti", "Siiri Pyykk\u00f6nen", "Peilun Hu", "Juha Hyypp\u00e4"], "title": "Multispectral airborne laser scanning for tree species classification: a benchmark of machine learning and deep learning algorithms", "categories": ["cs.CV"], "comment": null, "summary": "Climate-smart and biodiversity-preserving forestry demands precise\ninformation on forest resources, extending to the individual tree level.\nMultispectral airborne laser scanning (ALS) has shown promise in automated\npoint cloud processing and tree segmentation, but challenges remain in\nidentifying rare tree species and leveraging deep learning techniques. This\nstudy addresses these gaps by conducting a comprehensive benchmark of machine\nlearning and deep learning methods for tree species classification. For the\nstudy, we collected high-density multispectral ALS data (>1000 pts/m$^2$) at\nthree wavelengths using the FGI-developed HeliALS system, complemented by\nexisting Optech Titan data (35 pts/m$^2$), to evaluate the species\nclassification accuracy of various algorithms in a test site located in\nSouthern Finland. Based on 5261 test segments, our findings demonstrate that\npoint-based deep learning methods, particularly a point transformer model,\noutperformed traditional machine learning and image-based deep learning\napproaches on high-density multispectral point clouds. For the high-density ALS\ndataset, a point transformer model provided the best performance reaching an\noverall (macro-average) accuracy of 87.9% (74.5%) with a training set of 1065\nsegments and 92.0% (85.1%) with 5000 training segments. The best image-based\ndeep learning method, DetailView, reached an overall (macro-average) accuracy\nof 84.3% (63.9%), whereas a random forest (RF) classifier achieved an overall\n(macro-average) accuracy of 83.2% (61.3%). Importantly, the overall\nclassification accuracy of the point transformer model on the HeliALS data\nincreased from 73.0% with no spectral information to 84.7% with single-channel\nreflectance, and to 87.9% with spectral information of all the three channels.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u591a\u5149\u8c31ALS\u6570\u636e\u5728\u6811\u79cd\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u57fa\u4e8e\u70b9\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u70b9\u53d8\u6362\u5668\u6a21\u578b\uff09\u5728\u9ad8\u5bc6\u5ea6\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u6c14\u5019\u667a\u80fd\u548c\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u7684\u6797\u4e1a\u9700\u8981\u7cbe\u786e\u7684\u68ee\u6797\u8d44\u6e90\u4fe1\u606f\uff0c\u5305\u62ec\u5355\u682a\u6811\u79cd\u7684\u8bc6\u522b\uff0c\u4f46\u76ee\u524d\u7684\u6280\u672f\u5728\u7a00\u6709\u6811\u79cd\u8bc6\u522b\u548c\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u9ad8\u5bc6\u5ea6\u591a\u5149\u8c31ALS\u6570\u636e\uff08HeliALS\u7cfb\u7edf\uff09\u548c\u73b0\u6709Optech Titan\u6570\u636e\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u7b97\u6cd5\u5728\u82ac\u5170\u5357\u90e8\u6d4b\u8bd5\u70b9\u7684\u6811\u79cd\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u70b9\u53d8\u6362\u5668\u6a21\u578b\u5728\u9ad8\u5bc6\u5ea6\u6570\u636e\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u603b\u4f53\u51c6\u786e\u7387\u8fbe87.9%\uff08\u5b8f\u5e73\u574774.5%\uff09\uff0c\u5149\u8c31\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u70b9\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u5149\u8c31ALS\u6570\u636e\u6811\u79cd\u5206\u7c7b\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u5149\u8c31\u4fe1\u606f\u5bf9\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.14766", "pdf": "https://arxiv.org/pdf/2504.14766", "abs": "https://arxiv.org/abs/2504.14766", "authors": ["Saniya Karwa", "Navpreet Singh"], "title": "Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Understanding the inner workings of neural embeddings, particularly in models\nsuch as BERT, remains a challenge because of their high-dimensional and opaque\nnature. This paper proposes a framework for uncovering the specific dimensions\nof vector embeddings that encode distinct linguistic properties (LPs). We\nintroduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which\nisolates ten key linguistic features such as synonymy, negation, tense, and\nquantity. Using this dataset, we analyze BERT embeddings with various methods,\nincluding the Wilcoxon signed-rank test, mutual information, and recursive\nfeature elimination, to identify the most influential dimensions for each LP.\nWe introduce a new metric, the Embedding Dimension Impact (EDI) score, which\nquantifies the relevance of each embedding dimension to a LP. Our findings show\nthat certain properties, such as negation and polarity, are robustly encoded in\nspecific dimensions, while others, like synonymy, exhibit more complex\npatterns. This study provides insights into the interpretability of embeddings,\nwhich can guide the development of more transparent and optimized language\nmodels, with implications for model bias mitigation and the responsible\ndeployment of AI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u63ed\u793aBERT\u7b49\u9ad8\u7ef4\u4e0d\u900f\u660e\u6a21\u578b\u4e2d\u5411\u91cf\u5d4c\u5165\u7684\u7279\u5b9a\u7ef4\u5ea6\u5982\u4f55\u7f16\u7801\u4e0d\u540c\u8bed\u8a00\u5c5e\u6027\uff08LPs\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u6570\u636e\u96c6LDSP-10\u548c\u5ea6\u91cf\u6807\u51c6EDI\u5206\u6570\u3002", "motivation": "\u7406\u89e3\u795e\u7ecf\u5d4c\u5165\u7684\u5185\u90e8\u673a\u5236\uff0c\u5c24\u5176\u662fBERT\u7b49\u9ad8\u7ef4\u4e0d\u900f\u660e\u6a21\u578b\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u4f7f\u7528LDSP-10\u6570\u636e\u96c6\uff0c\u7ed3\u5408Wilcoxon\u7b26\u53f7\u79e9\u68c0\u9a8c\u3001\u4e92\u4fe1\u606f\u548c\u9012\u5f52\u7279\u5f81\u6d88\u9664\u7b49\u65b9\u6cd5\uff0c\u5206\u6790BERT\u5d4c\u5165\uff0c\u5e76\u5f15\u5165EDI\u5206\u6570\u91cf\u5316\u7ef4\u5ea6\u5bf9LP\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5426\u5b9a\u548c\u6781\u6027\u7b49\u5c5e\u6027\u5728\u7279\u5b9a\u7ef4\u5ea6\u4e2d\u7f16\u7801\u8f83\u5f3a\uff0c\u800c\u540c\u4e49\u6027\u5219\u8868\u73b0\u51fa\u66f4\u590d\u6742\u7684\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5d4c\u5165\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u900f\u660e\u548c\u4f18\u5316\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5bf9\u6a21\u578b\u504f\u89c1\u7f13\u89e3\u548cAI\u7cfb\u7edf\u8d1f\u8d23\u4efb\u90e8\u7f72\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2504.14348", "pdf": "https://arxiv.org/pdf/2504.14348", "abs": "https://arxiv.org/abs/2504.14348", "authors": ["Le Wang", "Zonghao Ying", "Tianyuan Zhang", "Siyuan Liang", "Shengshan Hu", "Mingchuan Zhang", "Aishan Liu", "Xianglong Liu"], "title": "Manipulating Multimodal Agents via Cross-Modal Prompt Injection", "categories": ["cs.CV"], "comment": "17 pages, 4 figures", "summary": "The emergence of multimodal large language models has redefined the agent\nparadigm by integrating language and vision modalities with external data\nsources, enabling agents to better interpret human instructions and execute\nincreasingly complex tasks. However, in this work, we identify a critical yet\npreviously overlooked security vulnerability in multimodal agents: cross-modal\nprompt injection attacks. To exploit this vulnerability, we propose\nCrossInject, a novel attack framework in which attackers embed adversarial\nperturbations across multiple modalities to align with target malicious\ncontent, allowing external instructions to hijack the agent's decision-making\nprocess and execute unauthorized tasks. Our approach consists of two key\ncomponents. First, we introduce Visual Latent Alignment, where we optimize\nadversarial features to the malicious instructions in the visual embedding\nspace based on a text-to-image generative model, ensuring that adversarial\nimages subtly encode cues for malicious task execution. Subsequently, we\npresent Textual Guidance Enhancement, where a large language model is leveraged\nto infer the black-box defensive system prompt through adversarial meta\nprompting and generate an malicious textual command that steers the agent's\noutput toward better compliance with attackers' requests. Extensive experiments\ndemonstrate that our method outperforms existing injection attacks, achieving\nat least a +26.4% increase in attack success rates across diverse tasks.\nFurthermore, we validate our attack's effectiveness in real-world multimodal\nautonomous agents, highlighting its potential implications for safety-critical\napplications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u7684\u65b0\u578b\u5b89\u5168\u6f0f\u6d1e\u653b\u51fb\u65b9\u6cd5CrossInject\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u6210\u529f\u7387\u9ad8\u4e14\u5f71\u54cd\u6df1\u8fdc\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u63d0\u5347\u4e86\u4ee3\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5b89\u5168\u6f0f\u6d1e\uff08\u8de8\u6a21\u6001\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff09\u88ab\u5ffd\u89c6\uff0c\u53ef\u80fd\u5bfc\u81f4\u6076\u610f\u6307\u4ee4\u52ab\u6301\u4ee3\u7406\u51b3\u7b56\u3002", "method": "\u63d0\u51faCrossInject\u6846\u67b6\uff0c\u5305\u542b\u89c6\u89c9\u6f5c\u5728\u5bf9\u9f50\u548c\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u4e24\u90e8\u5206\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u6270\u52a8\u548c\u9ed1\u76d2\u9632\u5fa1\u7cfb\u7edf\u63d0\u793a\u63a8\u65ad\u5b9e\u73b0\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u81f3\u5c1126.4%\uff0c\u5e76\u5728\u5b9e\u9645\u591a\u6a21\u6001\u81ea\u4e3b\u4ee3\u7406\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u4ee3\u7406\u7684\u5b89\u5168\u9690\u60a3\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u5177\u6709\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2504.14772", "pdf": "https://arxiv.org/pdf/2504.14772", "abs": "https://arxiv.org/abs/2504.14772", "authors": ["Luyang Fang", "Xiaowei Yu", "Jiazhang Cai", "Yongkai Chen", "Shushan Wu", "Zhengliang Liu", "Zhenyuan Yang", "Haoran Lu", "Xilin Gong", "Yufang Liu", "Terry Ma", "Wei Ruan", "Ali Abbasi", "Jing Zhang", "Tao Wang", "Ehsan Latif", "Wei Liu", "Wei Zhang", "Soheil Kolouri", "Xiaoming Zhai", "Dajiang Zhu", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u548c\u6570\u636e\u96c6\u84b8\u998f\uff08DD\uff09\u4e24\u79cd\u4e92\u8865\u8303\u5f0f\uff0c\u65e8\u5728\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u540c\u65f6\u4fdd\u7559\u5176\u63a8\u7406\u80fd\u529b\u548c\u8bed\u8a00\u591a\u6837\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u7684\u6574\u5408\u6f5c\u529b\u4e0e\u5e94\u7528\u3002", "motivation": "\u968f\u7740LLMs\u7684\u6307\u6570\u589e\u957f\uff0c\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42\u6025\u5267\u589e\u52a0\uff0c\u4e9f\u9700\u9ad8\u6548\u7684\u538b\u7f29\u7b56\u7565\u4ee5\u89e3\u51b3\u6a21\u578b\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u4fdd\u7559\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86KD\u7684\u5173\u952e\u65b9\u6cd5\uff08\u5982\u4efb\u52a1\u5bf9\u9f50\u3001\u591a\u6559\u5e08\u6846\u67b6\uff09\u548cDD\u6280\u672f\uff08\u5982\u68af\u5ea6\u5339\u914d\u3001\u751f\u6210\u5408\u6210\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e8c\u8005\u7684\u6574\u5408\u7b56\u7565\u3002", "result": "KD\u548cDD\u7684\u6574\u5408\u4e3aLLMs\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u538b\u7f29\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u7597\u548c\u6559\u80b2\u7b49\u9886\u57df\uff0c\u4f46\u4fdd\u7559\u63a8\u7406\u80fd\u529b\u548c\u9002\u5e94\u52a8\u6001\u6570\u636e\u4ecd\u662f\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408KD\u548cDD\u539f\u5219\uff0c\u8bba\u6587\u4e3a\u53ef\u6301\u7eed\u3001\u8d44\u6e90\u9ad8\u6548\u7684LLMs\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u8bc4\u4f30\u534f\u8bae\u548c\u52a8\u6001\u9002\u5e94\u7b49\u95ee\u9898\u3002"}}
{"id": "2504.14359", "pdf": "https://arxiv.org/pdf/2504.14359", "abs": "https://arxiv.org/abs/2504.14359", "authors": ["Kyle Buettner", "Jacob Emmerson", "Adriana Kovashka"], "title": "A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u91cd\u6807\u9898\u7b56\u7565\uff0c\u901a\u8fc7\u4fee\u6539\u82f1\u6587\u6807\u9898\u7684\u63cf\u8ff0\u6765\u63d0\u5347\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u611f\u77e5\u591a\u6837\u6027\u7684\u7406\u89e3\uff0c\u5e76\u5728\u5fb7\u6587\u548c\u65e5\u6587\u7684\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\u4f7f\u7528\u8005\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u611f\u77e5\u504f\u89c1\u548c\u6a21\u578b\u7075\u6d3b\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u8de8\u6587\u5316\u611f\u77e5\u591a\u6837\u6027\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u6a21\u6001\u91cd\u6807\u9898\u7b56\u7565\uff0c\u901a\u8fc7\u4fee\u6539\u82f1\u6587\u6807\u9898\u7684\u63cf\u8ff0\uff0c\u5e76\u7ed3\u5408\u76ee\u6807\u8bed\u8a00\u7684\u672c\u571f\u4f7f\u7528\u8005\u6570\u636e\u8fdb\u884c\u591a\u6a21\u6001\u673a\u5236\u4f18\u5316\u3002", "result": "\u5728\u5fb7\u6587\u548c\u65e5\u6587\u7684\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u53ec\u56de\u7387\u63d0\u5347\u4e863.5%\uff0c\u5728\u975e\u6bcd\u8bed\u9519\u8bef\u6848\u4f8b\u4e2d\u63d0\u5347\u4e864.7%\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u63d0\u5347\u4e86\u591a\u8bed\u8a00VLMs\u5bf9\u611f\u77e5\u591a\u6837\u6027\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u6570\u636e\u96c6\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u7684\u5206\u6790\u673a\u5236\u3002"}}
{"id": "2504.14804", "pdf": "https://arxiv.org/pdf/2504.14804", "abs": "https://arxiv.org/abs/2504.14804", "authors": ["Jiaxin GUO", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Zongyao Li", "Hengchao Shang", "Daimeng Wei", "Hao Yang"], "title": "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u7684\u81ea\u52a8\u8bc4\u4f30\u73b0\u72b6\uff0c\u5206\u6790\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u81ea\u52a8\u8bc4\u4f30\u4ecd\u5b58\u5728\u8bf8\u591a\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u6709\u53c2\u8003\u548c\u65e0\u53c2\u8003\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u4f20\u7edf\u6307\u6807\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u6307\u6807\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u6807\u3002", "result": "\u6307\u51fa\u4e86\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5982\u53c2\u8003\u6587\u672c\u591a\u6837\u6027\u4e0d\u8db3\u3001\u4f9d\u8d56\u53e5\u5b50\u7ea7\u5bf9\u9f50\u4fe1\u606f\u4ee5\u53caLLM\u8bc4\u4f30\u65b9\u6cd5\u7684\u504f\u5dee\u548c\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u5f00\u53d1\u66f4\u53cb\u597d\u7684\u6587\u6863\u7ea7\u8bc4\u4f30\u65b9\u6cd5\u548c\u66f4\u7a33\u5065\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5bf9\u53e5\u5b50\u7ea7\u4fe1\u606f\u7684\u4f9d\u8d56\u3002"}}
{"id": "2504.14371", "pdf": "https://arxiv.org/pdf/2504.14371", "abs": "https://arxiv.org/abs/2504.14371", "authors": ["Peixi Wu", "Bosong Chai", "Menghua Zheng", "Wei Li", "Zhangchi Hu", "Jie Chen", "Zheyu Zhang", "Hebei Li", "Xiaoyan Sun"], "title": "Efficient Spiking Point Mamba for Point Cloud Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Bio-inspired Spiking Neural Networks (SNNs) provide an energy-efficient way\nto extract 3D spatio-temporal features. However, existing 3D SNNs have\nstruggled with long-range dependencies until the recent emergence of Mamba,\nwhich offers superior computational efficiency and sequence modeling\ncapability. In this work, we propose Spiking Point Mamba (SPM), the first\nMamba-based SNN in the 3D domain. Due to the poor performance of simply\ntransferring Mamba to 3D SNNs, SPM is designed to utilize both the sequence\nmodeling capabilities of Mamba and the temporal feature extraction of SNNs.\nSpecifically, we first introduce Hierarchical Dynamic Encoding (HDE), an\nimproved direct encoding method that effectively introduces dynamic temporal\nmechanism, thereby facilitating temporal interactions. Then, we propose a\nSpiking Mamba Block (SMB), which builds upon Mamba while learning\ninter-time-step features and minimizing information loss caused by spikes.\nFinally, to further enhance model performance, we adopt an asymmetric SNN-ANN\narchitecture for spike-based pre-training and finetune. Compared with the\nprevious state-of-the-art SNN models, SPM improves OA by +6.2%, +6.1%, and\n+7.4% on three variants of ScanObjectNN, and boosts instance mIOU by +1.9% on\nShapeNetPart. Meanwhile, its energy consumption is at least 3.5x lower than\nthat of its ANN counterpart. The code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u76843D\u8109\u51b2\u795e\u7ecf\u7f51\u7edcSPM\uff0c\u7ed3\u5408\u4e86Mamba\u7684\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u548cSNN\u7684\u65f6\u5e8f\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u73b0\u67093D SNN\u5728\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800cMamba\u7684\u9ad8\u6548\u8ba1\u7b97\u548c\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86Hierarchical Dynamic Encoding (HDE)\u548cSpiking Mamba Block (SMB)\uff0c\u5e76\u91c7\u7528\u975e\u5bf9\u79f0SNN-ANN\u67b6\u6784\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5728ScanObjectNN\u548cShapeNetPart\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u80fd\u8017\u6bd4ANN\u4f4e\u81f3\u5c113.5\u500d\u3002", "conclusion": "SPM\u662f\u9996\u4e2a\u57fa\u4e8eMamba\u76843D SNN\uff0c\u6210\u529f\u7ed3\u5408\u4e86Mamba\u548cSNN\u7684\u4f18\u52bf\uff0c\u4e3a\u9ad8\u6548\u65f6\u5e8f\u7279\u5f81\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14808", "pdf": "https://arxiv.org/pdf/2504.14808", "abs": "https://arxiv.org/abs/2504.14808", "authors": ["Mario M. Kubek", "Shiraj Pokharel", "Thomas B\u00f6hme", "Emma L. McDaniel", "Herwig Unger", "Armin R. Mikler"], "title": "On Self-improving Token Embeddings", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50, 68T07", "I.2.6; I.2.7; H.3.3"], "comment": "18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025", "summary": "This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4f18\u5316\u9884\u8bad\u7ec3\u9759\u6001\u8bcd\u6216\u6807\u8bb0\u5d4c\u5165\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u8bed\u6599\u4e2d\u76f8\u90bb\u6807\u8bb0\u7684\u5d4c\u5165\uff0c\u6301\u7eed\u66f4\u65b0\u6bcf\u4e2a\u6807\u8bb0\u7684\u8868\u793a\uff0c\u5305\u62ec\u90a3\u4e9b\u6ca1\u6709\u9884\u5206\u914d\u5d4c\u5165\u7684\u6807\u8bb0\u3002\u8be5\u65b9\u6cd5\u72ec\u7acb\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\uff0c\u9002\u7528\u4e8e\u8bed\u6599\u63a2\u7d22\u3001\u6982\u5ff5\u641c\u7d22\u548c\u8bcd\u4e49\u6d88\u6b67\u7b49\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u5d4c\u5165\u4e2d\u7684\u8bcd\u6c47\u5916\u95ee\u9898\u548c\u901a\u7528\u5d4c\u5165\u5728\u7279\u5b9a\u9886\u57df\u4e2d\u7684\u4e0d\u9002\u7528\u6027\uff0c\u63d0\u5347\u6807\u8bb0\u5728\u4e3b\u9898\u540c\u8d28\u8bed\u6599\u4e2d\u7684\u8868\u793a\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u76f8\u90bb\u6807\u8bb0\u7684\u5d4c\u5165\u6301\u7eed\u66f4\u65b0\u6807\u8bb0\u8868\u793a\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u8bed\u6599\uff0c\u4e0d\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u3002", "result": "\u5728NOAA\u98ce\u66b4\u4e8b\u4ef6\u6570\u636e\u5e93\u4e2d\u5e94\u7528\uff0c\u6539\u8fdb\u4e86\u98ce\u66b4\u76f8\u5173\u672f\u8bed\u7684\u8868\u793a\uff0c\u63ed\u793a\u4e86\u707e\u96be\u53d9\u4e8b\u7684\u6f14\u53d8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7279\u5b9a\u9886\u57df\u4e2d\u6807\u8bb0\u5d4c\u5165\u7684\u8d28\u91cf\uff0c\u4e3a\u8bed\u6599\u63a2\u7d22\u548c\u6982\u5ff5\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.14386", "pdf": "https://arxiv.org/pdf/2504.14386", "abs": "https://arxiv.org/abs/2504.14386", "authors": ["Md Abtahi Majeed Chowdhury", "Md Rifat Ur Rahman", "Akil Ahmad Taki"], "title": "LOOPE: Learnable Optimal Patch Order in Positional Embeddings for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Positional embeddings (PE) play a crucial role in Vision Transformers (ViTs)\nby providing spatial information otherwise lost due to the permutation\ninvariant nature of self attention. While absolute positional embeddings (APE)\nhave shown theoretical advantages over relative positional embeddings (RPE),\nparticularly due to the ability of sinusoidal functions to preserve spatial\ninductive biases like monotonicity and shift invariance, a fundamental\nchallenge arises when mapping a 2D grid to a 1D sequence. Existing methods have\nmostly overlooked or never explored the impact of patch ordering in positional\nembeddings. To address this, we propose LOOPE, a learnable patch-ordering\nmethod that optimizes spatial representation for a given set of frequencies,\nproviding a principled approach to patch order optimization. Empirical results\nshow that our PE significantly improves classification accuracy across various\nViT architectures. To rigorously evaluate the effectiveness of positional\nembeddings, we introduce the \"Three Cell Experiment\", a novel benchmarking\nframework that assesses the ability of PEs to retain relative and absolute\npositional information across different ViT architectures. Unlike standard\nevaluations, which typically report a performance gap of 4 to 6% between models\nwith and without PE, our method reveals a striking 30 to 35% difference,\noffering a more sensitive diagnostic tool to measure the efficacy of PEs. Our\nexperimental analysis confirms that the proposed LOOPE demonstrates enhanced\neffectiveness in retaining both relative and absolute positional information.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8865\u4e01\u6392\u5e8f\u65b9\u6cd5LOOPE\uff0c\u7528\u4e8e\u4f18\u5316\u89c6\u89c9\u53d8\u6362\u5668\u4e2d\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u65b0\u5b9e\u9a8c\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\u5728\u5c062D\u7f51\u683c\u6620\u5c04\u52301D\u5e8f\u5217\u65f6\u5ffd\u7565\u8865\u4e01\u6392\u5e8f\u5f71\u54cd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLOOPE\u65b9\u6cd5\uff0c\u4f18\u5316\u8865\u4e01\u6392\u5e8f\u4ee5\u6539\u8fdb\u7a7a\u95f4\u8868\u793a\uff0c\u5e76\u5f15\u5165\u201c\u4e09\u7ec6\u80de\u5b9e\u9a8c\u201d\u6846\u67b6\u8bc4\u4f30\u4f4d\u7f6e\u5d4c\u5165\u6548\u679c\u3002", "result": "LOOPE\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u76f8\u5bf9\u548c\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u3002", "conclusion": "LOOPE\u4e3a\u4f4d\u7f6e\u5d4c\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u5b9e\u9a8c\u6846\u67b6\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.14856", "pdf": "https://arxiv.org/pdf/2504.14856", "abs": "https://arxiv.org/abs/2504.14856", "authors": ["Jiajun Shen", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation", "categories": ["cs.CL"], "comment": "19 pages, 14 figures", "summary": "While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5916\u90e8\u548c\u5185\u90e8\u77e5\u8bc6\u7684\u5f15\u6587\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86RAEL\u8303\u5f0f\u548cINTRALIGN\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5f15\u6587\u65f6\u5bf9\u5185\u90e8\u77e5\u8bc6\u5229\u7528\u4e0d\u900f\u660e\u53ca\u53ef\u4fe1\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51faContext-Prior Augmented Citation Generation\u4efb\u52a1\uff0c\u8bbe\u8ba1RAEL\u8303\u5f0f\u548cINTRALIGN\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u751f\u6210\u548c\u5bf9\u9f50\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u68c0\u7d22\u8d28\u91cf\u3001\u95ee\u9898\u7c7b\u578b\u548c\u6a21\u578b\u77e5\u8bc6\u5bf9\u5f15\u6587\u53ef\u4fe1\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5916\u90e8\u548c\u5185\u90e8\u77e5\u8bc6\uff0c\u8bba\u6587\u65b9\u6cd5\u63d0\u5347\u4e86\u5f15\u6587\u751f\u6210\u7684\u53ef\u4fe1\u5ea6\u548c\u6027\u80fd\u3002"}}
{"id": "2504.14391", "pdf": "https://arxiv.org/pdf/2504.14391", "abs": "https://arxiv.org/abs/2504.14391", "authors": ["Rahul Thapa", "Andrew Li", "Qingyang Wu", "Bryan He", "Yuki Sahashi", "Christina Binder", "Angela Zhang", "Ben Athiwaratkun", "Shuaiwen Leon Song", "David Ouyang", "James Zou"], "title": "How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?", "categories": ["cs.CV"], "comment": null, "summary": "Publicly available biomedical videos, such as those on YouTube, serve as\nvaluable educational resources for medical students. Unlike standard machine\nlearning datasets, these videos are designed for human learners, often mixing\nmedical imagery with narration, explanatory diagrams, and contextual framing.\nIn this work, we investigate whether such pedagogically rich, yet\nnon-standardized and heterogeneous videos can effectively teach general-domain\nvision-language models biomedical knowledge. To this end, we introduce\nOpenBiomedVi, a biomedical video instruction tuning dataset comprising 1031\nhours of video-caption and Q/A pairs, curated through a multi-step\nhuman-in-the-loop pipeline. Diverse biomedical video datasets are rare, and\nOpenBiomedVid fills an important gap by providing instruction-style supervision\ngrounded in real-world educational content. Surprisingly, despite the informal\nand heterogeneous nature of these videos, the fine-tuned Qwen-2-VL models\nexhibit substantial performance improvements across most benchmarks. The 2B\nmodel achieves gains of 98.7% on video tasks, 71.2% on image tasks, and 0.2% on\ntext tasks. The 7B model shows improvements of 37.09% on video and 11.2% on\nimage tasks, with a slight degradation of 2.7% on text tasks compared to their\nrespective base models. To address the lack of standardized biomedical video\nevaluation datasets, we also introduce two new expert curated benchmarks,\nMIMICEchoQA and SurgeryVideoQA. On these benchmarks, the 2B model achieves\ngains of 99.1% and 98.1%, while the 7B model shows gains of 22.5% and 52.1%,\nrespectively, demonstrating the models' ability to generalize and perform\nbiomedical video understanding on cleaner and more standardized datasets than\nthose seen during training. These results suggest that educational videos\ncreated for human learning offer a surprisingly effective training signal for\nbiomedical VLMs.", "AI": {"tldr": "OpenBiomedVi\u6570\u636e\u96c6\u5229\u7528\u6559\u80b2\u6027\u751f\u7269\u533b\u5b66\u89c6\u9891\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u975e\u6807\u51c6\u5316\u3001\u6559\u80b2\u6027\u751f\u7269\u533b\u5b66\u89c6\u9891\u662f\u5426\u80fd\u6709\u6548\u8bad\u7ec3\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u6784\u5efaOpenBiomedVi\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u6b65\u4eba\u5de5\u53c2\u4e0e\u6d41\u7a0b\u6536\u96c6\u89c6\u9891-\u5b57\u5e55\u548cQ/A\u5bf9\uff0c\u5e76\u5fae\u8c03Qwen-2-VL\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5728\u89c6\u9891\u548c\u56fe\u50cf\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff0c2B\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e0a\u63d0\u534798.7%\uff0c7B\u6a21\u578b\u5728\u89c6\u9891\u4efb\u52a1\u4e0a\u63d0\u534737.09%\u3002", "conclusion": "\u6559\u80b2\u6027\u751f\u7269\u533b\u5b66\u89c6\u9891\u4e3a\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002"}}
{"id": "2504.14871", "pdf": "https://arxiv.org/pdf/2504.14871", "abs": "https://arxiv.org/abs/2504.14871", "authors": ["Teppei Suzuki", "Ryokan Ri", "Sho Takase"], "title": "Natural Fingerprints of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u76f8\u540c\uff0c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ecd\u4f1a\u56e0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u5fae\u5dee\u5f02\uff08\u5982\u53c2\u6570\u5927\u5c0f\u3001\u4f18\u5316\u8bbe\u7f6e\u3001\u968f\u673a\u79cd\u5b50\uff09\u4ea7\u751f\u72ec\u7279\u7684\u81ea\u7136\u6307\u7eb9\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u884c\u4e3a\u504f\u5dee\u7684\u6765\u6e90\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u5b58\u5728\u7684\u7cfb\u7edf\u6027\u504f\u5dee\uff08\u81ea\u7136\u6307\u7eb9\uff09\u7684\u6210\u56e0\uff0c\u4ee5\u6539\u8fdb\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u63a7\u5236\u8bad\u7ec3\u6761\u4ef6\uff08\u5982\u53c2\u6570\u5927\u5c0f\u3001\u4f18\u5316\u8bbe\u7f6e\u3001\u968f\u673a\u79cd\u5b50\uff09\uff0c\u5206\u6790\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u72ec\u7279\u7279\u5f81\u3002", "result": "\u53d1\u73b0\u5373\u4f7f\u8bad\u7ec3\u6570\u636e\u76f8\u540c\uff0cLLMs\u4ecd\u4f1a\u56e0\u8bad\u7ec3\u8fc7\u7a0b\u7684\u7ec6\u5fae\u5dee\u5f02\u4ea7\u751f\u53ef\u533a\u5206\u7684\u81ea\u7136\u6307\u7eb9\u3002", "conclusion": "\u7406\u89e3\u81ea\u7136\u6307\u7eb9\u6709\u52a9\u4e8e\u63ed\u793a\u6a21\u578b\u504f\u5dee\u7684\u8d77\u6e90\uff0c\u5e76\u4e3a\u6539\u8fdbLLM\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14395", "pdf": "https://arxiv.org/pdf/2504.14395", "abs": "https://arxiv.org/abs/2504.14395", "authors": ["Chung-En", "Yu", "Hsuan-Chih", "Chen", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.MA"], "comment": null, "summary": "To develop trustworthy Vision-Language Models (VLMs), it is essential to\naddress adversarial robustness and hallucination mitigation, both of which\nimpact factual accuracy in high-stakes applications such as defense and\nhealthcare. Existing methods primarily focus on either adversarial defense or\nhallucination post-hoc correction, leaving a gap in unified robustness\nstrategies. We introduce \\textbf{Hydra}, an adaptive agentic framework that\nenhances plug-in VLMs through iterative reasoning, structured critiques, and\ncross-model verification, improving both resilience to adversarial\nperturbations and intrinsic model errors. Hydra employs an Action-Critique\nLoop, where it retrieves and critiques visual information, leveraging\nChain-of-Thought (CoT) and In-Context Learning (ICL) techniques to refine\noutputs dynamically. Unlike static post-hoc correction methods, Hydra adapts to\nboth adversarial manipulations and intrinsic model errors, making it robust to\nmalicious perturbations and hallucination-related inaccuracies. We evaluate\nHydra on four VLMs, three hallucination benchmarks, two adversarial attack\nstrategies, and two adversarial defense methods, assessing performance on both\nclean and adversarial inputs. Results show that Hydra surpasses plug-in VLMs\nand state-of-the-art (SOTA) dehallucination methods, even without explicit\nadversarial defenses, demonstrating enhanced robustness and factual\nconsistency. By bridging adversarial resistance and hallucination mitigation,\nHydra provides a scalable, training-free solution for improving the reliability\nof VLMs in real-world applications.", "AI": {"tldr": "Hydra\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u8de8\u6a21\u578b\u9a8c\u8bc1\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5bf9\u6297\u9632\u5fa1\u6216\u5e7b\u89c9\u540e\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u9c81\u68d2\u6027\u7b56\u7565\u3002Hydra\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "Hydra\u91c7\u7528Action-Critique Loop\uff0c\u7ed3\u5408Chain-of-Thought\u548cIn-Context Learning\u6280\u672f\uff0c\u52a8\u6001\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5728\u591a\u4e2aVLMs\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHydra\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u6297\u9632\u5fa1\u5373\u53ef\u63d0\u5347\u9c81\u68d2\u6027\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "Hydra\u4e3aVLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.14891", "pdf": "https://arxiv.org/pdf/2504.14891", "abs": "https://arxiv.org/abs/2504.14891", "authors": ["Aoran Gan", "Hao Yu", "Kai Zhang", "Qi Liu", "Wenyu Yan", "Zhenya Huang", "Shiwei Tong", "Guoping Hu"], "title": "Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey", "categories": ["cs.CL"], "comment": "18 pages, 5 figures", "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4f20\u7edf\u4e0e\u65b0\u5174\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6574\u7406\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u6846\u67b6\u3002", "motivation": "RAG\u7cfb\u7edf\u56e0\u5176\u6df7\u5408\u67b6\u6784\u548c\u52a8\u6001\u77e5\u8bc6\u4f9d\u8d56\uff0c\u8bc4\u4f30\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u65b9\u6cd5\u4ee5\u63a8\u52a8\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u56de\u987eRAG\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5305\u62ec\u6027\u80fd\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u8fdb\u884c\u5143\u5206\u6790\u3002", "result": "\u603b\u7ed3\u4e86RAG\u8bc4\u4f30\u7684\u73b0\u72b6\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u6846\u67b6\u7684\u5206\u7c7b\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u4e0eLLM\u9a71\u52a8\u65b9\u6cd5\u7684\u7a7a\u767d\u3002", "conclusion": "\u672c\u6587\u662fRAG\u8bc4\u4f30\u9886\u57df\u6700\u5168\u9762\u7684\u7efc\u8ff0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2504.14396", "pdf": "https://arxiv.org/pdf/2504.14396", "abs": "https://arxiv.org/abs/2504.14396", "authors": ["Minho Park", "Taewoong Kang", "Jooyeol Yun", "Sungwon Hwang", "Jaegul Choo"], "title": "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation", "categories": ["cs.CV"], "comment": null, "summary": "The increasing demand for AR/VR applications has highlighted the need for\nhigh-quality 360-degree panoramic content. However, generating high-quality\n360-degree panoramic images and videos remains a challenging task due to the\nsevere distortions introduced by equirectangular projection (ERP). Existing\napproaches either fine-tune pretrained diffusion models on limited ERP datasets\nor attempt tuning-free methods that still rely on ERP latent representations,\nleading to discontinuities near the poles. In this paper, we introduce\nSphereDiff, a novel approach for seamless 360-degree panoramic image and video\ngeneration using state-of-the-art diffusion models without additional tuning.\nWe define a spherical latent representation that ensures uniform distribution\nacross all perspectives, mitigating the distortions inherent in ERP. We extend\nMultiDiffusion to spherical latent space and propose a spherical latent\nsampling method to enable direct use of pretrained diffusion models. Moreover,\nwe introduce distortion-aware weighted averaging to further improve the\ngeneration quality in the projection process. Our method outperforms existing\napproaches in generating 360-degree panoramic content while maintaining high\nfidelity, making it a robust solution for immersive AR/VR applications. The\ncode is available here. https://github.com/pmh9960/SphereDiff", "AI": {"tldr": "SphereDiff\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u7f1d360\u5ea6\u5168\u666f\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7403\u5f62\u6f5c\u5728\u8868\u793a\u548c\u52a0\u6743\u5e73\u5747\u6280\u672f\u89e3\u51b3\u4e86\u4f20\u7edfERP\u6295\u5f71\u7684\u5931\u771f\u95ee\u9898\u3002", "motivation": "AR/VR\u5e94\u7528\u5bf9\u9ad8\u8d28\u91cf360\u5ea6\u5168\u666f\u5185\u5bb9\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0ERP\u6295\u5f71\u7684\u4e25\u91cd\u5931\u771f\u800c\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "SphereDiff\u5b9a\u4e49\u4e86\u7403\u5f62\u6f5c\u5728\u8868\u793a\uff0c\u6269\u5c55\u4e86MultiDiffusion\u5230\u7403\u5f62\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u7403\u5f62\u6f5c\u5728\u91c7\u6837\u548c\u5931\u771f\u611f\u77e5\u52a0\u6743\u5e73\u5747\u6280\u672f\u3002", "result": "SphereDiff\u5728\u751f\u6210360\u5ea6\u5168\u666f\u5185\u5bb9\u65f6\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4fdd\u6301\u4e86\u9ad8\u4fdd\u771f\u5ea6\u3002", "conclusion": "SphereDiff\u4e3aAR/VR\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684360\u5ea6\u5168\u666f\u5185\u5bb9\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14905", "pdf": "https://arxiv.org/pdf/2504.14905", "abs": "https://arxiv.org/abs/2504.14905", "authors": ["Yingming Zheng", "Xiaoliang Liu", "Peng Wu", "Li Pan"], "title": "CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE.", "AI": {"tldr": "CRAVE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51b2\u7a81\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u58f0\u660e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u51b2\u7a81\u7acb\u573a\uff0c\u5e76\u901a\u8fc7\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u8fdb\u884c\u6700\u7ec8\u5224\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u6570\u5b57\u5a92\u4f53\u548cAI\u751f\u6210\u5185\u5bb9\u5bfc\u81f4\u9519\u8bef\u4fe1\u606f\u5feb\u901f\u4f20\u64ad\uff0c\u4f20\u7edf\u4f9d\u8d56\u4e13\u5bb6\u6807\u6ce8\u8bc1\u636e\u7684\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7cfb\u7edf\u5bf9\u590d\u6742\u58f0\u660e\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002", "method": "CRAVE\u91c7\u7528\u4e09\u6a21\u5757\u6846\u67b6\uff1a1) \u6d88\u9664\u6b67\u4e49\u5e76\u68c0\u7d22\u8bc1\u636e\uff1b2) \u5229\u7528LLMs\u4ece\u56db\u4e2a\u7ef4\u5ea6\u63a8\u7406\u51b2\u7a81\u7acb\u573a\u5e76\u521d\u6b65\u5224\u65ad\uff1b3) \u901a\u8fc7SLM\u8bc4\u4f30\u51b2\u7a81\u7acb\u573a\u5e76\u6700\u7ec8\u5224\u65ad\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cCRAVE\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc1\u636e\u68c0\u7d22\u80fd\u529b\u548c\u6a21\u578b\u9884\u6d4b\u89e3\u91ca\u6027\u3002", "conclusion": "CRAVE\u901a\u8fc7\u51b2\u7a81\u63a8\u7406\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u58f0\u660e\u7684\u9a8c\u8bc1\u95ee\u9898\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.14423", "pdf": "https://arxiv.org/pdf/2504.14423", "abs": "https://arxiv.org/abs/2504.14423", "authors": ["Qiang Chen", "Xiao Wang", "Haowen Wang", "Bo Jiang", "Lin Zhu", "Dawei Zhang", "Yonghong Tian", "Jin Tang"], "title": "Adversarial Attack for RGB-Event based Visual Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual object tracking is a crucial research topic in the fields of computer\nvision and multi-modal fusion. Among various approaches, robust visual tracking\nthat combines RGB frames with Event streams has attracted increasing attention\nfrom researchers. While striving for high accuracy and efficiency in tracking,\nit is also important to explore how to effectively conduct adversarial attacks\nand defenses on RGB-Event stream tracking algorithms, yet research in this area\nremains relatively scarce. To bridge this gap, in this paper, we propose a\ncross-modal adversarial attack algorithm for RGB-Event visual tracking. Because\nof the diverse representations of Event streams, and given that Event voxels\nand frames are more commonly used, this paper will focus on these two\nrepresentations for an in-depth study. Specifically, for the RGB-Event voxel,\nwe first optimize the perturbation by adversarial loss to generate RGB frame\nadversarial examples. For discrete Event voxel representations, we propose a\ntwo-step attack strategy, more in detail, we first inject Event voxels into the\ntarget region as initialized adversarial examples, then, conduct a\ngradient-guided optimization by perturbing the spatial location of the Event\nvoxels. For the RGB-Event frame based tracking, we optimize the cross-modal\nuniversal perturbation by integrating the gradient information from multimodal\ndata. We evaluate the proposed approach against attacks on three widely used\nRGB-Event Tracking datasets, i.e., COESOT, FE108, and VisEvent. Extensive\nexperiments show that our method significantly reduces the performance of the\ntracker across numerous datasets in both unimodal and multimodal scenarios. The\nsource code will be released on\nhttps://github.com/Event-AHU/Adversarial_Attack_Defense", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9RGB-Event\u89c6\u89c9\u8ddf\u8e2a\u7684\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\uff0c\u7814\u7a76\u4e86Event\u4f53\u7d20\u548c\u5e27\u4e24\u79cd\u8868\u793a\u5f62\u5f0f\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "RGB-Event\u6d41\u8ddf\u8e2a\u7b97\u6cd5\u5728\u5bf9\u6297\u653b\u51fb\u548c\u9632\u5fa1\u65b9\u9762\u7684\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9488\u5bf9Event\u4f53\u7d20\u548c\u5e27\u4e24\u79cd\u8868\u793a\u5f62\u5f0f\uff0c\u5206\u522b\u8bbe\u8ba1\u4e86\u4f18\u5316\u6270\u52a8\u548c\u4e24\u6b65\u653b\u51fb\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4fe1\u606f\u4f18\u5316\u8de8\u6a21\u6001\u901a\u7528\u6270\u52a8\u3002", "result": "\u5728COESOT\u3001FE108\u548cVisEvent\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ddf\u8e2a\u5668\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u7b97\u6cd5\u6709\u6548\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u7b56\u7565\u3002"}}
{"id": "2504.14963", "pdf": "https://arxiv.org/pdf/2504.14963", "abs": "https://arxiv.org/abs/2504.14963", "authors": ["Rui Ribeiro", "Lu\u00edsa Coheur", "Joao P. Carvalho"], "title": "Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "cs.NE"], "comment": "Paper accepted at the FUZZY IEEE 2025 conference", "summary": "Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u6a21\u7cca\u6307\u7eb9\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u6539\u8fdb\u57fa\u4e8e\u6587\u672c\u7684\u8bf4\u8bdd\u4eba\u8bc6\u522b\uff0c\u7ed3\u5408\u8bf4\u8bdd\u4eba\u7279\u5b9a\u6807\u8bb0\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u4ec5\u4f9d\u8d56\u6587\u672c\u6570\u636e\u65f6\u6548\u679c\u6709\u9650\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65b0\u65b9\u6cd5\u63d0\u5347\u6587\u672c\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u7cca\u6307\u7eb9\u6280\u672f\uff0c\u7ed3\u5408\u8bf4\u8bdd\u4eba\u7279\u5b9a\u6807\u8bb0\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5efa\u6a21\uff0c\u4f18\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728Friends\u548cBig Bang Theory\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523070.6%\u548c67.7%\u7684\u51c6\u786e\u7387\uff0c\u6a21\u7cca\u6307\u7eb9\u63a5\u8fd1\u5168\u5fae\u8c03\u6027\u80fd\u3002", "conclusion": "\u6a21\u7cca\u6307\u7eb9\u548c\u4e0a\u4e0b\u6587\u5efa\u6a21\u663e\u8457\u63d0\u5347\u6587\u672c\u8bf4\u8bdd\u4eba\u8bc6\u522b\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.14429", "pdf": "https://arxiv.org/pdf/2504.14429", "abs": "https://arxiv.org/abs/2504.14429", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM-2: Addressing ResNetVLLM's Multi-Modal Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have transformed natural language processing\n(NLP) tasks, but they suffer from hallucination, generating plausible yet\nfactually incorrect content. This issue extends to Video-Language Models\n(VideoLLMs), where textual descriptions may inaccurately represent visual\ncontent, resulting in multi-modal hallucinations. In this paper, we address\nhallucination in ResNetVLLM, a video-language model combining ResNet visual\nencoders with LLMs. We introduce a two-step protocol: (1) a faithfulness\ndetection strategy that uses a modified Lynx model to assess semantic alignment\nbetween generated captions and ground-truth video references, and (2) a\nhallucination mitigation strategy using Retrieval-Augmented Generation (RAG)\nwith an ad-hoc knowledge base dynamically constructed during inference. Our\nenhanced model, ResNetVLLM-2, reduces multi-modal hallucinations by\ncross-verifying generated content against external knowledge, improving factual\nconsistency. Evaluation on the ActivityNet-QA benchmark demonstrates a\nsubstantial accuracy increase from 54.8% to 65.3%, highlighting the\neffectiveness of our hallucination detection and mitigation strategies in\nenhancing video-language model reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08ResNetVLLM\uff09\u4e2d\u591a\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u6b65\u7b56\u7565\uff1a\u8bed\u4e49\u5bf9\u9f50\u68c0\u6d4b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VideoLLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u7684\u5185\u5bb9\u53ef\u80fd\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u7b56\u7565\uff1a1\uff09\u4f7f\u7528\u6539\u8fdb\u7684Lynx\u6a21\u578b\u68c0\u6d4b\u751f\u6210\u63cf\u8ff0\u4e0e\u771f\u5b9e\u89c6\u9891\u5185\u5bb9\u7684\u8bed\u4e49\u5bf9\u9f50\uff1b2\uff09\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u7684\u77e5\u8bc6\u5e93\u548cRAG\u6280\u672f\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578bResNetVLLM-2\u5728ActivityNet-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u4ece54.8%\u63d0\u5347\u81f365.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.14969", "pdf": "https://arxiv.org/pdf/2504.14969", "abs": "https://arxiv.org/abs/2504.14969", "authors": ["Xiaodong Yang"], "title": "Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u4e2d\u6587\u8bdd\u9898\u7ed3\u6784\u654f\u611f\u6027\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u5bf9\u5b64\u5c9b\u7ea6\u675f\u7684\u654f\u611f\u6027\u3002", "motivation": "\u53d7Tian\u7b49\u4eba\uff082024\uff09\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u5e76\u5f81\u6c42\u65b9\u6cd5\u5b66\u53cd\u9988\u3002", "method": "\u8bbe\u8ba1\u5b9e\u9a8c\u6d4b\u8bd5LLMs\u5bf9\u6c49\u8bed\u8bed\u6cd5\u7684\u77e5\u8bc6\uff0c\u5c1a\u672a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u6682\u65e0\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4ec5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6846\u67b6\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u6b22\u8fce\u5bf9\u65b9\u6cd5\u8bba\u7684\u53cd\u9988\u3002"}}
{"id": "2504.14432", "pdf": "https://arxiv.org/pdf/2504.14432", "abs": "https://arxiv.org/abs/2504.14432", "authors": ["Ahmad Khalil", "Mahmoud Khalil", "Alioune Ngom"], "title": "ResNetVLLM -- Multi-modal Vision LLM for the Video Understanding Task", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce ResNetVLLM (ResNet Vision LLM), a novel\ncross-modal framework for zero-shot video understanding that integrates a\nResNet-based visual encoder with a Large Language Model (LLM. ResNetVLLM\naddresses the challenges associated with zero-shot video models by avoiding\nreliance on pre-trained video understanding models and instead employing a\nnon-pretrained ResNet to extract visual features. This design ensures the model\nlearns visual and semantic representations within a unified architecture,\nenhancing its ability to generate accurate and contextually relevant textual\ndescriptions from video inputs. Our experimental results demonstrate that\nResNetVLLM achieves state-of-the-art performance in zero-shot video\nunderstanding (ZSVU) on several benchmarks, including MSRVTT-QA, MSVD-QA,\nTGIF-QA FrameQA, and ActivityNet-QA.", "AI": {"tldr": "ResNetVLLM\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408ResNet\u89c6\u89c9\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u8bad\u7ec3\u89c6\u9891\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u4e2d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u5b66\u4e60\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u793a\u3002", "method": "\u4f7f\u7528\u975e\u9884\u8bad\u7ec3\u7684ResNet\u63d0\u53d6\u89c6\u89c9\u7279\u5f81\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MSRVTT-QA\u3001MSVD-QA\u7b49\uff09\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "conclusion": "ResNetVLLM\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u96f6\u6837\u672c\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002"}}
{"id": "2504.14992", "pdf": "https://arxiv.org/pdf/2504.14992", "abs": "https://arxiv.org/abs/2504.14992", "authors": ["Bohong Wu", "Shen Yan", "Sijun Zhang", "Jianqiao Lu", "Yutao Zeng", "Ya Wang", "Xun Zhou"], "title": "Efficient Pretraining Length Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPHD-Transformer\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u5ea6\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u63a2\u7d22\u957f\u5ea6\u6269\u5c55\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u521b\u65b0\u7684KV\u7f13\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u533a\u5206\u539f\u59cb\u4ee4\u724c\u548c\u9690\u85cf\u89e3\u7801\u4ee4\u724c\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u4f18\u5316\u53d8\u4f53\uff08PHD-SWA\u548cPHD-CSWA\uff09\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PHD-Transformer\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u5ea6\u6269\u5c55\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.14445", "pdf": "https://arxiv.org/pdf/2504.14445", "abs": "https://arxiv.org/abs/2504.14445", "authors": ["Mingya Zhang", "Liang Wang", "Limei Gu", "Tingsheng Ling", "Xianping Tao"], "title": "WT-BCP: Wavelet Transform based Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "6 pages", "summary": "Semi-supervised medical image segmentation (SSMIS) shows promise in reducing\nreliance on scarce labeled medical data. However, SSMIS field confronts\nchallenges such as distribution mismatches between labeled and unlabeled data,\nartificial perturbations causing training biases, and inadequate use of raw\nimage information, especially low-frequency (LF) and high-frequency (HF)\ncomponents.To address these challenges, we propose a Wavelet Transform based\nBidirectional Copy-Paste SSMIS framework, named WT-BCP, which improves upon the\nMean Teacher approach. Our method enhances unlabeled data understanding by\ncopying random crops between labeled and unlabeled images and employs WT to\nextract LF and HF details.We propose a multi-input and multi-output model named\nXNet-Plus, to receive the fused information after WT. Moreover, consistency\ntraining among multiple outputs helps to mitigate learning biases introduced by\nartificial perturbations. During consistency training, the mixed images\nresulting from WT are fed into both models, with the student model's output\nbeing supervised by pseudo-labels and ground-truth. Extensive experiments\nconducted on 2D and 3D datasets confirm the effectiveness of our model.Code:\nhttps://github.com/simzhangbest/WT-BCP.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u53cc\u5411\u590d\u5236\u7c98\u8d34\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff08WT-BCP\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\u4ee5\u53ca\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u5e03\u4e0d\u5339\u914d\u548c\u8bad\u7ec3\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff08SSMIS\uff09\u4f9d\u8d56\u7a00\u7f3a\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u9762\u4e34\u6807\u6ce8\u4e0e\u672a\u6807\u6ce8\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u3001\u4eba\u5de5\u6270\u52a8\u5bfc\u81f4\u7684\u8bad\u7ec3\u504f\u5dee\u4ee5\u53ca\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faWT-BCP\u6846\u67b6\uff0c\u7ed3\u5408\u5c0f\u6ce2\u53d8\u6362\u63d0\u53d6\u4f4e\u9891\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u4f7f\u7528\u53cc\u5411\u590d\u5236\u7c98\u8d34\u589e\u5f3a\u672a\u6807\u6ce8\u6570\u636e\u7684\u7406\u89e3\uff0c\u5e76\u8bbe\u8ba1XNet-Plus\u6a21\u578b\u8fdb\u884c\u591a\u8f93\u5165\u591a\u8f93\u51fa\u5904\u7406\u3002\u901a\u8fc7\u4e00\u81f4\u6027\u8bad\u7ec3\u51cf\u5c11\u4eba\u5de5\u6270\u52a8\u5e26\u6765\u7684\u504f\u5dee\u3002", "result": "\u57282D\u548c3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "WT-BCP\u6846\u67b6\u901a\u8fc7\u7efc\u5408\u5229\u7528\u56fe\u50cf\u4fe1\u606f\u548c\u4e00\u81f4\u6027\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15013", "pdf": "https://arxiv.org/pdf/2504.15013", "abs": "https://arxiv.org/abs/2504.15013", "authors": ["Yow-Fu Liou", "Yu-Chien Tang", "An-Zi Yen"], "title": "Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs", "categories": ["cs.CL"], "comment": "Accepted by iRAISE@AAAI2025", "summary": "The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u751f\u6210\u6559\u80b2\u6750\u6599\u548c\u8bfe\u7a0b\u5efa\u8bae\u7684\u6f5c\u529b\uff0c\u4ee5\u51cf\u8f7b\u6559\u80b2\u5de5\u4f5c\u8005\u7684\u8d1f\u62c5\u3002\u901a\u8fc7TED-Ed Dig Deeper\u90e8\u5206\u4f5c\u4e3a\u6848\u4f8b\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u751f\u6210\u6269\u5c55\u6587\u7ae0\u5e76\u63a8\u8350\u76f8\u5173\u8bfe\u7a0b\u3002", "motivation": "\u6559\u80b2\u6750\u6599\u5236\u4f5c\u8017\u65f6\u4e14\u7e41\u91cd\uff0c\u7814\u7a76\u65e8\u5728\u5229\u7528LLMs\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63d0\u5347\u6548\u7387\u548c\u8d28\u91cf\u3002", "method": "\u4ece\u89c6\u9891\u8f6c\u5f55\u751f\u6210\u6269\u5c55\u6587\u7ae0\uff0c\u7ed3\u5408\u5386\u53f2\u3001\u6587\u5316\u548c\u8f76\u4e8b\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u63a8\u8350\u8bfe\u7a0b\uff0c\u6700\u540e\u7528LLM\u4f18\u5316\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u8d28\u91cf\u9ad8\uff0c\u8bfe\u7a0b\u63a8\u8350\u51c6\u786e\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u4f53\u9a8c\u3002", "conclusion": "LLMs\u80fd\u6709\u6548\u8fde\u63a5\u6838\u5fc3\u5185\u5bb9\u4e0e\u8865\u5145\u5b66\u4e60\u8d44\u6e90\uff0c\u8f85\u52a9\u6559\u5e08\u8bbe\u8ba1\u6750\u6599\uff0c\u540c\u65f6\u4e3a\u5b66\u751f\u63d0\u4f9b\u66f4\u591a\u5b66\u4e60\u8d44\u6e90\u3002"}}
{"id": "2504.14446", "pdf": "https://arxiv.org/pdf/2504.14446", "abs": "https://arxiv.org/abs/2504.14446", "authors": ["Carlos Caetano", "Gabriel O. dos Santos", "Caio Petrucci", "Artur Barros", "Camila Laranjeira", "Leo S. F. Ribeiro", "J\u00falia F. de Mendon\u00e7a", "Jefersson A. dos Santos", "Sandra Avila"], "title": "Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability", "categories": ["cs.CV", "cs.CY", "cs.LG"], "comment": "ACM Conference on Fairness, Accountability, and Transparency (FAccT\n  2025)", "summary": "Including children's images in datasets has raised ethical concerns,\nparticularly regarding privacy, consent, data protection, and accountability.\nThese datasets, often built by scraping publicly available images from the\nInternet, can expose children to risks such as exploitation, profiling, and\ntracking. Despite the growing recognition of these issues, approaches for\naddressing them remain limited. We explore the ethical implications of using\nchildren's images in AI datasets and propose a pipeline to detect and remove\nsuch images. As a use case, we built the pipeline on a Vision-Language Model\nunder the Visual Question Answering task and tested it on the #PraCegoVer\ndataset. We also evaluate the pipeline on a subset of 100,000 images from the\nOpen Images V7 dataset to assess its effectiveness in detecting and removing\nimages of children. The pipeline serves as a baseline for future research,\nproviding a starting point for more comprehensive tools and methodologies.\nWhile we leverage existing models trained on potentially problematic data, our\ngoal is to expose and address this issue. We do not advocate for training or\ndeploying such models, but instead call for urgent community reflection and\naction to protect children's rights. Ultimately, we aim to encourage the\nresearch community to exercise - more than an additional - care in creating new\ndatasets and to inspire the development of tools to protect the fundamental\nrights of vulnerable groups, particularly children.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728AI\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u513f\u7ae5\u56fe\u50cf\u7684\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u548c\u79fb\u9664\u8fd9\u4e9b\u56fe\u50cf\u7684\u6d41\u7a0b\u3002", "motivation": "\u513f\u7ae5\u56fe\u50cf\u5728\u6570\u636e\u96c6\u4e2d\u7684\u4f7f\u7528\u5f15\u53d1\u4e86\u9690\u79c1\u3001\u540c\u610f\u548c\u6570\u636e\u4fdd\u62a4\u7b49\u4f26\u7406\u95ee\u9898\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u79fb\u9664\u513f\u7ae5\u56fe\u50cf\uff0c\u5e76\u5728#PraCegoVer\u548cOpen Images V7\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u6d41\u7a0b\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u5de5\u5177\u3002", "conclusion": "\u547c\u5401\u7814\u7a76\u793e\u533a\u53cd\u601d\u5e76\u91c7\u53d6\u884c\u52a8\u4fdd\u62a4\u513f\u7ae5\u6743\u5229\uff0c\u540c\u65f6\u9f13\u52b1\u5f00\u53d1\u66f4\u5168\u9762\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2504.15022", "pdf": "https://arxiv.org/pdf/2504.15022", "abs": "https://arxiv.org/abs/2504.15022", "authors": ["Muhammad Uzair Ul Haq", "Davide Rigoni", "Alessandro Sperduti"], "title": "LLMs as Data Annotators: How Close Are We to Human Performance", "categories": ["cs.CL"], "comment": "27 pages, 4 figures", "summary": "In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728NLP\u4e2d\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540cLLM\u548c\u5d4c\u5165\u6a21\u578b\u5728NER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u6570\u636e\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u800c\u73b0\u6709ICL\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u9009\u62e9\u4e0a\u4e0b\u6587\u793a\u4f8b\uff0c\u6548\u7387\u4f4e\u4e14\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u591a\u79cdLLM\u548c\u5d4c\u5165\u6a21\u578b\u5728NER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165RAG\u65b9\u6cd5\u81ea\u52a8\u68c0\u7d22\u4e0a\u4e0b\u6587\u793a\u4f8b\u3002", "result": "\u7ed3\u679c\u8868\u660e\u9009\u62e9\u5408\u9002\u7684LLM\u548c\u5d4c\u5165\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u9700\u6743\u8861\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\uff0c\u5e76\u5173\u6ce8\u66f4\u5177\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6539\u8fdbICL\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2504.14450", "pdf": "https://arxiv.org/pdf/2504.14450", "abs": "https://arxiv.org/abs/2504.14450", "authors": ["Weizhi Nie", "Zichun Zhang", "Weijie Wang", "Bruno Lepri", "Anan Liu", "Nicu Seb"], "title": "Causal Disentanglement for Robust Long-tail Medical Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Counterfactual medical image generation effectively addresses data scarcity\nand enhances the interpretability of medical images. However, due to the\ncomplex and diverse pathological features of medical images and the imbalanced\nclass distribution in medical data, generating high-quality and diverse medical\nimages from limited data is significantly challenging. Additionally, to fully\nleverage the information in limited data, such as anatomical structure\ninformation and generate more structurally stable medical images while avoiding\ndistortion or inconsistency. In this paper, in order to enhance the clinical\nrelevance of generated data and improve the interpretability of the model, we\npropose a novel medical image generation framework, which generates independent\npathological and structural features based on causal disentanglement and\nutilizes text-guided modeling of pathological features to regulate the\ngeneration of counterfactual images. First, we achieve feature separation\nthrough causal disentanglement and analyze the interactions between features.\nHere, we introduce group supervision to ensure the independence of pathological\nand identity features. Second, we leverage a diffusion model guided by\npathological findings to model pathological features, enabling the generation\nof diverse counterfactual images. Meanwhile, we enhance accuracy by leveraging\na large language model to extract lesion severity and location from medical\nreports. Additionally, we improve the performance of the latent diffusion model\non long-tailed categories through initial noise optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u89e3\u8026\u548c\u6587\u672c\u5f15\u5bfc\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u533b\u5b66\u56fe\u50cf\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u4e14\u7c7b\u522b\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u533b\u5b66\u56fe\u50cf\u5177\u6709\u6311\u6218\u6027\uff0c\u540c\u65f6\u9700\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u56e0\u679c\u89e3\u8026\u5b9e\u73b0\u75c5\u7406\u548c\u7ed3\u6784\u7279\u5f81\u5206\u79bb\uff0c\u5f15\u5165\u5206\u7ec4\u76d1\u7763\u786e\u4fdd\u72ec\u7acb\u6027\uff1b\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u6587\u672c\u5f15\u5bfc\u5efa\u6a21\u75c5\u7406\u7279\u5f81\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u75c5\u53d8\u4fe1\u606f\uff0c\u4f18\u5316\u521d\u59cb\u566a\u58f0\u63d0\u5347\u957f\u5c3e\u7c7b\u522b\u6027\u80fd\u3002", "result": "\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u533b\u5b66\u56fe\u50cf\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u9ad8\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u751f\u6210\u7684\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.15027", "pdf": "https://arxiv.org/pdf/2504.15027", "abs": "https://arxiv.org/abs/2504.15027", "authors": ["Chengyu Wang", "Junbing Yan", "Yuanhao Yue", "Jun Huang"], "title": "DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community.", "AI": {"tldr": "DistilQwen2.5\u662f\u4e00\u7cfb\u5217\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u84b8\u998f\u6280\u672f\u4eceQwen2.5\u6a21\u578b\u884d\u751f\u800c\u6765\uff0c\u63d0\u5347\u4e86\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5e76\u5728\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u5c55\u793a\u4e86\u9ad8\u6548\u90e8\u7f72\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u90e8\u7f72\u6210\u672c\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u4ee3\u7406\u6559\u5e08\u6a21\u578b\u9009\u62e9\u548c\u6539\u5199\u6307\u4ee4-\u54cd\u5e94\u5bf9\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u878d\u5408\u6280\u672f\u9010\u6b65\u6574\u5408\u6559\u5e08\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u3002", "result": "\u84b8\u998f\u540e\u7684\u6a21\u578b\u5728\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u539f\u59cb\u6a21\u578b\u3002", "conclusion": "DistilQwen2.5\u6a21\u578b\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.14460", "pdf": "https://arxiv.org/pdf/2504.14460", "abs": "https://arxiv.org/abs/2504.14460", "authors": ["Junyan Su", "Baozhu Zhao", "Xiaohan Zhang", "Qi Liu"], "title": "Metamon-GS: Enhancing Representability with Variance-Guided Densification and Light Encoding", "categories": ["cs.CV"], "comment": null, "summary": "The introduction of 3D Gaussian Splatting (3DGS) has advanced novel view\nsynthesis by utilizing Gaussians to represent scenes. Encoding Gaussian point\nfeatures with anchor embeddings has significantly enhanced the performance of\nnewer 3DGS variants. While significant advances have been made, it is still\nchallenging to boost rendering performance. Feature embeddings have difficulty\naccurately representing colors from different perspectives under varying\nlighting conditions, which leads to a washed-out appearance. Another reason is\nthe lack of a proper densification strategy that prevents Gaussian point growth\nin thinly initialized areas, resulting in blurriness and needle-shaped\nartifacts. To address them, we propose Metamon-GS, from innovative viewpoints\nof variance-guided densification strategy and multi-level hash grid. The\ndensification strategy guided by variance specifically targets Gaussians with\nhigh gradient variance in pixels and compensates for the importance of regions\nwith extra Gaussians to improve reconstruction. The latter studies implicit\nglobal lighting conditions and accurately interprets color from different\nperspectives and feature embeddings. Our thorough experiments on publicly\navailable datasets show that Metamon-GS surpasses its baseline model and\nprevious versions, delivering superior quality in rendering novel views.", "AI": {"tldr": "Metamon-GS\u901a\u8fc7\u65b9\u5dee\u5f15\u5bfc\u7684\u5bc6\u5ea6\u7b56\u7565\u548c\u591a\u7ea7\u54c8\u5e0c\u7f51\u683c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u6e32\u67d3\u6027\u80fd\u4e0a\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "3DGS\u5728\u6e32\u67d3\u6027\u80fd\u4e0a\u5b58\u5728\u989c\u8272\u8868\u793a\u4e0d\u51c6\u786e\u548c\u5bc6\u5ea6\u7b56\u7565\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u56fe\u50cf\u6a21\u7cca\u548c\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u65b9\u5dee\u5f15\u5bfc\u7684\u5bc6\u5ea6\u7b56\u7565\u548c\u591a\u7ea7\u54c8\u5e0c\u7f51\u683c\uff0c\u524d\u8005\u9488\u5bf9\u9ad8\u68af\u5ea6\u65b9\u5dee\u533a\u57df\u589e\u52a0\u9ad8\u65af\u70b9\uff0c\u540e\u8005\u7814\u7a76\u5168\u5c40\u5149\u7167\u6761\u4ef6\u4ee5\u51c6\u786e\u8868\u793a\u989c\u8272\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMetamon-GS\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u548c\u5148\u524d\u7248\u672c\uff0c\u6e32\u67d3\u8d28\u91cf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Metamon-GS\u901a\u8fc7\u521b\u65b0\u7684\u5bc6\u5ea6\u7b56\u7565\u548c\u5149\u7167\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u4e863DGS\u7684\u6e32\u67d3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2504.15047", "pdf": "https://arxiv.org/pdf/2504.15047", "abs": "https://arxiv.org/abs/2504.15047", "authors": ["Quy-Anh Dang", "Chris Ngo", "Truong-Son Hy"], "title": "RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.", "AI": {"tldr": "RainbowPlus\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u65b0\u578b\u7ea2\u961f\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8d28\u91cf-\u591a\u6837\u6027\u641c\u7d22\u589e\u5f3a\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u591a\u6837\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6613\u53d7\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\uff0c\u73b0\u6709\u7ea2\u961f\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8d44\u6e90\u5bc6\u96c6\u6216\u653b\u51fb\u7b56\u7565\u5355\u4e00\u7684\u95ee\u9898\u3002", "method": "RainbowPlus\u91c7\u7528\u591a\u5143\u7d20\u5b58\u6863\u5b58\u50a8\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u7efc\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u8bc4\u4f30\u591a\u4e2a\u63d0\u793a\uff0c\u6539\u8fdb\u4e86\u4f20\u7edf\u8d28\u91cf-\u591a\u6837\u6027\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548cLLMs\u4e0a\uff0cRainbowPlus\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u591a\u6837\u6027\uff0c\u751f\u6210\u66f4\u591a\u72ec\u7279\u63d0\u793a\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "RainbowPlus\u4e3aLLM\u5b89\u5168\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u5de5\u5177\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u4e86\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.14467", "pdf": "https://arxiv.org/pdf/2504.14467", "abs": "https://arxiv.org/abs/2504.14467", "authors": ["Jiachen Li", "Qing Xie", "Xiaohan Yu", "Hongyun Wang", "Jinyu Xu", "Yongjian Liu", "Yongsheng Gao"], "title": "LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot referring image segmentation aims to locate and segment the target\nregion based on a referring expression, with the primary challenge of aligning\nand matching semantics across visual and textual modalities without training.\nPrevious works address this challenge by utilizing Vision-Language Models and\nmask proposal networks for region-text matching. However, this paradigm may\nlead to incorrect target localization due to the inherent ambiguity and\ndiversity of free-form referring expressions. To alleviate this issue, we\npresent LGD (Leveraging Generative Descriptions), a framework that utilizes the\nadvanced language generation capabilities of Multi-Modal Large Language Models\nto enhance region-text matching performance in Vision-Language Models.\nSpecifically, we first design two kinds of prompts, the attribute prompt and\nthe surrounding prompt, to guide the Multi-Modal Large Language Models in\ngenerating descriptions related to the crucial attributes of the referent\nobject and the details of surrounding objects, referred to as attribute\ndescription and surrounding description, respectively. Secondly, three\nvisual-text matching scores are introduced to evaluate the similarity between\ninstance-level visual features and textual features, which determines the mask\nmost associated with the referring expression. The proposed method achieves new\nstate-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and\nRefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU\ncompared to previous methods.", "AI": {"tldr": "LGD\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u5c5e\u6027\u63d0\u793a\u548c\u5468\u56f4\u63d0\u793a\u751f\u6210\u63cf\u8ff0\uff0c\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u53c2\u8003\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u53c2\u8003\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u81ea\u7531\u5f62\u5f0f\u8868\u8fbe\u5bfc\u81f4\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u5339\u914d\u95ee\u9898\uff0c\u907f\u514d\u9519\u8bef\u7684\u76ee\u6807\u5b9a\u4f4d\u3002", "method": "\u8bbe\u8ba1\u5c5e\u6027\u63d0\u793a\u548c\u5468\u56f4\u63d0\u793a\uff0c\u751f\u6210\u5c5e\u6027\u63cf\u8ff0\u548c\u5468\u56f4\u63cf\u8ff0\uff1b\u5f15\u5165\u4e09\u79cd\u89c6\u89c9-\u6587\u672c\u5339\u914d\u5206\u6570\u8bc4\u4f30\u76f8\u4f3c\u6027\u3002", "result": "\u5728RefCOCO\u3001RefCOCO+\u548cRefCOCOg\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0SOTA\uff0coIoU\u548cmIoU\u6700\u5927\u63d0\u5347\u5206\u522b\u4e3a9.97%\u548c11.29%\u3002", "conclusion": "LGD\u6846\u67b6\u901a\u8fc7\u751f\u6210\u63cf\u8ff0\u663e\u8457\u63d0\u5347\u4e86\u533a\u57df-\u6587\u672c\u5339\u914d\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15052", "pdf": "https://arxiv.org/pdf/2504.15052", "abs": "https://arxiv.org/abs/2504.15052", "authors": ["Joachim Minder", "Guillaume Wisniewski", "Natalie K\u00fcbler"], "title": "Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted for publication in the proceedings of MT Summit 2025", "summary": "This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86ChatGPT\u5728\u57fa\u4e8e\u9519\u8bef\u7c7b\u578b\u5b66\u6807\u6ce8\u673a\u5668\u7ffb\u8bd1\u8f93\u51fa\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u4e13\u4e1a\u7ffb\u8bd1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u81ea\u6211\u8bc4\u4f30\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\uff09\u5728\u4e13\u4e1a\u7ffb\u8bd1\u9519\u8bef\u6807\u6ce8\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u5bf9\u901a\u7528\u8bed\u8a00\u7684\u504f\u91cd\u3002", "method": "\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u63d0\u793a\u548c\u5b9a\u5236\u9519\u8bef\u7c7b\u578b\u5b66\uff0c\u6bd4\u8f83ChatGPT\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5bf9DeepL\u548cChatGPT\u7ffb\u8bd1\u7684\u6807\u6ce8\u3002", "result": "ChatGPT\u5bf9DeepL\u7ffb\u8bd1\u7684\u53ec\u56de\u7387\u548c\u7cbe\u786e\u5ea6\u8f83\u9ad8\uff0c\u4f46\u81ea\u6211\u8bc4\u4f30\u8868\u73b0\u8f83\u5dee\uff1b\u63d0\u793a\u7684\u7ec6\u8282\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "LLMs\u5728\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\u4f46\u5b58\u5728\u5c40\u9650\uff0c\u672a\u6765\u53ef\u7814\u7a76\u5f00\u6e90LLMs\u53ca\u5176\u5b9e\u8df5\u5e94\u7528\u3002"}}
{"id": "2504.14470", "pdf": "https://arxiv.org/pdf/2504.14470", "abs": "https://arxiv.org/abs/2504.14470", "authors": ["Jingjing Ren", "Wenbo Li", "Zhongdao Wang", "Haoze Sun", "Bangzhen Liu", "Haoyu Chen", "Jiaqi Xu", "Aoxue Li", "Shifeng Zhang", "Bin Shao", "Yong Guo", "Lei Zhu"], "title": "Turbo2K: Towards Ultra-Efficient and High-Quality 2K Video Synthesis", "categories": ["cs.CV"], "comment": "Webpage at https://jingjingrenabc.github.io/turbo2k/", "summary": "Demand for 2K video synthesis is rising with increasing consumer expectations\nfor ultra-clear visuals. While diffusion transformers (DiTs) have demonstrated\nremarkable capabilities in high-quality video generation, scaling them to 2K\nresolution remains computationally prohibitive due to quadratic growth in\nmemory and processing costs. In this work, we propose Turbo2K, an efficient and\npractical framework for generating detail-rich 2K videos while significantly\nimproving training and inference efficiency. First, Turbo2K operates in a\nhighly compressed latent space, reducing computational complexity and memory\nfootprint, making high-resolution video synthesis feasible. However, the high\ncompression ratio of the VAE and limited model size impose constraints on\ngenerative quality. To mitigate this, we introduce a knowledge distillation\nstrategy that enables a smaller student model to inherit the generative\ncapacity of a larger, more powerful teacher model. Our analysis reveals that,\ndespite differences in latent spaces and architectures, DiTs exhibit structural\nsimilarities in their internal representations, facilitating effective\nknowledge transfer. Second, we design a hierarchical two-stage synthesis\nframework that first generates multi-level feature at lower resolutions before\nguiding high-resolution video generation. This approach ensures structural\ncoherence and fine-grained detail refinement while eliminating redundant\nencoding-decoding overhead, further enhancing computational efficiency.Turbo2K\nachieves state-of-the-art efficiency, generating 5-second, 24fps, 2K videos\nwith significantly reduced computational cost. Compared to existing methods,\nTurbo2K is up to 20$\\times$ faster for inference, making high-resolution video\ngeneration more scalable and practical for real-world applications.", "AI": {"tldr": "Turbo2K\u662f\u4e00\u4e2a\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u62102K\u5206\u8fa8\u7387\u89c6\u9891\uff0c\u901a\u8fc7\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u548c\u77e5\u8bc6\u84b8\u998f\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u968f\u7740\u6d88\u8d39\u8005\u5bf9\u8d85\u9ad8\u6e05\u89c6\u89c9\u7684\u9700\u6c42\u589e\u52a0\uff0c2K\u89c6\u9891\u5408\u6210\u7684\u9700\u6c42\u4e0a\u5347\uff0c\u4f46\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\uff08DiTs\uff09\u57282K\u5206\u8fa8\u7387\u4e0b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "Turbo2K\u5728\u9ad8\u5ea6\u538b\u7f29\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u548c\u5206\u5c42\u4e24\u9636\u6bb5\u5408\u6210\u6846\u67b6\uff0c\u63d0\u5347\u6548\u7387\u548c\u8d28\u91cf\u3002", "result": "Turbo2K\u57285\u79d224fps\u76842K\u89c6\u9891\u751f\u6210\u4e2d\u8fbe\u5230\u6700\u9ad8\u6548\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb20\u500d\u3002", "conclusion": "Turbo2K\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u4f7f\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u66f4\u5b9e\u7528\u548c\u53ef\u6269\u5c55\u3002"}}
{"id": "2504.15093", "pdf": "https://arxiv.org/pdf/2504.15093", "abs": "https://arxiv.org/abs/2504.15093", "authors": ["K. Wong", "B. Wu", "S. Bulathwela", "M. Cukurova"], "title": "Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure", "summary": "Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u591a\u6a21\u6001\u6570\u636e\u5728\u8bca\u65ad\u5b66\u751f\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\uff08CPS\uff09\u80fd\u529b\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u6570\u636e\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6807\u7b7e\u590d\u6742\u6027\u548c\u6570\u636e\u96c6\u7ec4\u6210\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u6570\u636e\u548c\u5148\u8fdb\u6a21\u578b\u5728\u68c0\u6d4b\u590d\u6742CPS\u884c\u4e3a\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u6559\u80b2\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u6587\u672c\u5d4c\u5165\u548c\u58f0\u5b66\u5d4c\u5165\u6784\u5efa\u591a\u6a21\u6001\u5206\u7c7b\u6a21\u578b\uff0c\u6bd4\u8f83\u4f20\u7edf\u6a21\u578b\u548c\u57fa\u4e8eTransformer\u7684\u5355\u6a21\u6001\u4e0e\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u6570\u636e\u5728Transformer\u6a21\u578b\u4e2d\u63d0\u5347\u4e86\u793e\u4ea4\u8ba4\u77e5\u7c7bCPS\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u4f46\u5bf9\u4f20\u7edf\u6a21\u578b\u65e0\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u591a\u6a21\u6001\u548c\u6a21\u578b\u9009\u62e9\u9700\u6839\u636e\u5177\u4f53CPS\u6307\u6807\u7c7b\u578b\u548c\u6570\u636e\u96c6\u7279\u6027\u8c28\u614e\u8bc4\u4f30\uff0c\u672a\u6765\u9700\u7ed3\u5408\u4eba\u7c7b\u4e0eAI\u4f18\u52bf\u5e76\u63a2\u7d22\u66f4\u4f18\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2504.14471", "pdf": "https://arxiv.org/pdf/2504.14471", "abs": "https://arxiv.org/abs/2504.14471", "authors": ["Yichi Zhang", "Qianqian Yang"], "title": "Efficient Implicit Neural Compression of Point Clouds via Learnable Activation in Latent Space", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Implicit Neural Representations (INRs), also known as neural fields, have\nemerged as a powerful paradigm in deep learning, parameterizing continuous\nspatial fields using coordinate-based neural networks. In this paper, we\npropose \\textbf{PICO}, an INR-based framework for static point cloud\ncompression. Unlike prevailing encoder-decoder paradigms, we decompose the\npoint cloud compression task into two separate stages: geometry compression and\nattribute compression, each with distinct INR optimization objectives. Inspired\nby Kolmogorov-Arnold Networks (KANs), we introduce a novel network\narchitecture, \\textbf{LeAFNet}, which leverages learnable activation functions\nin the latent space to better approximate the target signal's implicit\nfunction. By reformulating point cloud compression as neural parameter\ncompression, we further improve compression efficiency through quantization and\nentropy coding. Experimental results demonstrate that \\textbf{LeAFNet}\noutperforms conventional MLPs in INR-based point cloud compression.\nFurthermore, \\textbf{PICO} achieves superior geometry compression performance\ncompared to the current MPEG point cloud compression standard, yielding an\naverage improvement of $4.92$ dB in D1 PSNR. In joint geometry and attribute\ncompression, our approach exhibits highly competitive results, with an average\nPCQM gain of $2.7 \\times 10^{-3}$.", "AI": {"tldr": "PICO\u662f\u4e00\u4e2a\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684\u9759\u6001\u70b9\u4e91\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u5c5e\u6027\u538b\u7f29\u4e24\u9636\u6bb5\u5206\u89e3\u4efb\u52a1\uff0c\u5e76\u5f15\u5165LeAFNet\u7f51\u7edc\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u70b9\u4e91\u538b\u7f29\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u6709\u9650\uff0cPICO\u65e8\u5728\u901a\u8fc7INR\u63d0\u5347\u538b\u7f29\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u538b\u7f29\u4efb\u52a1\uff08\u51e0\u4f55\u548c\u5c5e\u6027\uff09\uff0c\u8bbe\u8ba1LeAFNet\u7f51\u7edc\uff08\u57fa\u4e8e\u53ef\u5b66\u4e60\u6fc0\u6d3b\u51fd\u6570\uff09\uff0c\u5e76\u5f15\u5165\u91cf\u5316\u548c\u71b5\u7f16\u7801\u4f18\u5316\u538b\u7f29\u6548\u7387\u3002", "result": "LeAFNet\u4f18\u4e8e\u4f20\u7edfMLP\uff0cPICO\u5728\u51e0\u4f55\u538b\u7f29\u4e0a\u6bd4MPEG\u6807\u51c6\u63d0\u53474.92 dB D1 PSNR\uff0c\u8054\u5408\u538b\u7f29\u4e2dPCQM\u589e\u76ca2.7\u00d710\u207b\u00b3\u3002", "conclusion": "PICO\u901a\u8fc7INR\u548cLeAFNet\u663e\u8457\u63d0\u5347\u70b9\u4e91\u538b\u7f29\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.15120", "pdf": "https://arxiv.org/pdf/2504.15120", "abs": "https://arxiv.org/abs/2504.15120", "authors": ["Khalil Hennara", "Sara Chrouf", "Mohamed Motaism Hamed", "Zeina Aldallal", "Omar Hadid", "Safwan AlModhayan"], "title": "Kuwain 1.5B: An Arabic SLM via Language Injection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u65b0\u8bed\u8a00\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u5c06\u963f\u62c9\u4f2f\u8bed\u6ce8\u5165\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u6027\u80fd\u63d0\u53478%\uff0c\u4e14\u4e0d\u635f\u5bb3\u539f\u6709\u77e5\u8bc6\u3002", "motivation": "\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u4ee5\u878d\u5165\u65b0\u77e5\u8bc6\u662fAI\u53d1\u5c55\u7684\u5173\u952e\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u901a\u8fc7\u5411\u4e00\u4e2a\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u6ce8\u5165\u963f\u62c9\u4f2f\u8bed\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a15\u4ebf\u53c2\u6570\u7684\u5fae\u578b\u6a21\u578bKuwain\u3002", "result": "\u963f\u62c9\u4f2f\u8bed\u6027\u80fd\u5e73\u5747\u63d0\u53478%\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u6709\u77e5\u8bc6\uff0c\u4e14\u6240\u9700\u539f\u59cb\u6a21\u578b\u6570\u636e\u91cf\u6781\u5c11\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u3001\u5b9a\u5411\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6210\u672c\u6548\u76ca\u9ad8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u3002"}}
{"id": "2504.14481", "pdf": "https://arxiv.org/pdf/2504.14481", "abs": "https://arxiv.org/abs/2504.14481", "authors": ["Guoyi Zhang", "Siyang Chen", "Guangsheng Xu", "Han Wang", "Xiaohu Zhang"], "title": "Vision-Centric Representation-Efficient Fine-Tuning for Robust Universal Foreground Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Foreground segmentation is crucial for scene understanding, yet\nparameter-efficient fine-tuning (PEFT) of vision foundation models (VFMs) often\nfails in complex scenarios, such as camouflage and infrared imagery. We\nattribute this challenge to the inherent texture bias in VFMs, which is\nexacerbated during fine-tuning and limits generalization in texture-sparse\nenvironments. To address this, we propose Ladder Shape-bias Representation\nSide-tuning (LSR-ST), a lightweight PEFT framework that enhances model\nrobustness by introducing shape-biased inductive priors. LSR-ST captures\nshape-aware features using a simple HDConv Block, which integrates large-kernel\nattention and residual learning. The method satisfies three key conditions for\ninducing shape bias: large receptive fields, multi-order feature interactions,\nand sparse connectivity. Our analysis reveals that these improvements stem from\nrepresentation efficiency-the ability to extract task-relevant, structurally\ngrounded features while minimizing redundancy. We formalize this concept via\nInformation Bottleneck theory and advocate for it as a key PEFT objective.\nUnlike traditional NLP paradigms that focus on optimizing parameters and\nmemory, visual tasks require models that extract task-defined semantics, rather\nthan just relying on pre-encoded features. This shift enables our approach to\nmove beyond conventional trade-offs, offering more robust and generalizable\nsolutions for vision tasks. With minimal changes to SAM2-UNet, LSR-ST achieves\nconsistent improvements across 17 datasets and 6 tasks using only 4.719M\ntrainable parameters. These results highlight the potential of representation\nefficiency for robust and adaptable VFMs within complex visual environments.", "AI": {"tldr": "LSR-ST\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7PEFT\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5f62\u72b6\u504f\u7f6e\u7684\u5f52\u7eb3\u5148\u9a8c\uff0c\u63d0\u5347\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u4f2a\u88c5\u548c\u7ea2\u5916\u56fe\u50cf\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u5176\u56fa\u6709\u7684\u7eb9\u7406\u504f\u7f6e\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u88ab\u653e\u5927\uff0c\u9650\u5236\u4e86\u5728\u7eb9\u7406\u7a00\u758f\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faLSR-ST\u6846\u67b6\uff0c\u5229\u7528HDConv Block\u6355\u6349\u5f62\u72b6\u611f\u77e5\u7279\u5f81\uff0c\u6ee1\u8db3\u5927\u611f\u53d7\u91ce\u3001\u591a\u9636\u7279\u5f81\u4ea4\u4e92\u548c\u7a00\u758f\u8fde\u63a5\u7684\u6761\u4ef6\u3002", "result": "\u572817\u4e2a\u6570\u636e\u96c6\u548c6\u4e2a\u4efb\u52a1\u4e2d\uff0c\u4ec5\u4f7f\u75284.719M\u53ef\u8bad\u7ec3\u53c2\u6570\uff0cLSR-ST\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "LSR-ST\u901a\u8fc7\u8868\u793a\u6548\u7387\u7684\u6982\u5ff5\uff0c\u4e3a\u590d\u6742\u89c6\u89c9\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15133", "pdf": "https://arxiv.org/pdf/2504.15133", "abs": "https://arxiv.org/abs/2504.15133", "authors": ["Ziwen Xu", "Shuxun Wang", "Kewei Xu", "Haoming Xu", "Mengru Wang", "Xinle Deng", "Yunzhi Yao", "Guozhou Zheng", "Huajun Chen", "Ningyu Zhang"], "title": "EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit", "summary": "In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.", "AI": {"tldr": "EasyEdit2\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u652f\u6301\u901a\u8fc7\u63d2\u4ef6\u5f0f\u8c03\u6574\u63a7\u5236\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u884c\u4e3a\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u63d0\u4f9b\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63a7\u5236LLM\u7684\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e72\u9884\u573a\u666f\uff0c\u5982\u5b89\u5168\u6027\u3001\u60c5\u611f\u3001\u4e8b\u5b9e\u6027\u7b49\u3002", "method": "\u91c7\u7528\u65b0\u7684\u67b6\u6784\uff0c\u5305\u62ec\u5bfc\u5411\u5411\u91cf\u751f\u6210\u5668\u548c\u5e94\u7528\u5668\uff0c\u901a\u8fc7\u5355\u4e00\u6837\u672c\u81ea\u52a8\u751f\u6210\u548c\u5e94\u7528\u5bfc\u5411\u5411\u91cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEasyEdit2\u5728\u4e0d\u540cLLM\u4e0a\u8868\u73b0\u6709\u6548\uff0c\u7528\u6237\u53cb\u597d\u4e14\u64cd\u4f5c\u7b80\u4fbf\u3002", "conclusion": "EasyEdit2\u4e3aLLM\u884c\u4e3a\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u548c\u6f14\u793a\u8d44\u6e90\u3002"}}
{"id": "2504.14491", "pdf": "https://arxiv.org/pdf/2504.14491", "abs": "https://arxiv.org/abs/2504.14491", "authors": ["Shang Zhang", "Xiaobo Ding", "Huanbin Zhang", "Ruoyan Xiong", "Yue Zhang"], "title": "STARS: Sparse Learning Correlation Filter with Spatio-temporal Regularization and Super-resolution Reconstruction for Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared (TIR) target tracking methods often adopt the correlation\nfilter (CF) framework due to its computational efficiency. However, the low\nresolution of TIR images, along with tracking interference, significantly\nlimits the perfor-mance of TIR trackers. To address these challenges, we\nintroduce STARS, a novel sparse learning-based CF tracker that incorporates\nspatio-temporal regulari-zation and super-resolution reconstruction. First, we\napply adaptive sparse filter-ing and temporal domain filtering to extract key\nfeatures of the target while reduc-ing interference from background clutter and\nnoise. Next, we introduce an edge-preserving sparse regularization method to\nstabilize target features and prevent excessive blurring. This regularization\nintegrates multiple terms and employs the alternating direction method of\nmultipliers to optimize the solution. Finally, we propose a gradient-enhanced\nsuper-resolution method to extract fine-grained TIR target features and improve\nthe resolution of TIR images, addressing performance degradation in tracking\ncaused by low-resolution sequences. To the best of our knowledge, STARS is the\nfirst to integrate super-resolution methods within a sparse learning-based CF\nframework. Extensive experiments on the LSOTB-TIR, PTB-TIR, VOT-TIR2015, and\nVOT-TIR2017 benchmarks demonstrate that STARS outperforms state-of-the-art\ntrackers in terms of robustness.", "AI": {"tldr": "STARS\u662f\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5b66\u4e60\u7684\u76f8\u5173\u6ee4\u6ce2\u5668\u8ddf\u8e2a\u5668\uff0c\u7ed3\u5408\u65f6\u7a7a\u6b63\u5219\u5316\u548c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u7684\u6027\u80fd\u3002", "motivation": "\u70ed\u7ea2\u5916\u56fe\u50cf\u5206\u8fa8\u7387\u4f4e\u4e14\u6613\u53d7\u5e72\u6270\uff0c\u9650\u5236\u4e86\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u7a00\u758f\u6ee4\u6ce2\u548c\u65f6\u57df\u6ee4\u6ce2\u63d0\u53d6\u76ee\u6807\u7279\u5f81\uff0c\u5f15\u5165\u8fb9\u7f18\u4fdd\u6301\u7a00\u758f\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u68af\u5ea6\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTARS\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u8ddf\u8e2a\u5668\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "STARS\u9996\u6b21\u5c06\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u96c6\u6210\u5230\u7a00\u758f\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u70ed\u7ea2\u5916\u8ddf\u8e2a\u4e2d\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2504.15160", "pdf": "https://arxiv.org/pdf/2504.15160", "abs": "https://arxiv.org/abs/2504.15160", "authors": ["Joan C. Timoneda"], "title": "The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u63d2\u8865\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0fLLM\uff08\u5982GPT-4o\uff09\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u4ee5\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u572875\u4e2a\u539f\u59cb\u6837\u672c\u65f6\u6027\u80fd\u4e0e\u5b8c\u6574\u6837\u672c\u76f8\u5f53\uff0c\u4e14\u8fc7\u62df\u5408\u53ef\u63a7\u3002", "motivation": "\u5728\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u96c6\u65f6\uff0c\u67d0\u4e9b\u7c7b\u522b\u7684\u6837\u672c\u53ef\u80fd\u4e0d\u8db3\uff0c\u5f71\u54cd\u7f16\u7801\u5668-\u89e3\u7801\u5668LLM\uff08\u5982BERT\u548cRoBERTa\uff09\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5f0fLLM\uff08GPT-4o\uff09\u57fa\u4e8e\u5c11\u91cf\u539f\u59cb\u6837\u672c\u751f\u6210\u5408\u6210\u6587\u672c\uff0c\u786e\u4fdd\u65b0\u6587\u672c\u4e0e\u539f\u59cb\u6587\u672c\u6709\u8db3\u591f\u5dee\u5f02\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u540c\u65f6\u4fdd\u7559\u8bed\u4e49\u3002", "result": "\u572875\u4e2a\u6216\u66f4\u591a\u539f\u59cb\u6837\u672c\u65f6\uff0c\u5408\u6210\u63d2\u8865\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u5b8c\u6574\u6837\u672c\u76f8\u5f53\uff1b50\u4e2a\u6837\u672c\u65f6\u8fc7\u62df\u5408\u4f4e\u4e14\u53ef\u4fee\u6b63\u3002", "conclusion": "\u5408\u6210\u63d2\u8865\u65b9\u6cd5\u4e3a\u751f\u6210\u5f0fLLM\u5728\u7814\u7a76\u4e2d\u63d0\u4f9b\u4e86\u65b0\u7528\u9014\uff0c\u5e2e\u52a9\u5e94\u7528\u7814\u7a76\u8005\u5e73\u8861\u6570\u636e\u96c6\u4ee5\u83b7\u5f97\u6700\u4f73\u6027\u80fd\u3002"}}
{"id": "2504.14509", "pdf": "https://arxiv.org/pdf/2504.14509", "abs": "https://arxiv.org/abs/2504.14509", "authors": ["Fulong Ye", "Miao Hua", "Pengze Zhang", "Xinghui Li", "Qichao Sun", "Songtao Zhao", "Qian He", "Xinglong Wu"], "title": "DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we introduce DreamID, a diffusion-based face swapping model\nthat achieves high levels of ID similarity, attribute preservation, image\nfidelity, and fast inference speed. Unlike the typical face swapping training\nprocess, which often relies on implicit supervision and struggles to achieve\nsatisfactory results. DreamID establishes explicit supervision for face\nswapping by constructing Triplet ID Group data, significantly enhancing\nidentity similarity and attribute preservation. The iterative nature of\ndiffusion models poses challenges for utilizing efficient image-space loss\nfunctions, as performing time-consuming multi-step sampling to obtain the\ngenerated image during training is impractical. To address this issue, we\nleverage the accelerated diffusion model SD Turbo, reducing the inference steps\nto a single iteration, enabling efficient pixel-level end-to-end training with\nexplicit Triplet ID Group supervision. Additionally, we propose an improved\ndiffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter.\nThis robust architecture fully unlocks the power of the Triplet ID Group\nexplicit supervision. Finally, to further extend our method, we explicitly\nmodify the Triplet ID Group data during training to fine-tune and preserve\nspecific attributes, such as glasses and face shape. Extensive experiments\ndemonstrate that DreamID outperforms state-of-the-art methods in terms of\nidentity similarity, pose and expression preservation, and image fidelity.\nOverall, DreamID achieves high-quality face swapping results at 512*512\nresolution in just 0.6 seconds and performs exceptionally well in challenging\nscenarios such as complex lighting, large angles, and occlusions.", "AI": {"tldr": "DreamID\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u548cTriplet ID Group\u6570\u636e\u63d0\u5347\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4fdd\u7559\uff0c\u7ed3\u5408SD Turbo\u52a0\u901f\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5e76\u5728512*512\u5206\u8fa8\u7387\u4e0b0.6\u79d2\u5185\u5b8c\u6210\u9ad8\u8d28\u91cf\u4ea4\u6362\u3002", "motivation": "\u4f20\u7edf\u4eba\u8138\u4ea4\u6362\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u76d1\u7763\uff0c\u6548\u679c\u4e0d\u4f73\u3002DreamID\u65e8\u5728\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u548c\u9ad8\u6548\u67b6\u6784\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "method": "\u6784\u5efaTriplet ID Group\u6570\u636e\u5b9e\u73b0\u663e\u5f0f\u76d1\u7763\uff0c\u5229\u7528SD Turbo\u52a0\u901f\u6a21\u578b\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u51faSwapNet\u3001FaceNet\u548cID Adapter\u7684\u6539\u8fdb\u67b6\u6784\u3002", "result": "DreamID\u5728\u8eab\u4efd\u76f8\u4f3c\u6027\u3001\u59ff\u6001\u548c\u8868\u60c5\u4fdd\u7559\u3001\u56fe\u50cf\u4fdd\u771f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c0.6\u79d2\u5185\u5b8c\u6210512*512\u5206\u8fa8\u7387\u7684\u9ad8\u8d28\u91cf\u4ea4\u6362\u3002", "conclusion": "DreamID\u901a\u8fc7\u663e\u5f0f\u76d1\u7763\u548c\u9ad8\u6548\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u7684\u4eba\u8138\u4ea4\u6362\uff0c\u9002\u7528\u4e8e\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2504.15168", "pdf": "https://arxiv.org/pdf/2504.15168", "abs": "https://arxiv.org/abs/2504.15168", "authors": ["Qilin Tian"], "title": "On true empty category", "categories": ["cs.CL"], "comment": null, "summary": "According to Chomsky (1981, 1986), empty categories consist of PRO, pro,\ntrace, and variable. However, some empty object positions seem to be\nincompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)\nand Li & Wei (2014) raise the true empty category hypothesis, which holds that\ntrue empty category is only an empty position with category and Case features.\nAs a last resort option, it is used mainly to meet the subcatgorization of a\nverb. This assumption is ingenious, and if proved to be true, it will exert a\ngreat impact on the study of UG. In this paper, we evaluate their evidence from\ntopicalization and demonstrate that it can be accounted for without invoking\ntrue empty category.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u7a7a\u8bed\u7c7b\u5047\u8bbe\uff0c\u8ba4\u4e3a\u67d0\u4e9b\u7a7a\u5bbe\u8bed\u4f4d\u7f6e\u65e0\u6cd5\u7528\u73b0\u6709\u7a7a\u8bed\u7c7b\u89e3\u91ca\uff0c\u4f46\u901a\u8fc7\u8bdd\u9898\u5316\u5206\u6790\u53ef\u4ee5\u907f\u514d\u5f15\u5165\u2018\u771f\u6b63\u7a7a\u8bed\u7c7b\u2019\u7684\u6982\u5ff5\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709\u7a7a\u8bed\u7c7b\u7406\u8bba\uff08\u5982PRO\u3001pro\u3001trace\u7b49\uff09\u662f\u5426\u8db3\u4ee5\u89e3\u91ca\u6240\u6709\u7a7a\u5bbe\u8bed\u73b0\u8c61\uff0c\u907f\u514d\u5f15\u5165\u4e0d\u5fc5\u8981\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u8bdd\u9898\u5316\u73b0\u8c61\u5206\u6790\uff0c\u8bc4\u4f30Li\u7b49\u4eba\u63d0\u51fa\u7684\u2018\u771f\u6b63\u7a7a\u8bed\u7c7b\u2019\u5047\u8bbe\u7684\u8bc1\u636e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bdd\u9898\u5316\u73b0\u8c61\u65e0\u9700\u5f15\u5165\u2018\u771f\u6b63\u7a7a\u8bed\u7c7b\u2019\u5373\u53ef\u89e3\u91ca\u3002", "conclusion": "\u73b0\u6709\u7a7a\u8bed\u7c7b\u7406\u8bba\u8db3\u4ee5\u89e3\u91ca\u76f8\u5173\u73b0\u8c61\uff0c\u65e0\u9700\u5f15\u5165\u2018\u771f\u6b63\u7a7a\u8bed\u7c7b\u2019\u5047\u8bbe\u3002"}}
{"id": "2504.14516", "pdf": "https://arxiv.org/pdf/2504.14516", "abs": "https://arxiv.org/abs/2504.14516", "authors": ["Weirong Chen", "Ganlin Zhang", "Felix Wimbauer", "Rui Wang", "Nikita Araslanov", "Andrea Vedaldi", "Daniel Cremers"], "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://wrchen530.github.io/projects/batrack/", "summary": "Traditional SLAM systems, which rely on bundle adjustment, struggle with\nhighly dynamic scenes commonly found in casual videos. Such videos entangle the\nmotion of dynamic elements, undermining the assumption of static environments\nrequired by traditional systems. Existing techniques either filter out dynamic\nelements or model their motion independently. However, the former often results\nin incomplete reconstructions, whereas the latter can lead to inconsistent\nmotion estimates. Taking a novel approach, this work leverages a 3D point\ntracker to separate the camera-induced motion from the observed motion of\ndynamic objects. By considering only the camera-induced component, bundle\nadjustment can operate reliably on all scene elements as a result. We further\nensure depth consistency across video frames with lightweight post-processing\nbased on scale maps. Our framework combines the core of traditional SLAM --\nbundle adjustment -- with a robust learning-based 3D tracker front-end.\nIntegrating motion decomposition, bundle adjustment and depth refinement, our\nunified framework, BA-Track, accurately tracks the camera motion and produces\ntemporally coherent and scale-consistent dense reconstructions, accommodating\nboth static and dynamic elements. Our experiments on challenging datasets\nreveal significant improvements in camera pose estimation and 3D reconstruction\naccuracy.", "AI": {"tldr": "BA-Track\u662f\u4e00\u79cd\u65b0\u578bSLAM\u6846\u67b6\uff0c\u901a\u8fc73D\u70b9\u8ddf\u8e2a\u5668\u5206\u79bb\u76f8\u673a\u8fd0\u52a8\u4e0e\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\uff0c\u7ed3\u5408\u4f20\u7edf\u675f\u8c03\u6574\u4e0e\u6df1\u5ea6\u4e00\u81f4\u6027\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u573a\u666f\u4e0b\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edfSLAM\u7cfb\u7edf\u5728\u52a8\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u6ee4\u52a8\u6001\u5143\u7d20\uff0c\u8981\u4e48\u72ec\u7acb\u5efa\u6a21\u5176\u8fd0\u52a8\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u6574\u6216\u8fd0\u52a8\u4f30\u8ba1\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u75283D\u70b9\u8ddf\u8e2a\u5668\u5206\u79bb\u76f8\u673a\u8fd0\u52a8\u4e0e\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\uff0c\u7ed3\u5408\u675f\u8c03\u6574\u548c\u57fa\u4e8e\u5c3a\u5ea6\u56fe\u7684\u8f7b\u91cf\u7ea7\u540e\u5904\u7406\uff0c\u786e\u4fdd\u6df1\u5ea6\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cBA-Track\u663e\u8457\u63d0\u5347\u4e86\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c3D\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "conclusion": "BA-Track\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6709\u6548\u5904\u7406\u52a8\u6001\u573a\u666f\uff0c\u4e3aSLAM\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15205", "pdf": "https://arxiv.org/pdf/2504.15205", "abs": "https://arxiv.org/abs/2504.15205", "authors": ["Nandan Thakur", "Ronak Pradeep", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at SIGIR 2025 (short)", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u5f15\u7528\u6587\u6863\u652f\u6301\u7b54\u6848\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86GPT-4o\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u6548\u679c\uff0c\u53d1\u73b0GPT-4o\u5728\u652f\u6301\u8bc4\u4f30\u4e2d\u8868\u73b0\u53ef\u9760\u3002", "motivation": "\u63a2\u8ba8RAG\u7cfb\u7edf\u4e2d\u5f15\u7528\u6587\u6863\u652f\u6301\u7b54\u6848\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u7cfb\u7edf\u5e7b\u89c9\u5e76\u63d0\u9ad8\u751f\u6210\u7b54\u6848\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5bf9TREC 2024 RAG Track\u768445\u4efd\u63d0\u4ea4\u548c36\u4e2a\u4e3b\u9898\u8fdb\u884c\u5927\u89c4\u6a21\u6bd4\u8f83\u7814\u7a76\uff0c\u6bd4\u8f83GPT-4o\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u6548\u679c\uff0c\u5305\u62ec\u5b8c\u5168\u4eba\u5de5\u8bc4\u4f30\u548c\u57fa\u4e8eLLM\u9884\u6d4b\u540e\u7f16\u8f91\u7684\u8bc4\u4f30\u3002", "result": "GPT-4o\u4e0e\u4eba\u5de5\u8bc4\u4f30\u5728\u5b8c\u5168\u4eba\u5de5\u6761\u4ef6\u4e0b\u7684\u5339\u914d\u7387\u4e3a56%\uff0c\u5728\u540e\u7f16\u8f91\u6761\u4ef6\u4e0b\u63d0\u5347\u81f372%\uff1b\u72ec\u7acb\u4eba\u5de5\u8bc4\u4f30\u4e0eGPT-4o\u76f8\u5173\u6027\u66f4\u9ad8\u3002", "conclusion": "GPT-4o\u53ef\u4f5c\u4e3a\u652f\u6301\u8bc4\u4f30\u7684\u53ef\u9760\u66ff\u4ee3\u65b9\u6848\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u51cf\u5c11\u9519\u8bef\u3002"}}
{"id": "2504.14526", "pdf": "https://arxiv.org/pdf/2504.14526", "abs": "https://arxiv.org/abs/2504.14526", "authors": ["Tong Zeng", "Longfeng Wu", "Liang Shi", "Dawei Zhou", "Feng Guo"], "title": "Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision Large Language Models (VLLMs) have demonstrated impressive\ncapabilities in general visual tasks such as image captioning and visual\nquestion answering. However, their effectiveness in specialized,\nsafety-critical domains like autonomous driving remains largely unexplored.\nAutonomous driving systems require sophisticated scene understanding in complex\nenvironments, yet existing multimodal benchmarks primarily focus on normal\ndriving conditions, failing to adequately assess VLLMs' performance in\nsafety-critical scenarios. To address this, we introduce DVBench, a pioneering\nbenchmark designed to evaluate the performance of VLLMs in understanding\nsafety-critical driving videos. Built around a hierarchical ability taxonomy\nthat aligns with widely adopted frameworks for describing driving scenarios\nused in assessing highly automated driving systems, DVBench features 10,000\nmultiple-choice questions with human-annotated ground-truth answers, enabling a\ncomprehensive evaluation of VLLMs' capabilities in perception and reasoning.\nExperiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal\nsignificant performance gaps, with no model achieving over 40% accuracy,\nhighlighting critical limitations in understanding complex driving scenarios.\nTo probe adaptability, we fine-tuned selected models using domain-specific data\nfrom DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage\npoints, with relative improvements of up to 43.59%. This improvement\nunderscores the necessity of targeted adaptation to bridge the gap between\ngeneral-purpose VLLMs and mission-critical driving applications. DVBench\nestablishes an essential evaluation framework and research roadmap for\ndeveloping VLLMs that meet the safety and robustness requirements for\nreal-world autonomous systems. We released the benchmark toolbox and the\nfine-tuned model at: https://github.com/tong-zeng/DVBench.git.", "AI": {"tldr": "DVBench\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff08VLLMs\uff09\u5728\u5b89\u5168\u5173\u952e\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLLMs\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u7684\u6027\u80fd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7f3a\u4e4f\u9488\u5bf9\u590d\u6742\u9a7e\u9a76\u573a\u666f\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faDVBench\u57fa\u51c6\uff0c\u5305\u542b10,000\u9053\u9009\u62e9\u9898\uff0c\u57fa\u4e8e\u5206\u5c42\u80fd\u529b\u5206\u7c7b\u6cd5\u8bc4\u4f30VLLMs\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5bf914\u4e2aSOTA\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u9ad8\u51c6\u786e\u7387<40%\uff09\uff0c\u4f46\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u76f8\u5bf9\u6539\u8fdb\u8fbe43.59%\uff09\u3002", "conclusion": "DVBench\u586b\u8865\u4e86\u8bc4\u4f30VLLMs\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u5f3a\u8c03\u4e86\u9886\u57df\u9002\u5e94\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6846\u67b6\u548c\u5de5\u5177\u3002"}}
{"id": "2504.15219", "pdf": "https://arxiv.org/pdf/2504.15219", "abs": "https://arxiv.org/abs/2504.15219", "authors": ["Manya Wadhwa", "Zayne Sprague", "Chaitanya Malaviya", "Philippe Laban", "Junyi Jessy Li", "Greg Durrett"], "title": "EvalAgent: Discovering Implicit Evaluation Criteria from the Web", "categories": ["cs.CL"], "comment": null, "summary": "Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86EvalAgent\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u9690\u542b\u548c\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u6807\u51c6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6307\u5bfc\u548cLLM\u751f\u6210\u6807\u51c6\uff0c\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u6807\u51c6\u6216LLM\u751f\u6210\u7684\u6807\u51c6\uff0c\u4f46\u5ffd\u7565\u4e86\u9690\u542b\u7684\u9ad8\u8d28\u91cf\u7279\u5f81\uff0c\u5982\u5b66\u672f\u6f14\u8bb2\u7684\u5178\u578b\u7ed3\u6784\u3002", "method": "EvalAgent\u901a\u8fc7\u6316\u6398\u4e13\u5bb6\u5728\u7ebf\u6307\u5bfc\uff0c\u63d0\u51fa\u57fa\u4e8e\u5916\u90e8\u8bc1\u636e\u7684\u591a\u6837\u5316\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u4e0eLLM\u751f\u6210\u6807\u51c6\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEvalAgent\u63d0\u51fa\u7684\u6807\u51c6\u5177\u6709\u9690\u542b\u6027\u548c\u7cbe\u786e\u6027\uff0c\u4e14\u80fd\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u8f93\u51fa\u3002\u7ed3\u5408LLM\u6807\u51c6\u540e\uff0c\u80fd\u53d1\u73b0\u66f4\u591a\u4eba\u8ba4\u53ef\u7684\u6807\u51c6\u3002", "conclusion": "EvalAgent\u6846\u67b6\u80fd\u6709\u6548\u8865\u5145\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u3002"}}
{"id": "2504.14534", "pdf": "https://arxiv.org/pdf/2504.14534", "abs": "https://arxiv.org/abs/2504.14534", "authors": ["Liang Peng", "Boxi Wu", "Haoran Cheng", "Yibo Zhao", "Xiaofei He"], "title": "SUDO: Enhancing Text-to-Image Diffusion Models with Self-Supervised Direct Preference Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Previous text-to-image diffusion models typically employ supervised\nfine-tuning (SFT) to enhance pre-trained base models. However, this approach\nprimarily minimizes the loss of mean squared error (MSE) at the pixel level,\nneglecting the need for global optimization at the image level, which is\ncrucial for achieving high perceptual quality and structural coherence. In this\npaper, we introduce Self-sUpervised Direct preference Optimization (SUDO), a\nnovel paradigm that optimizes both fine-grained details at the pixel level and\nglobal image quality. By integrating direct preference optimization into the\nmodel, SUDO generates preference image pairs in a self-supervised manner,\nenabling the model to prioritize global-level learning while complementing the\npixel-level MSE loss. As an effective alternative to supervised fine-tuning,\nSUDO can be seamlessly applied to any text-to-image diffusion model.\nImportantly, it eliminates the need for costly data collection and annotation\nefforts typically associated with traditional direct preference optimization\nmethods. Through extensive experiments on widely-used models, including Stable\nDiffusion 1.5 and XL, we demonstrate that SUDO significantly enhances both\nglobal and local image quality. The codes are provided at\n\\href{https://github.com/SPengLiang/SUDO}{this link}.", "AI": {"tldr": "SUDO\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5168\u5c40\u548c\u5c40\u90e8\u56fe\u50cf\u8d28\u91cf\uff0c\u65e0\u9700\u6602\u8d35\u7684\u6570\u636e\u6807\u6ce8\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4ec5\u4f18\u5316\u50cf\u7d20\u7ea7MSE\u635f\u5931\uff0c\u5ffd\u7565\u4e86\u5168\u5c40\u56fe\u50cf\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u751f\u6210\u504f\u597d\u56fe\u50cf\u5bf9\uff0c\u7ed3\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u540c\u65f6\u4f18\u5316\u50cf\u7d20\u7ea7\u548c\u5168\u5c40\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5728Stable Diffusion 1.5\u548cXL\u7b49\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "SUDO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u6807\u6ce8\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2504.15220", "pdf": "https://arxiv.org/pdf/2504.15220", "abs": "https://arxiv.org/abs/2504.15220", "authors": ["Juli\u00e1n Cendrero", "Julio Gonzalo", "Ivar Zapata"], "title": "Fully Bayesian Approaches to Topics over Time", "categories": ["cs.CL", "cs.LG"], "comment": "25 pages", "summary": "The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u8d1d\u53f6\u65af\u7684Topics over Time\u6a21\u578b\uff08BToT\uff09\uff0c\u901a\u8fc7\u5f15\u5165Beta\u5206\u5e03\u7684\u5171\u8f6d\u5148\u9a8c\u89e3\u51b3\u539fToT\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u52a0\u6743\u7248\u672c\uff08WBToT\uff09\u4ee5\u5e73\u8861\u65f6\u95f4\u548c\u8bcd\u6a21\u6001\u7684\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660eWBToT\u5728\u4e8b\u4ef6\u6355\u6349\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u539fToT\u6a21\u578b\u672a\u91c7\u7528\u5b8c\u5168\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u5bfc\u81f4\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e14\u5355\u65f6\u95f4\u89c2\u6d4b\u4e0e\u6587\u6863\u8bcd\u9891\u4e4b\u95f4\u5b58\u5728\u5c3a\u5ea6\u5dee\u5f02\u3002", "method": "\u5f15\u5165Beta\u5206\u5e03\u7684\u5171\u8f6d\u5148\u9a8c\u4f5c\u4e3a\u6b63\u5219\u5316\uff0c\u63d0\u51faBToT\uff1b\u8fdb\u4e00\u6b65\u901a\u8fc7\u91cd\u590d\u6587\u6863\u53d1\u5e03\u65f6\u95f4\u63d0\u51faWBToT\u4ee5\u5e73\u8861\u6a21\u6001\u5f71\u54cd\u3002", "result": "WBToT\u5728SOTU\u548cCOVID-19\u63a8\u6587\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eLDA\u548cBERTopic\uff0c\u4e3b\u9898\u65f6\u95f4\u504f\u5dee\u5206\u522b\u51cf\u5c1151%\u548c34%\uff0c\u4e14\u5728\u7ebf\u4f18\u5316\u7b97\u6cd5\u66f4\u7a33\u5b9a\u3002", "conclusion": "WBToT\u901a\u8fc7\u5e73\u8861\u65f6\u95f4\u548c\u8bcd\u6a21\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u4e8b\u4ef6\u6355\u6349\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u6570\u636e\u96c6\u3002"}}
{"id": "2504.14535", "pdf": "https://arxiv.org/pdf/2504.14535", "abs": "https://arxiv.org/abs/2504.14535", "authors": ["Kuanting Wu", "Kei Ota", "Asako Kanezaki"], "title": "FlowLoss: Dynamic Flow-Conditioned Loss Strategy for Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Video Diffusion Models (VDMs) can generate high-quality videos, but often\nstruggle with producing temporally coherent motion. Optical flow supervision is\na promising approach to address this, with prior works commonly employing\nwarping-based strategies that avoid explicit flow matching. In this work, we\nexplore an alternative formulation, FlowLoss, which directly compares flow\nfields extracted from generated and ground-truth videos. To account for the\nunreliability of flow estimation under high-noise conditions in diffusion, we\npropose a noise-aware weighting scheme that modulates the flow loss across\ndenoising steps. Experiments on robotic video datasets suggest that FlowLoss\nimproves motion stability and accelerates convergence in early training stages.\nOur findings offer practical insights for incorporating motion-based\nsupervision into noise-conditioned generative models.", "AI": {"tldr": "FlowLoss\u901a\u8fc7\u76f4\u63a5\u6bd4\u8f83\u751f\u6210\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u7684\u5149\u6d41\u573a\uff0c\u7ed3\u5408\u566a\u58f0\u611f\u77e5\u6743\u91cd\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65f6\uff0c\u5e38\u9762\u4e34\u65f6\u95f4\u4e00\u81f4\u6027\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u5149\u6d41\u76d1\u7763\u662f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u6f5c\u5728\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFlowLoss\uff0c\u76f4\u63a5\u6bd4\u8f83\u751f\u6210\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u7684\u5149\u6d41\u573a\uff0c\u5e76\u8bbe\u8ba1\u566a\u58f0\u611f\u77e5\u6743\u91cd\u65b9\u6848\u4ee5\u8c03\u8282\u53bb\u566a\u6b65\u9aa4\u4e2d\u7684\u5149\u6d41\u635f\u5931\u3002", "result": "\u5728\u673a\u5668\u4eba\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlowLoss\u63d0\u9ad8\u4e86\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u65e9\u671f\u8bad\u7ec3\u9636\u6bb5\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "FlowLoss\u4e3a\u566a\u58f0\u6761\u4ef6\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u57fa\u4e8e\u8fd0\u52a8\u7684\u76d1\u7763\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2504.15236", "pdf": "https://arxiv.org/pdf/2504.15236", "abs": "https://arxiv.org/abs/2504.15236", "authors": ["Saffron Huang", "Esin Durmus", "Miles McCain", "Kunal Handa", "Alex Tamkin", "Jerry Hong", "Michael Stern", "Arushi Somani", "Xiuruo Zhang", "Deep Ganguli"], "title": "Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "44 pages", "summary": "AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u63d0\u53d6Claude 3\u548c3.5\u6a21\u578b\u5728\u771f\u5b9e\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u7684\u4ef7\u503c\u89c2\uff0c\u53d1\u73b0\u5176\u652f\u6301\u79ef\u6781\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u5e76\u56e0\u4e0a\u4e0b\u6587\u4e0d\u540c\u800c\u5448\u73b0\u591a\u6837\u6027\u3002", "motivation": "AI\u52a9\u624b\u53ef\u80fd\u901a\u8fc7\u4ef7\u503c\u5224\u65ad\u5f71\u54cd\u7528\u6237\u51b3\u7b56\u548c\u4e16\u754c\u89c2\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5b9e\u9645\u4ef7\u503c\u89c2\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u6570\u5341\u4e07\u6b21\u771f\u5b9e\u4ea4\u4e92\u4e2d\u6a21\u578b\u8868\u73b0\u51fa\u7684\u4ef7\u503c\u89c2\u3002", "result": "\u63d0\u53d6\u4e863,307\u79cdAI\u4ef7\u503c\u89c2\uff0c\u53d1\u73b0\u5176\u652f\u6301\u900f\u660e\u3001\u5065\u5eb7\u8fb9\u754c\u7b49\u79ef\u6781\u4ef7\u503c\u89c2\uff0c\u4e14\u4ef7\u503c\u89c2\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u7cfb\u7edf\u7684\u4ef7\u503c\u89c2\u8bc4\u4f30\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002"}}
{"id": "2504.14548", "pdf": "https://arxiv.org/pdf/2504.14548", "abs": "https://arxiv.org/abs/2504.14548", "authors": ["Lifeng Lin", "Rongfeng Lu", "Quan Chen", "Haofan Ren", "Ming Lu", "Yaoqi Sun", "Chenggang Yan", "Anke Xue"], "title": "VGNC: Reducing the Overfitting of Sparse-view 3DGS via Validation-guided Gaussian Number Control", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages,8 figures", "summary": "Sparse-view 3D reconstruction is a fundamental yet challenging task in\npractical 3D reconstruction applications. Recently, many methods based on the\n3D Gaussian Splatting (3DGS) framework have been proposed to address\nsparse-view 3D reconstruction. Although these methods have made considerable\nadvancements, they still show significant issues with overfitting. To reduce\nthe overfitting, we introduce VGNC, a novel Validation-guided Gaussian Number\nControl (VGNC) approach based on generative novel view synthesis (NVS) models.\nTo the best of our knowledge, this is the first attempt to alleviate the\noverfitting issue of sparse-view 3DGS with generative validation images.\nSpecifically, we first introduce a validation image generation method based on\na generative NVS model. We then propose a Gaussian number control strategy that\nutilizes generated validation images to determine the optimal Gaussian numbers,\nthereby reducing the issue of overfitting. We conducted detailed experiments on\nvarious sparse-view 3DGS baselines and datasets to evaluate the effectiveness\nof VGNC. Extensive experiments show that our approach not only reduces\noverfitting but also improves rendering quality on the test set while\ndecreasing the number of Gaussian points. This reduction lowers storage demands\nand accelerates both training and rendering. The code will be released.", "AI": {"tldr": "VGNC\u662f\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u6a21\u578b\u7684\u9a8c\u8bc1\u5f15\u5bfc\u9ad8\u65af\u6570\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u7a00\u758f\u89c6\u89d23D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u91cd\u5efa\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6e32\u67d3\u8d28\u91cf\u5e76\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u7a00\u758f\u89c6\u89d23D\u91cd\u5efa\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u73b0\u67093DGS\u65b9\u6cd5\u867d\u6709\u6240\u6539\u8fdb\u4f46\u4ecd\u672a\u5f7b\u5e95\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u6a21\u578b\u751f\u6210\u9a8c\u8bc1\u56fe\u50cf\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u9ad8\u65af\u6570\u63a7\u5236\u7b56\u7565\uff0c\u4f18\u5316\u9ad8\u65af\u6570\u91cf\u4ee5\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVGNC\u4e0d\u4ec5\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u8fd8\u63d0\u5347\u4e86\u6d4b\u8bd5\u96c6\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9ad8\u65af\u70b9\u6570\u91cf\uff0c\u51cf\u5c11\u4e86\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "conclusion": "VGNC\u4e3a\u7a00\u758f\u89c6\u89d23DGS\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8fc7\u62df\u5408\u7f13\u89e3\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.15241", "pdf": "https://arxiv.org/pdf/2504.15241", "abs": "https://arxiv.org/abs/2504.15241", "authors": ["Yahan Yang", "Soham Dan", "Shuo Li", "Dan Roth", "Insup Lee"], "title": "MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u9632\u62a4\u680f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u3001\u76d1\u7763\u5fae\u8c03\u548cGRPO\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLMs\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u591a\u8bed\u8a00\u73af\u5883\u4e0bLLMs\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u4e14\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u6709\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u8de8\u8bed\u8a00\u68c0\u6d4b\u548c\u8fc7\u6ee4\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u9632\u62a4\u680f\u3002", "method": "\u5305\u62ec\u5408\u6210\u591a\u8bed\u8a00\u6570\u636e\u751f\u6210\u3001\u76d1\u7763\u5fae\u8c03\u548cGRPO\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57df\u5185\u548c\u57df\u5916\u8bed\u8a00\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u751f\u6210\u591a\u8bed\u8a00\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u8bed\u8a00\u7279\u5b9a\u98ce\u9669\u8bc6\u522b\u3002"}}
{"id": "2504.14553", "pdf": "https://arxiv.org/pdf/2504.14553", "abs": "https://arxiv.org/abs/2504.14553", "authors": ["Weijun Zhuang", "Qizhang Li", "Xin Li", "Ming Liu", "Xiaopeng Hong", "Feng Gao", "Fan Yang", "Wangmeng Zuo"], "title": "Grounding-MD: Grounded Video-language Pre-training for Open-World Moment Detection", "categories": ["cs.CV"], "comment": null, "summary": "Temporal Action Detection and Moment Retrieval constitute two pivotal tasks\nin video understanding, focusing on precisely localizing temporal segments\ncorresponding to specific actions or events. Recent advancements introduced\nMoment Detection to unify these two tasks, yet existing approaches remain\nconfined to closed-set scenarios, limiting their applicability in open-world\ncontexts. To bridge this gap, we present Grounding-MD, an innovative, grounded\nvideo-language pre-training framework tailored for open-world moment detection.\nOur framework incorporates an arbitrary number of open-ended natural language\nqueries through a structured prompt mechanism, enabling flexible and scalable\nmoment detection. Grounding-MD leverages a Cross-Modality Fusion Encoder and a\nText-Guided Fusion Decoder to facilitate comprehensive video-text alignment and\nenable effective cross-task collaboration. Through large-scale pre-training on\ntemporal action detection and moment retrieval datasets, Grounding-MD\ndemonstrates exceptional semantic representation learning capabilities,\neffectively handling diverse and complex query conditions. Comprehensive\nevaluations across four benchmark datasets including ActivityNet, THUMOS14,\nActivityNet-Captions, and Charades-STA demonstrate that Grounding-MD\nestablishes new state-of-the-art performance in zero-shot and supervised\nsettings in open-world moment detection scenarios. All source code and trained\nmodels will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrounding-MD\u7684\u521b\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u65f6\u523b\u68c0\u6d4b\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u9891-\u8bed\u8a00\u9884\u8bad\u7ec3\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5c01\u95ed\u573a\u666f\uff0c\u65e0\u6cd5\u9002\u5e94\u5f00\u653e\u4e16\u754c\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4efb\u610f\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "Grounding-MD\u91c7\u7528\u8de8\u6a21\u6001\u878d\u5408\u7f16\u7801\u5668\u548c\u6587\u672c\u5f15\u5bfc\u878d\u5408\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u89c6\u9891\u4e0e\u6587\u672c\u7684\u5168\u9762\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGrounding-MD\u5728\u96f6\u6837\u672c\u548c\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Grounding-MD\u4e3a\u5f00\u653e\u4e16\u754c\u65f6\u523b\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8bed\u4e49\u8868\u793a\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2504.15253", "pdf": "https://arxiv.org/pdf/2504.15253", "abs": "https://arxiv.org/abs/2504.15253", "authors": ["Yilun Zhou", "Austin Xu", "Peifeng Wang", "Caiming Xiong", "Shafiq Joty"], "title": "Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark", "summary": "Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u8ba1\u7b97\u4e2d\u4f7f\u7528LLM-judges\uff08\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8bc4\u4f30\u7684\u6a21\u578b\uff09\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u4e0d\u5982\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22LLM-judges\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u8ba1\u7b97\u4e2d\u7684\u6709\u6548\u6027\uff0c\u586b\u8865\u5176\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\u7a7a\u767d\u3002", "method": "\u5f15\u5165JETTS\u57fa\u51c6\uff0c\u8bc4\u4f3010\u79cd\u6cd5\u5b98\u6a21\u578b\u5728\u4e09\u4e2a\u9886\u57df\uff08\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001\u6307\u4ee4\u9075\u5faa\uff09\u548c\u4e09\u79cd\u4efb\u52a1\u8bbe\u7f6e\uff08\u54cd\u5e94\u91cd\u6392\u3001\u6b65\u9aa4\u7ea7\u675f\u641c\u7d22\u3001\u57fa\u4e8e\u6279\u5224\u7684\u54cd\u5e94\u4f18\u5316\uff09\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6cd5\u5b98\u6a21\u578b\u5728\u54cd\u5e94\u91cd\u6392\u4e2d\u4e0e\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u7ade\u4e89\uff0c\u4f46\u5728\u675f\u641c\u7d22\u4e2d\u4e0d\u5982\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u4e14\u5176\u81ea\u7136\u8bed\u8a00\u6279\u5224\u5bf9\u751f\u6210\u5668\u7684\u6307\u5bfc\u6548\u679c\u6709\u9650\u3002", "conclusion": "LLM-judges\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u6574\u4f53\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u5728\u6279\u5224\u6307\u5bfc\u751f\u6210\u65b9\u9762\u3002"}}
{"id": "2504.14566", "pdf": "https://arxiv.org/pdf/2504.14566", "abs": "https://arxiv.org/abs/2504.14566", "authors": ["Shang Zhang", "HuiPan Guan", "XiaoBo Ding", "Ruoyan Xiong", "Yue Zhang"], "title": "SMTT: Novel Structured Multi-task Tracking with Graph-Regularized Sparse Representation for Robust Thermal Infrared Target Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Thermal infrared target tracking is crucial in applications such as\nsurveillance, autonomous driving, and military operations. In this paper, we\npropose a novel tracker, SMTT, which effectively addresses common challenges in\nthermal infrared imagery, such as noise, occlusion, and rapid target motion, by\nleveraging multi-task learning, joint sparse representation, and adaptive graph\nregularization. By reformulating the tracking task as a multi-task learning\nproblem, the SMTT tracker independently optimizes the representation of each\nparticle while dynamically capturing spatial and feature-level similarities\nusing a weighted mixed-norm regularization strategy. To ensure real-time\nperformance, we incorporate the Accelerated Proximal Gradient method for\nefficient optimization. Extensive experiments on benchmark datasets - including\nVOT-TIR, PTB-TIR, and LSOTB-TIR - demonstrate that SMTT achieves superior\naccuracy, robustness, and computational efficiency. These results highlight\nSMTT as a reliable and high-performance solution for thermal infrared target\ntracking in complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSMTT\u7684\u65b0\u578b\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u3001\u8054\u5408\u7a00\u758f\u8868\u793a\u548c\u81ea\u9002\u5e94\u56fe\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u3001\u906e\u6321\u548c\u5feb\u901f\u76ee\u6807\u8fd0\u52a8\u7b49\u5e38\u89c1\u95ee\u9898\u3002", "motivation": "\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u5728\u76d1\u63a7\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u519b\u4e8b\u884c\u52a8\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u566a\u58f0\u3001\u906e\u6321\u548c\u5feb\u901f\u76ee\u6807\u8fd0\u52a8\u7b49\u6311\u6218\u3002", "method": "\u5c06\u8ddf\u8e2a\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u52a0\u6743\u6df7\u5408\u8303\u6570\u6b63\u5219\u5316\u7b56\u7565\u52a8\u6001\u6355\u83b7\u7a7a\u95f4\u548c\u7279\u5f81\u7ea7\u76f8\u4f3c\u6027\uff0c\u5e76\u91c7\u7528\u52a0\u901f\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\u4f18\u5316\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728VOT-TIR\u3001PTB-TIR\u548cLSOTB-TIR\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSMTT\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SMTT\u662f\u4e00\u79cd\u5728\u590d\u6742\u73af\u5883\u4e2d\u53ef\u9760\u4e14\u9ad8\u6027\u80fd\u7684\u70ed\u7ea2\u5916\u76ee\u6807\u8ddf\u8e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13847", "pdf": "https://arxiv.org/pdf/2504.13847", "abs": "https://arxiv.org/abs/2504.13847", "authors": ["Zhe Liu"], "title": "Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution", "categories": ["cs.HC", "cs.CL"], "comment": "4 pages, 2 figures, submitted and accepted by IUI 2025 Doctoral\n  Consortium", "summary": "Recent advances in large language models (LLMs) offer unprecedented\nopportunities to enhance human-AI collaboration in qualitative research\nmethods, including interviews. While interviews are highly valued for gathering\ndeep, contextualized insights, interviewers often face significant cognitive\nchallenges, such as real-time information processing, question adaptation, and\nrapport maintenance. My doctoral research introduces Interview AI-ssistant, a\nsystem designed for real-time interviewer-AI collaboration during both the\npreparation and execution phases. Through four interconnected studies, this\nresearch investigates the design of effective human-AI collaboration in\ninterviewing contexts, beginning with a formative study of interviewers' needs,\nfollowed by a prototype development study focused on AI-assisted interview\npreparation, an experimental evaluation of real-time AI assistance during\ninterviews, and a field study deploying the system in a real-world research\nsetting. Beyond informing practical implementations of intelligent interview\nsupport systems, this work contributes to the Intelligent User Interfaces (IUI)\ncommunity by advancing the understanding of human-AI collaborative interfaces\nin complex social tasks and establishing design guidelines for AI-enhanced\nqualitative research tools.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aInterview AI-ssistant\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7AI\u5b9e\u65f6\u8f85\u52a9\u8bbf\u8c08\u8005\uff0c\u63d0\u5347\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u8bbf\u8c08\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u8bbf\u8c08\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u8bbf\u8c08\u8005\u9762\u4e34\u5b9e\u65f6\u4fe1\u606f\u5904\u7406\u3001\u95ee\u9898\u8c03\u6574\u548c\u5173\u7cfb\u7ef4\u62a4\u7b49\u8ba4\u77e5\u6311\u6218\uff0cAI\u7684\u5f15\u5165\u6709\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u56db\u9879\u76f8\u4e92\u5173\u8054\u7684\u7814\u7a76\uff0c\u5305\u62ec\u9700\u6c42\u8c03\u7814\u3001\u539f\u578b\u5f00\u53d1\u3001\u5b9e\u9a8c\u8bc4\u4f30\u548c\u5b9e\u5730\u90e8\u7f72\uff0c\u63a2\u7d22AI\u5728\u8bbf\u8c08\u4e2d\u7684\u534f\u4f5c\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u4e0d\u4ec5\u4e3a\u667a\u80fd\u8bbf\u8c08\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u8fd8\u63a8\u52a8\u4e86\u4eba\u673a\u534f\u4f5c\u754c\u9762\u5728\u590d\u6742\u793e\u4ea4\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u7406\u89e3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aAI\u589e\u5f3a\u7684\u5b9a\u6027\u7814\u7a76\u5de5\u5177\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u6307\u5357\uff0c\u5e76\u4e30\u5bcc\u4e86\u667a\u80fd\u7528\u6237\u754c\u9762\u793e\u533a\u7684\u77e5\u8bc6\u3002"}}
{"id": "2504.14582", "pdf": "https://arxiv.org/pdf/2504.14582", "abs": "https://arxiv.org/abs/2504.14582", "authors": ["Zheng Chen", "Kai Liu", "Jue Gong", "Jingkai Wang", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Xiangyu Kong", "Xiaoxuan Yu", "Hyunhee Park", "Suejin Han", "Hakjae Jeon", "Dafeng Zhang", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Lu Zhao", "Yuyi Zhang", "Pengyu Yan", "Jiawei Hu", "Pengwei Liu", "Fengjun Guo", "Hongyuan Yu", "Pufan Xu", "Zhijuan Huang", "Shuyuan Cui", "Peng Guo", "Jiahui Liu", "Dongkai Zhang", "Heng Zhang", "Huiyuan Fu", "Huadong Ma", "Yanhui Guo", "Sisi Tian", "Xin Liu", "Jinwen Liang", "Jie Liu", "Jie Tang", "Gangshan Wu", "Zeyu Xiao", "Zhuoyuan Li", "Yinxiang Zhang", "Wenxuan Cai", "Vijayalaxmi Ashok Aralikatti", "Nikhil Akalwadi", "G Gyaneshwar Rao", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Marcos V. Conde", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Weijun Yuan", "Zhan Li", "Zhanglu Chen", "Boyang Yao", "Aagam Jain", "Milan Kumar Singh", "Ankit Kumar", "Shubh Kawa", "Divyavardhan Singh", "Anjali Sarvaiya", "Kishor Upla", "Raghavendra Ramachandra", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu", "Risheek V Hiremath", "Yashaswini Palani", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jingwei Liao", "Yuqing Yang", "Wenda Shao", "Junyi Zhao", "Qisheng Xu", "Kele Xu", "Sunder Ali Khowaja", "Ik Hyun Lee", "Snehal Singh Tomar", "Rajarshi Ray", "Klaus Mueller", "Sachin Chaudhary", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Bilel Benjdira", "Anas M. Ali", "Wadii Boulila", "Zahra Moammeri", "Ahmad Mahmoudi-Aznaveh", "Ali Karbasi", "Hossein Motamednia", "Liangyan Li", "Guanhua Zhao", "Kevin Le", "Yimo Ning", "Haoxuan Huang", "Jun Chen"], "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_ImageSR_x4", "summary": "This paper presents the NTIRE 2025 image super-resolution ($\\times$4)\nchallenge, one of the associated competitions of the 10th NTIRE Workshop at\nCVPR 2025. The challenge aims to recover high-resolution (HR) images from\nlow-resolution (LR) counterparts generated through bicubic downsampling with a\n$\\times$4 scaling factor. The objective is to develop effective network designs\nor solutions that achieve state-of-the-art SR performance. To reflect the dual\nobjectives of image SR research, the challenge includes two sub-tracks: (1) a\nrestoration track, emphasizes pixel-wise accuracy and ranks submissions based\non PSNR; (2) a perceptual track, focuses on visual realism and ranks results by\na perceptual score. A total of 286 participants registered for the competition,\nwith 25 teams submitting valid entries. This report summarizes the challenge\ndesign, datasets, evaluation protocol, the main results, and methods of each\nteam. The challenge serves as a benchmark to advance the state of the art and\nfoster progress in image SR.", "AI": {"tldr": "NTIRE 2025\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6311\u6218\u8d5b\u65e8\u5728\u901a\u8fc7\u5f00\u53d1\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\u6216\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u6062\u590d\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u5206\u4e3a\u6062\u590d\u548c\u611f\u77e5\u4e24\u4e2a\u5b50\u8d5b\u9053\u3002", "motivation": "\u63a8\u52a8\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u53d1\u5c55\uff0c\u901a\u8fc7\u7ade\u8d5b\u5f62\u5f0f\u4fc3\u8fdb\u9ad8\u6548\u7f51\u7edc\u8bbe\u8ba1\u548c\u89e3\u51b3\u65b9\u6848\u7684\u521b\u65b0\u3002", "method": "\u6311\u6218\u8d5b\u5206\u4e3a\u4e24\u4e2a\u5b50\u8d5b\u9053\uff1a\u6062\u590d\u8d5b\u9053\uff08\u57fa\u4e8ePSNR\u8bc4\u4f30\uff09\u548c\u611f\u77e5\u8d5b\u9053\uff08\u57fa\u4e8e\u611f\u77e5\u8bc4\u5206\u8bc4\u4f30\uff09\uff0c\u53c2\u4e0e\u8005\u63d0\u4ea4\u89e3\u51b3\u65b9\u6848\u3002", "result": "286\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\uff0c25\u652f\u56e2\u961f\u63d0\u4ea4\u6709\u6548\u65b9\u6848\uff0c\u6311\u6218\u8d5b\u603b\u7ed3\u4e86\u8bbe\u8ba1\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u53ca\u56e2\u961f\u65b9\u6cd5\u3002", "conclusion": "\u6311\u6218\u8d5b\u4f5c\u4e3a\u57fa\u51c6\u63a8\u52a8\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861", "abs": "https://arxiv.org/abs/2504.13861", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "26 pages, 8 figures, 7 tables", "summary": "Large Vision-Language Models (LVLMs) are increasingly being explored for\napplications in telemedicine, yet their ability to engage with diverse patient\nbehaviors remains underexplored. We introduce 3MDBench (Medical Multimodal\nMulti-agent Dialogue Benchmark), an open-source evaluation framework designed\nto assess LLM-driven medical consultations. Unlike existing benchmarks,\n3MDBench simulates real-world patient variability by incorporating four\ntemperament-driven Patient Agents and an Assessor Agent that evaluates\ndiagnostic accuracy and dialogue quality. The benchmark integrates textual and\nimage-based patient data across 34 common diagnoses, mirroring real-world\ntelemedicine interactions. Under different diagnostic strategies, we evaluate\nstate-of-the-art LVLMs. Our findings demonstrate that incorporating dialogue\nimproves the F1 score from 50.4 to 54.2 compared to non-dialogue settings,\nunderscoring the value of context-driven, information-seeking questioning.\nAdditionally, we demonstrate that multimodal inputs enhance diagnostic\nefficiency. Image-supported models outperform text-only counterparts by raising\nthe diagnostic F1 score from 52.8 to 54.2 in a similar dialogue setting.\nFinally, we suggest an approach that improves the diagnostic F1-score to 70.3\nby training the CNN model on the diagnosis prediction task and incorporating\nits top-3 predictions into the LVLM context. 3MDBench provides a reproducible\nand extendable evaluation framework for AI-driven medical assistants. It offers\ninsights into how patient temperament, dialogue strategies, and multimodal\nreasoning influence diagnosis quality. By addressing real-world complexities in\ntelemedicine, our benchmark paves the way for more empathetic, reliable, and\ncontext-aware AI-driven healthcare solutions. The source code of our benchmark\nis publicly available: https://github.com/univanxx/3mdbench", "AI": {"tldr": "3MDBench\u662f\u4e00\u4e2a\u5f00\u6e90\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u6d4b\u8bd5LVLM\u5728\u533b\u7597\u54a8\u8be2\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u6837\u60a3\u8005\u884c\u4e3a\u548c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u63d0\u5347\u8bca\u65ad\u6548\u7387\u3002", "motivation": "\u63a2\u7d22LVLM\u5728\u8fdc\u7a0b\u533b\u7597\u4e2d\u5904\u7406\u591a\u6837\u60a3\u8005\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u8bc4\u4f30\u5de5\u5177\u7684\u4e0d\u8db3\u3002", "method": "\u5f00\u53d13MDBench\u6846\u67b6\uff0c\u6a21\u62df\u771f\u5b9e\u60a3\u8005\u884c\u4e3a\uff0c\u6574\u5408\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\uff0c\u8bc4\u4f30LVLM\u5728\u4e0d\u540c\u8bca\u65ad\u7b56\u7565\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5bf9\u8bdd\u548c\u591a\u6a21\u6001\u8f93\u5165\u663e\u8457\u63d0\u5347\u8bca\u65adF1\u5206\u6570\uff0c\u7ed3\u5408CNN\u6a21\u578b\u540eF1\u5206\u6570\u8fbe\u523070.3\u3002", "conclusion": "3MDBench\u4e3aAI\u533b\u7597\u52a9\u624b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u66f4\u53ef\u9760\u3001\u60c5\u5883\u611f\u77e5\u7684\u533b\u7597\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14583", "pdf": "https://arxiv.org/pdf/2504.14583", "abs": "https://arxiv.org/abs/2504.14583", "authors": ["Akshit Gupta", "Remko Uijlenhoet"], "title": "Using street view imagery and deep generative modeling for estimating the health of urban forests", "categories": ["cs.CV", "cs.CY"], "comment": "Accepted at ICLR 2025 Workshop", "summary": "Healthy urban forests comprising of diverse trees and shrubs play a crucial\nrole in mitigating climate change. They provide several key advantages such as\nproviding shade for energy conservation, and intercepting rainfall to reduce\nflood runoff and soil erosion. Traditional approaches for monitoring the health\nof urban forests require instrumented inspection techniques, often involving a\nhigh amount of human labor and subjective evaluations. As a result, they are\nnot scalable for cities which lack extensive resources. Recent approaches\ninvolving multi-spectral imaging data based on terrestrial sensing and\nsatellites, are constrained respectively with challenges related to dedicated\ndeployments and limited spatial resolutions. In this work, we propose an\nalternative approach for monitoring the urban forests using simplified inputs:\nstreet view imagery, tree inventory data and meteorological conditions. We\npropose to use image-to-image translation networks to estimate two urban forest\nhealth parameters, namely, NDVI and CTD. Finally, we aim to compare the\ngenerated results with ground truth data using an onsite campaign utilizing\nhandheld multi-spectral and thermal imaging sensors. With the advent and\nexpansion of street view imagery platforms such as Google Street View and\nMapillary, this approach should enable effective management of urban forests\nfor the authorities in cities at scale.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8857\u666f\u56fe\u50cf\u3001\u6811\u6728\u6e05\u5355\u548c\u6c14\u8c61\u6570\u636e\u7684\u57ce\u5e02\u68ee\u6797\u5065\u5eb7\u76d1\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u8f6c\u6362\u7f51\u7edc\u4f30\u7b97NDVI\u548cCTD\u53c2\u6570\uff0c\u5e76\u4e0e\u5b9e\u5730\u6570\u636e\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u57ce\u5e02\u68ee\u6797\u5065\u5eb7\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u548c\u9ad8\u6210\u672c\u8bbe\u5907\uff0c\u96be\u4ee5\u89c4\u6a21\u5316\uff1b\u591a\u5149\u8c31\u9065\u611f\u6570\u636e\u4e5f\u5b58\u5728\u90e8\u7f72\u548c\u5206\u8fa8\u7387\u9650\u5236\u3002", "method": "\u4f7f\u7528\u8857\u666f\u56fe\u50cf\u3001\u6811\u6728\u6e05\u5355\u548c\u6c14\u8c61\u6570\u636e\uff0c\u901a\u8fc7\u56fe\u50cf\u8f6c\u6362\u7f51\u7edc\u4f30\u7b97NDVI\u548cCTD\u53c2\u6570\uff0c\u5e76\u4e0e\u5b9e\u5730\u591a\u5149\u8c31\u548c\u70ed\u6210\u50cf\u4f20\u611f\u5668\u6570\u636e\u5bf9\u6bd4\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5229\u7528\u73b0\u6709\u8857\u666f\u5e73\u53f0\uff08\u5982Google Street View\uff09\u6570\u636e\uff0c\u6709\u671b\u5b9e\u73b0\u57ce\u5e02\u68ee\u6797\u5065\u5eb7\u7684\u5927\u89c4\u6a21\u6709\u6548\u7ba1\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u57ce\u5e02\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u57ce\u5e02\u68ee\u6797\u5065\u5eb7\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13865", "pdf": "https://arxiv.org/pdf/2504.13865", "abs": "https://arxiv.org/abs/2504.13865", "authors": ["Fei Tang", "Haolei Xu", "Hang Zhang", "Siqi Chen", "Xingyu Wu", "Yongliang Shen", "Wenqi Zhang", "Guiyang Hou", "Zeqi Tan", "Yuchen Yan", "Kaitao Song", "Jian Shao", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "A Survey on (M)LLM-Based GUI Agents", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative\nparadigm in human-computer interaction, evolving from rule-based automation\nscripts to sophisticated AI-driven systems capable of understanding and\nexecuting complex interface operations. This survey provides a comprehensive\nexamination of the rapidly advancing field of LLM-based GUI Agents,\nsystematically analyzing their architectural foundations, technical components,\nand evaluation methodologies. We identify and analyze four fundamental\ncomponents that constitute modern GUI Agents: (1) perception systems that\nintegrate text-based parsing with multimodal understanding for comprehensive\ninterface comprehension; (2) exploration mechanisms that construct and maintain\nknowledge bases through internal modeling, historical experience, and external\ninformation retrieval; (3) planning frameworks that leverage advanced reasoning\nmethodologies for task decomposition and execution; and (4) interaction systems\nthat manage action generation with robust safety controls. Through rigorous\nanalysis of these components, we reveal how recent advances in large language\nmodels and multimodal learning have revolutionized GUI automation across\ndesktop, mobile, and web platforms. We critically examine current evaluation\nframeworks, highlighting methodological limitations in existing benchmarks\nwhile proposing directions for standardization. This survey also identifies key\ntechnical challenges, including accurate element localization, effective\nknowledge retrieval, long-horizon planning, and safety-aware execution control,\nwhile outlining promising research directions for enhancing GUI Agents'\ncapabilities. Our systematic review provides researchers and practitioners with\na thorough understanding of the field's current state and offers insights into\nfuture developments in intelligent interface automation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684GUI\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u6280\u672f\u7ec4\u4ef6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22GUI\u4ee3\u7406\u4ece\u89c4\u5219\u811a\u672c\u5230AI\u9a71\u52a8\u7cfb\u7edf\u7684\u6f14\u53d8\uff0c\u4ee5\u53ca\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86GUI\u4ee3\u7406\u7684\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u611f\u77e5\u7cfb\u7edf\u3001\u63a2\u7d22\u673a\u5236\u3001\u89c4\u5212\u6846\u67b6\u548c\u4ea4\u4e92\u7cfb\u7edf\u3002", "result": "\u63ed\u793a\u4e86LLM\u548c\u591a\u6a21\u6001\u5b66\u4e60\u5982\u4f55\u9769\u65b0GUI\u81ea\u52a8\u5316\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u603b\u7ed3\u4e86GUI\u4ee3\u7406\u7684\u73b0\u72b6\u3001\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u53c2\u8003\u3002"}}
{"id": "2504.14600", "pdf": "https://arxiv.org/pdf/2504.14600", "abs": "https://arxiv.org/abs/2504.14600", "authors": ["Zheng Chen", "Jingkai Wang", "Kai Liu", "Jue Gong", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Jianxing Zhang", "Jinlong Wu", "Jun Wang", "Zheng Xie", "Hakjae Jeon", "Suejin Han", "Hyung-Ju Chun", "Hyunhee Park", "Zhicun Yin", "Junjie Chen", "Ming Liu", "Xiaoming Li", "Chao Zhou", "Wangmeng Zuo", "Weixia Zhang", "Dingquan Li", "Kede Ma", "Yun Zhang", "Zhuofan Zheng", "Yuyue Liu", "Shizhen Tang", "Zihao Zhang", "Yi Ning", "Hao Jiang", "Wenjie An", "Kangmeng Yu", "Chenyang Wang", "Kui Jiang", "Xianming Liu", "Junjun Jiang", "Yingfu Zhang", "Gang He", "Siqi Wang", "Kepeng Xu", "Zhenyang Liu", "Changxin Zhou", "Shanlan Shen", "Yubo Duan", "Yiang Chen", "Jin Guo", "Mengru Yang", "Jen-Wei Lee", "Chia-Ming Lee", "Chih-Chung Hsu", "Hu Peng", "Chunming He"], "title": "NTIRE 2025 Challenge on Real-World Face Restoration: Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 webpage: https://www.cvlai.net/ntire/2025. Code:\n  https://github.com/zhengchen1999/NTIRE2025_RealWorld_Face_Restoration", "summary": "This paper provides a review of the NTIRE 2025 challenge on real-world face\nrestoration, highlighting the proposed solutions and the resulting outcomes.\nThe challenge focuses on generating natural, realistic outputs while\nmaintaining identity consistency. Its goal is to advance state-of-the-art\nsolutions for perceptual quality and realism, without imposing constraints on\ncomputational resources or training data. The track of the challenge evaluates\nperformance using a weighted image quality assessment (IQA) score and employs\nthe AdaFace model as an identity checker. The competition attracted 141\nregistrants, with 13 teams submitting valid models, and ultimately, 10 teams\nachieved a valid score in the final ranking. This collaborative effort advances\nthe performance of real-world face restoration while offering an in-depth\noverview of the latest trends in the field.", "AI": {"tldr": "NTIRE 2025\u6311\u6218\u8d5b\u7efc\u8ff0\uff0c\u805a\u7126\u771f\u5b9e\u4eba\u8138\u4fee\u590d\uff0c\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\u53ca\u6210\u679c\uff0c\u63a8\u52a8\u611f\u77e5\u8d28\u91cf\u548c\u771f\u5b9e\u6027\u7684\u524d\u6cbf\u6280\u672f\u3002", "motivation": "\u63d0\u5347\u771f\u5b9e\u4eba\u8138\u4fee\u590d\u7684\u611f\u77e5\u8d28\u91cf\u548c\u771f\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\u5206\u6570\u548cAdaFace\u6a21\u578b\u4f5c\u4e3a\u8eab\u4efd\u68c0\u67e5\u5668\uff0c\u8bc4\u4f30\u53c2\u8d5b\u6a21\u578b\u6027\u80fd\u3002", "result": "141\u540d\u6ce8\u518c\u8005\uff0c13\u652f\u56e2\u961f\u63d0\u4ea4\u6709\u6548\u6a21\u578b\uff0c10\u652f\u56e2\u961f\u8fdb\u5165\u6700\u7ec8\u6392\u540d\u3002", "conclusion": "\u6311\u6218\u8d5b\u63a8\u52a8\u4e86\u771f\u5b9e\u4eba\u8138\u4fee\u590d\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u5e76\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u6700\u65b0\u8d8b\u52bf\u3002"}}
{"id": "2504.13882", "pdf": "https://arxiv.org/pdf/2504.13882", "abs": "https://arxiv.org/abs/2504.13882", "authors": ["Megan Gu", "Chloe Qianhui Zhao", "Claire Liu", "Nikhil Patel", "Jahnvi Shah", "Jionghao Lin", "Kenneth R. Koedinger"], "title": "Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation", "categories": ["cs.HC", "cs.CL"], "comment": "Manuscript accepted to the Workshop on \"From Data to Discovery: LLMs\n  for Qualitative Analysis in Education\" at LAK25", "summary": "Our study introduces an automated system leveraging large language models\n(LLMs) to assess the effectiveness of five key tutoring strategies: 1. giving\neffective praise, 2. reacting to errors, 3. determining what students know, 4.\nhelping students manage inequity, and 5. responding to negative self-talk.\nUsing a public dataset from the Teacher-Student Chatroom Corpus, our system\nclassifies each tutoring strategy as either being employed as desired or\nundesired. Our study utilizes GPT-3.5 with few-shot prompting to assess the use\nof these strategies and analyze tutoring dialogues. The results show that for\nthe five tutoring strategies, True Negative Rates (TNR) range from 0.655 to\n0.738, and Recall ranges from 0.327 to 0.432, indicating that the model is\neffective at excluding incorrect classifications but struggles to consistently\nidentify the correct strategy. The strategy \\textit{helping students manage\ninequity} showed the highest performance with a TNR of 0.738 and Recall of\n0.432. The study highlights the potential of LLMs in tutoring strategy analysis\nand outlines directions for future improvements, including incorporating more\nadvanced models for more nuanced feedback.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u8bc4\u4f30\u4e94\u79cd\u5173\u952e\u8f85\u5bfc\u7b56\u7565\u6709\u6548\u6027\u7684\u7cfb\u7edf\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6392\u9664\u9519\u8bef\u5206\u7c7b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bc6\u522b\u6b63\u786e\u7b56\u7565\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22LLM\u5728\u8f85\u5bfc\u7b56\u7565\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u6559\u80b2\u6280\u672f\u9886\u57df\u63d0\u4f9b\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u7528GPT-3.5\u548c\u5c11\u91cf\u793a\u4f8b\u63d0\u793a\u5bf9\u516c\u5f00\u6570\u636e\u96c6\uff08Teacher-Student Chatroom Corpus\uff09\u4e2d\u7684\u8f85\u5bfc\u5bf9\u8bdd\u8fdb\u884c\u5206\u6790\uff0c\u5206\u7c7b\u4e94\u79cd\u7b56\u7565\u7684\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u6a21\u578b\u5728\u4e94\u79cd\u7b56\u7565\u4e0a\u7684\u771f\u9634\u6027\u7387\uff08TNR\uff09\u4e3a0.655\u81f30.738\uff0c\u53ec\u56de\u7387\u4e3a0.327\u81f30.432\uff0c\u5176\u4e2d\u201c\u5e2e\u52a9\u5b66\u751f\u7ba1\u7406\u4e0d\u5e73\u7b49\u201d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLM\u5728\u8f85\u5bfc\u7b56\u7565\u5206\u6790\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.14606", "pdf": "https://arxiv.org/pdf/2504.14606", "abs": "https://arxiv.org/abs/2504.14606", "authors": ["Siyi Jiao", "Wenzheng Zeng", "Yerong Li", "Huayu Zhang", "Changxin Gao", "Nong Sang", "Mike Zheng Shou"], "title": "MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation", "categories": ["cs.CV"], "comment": "Accepted by ICLR 2025", "summary": "Human instance matting aims to estimate an alpha matte for each human\ninstance in an image, which is challenging as it easily fails in complex cases\nrequiring disentangling mingled pixels belonging to multiple instances along\nhairy and thin boundary structures. In this work, we address this by\nintroducing MP-Mat, a novel 3D-and-instance-aware matting framework with\nmultiplane representation, where the multiplane concept is designed from two\ndifferent perspectives: scene geometry level and instance level. Specifically,\nwe first build feature-level multiplane representations to split the scene into\nmultiple planes based on depth differences. This approach makes the scene\nrepresentation 3D-aware, and can serve as an effective clue for splitting\ninstances in different 3D positions, thereby improving interpretability and\nboundary handling ability especially in occlusion areas. Then, we introduce\nanother multiplane representation that splits the scene in an instance-level\nperspective, and represents each instance with both matte and color. We also\ntreat background as a special instance, which is often overlooked by existing\nmethods. Such an instance-level representation facilitates both foreground and\nbackground content awareness, and is useful for other down-stream tasks like\nimage editing. Once built, the representation can be reused to realize\ncontrollable instance-level image editing with high efficiency. Extensive\nexperiments validate the clear advantage of MP-Mat in matting task. We also\ndemonstrate its superiority in image editing tasks, an area under-explored by\nexisting matting-focused methods, where our approach under zero-shot inference\neven outperforms trained specialized image editing techniques by large margins.\nCode is open-sourced at https://github.com/JiaoSiyi/MPMat.git}.", "AI": {"tldr": "MP-Mat\u662f\u4e00\u79cd\u65b0\u9896\u76843D\u548c\u5b9e\u4f8b\u611f\u77e5\u7684\u62a0\u56fe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5e73\u9762\u8868\u793a\u4ece\u573a\u666f\u51e0\u4f55\u548c\u5b9e\u4f8b\u4e24\u4e2a\u89c6\u89d2\u89e3\u51b3\u590d\u6742\u60c5\u51b5\u4e0b\u7684\u4eba\u50cf\u62a0\u56fe\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u591a\u5b9e\u4f8b\u91cd\u53e0\u3001\u6bdb\u53d1\u548c\u7ec6\u8fb9\u754c\u7ed3\u6784\uff09\u4e2d\u96be\u4ee5\u51c6\u786e\u5206\u79bb\u50cf\u7d20\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u5e73\u9762\u8868\u793a\u65b9\u6cd5\uff0c\u4ece\u573a\u666f\u51e0\u4f55\u5c42\u9762\u548c\u5b9e\u4f8b\u5c42\u9762\u5206\u522b\u6784\u5efa\u7279\u5f81\u7ea7\u591a\u5e73\u9762\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u80cc\u666f\u4f5c\u4e3a\u7279\u6b8a\u5b9e\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMP-Mat\u5728\u62a0\u56fe\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u96f6\u6837\u672c\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e13\u4e1a\u65b9\u6cd5\u3002", "conclusion": "MP-Mat\u901a\u8fc7\u591a\u5e73\u9762\u8868\u793a\u663e\u8457\u63d0\u5347\u4e86\u62a0\u56fe\u6548\u679c\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13887", "pdf": "https://arxiv.org/pdf/2504.13887", "abs": "https://arxiv.org/abs/2504.13887", "authors": ["Isabel Villanueva", "Tara Bobinac", "Binwei Yao", "Junjie Hu", "Kaiping Chen"], "title": "AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants", "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": null, "summary": "Despite the growing integration of AI chatbots as conversational agents in\npublic discourse, empirical evidence regarding their capacity to foster\nintercultural empathy remains limited. Using a randomized dialogue experiment,\nwe examined how different types of AI chatbot interaction, i.e., deliberative\nversus non-deliberative and culturally aligned versus non-aligned, affect\nintercultural empathy across cultural groups. Results show that deliberative\nconversations increased intercultural empathy among American participants but\nnot Latin American participants, who perceived AI responses as culturally\ninaccurate and failing to represent their cultural contexts and perspectives\nauthentically. Real-time interaction analyses reveal that these differences\nstem from cultural knowledge gaps inherent in Large Language Models. Despite\nexplicit prompting and instruction to represent cultural perspectives in\nparticipants' native languages, AI systems still exhibit significant\ndisparities in cultural representation. This highlights the importance of\ndesigning AI systems capable of culturally authentic engagement in deliberative\nconversations. Our study contributes to deliberation theory and AI alignment\nresearch by underscoring AI's role in intercultural dialogue and the persistent\nchallenge of representational asymmetry in democratic discourse.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u5728\u4fc3\u8fdb\u8de8\u6587\u5316\u5171\u60c5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u6587\u5316\u5bf9\u9f50\u548c\u5bf9\u8bdd\u7c7b\u578b\u4e0a\u7684\u5dee\u5f02\u663e\u8457\u5f71\u54cd\u6548\u679c\u3002", "motivation": "\u63a2\u8ba8AI\u804a\u5929\u673a\u5668\u4eba\u5728\u8de8\u6587\u5316\u5bf9\u8bdd\u4e2d\u662f\u5426\u80fd\u6709\u6548\u4fc3\u8fdb\u5171\u60c5\uff0c\u4ee5\u53ca\u6587\u5316\u5bf9\u9f50\u548c\u5bf9\u8bdd\u7c7b\u578b\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u968f\u673a\u5bf9\u8bdd\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5ba1\u8bae\u5f0f\u4e0e\u975e\u5ba1\u8bae\u5f0f\u5bf9\u8bdd\uff0c\u4ee5\u53ca\u6587\u5316\u5bf9\u9f50\u4e0e\u975e\u5bf9\u9f50\u7684AI\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\u3002", "result": "\u5ba1\u8bae\u5f0f\u5bf9\u8bdd\u5bf9\u7f8e\u56fd\u53c2\u4e0e\u8005\u6709\u6548\uff0c\u4f46\u5bf9\u62c9\u4e01\u7f8e\u6d32\u53c2\u4e0e\u8005\u65e0\u6548\uff0c\u56e0\u540e\u8005\u8ba4\u4e3aAI\u7684\u6587\u5316\u8868\u8fbe\u4e0d\u51c6\u786e\u3002\u5b9e\u65f6\u5206\u6790\u663e\u793a\uff0c\u6587\u5316\u77e5\u8bc6\u5dee\u8ddd\u662f\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "AI\u7cfb\u7edf\u9700\u6539\u8fdb\u6587\u5316\u771f\u5b9e\u6027\uff0c\u4ee5\u652f\u6301\u8de8\u6587\u5316\u5bf9\u8bdd\u3002\u7814\u7a76\u4e3a\u5ba1\u8bae\u7406\u8bba\u548cAI\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.14618", "pdf": "https://arxiv.org/pdf/2504.14618", "abs": "https://arxiv.org/abs/2504.14618", "authors": ["Han Bi", "Ge Yu", "Yu He", "Wenzhuo Liu", "Zijie Zheng"], "title": "VM-BHINet:Vision Mamba Bimanual Hand Interaction Network for 3D Interacting Hand Mesh Recovery From a Single RGB Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding bimanual hand interactions is essential for realistic 3D pose\nand shape reconstruction. However, existing methods struggle with occlusions,\nambiguous appearances, and computational inefficiencies. To address these\nchallenges, we propose Vision Mamba Bimanual Hand Interaction Network\n(VM-BHINet), introducing state space models (SSMs) into hand reconstruction to\nenhance interaction modeling while improving computational efficiency. The core\ncomponent, Vision Mamba Interaction Feature Extraction Block (VM-IFEBlock),\ncombines SSMs with local and global feature operations, enabling deep\nunderstanding of hand interactions. Experiments on the InterHand2.6M dataset\nshow that VM-BHINet reduces Mean per-joint position error (MPJPE) and Mean\nper-vertex position error (MPVPE) by 2-3%, significantly surpassing\nstate-of-the-art methods.", "AI": {"tldr": "VM-BHINet\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6539\u8fdb\u53cc\u624b\u4ea4\u4e92\u91cd\u5efa\uff0c\u663e\u8457\u964d\u4f4e\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u3001\u6a21\u7cca\u5916\u89c2\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u6539\u8fdb\u53cc\u624b\u4ea4\u4e92\u5efa\u6a21\u3002", "method": "\u63d0\u51faVM-BHINet\uff0c\u7ed3\u5408\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4e0e\u5c40\u90e8\u5168\u5c40\u7279\u5f81\u64cd\u4f5c\uff0c\u6838\u5fc3\u4e3aVM-IFEBlock\u3002", "result": "\u5728InterHand2.6M\u6570\u636e\u96c6\u4e0a\uff0cMPJPE\u548cMPVPE\u964d\u4f4e2-3%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VM-BHINet\u6709\u6548\u63d0\u5347\u53cc\u624b\u4ea4\u4e92\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.13888", "pdf": "https://arxiv.org/pdf/2504.13888", "abs": "https://arxiv.org/abs/2504.13888", "authors": ["Paul Taele", "Jung In Koh", "Tracy Hammond"], "title": "Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Kanji script writing is a skill that is often introduced to novice Japanese\nforeign language students for achieving Japanese writing mastery, but often\nposes difficulties to students with primarily English fluency due to their its\nvast differences with written English. Instructors often introduce various\npedagogical methods -- such as visual structure and written techniques -- to\nassist students in kanji study, but may lack availability providing direct\nfeedback on students' writing outside of class. Current educational\napplications are also limited due to lacking richer instructor-emulated\nfeedback. We introduce Kanji Workbook, a writing-based intelligent tutoring\nsystem for students to receive intelligent assessment that emulates human\ninstructor feedback. Our interface not only leverages students' computing\ndevices for allowing them to learn, practice, and review the writing of\nprompted characters from their course's kanji script lessons, but also provides\na diverse set of writing assessment metrics -- derived from instructor\ninterviews and classroom observation insights -- through intelligent scoring\nand visual animations. We deployed our interface onto novice- and\nintermediate-level university courses over an entire academic year, and\nobserved that interface users on average achieved higher course grades than\ntheir peers and also reacted positively to our interface's various features.", "AI": {"tldr": "Kanji Workbook\u662f\u4e00\u4e2a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u6a21\u62df\u6559\u5e08\u53cd\u9988\u5e2e\u52a9\u5b66\u751f\u5b66\u4e60\u65e5\u8bed\u6c49\u5b57\u4e66\u5199\uff0c\u63d0\u5347\u8bfe\u7a0b\u6210\u7ee9\u3002", "motivation": "\u82f1\u8bed\u6bcd\u8bed\u5b66\u751f\u5728\u5b66\u4e60\u65e5\u8bed\u6c49\u5b57\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u73b0\u6709\u6559\u80b2\u5de5\u5177\u7f3a\u4e4f\u6559\u5e08\u6a21\u62df\u53cd\u9988\u3002", "method": "\u5f00\u53d1Kanji Workbook\u7cfb\u7edf\uff0c\u7ed3\u5408\u6559\u5e08\u8bbf\u8c08\u548c\u8bfe\u5802\u89c2\u5bdf\uff0c\u63d0\u4f9b\u667a\u80fd\u8bc4\u5206\u548c\u89c6\u89c9\u52a8\u753b\u53cd\u9988\u3002", "result": "\u4f7f\u7528\u8be5\u7cfb\u7edf\u7684\u5b66\u751f\u5728\u8bfe\u7a0b\u6210\u7ee9\u4e0a\u4f18\u4e8e\u540c\u9f84\u4eba\uff0c\u5e76\u5bf9\u7cfb\u7edf\u529f\u80fd\u53cd\u5e94\u79ef\u6781\u3002", "conclusion": "Kanji Workbook\u901a\u8fc7\u667a\u80fd\u53cd\u9988\u6709\u6548\u8f85\u52a9\u5b66\u751f\u6c49\u5b57\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.14621", "pdf": "https://arxiv.org/pdf/2504.14621", "abs": "https://arxiv.org/abs/2504.14621", "authors": ["Zhenkui Yang", "Zeyi Huang", "Ge Wang", "Han Ding", "Tony Xiao Han", "Fei Wang"], "title": "Talk is Not Always Cheap: Promoting Wireless Sensing Models with Text Prompts", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Wireless signal-based human sensing technologies, such as WiFi,\nmillimeter-wave (mmWave) radar, and Radio Frequency Identification (RFID),\nenable the detection and interpretation of human presence, posture, and\nactivities, thereby providing critical support for applications in public\nsecurity, healthcare, and smart environments. These technologies exhibit\nnotable advantages due to their non-contact operation and environmental\nadaptability; however, existing systems often fail to leverage the textual\ninformation inherent in datasets. To address this, we propose an innovative\ntext-enhanced wireless sensing framework, WiTalk, that seamlessly integrates\nsemantic knowledge through three hierarchical prompt strategies-label-only,\nbrief description, and detailed action description-without requiring\narchitectural modifications or incurring additional data costs. We rigorously\nvalidate this framework across three public benchmark datasets: XRF55 for human\naction recognition (HAR), and WiFiTAL and XRFV2 for WiFi temporal action\nlocalization (TAL). Experimental results demonstrate significant performance\nimprovements: on XRF55, accuracy for WiFi, RFID, and mmWave increases by 3.9%,\n2.59%, and 0.46%, respectively; on WiFiTAL, the average performance of WiFiTAD\nimproves by 4.98%; and on XRFV2, the mean average precision gains across\nvarious methods range from 4.02% to 13.68%. Our codes have been included in\nhttps://github.com/yangzhenkui/WiTalk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6587\u672c\u589e\u5f3a\u7684\u65e0\u7ebf\u4f20\u611f\u6846\u67b6WiTalk\uff0c\u901a\u8fc7\u4e09\u79cd\u5206\u5c42\u63d0\u793a\u7b56\u7565\u6574\u5408\u8bed\u4e49\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u4fe1\u53f7\uff08\u5982WiFi\u3001RFID\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff09\u5728\u4eba\u7c7b\u884c\u4e3a\u8bc6\u522b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u4f20\u611f\u6280\u672f\u867d\u5728\u975e\u63a5\u89e6\u64cd\u4f5c\u548c\u73af\u5883\u9002\u5e94\u6027\u4e0a\u6709\u4f18\u52bf\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u6570\u636e\u96c6\u4e2d\u7684\u6587\u672c\u4fe1\u606f\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u77e5\u8bc6\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faWiTalk\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u5206\u5c42\u63d0\u793a\u7b56\u7565\uff08\u6807\u7b7e\u3001\u7b80\u8981\u63cf\u8ff0\u548c\u8be6\u7ec6\u52a8\u4f5c\u63cf\u8ff0\uff09\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u6216\u589e\u52a0\u6570\u636e\u6210\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff1aXRF55\u4e0aWiFi\u3001RFID\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad83.9%\u30012.59%\u548c0.46%\uff1bWiFiTAL\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53474.98%\uff1bXRFV2\u4e0a\u5e73\u5747\u7cbe\u5ea6\u63d0\u53474.02%\u81f313.68%\u3002", "conclusion": "WiTalk\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8bed\u4e49\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u7ebf\u4f20\u611f\u6280\u672f\u7684\u6027\u80fd\uff0c\u4e3a\u516c\u5171\u5b89\u5168\u3001\u533b\u7597\u548c\u667a\u80fd\u73af\u5883\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u652f\u6301\u3002"}}
{"id": "2504.13890", "pdf": "https://arxiv.org/pdf/2504.13890", "abs": "https://arxiv.org/abs/2504.13890", "authors": ["Chen Shani", "Elizabeth C. Stade"], "title": "Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches", "categories": ["cs.HC", "cs.CL"], "comment": "No figures, 1 Table", "summary": "Computational mental health research develops models to predict and\nunderstand psychological phenomena, but often relies on inappropriate measures\nof psychopathology constructs, undermining validity. We identify three key\nissues: (1) reliance on unvalidated measures (e.g., self-declared diagnosis)\nover validated ones (e.g., diagnosis by clinician); (2) treating mental health\nconstructs as categorical rather than dimensional; and (3) focusing on\ndisorder-specific constructs instead of transdiagnostic ones. We outline the\nbenefits of using validated, dimensional, and transdiagnostic measures and\noffer practical recommendations for practitioners. Using valid measures that\nreflect the nature and structure of psychopathology is essential for\ncomputational mental health research.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u4e2d\u4f7f\u7528\u7684\u5fc3\u7406\u75c5\u7406\u5b66\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f7f\u7528\u9a8c\u8bc1\u6027\u3001\u7ef4\u5ea6\u548c\u8de8\u8bca\u65ad\u6d4b\u91cf\u7684\u5efa\u8bae\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u4e2d\u56e0\u4f7f\u7528\u4e0d\u9002\u5f53\u7684\u5fc3\u7406\u75c5\u7406\u5b66\u6d4b\u91cf\u65b9\u6cd5\u800c\u5bfc\u81f4\u7684\u6548\u5ea6\u95ee\u9898\u3002", "method": "\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4f9d\u8d56\u672a\u9a8c\u8bc1\u7684\u6d4b\u91cf\u3001\u5c06\u5fc3\u7406\u5065\u5eb7\u6784\u5ff5\u89c6\u4e3a\u5206\u7c7b\u800c\u975e\u7ef4\u5ea6\u3001\u5173\u6ce8\u7279\u5b9a\u969c\u788d\u800c\u975e\u8de8\u8bca\u65ad\u6784\u5ff5\u3002", "result": "\u63d0\u51fa\u4e86\u4f7f\u7528\u9a8c\u8bc1\u6027\u3001\u7ef4\u5ea6\u548c\u8de8\u8bca\u65ad\u6d4b\u91cf\u7684\u4f18\u52bf\u53ca\u5b9e\u8df5\u5efa\u8bae\u3002", "conclusion": "\u4f7f\u7528\u53cd\u6620\u5fc3\u7406\u75c5\u7406\u5b66\u672c\u8d28\u548c\u7ed3\u6784\u7684\u6709\u6548\u6d4b\u91cf\u65b9\u6cd5\u5bf9\u8ba1\u7b97\u5fc3\u7406\u5065\u5eb7\u7814\u7a76\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.14626", "pdf": "https://arxiv.org/pdf/2504.14626", "abs": "https://arxiv.org/abs/2504.14626", "authors": ["Santanu Roy", "Shweta Singh", "Palak Sahu", "Ashvath Suresh", "Debashish Das"], "title": "MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification", "categories": ["cs.CV"], "comment": null, "summary": "Lung cancer, a severe form of malignant tumor that originates in the tissues\nof the lungs, can be fatal if not detected in its early stages. It ranks among\nthe top causes of cancer-related mortality worldwide. Detecting lung cancer\nmanually using chest X-Ray image or Computational Tomography (CT) scans image\nposes significant challenges for radiologists. Hence, there is a need for\nautomatic diagnosis system of lung cancers from radiology images. With the\nrecent emergence of deep learning, particularly through Convolutional Neural\nNetworks (CNNs), the automated detection of lung cancer has become a much\nsimpler task. Nevertheless, numerous researchers have addressed that the\nperformance of conventional CNNs may be hindered due to class imbalance issue,\nwhich is prevalent in medical images. In this research work, we have proposed a\nnovel CNN architecture ``Multi-Scale Dense Network (MSD-Net)''\n(trained-from-scratch). The novelties we bring in the proposed model are (I) We\nintroduce novel dense modules in the 4th block and 5th block of the CNN model.\nWe have leveraged 3 depthwise separable convolutional (DWSC) layers, and one\n1x1 convolutional layer in each dense module, in order to reduce complexity of\nthe model considerably. (II) Additionally, we have incorporated one skip\nconnection from 3rd block to 5th block and one parallel branch connection from\n4th block to Global Average Pooling (GAP) layer. We have utilized dilated\nconvolutional layer (with dilation rate=2) in the last parallel branch in order\nto extract multi-scale features. Extensive experiments reveal that our proposed\nmodel has outperformed latest CNN model ConvNext-Tiny, recent trend Vision\nTransformer (ViT), Pooling-based ViT (PiT), and other existing models by\nsignificant margins.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578bCNN\u67b6\u6784MSD-Net\uff0c\u901a\u8fc7\u5f15\u5165\u5bc6\u96c6\u6a21\u5757\u548c\u7279\u6b8a\u8fde\u63a5\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80ba\u764c\u68c0\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u80ba\u764c\u65e9\u671f\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u624b\u52a8\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u6311\u6218\uff0c\u4e14\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u6027\u80fd\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86MSD-Net\uff0c\u5305\u542b\u5bc6\u96c6\u6a21\u5757\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5c42\u3001\u8df3\u8dc3\u8fde\u63a5\u548c\u5e73\u884c\u5206\u652f\uff0c\u4ee5\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\u5e76\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMSD-Net\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eConvNext-Tiny\u3001ViT\u3001PiT\u7b49\u6700\u65b0\u6a21\u578b\u3002", "conclusion": "MSD-Net\u4e3a\u80ba\u764c\u81ea\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13892", "pdf": "https://arxiv.org/pdf/2504.13892", "abs": "https://arxiv.org/abs/2504.13892", "authors": ["Stefano De Paoli", "Alex Fawzi"], "title": "TALLMesh: a simple application for performing Thematic Analysis with Large Language Models", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Thematic analysis (TA) is a widely used qualitative research method for\nidentifying and interpreting patterns within textual data, such as qualitative\ninterviews. Recent research has shown that it is possible to satisfactorily\nperform TA using Large Language Models (LLMs). This paper presents a novel\napplication using LLMs to assist researchers in conducting TA. The application\nenables users to upload textual data, generate initial codes and themes. All of\nthis is possible through a simple Graphical User Interface, (GUI) based on the\nstreamlit framework, working with python scripts for the analysis, and using\nApplication Program Interfaces of LLMs. Having a GUI is particularly important\nfor researchers in fields where coding skills may not be prevalent, such as\nsocial sciences or humanities. With the app, users can iteratively refine codes\nand themes adopting a human-in-the-loop process, without the need to work with\nprogramming and scripting. The paper describes the application key features,\nhighlighting its potential for qualitative research while preserving\nmethodological rigor. The paper discusses the design and interface of the app\nand outlines future directions for this work.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u5e94\u7528\uff0c\u7528\u4e8e\u8f85\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff08TA\uff09\uff0c\u65e0\u9700\u7f16\u7a0b\u6280\u80fd\u5373\u53ef\u64cd\u4f5c\u3002", "motivation": "\u4e3a\u7f3a\u4e4f\u7f16\u7a0b\u6280\u80fd\u7684\u7814\u7a76\u8005\uff08\u5982\u793e\u4f1a\u79d1\u5b66\u6216\u4eba\u6587\u5b66\u79d1\uff09\u63d0\u4f9b\u4e00\u79cd\u7b80\u5355\u5de5\u5177\uff0c\u5229\u7528LLMs\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff0c\u540c\u65f6\u4fdd\u6301\u65b9\u6cd5\u4e25\u8c28\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8estreamlit\u6846\u67b6\u7684GUI\u5e94\u7528\uff0c\u7528\u6237\u53ef\u4e0a\u4f20\u6587\u672c\u6570\u636e\uff0c\u751f\u6210\u521d\u59cb\u4ee3\u7801\u548c\u4e3b\u9898\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u5b8c\u5584\u5206\u6790\u3002", "result": "\u5e94\u7528\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7LLMs\u8f85\u52a9\u4e3b\u9898\u5206\u6790\u7684\u529f\u80fd\uff0c\u7b80\u5316\u4e86\u7814\u7a76\u6d41\u7a0b\uff0c\u5c24\u5176\u9002\u5408\u975e\u6280\u672f\u80cc\u666f\u7684\u7814\u7a76\u8005\u3002", "conclusion": "\u8be5\u5e94\u7528\u5c55\u793a\u4e86LLMs\u5728\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u529f\u80fd\u548c\u6269\u5c55\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.14638", "pdf": "https://arxiv.org/pdf/2504.14638", "abs": "https://arxiv.org/abs/2504.14638", "authors": ["Junyuan Fang", "Zihan Wang", "Yejun Zhang", "Shuzhe Wang", "Iaroslav Melekhov", "Juho Kannala"], "title": "NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation", "categories": ["cs.CV"], "comment": "15 pages, 4 figures, Scandinavian Conference on Image Analysis 2025", "summary": "Vision-language models (VLMs) have demonstrated impressive zero-shot transfer\ncapabilities in image-level visual perception tasks. However, they fall short\nin 3D instance-level segmentation tasks that require accurate localization and\nrecognition of individual objects. To bridge this gap, we introduce a novel 3D\nGaussian Splatting based hard visual prompting approach that leverages camera\ninterpolation to generate diverse viewpoints around target objects without any\n2D-3D optimization or fine-tuning. Our method simulates realistic 3D\nperspectives, effectively augmenting existing hard visual prompts by enforcing\ngeometric consistency across viewpoints. This training-free strategy seamlessly\nintegrates with prior hard visual prompts, enriching object-descriptive\nfeatures and enabling VLMs to achieve more robust and accurate 3D instance\nsegmentation in diverse 3D scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u786c\u89c6\u89c9\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u673a\u63d2\u503c\u751f\u6210\u591a\u89c6\u89d2\uff0c\u65e0\u97002D-3D\u4f18\u5316\u6216\u5fae\u8c03\uff0c\u63d0\u53473D\u5b9e\u4f8b\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u5b9e\u4f8b\u7ea7\u5206\u5272\u4efb\u52a1\u4e2d\u5b9a\u4f4d\u548c\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u548c\u76f8\u673a\u63d2\u503c\u751f\u6210\u591a\u89c6\u89d2\uff0c\u589e\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u65b9\u6cd5\u63d0\u5347\u4e863D\u5b9e\u4f8b\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13904", "pdf": "https://arxiv.org/pdf/2504.13904", "abs": "https://arxiv.org/abs/2504.13904", "authors": ["Donghuo Zeng", "Roberto Legaspi", "Yuewen Sun", "Xinshuai Dong", "Kazushi Ikeda", "Peter Spirtes", "Kun Zhang"], "title": "Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "12 pages, 10 figures, 1 table. Accepted by ACM UMAP 2025", "summary": "We hypothesize that optimal system responses emerge from adaptive strategies\ngrounded in causal and counterfactual knowledge. Counterfactual inference\nallows us to create hypothetical scenarios to examine the effects of\nalternative system responses. We enhance this process through causal discovery,\nwhich identifies the strategies informed by the underlying causal structure\nthat govern system behaviors. Moreover, we consider the psychological\nconstructs and unobservable noises that might be influencing user-system\ninteractions as latent factors. We show that these factors can be effectively\nestimated. We employ causal discovery to identify strategy-level causal\nrelationships among user and system utterances, guiding the generation of\npersonalized counterfactual dialogues. We model the user utterance strategies\nas causal factors, enabling system strategies to be treated as counterfactual\nactions. Furthermore, we optimize policies for selecting system responses based\non counterfactual data. Our results using a real-world dataset on social good\ndemonstrate significant improvements in persuasive system outcomes, with\nincreased cumulative rewards validating the efficacy of causal discovery in\nguiding personalized counterfactual inference and optimizing dialogue policies\nfor a persuasive dialogue system.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u548c\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u7684\u81ea\u9002\u5e94\u7b56\u7565\u751f\u6210\u6700\u4f18\u7cfb\u7edf\u54cd\u5e94\uff0c\u901a\u8fc7\u56e0\u679c\u53d1\u73b0\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u4f18\u5316\u5bf9\u8bdd\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8bf4\u670d\u6027\u5bf9\u8bdd\u7cfb\u7edf\u7684\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u56e0\u679c\u548c\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u4f18\u5316\u7cfb\u7edf\u54cd\u5e94\uff0c\u4ee5\u63d0\u5347\u7528\u6237\u4e0e\u7cfb\u7edf\u4ea4\u4e92\u7684\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u8bf4\u670d\u6027\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u56e0\u679c\u53d1\u73b0\u8bc6\u522b\u7b56\u7565\u7ea7\u56e0\u679c\u5173\u7cfb\uff0c\u53cd\u4e8b\u5b9e\u63a8\u7406\u751f\u6210\u4e2a\u6027\u5316\u5bf9\u8bdd\uff0c\u4ee5\u53ca\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u6570\u636e\u4f18\u5316\u7cfb\u7edf\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8bf4\u670d\u6027\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7d2f\u79ef\u5956\u52b1\uff0c\u9a8c\u8bc1\u4e86\u56e0\u679c\u53d1\u73b0\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u7ed3\u5408\u56e0\u679c\u53d1\u73b0\u548c\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316\u5bf9\u8bdd\u7b56\u7565\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2504.14642", "pdf": "https://arxiv.org/pdf/2504.14642", "abs": "https://arxiv.org/abs/2504.14642", "authors": ["Lin Li", "Wei Chen", "Jiahui Li", "Long Chen"], "title": "Relation-R1: Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relational Comprehension", "categories": ["cs.CV"], "comment": "Ongoing project", "summary": "Recent advances in multi-modal large language models (MLLMs) have\nsignificantly improved object-level grounding and region captioning, but remain\nlimited in visual relation understanding (\\eg, scene graph generation),\nparticularly in modeling \\textit{N}-ary relationships that identify multiple\nsemantic roles among an action event. Such a lack of \\textit{semantic\ndependencies} modeling among multi-entities leads to unreliable outputs,\nintensifying MLLMs' hallucinations and over-reliance on language priors. To\nthis end, we propose Relation-R1, the first unified relational comprehension\nframework that explicitly integrates cognitive chain-of-thought (CoT)-guided\nSupervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO)\nwithin a reinforcement learning (RL) paradigm. Specifically, we first establish\nfoundational reasoning capabilities via SFT, enforcing structured outputs with\nthinking processes. Then, GRPO is utilized to refine these outputs via\nmulti-reward optimization, prioritizing visual-semantic grounding over\nlanguage-induced biases, thereby improving generalization capability. Extensive\nexperiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1\nachieves state-of-the-art performance in both binary and \\textit{N}-ary\nrelation understanding.", "AI": {"tldr": "Relation-R1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5173\u7cfb\u7406\u89e3\u6846\u67b6\uff0c\u901a\u8fc7\u8ba4\u77e5\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5f15\u5bfc\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u5173\u7cfb\u7406\u89e3\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u5173\u7cfb\u7406\u89e3\uff08\u5982\u573a\u666f\u56fe\u751f\u6210\uff09\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u662f\u5bf9N\u5143\u5173\u7cfb\u7684\u5efa\u6a21\u4e0d\u8db3\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e0d\u53ef\u9760\u3002Relation-R1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8ba4\u77e5\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5f15\u5bfc\u7684SFT\u548cGRPO\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6a21\u578b\u8f93\u51fa\uff0c\u4f18\u5148\u8003\u8651\u89c6\u89c9\u8bed\u4e49\u57fa\u7840\u800c\u975e\u8bed\u8a00\u5148\u9a8c\u3002", "result": "\u5728PSG\u548cSWiG\u6570\u636e\u96c6\u4e0a\uff0cRelation-R1\u5728\u4e8c\u5143\u548cN\u5143\u5173\u7cfb\u7406\u89e3\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "Relation-R1\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u591a\u5956\u52b1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLMs\u5728\u89c6\u89c9\u5173\u7cfb\u7406\u89e3\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.13924", "pdf": "https://arxiv.org/pdf/2504.13924", "abs": "https://arxiv.org/abs/2504.13924", "authors": ["Akash V. Maharaj", "David Arbour", "Daniel Lee", "Uttaran Bhattacharya", "Anup Rao", "Austin Zane", "Avi Feller", "Kun Qian", "Yunyao Li"], "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "7 pages, 5 figures. Accepted at IAAI-25", "summary": "Enterprise AI Assistants are increasingly deployed in domains where accuracy\nis paramount, making each erroneous output a potentially significant incident.\nThis paper presents a comprehensive framework for monitoring, benchmarking, and\ncontinuously improving such complex, multi-component systems under active\ndevelopment by multiple teams. Our approach encompasses three key elements: (1)\na hierarchical ``severity'' framework for incident detection that identifies\nand categorizes errors while attributing component-specific error rates,\nfacilitating targeted improvements; (2) a scalable and principled methodology\nfor benchmark construction, evaluation, and deployment, designed to accommodate\nmultiple development teams, mitigate overfitting risks, and assess the\ndownstream impact of system modifications; and (3) a continual improvement\nstrategy leveraging multidimensional evaluation, enabling the identification\nand implementation of diverse enhancement opportunities. By adopting this\nholistic framework, organizations can systematically enhance the reliability\nand performance of their AI Assistants, ensuring their efficacy in critical\nenterprise environments. We conclude by discussing how this multifaceted\nevaluation approach opens avenues for various classes of enhancements, paving\nthe way for more robust and trustworthy AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u63a7\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6301\u7eed\u6539\u8fdb\u4f01\u4e1aAI\u52a9\u624b\uff0c\u6db5\u76d6\u9519\u8bef\u68c0\u6d4b\u3001\u57fa\u51c6\u6784\u5efa\u548c\u6301\u7eed\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u4f01\u4e1aAI\u52a9\u624b\u5728\u51c6\u786e\u6027\u8981\u6c42\u9ad8\u7684\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u9519\u8bef\u8f93\u51fa\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u4e8b\u6545\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4e09\u90e8\u5206\uff1a\u5206\u5c42\u9519\u8bef\u68c0\u6d4b\u3001\u53ef\u6269\u5c55\u57fa\u51c6\u6784\u5efa\u65b9\u6cd5\uff0c\u4ee5\u53ca\u591a\u7ef4\u8bc4\u4f30\u7684\u6301\u7eed\u6539\u8fdb\u7b56\u7565\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u7cfb\u7edf\u6027\u63d0\u5347AI\u52a9\u624b\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\uff0c\u786e\u4fdd\u5176\u5728\u5173\u952e\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u4e3aAI\u7cfb\u7edf\u7684\u589e\u5f3a\u5f00\u8f9f\u4e86\u9014\u5f84\uff0c\u63a8\u52a8\u66f4\u5f3a\u5927\u548c\u53ef\u4fe1\u8d56\u7684AI\u53d1\u5c55\u3002"}}
{"id": "2504.14658", "pdf": "https://arxiv.org/pdf/2504.14658", "abs": "https://arxiv.org/abs/2504.14658", "authors": ["Jing Zhang", "Dan Guo", "Zhangbin Li", "Meng Wang"], "title": "EmoSEM: Segment and Explain Emotion Stimuli in Visual Art", "categories": ["cs.CV"], "comment": null, "summary": "This paper focuses on a key challenge in visual art understanding: given an\nart image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for the emotional arousal.\nDespite recent advances in art understanding, pixel-level emotion understanding\nstill faces a dual challenge: first, the subjectivity of emotion makes it\ndifficult for general segmentation models like SAM to adapt to emotion-oriented\nsegmentation tasks; and second, the abstract nature of art expression makes it\ndifficult for captioning models to balance pixel-level semantic understanding\nand emotion reasoning. To solve the above problems, this paper proposes the\nEmotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the\nsegmentation model SAM with emotion comprehension capability. First, to enable\nthe model to perform segmentation under the guidance of emotional intent well,\nwe introduce an emotional prompt with a learnable mask token as the conditional\ninput for segmentation decoding. Then, we design an emotion projector to\nestablish the association between emotion and visual features. Next, more\nimportantly, to address emotion-visual stimuli alignment, we develop a\nlightweight prefix projector, a module that fuses the learned emotional mask\nwith the corresponding emotion into a unified representation compatible with\nthe language model.Finally, we input the joint visual, mask, and emotional\ntokens into the language model and output the emotional explanations. It\nensures that the generated interpretations remain semantically and emotionally\ncoherent with the visual stimuli. The method innovatively realizes end-to-end\nmodeling from low-level pixel features to high-level emotion interpretation,\nproviding the first interpretable fine-grained analysis framework for artistic\nemotion computing. Extensive experiments validate the effectiveness of our\nmodel.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEmoSEM\u6a21\u578b\uff0c\u901a\u8fc7\u60c5\u611f\u63d0\u793a\u548c\u8f7b\u91cf\u7ea7\u524d\u7f00\u6295\u5f71\u5668\uff0c\u5b9e\u73b0\u827a\u672f\u56fe\u50cf\u4e2d\u60c5\u611f\u89e6\u53d1\u533a\u57df\u7684\u50cf\u7d20\u7ea7\u5206\u5272\u548c\u60c5\u611f\u89e3\u91ca\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u827a\u672f\u56fe\u50cf\u4e2d\u50cf\u7d20\u7ea7\u60c5\u611f\u7406\u89e3\u7684\u6311\u6218\uff0c\u5305\u62ec\u60c5\u611f\u4e3b\u89c2\u6027\u548c\u827a\u672f\u8868\u8fbe\u7684\u62bd\u8c61\u6027\u3002", "method": "\u7ed3\u5408\u60c5\u611f\u63d0\u793a\u3001\u60c5\u611f\u6295\u5f71\u5668\u548c\u524d\u7f00\u6295\u5f71\u5668\uff0c\u5c06\u60c5\u611f\u4e0e\u89c6\u89c9\u7279\u5f81\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u60c5\u611f\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u50cf\u7d20\u7279\u5f81\u5230\u60c5\u611f\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u5efa\u6a21\u3002", "conclusion": "EmoSEM\u4e3a\u827a\u672f\u60c5\u611f\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u89e3\u91ca\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u6846\u67b6\u3002"}}
{"id": "2504.13932", "pdf": "https://arxiv.org/pdf/2504.13932", "abs": "https://arxiv.org/abs/2504.13932", "authors": ["Deyu Cao", "Samin Aref"], "title": "Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining", "categories": ["cs.LG", "cs.CL", "68T50, 68T07, 68T09, 68U15", "I.2.7; I.2.6; I.2.4"], "comment": "28 pages, 5 figures, 11 tables", "summary": "Large language models offer remarkable capabilities, but their size and\ncomputational demands pose practical challenges. Quantization methods compress\ntheir size through replacing their high-precision parameters by quantized\nvalues of lower precision. Post-training quantization reduces model size\nefficiently at the cost of decreased accuracy, while quantization-aware\ntraining better preserves accuracy but is resource-intensive. Among existing\npost-training quantization algorithms, the ApiQ method achieves superior\naccuracy preservation at minimal memory and time overhead. We investigate two\nideas to extend performance in ultra-low-bit quantization beyond ApiQ's level.\nFirst, we look into combining existing quantization-aware training techniques\nwith ApiQ's partial training. We show that this does not outperform the\nbaseline ApiQ method with limited training data and frozen weights. This leads\nto two key insights: (1) The substantial representational capacity that is\ngained through full retraining may not be feasible through partial training.\n(2) This gain seems to depend on using a large and diverse dataset in\nquantization-aware training. Second, through a novel approach informed by the\ntwo insights, we propose an ultra-low-bit quantization method that builds upon\nApiQ and extends its performance without the need for full retraining. It\nrelies on a saliency-aware regularization term that prioritizes preserving the\nmost impactful parameters during quantization. Our experiments on benchmark\nlanguage models from the LLaMA family show that our proposed approach boosts\naccuracy and tightens the gap between the quantized model and the\nfull-precision model, with minimal overhead. Our method will be made publicly\navailable to facilitate future developments in ultra-low-bit quantization of\nlarge language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eApiQ\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u663e\u8457\u6027\u611f\u77e5\u6b63\u5219\u5316\uff0c\u5728\u8d85\u4f4e\u4f4d\u91cf\u5316\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u91cf\u5316\u65b9\u6cd5\u80fd\u538b\u7f29\u6a21\u578b\u4f46\u53ef\u80fd\u727a\u7272\u7cbe\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u5982ApiQ\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u7ed3\u5408ApiQ\u7684\u90e8\u5206\u8bad\u7ec3\u4e0e\u663e\u8457\u6027\u611f\u77e5\u6b63\u5219\u5316\uff0c\u4f18\u5148\u4fdd\u7559\u5173\u952e\u53c2\u6570\uff0c\u63d0\u51fa\u4e00\u79cd\u8d85\u4f4e\u4f4d\u91cf\u5316\u65b9\u6cd5\u3002", "result": "\u5728LLaMA\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u63d0\u5347\u4e86\u7cbe\u5ea6\uff0c\u7f29\u5c0f\u4e86\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u8d85\u4f4e\u4f4d\u91cf\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u672a\u6765\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2504.14664", "pdf": "https://arxiv.org/pdf/2504.14664", "abs": "https://arxiv.org/abs/2504.14664", "authors": ["Jixiang Sun", "Fei Lei", "Jiawei Zhang", "Wenxiu Sun", "Yujiu Yang"], "title": "Frequency-domain Learning with Kernel Prior for Blind Image Deblurring", "categories": ["cs.CV"], "comment": null, "summary": "While achieving excellent results on various datasets, many deep learning\nmethods for image deblurring suffer from limited generalization capabilities\nwith out-of-domain data. This limitation is likely caused by their dependence\non certain domain-specific datasets. To address this challenge, we argue that\nit is necessary to introduce the kernel prior into deep learning methods, as\nthe kernel prior remains independent of the image context. For effective fusion\nof kernel prior information, we adopt a rational implementation method inspired\nby traditional deblurring algorithms that perform deconvolution in the\nfrequency domain. We propose a module called Frequency Integration Module (FIM)\nfor fusing the kernel prior and combine it with a frequency-based deblurring\nTransfomer network. Experimental results demonstrate that our method\noutperforms state-of-the-art methods on multiple blind image deblurring tasks,\nshowcasing robust generalization abilities. Source code will be available soon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6838\u5148\u9a8c\u548c\u9891\u7387\u57df\u65b9\u6cd5\u7684\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u53bb\u6a21\u7cca\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u96c6\u6210\u6a21\u5757\uff08FIM\uff09\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u56fe\u50cf\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u3002\u6838\u5148\u9a8c\u56e0\u5176\u4e0e\u56fe\u50cf\u5185\u5bb9\u65e0\u5173\uff0c\u53ef\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9891\u7387\u57df\u89e3\u5377\u79ef\u7684\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u9891\u7387\u96c6\u6210\u6a21\u5757\uff08FIM\uff09\u878d\u5408\u6838\u5148\u9a8c\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u9891\u7387\u7684\u53bb\u6a21\u7ccaTransformer\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u76f2\u56fe\u50cf\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f15\u5165\u6838\u5148\u9a8c\u5e76\u901a\u8fc7\u9891\u7387\u57df\u65b9\u6cd5\u878d\u5408\uff0c\u80fd\u6709\u6548\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u53bb\u6a21\u7cca\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.13955", "pdf": "https://arxiv.org/pdf/2504.13955", "abs": "https://arxiv.org/abs/2504.13955", "authors": ["Suhas BN", "Dominik Mattioli", "Saeed Abdullah", "Rosa I. Arriaga", "Chris W. Wiese", "Andrew M. Sherrill"], "title": "Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "68T50", "I.2.7; H.5.2"], "comment": "14 pages, 6 figures", "summary": "The advancement of AI systems for mental health support is hindered by\nlimited access to therapeutic conversation data, particularly for trauma\ntreatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset\nof 3,000 therapy conversations based on Prolonged Exposure therapy protocols\nfor Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique\ncases, each explored through six conversational perspectives that mirror the\nprogression of therapy from initial anxiety to peak distress to emotional\nprocessing. We incorporated diverse demographic profiles (ages 18-80, M=49.3,\n49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10\ntrauma-related behaviors using deterministic and probabilistic generation\nmethods. Analysis reveals realistic distributions of trauma types (witnessing\nviolence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse\n20.8%). Clinical experts validated the dataset's therapeutic fidelity,\nhighlighting its emotional depth while suggesting refinements for greater\nauthenticity. We also developed an emotional trajectory benchmark with\nstandardized metrics for evaluating model responses. This privacy-preserving\ndataset addresses critical gaps in trauma-focused mental health data, offering\na valuable resource for advancing both patient-facing applications and\nclinician training tools.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a'Thousand Voices of Trauma'\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b3,000\u4e2a\u57fa\u4e8ePTSD\u6cbb\u7597\u534f\u8bae\u7684\u6a21\u62df\u6cbb\u7597\u5bf9\u8bdd\uff0c\u586b\u8865\u4e86\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u4e2d\u7684\u5173\u952e\u7a7a\u767d\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u9886\u57df\u7684\u8fdb\u5c55\u53d7\u5230\u6cbb\u7597\u5bf9\u8bdd\u6570\u636e\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u521b\u4f24\u6cbb\u7597\u7684\u6570\u636e\u3002", "method": "\u901a\u8fc7\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u751f\u6210\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5305\u542b500\u4e2a\u72ec\u7279\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u6848\u4f8b\u901a\u8fc7\u516d\u4e2a\u5bf9\u8bdd\u89c6\u89d2\u5c55\u73b0\u6cbb\u7597\u8fdb\u7a0b\uff0c\u5e76\u6db5\u76d6\u591a\u6837\u5316\u7684 demographics\u3001\u521b\u4f24\u7c7b\u578b\u548c\u884c\u4e3a\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u771f\u5b9e\u7684\u521b\u4f24\u7c7b\u578b\u548c\u75c7\u72b6\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\u4e86\u5176\u6cbb\u7597\u4fdd\u771f\u5ea6\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u60c5\u611f\u8f68\u8ff9\u57fa\u51c6\u7528\u4e8e\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u96c6\u4e3a\u521b\u4f24\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u7684\u4e0d\u8db3\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u60a3\u8005\u5e94\u7528\u548c\u4e34\u5e8a\u57f9\u8bad\u5de5\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2504.14665", "pdf": "https://arxiv.org/pdf/2504.14665", "abs": "https://arxiv.org/abs/2504.14665", "authors": ["A S M Sharifuzzaman Sagar", "Yu Chen", "Jun Hoong Chan"], "title": "DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback Representations", "categories": ["cs.CV"], "comment": null, "summary": "Traditional predictive coding networks, inspired by theories of brain\nfunction, consistently achieve promising results across various domains,\nextending their influence into the field of computer vision. However, the\nperformance of the predictive coding networks is limited by their error\nfeedback mechanism, which traditionally employs either local or global\nrecurrent updates, leading to suboptimal performance in processing both local\nand broader details simultaneously. In addition, traditional predictive coding\nnetworks face difficulties in dynamically adjusting to the complexity and\ncontext of varying input data, which is crucial for achieving high levels of\nperformance in diverse scenarios. Furthermore, there is a gap in the\ndevelopment and application of specific loss functions that could more\neffectively guide the model towards optimal performance. To deal with these\nissues, this paper introduces a hybrid prediction error feedback mechanism with\ndynamic modulation for deep predictive coding networks by effectively combining\nglobal contexts and local details while adjusting feedback based on input\ncomplexity. Additionally, we present a loss function tailored to this framework\nto improve accuracy by focusing on precise prediction error minimization.\nExperimental results demonstrate the superiority of our model over other\napproaches, showcasing faster convergence and higher predictive accuracy in\nCIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u9884\u6d4b\u8bef\u5dee\u53cd\u9988\u673a\u5236\u548c\u52a8\u6001\u8c03\u5236\u7684\u6df1\u5ea6\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5c40\u90e8\u548c\u5168\u5c40\u7ec6\u8282\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u7528\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u5728\u5c40\u90e8\u548c\u5168\u5c40\u8bef\u5dee\u53cd\u9988\u673a\u5236\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u96be\u4ee5\u52a8\u6001\u9002\u5e94\u8f93\u5165\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u5f15\u5165\u6df7\u5408\u9884\u6d4b\u8bef\u5dee\u53cd\u9988\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7ec6\u8282\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u53cd\u9988\uff1b\u8bbe\u8ba1\u4e13\u7528\u635f\u5931\u51fd\u6570\u4ee5\u4f18\u5316\u9884\u6d4b\u8bef\u5dee\u6700\u5c0f\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u5728CIFAR-10\u3001CIFAR-100\u3001MNIST\u548cFashionMNIST\u6570\u636e\u96c6\u4e0a\u6536\u655b\u66f4\u5feb\u4e14\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7f16\u7801\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2504.13958", "pdf": "https://arxiv.org/pdf/2504.13958", "abs": "https://arxiv.org/abs/2504.13958", "authors": ["Cheng Qian", "Emre Can Acikgoz", "Qi He", "Hongru Wang", "Xiusi Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Heng Ji"], "title": "ToolRL: Reward is All Tool Learning Needs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "19 Pages, 12 Figures, 12 Tables", "summary": "Current Large Language Models (LLMs) often undergo supervised fine-tuning\n(SFT) to acquire tool use capabilities. However, SFT struggles to generalize to\nunfamiliar or complex tool use scenarios. Recent advancements in reinforcement\nlearning (RL), particularly with R1-like models, have demonstrated promising\nreasoning and generalization abilities. Yet, reward design for tool use\npresents unique challenges: multiple tools may be invoked with diverse\nparameters, and coarse-grained reward signals, such as answer matching, fail to\noffer the finegrained feedback required for effective learning. In this work,\nwe present the first comprehensive study on reward design for tool selection\nand application tasks within the RL paradigm. We systematically explore a wide\nrange of reward strategies, analyzing their types, scales, granularity, and\ntemporal dynamics. Building on these insights, we propose a principled reward\ndesign tailored for tool use tasks and apply it to train LLMs using Group\nRelative Policy Optimization (GRPO). Empirical evaluations across diverse\nbenchmarks demonstrate that our approach yields robust, scalable, and stable\ntraining, achieving a 17% improvement over base models and a 15% gain over SFT\nmodels. These results highlight the critical role of thoughtful reward design\nin enhancing the tool use capabilities and generalization performance of LLMs.\nAll the codes are released to facilitate future research.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5f3a\u5316\u5b66\u4e60\u4e2d\u5956\u52b1\u8bbe\u8ba1\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u590d\u6742\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u5956\u52b1\u8bbe\u8ba1\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u53cd\u9988\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u4e86\u591a\u79cd\u5956\u52b1\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u7684\u5956\u52b1\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528GRPO\u8bad\u7ec3LLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u534717%\uff0c\u6bd4SFT\u6a21\u578b\u63d0\u534715%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.14666", "pdf": "https://arxiv.org/pdf/2504.14666", "abs": "https://arxiv.org/abs/2504.14666", "authors": ["Kaihang Pan", "Wang Lin", "Zhongqi Yue", "Tenglong Ao", "Liyu Jia", "Wei Zhao", "Juncheng Li", "Siliang Tang", "Hanwang Zhang"], "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Oral)", "summary": "Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation by combining LLM and diffusion models, the\nstate-of-the-art in each task, respectively. Existing approaches rely on\nspatial visual tokens, where image patches are encoded and arranged according\nto a spatial order (e.g., raster scan). However, we show that spatial tokens\nlack the recursive structure inherent to languages, hence form an impossible\nlanguage for LLM to master. In this paper, we build a proper visual language by\nleveraging diffusion timesteps to learn discrete, recursive visual tokens. Our\nproposed tokens recursively compensate for the progressive attribute loss in\nnoisy images as timesteps increase, enabling the diffusion model to reconstruct\nthe original image at any timestep. This approach allows us to effectively\nintegrate the strengths of LLMs in autoregressive reasoning and diffusion\nmodels in precise image generation, achieving seamless multimodal comprehension\nand generation within a unified framework. Extensive experiments show that we\nachieve superior performance for multimodal comprehension and generation\nsimultaneously compared with other MLLMs. Project Page:\nhttps://DDT-LLaMA.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u65f6\u95f4\u6b65\u5b66\u4e60\u79bb\u6563\u3001\u9012\u5f52\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7a7a\u95f4\u6807\u8bb0\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a7a\u95f4\u89c6\u89c9\u6807\u8bb0\uff0c\u4f46\u8fd9\u4e9b\u6807\u8bb0\u7f3a\u4e4f\u8bed\u8a00\u7684\u9012\u5f52\u7ed3\u6784\uff0c\u9650\u5236\u4e86LLM\u7684\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u4e00\u79cd\u66f4\u9002\u5408LLM\u7684\u89c6\u89c9\u8bed\u8a00\u3002", "method": "\u901a\u8fc7\u6269\u6563\u65f6\u95f4\u6b65\u5b66\u4e60\u9012\u5f52\u89c6\u89c9\u6807\u8bb0\uff0c\u8865\u507f\u566a\u58f0\u56fe\u50cf\u4e2d\u7684\u5c5e\u6027\u635f\u5931\uff0c\u7ed3\u5408LLM\u7684\u81ea\u56de\u5f52\u63a8\u7406\u548c\u6269\u6563\u6a21\u578b\u7684\u7cbe\u786e\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6MLLM\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89c9\u8bed\u8a00\u65b9\u6cd5\u6709\u6548\u6574\u5408\u4e86LLM\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u65e0\u7f1d\u7684\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u3002"}}
{"id": "2504.13959", "pdf": "https://arxiv.org/pdf/2504.13959", "abs": "https://arxiv.org/abs/2504.13959", "authors": ["Sanchaita Hazra", "Bodhisattwa Prasad Majumder", "Tuhin Chakrabarty"], "title": "AI Safety Should Prioritize the Future of Work", "categories": ["cs.CY", "cs.AI", "cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Current efforts in AI safety prioritize filtering harmful content, preventing\nmanipulation of human behavior, and eliminating existential risks in\ncybersecurity or biosecurity. While pressing, this narrow focus overlooks\ncritical human-centric considerations that shape the long-term trajectory of a\nsociety. In this position paper, we identify the risks of overlooking the\nimpact of AI on the future of work and recommend comprehensive transition\nsupport towards the evolution of meaningful labor with human agency. Through\nthe lens of economic theories, we highlight the intertemporal impacts of AI on\nhuman livelihood and the structural changes in labor markets that exacerbate\nincome inequality. Additionally, the closed-source approach of major\nstakeholders in AI development resembles rent-seeking behavior through\nexploiting resources, breeding mediocrity in creative labor, and monopolizing\ninnovation. To address this, we argue in favor of a robust international\ncopyright anatomy supported by implementing collective licensing that ensures\nfair compensation mechanisms for using data to train AI models. We strongly\nrecommend a pro-worker framework of global AI governance to enhance shared\nprosperity and economic justice while reducing technical debt.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u6280\u672f\u98ce\u9669\uff0c\u5ffd\u89c6\u4e86AI\u5bf9\u5de5\u4f5c\u672a\u6765\u548c\u4eba\u7c7b\u751f\u8ba1\u7684\u5f71\u54cd\uff0c\u5efa\u8bae\u901a\u8fc7\u7ecf\u6d4e\u7406\u8bba\u548c\u5168\u7403\u6cbb\u7406\u6846\u67b6\u652f\u6301\u516c\u5e73\u8fc7\u6e21\u3002", "motivation": "\u5f53\u524dAI\u5b89\u5168\u7814\u7a76\u96c6\u4e2d\u4e8e\u6280\u672f\u98ce\u9669\uff0c\u5ffd\u7565\u4e86AI\u5bf9\u793e\u4f1a\u7ed3\u6784\u548c\u4eba\u7c7b\u5de5\u4f5c\u7684\u957f\u671f\u5f71\u54cd\uff0c\u53ef\u80fd\u5bfc\u81f4\u6536\u5165\u4e0d\u5e73\u7b49\u52a0\u5267\u548c\u521b\u65b0\u5784\u65ad\u3002", "method": "\u901a\u8fc7\u7ecf\u6d4e\u7406\u8bba\u5206\u6790AI\u5bf9\u52b3\u52a8\u529b\u5e02\u573a\u7684\u7ed3\u6784\u6027\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u5168\u7403\u6cbb\u7406\u6846\u67b6\u548c\u96c6\u4f53\u8bb8\u53ef\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u53ef\u80fd\u52a0\u5267\u6536\u5165\u4e0d\u5e73\u7b49\u548c\u521b\u65b0\u5784\u65ad\uff0c\u9700\u901a\u8fc7\u516c\u5e73\u8865\u507f\u673a\u5236\u548c\u5168\u7403\u6cbb\u7406\u6846\u67b6\u89e3\u51b3\u3002", "conclusion": "\u5efa\u8bae\u5efa\u7acb\u652f\u6301\u5de5\u4eba\u6743\u76ca\u7684\u5168\u7403AI\u6cbb\u7406\u6846\u67b6\uff0c\u786e\u4fdd\u7ecf\u6d4e\u516c\u5e73\u548c\u521b\u65b0\u5171\u4eab\u3002"}}
{"id": "2504.14687", "pdf": "https://arxiv.org/pdf/2504.14687", "abs": "https://arxiv.org/abs/2504.14687", "authors": ["Seokju Cho", "Jiahui Huang", "Seungryong Kim", "Joon-Young Lee"], "title": "Seurat: From Moving Points to Depth", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlight. Project page: https://seurat-cvpr.github.io", "summary": "Accurate depth estimation from monocular videos remains challenging due to\nambiguities inherent in single-view geometry, as crucial depth cues like\nstereopsis are absent. However, humans often perceive relative depth\nintuitively by observing variations in the size and spacing of objects as they\nmove. Inspired by this, we propose a novel method that infers relative depth by\nexamining the spatial relationships and temporal evolution of a set of tracked\n2D trajectories. Specifically, we use off-the-shelf point tracking models to\ncapture 2D trajectories. Then, our approach employs spatial and temporal\ntransformers to process these trajectories and directly infer depth changes\nover time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust\nzero-shot performance, generalizing effectively from synthetic to real-world\ndatasets. Results indicate that our approach achieves temporally smooth,\nhigh-accuracy depth predictions across diverse domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5206\u67902D\u8f68\u8ff9\u63a8\u65ad\u76f8\u5bf9\u6df1\u5ea6\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u6362\u5668\u5904\u7406\u8f68\u8ff9\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u9884\u6d4b\u3002", "motivation": "\u5355\u76ee\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u53d7\u4eba\u7c7b\u901a\u8fc7\u89c2\u5bdf\u7269\u4f53\u5927\u5c0f\u548c\u95f4\u8ddd\u53d8\u5316\u611f\u77e5\u6df1\u5ea6\u7684\u542f\u53d1\u3002", "method": "\u4f7f\u7528\u73b0\u6709\u70b9\u8ddf\u8e2a\u6a21\u578b\u6355\u83b72D\u8f68\u8ff9\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u6362\u5668\u5904\u7406\u8f68\u8ff9\u5e76\u63a8\u65ad\u6df1\u5ea6\u53d8\u5316\u3002", "result": "\u5728TAPVid-3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u9884\u6d4b\u7ed3\u679c\u5e73\u6ed1\u4e14\u51c6\u786e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4ece\u5408\u6210\u6570\u636e\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6df1\u5ea6\u9884\u6d4b\u3002"}}
{"id": "2504.13984", "pdf": "https://arxiv.org/pdf/2504.13984", "abs": "https://arxiv.org/abs/2504.13984", "authors": ["Amrit Diggavi Seshadri"], "title": "One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To reduce the time and computational costs of inference of large language\nmodels, there has been interest in parameter-efficient low-rank early-exit\ncasting of transformer hidden-representations to final-representations. Such\nlow-rank short-cutting has been shown to outperform identity shortcuts at early\nmodel stages while offering parameter-efficiency in shortcut jumps. However,\ncurrent low-rank methods maintain a separate early-exit shortcut jump to\nfinal-representations for each transformer intermediate block-level during\ninference. In this work, we propose selection of a single One-Jump-Fits-All\n(OJFA) low-rank shortcut that offers over a 30x reduction in shortcut parameter\ncosts during inference. We show that despite this extreme reduction, our OJFA\nchoice largely matches the performance of maintaining multiple shortcut jumps\nduring inference and offers stable precision from all transformer block-levels\nfor GPT2-XL, Phi3-Mini and Llama2-7B transformer models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOJFA\u7684\u4f4e\u79e9\u6377\u5f84\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u7684\u53c2\u6570\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5355\u4e00\u7684\u4f4e\u79e9\u6377\u5f84\uff08OJFA\uff09\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u591a\u6377\u5f84\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u53c2\u6570\u6210\u672c\u3002", "result": "OJFA\u65b9\u6cd5\u5728GPT2-XL\u3001Phi3-Mini\u548cLlama2-7B\u6a21\u578b\u4e0a\u8868\u73b0\u7a33\u5b9a\uff0c\u6027\u80fd\u63a5\u8fd1\u591a\u6377\u5f84\u65b9\u6cd5\u3002", "conclusion": "OJFA\u65b9\u6cd5\u5728\u53c2\u6570\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u591a\u79cdTransformer\u6a21\u578b\u3002"}}
{"id": "2504.14693", "pdf": "https://arxiv.org/pdf/2504.14693", "abs": "https://arxiv.org/abs/2504.14693", "authors": ["Enxin Song", "Wenhao Chai", "Weili Xu", "Jianwen Xie", "Yuxuan Liu", "Gaoang Wang"], "title": "Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/", "summary": "Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension.", "AI": {"tldr": "Video-MMLU\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u5b66\u79d1\u8bb2\u5ea7\u7406\u89e3\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u611f\u77e5\u4e0e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u5b66\u79d1\u8bb2\u5ea7\u7684\u7406\u89e3\u4efb\u52a1\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u800c\u73b0\u6709LMMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u8bc4\u4f3090\u591a\u4e2a\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff08\u53c2\u6570\u89c4\u6a21\u4ece0.5B\u523040B\uff09\uff0c\u5e76\u5206\u6790\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u591a\u5b66\u79d1\u8bb2\u5ea7\u7684\u8ba4\u77e5\u6311\u6218\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u611f\u77e5\u4e0e\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u63a8\u7406\u5728\u8bb2\u5ea7\u7406\u89e3\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.13989", "pdf": "https://arxiv.org/pdf/2504.13989", "abs": "https://arxiv.org/abs/2504.13989", "authors": ["Lucas Maisonnave", "Cyril Moineau", "Olivier Bichler", "Fabrice Rastello"], "title": "Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40\\% increase\nin accuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHadamard\u77e9\u9635\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u56e0\u6fc0\u6d3b\u503c\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e863\u6bd4\u7279\u91cf\u5316\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u53d7\u5230\u5176\u5e9e\u5927\u53c2\u6570\u89c4\u6a21\u7684\u9650\u5236\uff0c\u800c\u91cf\u5316\u662f\u51cf\u5c11\u5185\u5b58\u548c\u63a8\u7406\u65f6\u95f4\u7684\u5e38\u7528\u65b9\u6cd5\u3002\u7136\u800c\uff0cLLMs\u6fc0\u6d3b\u503c\u4e2d\u7684\u5f02\u5e38\u503c\u4f7f\u5f97\u4f4e\u6bd4\u7279\u91cf\u5316\u9762\u4e34\u6311\u6218\u3002", "method": "\u5229\u7528Hadamard\u77e9\u9635\u7684\u7406\u8bba\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9010\u6b65\u4e8c\u8fdb\u5236\u641c\u7d22\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6743\u91cd\u3001\u6fc0\u6d3b\u503c\u548cKV\u7f13\u5b58\u76843\u6bd4\u7279\u91cf\u5316\uff0c\u5e76\u901a\u8fc7Paley\u7b97\u6cd5\u652f\u6301\u975e2\u7684\u5e42\u6b21\u5d4c\u5165\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Mistral\u3001LLaMA\u548cQwen\u7b49\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8640%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u6210\u529f\u652f\u63013\u6bd4\u7279\u91cf\u5316\u3002", "conclusion": "Hadamard\u77e9\u9635\u5728\u51cf\u5c11\u5f02\u5e38\u503c\u65b9\u9762\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u8be5\u65b9\u6cd5\u4e3aLLMs\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.14699", "pdf": "https://arxiv.org/pdf/2504.14699", "abs": "https://arxiv.org/abs/2504.14699", "authors": ["Sascha Jecklin", "Aidana Massalimova", "Ruyi Zha", "Lilian Calvet", "Christoph J. Laux", "Mazda Farshad", "Philipp F\u00fcrnstahl"], "title": "IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed Real X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758fX\u5c04\u7ebf\u91cd\u5efa3D\u810a\u67f1\u89e3\u5256\u7ed3\u6784\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u7684\u6807\u51c6\u5316\u6b65\u9aa4\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u810a\u67f1\u624b\u672f\u9700\u8981\u9ad8\u7cbe\u5ea6\u76843D\u89e3\u5256\u91cd\u5efa\uff0c\u4f46\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u53ef\u80fd\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6269\u5c55\u4e86$R^2$-\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u5f15\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u7684\u653e\u5c04\u6807\u51c6\u5316\u6b65\u9aa4\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u63d0\u9ad8\u89c6\u56fe\u95f4\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u79bb\u4f53\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u4e863D\u91cd\u5efa\u7684\u4e34\u5e8a\u5b9e\u7528\u6027\uff0c\u6807\u51c6\u5316\u6b65\u9aa4\u663e\u8457\u63d0\u5347\u4e86\u89e3\u5256\u6e05\u6670\u5ea6\u3002\u5b9a\u91cf\u6307\u6807\uff08PSNR/SSIM\uff09\u663e\u793a\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u4ece\u7a00\u758fX\u5c04\u7ebf\u8fdb\u884c\u5b9e\u4f8b\u5316\u4f53\u79ef\u91cd\u5efa\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u624b\u672f\u5bfc\u822a\u4e2d\u76843D\u6210\u50cf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.14053", "pdf": "https://arxiv.org/pdf/2504.14053", "abs": "https://arxiv.org/abs/2504.14053", "authors": ["Ali Safari"], "title": "Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "This research examines whether Airbnb guests' positive and negative comments\ninfluence acceptance rates and rental prices across six U.S. regions: Rhode\nIsland, Broward County, Chicago, Dallas, San Diego, and Boston. Thousands of\nreviews were collected and analyzed using Natural Language Processing (NLP) to\nclassify sentiments as positive or negative, followed by statistical testing\n(t-tests and basic correlations) on the average scores. The findings reveal\nthat over 90 percent of reviews in each region are positive, indicating that\nhaving additional reviews does not significantly enhance prices. However,\nlistings with predominantly positive feedback exhibit slightly higher\nacceptance rates, suggesting that sentiment polarity, rather than the sheer\nvolume of reviews, is a more critical factor for host success. Additionally,\nbudget listings often gather extensive reviews while maintaining competitive\npricing, whereas premium listings sustain higher prices with fewer but highly\npositive reviews. These results underscore the importance of sentiment quality\nover quantity in shaping guest behavior and pricing strategies in an\noverwhelmingly positive review environment.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86Airbnb\u8bc4\u8bba\u7684\u60c5\u611f\u6781\u6027\u5bf9\u623f\u6e90\u63a5\u53d7\u7387\u548c\u4ef7\u683c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6b63\u9762\u8bc4\u8bba\u5360\u6bd4\u9ad8\uff0c\u4f46\u6570\u91cf\u5bf9\u4ef7\u683c\u5f71\u54cd\u4e0d\u5927\uff0c\u60c5\u611f\u6781\u6027\u66f4\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8Airbnb\u8bc4\u8bba\u7684\u60c5\u611f\u6781\u6027\uff08\u6b63\u9762/\u8d1f\u9762\uff09\u662f\u5426\u5f71\u54cd\u623f\u6e90\u7684\u63a5\u53d7\u7387\u548c\u4ef7\u683c\u3002", "method": "\u6536\u96c6\u6570\u5343\u6761\u8bc4\u8bba\uff0c\u4f7f\u7528NLP\u5206\u7c7b\u60c5\u611f\uff0c\u5e76\u901a\u8fc7t\u68c0\u9a8c\u548c\u76f8\u5173\u5206\u6790\u9a8c\u8bc1\u5f71\u54cd\u3002", "result": "90%\u4ee5\u4e0a\u8bc4\u8bba\u4e3a\u6b63\u9762\uff0c\u6570\u91cf\u5bf9\u4ef7\u683c\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u6b63\u9762\u8bc4\u8bba\u591a\u7684\u623f\u6e90\u63a5\u53d7\u7387\u7565\u9ad8\uff1b\u9884\u7b97\u623f\u6e90\u8bc4\u8bba\u591a\u4f46\u4ef7\u683c\u7ade\u4e89\u6fc0\u70c8\uff0c\u9ad8\u7aef\u623f\u6e90\u8bc4\u8bba\u5c11\u4f46\u4ef7\u683c\u9ad8\u3002", "conclusion": "\u8bc4\u8bba\u7684\u60c5\u611f\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u80fd\u5f71\u54cd\u5ba2\u6237\u884c\u4e3a\u548c\u5b9a\u4ef7\u7b56\u7565\u3002"}}
{"id": "2504.14708", "pdf": "https://arxiv.org/pdf/2504.14708", "abs": "https://arxiv.org/abs/2504.14708", "authors": ["Parshuram N. Aarotale", "Ajita Rattani"], "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eEMG\u7684\u624b\u52bf\u8bc6\u522b\u65b0\u65b9\u6cd5XMANet\uff0c\u901a\u8fc7\u8de8\u5c42\u4e92\u6ce8\u610f\u529b\u7ed3\u5408\u5c40\u90e8\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u63d0\u5347EMG\u624b\u52bf\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u5047\u80a2\u63a7\u5236\u3001\u5eb7\u590d\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528XMANet\u7ed3\u5408STFT\u548cWT\u751f\u6210\u7684\u8c31\u56fe\u548c\u5c3a\u5ea6\u56fe\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\u4e0a\uff0cXMANet\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6700\u9ad8\u63d0\u53479.36%\u3002", "conclusion": "XMANet\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7279\u5f81\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684EMG\u5206\u7c7b\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14107", "pdf": "https://arxiv.org/pdf/2504.14107", "abs": "https://arxiv.org/abs/2504.14107", "authors": ["Jennifer Hu", "Michael A. Lepori", "Michael Franke"], "title": "Linking forward-pass dynamics in Transformers and real-time human processing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern AI models are increasingly being used as theoretical tools to study\nhuman cognition. One dominant approach is to evaluate whether human-derived\nmeasures (such as offline judgments or real-time processing) are predicted by a\nmodel's output: that is, the end-product of forward pass(es) through the\nnetwork. At the same time, recent advances in mechanistic interpretability have\nbegun to reveal the internal processes that give rise to model outputs, raising\nthe question of whether models and humans might arrive at outputs using similar\n\"processing strategies\". Here, we investigate the link between real-time\nprocessing in humans and \"layer-time\" dynamics in Transformer models. Across\nfive studies spanning domains and modalities, we test whether the dynamics of\ncomputation in a single forward pass of pre-trained Transformers predict\nsignatures of processing in humans, above and beyond properties of the model's\noutput probability distribution. We consistently find that layer-time dynamics\nprovide additional predictive power on top of output measures. Our results\nsuggest that Transformer processing and human processing may be facilitated or\nimpeded by similar properties of an input stimulus, and this similarity has\nemerged through general-purpose objectives such as next-token prediction or\nimage recognition. Our work suggests a new way of using AI models to study\nhuman cognition: not just as a black box mapping stimuli to responses, but\npotentially also as explicit processing models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Transformer\u6a21\u578b\u7684\u5185\u90e8\u5904\u7406\u52a8\u6001\u662f\u5426\u4e0e\u4eba\u7c7b\u5b9e\u65f6\u5904\u7406\u76f8\u4f3c\uff0c\u53d1\u73b0\u5c42\u65f6\u95f4\u52a8\u6001\u80fd\u63d0\u4f9b\u989d\u5916\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u7814\u7a76AI\u6a21\u578b\uff08\u5982Transformer\uff09\u7684\u5185\u90e8\u5904\u7406\u52a8\u6001\u662f\u5426\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5904\u7406\u76f8\u4f3c\uff0c\u4ee5\u63a2\u7d22AI\u6a21\u578b\u4f5c\u4e3a\u4eba\u7c7b\u8ba4\u77e5\u7814\u7a76\u5de5\u5177\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u8de8\u9886\u57df\u548c\u6a21\u6001\u7684\u7814\u7a76\uff0c\u6d4b\u8bd5\u9884\u8bad\u7ec3Transformer\u7684\u5355\u6b21\u524d\u5411\u4f20\u9012\u7684\u8ba1\u7b97\u52a8\u6001\u662f\u5426\u80fd\u9884\u6d4b\u4eba\u7c7b\u5904\u7406\u7279\u5f81\u3002", "result": "\u5c42\u65f6\u95f4\u52a8\u6001\u5728\u6a21\u578b\u8f93\u51fa\u6982\u7387\u5206\u5e03\u7684\u57fa\u7840\u4e0a\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "Transformer\u548c\u4eba\u7c7b\u7684\u5904\u7406\u53ef\u80fd\u53d7\u8f93\u5165\u523a\u6fc0\u7684\u76f8\u4f3c\u5c5e\u6027\u5f71\u54cd\uff0c\u8868\u660eAI\u6a21\u578b\u53ef\u4f5c\u4e3a\u663e\u5f0f\u5904\u7406\u6a21\u578b\u7814\u7a76\u4eba\u7c7b\u8ba4\u77e5\u3002"}}
{"id": "2504.14709", "pdf": "https://arxiv.org/pdf/2504.14709", "abs": "https://arxiv.org/abs/2504.14709", "authors": ["Hui Zhou", "Shaoshuai Shi", "Hongsheng Li"], "title": "Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u95ed\u73af\u6a21\u62df\u5668\u548c\u56e0\u679c\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u9a8c\u8bc1\u5176\u662f\u5426\u771f\u6b63\u7406\u89e3\u9a7e\u9a76\u539f\u5219\uff0c\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\u5e38\u89c1\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5e76\u5f00\u53d1\u4e86\u95ed\u73af\u6a21\u62df\u5668\u548c\u56e0\u679c\u57fa\u51c6\u3002", "result": "\u65b0\u6846\u67b6\u65e8\u5728\u514b\u670d\u6a21\u4eff\u5b66\u4e60\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u5bf9\u7f55\u89c1\u6216\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\u3002"}}
{"id": "2504.14110", "pdf": "https://arxiv.org/pdf/2504.14110", "abs": "https://arxiv.org/abs/2504.14110", "authors": ["Theo Jaffrelot Inizan", "Sherry Yang", "Aaron Kaplan", "Yen-hsu Lin", "Jian Yin", "Saber Mirzaei", "Mona Abdelgaid", "Ali H. Alawadhi", "KwangHwan Cho", "Zhiling Zheng", "Ekin Dogus Cubuk", "Christian Borgs", "Jennifer T. Chayes", "Kristin A. Persson", "Omar M. Yaghi"], "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks", "categories": ["cond-mat.mtrl-sci", "cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Generative models and machine learning promise accelerated material discovery\nin MOFs for CO2 capture and water harvesting but face significant challenges\nnavigating vast chemical spaces while ensuring synthetizability. Here, we\npresent MOFGen, a system of Agentic AI comprising interconnected agents: a\nlarge language model that proposes novel MOF compositions, a diffusion model\nthat generates crystal structures, quantum mechanical agents that optimize and\nfilter candidates, and synthetic-feasibility agents guided by expert rules and\nmachine learning. Trained on all experimentally reported MOFs and computational\ndatabases, MOFGen generated hundreds of thousands of novel MOF structures and\nsynthesizable organic linkers. Our methodology was validated through\nhigh-throughput experiments and the successful synthesis of five \"AI-dreamt\"\nMOFs, representing a major step toward automated synthesizable material\ndiscovery.", "AI": {"tldr": "MOFGen\u5229\u7528\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\uff08\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u548c\u91cf\u5b50\u529b\u5b66\u4ee3\u7406\uff09\u751f\u6210\u65b0\u578bMOF\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5408\u6210\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5728\u6750\u6599\u53d1\u73b0\u4e2d\u9762\u4e34\u7684\u5316\u5b66\u7a7a\u95f4\u63a2\u7d22\u548c\u5408\u6210\u53ef\u884c\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u3001\u91cf\u5b50\u529b\u5b66\u4ee3\u7406\u548c\u5408\u6210\u53ef\u884c\u6027\u4ee3\u7406\uff0c\u751f\u6210\u5e76\u4f18\u5316MOF\u7ed3\u6784\u3002", "result": "\u751f\u6210\u6570\u5341\u4e07\u65b0\u578bMOF\u7ed3\u6784\uff0c\u6210\u529f\u5408\u6210\u4e94\u79cdAI\u8bbe\u8ba1\u7684MOF\u3002", "conclusion": "MOFGen\u4e3a\u81ea\u52a8\u5316\u5408\u6210\u6750\u6599\u53d1\u73b0\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.14715", "pdf": "https://arxiv.org/pdf/2504.14715", "abs": "https://arxiv.org/abs/2504.14715", "authors": ["Md. Sanaullah Chowdhury", "Salauddin Tapu", "Noyon Kumar Sarkar", "Ferdous Bin Ali", "Lameya Sabrin"], "title": "Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate and efficient medical image segmentation is crucial for advancing\nclinical diagnostics and surgical planning, yet remains a complex challenge due\nto the variability in anatomical structures and the demand for low-complexity\nmodels. In this paper, we introduced Med-2D SegNet, a novel and highly\nefficient segmentation architecture that delivers outstanding accuracy while\nmaintaining a minimal computational footprint. Med-2D SegNet achieves\nstate-of-the-art performance across multiple benchmark datasets, including\nKVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient\n(DSC) of 89.77% across 20 diverse datasets. Central to its success is the\ncompact Med Block, a specialized encoder design that incorporates dimension\nexpansion and parameter reduction, enabling precise feature extraction while\nkeeping model parameters to a low count of just 2.07 million. Med-2D SegNet\nexcels in cross-dataset generalization, particularly in polyp segmentation,\nwhere it was trained on KVASIR-SEG and showed strong performance on unseen\ndatasets, demonstrating its robustness in zero-shot learning scenarios, even\nthough we acknowledge that further improvements are possible. With top-tier\nperformance in both binary and multi-class segmentation, Med-2D SegNet\nredefines the balance between accuracy and efficiency, setting a new benchmark\nfor medical image analysis. This work paves the way for developing accessible,\nhigh-performance diagnostic tools suitable for clinical environments and\nresource-constrained settings, making it a step forward in the democratization\nof advanced medical technology.", "AI": {"tldr": "Med-2D SegNet\u662f\u4e00\u79cd\u9ad8\u6548\u533b\u5b66\u56fe\u50cf\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7\u7d27\u51d1\u7684Med Block\u8bbe\u8ba1\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6027\u548c\u6548\u7387\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faMed-2D SegNet\uff0c\u91c7\u7528Med Block\u8bbe\u8ba1\uff0c\u7ed3\u5408\u7ef4\u5ea6\u6269\u5c55\u548c\u53c2\u6570\u51cf\u5c11\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747DSC\u8fbe89.77%\uff0c\u53c2\u6570\u4ec52.07\u767e\u4e07\uff0c\u96f6\u6837\u672c\u5b66\u4e60\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Med-2D SegNet\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u533b\u7597\u6280\u672f\u666e\u53ca\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.14123", "pdf": "https://arxiv.org/pdf/2504.14123", "abs": "https://arxiv.org/abs/2504.14123", "authors": ["Mingyu Kim", "Jongwoo Ko", "Mijung Park"], "title": "Bayesian Principles Improve Prompt Learning In Vision-Language Models", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "AISTATS2025", "summary": "Prompt learning is a popular fine-tuning method for vision-language models\ndue to its efficiency. It requires a small number of additional learnable\nparameters while significantly enhancing performance on target tasks. However,\nmost existing methods suffer from overfitting to fine-tuning data, yielding\npoor generalizability. To address this, we propose a new training objective\nfunction based on a Bayesian learning principle to balance adaptability and\ngeneralizability. We derive a prior over the logits, where the mean function is\nparameterized by the pre-trained model, while the posterior corresponds to the\nfine-tuned model. This objective establishes a balance by allowing the\nfine-tuned model to adapt to downstream tasks while remaining close to the\npre-trained model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5b66\u4e60\u539f\u7406\u7684\u65b0\u8bad\u7ec3\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u63d0\u793a\u5b66\u4e60\u4e2d\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e73\u8861\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8c03\u6570\u636e\u4e0a\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u6cdb\u5316\u6027\u5dee\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u5b66\u4e60\u539f\u7406\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u5316\u4e3a\u5148\u9a8c\uff0c\u5fae\u8c03\u6a21\u578b\u5bf9\u5e94\u540e\u9a8c\u3002", "result": "\u65b0\u76ee\u6807\u51fd\u6570\u4f7f\u5fae\u8c03\u6a21\u578b\u65e2\u80fd\u9002\u5e94\u4e0b\u6e38\u4efb\u52a1\uff0c\u53c8\u4fdd\u6301\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u63a5\u8fd1\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u9002\u5e94\u6027\u548c\u6cdb\u5316\u6027\uff0c\u63d0\u5347\u4e86\u63d0\u793a\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14717", "pdf": "https://arxiv.org/pdf/2504.14717", "abs": "https://arxiv.org/abs/2504.14717", "authors": ["Bowei Zhang", "Lei Ke", "Adam W. Harley", "Katerina Fragkiadaki"], "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry", "categories": ["cs.CV", "cs.LG"], "comment": "Long-term feed-forward 3D point tracking in persistent 3D point maps.\n  Code:https://github.com/zbw001/TAPIP3D", "summary": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io", "AI": {"tldr": "TAPIP3D\u662f\u4e00\u79cd\u7528\u4e8e\u5355\u76eeRGB\u548cRGB-D\u89c6\u9891\u4e2d\u957f\u671f3D\u70b9\u8ddf\u8e2a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u89c6\u9891\u8868\u793a\u4e3a\u76f8\u673a\u7a33\u5b9a\u7684\u65f6\u7a7a\u7279\u5f81\u4e91\uff0c\u5e76\u5229\u7528\u6df1\u5ea6\u548c\u76f8\u673a\u8fd0\u52a8\u4fe1\u606f\u63d0\u53472D\u7279\u5f81\u52303D\u4e16\u754c\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5728\u957f\u671f\u8ddf\u8e2a\u548c\u76f8\u673a\u8fd0\u52a8\u8865\u507f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cTAPIP3D\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TAPIP3D\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u591a\u5e273D\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5e76\u5f15\u5165\u5c40\u90e8\u5bf9\u6ce8\u610f\u529b\u673a\u5236\u6765\u5229\u75283D\u7a7a\u95f4\u5173\u7cfb\uff0c\u5f62\u6210\u7cbe\u786e\u76843D\u8f68\u8ff9\u4f30\u8ba1\u3002", "result": "TAPIP3D\u57283D\u70b9\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u6709\u51c6\u786e\u6df1\u5ea6\u4fe1\u606f\u65f6\u63d0\u5347\u4e862D\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "TAPIP3D\u901a\u8fc7\u76f8\u673a\u8fd0\u52a8\u8865\u507f\u548c3D\u4e0a\u4e0b\u6587\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u51c6\u786e\u76843D\u70b9\u8ddf\u8e2a\u3002"}}
{"id": "2504.14126", "pdf": "https://arxiv.org/pdf/2504.14126", "abs": "https://arxiv.org/abs/2504.14126", "authors": ["Saad Hameed", "Basheer Qolomany", "Samir Brahim Belhaouari", "Mohamed Abdallah", "Junaid Qadir", "Ala Al-Fuqaha"], "title": "Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Determining the ideal architecture for deep learning models, such as the\nnumber of layers and neurons, is a difficult and resource-intensive process\nthat frequently relies on human tuning or computationally costly optimization\napproaches. While Particle Swarm Optimization (PSO) and Large Language Models\n(LLMs) have been individually applied in optimization and deep learning, their\ncombined use for enhancing convergence in numerical optimization tasks remains\nunderexplored. Our work addresses this gap by integrating LLMs into PSO to\nreduce model evaluations and improve convergence for deep learning\nhyperparameter tuning. The proposed LLM-enhanced PSO method addresses the\ndifficulties of efficiency and convergence by using LLMs (particularly\nChatGPT-3.5 and Llama3) to improve PSO performance, allowing for faster\nachievement of target objectives. Our method speeds up search space exploration\nby substituting underperforming particle placements with best suggestions\noffered by LLMs. Comprehensive experiments across three scenarios -- (1)\noptimizing the Rastrigin function, (2) using Long Short-Term Memory (LSTM)\nnetworks for time series regression, and (3) using Convolutional Neural\nNetworks (CNNs) for material classification -- show that the method\nsignificantly improves convergence rates and lowers computational costs.\nDepending on the application, computational complexity is lowered by 20% to 60%\ncompared to traditional PSO methods. Llama3 achieved a 20% to 40% reduction in\nmodel calls for regression tasks, whereas ChatGPT-3.5 reduced model calls by\n60% for both regression and classification tasks, all while preserving accuracy\nand error rates. This groundbreaking methodology offers a very efficient and\neffective solution for optimizing deep learning models, leading to substantial\ncomputational performance improvements across a wide range of applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6536\u655b\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u8c03\u4f18\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f18\u5316\u65b9\u6cd5\uff0c\u6548\u7387\u4f4e\u4e0b\u3002LLMs\u548cPSO\u7684\u7ed3\u5408\u5728\u8fd9\u4e00\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5c06LLMs\uff08\u5982ChatGPT-3.5\u548cLlama3\uff09\u96c6\u6210\u5230PSO\u4e2d\uff0c\u66ff\u4ee3\u8868\u73b0\u4e0d\u4f73\u7684\u7c92\u5b50\u4f4d\u7f6e\uff0c\u4ee5\u52a0\u901f\u641c\u7d22\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u573a\u666f\uff08Rastrigin\u51fd\u6570\u4f18\u5316\u3001LSTM\u65f6\u95f4\u5e8f\u5217\u56de\u5f52\u548cCNN\u6750\u6599\u5206\u7c7b\uff09\u4e2d\u663e\u8457\u63d0\u5347\u6536\u655b\u901f\u5ea6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e20%\u81f360%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14736", "pdf": "https://arxiv.org/pdf/2504.14736", "abs": "https://arxiv.org/abs/2504.14736", "authors": ["Nicol\u00e1s Gaggion", "Rodrigo Bonazzola", "Mar\u00eda Florencia Legascue", "Mar\u00eda Florencia Mammarella", "Florencia Sol Rodriguez", "Federico Emanuel Aballay", "Florencia Bel\u00e9n Catulo", "Andana Barrios", "Franco Accavallo", "Santiago Nahuel Villarreal", "Martin Crespi", "Martiniano Mar\u00eda Ricardi", "Ezequiel Petrillo", "Thomas Blein", "Federico Ariel", "Enzo Ferrante"], "title": "ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The analysis of plant developmental plasticity, including root system\narchitecture, is fundamental to understanding plant adaptability and\ndevelopment, particularly in the context of climate change and agricultural\nsustainability. While significant advances have been made in plant phenotyping\ntechnologies, comprehensive temporal analysis of root development remains\nchallenging, with most existing solutions providing either limited throughput\nor restricted structural analysis capabilities. Here, we present ChronoRoot\n2.0, an integrated open-source platform that combines affordable hardware with\nadvanced artificial intelligence to enable sophisticated temporal plant\nphenotyping. The system introduces several major advances, offering an integral\nperspective of seedling development: (i) simultaneous multi-organ tracking of\nsix distinct plant structures, (ii) quality control through real-time\nvalidation, (iii) comprehensive architectural measurements including novel\ngravitropic response parameters, and (iv) dual specialized user interfaces for\nboth architectural analysis and high-throughput screening. We demonstrate the\nsystem's capabilities through three use cases for Arabidopsis thaliana:\ncharacterization of circadian growth patterns under different light conditions,\ndetailed analysis of gravitropic responses in transgenic plants, and\nhigh-throughput screening of etiolation responses across multiple genotypes.\nChronoRoot 2.0 maintains its predecessor's advantages of low cost and\nmodularity while significantly expanding its capabilities, making sophisticated\ntemporal phenotyping more accessible to the broader plant science community.\nThe system's open-source nature, combined with extensive documentation and\ncontainerized deployment options, ensures reproducibility and enables\ncommunity-driven development of new analytical capabilities.", "AI": {"tldr": "ChronoRoot 2.0\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u7ed3\u5408\u4f4e\u6210\u672c\u786c\u4ef6\u548cAI\u6280\u672f\uff0c\u7528\u4e8e\u690d\u7269\u6839\u7cfb\u53d1\u80b2\u7684\u65f6\u5e8f\u8868\u578b\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u901a\u91cf\u4f4e\u548c\u7ed3\u6784\u5206\u6790\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u690d\u7269\u53d1\u80b2\u53ef\u5851\u6027\uff08\u5982\u6839\u7cfb\u7ed3\u6784\uff09\u5bf9\u7406\u89e3\u690d\u7269\u9002\u5e94\u6027\u548c\u519c\u4e1a\u53ef\u6301\u7eed\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8868\u578b\u6280\u672f\u96be\u4ee5\u6ee1\u8db3\u65f6\u5e8f\u5206\u6790\u9700\u6c42\u3002", "method": "ChronoRoot 2.0\u6574\u5408\u4e86\u591a\u5668\u5b98\u8ffd\u8e2a\u3001\u5b9e\u65f6\u8d28\u91cf\u63a7\u5236\u3001\u5168\u9762\u7ed3\u6784\u6d4b\u91cf\u548c\u4e13\u7528\u7528\u6237\u754c\u9762\uff0c\u652f\u6301\u9ad8\u901a\u91cf\u7b5b\u9009\u548c\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u901a\u8fc7\u62df\u5357\u82a5\u7684\u4e09\u4e2a\u6848\u4f8b\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u529f\u80fd\uff0c\u5305\u62ec\u663c\u591c\u751f\u957f\u6a21\u5f0f\u3001\u8f6c\u57fa\u56e0\u690d\u7269\u7684\u5411\u91cd\u529b\u53cd\u5e94\u53ca\u591a\u57fa\u56e0\u578b\u7684\u9ec4\u5316\u53cd\u5e94\u7b5b\u9009\u3002", "conclusion": "ChronoRoot 2.0\u4ee5\u4f4e\u6210\u672c\u3001\u6a21\u5757\u5316\u548c\u5f00\u6e90\u7279\u6027\uff0c\u4e3a\u690d\u7269\u79d1\u5b66\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u6613\u7528\u7684\u65f6\u5e8f\u8868\u578b\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2504.14128", "pdf": "https://arxiv.org/pdf/2504.14128", "abs": "https://arxiv.org/abs/2504.14128", "authors": ["Christopher Zhang Cui", "Xingdi Yuan", "Zhang Xiao", "Prithviraj Ammanabrolu", "Marc-Alexandre C\u00f4t\u00e9"], "title": "TALES: Text Adventure Learning Environment Suite", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning is an essential skill to enable Large Language Models (LLMs) to\ninteract with the world. As tasks become more complex, they demand increasingly\nsophisticated and diverse reasoning capabilities for sequential\ndecision-making, requiring structured reasoning over the context history to\ndetermine the next best action. We introduce TALES, a diverse collection of\nsynthetic and human-written text-adventure games designed to challenge and\nevaluate diverse reasoning capabilities. We present results over a range of\nLLMs, open- and closed-weights, performing a qualitative analysis on the top\nperforming models. Despite an impressive showing on synthetic games, even the\ntop LLM-driven agents fail to achieve 15% on games designed for human\nenjoyment. Code and visualization of the experiments can be found at\nhttps://microsoft.github.io/tales.", "AI": {"tldr": "TALES\u662f\u4e00\u4e2a\u591a\u6837\u5316\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\u96c6\u5408\uff0c\u65e8\u5728\u6311\u6218\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u5c3d\u7ba1\u5728\u5408\u6210\u6e38\u620f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46LLMs\u5728\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\u6765\u652f\u6301\u987a\u5e8f\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30LLMs\u7684\u591a\u6837\u5316\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528TALES\uff08\u5408\u6210\u548c\u4eba\u7c7b\u7f16\u5199\u7684\u6587\u672c\u5192\u9669\u6e38\u620f\uff09\u5bf9\u591a\u79cdLLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u5b9a\u6027\u5206\u6790\u3002", "result": "\u5c3d\u7ba1\u5728\u5408\u6210\u6e38\u620f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46LLMs\u5728\u4eba\u7c7b\u8bbe\u8ba1\u7684\u6e38\u620f\u4e2d\u6210\u529f\u7387\u4f4e\u4e8e15%\u3002", "conclusion": "TALES\u4e3a\u8bc4\u4f30LLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u4f46LLMs\u5728\u590d\u6742\u4eba\u7c7b\u4efb\u52a1\u4e2d\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2504.14737", "pdf": "https://arxiv.org/pdf/2504.14737", "abs": "https://arxiv.org/abs/2504.14737", "authors": ["Shuang Zeng", "Lei Zhu", "Xinliang Zhang", "Hangzhou He", "Yanye Lu"], "title": "SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSuperCL\u7684\u65b0\u578b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u5229\u7528\u56fe\u50cf\u7684\u7ed3\u6784\u5148\u9a8c\u548c\u50cf\u7d20\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u591a\u5173\u6ce8\u5b9e\u4f8b\u7ea7\u6216\u50cf\u7d20\u7ea7\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u5185\u76f8\u4f3c\u50cf\u7d20\u7ec4\u7684\u7279\u6027\uff0c\u4e14\u5bf9\u6bd4\u5bf9\u751f\u6210\u4f9d\u8d56\u4eba\u5de5\u9608\u503c\u8bbe\u7f6e\uff0c\u6548\u7387\u4f4e\u4e14\u6cdb\u5316\u6027\u5dee\u3002", "method": "SuperCL\u5f15\u5165\u4e24\u79cd\u5bf9\u6bd4\u5bf9\u751f\u6210\u7b56\u7565\uff1a\u56fe\u50cf\u5185\u5c40\u90e8\u5bf9\u6bd4\u5bf9\uff08ILCP\uff09\u548c\u56fe\u50cf\u95f4\u5168\u5c40\u5bf9\u6bd4\u5bf9\uff08IGCP\uff09\uff0c\u5229\u7528\u8d85\u50cf\u7d20\u56fe\u751f\u6210\u4f2a\u63a9\u7801\u6307\u5bfc\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u63d0\u51faASP\u548cCCL\u6a21\u5757\u8fdb\u4e00\u6b65\u5229\u7528\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u57288\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSuperCL\u4f18\u4e8e12\u79cd\u73b0\u6709\u65b9\u6cd5\uff0cDSC\u6307\u6807\u5206\u522b\u63d0\u53473.15%\u30015.44%\u30017.89%\uff0c\u4e14\u53ef\u89c6\u5316\u7ed3\u679c\u66f4\u7cbe\u786e\u3002", "conclusion": "SuperCL\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6bd4\u5bf9\u751f\u6210\u7b56\u7565\u548c\u7ed3\u6784\u4fe1\u606f\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.14147", "pdf": "https://arxiv.org/pdf/2504.14147", "abs": "https://arxiv.org/abs/2504.14147", "authors": ["Jiakai Tang", "Jingsen Zhang", "Zihang Tian", "Xueyang Feng", "Lei Wang", "Xu Chen"], "title": "HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u4f18\u5316\u7684\u53ef\u89e3\u91ca\u63a8\u8350\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u53cd\u9988\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u63d0\u5347\u89e3\u91ca\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u63a8\u8350\u65b9\u6cd5\u5728\u7a00\u758f\u4ea4\u4e92\u6570\u636e\u4e0b\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u4fe1\u53f7\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u91c7\u7528\u52a8\u6001\u4ea4\u4e92\u4f18\u5316\u673a\u5236\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u53cd\u9988\uff0c\u5f15\u5165\u5b9a\u5236\u5316\u5956\u52b1\u8bc4\u5206\u548c\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u63d0\u5347\u89e3\u91ca\u6027\u80fd\u7684\u540c\u65f6\uff0c\u964d\u4f4e\u4e86\u4eba\u529b\u6210\u672c\uff0c\u6ee1\u8db3\u4e86\u591a\u6837\u5316\u9700\u6c42\u3002"}}
{"id": "2504.14753", "pdf": "https://arxiv.org/pdf/2504.14753", "abs": "https://arxiv.org/abs/2504.14753", "authors": ["Guodong Shen", "Yuqi Ouyang", "Junru Lu", "Yixuan Yang", "Victor Sanchez"], "title": "Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for Enhanced Single- and Multi-Task Approaches", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by IEEE Transactions on Image Processing (TIP)", "summary": "Despite the prevailing transition from single-task to multi-task approaches\nin video anomaly detection, we observe that many adopt sub-optimal frameworks\nfor individual proxy tasks. Motivated by this, we contend that optimizing\nsingle-task frameworks can advance both single- and multi-task approaches.\nAccordingly, we leverage middle-frame prediction as the primary proxy task, and\nintroduce an effective hybrid framework designed to generate accurate\npredictions for normal frames and flawed predictions for abnormal frames. This\nhybrid framework is built upon a bi-directional structure that seamlessly\nintegrates both vision transformers and ConvLSTMs. Specifically, we utilize\nthis bi-directional structure to fully analyze the temporal dimension by\npredicting frames in both forward and backward directions, significantly\nboosting the detection stability. Given the transformer's capacity to model\nlong-range contextual dependencies, we develop a convolutional temporal\ntransformer that efficiently associates feature maps from all context frames to\ngenerate attention-based predictions for target frames. Furthermore, we devise\na layer-interactive ConvLSTM bridge that facilitates the smooth flow of\nlow-level features across layers and time-steps, thereby strengthening\npredictions with fine details. Anomalies are eventually identified by\nscrutinizing the discrepancies between target frames and their corresponding\npredictions. Several experiments conducted on public benchmarks affirm the\nefficacy of our hybrid framework, whether used as a standalone single-task\napproach or integrated as a branch in a multi-task approach. These experiments\nalso underscore the advantages of merging vision transformers and ConvLSTMs for\nvideo anomaly detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u53d8\u6362\u5668\u548cConvLSTM\uff0c\u901a\u8fc7\u53cc\u5411\u7ed3\u6784\u4f18\u5316\u5355\u4efb\u52a1\u6846\u67b6\uff0c\u63d0\u5347\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u6846\u67b6\u4e2d\u91c7\u7528\u6b21\u4f18\u7684\u5355\u4efb\u52a1\u6846\u67b6\uff0c\u4f18\u5316\u5355\u4efb\u52a1\u6846\u67b6\u53ef\u540c\u65f6\u63d0\u5347\u5355\u4efb\u52a1\u548c\u591a\u4efb\u52a1\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e2d\u95f4\u5e27\u9884\u6d4b\u4f5c\u4e3a\u4ee3\u7406\u4efb\u52a1\uff0c\u8bbe\u8ba1\u53cc\u5411\u7ed3\u6784\u6574\u5408\u89c6\u89c9\u53d8\u6362\u5668\u548cConvLSTM\uff0c\u901a\u8fc7\u5377\u79ef\u65f6\u95f4\u53d8\u6362\u5668\u548c\u5c42\u4ea4\u4e92ConvLSTM\u6865\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u6df7\u5408\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u65e0\u8bba\u662f\u5355\u4efb\u52a1\u8fd8\u662f\u591a\u4efb\u52a1\u5206\u652f\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u53d8\u6362\u5668\u548cConvLSTM\u7684\u6df7\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.14177", "pdf": "https://arxiv.org/pdf/2504.14177", "abs": "https://arxiv.org/abs/2504.14177", "authors": ["Li He", "He Zhao", "Stephen Wan", "Dadong Wang", "Lina Yao", "Tongliang Liu"], "title": "Direct Advantage Regression: Aligning LLMs with Online AI Reward", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Online AI Feedback (OAIF) presents a promising alternative to Reinforcement\nLearning from Human Feedback (RLHF) by utilizing online AI preference in\naligning language models (LLMs). However, the straightforward replacement of\nhumans with AI deprives LLMs from learning more fine-grained AI supervision\nbeyond binary signals. In this paper, we propose Direct Advantage Regression\n(DAR), a simple alignment algorithm using online AI reward to optimize policy\nimprovement through weighted supervised fine-tuning. As an RL-free approach,\nDAR maintains theoretical consistency with online RLHF pipelines while\nsignificantly reducing implementation complexity and improving learning\nefficiency. Our empirical results underscore that AI reward is a better form of\nAI supervision consistently achieving higher human-AI agreement as opposed to\nAI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show\nthat DAR outperforms both OAIF and online RLHF baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAR\u7684\u7b80\u5355\u5bf9\u9f50\u7b97\u6cd5\uff0c\u5229\u7528\u5728\u7ebfAI\u5956\u52b1\u4f18\u5316\u7b56\u7565\u6539\u8fdb\uff0c\u907f\u514d\u4e86RL\u7684\u590d\u6742\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eOAIF\u548c\u5728\u7ebfRLHF\u57fa\u7ebf\u3002", "motivation": "\u5728\u7ebfAI\u53cd\u9988\uff08OAIF\uff09\u5728\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4e2d\u66ff\u4ee3\u4eba\u7c7b\u53cd\u9988\uff08RLHF\uff09\u65f6\uff0c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7DAR\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faDirect Advantage Regression\uff08DAR\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebfAI\u5956\u52b1\u7684\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u65e0\u9700\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDAR\u5728\u4eba\u7c7b-AI\u4e00\u81f4\u6027\u548c\u6027\u80fd\u4e0a\u4f18\u4e8eOAIF\u548c\u5728\u7ebfRLHF\uff0cGPT-4-Turbo\u548cMT-bench\u8bc4\u4f30\u7ed3\u679c\u652f\u6301\u8fd9\u4e00\u7ed3\u8bba\u3002", "conclusion": "DAR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u4e00\u81f4\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u5b9e\u73b0\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2504.14783", "pdf": "https://arxiv.org/pdf/2504.14783", "abs": "https://arxiv.org/abs/2504.14783", "authors": ["Wenhui Zhu", "Peijie Qiu", "Xiwen Chen", "Zhangsihao Yang", "Aristeidis Sotiras", "Abolfazl Razi", "Yalin Wang"], "title": "How Effective Can Dropout Be in Multiple Instance Learning ?", "categories": ["cs.CV", "cs.AI", "eess.IV", "stat.ML"], "comment": null, "summary": "Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIL-Dropout\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e22\u5f03\u5305\u4e2d\u6700\u91cd\u8981\u5b9e\u4f8b\u6765\u63d0\u5347\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "MIL\u5728\u7ec4\u7ec7\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u7c7b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u56e0\u7279\u5f81\u5d4c\u5165\u566a\u58f0\u548c\u5f31\u76d1\u7763\u95ee\u9898\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faMIL-Dropout\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u4e22\u5f03\u5305\u4e2d\u6700\u91cd\u8981\u7684\u5b9e\u4f8b\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u4e94\u4e2aMIL\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2aWSI\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MIL-Dropout\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e14\u8ba1\u7b97\u6210\u672c\u53ef\u5ffd\u7565\u3002", "conclusion": "MIL-Dropout\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347MIL\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.14183", "pdf": "https://arxiv.org/pdf/2504.14183", "abs": "https://arxiv.org/abs/2504.14183", "authors": ["Natalia Tomashenko", "Xiaoxiao Miao", "Emmanuel Vincent", "Junichi Yamagishi"], "title": "The First VoicePrivacy Attacker Challenge", "categories": ["eess.AS", "cs.CL", "cs.CR"], "comment": "Published in: ICASSP 2025 - 2025 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)", "summary": "The First VoicePrivacy Attacker Challenge is an ICASSP 2025 SP Grand\nChallenge which focuses on evaluating attacker systems against a set of voice\nanonymization systems submitted to the VoicePrivacy 2024 Challenge. Training,\ndevelopment, and evaluation datasets were provided along with a baseline\nattacker. Participants developed their attacker systems in the form of\nautomatic speaker verification systems and submitted their scores on the\ndevelopment and evaluation data. The best attacker systems reduced the equal\nerror rate (EER) by 25-44% relative w.r.t. the baseline.", "AI": {"tldr": "ICASSP 2025 SP Grand Challenge\u8bc4\u4f30\u4e86\u9488\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u653b\u51fb\u8005\u7cfb\u7edf\uff0c\u6700\u4f73\u653b\u51fb\u7cfb\u7edf\u5c06EER\u964d\u4f4e\u4e8625-44%\u3002", "motivation": "\u8bc4\u4f30\u653b\u51fb\u8005\u7cfb\u7edf\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u63a8\u52a8\u8bed\u97f3\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u4f9b\u8bad\u7ec3\u3001\u5f00\u53d1\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u53ca\u57fa\u7ebf\u653b\u51fb\u8005\uff0c\u53c2\u4e0e\u8005\u5f00\u53d1\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u5e76\u63d0\u4ea4\u5206\u6570\u3002", "result": "\u6700\u4f73\u653b\u51fb\u7cfb\u7edf\u5c06EER\u76f8\u5bf9\u57fa\u7ebf\u964d\u4f4e\u4e8625-44%\u3002", "conclusion": "\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u653b\u51fb\u8005\u7cfb\u7edf\u5bf9\u8bed\u97f3\u533f\u540d\u5316\u7cfb\u7edf\u7684\u663e\u8457\u6539\u8fdb\u6f5c\u529b\u3002"}}
{"id": "2504.14785", "pdf": "https://arxiv.org/pdf/2504.14785", "abs": "https://arxiv.org/abs/2504.14785", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Pei Wang"], "title": "When Cloud Removal Meets Diffusion Model in Remote Sensing", "categories": ["cs.CV"], "comment": null, "summary": "Cloud occlusion significantly hinders remote sensing applications by\nobstructing surface information and complicating analysis. To address this, we\npropose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal\ndiffusion-based framework for cloud removal in remote sensing imagery. Our\nmethod introduces prompt-driven control, allowing selective removal of thin and\nthick clouds without relying on pre-generated cloud masks, thereby enhancing\npreprocessing efficiency and model adaptability. Additionally, we integrate\nlow-rank adaptation for computational efficiency, subject-driven generation for\nimproved generalization, and grouped learning to enhance performance on small\ndatasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into\nexisting cloud removal models, providing a scalable and robust solution.\nExtensive experiments on the RICE and CUHK-CR datasets demonstrate\nstate-of-the-art performance, achieving superior cloud removal across diverse\nconditions. This work presents a practical and efficient approach for remote\nsensing image processing with broad real-world applications.", "AI": {"tldr": "DC4CR\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563\u7684\u65b0\u578b\u4e91\u53bb\u9664\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u9a71\u52a8\u63a7\u5236\u548c\u4f4e\u79e9\u9002\u914d\u7b49\u6280\u672f\uff0c\u9ad8\u6548\u53bb\u9664\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u4e91\u5c42\uff0c\u65e0\u9700\u9884\u751f\u6210\u4e91\u63a9\u819c\u3002", "motivation": "\u4e91\u906e\u6321\u4e25\u91cd\u963b\u788d\u9065\u611f\u5e94\u7528\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u751f\u6210\u4e91\u63a9\u819c\u4e14\u6548\u7387\u4f4e\uff0cDC4CR\u65e8\u5728\u63d0\u4f9b\u66f4\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDC4CR\u6846\u67b6\uff0c\u7ed3\u5408\u63d0\u793a\u9a71\u52a8\u63a7\u5236\u3001\u4f4e\u79e9\u9002\u914d\u3001\u4e3b\u9898\u9a71\u52a8\u751f\u6210\u548c\u5206\u7ec4\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e91\u53bb\u9664\u3002", "result": "\u5728RICE\u548cCUHK-CR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u6761\u4ef6\u4e0b\u7684\u5148\u8fdb\u4e91\u53bb\u9664\u6548\u679c\u3002", "conclusion": "DC4CR\u4e3a\u9065\u611f\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.14191", "pdf": "https://arxiv.org/pdf/2504.14191", "abs": "https://arxiv.org/abs/2504.14191", "authors": ["Yansheng Qiu", "Haoquan Zhang", "Zhaopan Xu", "Ming Li", "Diping Song", "Zheng Wang", "Kaipeng Zhang"], "title": "AI Idea Bench 2025: AI Research Idea Generation Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large-scale Language Models (LLMs) have revolutionized human-AI interaction\nand achieved significant success in the generation of novel ideas. However,\ncurrent assessments of idea generation overlook crucial factors such as\nknowledge leakage in LLMs, the absence of open-ended benchmarks with grounded\ntruth, and the limited scope of feasibility analysis constrained by prompt\ndesign. These limitations hinder the potential of uncovering groundbreaking\nresearch ideas. In this paper, we present AI Idea Bench 2025, a framework\ndesigned to quantitatively evaluate and compare the ideas generated by LLMs\nwithin the domain of AI research from diverse perspectives. The framework\ncomprises a comprehensive dataset of 3,495 AI papers and their associated\ninspired works, along with a robust evaluation methodology. This evaluation\nsystem gauges idea quality in two dimensions: alignment with the ground-truth\ncontent of the original papers and judgment based on general reference\nmaterial. AI Idea Bench 2025's benchmarking system stands to be an invaluable\nresource for assessing and comparing idea-generation techniques, thereby\nfacilitating the automation of scientific discovery.", "AI": {"tldr": "AI Idea Bench 2025\u662f\u4e00\u4e2a\u8bc4\u4f30LLMs\u751f\u6210AI\u7814\u7a76\u60f3\u6cd5\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5ffd\u89c6\u4e86\u77e5\u8bc6\u6cc4\u6f0f\u3001\u7f3a\u4e4f\u5f00\u653e\u57fa\u51c6\u548c\u53ef\u884c\u6027\u5206\u6790\u53d7\u9650\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7a81\u7834\u6027\u7814\u7a76\u60f3\u6cd5\u7684\u53d1\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b3,495\u7bc7AI\u8bba\u6587\u53ca\u5176\u76f8\u5173\u5de5\u4f5c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u539f\u59cb\u8bba\u6587\u5185\u5bb9\u548c\u901a\u7528\u53c2\u8003\u6750\u6599\u7684\u53cc\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "AI Idea Bench 2025\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u6bd4\u8f83\u548c\u8bc4\u4f30LLMs\u751f\u6210\u7684\u60f3\u6cd5\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u5b66\u53d1\u73b0\u7684\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8AI\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.14807", "pdf": "https://arxiv.org/pdf/2504.14807", "abs": "https://arxiv.org/abs/2504.14807", "authors": ["Deepak Ghimire", "Sunghwan Jeong", "Sunhong Yoon", "Sanghyun Park", "Juhwan Choi"], "title": "Real-Time Sleepiness Detection for Driver State Monitoring System", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "8 pages, published in GST 2015", "summary": "A driver face monitoring system can detect driver fatigue, which is a\nsignificant factor in many accidents, using computer vision techniques. In this\npaper, we present a real-time technique for driver eye state detection. First,\nthe face is detected, and the eyes are located within the face region for\ntracking. A normalized cross-correlation-based online dynamic template matching\ntechnique, combined with Kalman filter tracking, is proposed to track the\ndetected eye positions in subsequent image frames. A support vector machine\nwith histogram of oriented gradients (HOG) features is used to classify the\nstate of the eyes as open or closed. If the eyes remain closed for a specified\nperiod, the driver is considered to be asleep, and an alarm is triggered.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u9a7e\u9a76\u5458\u773c\u775b\u72b6\u6001\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u6a21\u677f\u5339\u914d\u548cKalman\u6ee4\u6ce2\u8ddf\u8e2a\uff0c\u4f7f\u7528SVM\u5206\u7c7b\u5668\u5224\u65ad\u773c\u775b\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u75b2\u52b3\u65f6\u89e6\u53d1\u8b66\u62a5\u3002", "motivation": "\u9a7e\u9a76\u5458\u75b2\u52b3\u662f\u8bb8\u591a\u4e8b\u6545\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u773c\u775b\u72b6\u6001\u53ef\u4ee5\u6709\u6548\u9884\u9632\u75b2\u52b3\u9a7e\u9a76\u3002", "method": "\u4f7f\u7528\u52a8\u6001\u6a21\u677f\u5339\u914d\u548cKalman\u6ee4\u6ce2\u8ddf\u8e2a\u773c\u775b\u4f4d\u7f6e\uff0c\u7ed3\u5408HOG\u7279\u5f81\u548cSVM\u5206\u7c7b\u5668\u5224\u65ad\u773c\u775b\u5f00\u95ed\u72b6\u6001\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u9a7e\u9a76\u5458\u773c\u775b\u72b6\u6001\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u75b2\u52b3\u65f6\u89e6\u53d1\u8b66\u62a5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u9a7e\u9a76\u5458\u75b2\u52b3\u76d1\u6d4b\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u75b2\u52b3\u9a7e\u9a76\u5f15\u53d1\u7684\u4e8b\u6545\u3002"}}
{"id": "2504.14232", "pdf": "https://arxiv.org/pdf/2504.14232", "abs": "https://arxiv.org/abs/2504.14232", "authors": ["Antoun Yaacoub", "J\u00e9r\u00f4me Da-Rugna", "Zainab Assaghir"], "title": "Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment", "categories": ["cs.AI", "cs.CL"], "comment": "This paper was presented in the 17th Int. Conf. on Computer Science\n  and Information Technology (ICCSIT 2024), Dubai, United Arab Emirates, 2024,\n  Oct. 23-25. IT's now in production to be published in the International\n  Journal of Computer Theory and Engineering", "summary": "This study evaluates the integration of Bloom's Taxonomy into OneClickQuiz,\nan Artificial Intelligence (AI) driven plugin for automating Multiple-Choice\nQuestion (MCQ) generation in Moodle. Bloom's Taxonomy provides a structured\nframework for categorizing educational objectives into hierarchical cognitive\nlevels. Our research investigates whether incorporating this taxonomy can\nimprove the alignment of AI-generated questions with specific cognitive\nobjectives. We developed a dataset of 3691 questions categorized according to\nBloom's levels and employed various classification models-Multinomial Logistic\nRegression, Naive Bayes, Linear Support Vector Classification (SVC), and a\nTransformer-based model (DistilBERT)-to evaluate their effectiveness in\ncategorizing questions. Our results indicate that higher Bloom's levels\ngenerally correlate with increased question length, Flesch-Kincaid Grade Level\n(FKGL), and Lexical Density (LD), reflecting the increased complexity of higher\ncognitive demands. Multinomial Logistic Regression showed varying accuracy\nacross Bloom's levels, performing best for \"Knowledge\" and less accurately for\nhigher-order levels. Merging higher-level categories improved accuracy for\ncomplex cognitive tasks. Naive Bayes and Linear SVC also demonstrated effective\nclassification for lower levels but struggled with higher-order tasks.\nDistilBERT achieved the highest performance, significantly improving\nclassification of both lower and higher-order cognitive levels, achieving an\noverall validation accuracy of 91%. This study highlights the potential of\nintegrating Bloom's Taxonomy into AI-driven assessment tools and underscores\nthe advantages of advanced models like DistilBERT for enhancing educational\ncontent generation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5c06\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u6574\u5408\u5230OneClickQuiz\uff08\u4e00\u4e2aAI\u9a71\u52a8\u7684Moodle\u63d2\u4ef6\uff09\u4e2d\uff0c\u4ee5\u6539\u8fdbAI\u751f\u6210\u7684\u591a\u9009\u9898\u4e0e\u8ba4\u77e5\u76ee\u6807\u7684\u5339\u914d\u3002\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u7ea7\u8ba4\u77e5\u6c34\u5e73\u7684\u95ee\u9898\u66f4\u590d\u6742\uff0cDistilBERT\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u8ba8\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u662f\u5426\u80fd\u63d0\u5347AI\u751f\u6210\u95ee\u9898\u7684\u8ba4\u77e5\u76ee\u6807\u5339\u914d\u6027\uff0c\u4ee5\u4f18\u5316\u6559\u80b2\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u75283691\u4e2a\u6309\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u5206\u7ea7\u7684\u95ee\u9898\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u591a\u79cd\u5206\u7c7b\u6a21\u578b\uff08\u5982\u903b\u8f91\u56de\u5f52\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u3001\u7ebf\u6027SVC\u548cDistilBERT\uff09\u3002", "result": "\u9ad8\u7ea7\u8ba4\u77e5\u6c34\u5e73\u95ee\u9898\u66f4\u957f\u4e14\u66f4\u590d\u6742\uff0cDistilBERT\u8868\u73b0\u6700\u4f73\uff08\u9a8c\u8bc1\u51c6\u786e\u738791%\uff09\u3002", "conclusion": "\u5e03\u9c81\u59c6\u5206\u7c7b\u6cd5\u6574\u5408\u5230AI\u5de5\u5177\u4e2d\u5177\u6709\u6f5c\u529b\uff0cDistilBERT\u80fd\u663e\u8457\u63d0\u5347\u6559\u80b2\u5185\u5bb9\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.14825", "pdf": "https://arxiv.org/pdf/2504.14825", "abs": "https://arxiv.org/abs/2504.14825", "authors": ["Zhoujie Qian"], "title": "ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance.", "AI": {"tldr": "ECViT\u662f\u4e00\u79cd\u7ed3\u5408CNN\u548cTransformer\u4f18\u52bf\u7684\u9ad8\u6548\u6df7\u5408\u67b6\u6784\uff0c\u89e3\u51b3\u4e86ViTs\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u636e\u9700\u6c42\u5927\u7684\u95ee\u9898\u3002", "motivation": "Vision Transformers\uff08ViTs\uff09\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u636e\u9700\u6c42\u5927\u7684\u6311\u6218\u3002", "method": "ECViT\u901a\u8fc7\u5f15\u5165CNN\u7684\u5f52\u7eb3\u504f\u7f6e\uff08\u5982\u5c40\u90e8\u6027\u548c\u5e73\u79fb\u4e0d\u53d8\u6027\uff09\uff0c\u7ed3\u5408\u5c40\u90e8\u6ce8\u610f\u529b\u548c\u91d1\u5b57\u5854\u7ed3\u6784\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECViT\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\u3002", "conclusion": "ECViT\u4e3a\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14239", "pdf": "https://arxiv.org/pdf/2504.14239", "abs": "https://arxiv.org/abs/2504.14239", "authors": ["Yuhang Liu", "Pengxiang Li", "Congkai Xie", "Xavier Hu", "Xiaotian Han", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages, 3 figures, work in progress", "summary": "Multimodal Large Language Models (MLLMs) have powered Graphical User\nInterface (GUI) Agents, showing promise in automating tasks on computing\ndevices. Recent works have begun exploring reasoning in GUI tasks with\nencouraging results. However, many current approaches rely on manually designed\nreasoning templates, which may result in reasoning that is not sufficiently\nrobust and adaptive for complex GUI environments. Meanwhile, some existing\nagents continue to operate as Reactive Actors, relying primarily on implicit\nreasoning that may lack sufficient depth for GUI tasks demanding planning and\nerror recovery. We argue that advancing these agents requires a shift from\nreactive acting towards acting based on deliberate reasoning. To facilitate\nthis transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed\nthrough our Actor2Reasoner framework, a reasoning-centric, two-stage training\napproach designed to progressively evolve agents from Reactive Actors to\nDeliberative Reasoners. The first stage, Reasoning Injection, focuses on\nestablishing a basic reasoner. We employ Spatial Reasoning Distillation to\ntransfer cross-modal spatial reasoning capabilities from teacher models to\nMLLMs through trajectories with explicit reasoning steps, enabling models to\nintegrate GUI visual-spatial information with logical reasoning before action\ngeneration. The second stage, Deliberation Enhancement, refines the basic\nreasoner into a deliberative one using Reinforcement Learning. This stage\nintroduces two approaches: Sub-goal Guidance, which rewards models for\ngenerating accurate intermediate sub-goals, and Error Recovery Scenario\nConstruction, which creates failure-and-recovery training scenarios from\nidentified prone-to-error steps. Experimental results show InfiGUI-R1 achieves\nstrong performance in GUI grounding and trajectory tasks. Resources at\nhttps://github.com/Reallm-Labs/InfiGUI-R1.", "AI": {"tldr": "InfiGUI-R1\u662f\u4e00\u4e2a\u57fa\u4e8eMLLM\u7684GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6Actor2Reasoner\uff0c\u4ece\u53cd\u5e94\u5f0f\u6267\u884c\u8005\u9010\u6b65\u6f14\u5316\u4e3a\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u8005\uff0c\u63d0\u5347\u4e86GUI\u4efb\u52a1\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u63a8\u7406\u6a21\u677f\u6216\u9690\u5f0f\u63a8\u7406\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742GUI\u73af\u5883\u7684\u9c81\u68d2\u6027\u548c\u6df1\u5ea6\u89c4\u5212\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1. \u63a8\u7406\u6ce8\u5165\uff08Spatial Reasoning Distillation\uff09\uff1b2. \u6df1\u601d\u719f\u8651\u589e\u5f3a\uff08\u5f3a\u5316\u5b66\u4e60\uff0c\u5305\u62ec\u5b50\u76ee\u6807\u6307\u5bfc\u548c\u9519\u8bef\u6062\u590d\u573a\u666f\u6784\u5efa\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660eInfiGUI-R1\u5728GUI\u57fa\u7840\u548c\u8f68\u8ff9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7Actor2Reasoner\u6846\u67b6\uff0cGUI\u4ee3\u7406\u4ece\u53cd\u5e94\u5f0f\u6267\u884c\u8005\u6210\u529f\u8f6c\u578b\u4e3a\u6df1\u601d\u719f\u8651\u7684\u63a8\u7406\u8005\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2504.14826", "pdf": "https://arxiv.org/pdf/2504.14826", "abs": "https://arxiv.org/abs/2504.14826", "authors": ["Zhuoran Zheng", "Xin Su", "Chen Wu", "Xiuyi Jia"], "title": "Distribution-aware Dataset Distillation for Efficient Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "With the exponential increase in image data, training an image restoration\nmodel is laborious. Dataset distillation is a potential solution to this\nproblem, yet current distillation techniques are a blank canvas in the field of\nimage restoration. To fill this gap, we propose the Distribution-aware Dataset\nDistillation method (TripleD), a new framework that extends the principles of\ndataset distillation to image restoration. Specifically, TripleD uses a\npre-trained vision Transformer to extract features from images for complexity\nevaluation, and the subset (the number of samples is much smaller than the\noriginal training set) is selected based on complexity. The selected subset is\nthen fed through a lightweight CNN that fine-tunes the image distribution to\nalign with the distribution of the original dataset at the feature level. To\nefficiently condense knowledge, the training is divided into two stages. Early\nstages focus on simpler, low-complexity samples to build foundational\nknowledge, while later stages select more complex and uncertain samples as the\nmodel matures. Our method achieves promising performance on multiple image\nrestoration tasks, including multi-task image restoration, all-in-one image\nrestoration, and ultra-high-definition image restoration tasks. Note that we\ncan train a state-of-the-art image restoration model on an\nultra-high-definition (4K resolution) dataset using only one consumer-grade GPU\nin less than 8 hours (500 savings in computing resources and immeasurable\ntraining time).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTripleD\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3ViT\u8bc4\u4f30\u56fe\u50cf\u590d\u6742\u5ea6\u5e76\u9009\u62e9\u5b50\u96c6\uff0c\u7ed3\u5408\u8f7b\u91cfCNN\u8c03\u6574\u7279\u5f81\u5206\u5e03\uff0c\u5206\u9636\u6bb5\u8bad\u7ec3\u4ee5\u9ad8\u6548\u5b9e\u73b0\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6570\u636e\u6fc0\u589e\u5bfc\u81f4\u8bad\u7ec3\u56fe\u50cf\u6062\u590d\u6a21\u578b\u8017\u65f6\u7684\u95ee\u9898\uff0c\u586b\u8865\u5f53\u524d\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u5728\u56fe\u50cf\u6062\u590d\u9886\u57df\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3ViT\u63d0\u53d6\u7279\u5f81\u8bc4\u4f30\u590d\u6742\u5ea6\uff0c\u9009\u62e9\u5b50\u96c6\u5e76\u901a\u8fc7\u8f7b\u91cfCNN\u8c03\u6574\u7279\u5f81\u5206\u5e03\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u5148\u7b80\u5355\u6837\u672c\u540e\u590d\u6742\u6837\u672c\uff09\u3002", "result": "\u5728\u591a\u9879\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u7528\u6d88\u8d39\u7ea7GPU\u57288\u5c0f\u65f6\u5185\u5b8c\u62104K\u5206\u8fa8\u7387\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u8282\u7701\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "TripleD\u4e3a\u56fe\u50cf\u6062\u590d\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2504.14847", "pdf": "https://arxiv.org/pdf/2504.14847", "abs": "https://arxiv.org/abs/2504.14847", "authors": ["Xixi Wan", "Aihua Zheng", "Zi Wang", "Bo Jiang", "Jin Tang", "Jixin Ma"], "title": "Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal data provides abundant and diverse object information, crucial\nfor effective modal interactions in Re-Identification (ReID) tasks. However,\nexisting approaches often overlook the quality variations in local features and\nfail to fully leverage the complementary information across modalities,\nparticularly in the case of low-quality features. In this paper, we propose to\naddress this issue by leveraging a novel graph reasoning model, termed the\nModality-aware Graph Reasoning Network (MGRNet). Specifically, we first\nconstruct modality-aware graphs to enhance the extraction of fine-grained local\ndetails by effectively capturing and modeling the relationships between\npatches. Subsequently, the selective graph nodes swap operation is employed to\nalleviate the adverse effects of low-quality local features by considering both\nlocal and global information, enhancing the representation of discriminative\ninformation. Finally, the swapped modality-aware graphs are fed into the\nlocal-aware graph reasoning module, which propagates multi-modal information to\nyield a reliable feature representation. Another advantage of the proposed\ngraph reasoning approach is its ability to reconstruct missing modal\ninformation by exploiting inherent structural relationships, thereby minimizing\ndisparities between different modalities. Experimental results on four\nbenchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the\nproposed method achieves state-of-the-art performance in multi-modal object\nReID. The code for our method will be available upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMGRNet\u7684\u56fe\u63a8\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001ReID\u4efb\u52a1\u4e2d\u5c40\u90e8\u7279\u5f81\u8d28\u91cf\u4e0d\u5747\u548c\u8de8\u6a21\u6001\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u5c40\u90e8\u7279\u5f81\u8d28\u91cf\u5dee\u5f02\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u8de8\u6a21\u6001\u4e92\u8865\u4fe1\u606f\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d28\u91cf\u7279\u5f81\u60c5\u51b5\u4e0b\u3002", "method": "\u6784\u5efa\u6a21\u6001\u611f\u77e5\u56fe\u4ee5\u589e\u5f3a\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7ec6\u8282\u63d0\u53d6\uff0c\u91c7\u7528\u9009\u62e9\u6027\u56fe\u8282\u70b9\u4ea4\u6362\u64cd\u4f5c\u4f18\u5316\u4f4e\u8d28\u91cf\u7279\u5f81\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8\u611f\u77e5\u56fe\u63a8\u7406\u6a21\u5757\u4f20\u64ad\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MGRNet\u901a\u8fc7\u56fe\u63a8\u7406\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001ReID\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u91cd\u5efa\u7f3a\u5931\u6a21\u6001\u4fe1\u606f\u3002"}}
{"id": "2504.14848", "pdf": "https://arxiv.org/pdf/2504.14848", "abs": "https://arxiv.org/abs/2504.14848", "authors": ["Yunpu Zhao", "Rui Zhang", "Junbin Xiao", "Ruibo Hou", "Jiaming Guo", "Zihao Zhang", "Yifan Hao", "Yunji Chen"], "title": "Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u6270\u52a8\uff08CSP\uff09\u6821\u51c6\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7f6e\u4fe1\u5ea6\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u54cd\u5e94\u6b63\u786e\u6027\u7684\u5bf9\u9f50\u3002", "motivation": "VLMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0d\u4f73\uff0c\u5bfc\u81f4\u7528\u6237\u4fe1\u4efb\u5ea6\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u9519\u8bef\u6216\u865a\u6784\u4fe1\u606f\u65f6\u4ecd\u8868\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u9ad8\u65af\u566a\u58f0\u6270\u52a8\u5173\u952e\u5bf9\u8c61\u533a\u57df\u6a21\u62df\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff0c\u5efa\u7acb\u89c6\u89c9\u6a21\u7cca\u5ea6\u4e0e\u7f6e\u4fe1\u5ea6\u7684\u6620\u5c04\uff0c\u5e76\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u6270\u52a8\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347VLMs\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.14860", "pdf": "https://arxiv.org/pdf/2504.14860", "abs": "https://arxiv.org/abs/2504.14860", "authors": ["Ziyi Liu", "Yangcen Liu"], "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition", "summary": "Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method.", "AI": {"tldr": "PseudoFormer\u662f\u4e00\u4e2a\u4e24\u5206\u652f\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u548c\u5229\u7528\u4e0d\u540c\u5148\u9a8c\u77e5\u8bc6\uff0c\u7f29\u5c0f\u4e86\u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5f31\u76d1\u7763\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\uff08WTAL\uff09\u56e0\u7f3a\u4e4f\u65f6\u95f4\u6807\u6ce8\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f2a\u6807\u7b7e\u8d28\u91cf\u3001\u5148\u9a8c\u5229\u7528\u548c\u566a\u58f0\u6807\u7b7e\u8bad\u7ec3\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faRickerFusion\u6620\u5c04\u52a8\u4f5c\u63d0\u8bae\u5230\u5171\u4eab\u7a7a\u95f4\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u7247\u6bb5\u7ea7\u548c\u63d0\u8bae\u7ea7\u6807\u7b7e\u8bad\u7ec3\u56de\u5f52\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u63a9\u7801\u548c\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u3002", "result": "\u5728THUMOS14\u548cActivityNet1.3\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8d21\u732e\u3002", "conclusion": "PseudoFormer\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86WTAL\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.14361", "pdf": "https://arxiv.org/pdf/2504.14361", "abs": "https://arxiv.org/abs/2504.14361", "authors": ["Till Rossner", "Ziteng Li", "Jonas Balke", "Nikoo Salehfard", "Tom Seifert", "Ming Tang"], "title": "Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "8 pages, 6 figures", "summary": "In this study, we propose an innovative methodology for predicting Cancer\nDrug Response (CDR) through the integration of the scGPT foundation model\nwithin the DeepCDR model. Our approach utilizes scGPT to generate embeddings\nfrom gene expression data, which are then used as gene expression input data\nfor DeepCDR. The experimental findings demonstrate the efficacy of this\nscGPT-based method in outperforming previous related works, including the\noriginal DeepCDR model and the scFoundation-based model. This study highlights\nthe potential of scGPT embeddings to enhance the accuracy of CDR predictions\nand offers a promising alternative to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408scGPT\u548cDeepCDR\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u764c\u75c7\u836f\u7269\u53cd\u5e94\uff08CDR\uff09\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CDR\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u5229\u7528scGPT\u751f\u6210\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u7684\u5d4c\u5165\uff0c\u4f5c\u4e3aDeepCDR\u7684\u8f93\u5165\u3002", "result": "scGPT\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u539fDeepCDR\u6a21\u578b\u53ca\u5176\u4ed6\u76f8\u5173\u65b9\u6cd5\u3002", "conclusion": "scGPT\u5d4c\u5165\u80fd\u663e\u8457\u63d0\u9ad8CDR\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.14868", "pdf": "https://arxiv.org/pdf/2504.14868", "abs": "https://arxiv.org/abs/2504.14868", "authors": ["Jianhui Wang", "Yangfan He", "Yan Zhong", "Xinyuan Song", "Jiayi Su", "Yuheng Feng", "Hongyang He", "Wenyu Zhu", "Xinhang Yuan", "Kuan Lu", "Menghao Huo", "Miao Zhang", "Keqin Li", "Jiaqi Chen", "Tianyu Shi", "Xueqian Wang"], "title": "Twin Co-Adaptive Dialogue for Progressive Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Modern text-to-image generation systems have enabled the creation of\nremarkably realistic and high-quality visuals, yet they often falter when\nhandling the inherent ambiguities in user prompts. In this work, we present\nTwin-Co, a framework that leverages synchronized, co-adaptive dialogue to\nprogressively refine image generation. Instead of a static generation process,\nTwin-Co employs a dynamic, iterative workflow where an intelligent dialogue\nagent continuously interacts with the user. Initially, a base image is\ngenerated from the user's prompt. Then, through a series of synchronized\ndialogue exchanges, the system adapts and optimizes the image according to\nevolving user feedback. The co-adaptive process allows the system to\nprogressively narrow down ambiguities and better align with user intent.\nExperiments demonstrate that Twin-Co not only enhances user experience by\nreducing trial-and-error iterations but also improves the quality of the\ngenerated images, streamlining the creative process across various\napplications.", "AI": {"tldr": "Twin-Co\u662f\u4e00\u4e2a\u901a\u8fc7\u52a8\u6001\u5bf9\u8bdd\u9010\u6b65\u4f18\u5316\u56fe\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u51cf\u5c11\u7528\u6237\u8bd5\u9519\u5e76\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u5728\u5904\u7406\u7528\u6237\u63d0\u793a\u6a21\u7cca\u6027\u65f6\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u540c\u6b65\u3001\u534f\u540c\u9002\u5e94\u7684\u5bf9\u8bdd\u673a\u5236\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u56fe\u50cf\u751f\u6210\u3002", "result": "\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u751f\u6210\u56fe\u50cf\u8d28\u91cf\uff0c\u51cf\u5c11\u8bd5\u9519\u3002", "conclusion": "Twin-Co\u901a\u8fc7\u52a8\u6001\u5bf9\u8bdd\u4f18\u5316\u56fe\u50cf\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u6548\u679c\u3002"}}
{"id": "2504.14363", "pdf": "https://arxiv.org/pdf/2504.14363", "abs": "https://arxiv.org/abs/2504.14363", "authors": ["Shihan Dou", "Muling Wu", "Jingwen Xu", "Rui Zheng", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "Improving RL Exploration for LLM Reasoning through Retrospective Replay", "categories": ["cs.LG", "cs.CL"], "comment": "13 pages, 3 figures", "summary": "Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRRL\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u653e\u673a\u5236\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5728LLM\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\uff0c\u65e9\u671f\u63a2\u7d22\u9636\u6bb5\u7684\u6709\u4ef7\u503c\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u88ab\u6291\u5236\uff0c\u5bfc\u81f4\u540e\u671f\u8bad\u7ec3\u4e2d\u6a21\u578b\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002", "method": "\u63d0\u51faRetrospective Replay-based Reinforcement Learning (RRL)\u7b97\u6cd5\uff0c\u5f15\u5165\u52a8\u6001\u91cd\u653e\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u91cd\u65b0\u8bbf\u95ee\u65e9\u671f\u6709\u6f5c\u529b\u7684\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRRL\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u548c\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "RRL\u4e0d\u4ec5\u4f18\u5316\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8fd8\u63d0\u5347\u4e86RLHF\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.14875", "pdf": "https://arxiv.org/pdf/2504.14875", "abs": "https://arxiv.org/abs/2504.14875", "authors": ["Chris Dongjoo Kim", "Jihwan Moon", "Sangwoo Moon", "Heeseung Yun", "Sihaeng Lee", "Aniruddha Kembhavi", "Soonyoung Lee", "Gunhee Kim", "Sangho Lee", "Christopher Clark"], "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "CVPR 2025 (main conference)", "summary": "The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.", "AI": {"tldr": "ReSpec\u662f\u4e00\u79cd\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u7279\u5f02\u6027\u7684\u5728\u7ebf\u8fc7\u6ee4\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u7b5b\u9009\u89c6\u9891-\u6587\u672c\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891-\u6587\u672c\u6570\u636e\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6311\u6218\uff0c\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u63d0\u51faReSpec\u6846\u67b6\uff0c\u57fa\u4e8e\u56db\u4e2a\u6807\u51c6\uff08\u6a21\u6001\u5bf9\u9f50\u3001\u4efb\u52a1\u76f8\u5173\u6027\u3001\u7279\u5f02\u6027\u548c\u6548\u7387\uff09\u5b9e\u65f6\u7b5b\u9009\u6570\u636e\u3002", "result": "\u5728WebVid2M\u548cVideoCC3M\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u4f7f\u75285%\u6570\u636e\u5373\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u89c6\u9891\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "ReSpec\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u7b5b\u9009\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2504.14370", "pdf": "https://arxiv.org/pdf/2504.14370", "abs": "https://arxiv.org/abs/2504.14370", "authors": ["Jon Kleinberg", "Fan Wei"], "title": "Density Measures for Language Generation", "categories": ["math.CO", "cs.CL", "cs.DM", "cs.LG"], "comment": null, "summary": "The recent successes of large language models (LLMs) have led to a surge of\ntheoretical research into language generation. A recent line of work proposes\nan abstract view, called language generation in the limit, where generation is\nseen as a game between an adversary and an algorithm: the adversary generates\nstrings from an unknown language $K$, chosen from a countable collection of\ncandidate languages, and after seeing a finite set of these strings, the\nalgorithm must generate new strings from $K$ that it has not seen before. This\nformalism highlights a key tension: the trade-off between validity (the\nalgorithm should only produce strings from the language) and breadth (it should\nbe able to produce many strings from the language). This trade-off is central\nin applied language generation as well, where it appears as a balance between\nhallucination (generating invalid utterances) and mode collapse (generating\nonly a restricted set of outputs). Despite its importance, this trade-off has\nbeen challenging to study quantitatively. We develop ways to quantify this\ntrade-off by formalizing breadth using measures of density. Existing algorithms\nfor language generation in the limit produce output sets that can have zero\ndensity in the true language, and this important failure of breadth might seem\nunavoidable. We show, however, that such a failure is not necessary: we provide\nan algorithm for language generation in the limit whose outputs have strictly\npositive density in $K$. We also study the internal representations built by\nthese algorithms, specifically the sequence of hypothesized candidate languages\nthey consider, and show that achieving the strongest form of breadth may\nrequire oscillating indefinitely between high- and low-density representations.\nOur analysis introduces a novel topology on language families, with notions of\nconvergence and limit points playing a key role.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u7684\u8bed\u8a00\u751f\u6210\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u7b97\u6cd5\u5728\u751f\u6210\u65b0\u5b57\u7b26\u4e32\u65f6\u7684\u6709\u6548\u6027\u4e0e\u5e7f\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u901a\u8fc7\u5bc6\u5ea6\u5ea6\u91cf\u91cf\u5316\u4e86\u8fd9\u79cd\u6743\u8861\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u7406\u8bba\u95ee\u9898\uff0c\u7279\u522b\u662f\u7b97\u6cd5\u5982\u4f55\u5728\u751f\u6210\u65b0\u5b57\u7b26\u4e32\u65f6\u5e73\u8861\u6709\u6548\u6027\u4e0e\u5e7f\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u62bd\u8c61\u7684\u8bed\u8a00\u751f\u6210\u6846\u67b6\uff0c\u5c06\u751f\u6210\u89c6\u4e3a\u5bf9\u624b\u4e0e\u7b97\u6cd5\u4e4b\u95f4\u7684\u535a\u5f08\uff0c\u5e76\u5f15\u5165\u5bc6\u5ea6\u5ea6\u91cf\u6765\u91cf\u5316\u5e7f\u5ea6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u5176\u8f93\u51fa\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u5177\u6709\u4e25\u683c\u6b63\u5bc6\u5ea6\uff0c\u5e76\u7814\u7a76\u4e86\u7b97\u6cd5\u5185\u90e8\u8868\u793a\u7684\u7279\u6027\u3002", "conclusion": "\u901a\u8fc7\u65b0\u7684\u62d3\u6251\u7ed3\u6784\u5206\u6790\u8bed\u8a00\u5bb6\u65cf\uff0c\u63ed\u793a\u4e86\u5b9e\u73b0\u6700\u4f73\u5e7f\u5ea6\u53ef\u80fd\u9700\u8981\u5728\u9ad8\u5bc6\u5ea6\u548c\u4f4e\u5bc6\u5ea6\u8868\u793a\u4e4b\u95f4\u65e0\u9650\u632f\u8361\u3002"}}
{"id": "2504.14877", "pdf": "https://arxiv.org/pdf/2504.14877", "abs": "https://arxiv.org/abs/2504.14877", "authors": ["Aihua Zheng", "Yongqi Sun", "Zi Wang", "Chenglong Li", "Jin Tang"], "title": "Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle Re-identification", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-spectral vehicle Re-identification (ReID) is\nsignificantly degraded when some important discriminative cues in visible, near\ninfrared and thermal infrared spectra are lost. Existing methods generate or\nenhance missing details in low-quality spectra data using the high-quality one,\ngenerally called the primary spectrum, but how to justify the primary spectrum\nis a challenging problem. In addition, when the quality of the primary spectrum\nis low, the enhancement effect would be greatly degraded, thus limiting the\nperformance of multi-spectral vehicle ReID. To address these problems, we\npropose the Collaborative Enhancement Network (CoEN), which generates a\nhigh-quality proxy from all spectra data and leverages it to supervise the\nselection of primary spectrum and enhance all spectra features in a\ncollaborative manner, for robust multi-spectral vehicle ReID. First, to\nintegrate the rich cues from all spectra data, we design the Proxy Generator\n(PG) to progressively aggregate multi-spectral features. Second, we design the\nDynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring\ntheir correlations with the proxy, to accurately select the primary spectra\nwith the highest correlation. Finally, we design the Collaborative Enhancement\nModule (CEM) to effectively compensate for missing contents of all spectra by\ncollaborating the primary spectra and the proxy, thereby mitigating the impact\nof low-quality primary spectra. Extensive experiments on three benchmark\ndatasets are conducted to validate the efficacy of the proposed approach\nagainst other multi-spectral vehicle ReID methods. The codes will be released\nat https://github.com/yongqisun/CoEN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u589e\u5f3a\u7f51\u7edc\uff08CoEN\uff09\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7406\u5e76\u52a8\u6001\u9009\u62e9\u4e3b\u5149\u8c31\uff0c\u4ee5\u63d0\u5347\u591a\u5149\u8c31\u8f66\u8f86\u91cd\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u5149\u8c31\u589e\u5f3a\u4f4e\u8d28\u91cf\u5149\u8c31\u6570\u636e\uff0c\u4f46\u4e3b\u5149\u8c31\u9009\u62e9\u56f0\u96be\u4e14\u4f4e\u8d28\u91cf\u4e3b\u5149\u8c31\u4f1a\u964d\u4f4e\u589e\u5f3a\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4ee3\u7406\u751f\u6210\u5668\uff08PG\uff09\u3001\u52a8\u6001\u8d28\u91cf\u6392\u5e8f\u6a21\u5757\uff08DQSM\uff09\u548c\u534f\u4f5c\u589e\u5f3a\u6a21\u5757\uff08CEM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u751f\u6210\u4ee3\u7406\u3001\u52a8\u6001\u9009\u62e9\u4e3b\u5149\u8c31\u548c\u534f\u4f5c\u589e\u5f3a\u5149\u8c31\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CoEN\u4f18\u4e8e\u5176\u4ed6\u591a\u5149\u8c31\u8f66\u8f86\u91cd\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "CoEN\u901a\u8fc7\u534f\u4f5c\u589e\u5f3a\u548c\u52a8\u6001\u9009\u62e9\u4e3b\u5149\u8c31\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u5149\u8c31\u8f66\u8f86\u91cd\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.14439", "pdf": "https://arxiv.org/pdf/2504.14439", "abs": "https://arxiv.org/abs/2504.14439", "authors": ["Avinandan Bose", "Zhihan Xiong", "Yuejie Chi", "Simon Shaolei Du", "Lin Xiao", "Maryam Fazel"], "title": "LoRe: Personalizing LLMs via Low-Rank Reward Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u504f\u597d\u5efa\u6a21\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u504f\u597d\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u7684\u4ef7\u503c\u8868\u793a\uff0c\u96be\u4ee5\u9002\u5e94\u4e2a\u4f53\u504f\u597d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u8868\u793a\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5c06\u4e2a\u4f53\u504f\u597d\u5efa\u6a21\u4e3a\u5171\u4eab\u57fa\u51fd\u6570\u7684\u52a0\u6743\u7ec4\u5408\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "\u5728\u591a\u4e2a\u504f\u597d\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u793a\u51fa\u5bf9\u672a\u89c1\u7528\u6237\u7684\u4f18\u8d8a\u6cdb\u5316\u80fd\u529b\u548c\u504f\u597d\u9884\u6d4b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u907f\u514d\u4e86\u50f5\u5316\u7684\u7528\u6237\u5206\u7c7b\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u9002\u5e94\u3002"}}
{"id": "2504.14884", "pdf": "https://arxiv.org/pdf/2504.14884", "abs": "https://arxiv.org/abs/2504.14884", "authors": ["Jingyu Xing", "Chenwei Tang", "Tao Wang", "Rong Xiao", "Wei Ju", "Ji-Zhe Zhou", "Liangli Zhen", "Jiancheng Lv"], "title": "Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in unsupervised anomaly detection (UAD) have shifted from\nsingle-class to multi-class scenarios. In such complex contexts, the increasing\npattern diversity has brought two challenges to reconstruction-based\napproaches: (1) over-generalization: anomalies that are subtle or share\ncompositional similarities with normal patterns may be reconstructed with high\nfidelity, making them difficult to distinguish from normal instances; and (2)\ninsufficient normality reconstruction: complex normal features, such as\nintricate textures or fine-grained structures, may not be faithfully\nreconstructed due to the model's limited representational capacity, resulting\nin false positives. Existing methods typically focus on addressing the former,\nwhich unintentionally exacerbate the latter, resulting in inadequate\nrepresentation of intricate normal patterns. To concurrently address these two\nchallenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This\nnetwork includes two critical components: a Dual-Decoder Reverse Distillation\nNetwork (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the\nDRD-Net incorporates a restoration decoder designed to recover normal features\nfrom synthetic abnormal inputs and an identity decoder to reconstruct features\nthat maintain the anomalous semantics. By exploiting the discrepancy between\nfeatures produced by two decoders, our approach refines anomaly scores beyond\nthe conventional encoder-decoder comparison paradigm, effectively reducing\nfalse positives and enhancing localization accuracy. Furthermore, the CMM\nexplicitly encodes and preserves class-specific normal prototypes, actively\nsteering the network away from anomaly reconstruction. Comprehensive\nexperimental results across several benchmarks demonstrate the superior\nperformance of our MDD-Net framework over current SoTA approaches in\nmulti-class UAD tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMDD-Net\u7684\u8bb0\u5fc6\u589e\u5f3a\u53cc\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8fc7\u6cdb\u5316\u548c\u6b63\u5e38\u7279\u5f81\u91cd\u5efa\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\uff0c\u91cd\u5efa\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8fc7\u6cdb\u5316\u5bfc\u81f4\u5f02\u5e38\u96be\u4ee5\u533a\u5206\uff0c\u4ee5\u53ca\u6b63\u5e38\u7279\u5f81\u91cd\u5efa\u4e0d\u8db3\u5bfc\u81f4\u8bef\u62a5\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u89e3\u51b3\u524d\u8005\uff0c\u53cd\u800c\u52a0\u5267\u540e\u8005\u3002", "method": "MDD-Net\u5305\u542b\u53cc\u89e3\u7801\u5668\u53cd\u5411\u84b8\u998f\u7f51\u7edc\uff08DRD-Net\uff09\u548c\u7c7b\u611f\u77e5\u8bb0\u5fc6\u6a21\u5757\uff08CMM\uff09\u3002DRD-Net\u901a\u8fc7\u4e24\u4e2a\u89e3\u7801\u5668\u7684\u7279\u5f81\u5dee\u5f02\u4f18\u5316\u5f02\u5e38\u8bc4\u5206\uff0cCMM\u5219\u4fdd\u5b58\u7c7b\u7279\u5b9a\u6b63\u5e38\u539f\u578b\u4ee5\u907f\u514d\u5f02\u5e38\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMDD-Net\u6846\u67b6\u8868\u73b0\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "MDD-Net\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u548c\u8bb0\u5fc6\u6a21\u5757\u7684\u534f\u540c\u4f5c\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.14520", "pdf": "https://arxiv.org/pdf/2504.14520", "abs": "https://arxiv.org/abs/2504.14520", "authors": ["Ahsan Bilal", "Muhammad Ahmed Mohsin", "Muhammad Umer", "Muhammad Awais Khan Bangash", "Muhammad Ali Jamshed"], "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to IEEE Transactions on Artificial Intelligence", "summary": "This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.", "AI": {"tldr": "\u8be5\u8c03\u67e5\u4ece\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u89d2\u5ea6\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5143\u601d\u7ef4\u80fd\u529b\u53d1\u5c55\uff0c\u63d0\u51fa\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u67b6\u6784\u589e\u5f3aLLM\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5b58\u5728\u5e7b\u89c9\u548c\u7f3a\u4e4f\u81ea\u6211\u8bc4\u4f30\u673a\u5236\u7b49\u5c40\u9650\u6027\uff0c\u9700\u901a\u8fc7\u5143\u601d\u7ef4\u63d0\u5347\u5176\u590d\u6742\u6216\u9ad8\u98ce\u9669\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5206\u6790\u4e86RLHF\u3001\u81ea\u84b8\u998f\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b49\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff08\u5982\u76d1\u7763\u8005-\u4ee3\u7406\u5c42\u6b21\u3001\u4ee3\u7406\u8fa9\u8bba\u548c\u5fc3\u7406\u7406\u8bba\u6846\u67b6\uff09\u5982\u4f55\u6a21\u62df\u4eba\u7c7b\u5185\u7701\u884c\u4e3a\u3002", "result": "\u901a\u8fc7MARL\u7684\u5956\u52b1\u673a\u5236\u3001\u81ea\u6211\u5bf9\u5f08\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u5185\u7701\u3001\u81ea\u9002\u5e94\u4e14\u53ef\u4fe1\u7684LLM\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u8bc4\u4f30\u6307\u6807\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u67b6\u6784\u548c\u6df7\u5408\u7b26\u53f7\u63a8\u7406\uff09\uff0c\u4e3aLLM\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.14888", "pdf": "https://arxiv.org/pdf/2504.14888", "abs": "https://arxiv.org/abs/2504.14888", "authors": ["Xinran Xu", "Yuliang Ma", "Sifu Cai"], "title": "WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal Vessel Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel retinal vessel segmentation network, the Weighted\nMulti-Kernel Attention Network (WMKA-Net), which aims to address the issues of\ninsufficient multiscale feature capture, loss of contextual information, and\nnoise sensitivity in retinal vessel segmentation. WMKA-Net significantly\nimproves the segmentation performance of small vessels and low-contrast regions\nby integrating several innovative components, including the MultiKernelFeature\nFusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),\nand the Attention Mechanism Module (AttentionBlock). The MKDC module employs\nmultiscale parallel convolutional kernels to extract vessel characteristics,\nthereby enhancing the ability to capture complex vascular structures. The UDFF\nstrategy optimizes the transmission of feature information by weighted fusion\nof high- and low-level features. The AttentionBlock highlights key regions and\nsuppresses noise interference through the attention mechanism. Experimental\nresults demonstrate that WMKA-Net achieves excellent segmentation performance\nin multiple public datasets, particularly in segmentation of small vessels and\nprocessing of pathological regions. This work provides a robust and efficient\nnew method for segmentation of the retinal vessel.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u7f51\u7edcWMKA-Net\uff0c\u901a\u8fc7\u591a\u6838\u7279\u5f81\u878d\u5408\u3001\u6e10\u8fdb\u7279\u5f81\u52a0\u6743\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u8840\u7ba1\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4e2d\u591a\u5c3a\u5ea6\u7279\u5f81\u6355\u6349\u4e0d\u8db3\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\u548c\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u6838\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08MKDC\uff09\u3001\u6e10\u8fdb\u7279\u5f81\u52a0\u6743\u878d\u5408\u7b56\u7565\uff08UDFF\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\u6a21\u5757\uff08AttentionBlock\uff09\uff0c\u5206\u522b\u7528\u4e8e\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u7279\u5f81\u4fe1\u606f\u4f18\u5316\u548c\u5173\u952e\u533a\u57df\u589e\u5f3a\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u5c0f\u8840\u7ba1\u5206\u5272\u548c\u75c5\u7406\u533a\u57df\u5904\u7406\u4e0a\u6548\u679c\u663e\u8457\u3002", "conclusion": "WMKA-Net\u4e3a\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.14899", "pdf": "https://arxiv.org/pdf/2504.14899", "abs": "https://arxiv.org/abs/2504.14899", "authors": ["Chenjie Cao", "Jingkai Zhou", "Shikai Li", "Jingyun Liang", "Chaohui Yu", "Fan Wang", "Xiangyang Xue", "Yanwei Fu"], "title": "Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://github.com/ewrfcas/Uni3C", "summary": "Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.", "AI": {"tldr": "Uni3C\u662f\u4e00\u4e2a\u7edf\u4e00\u76843D\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u4e2d\u76f8\u673a\u548c\u4eba\u4f53\u8fd0\u52a8\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u901a\u8fc7\u70b9\u4e91\u548cSMPL-X\u89d2\u8272\u5b9e\u73b0\u7075\u6d3b\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5355\u72ec\u5904\u7406\u76f8\u673a\u548c\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\uff0c\u4e14\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u6709\u9650\u3002", "method": "\u63d0\u51faPCDController\u6a21\u5757\u548c\u8054\u5408\u5bf9\u9f50\u76843D\u4e16\u754c\u5f15\u5bfc\uff0c\u5206\u522b\u5b9e\u73b0\u76f8\u673a\u63a7\u5236\u548c\u4eba\u4f53\u8fd0\u52a8\u63a7\u5236\u7684\u7edf\u4e00\u3002", "result": "Uni3C\u5728\u76f8\u673a\u53ef\u63a7\u6027\u548c\u4eba\u4f53\u8fd0\u52a8\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Uni3C\u901a\u8fc7\u7edf\u4e00\u63a7\u5236\u4fe1\u53f7\u548c\u6a21\u5757\u5316\u8bad\u7ec3\uff0c\u51cf\u5c11\u4e86\u5bf9\u8054\u5408\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.14594", "pdf": "https://arxiv.org/pdf/2504.14594", "abs": "https://arxiv.org/abs/2504.14594", "authors": ["Fan Gao", "Xinjie Zhao", "Ding Xia", "Zhongyi Zhou", "Rui Yang", "Jinghui Lu", "Hang Jiang", "Chanjun Park", "Irene Li"], "title": "HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG.", "AI": {"tldr": "HealthGenie\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u996e\u98df\u5efa\u8bae\u548c\u53ef\u89c6\u5316\u4fe1\u606f\uff0c\u51cf\u5c11\u7528\u6237\u4ea4\u4e92\u8d1f\u62c5\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "motivation": "\u89e3\u51b3\u7528\u6237\u5728\u83b7\u53d6\u996e\u98df\u5efa\u8bae\u65f6\u9700\u8981\u5904\u7406\u590d\u6742\u4e13\u4e1a\u77e5\u8bc6\u548c\u4e2a\u4f53\u5065\u5eb7\u6761\u4ef6\u7684\u95ee\u9898\u3002", "method": "HealthGenie\u901a\u8fc7\u67e5\u8be2\u7cbe\u70bc\u548c\u9884\u5efaKG\u68c0\u7d22\u4fe1\u606f\uff0c\u7ed3\u5408LLM\u751f\u6210\u89e3\u91ca\u6027\u5efa\u8bae\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002", "result": "\u8bc4\u4f30\u663e\u793aHealthGenie\u6709\u6548\u652f\u6301\u4e2a\u6027\u5316\u996e\u98df\u5efa\u8bae\uff0c\u964d\u4f4e\u7528\u6237\u4ea4\u4e92\u8d1f\u62c5\u548c\u8ba4\u77e5\u8d1f\u8377\u3002", "conclusion": "LLM\u4e0eKG\u7ed3\u5408\u5728\u652f\u6301\u51b3\u7b56\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u53c2\u8003\u6b64\u65b9\u6cd5\u3002"}}
{"id": "2504.14913", "pdf": "https://arxiv.org/pdf/2504.14913", "abs": "https://arxiv.org/abs/2504.14913", "authors": ["Kenji Iwata", "Eiki Ishidera", "Toshifumi Yamaai", "Yutaka Satoh", "Hiroshi Tanaka", "Katsuhiko Takahashi", "Akio Furuhata", "Yoshihisa Tanabe", "Hiroshi Matsumura"], "title": "Guidelines for External Disturbance Factors in the Use of OCR in Real-World Environments", "categories": ["cs.CV", "cs.AI", "I.5.2; I.5.m"], "comment": "16 pages, 14 figures", "summary": "The performance of OCR has improved with the evolution of AI technology. As\nOCR continues to broaden its range of applications, the increased likelihood of\ninterference introduced by various usage environments can prevent it from\nachieving its inherent performance. This results in reduced recognition\naccuracy under certain conditions, and makes the quality control of recognition\ndevices more challenging. Therefore, to ensure that users can properly utilize\nOCR, we compiled the real-world external disturbance factors that cause\nperformance degradation, along with the resulting image degradation phenomena,\ninto an external disturbance factor table and, by also indicating how to make\nuse of it, organized them into guidelines.", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86OCR\u6027\u80fd\u4e0b\u964d\u7684\u5916\u90e8\u5e72\u6270\u56e0\u7d20\uff0c\u5e76\u6574\u7406\u6210\u6307\u5357\u4ee5\u5e2e\u52a9\u7528\u6237\u6b63\u786e\u4f7f\u7528OCR\u3002", "motivation": "\u968f\u7740OCR\u5e94\u7528\u8303\u56f4\u7684\u6269\u5927\uff0c\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5f71\u54cd\u8bc6\u522b\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6574\u7406\u8fd9\u4e9b\u56e0\u7d20\u5e76\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6574\u7406\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u548c\u56fe\u50cf\u9000\u5316\u73b0\u8c61\uff0c\u7f16\u5236\u4e86\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u8868\uff0c\u5e76\u5f62\u6210\u4f7f\u7528\u6307\u5357\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5916\u90e8\u5e72\u6270\u56e0\u7d20\u8868\u548c\u76f8\u5e94\u7684\u4f7f\u7528\u6307\u5357\uff0c\u5e2e\u52a9\u7528\u6237\u4f18\u5316OCR\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6574\u7406\u5e72\u6270\u56e0\u7d20\u5e76\u63d0\u4f9b\u6307\u5357\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347OCR\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.14640", "pdf": "https://arxiv.org/pdf/2504.14640", "abs": "https://arxiv.org/abs/2504.14640", "authors": ["Yuheng Huang", "Lei Ma", "Keizaburo Nishikino", "Takumi Akazaki"], "title": "Risk Assessment Framework for Code LLMs via Leveraging Internal States", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited", "summary": "The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.", "AI": {"tldr": "PtTrust\u662f\u4e00\u4e2a\u57fa\u4e8e\u5185\u90e8\u72b6\u6001\u9884\u8bad\u7ec3\u7684\u4e24\u9636\u6bb5\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u4ee3\u7801LLM\u7684\u53ef\u4fe1\u5ea6\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u6709\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u8de8\u4efb\u52a1\u548c\u8bed\u8a00\u7684\u901a\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801LLM\u751f\u6210\u7684\u4ee3\u7801\u53ef\u80fd\u5b58\u5728\u9519\u8bef\u3001\u4e0d\u5b89\u5168\u6216\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\u4e14\u7f3a\u4e4f\u5de5\u4e1a\u7ea7\u53ef\u6269\u5c55\u6027\u3002", "method": "PtTrust\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u5b66\u4e60LLM\u72b6\u6001\u7684\u901a\u7528\u8868\u793a\uff1b2\uff09\u7528\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u98ce\u9669\u9884\u6d4b\u5668\u3002", "result": "PtTrust\u5728\u4ee3\u7801\u884c\u7ea7\u522b\u98ce\u9669\u8bc4\u4f30\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u80fd\u8de8\u4efb\u52a1\u548c\u8bed\u8a00\u6cdb\u5316\uff0c\u5e76\u63d0\u4f9b\u76f4\u89c2\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u3002", "conclusion": "PtTrust\u4e3a\u4ee3\u7801LLM\u7684\u53ef\u6269\u5c55\u548c\u53ef\u4fe1\u4fdd\u969c\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.14919", "pdf": "https://arxiv.org/pdf/2504.14919", "abs": "https://arxiv.org/abs/2504.14919", "authors": ["Donghyeong Kim", "Chaewon Park", "Suhwan Cho", "Hyeonjeong Lim", "Minseok Kang", "Jungho Lee", "Sangyoun Lee"], "title": "GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment.", "AI": {"tldr": "GenCLIP\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u591a\u5c42\u63d0\u793a\u548c\u53cc\u5206\u652f\u63a8\u7406\u66f4\u6709\u6548\u5730\u5b66\u4e60\u548c\u5229\u7528\u901a\u7528\u63d0\u793a\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08ZSAD\uff09\u9700\u8981\u5229\u7528CLIP\u7684\u96f6\u6837\u672c\u80fd\u529b\u5339\u914d\u6587\u672c\u63d0\u793a\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u4f46\u901a\u7528\u63d0\u793a\u7684\u7a33\u5b9a\u5b66\u4e60\u548c\u6709\u6548\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "GenCLIP\u91c7\u7528\u591a\u5c42\u63d0\u793a\u6574\u5408\u4e0d\u540cCLIP\u5c42\u7684\u7c7b\u522b\u7279\u5b9a\u89c6\u89c9\u7ebf\u7d22\uff0c\u5e76\u901a\u8fc7\u53cc\u5206\u652f\u63a8\u7406\u7b56\u7565\u5e73\u8861\u7279\u5f02\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u6587\u672c\u63d0\u793a\u8fc7\u6ee4\u673a\u5236\u548c\u591a\u5c42\u89c6\u89c9\u7279\u5f81\u589e\u5f3a\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "GenCLIP\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u5b66\u4e60\u548c\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14655", "pdf": "https://arxiv.org/pdf/2504.14655", "abs": "https://arxiv.org/abs/2504.14655", "authors": ["Yunhui Xia", "Wei Shen", "Yan Wang", "Jason Klein Liu", "Huifeng Sun", "Siyue Wu", "Jian Hu", "Xiaolong Xu"], "title": "LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs", "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.", "AI": {"tldr": "LeetCodeDataset\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u548c\u8bad\u7ec3\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86LLM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u63a8\u7406\u5bfc\u5411\u7684\u7f16\u7801\u57fa\u51c6\u548c\u81ea\u5305\u542b\u8bad\u7ec3\u6d4b\u8bd5\u5e73\u53f0\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u7814\u7a76\u4e2d\u7f3a\u4e4f\u63a8\u7406\u5bfc\u5411\u7684\u7f16\u7801\u57fa\u51c6\u548c\u81ea\u5305\u542b\u8bad\u7ec3\u6d4b\u8bd5\u5e73\u53f0\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6574\u7406LeetCode Python\u95ee\u9898\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u5143\u6570\u636e\u3001\u5e7f\u6cdb\u8986\u76d6\u3001\u6bcf\u4e2a\u95ee\u9898100+\u6d4b\u8bd5\u7528\u4f8b\u4ee5\u53ca\u65f6\u95f4\u5206\u5272\uff082024\u5e747\u6708\u524d\u540e\uff09\uff0c\u5b9e\u73b0\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u548c\u9ad8\u6548\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u4ec5\u75282.6K\u6a21\u578b\u751f\u6210\u7684\u89e3\u51b3\u65b9\u6848\u8fdb\u884cSFT\u5373\u53ef\u8fbe\u5230\u4e0e110K\u6837\u672c\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "LeetCodeDataset\u53ca\u5176\u8bc4\u4f30\u6846\u67b6\u5df2\u5728Hugging Face\u548cGithub\u4e0a\u53d1\u5e03\uff0c\u4e3a\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2504.14920", "pdf": "https://arxiv.org/pdf/2504.14920", "abs": "https://arxiv.org/abs/2504.14920", "authors": ["Geng Li", "Jinglin Xu", "Yunzhen Zhao", "Yuxin Peng"], "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight). Project page with code:\n  https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025", "summary": "Humans can effortlessly locate desired objects in cluttered environments,\nrelying on a cognitive mechanism known as visual search to efficiently filter\nout irrelevant information and focus on task-related regions. Inspired by this\nprocess, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing\nvisual search method that enhances fine-grained visual understanding in large\nmultimodal models (LMMs). Unlike existing approaches which require additional\nmodules or data collection, Dyfo leverages a bidirectional interaction between\nLMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to\nsimulate human-like focus adjustments. This enables LMMs to focus on key visual\nregions while filtering out irrelevant content, without introducing additional\ntraining caused by vocabulary expansion or the integration of specialized\nlocalization modules. Experimental results demonstrate that Dyfo significantly\nimproves fine-grained visual understanding and reduces hallucination issues in\nLMMs, achieving superior performance across both fixed and dynamic resolution\nmodels. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025", "AI": {"tldr": "Dyfo\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u548cMCTS\u7b97\u6cd5\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u7126\u70b9\u8c03\u6574\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u53d7\u4eba\u7c7b\u89c6\u89c9\u641c\u7d22\u673a\u5236\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6a21\u5757\u6216\u6570\u636e\u6536\u96c6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u53cc\u5411\u4ea4\u4e92\u548cMCTS\u7b97\u6cd5\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u7126\u70b9\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6a21\u5757\u3002", "result": "\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u56fa\u5b9a\u548c\u52a8\u6001\u5206\u8fa8\u7387\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Dyfo\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u641c\u7d22\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14773", "pdf": "https://arxiv.org/pdf/2504.14773", "abs": "https://arxiv.org/abs/2504.14773", "authors": ["Haoming Li", "Zhaoliang Chen", "Jonathan Zhang", "Fei Liu"], "title": "PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": "10 pages", "summary": "Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u6709\u89c4\u5212\u57fa\u51c6\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u548c\u63a8\u8350\uff0c\u4ee5\u6307\u5bfc\u7b97\u6cd5\u9009\u62e9\u548c\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u3002", "motivation": "\u89c4\u5212\u5728\u667a\u80fd\u4f53\u548c\u667a\u80fdAI\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u89c4\u5212\u57fa\u51c6\u7684\u5168\u9762\u7406\u89e3\uff0c\u5bfc\u81f4\u7b97\u6cd5\u6bd4\u8f83\u548c\u65b0\u573a\u666f\u9009\u62e9\u56f0\u96be\u3002", "method": "\u7814\u7a76\u591a\u79cd\u89c4\u5212\u57fa\u51c6\uff0c\u5c06\u5176\u5206\u7c7b\u4e3a\u5177\u8eab\u73af\u5883\u3001\u7f51\u7edc\u5bfc\u822a\u3001\u8c03\u5ea6\u3001\u6e38\u620f\u4e0e\u8c1c\u9898\u53ca\u65e5\u5e38\u4efb\u52a1\u81ea\u52a8\u5316\uff0c\u5e76\u5206\u6790\u5176\u9002\u7528\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u4e0d\u540c\u7b97\u6cd5\u7684\u6700\u5408\u9002\u57fa\u51c6\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u7684\u6f5c\u5728\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u89c4\u5212\u7b97\u6cd5\u7684\u9009\u62e9\u548c\u672a\u6765\u57fa\u51c6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2504.14921", "pdf": "https://arxiv.org/pdf/2504.14921", "abs": "https://arxiv.org/abs/2504.14921", "authors": ["Songping Wang", "Hanqing Liu", "Yueming Lyu", "Xiantao Hu", "Ziwen He", "Wei Wang", "Caifeng Shan", "Liang Wang"], "title": "Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%.", "AI": {"tldr": "VFAT-WS\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u6570\u636e\u7684\u5feb\u901f\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u9891\u7387\u589e\u5f3a\u548c\u5f31\u5230\u5f3a\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u5e73\u8861\u5e72\u51c0\u51c6\u786e\u6027\u548c\u5bf9\u6297\u9c81\u68d2\u6027\uff0cVFAT-WS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u9891\u7387\u589e\u5f3a\uff08TF-AUG\uff09\u53ca\u5176\u65f6\u7a7a\u589e\u5f3a\u7248\u672c\uff08STF-AUG\uff09\uff0c\u4ee5\u53ca\u5355\u6b65PGD\u653b\u51fb\uff0c\u540c\u65f6\u91c7\u7528\u5f31\u5230\u5f3a\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728UCF-101\u548cHMDB-51\u6570\u636e\u96c6\u4e0a\uff0cVFAT-WS\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6297\u5e72\u6270\u9c81\u68d2\u6027\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u8fd1490%\u3002", "conclusion": "VFAT-WS\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u6210\u529f\u5e73\u8861\u4e86\u5e72\u51c0\u51c6\u786e\u6027\u548c\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u4e3a\u89c6\u9891\u5bf9\u6297\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822", "abs": "https://arxiv.org/abs/2504.14822", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review.", "AI": {"tldr": "InsightAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u673a\u4ea4\u4e92AI\u4ee3\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u548c\u591a\u4ee3\u7406\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u7efc\u8ff0\uff08SRs\uff09\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5b9e\u65f6\u53cd\u9988\u673a\u5236\uff0c\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u57281.5\u5c0f\u65f6\u5185\u5b8c\u6210\u9ad8\u8d28\u91cfSRs\u3002", "motivation": "\u7cfb\u7edf\u7efc\u8ff0\uff08SRs\uff09\u5728\u533b\u7597\u7b49\u9ad8\u9700\u6c42\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7ed3\u679c\u3002", "method": "InsightAgent\u91c7\u7528\u8bed\u4e49\u5206\u5272\u548c\u591a\u4ee3\u7406\u8bbe\u8ba1\u5904\u7406\u6587\u732e\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u5de5\u5177\u548c\u5b9e\u65f6\u53cd\u9988\u673a\u5236\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cInsightAgent\u5c06SRs\u8d28\u91cf\u63d0\u534727.2%\uff0c\u8fbe\u5230\u4eba\u5de5\u5199\u4f5c\u8d28\u91cf\u768479.7%\uff0c\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u534734.4%\uff0c\u5b8c\u6210\u65f6\u95f4\u4ece\u6570\u6708\u7f29\u77ed\u81f31.5\u5c0f\u65f6\u3002", "conclusion": "InsightAgent\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u548cAI\u6280\u672f\u663e\u8457\u4f18\u5316\u4e86\u7cfb\u7edf\u7efc\u8ff0\u7684\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2504.14933", "pdf": "https://arxiv.org/pdf/2504.14933", "abs": "https://arxiv.org/abs/2504.14933", "authors": ["Mazharul Islam Rakib", "Showrin Rahman", "Joyanta Jyoti Mondal", "Xi Xiao", "David Lewis", "Alessandra Mileo", "Meem Arafat Manab"], "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models", "categories": ["cs.CV", "68T07, 68U10, 68T45"], "comment": "16 pages, 9 figures, preprint", "summary": "In today's age of social media and marketing, copyright issues can be a major\nroadblock to the free sharing of images. Generative AI models have made it\npossible to create high-quality images, but concerns about copyright\ninfringement are a hindrance to their abundant use. As these models use data\nfrom training images to generate new ones, it is often a daunting task to\nensure they do not violate intellectual property rights. Some AI models have\neven been noted to directly copy copyrighted images, a problem often referred\nto as source copying. Traditional copyright protection measures such as\nwatermarks and metadata have also proven to be futile in this regard. To\naddress this issue, we propose a novel two-step image generation model inspired\nby the conditional diffusion model. The first step involves creating an image\nsegmentation mask for some prompt-based generated images. This mask embodies\nthe shape of the image. Thereafter, the diffusion model is asked to generate\nthe image anew while avoiding the shape in question. This approach shows a\ndecrease in structural similarity from the training image, i.e. we are able to\navoid the source copying problem using this approach without expensive\nretraining of the model or user-centered prompt generation techniques. This\nmakes our approach the most computationally inexpensive approach to avoiding\nboth copyright infringement and source copying for diffusion model-based image\ngeneration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u4e24\u6b65\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u56fe\u50cf\u5206\u5272\u63a9\u7801\u5e76\u907f\u514d\u7279\u5b9a\u5f62\u72b6\uff0c\u6709\u6548\u51cf\u5c11\u4e0e\u8bad\u7ec3\u56fe\u50cf\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u907f\u514d\u7248\u6743\u4fb5\u6743\u548c\u6e90\u590d\u5236\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u751f\u6210AI\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u53ef\u80fd\u4fb5\u72af\u7248\u6743\u6216\u76f4\u63a5\u590d\u5236\u6e90\u56fe\u50cf\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982\u6c34\u5370\u548c\u5143\u6570\u636e\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u751f\u6210\u56fe\u50cf\u5206\u5272\u63a9\u7801\uff0c\u6355\u6349\u56fe\u50cf\u5f62\u72b6\uff1b\u7136\u540e\u6269\u6563\u6a21\u578b\u91cd\u65b0\u751f\u6210\u56fe\u50cf\u65f6\u907f\u514d\u8be5\u5f62\u72b6\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u56fe\u50cf\u4e0e\u8bad\u7ec3\u56fe\u50cf\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u907f\u514d\u4e86\u6e90\u590d\u5236\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u6602\u8d35\u7684\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\u6216\u7528\u6237\u63d0\u793a\u751f\u6210\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u8ba1\u7b97\u6210\u672c\u4f4e\u3001\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\uff0c\u907f\u514d\u7248\u6743\u4fb5\u6743\u548c\u6e90\u590d\u5236\u3002"}}
{"id": "2504.14858", "pdf": "https://arxiv.org/pdf/2504.14858", "abs": "https://arxiv.org/abs/2504.14858", "authors": ["Jiaqi Wei", "Hao Zhou", "Xiang Zhang", "Di Zhang", "Zijie Qiu", "Wei Wei", "Jinzhe Li", "Wanli Ouyang", "Siqi Sun"], "title": "AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation.", "AI": {"tldr": "AlignRAG\u901a\u8fc7\u8fed\u4ee3\u7684\u6279\u5224\u9a71\u52a8\u5bf9\u9f50\uff08CDA\uff09\u6b65\u9aa4\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e2d\u7684\u63a8\u7406\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u63a8\u7406\u8f68\u8ff9\u4e0e\u68c0\u7d22\u8bc1\u636e\u7684\u5bf9\u9f50\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faAlignRAG\u6846\u67b6\uff0c\u5305\u62ec\u6784\u5efa\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u8bad\u7ec3\u8bed\u6599\u3001\u751f\u6210\u5bf9\u6bd4\u6027\u6279\u5224\u3001\u8bad\u7ec3\u6279\u5224\u8bed\u8a00\u6a21\u578b\uff08CLM\uff09\u53ca\u8fed\u4ee3\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u3002", "result": "AlignRAG\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709RAG\u6d41\u7a0b\u4e2d\u3002", "conclusion": "AlignRAG\u4e3a\u68c0\u7d22\u611f\u77e5\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6539\u8fdb\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86RAG\u7684\u7ed3\u6784\u5316\u63a8\u7406\u8f68\u8ff9\u3002"}}
{"id": "2504.14952", "pdf": "https://arxiv.org/pdf/2504.14952", "abs": "https://arxiv.org/abs/2504.14952", "authors": ["Qianyu Zhu", "Junjie Wang", "Jeremiah Hu", "Jia Ai", "Yong Lee"], "title": "PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for PIV", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Deep learning algorithms have significantly reduced the computational time\nand improved the spatial resolution of particle image velocimetry~(PIV).\nHowever, the models trained on synthetic datasets might have a degraded\nperformance on practical particle images due to domain gaps. As a result,\nspecial residual patterns are often observed for the vector fields of deep\nlearning-based estimators. To reduce the special noise step-by-step, we employ\na denoising diffusion model~(FlowDiffuser) for PIV analysis. And the\ndata-hungry iterative denoising diffusion model is trained via a transfer\nlearning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)\npre-training a FlowDiffuser model with multiple optical flow datasets of the\ncomputer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the\npre-trained model on synthetic PIV datasets. Note that the PIV images are\nupsampled by a factor of two to resolve the small-scale turbulent flow\nstructures. The visualized results indicate that our PIV-FlowDiffuser\neffectively suppresses the noise patterns. Therefore, the denoising diffusion\nmodel reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV\nbaseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits\nenhanced generalization performance on unseen particle images due to transfer\nlearning. Overall, this study highlights the transfer-learning-based denoising\ndiffusion models for PIV. And a detailed implementation is recommended for\ninterested readers in the repository\nhttps://github.com/Zhu-Qianyu/PIV-FlowDiffuser.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08FlowDiffuser\uff09\u7684PIV\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u566a\u58f0\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728PIV\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u9886\u57df\u5dee\u8ddd\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u566a\u58f0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53bb\u566a\u6269\u6563\u6a21\u578b\uff08FlowDiffuser\uff09\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\uff08\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6570\u636e\u96c6\uff09\u548c\u5fae\u8c03\uff08\u5408\u6210PIV\u6570\u636e\uff09\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "PIV-FlowDiffuser\u5c06\u5e73\u5747\u7ec8\u70b9\u8bef\u5dee\uff08AEE\uff09\u964d\u4f4e\u4e8659.4%\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u7c92\u5b50\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u5728PIV\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u5b9e\u73b0\u7ec6\u8282\u53c2\u8003\u63d0\u4f9b\u7684\u4ee3\u7801\u5e93\u3002"}}
{"id": "2504.14870", "pdf": "https://arxiv.org/pdf/2504.14870", "abs": "https://arxiv.org/abs/2504.14870", "authors": ["Hongru Wang", "Cheng Qian", "Wanjun Zhong", "Xiusi Chen", "Jiahao Qiu", "Shijue Huang", "Bowen Jin", "Mengdi Wang", "Kam-Fai Wong", "Heng Ji"], "title": "OTC: Optimal Tool Calls via Reinforcement Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86OTC-PO\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5de5\u5177\u8c03\u7528\u6548\u7387\uff0c\u51cf\u5c11\u5de5\u5177\u4f7f\u7528\u6b21\u6570\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u63a8\u7406\u65b9\u6cd5\u5ffd\u89c6\u5de5\u5177\u4f7f\u7528\u6548\u7387\u548c\u6210\u672c\uff0c\u5bfc\u81f4\u5de5\u5177\u8c03\u7528\u8fc7\u591a\u6216\u4e0d\u8db3\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u5f00\u9500\u3002", "method": "\u63d0\u51faOTC-PO\u6846\u67b6\uff0c\u7ed3\u5408\u6b63\u786e\u6027\u548c\u5de5\u5177\u6548\u7387\u7684\u5956\u52b1\u673a\u5236\uff0c\u57fa\u4e8ePPO\u548cGRPO\u5b9e\u73b0OTC-PPO\u548cOTC-GRPO\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5de5\u5177\u8c03\u7528\u51cf\u5c1173.1%\uff0c\u5de5\u5177\u6548\u7387\u63d0\u5347229.4%\uff0c\u540c\u65f6\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u3002", "conclusion": "OTC-PO\u662f\u9996\u4e2a\u660e\u786e\u4f18\u5316\u5de5\u5177\u4f7f\u7528\u6548\u7387\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u96c6\u6210\u63a8\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2504.14967", "pdf": "https://arxiv.org/pdf/2504.14967", "abs": "https://arxiv.org/abs/2504.14967", "authors": ["Yating Wang", "Xuan Wang", "Ran Yi", "Yanbo Fan", "Jichen Hu", "Jingcheng Zhu", "Lizhuang Ma"], "title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to\nconstruct high-quality 3D head avatars. In this line of research, existing\nmethods either fail to capture the dynamic textures or incur significant\noverhead in terms of runtime speed or storage space. To this end, we propose a\nnovel method that addresses all the aforementioned demands. In specific, we\nintroduce an expressive and compact representation that encodes texture-related\nattributes of the 3D Gaussians in the tensorial format. We store appearance of\nneutral expression in static tri-planes, and represents dynamic texture details\nfor different expressions using lightweight 1D feature lines, which are then\ndecoded into opacity offset relative to the neutral face. We further propose\nadaptive truncated opacity penalty and class-balanced sampling to improve\ngeneralization across different expressions. Experiments show this design\nenables accurate face dynamic details capturing while maintains real-time\nrendering and significantly reduces storage costs, thus broadening the\napplicability to more scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u548c3DMM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d27\u51d1\u7684\u5f20\u91cf\u8868\u793a\u548c\u52a8\u6001\u7eb9\u7406\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf3D\u5934\u50cf\u7684\u52a8\u6001\u7ec6\u8282\u6355\u6349\uff0c\u540c\u65f6\u964d\u4f4e\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u7eb9\u7406\u6355\u6349\u6216\u8fd0\u884c\u65f6\u6548\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u517c\u987e\u9ad8\u8d28\u91cf\u548c\u4f4e\u5f00\u9500\u3002", "method": "\u91c7\u7528\u5f20\u91cf\u683c\u5f0f\u7f16\u78013D\u9ad8\u65af\u7684\u7eb9\u7406\u5c5e\u6027\uff0c\u9759\u6001\u4e2d\u6027\u8868\u60c5\u5b58\u50a8\u5728\u4e09\u89d2\u5e73\u9762\u4e2d\uff0c\u52a8\u6001\u7eb9\u7406\u7ec6\u8282\u901a\u8fc7\u8f7b\u91cf\u7ea71D\u7279\u5f81\u7ebf\u8868\u793a\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u622a\u65ad\u4e0d\u900f\u660e\u5ea6\u60e9\u7f5a\u548c\u7c7b\u522b\u5e73\u8861\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u6355\u6349\u9762\u90e8\u52a8\u6001\u7ec6\u8282\uff0c\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u7ec6\u8282\u6355\u6349\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u6269\u5c55\u4e863D\u5934\u50cf\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.14904", "pdf": "https://arxiv.org/pdf/2504.14904", "abs": "https://arxiv.org/abs/2504.14904", "authors": ["Xingyu Lu", "Tianke Zhang", "Chang Meng", "Xiaobei Wang", "Jinpeng Wang", "YiFan Zhang", "Shisong Tang", "Changyi Liu", "Haojie Ding", "Kaiyu Jiang", "Kaiyu Tang", "Bin Wen", "Hai-Tao Zheng", "Fan Yang", "Tingting Gao", "Di Zhang", "Kun Gai"], "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform", "categories": ["cs.SI", "cs.AI", "cs.CL", "cs.MM"], "comment": "20 pages, 6 figures", "summary": "Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86KuaiMod\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u7684\u6311\u6218\uff0c\u7ed3\u5408VLM\u548cCoT\u63a8\u7406\uff0c\u63d0\u5347\u5ba1\u6838\u51c6\u786e\u6027\u548c\u52a8\u6001\u66f4\u65b0\u80fd\u529b\u3002", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u5185\u5bb9\u5ba1\u6838\u5b58\u5728\u4eba\u5de5\u504f\u89c1\u3001\u81ea\u52a8\u5316\u65b9\u6cd5\u51c6\u786e\u6027\u4e0d\u8db3\u53ca\u884c\u4e1a\u89c4\u8303\u66f4\u65b0\u6162\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faKuaiMod\u6846\u67b6\uff0c\u5305\u542b\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u3001\u79bb\u7ebf\u9002\u5e94\u548c\u5728\u7ebf\u90e8\u7f72\u4e0e\u4f18\u5316\u4e09\u90e8\u5206\uff0c\u5229\u7528VLM\u548cCoT\u63a8\u7406\u5efa\u6a21\u89c6\u9891\u6bd2\u6027\u3002", "result": "KuaiMod\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u7528\u6237\u4e3e\u62a5\u7387\u964d\u4f4e20%\uff0cDAU\u548cAUT\u663e\u8457\u63d0\u5347\u3002", "conclusion": "KuaiMod\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5bb9\u5ba1\u6838\u7684\u6311\u6218\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u4e86\u52a8\u6001\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14975", "pdf": "https://arxiv.org/pdf/2504.14975", "abs": "https://arxiv.org/abs/2504.14975", "authors": ["Hongbin Xu", "Chaohui Yu", "Feng Xiao", "Jiazheng Xing", "Hai Ci", "Weitao Chen", "Ming Li"], "title": "Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency Regularization", "categories": ["cs.CV"], "comment": "Preprint version. The code will be open after finishing the reviewing\n  process", "summary": "Despite the remarkable progress of 3D generation, achieving controllability,\ni.e., ensuring consistency between generated 3D content and input conditions\nlike edge and depth, remains a significant challenge. Existing methods often\nstruggle to maintain accurate alignment, leading to noticeable discrepancies.\nTo address this issue, we propose \\name{}, a new framework that enhances\ncontrollable 3D generation by explicitly encouraging cyclic consistency between\nthe second-order 3D content, generated based on extracted signals from the\nfirst-order generation, and its original input controls. Specifically, we\nemploy an efficient feed-forward backbone that can generate a 3D object from an\ninput condition and a text prompt. Given an initial viewpoint and a control\nsignal, a novel view is rendered from the generated 3D content, from which the\nextracted condition is used to regenerate the 3D content. This re-generated\noutput is then rendered back to the initial viewpoint, followed by another\nround of control signal extraction, forming a cyclic process with two\nconsistency constraints. \\emph{View consistency} ensures coherence between the\ntwo generated 3D objects, measured by semantic similarity to accommodate\ngenerative diversity. \\emph{Condition consistency} aligns the final extracted\nsignal with the original input control, preserving structural or geometric\ndetails throughout the process. Extensive experiments on popular benchmarks\ndemonstrate that \\name{} significantly improves controllability, especially for\nfine-grained details, outperforming existing methods across various conditions\n(e.g., +14.17\\% PSNR for edge, +6.26\\% PSNR for sketch).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\name{}\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u589e\u5f3a\u53ef\u63a73D\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u5185\u5bb9\u4e0e\u8f93\u5165\u6761\u4ef6\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u751f\u6210\u4e2d\u96be\u4ee5\u4fdd\u6301\u8f93\u5165\u6761\u4ef6\uff08\u5982\u8fb9\u7f18\u548c\u6df1\u5ea6\uff09\u4e0e\u751f\u6210\u5185\u5bb9\u7684\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u660e\u663e\u5dee\u5f02\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684feed-forward\u4e3b\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u5faa\u73af\u8fc7\u7a0b\uff08\u5305\u62ec\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u6761\u4ef6\u4e00\u81f4\u6027\u7ea6\u675f\uff09\u751f\u6210\u548c\u91cd\u65b0\u751f\u62103D\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\\name{}\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u8fb9\u7f18PSNR\u63d0\u534714.17%\uff0c\u8349\u56fePSNR\u63d0\u53476.26%\uff09\u3002", "conclusion": "\\name{}\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u6709\u6548\u63d0\u5347\u4e863D\u751f\u6210\u7684\u53ef\u63a7\u6027\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.14928", "pdf": "https://arxiv.org/pdf/2504.14928", "abs": "https://arxiv.org/abs/2504.14928", "authors": ["Yao Shi", "Rongkeng Liang", "Yong Xu"], "title": "EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework", "categories": ["cs.AI", "cs.CE", "cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.", "AI": {"tldr": "EducationQ\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u8bc4\u4f30LLMs\u7684\u6559\u5b66\u80fd\u529b\uff0c\u53d1\u73b0\u6559\u5b66\u6548\u679c\u4e0e\u6a21\u578b\u89c4\u6a21\u6216\u901a\u7528\u63a8\u7406\u80fd\u529b\u65e0\u5173\uff0c\u90e8\u5206\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u5546\u4e1a\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLMs\u6559\u5b66\u80fd\u529b\u7684\u65b9\u6cd5\u8d44\u6e90\u5bc6\u96c6\u4e14\u590d\u6742\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u5f0f\u6559\u5b66\u6cd5\u7684\u5173\u6ce8\u3002", "method": "\u5f15\u5165EducationQ\u6846\u67b6\uff0c\u6a21\u62df\u52a8\u6001\u6559\u80b2\u573a\u666f\uff0c\u6d4b\u8bd514\u4e2aLLMs\u572813\u4e2a\u5b66\u79d1\u548c10\u4e2a\u96be\u5ea6\u7ea7\u522b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6559\u5b66\u6548\u679c\u4e0e\u6a21\u578b\u89c4\u6a21\u65e0\u5173\uff0c\u90e8\u5206\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff1b\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u4e0e\u81ea\u52a8\u5316\u5206\u6790\u4e00\u81f4\u3002", "conclusion": "LLMs\u4f5c\u4e3a\u6559\u5e08\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\uff0c\u672a\u6765\u6559\u80b2AI\u5e94\u5173\u6ce8\u7279\u5b9a\u6559\u5b66\u6548\u679c\u63d0\u5347\u3002"}}
{"id": "2504.14977", "pdf": "https://arxiv.org/pdf/2504.14977", "abs": "https://arxiv.org/abs/2504.14977", "authors": ["Jingkai Zhou", "Yifan Wu", "Shikai Li", "Min Wei", "Chao Fan", "Weihua Chen", "Wei Jiang", "Fan Wang"], "title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild", "categories": ["cs.CV"], "comment": "Project Page:\n  https://thefoxofsky.github.io/project_pages_new/RealisDance-DiT/index", "summary": "Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u7684\u7b80\u5355\u4fee\u6539\u65b9\u6cd5\uff08RealisDance-DiT\uff09\uff0c\u901a\u8fc7\u7075\u6d3b\u5fae\u8c03\u7b56\u7565\u89e3\u51b3\u53ef\u63a7\u89d2\u8272\u52a8\u753b\u4e2d\u7684\u7f55\u89c1\u59ff\u52bf\u3001\u98ce\u683c\u5316\u89d2\u8272\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53ef\u63a7\u89d2\u8272\u52a8\u753b\u4e2d\u7684\u7f55\u89c1\u59ff\u52bf\u3001\u98ce\u683c\u5316\u89d2\u8272\u3001\u89d2\u8272-\u7269\u4f53\u4ea4\u4e92\u7b49\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5728\u5f00\u653e\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eWan-2.1\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u51faRealisDance-DiT\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u67b6\u6784\u4fee\u6539\u548c\u7075\u6d3b\u5fae\u8c03\u7b56\u7565\uff08\u5982\u4f4e\u566a\u58f0\u9884\u70ed\u548c\u5927\u6279\u6b21\u5c0f\u8fed\u4ee3\uff09\u63d0\u5347\u6027\u80fd\u3002", "result": "RealisDance-DiT\u5728\u5b9e\u9a8c\u4e2d\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u6d4b\u8bd5\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u7b80\u5355\u4fee\u6539\u548c\u7075\u6d3b\u5fae\u8c03\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u53ef\u63a7\u89d2\u8272\u52a8\u753b\u7684\u590d\u6742\u6311\u6218\u3002"}}
{"id": "2504.14945", "pdf": "https://arxiv.org/pdf/2504.14945", "abs": "https://arxiv.org/abs/2504.14945", "authors": ["Jianhao Yan", "Yafu Li", "Zican Hu", "Zhi Wang", "Ganqu Cui", "Xiaoye Qu", "Yu Cheng", "Yue Zhang"], "title": "Learning to Reason under Off-Policy Guidance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.", "AI": {"tldr": "LUFFY\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u79bb\u7b56\u7565\u63a8\u7406\u8f68\u8ff9\u548c\u52a8\u6001\u5e73\u8861\u6a21\u4eff\u4e0e\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u96f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u7b56\u7565\u5185\u5b66\u4e60\uff0c\u65e0\u6cd5\u8d85\u8d8a\u521d\u59cb\u80fd\u529b\u83b7\u53d6\u66f4\u9ad8\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165LUFFY\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7b56\u7565\u63a8\u7406\u8f68\u8ff9\u548c\u7b56\u7565\u5185\u8bad\u7ec3\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u91cd\u8981\u6027\u91c7\u6837\u907f\u514d\u6d45\u5c42\u6a21\u4eff\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53477.0\u5206\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u4f18\u52bf\u8d85\u8fc76.2\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6a21\u4eff\u7684\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "LUFFY\u4e0d\u4ec5\u6709\u6548\u6a21\u4eff\uff0c\u8fd8\u80fd\u8d85\u8d8a\u6f14\u793a\u8fdb\u884c\u63a2\u7d22\uff0c\u4e3a\u8bad\u7ec3\u53ef\u6cdb\u5316\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2504.14988", "pdf": "https://arxiv.org/pdf/2504.14988", "abs": "https://arxiv.org/abs/2504.14988", "authors": ["Hong-Tao Yu", "Xiu-Shen Wei", "Yuxin Peng", "Serge Belongie"], "title": "Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable multimodal perception capabilities, garnering significant attention.\nWhile numerous evaluation studies have emerged, assessing LVLMs both\nholistically and on specialized tasks, fine-grained image tasks-fundamental to\ncomputer vision-remain largely unexplored. To fill this gap, we introduce a\ncomprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49\nmillion questions and 3.32 million images. Our evaluation systematically\nexamines LVLMs from both human-oriented and machine-oriented perspectives,\nfocusing on their semantic recognition and fine-grained feature representation\ncapabilities. Through extensive experiments on eight representative LVLMs/VLMs,\nwe uncover key findings regarding the influence of training paradigms, modality\nalignment, perturbation susceptibility, and fine-grained category reasoning on\ntask performance. This work provides critical insights into the limitations of\ncurrent LVLMs and offers guidance for future data construction and model design\nin the development of more advanced LVLMs. Our code is open-source and\navailable at https://github.com/SEU-VIPGroup/FG-BMK.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFG-BMK\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u8bed\u4e49\u8bc6\u522b\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u8868\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u5bf9LVLMs\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6574\u4f53\u548c\u4e13\u9879\u4efb\u52a1\u8bc4\u4f30\uff0c\u800c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u4efb\u52a1\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u5305\u542b349\u4e07\u95ee\u9898\u548c332\u4e07\u56fe\u50cf\u7684FG-BMK\u57fa\u51c6\uff0c\u4ece\u4eba\u7c7b\u548c\u673a\u5668\u89c6\u89d2\u7cfb\u7edf\u8bc4\u4f30\u4e868\u79cd\u4ee3\u8868\u6027LVLMs/VLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u8bad\u7ec3\u8303\u5f0f\u3001\u6a21\u6001\u5bf9\u9f50\u3001\u6270\u52a8\u654f\u611f\u6027\u548c\u7ec6\u7c92\u5ea6\u7c7b\u522b\u63a8\u7406\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3aLVLMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u672a\u6765\u6570\u636e\u6784\u5efa\u548c\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u66f4\u5148\u8fdb\u7684LVLMs\u53d1\u5c55\u3002"}}
{"id": "2504.15068", "pdf": "https://arxiv.org/pdf/2504.15068", "abs": "https://arxiv.org/abs/2504.15068", "authors": ["Ronak Pradeep", "Nandan Thakur", "Shivani Upadhyay", "Daniel Campos", "Nick Craswell", "Jimmy Lin"], "title": "The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": "To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607", "summary": "Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6AutoNuggetizer\uff0c\u7528\u4e8e\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7LLMs\u81ea\u52a8\u751f\u6210\u548c\u5206\u914d\u8bc4\u4f30\u5355\u5143\uff08nuggets\uff09\uff0c\u5e76\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7ed3\u679c\u5bf9\u6bd4\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u969c\u788d\uff0c\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u7684\u53d1\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528AutoNuggetizer\u6846\u67b6\uff0c\u5229\u7528LLMs\u81ea\u52a8\u751f\u6210\u548c\u5206\u914d\u8bc4\u4f30\u5355\u5143\uff08nuggets\uff09\uff0c\u5e76\u4e0e\u4eba\u5de5\u8bc4\u4f30\u65b9\u6cd5\uff08\u624b\u52a8\u6216\u534a\u624b\u52a8\uff09\u8fdb\u884c\u5bf9\u6bd4\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5168\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u5de5\u8bc4\u4f30\u65b9\u6cd5\u5728\u8fd0\u884c\u7ea7\u522b\u4e0a\u5177\u6709\u8f83\u5f3a\u7684\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u72ec\u7acb\u81ea\u52a8\u5316\u7ec4\u4ef6\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8bc4\u4f30\u8d28\u91cf\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5e73\u8861\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u6bcf\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8bca\u65ad\u7cfb\u7edf\u6545\u969c\u3002"}}
{"id": "2504.15003", "pdf": "https://arxiv.org/pdf/2504.15003", "abs": "https://arxiv.org/abs/2504.15003", "authors": ["Xin Li", "Xijun Wang", "Bingchen Li", "Kun Yuan", "Yizhen Shao", "Suhang Yao", "Ming Sun", "Chao Zhou", "Radu Timofte", "Zhibo Chen"], "title": "NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and Enhancement: KwaiSR Dataset and Study", "categories": ["cs.CV"], "comment": "KwaiSR dataset, a new dataset for image super-resolution, used for\n  CVPR NTIRE 2025 Challenge; CVPR 2025 workshop paper", "summary": "In this work, we build the first benchmark dataset for short-form UGC Image\nSuper-resolution in the wild, termed KwaiSR, intending to advance the research\non developing image super-resolution algorithms for short-form UGC platforms.\nThis dataset is collected from the Kwai Platform, which is composed of two\nparts, i.e., synthetic and wild parts. Among them, the synthetic dataset,\nincluding 1,900 image pairs, is produced by simulating the degradation\nfollowing the distribution of real-world low-quality short-form UGC images,\naiming to provide the ground truth for training and objective comparison in the\nvalidation/testing. The wild dataset contains low-quality images collected\ndirectly from the Kwai Platform, which are filtered using the quality\nassessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset\ncontains 1800 synthetic image pairs and 1900 wild images, which are divided\ninto training, validation, and testing parts with a ratio of 8:1:1. Based on\nthe KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form\nUGC Video quality assessment and enhancement, which attracts lots of\nresearchers to develop the algorithm for it. The results of this competition\nhave revealed that our KwaiSR dataset is pretty challenging for existing Image\nSR methods, which is expected to lead to a new direction in the image\nsuper-resolution field. The dataset can be found from\nhttps://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.", "AI": {"tldr": "KwaiSR\u662f\u9996\u4e2a\u9488\u5bf9\u77ed\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b\u5408\u6210\u548c\u91ce\u751f\u4e24\u90e8\u5206\uff0c\u7528\u4e8e\u63a8\u52a8\u77edUGC\u5e73\u53f0\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u7684\u7814\u7a76\u3002", "motivation": "\u63a8\u52a8\u77edUGC\u5e73\u53f0\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u7684\u7814\u7a76\uff0c\u586b\u8865\u8be5\u9886\u57df\u57fa\u51c6\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u5206\u4e3a\u5408\u6210\u90e8\u5206\uff08\u6a21\u62df\u771f\u5b9e\u4f4e\u8d28\u91cfUGC\u56fe\u50cf\uff09\u548c\u91ce\u751f\u90e8\u5206\uff08\u76f4\u63a5\u4eceKwai\u5e73\u53f0\u6536\u96c6\uff09\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5KVQ\u7b5b\u9009\u3002", "result": "KwaiSR\u6570\u636e\u96c6\u5305\u542b1800\u5bf9\u5408\u6210\u56fe\u50cf\u548c1900\u5f20\u91ce\u751f\u56fe\u50cf\uff0c\u6311\u6218\u8d5b\u7ed3\u679c\u663e\u793a\u73b0\u6709\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "conclusion": "KwaiSR\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\uff0c\u6709\u671b\u5f15\u9886\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u9886\u57df\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.15072", "pdf": "https://arxiv.org/pdf/2504.15072", "abs": "https://arxiv.org/abs/2504.15072", "authors": ["Yulong Li", "Zhixiang Lu", "Feilong Tang", "Simin Lai", "Ming Hu", "Yuxuan Zhang", "Haochen Xue", "Zhaodong Wu", "Imran Razzak", "Qingxia Li", "Jionglong Su"], "title": "Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "The rapid development of social media has significantly reshaped the dynamics\nof public opinion, resulting in complex interactions that traditional models\nfail to effectively capture. To address this challenge, we propose an\ninnovative approach that integrates multi-dimensional Hawkes processes with\nGraph Neural Network, modeling opinion propagation dynamics among nodes in a\nsocial network while considering the intricate hierarchical relationships\nbetween comments. The extended multi-dimensional Hawkes process captures the\nhierarchical structure, multi-dimensional interactions, and mutual influences\nacross different topics, forming a complex propagation network. Moreover,\nrecognizing the lack of high-quality datasets capable of comprehensively\ncapturing the evolution of public opinion dynamics, we introduce a new dataset,\nVISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015\nsecond-level comments, and 29,578 third-level comments, covering diverse\ndomains such as politics, entertainment, sports, health, and medicine. The\ndataset is annotated with detailed sentiment labels across 11 categories and\nclearly defined hierarchical relationships. When combined with our method, it\noffers strong interpretability by linking sentiment propagation to the comment\nhierarchy and temporal evolution. Our approach provides a robust baseline for\nfuture research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7ef4\u970d\u514b\u65af\u8fc7\u7a0b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u793e\u4ea4\u5a92\u4f53\u4e2d\u610f\u89c1\u4f20\u64ad\u7684\u52a8\u6001\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6VISTA\u652f\u6301\u7814\u7a76\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u793e\u4ea4\u5a92\u4f53\u4e2d\u590d\u6742\u7684\u516c\u5171\u610f\u89c1\u52a8\u6001\uff0c\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "\u6574\u5408\u591a\u7ef4\u970d\u514b\u65af\u8fc7\u7a0b\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5efa\u6a21\u793e\u4ea4\u7f51\u7edc\u4e2d\u8282\u70b9\u95f4\u7684\u610f\u89c1\u4f20\u64ad\u53ca\u8bc4\u8bba\u5c42\u7ea7\u5173\u7cfb\u3002", "result": "\u63d0\u51faVISTA\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u9886\u57df\u6570\u636e\u53ca\u8be6\u7ec6\u6807\u6ce8\uff0c\u7ed3\u5408\u65b9\u6cd5\u63d0\u4f9b\u5f3a\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c24\u5176\u5728\u610f\u89c1\u4f20\u64ad\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u3002"}}
{"id": "2504.15007", "pdf": "https://arxiv.org/pdf/2504.15007", "abs": "https://arxiv.org/abs/2504.15007", "authors": ["David C Wong", "Bin Wang", "Gorkem Durak", "Marouane Tliba", "Mohamed Amine Kerkouri", "Aladine Chetouani", "Ahmet Enis Cetin", "Cagdas Topel", "Nicolo Gennaro", "Camila Vendrami", "Tugce Agirlar Trabzonlu", "Amir Ali Rahsepar", "Laetitia Perronne", "Matthew Antalek", "Onural Ozturk", "Gokcan Okur", "Andrew C. Gordon", "Ayis Pyrros", "Frank H Miller", "Amir A Borhani", "Hatice Savas", "Eric M. Hart"], "title": "Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical Images", "categories": ["cs.CV", "cs.HC"], "comment": "This paper was accepted at ETRA 2025 Japan", "summary": "Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\u653e\u5c04\u79d1\u533b\u751f\u5bf9\u771f\u5b9e\u4e0e\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u653e\u5c04\u79d1\u533b\u751f\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5206\u914d\u6ce8\u610f\u529b\uff0c\u4ee5\u53ca\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u5bf9\u5176\u89c6\u89c9\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u5206\u6790\u773c\u52a8\u6a21\u5f0f\uff08\u5982\u626b\u89c6\u65b9\u5411\u3001\u5e45\u5ea6\uff09\u548c\u6ce8\u89c6\u504f\u5dee\u56fe\uff0c\u6bd4\u8f83\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u663e\u8457\u6027\u5dee\u5f02\u3002", "result": "\u63ed\u793a\u4e86\u653e\u5c04\u79d1\u533b\u751f\u5728\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u4e0a\u7684\u6ce8\u89c6\u5206\u5e03\u548c\u89c6\u89c9\u663e\u8457\u6027\u5dee\u5f02\u3002", "conclusion": "\u773c\u52a8\u8ffd\u8e2a\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3\u653e\u5c04\u79d1\u533b\u751f\u7684\u8bca\u65ad\u7b56\u7565\u53ca\u5176\u5bf9\u56fe\u50cf\u771f\u5b9e\u6027\u7684\u53cd\u5e94\u3002"}}
{"id": "2504.15135", "pdf": "https://arxiv.org/pdf/2504.15135", "abs": "https://arxiv.org/abs/2504.15135", "authors": ["Juyeon Kim", "Geon Lee", "Taeuk Kim", "Kijung Shin"], "title": "KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "SIGIR 2025 (Short)", "summary": "Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.", "AI": {"tldr": "KGMEL\u662f\u4e00\u4e2a\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u589e\u5f3a\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u3001\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u4e09\u4e2a\u9636\u6bb5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MEL\u65b9\u6cd5\u5ffd\u7565\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\uff0cKGMEL\u65e8\u5728\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u51cf\u5c11\u6b67\u4e49\u5e76\u63d0\u9ad8\u5bf9\u9f50\u51c6\u786e\u6027\u3002", "method": "KGMEL\u5206\u4e09\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u3001\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u68c0\u7d22\u5019\u9009\u5b9e\u4f53\u3001\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u6392\u5e8f\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cKGMEL\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KGMEL\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15009", "pdf": "https://arxiv.org/pdf/2504.15009", "abs": "https://arxiv.org/abs/2504.15009", "authors": ["Wensong Song", "Hong Jiang", "Zongxing Yang", "Ruijie Quan", "Yi Yang"], "title": "Insert Anything: Image Insertion via In-Context Editing in DiT", "categories": ["cs.CV"], "comment": null, "summary": "This work presents Insert Anything, a unified framework for reference-based\nimage insertion that seamlessly integrates objects from reference images into\ntarget scenes under flexible, user-specified control guidance. Instead of\ntraining separate models for individual tasks, our approach is trained once on\nour new AnyInsertion dataset--comprising 120K prompt-image pairs covering\ndiverse tasks such as person, object, and garment insertion--and effortlessly\ngeneralizes to a wide range of insertion scenarios. Such a challenging setting\nrequires capturing both identity features and fine-grained details, while\nallowing versatile local adaptations in style, color, and texture. To this end,\nwe propose to leverage the multimodal attention of the Diffusion Transformer\n(DiT) to support both mask- and text-guided editing. Furthermore, we introduce\nan in-context editing mechanism that treats the reference image as contextual\ninformation, employing two prompting strategies to harmonize the inserted\nelements with the target scene while faithfully preserving their distinctive\nfeatures. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD\nbenchmarks demonstrate that our method consistently outperforms existing\nalternatives, underscoring its great potential in real-world applications such\nas creative content generation, virtual try-on, and scene composition.", "AI": {"tldr": "Insert Anything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u4e8e\u53c2\u8003\u7684\u56fe\u50cf\u63d2\u5165\uff0c\u652f\u6301\u7075\u6d3b\u7684\u7528\u6237\u63a7\u5236\uff0c\u80fd\u591f\u5c06\u53c2\u8003\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u65e0\u7f1d\u96c6\u6210\u5230\u76ee\u6807\u573a\u666f\u4e2d\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u800cInsert Anything\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u591a\u6837\u5316\u6570\u636e\u96c6AnyInsertion\uff0c\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u63d2\u5165\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528Diffusion Transformer\uff08DiT\uff09\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u652f\u6301\u63a9\u7801\u548c\u6587\u672c\u5f15\u5bfc\u7684\u7f16\u8f91\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u7f16\u8f91\u673a\u5236\uff0c\u901a\u8fc7\u4e24\u79cd\u63d0\u793a\u7b56\u7565\u534f\u8c03\u63d2\u5165\u5143\u7d20\u4e0e\u76ee\u6807\u573a\u666f\u3002", "result": "\u5728AnyInsertion\u3001DreamBooth\u548cVTON-HD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "Insert Anything\u5728\u521b\u610f\u5185\u5bb9\u751f\u6210\u3001\u865a\u62df\u8bd5\u7a7f\u548c\u573a\u666f\u5408\u6210\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2504.15254", "pdf": "https://arxiv.org/pdf/2504.15254", "abs": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "AI": {"tldr": "CRUST-Bench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30C\u5230Rust\u8f6c\u8bd1\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b100\u4e2aC\u4ed3\u5e93\u53ca\u5176\u5bf9\u5e94\u7684\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u65e8\u5728\u89e3\u51b3\u590d\u6742\u9879\u76ee\u8f6c\u8bd1\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u8bc4\u4f30C\u5230\u5b89\u5168Rust\u7684\u8f6c\u8bd1\u80fd\u529b\uff0cCRUST-Bench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u652f\u6301\u590d\u6742\u9879\u76ee\u7684\u8f6c\u8bd1\u9a8c\u8bc1\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u624b\u52a8\u7f16\u5199\u7684\u5b89\u5168Rust\u63a5\u53e3\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0cCRUST-Bench\u786e\u4fdd\u8f6c\u8bd1\u7684\u4ee3\u7801\u7b26\u5408Rust\u7684\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\u8981\u6c42\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684LLM\uff08\u5982OpenAI o1\uff09\u5728\u5355\u6b21\u5c1d\u8bd5\u4e2d\u4ec5\u80fd\u89e3\u51b315\u4e2a\u4efb\u52a1\uff0c\u8868\u660e\u8f6c\u8bd1\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "CRUST-Bench\u4e3a\u6539\u8fdb\u8f6c\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u4eceC\u5230Rust\u7b49\u5185\u5b58\u5b89\u5168\u8bed\u8a00\u7684\u4ee3\u7801\u8fc1\u79fb\u3002"}}
{"id": "2504.15026", "pdf": "https://arxiv.org/pdf/2504.15026", "abs": "https://arxiv.org/abs/2504.15026", "authors": ["Zijin Yang", "Xin Zhang", "Kejiang Chen", "Kai Zeng", "Qiyi Yao", "Han Fang", "Weiming Zhang", "Nenghai Yu"], "title": "Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models", "categories": ["cs.CV", "cs.CR"], "comment": "18 pages, 8 figures", "summary": "Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGaussian Shading++\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5bc6\u94a5\u7ba1\u7406\u3001\u7528\u6237\u5b9a\u4e49\u53c2\u6570\u548c\u7b2c\u4e09\u65b9\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7248\u6743\u4fdd\u62a4\u548c\u4e0d\u5f53\u5185\u5bb9\u751f\u6210\u7684\u4f26\u7406\u95ee\u9898\uff0c\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u5ffd\u7565\u4e86\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u901a\u9053\u8bbe\u8ba1\uff0c\u7ed3\u5408\u4f2a\u968f\u673a\u7ea0\u9519\u7801\u548c\u8f6f\u51b3\u7b56\u89e3\u7801\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u516c\u94a5\u7b7e\u540d\u4ee5\u5b9e\u73b0\u7b2c\u4e09\u65b9\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGaussian Shading++\u5728\u4fdd\u6301\u6027\u80fd\u65e0\u635f\u7684\u540c\u65f6\uff0c\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Gaussian Shading++\u662f\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2504.15266", "pdf": "https://arxiv.org/pdf/2504.15266", "abs": "https://arxiv.org/abs/2504.15266", "authors": ["Vaishnavh Nagarajan", "Chen Henry Wu", "Charles Ding", "Aditi Raghunathan"], "title": "Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "37 pages", "summary": "We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity", "AI": {"tldr": "\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u5957\u6700\u5c0f\u7b97\u6cd5\u4efb\u52a1\uff0c\u7528\u4e8e\u91cf\u5316\u8bed\u8a00\u6a21\u578b\u7684\u521b\u9020\u529b\uff0c\u53d1\u73b0\u591a\u4ee4\u724c\u65b9\u6cd5\u4f18\u4e8e\u5355\u4ee4\u724c\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u566a\u58f0\u6ce8\u5165\u65b9\u6cd5\uff08hash-conditioning\uff09\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u7684\u521b\u9020\u529b\u9650\u5236\uff0c\u63a2\u7d22\u5982\u4f55\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\u4ee5\u751f\u6210\u66f4\u591a\u6837\u5316\u548c\u539f\u521b\u7684\u8f93\u51fa\u3002", "method": "\u8bbe\u8ba1\u62bd\u8c61\u4efb\u52a1\uff0c\u6bd4\u8f83\u5355\u4ee4\u724c\u548c\u591a\u4ee4\u724c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fahash-conditioning\u566a\u58f0\u6ce8\u5165\u6280\u672f\u3002", "result": "\u591a\u4ee4\u724c\u65b9\u6cd5\uff08\u5982\u65e0\u6559\u5e08\u8bad\u7ec3\u548c\u6269\u6563\u6a21\u578b\uff09\u8868\u73b0\u66f4\u597d\uff0chash-conditioning\u80fd\u6709\u6548\u5e73\u8861\u968f\u673a\u6027\u4e0e\u8fde\u8d2f\u6027\u3002", "conclusion": "\u4e3a\u5206\u6790\u5f00\u653e\u5f0f\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u652f\u6301\u8d85\u8d8a\u5355\u4ee4\u724c\u5b66\u4e60\u548csoftmax\u91c7\u6837\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2504.15032", "pdf": "https://arxiv.org/pdf/2504.15032", "abs": "https://arxiv.org/abs/2504.15032", "authors": ["Weijie He", "Mushui Liu", "Yunlong Yu", "Zhao Wang", "Chao Wu"], "title": "DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.", "AI": {"tldr": "DyST-XL\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e03\u5c40\u89c4\u5212\u3001\u53cc\u63d0\u793a\u63a7\u5236\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b9e\u4f53\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u591a\u5b9e\u4f53\u4ea4\u4e92\u7684\u52a8\u6001\u573a\u666f\u65f6\u5b58\u5728\u5e03\u5c40\u4e0d\u8fde\u7eed\u3001\u5b9e\u4f53\u8eab\u4efd\u6f02\u79fb\u548c\u4e0d\u5408\u7406\u7684\u4ea4\u4e92\u52a8\u6001\u95ee\u9898\u3002", "method": "DyST-XL\u7ed3\u5408\u52a8\u6001\u5e03\u5c40\u89c4\u5212\u5668\u3001\u53cc\u63d0\u793a\u63a7\u5236\u6ce8\u610f\u529b\u673a\u5236\u548c\u5b9e\u4f53\u4e00\u81f4\u6027\u7ea6\u675f\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDyST-XL\u5728\u590d\u6742\u63d0\u793a\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u586b\u8865\u4e86\u65e0\u9700\u8bad\u7ec3\u89c6\u9891\u5408\u6210\u7684\u5173\u952e\u7a7a\u767d\u3002", "conclusion": "DyST-XL\u4e3a\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2504.15270", "pdf": "https://arxiv.org/pdf/2504.15270", "abs": "https://arxiv.org/abs/2504.15270", "authors": ["Ji Qi", "Yuan Yao", "Yushi Bai", "Bin Xu", "Juanzi Li", "Zhiyuan Liu", "Tat-Seng Chua"], "title": "An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.", "AI": {"tldr": "Quicksviewer\u662f\u4e00\u79cd\u65b0\u578b\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u533a\u548c\u7edf\u4e00\u91cd\u91c7\u6837\u89c6\u9891\u5e27\uff0c\u663e\u8457\u63d0\u9ad8\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\uff0c\u51cf\u5c11\u65f6\u7a7a\u5197\u4f59\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u56fa\u5b9a\u5206\u533a\u7b56\u7565\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edfLMM\u5bf9\u89c6\u9891\u5e27\u7684\u5747\u5300\u611f\u77e5\u5728\u5904\u7406\u65f6\u95f4\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u5300\u7684\u89c6\u9891\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Gumbel Softmax\u5c06\u89c6\u9891\u52a8\u6001\u5206\u533a\u4e3a\u975e\u5747\u5300\u5bc6\u5ea6\u5757\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5757\u8fdb\u884c\u7edf\u4e00\u91cd\u91c7\u6837\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5b9e\u73b0\u4e8645\u500d\u7684\u538b\u7f29\u7387\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\uff08\u652f\u6301420\u79d2/1fps\u7684\u89c6\u9891\uff09\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u6700\u9ad88.72\u51c6\u786e\u7387\u63d0\u5347\uff09\uff0c\u5728Video-MME\u4e0a\u8fbe\u5230SOTA\u3002", "conclusion": "Quicksviewer\u7684\u52a8\u6001\u5206\u533a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u80fd\u529b\u7684\u5e42\u5f8b\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2504.15041", "pdf": "https://arxiv.org/pdf/2504.15041", "abs": "https://arxiv.org/abs/2504.15041", "authors": ["Shiben Liu", "Huijie Fan", "Qiang Wang", "Baojie Fan", "Yandong Tang", "Liangqiong Qu"], "title": "Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC outperform\nstate-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average\nmAP/R@1 on two training orders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAFC\u7684\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u7684\u63d0\u793a\u805a\u5408\u548c\u5206\u5e03\u611f\u77e5\u7684\u9057\u5fd8\u8865\u507f\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\uff08LReID\uff09\u9762\u4e34\u5728\u9002\u5e94\u65b0\u4fe1\u606f\u7684\u540c\u65f6\u4fdd\u7559\u65e7\u77e5\u8bc6\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6392\u7ec3\u548c\u6392\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff09\u5b58\u5728\u77e5\u8bc6\u9057\u5fd8\u6216\u5206\u5e03\u5b66\u4e60\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faDAFC\u6a21\u578b\uff0c\u5305\u62ec\u6587\u672c\u9a71\u52a8\u7684\u63d0\u793a\u805a\u5408\uff08TPA\uff09\u548c\u5206\u5e03\u611f\u77e5\u4e0e\u96c6\u6210\uff08DAI\uff09\uff0c\u4ee5\u53ca\u77e5\u8bc6\u5de9\u56fa\u673a\u5236\uff08KCM\uff09\uff0c\u901a\u8fc7\u8de8\u57df\u5171\u4eab\u8868\u793a\u5b66\u4e60\u548c\u57df\u7279\u5b9a\u5206\u5e03\u96c6\u6210\u89e3\u51b3\u9057\u5fd8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDAFC\u5728\u4e24\u4e2a\u8bad\u7ec3\u987a\u5e8f\u4e0a\u7684\u5e73\u5747mAP/R@1\u5206\u522b\u81f3\u5c11\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd59.8%/6.6%\u548c6.4%/6.2%\u3002", "conclusion": "DAFC\u901a\u8fc7\u5206\u5e03\u611f\u77e5\u548c\u77e5\u8bc6\u5de9\u56fa\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec8\u8eab\u884c\u4eba\u91cd\u8bc6\u522b\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.15280", "pdf": "https://arxiv.org/pdf/2504.15280", "abs": "https://arxiv.org/abs/2504.15280", "authors": ["Chun-Hsiao Yeh", "Chenyu Wang", "Shengbang Tong", "Ta-Ying Cheng", "Rouyu Wang", "Tianzhe Chu", "Yuexiang Zhai", "Yubei Chen", "Shenghua Gao", "Yi Ma"], "title": "Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://danielchyeh.github.io/All-Angles-Bench/", "summary": "Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86All-Angles Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u89c6\u89d2\u573a\u666f\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8de8\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u591a\u89c6\u89d2\u7406\u89e3\u662fMLLMs\u4f5c\u4e3a\u5177\u8eab\u4ee3\u7406\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8de8\u89c6\u89d2\u5bf9\u5e94\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b2,100\u4e2a\u591a\u89c6\u89d2\u95ee\u7b54\u5bf9\u7684\u57fa\u51c6All-Angles Bench\uff0c\u6db5\u76d66\u9879\u4efb\u52a1\uff0c\u6d4b\u8bd5\u6a21\u578b\u7684\u51e0\u4f55\u5bf9\u5e94\u80fd\u529b\u548c\u8de8\u89c6\u89d2\u4fe1\u606f\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c27\u79cd\u4ee3\u8868\u6027MLLMs\uff08\u5982Gemini-2.0-Flash\u3001Claude-3.7-Sonnet\u548cGPT-4o\uff09\u5728\u591a\u89c6\u89d2\u4efb\u52a1\u4e2d\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u906e\u6321\u89c6\u89d2\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u591a\u89c6\u89d2\u7406\u89e3\u4e0a\u4ecd\u9700\u6539\u8fdb\uff0cAll-Angles Bench\u4e3a\u9886\u57df\u4f18\u5316\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2504.15049", "pdf": "https://arxiv.org/pdf/2504.15049", "abs": "https://arxiv.org/abs/2504.15049", "authors": ["Mohamed el amine Boudjoghra", "Ivan Laptev", "Angela Dai"], "title": "ScanEdit: Hierarchically-Guided Functional 3D Scan Editing", "categories": ["cs.CV"], "comment": "Project webpage: https://aminebdj.github.io/scanedit/ Video:\n  https://www.youtube.com/watch?v=Dfmu2g6pVlg", "summary": "With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions.", "AI": {"tldr": "ScanEdit\u662f\u4e00\u79cd\u57fa\u4e8e\u6307\u4ee4\u9a71\u52a8\u76843D\u573a\u666f\u7f16\u8f91\u65b9\u6cd5\uff0c\u5229\u7528\u5c42\u6b21\u5316\u573a\u666f\u56fe\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b9e\u73b0\u9ad8\u6548\u7f16\u8f91\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u751f\u6210\u903c\u771f\u573a\u666f\u3002", "motivation": "\u968f\u77403D\u6355\u83b7\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c3D\u6570\u636e\u5927\u91cf\u6d8c\u73b0\uff0c\u9ad8\u6548\u76843D\u573a\u666f\u7f16\u8f91\u5bf9\u56fe\u5f62\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c42\u6b21\u5316\u573a\u666f\u56fe\u8868\u793a3D\u626b\u63cf\u5bf9\u8c61\uff0c\u5229\u7528LLM\u5c06\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u547d\u4ee4\uff0c\u5e76\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u751f\u6210\u573a\u666f\u3002", "result": "ScanEdit\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u771f\u5b9e\u573a\u666f\u548c\u8f93\u5165\u6307\u4ee4\u3002", "conclusion": "ScanEdit\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u903c\u771f\u76843D\u573a\u666f\u7f16\u8f91\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u6307\u4ee4\u548c\u7269\u7406\u7ea6\u675f\u3002"}}
{"id": "2504.15054", "pdf": "https://arxiv.org/pdf/2504.15054", "abs": "https://arxiv.org/abs/2504.15054", "authors": ["Xiangchen Yin", "Zhenda Yu", "Longtao Jiang", "Xin Gao", "Xiao Sun", "Zhi Liu", "Xun Yang"], "title": "Structure-guided Diffusion Transformer for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u6269\u6563\u53d8\u6362\u5668\uff08DiT\uff09\u5f15\u5165\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5f15\u5bfc\u7684\u6269\u6563\u53d8\u6362\u5668\u6846\u67b6\uff08SDTL\uff09\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u548c\u7ed3\u6784\u589e\u5f3a\u6a21\u5757\uff08SEM\uff09\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u7eb9\u7406\u589e\u5f3a\u6548\u679c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u5f53\u524d\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5728\u6062\u590d\u7ec6\u8282\u65f6\u4f1a\u653e\u5927\u566a\u58f0\uff0c\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22DiT\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51faSDTL\u6846\u67b6\uff0c\u5305\u62ec\u5c0f\u6ce2\u53d8\u6362\u538b\u7f29\u7279\u5f81\u3001\u7ed3\u6784\u589e\u5f3a\u6a21\u5757\uff08SEM\uff09\u548c\u7ed3\u6784\u5f15\u5bfc\u6ce8\u610f\u529b\u5757\uff08SAB\uff09\uff0c\u4ee5\u4f18\u5316\u7eb9\u7406\u589e\u5f3a\u548c\u566a\u58f0\u6291\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86SDTL\u5728\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548cDiT\u5728\u4f4e\u5149\u589e\u5f3a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "SDTL\u6846\u67b6\u6709\u6548\u7ed3\u5408DiT\u4e0e\u7ed3\u6784\u5f15\u5bfc\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6548\u679c\uff0c\u4e3aDiT\u5728\u6b64\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15085", "pdf": "https://arxiv.org/pdf/2504.15085", "abs": "https://arxiv.org/abs/2504.15085", "authors": ["Wangyu Wu", "Zhenhong Chen", "Siqi Song", "Xianglin Qiua", "Xiaowei Huang", "Fei Ma", "Jimin Xiao"], "title": "Hierarchical Attention Fusion of Visual and Textual Representations for Cross-Domain Sequential Recommendation", "categories": ["cs.CV"], "comment": "Accepted at CogSCI 2025", "summary": "Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAF-VT\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\u589e\u5f3a\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff0c\u5229\u7528\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002", "motivation": "\u8de8\u57df\u5e8f\u5217\u63a8\u8350\uff08CDSR\uff09\u9700\u8981\u66f4\u597d\u5730\u5efa\u6a21\u7528\u6237\u7684\u8de8\u57df\u504f\u597d\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u6765\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u51bb\u7ed3\u7684CLIP\u6a21\u578b\u751f\u6210\u56fe\u50cf\u548c\u6587\u672c\u5d4c\u5165\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u5b66\u4e60\u5355\u57df\u548c\u8de8\u57df\u504f\u597d\uff0c\u6a21\u62df\u4eba\u7c7b\u4fe1\u606f\u6574\u5408\u8fc7\u7a0b\u3002", "result": "\u5728\u56db\u4e2a\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHAF-VT\u5728\u6355\u6349\u8de8\u57df\u7528\u6237\u5174\u8da3\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HAF-VT\u6210\u529f\u5c06\u8ba4\u77e5\u539f\u7406\u4e0e\u8ba1\u7b97\u6a21\u578b\u7ed3\u5408\uff0c\u7a81\u51fa\u4e86\u591a\u6a21\u6001\u6570\u636e\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.15095", "pdf": "https://arxiv.org/pdf/2504.15095", "abs": "https://arxiv.org/abs/2504.15095", "authors": ["Mingxia Zhan", "Li Zhang", "XiaoMeng Chu", "Beibei Wang"], "title": "VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures, 4 tables", "summary": "Monocular depth estimation (MDE) aims to predict per-pixel depth values from\na single RGB image. Recent advancements have positioned diffusion models as\neffective MDE tools by framing the challenge as a conditional image generation\ntask. Despite their progress, these methods often struggle with accurately\nreconstructing distant depths, due largely to the imbalanced distribution of\ndepth values and an over-reliance on spatial-domain features. To overcome these\nlimitations, we introduce VistaDepth, a novel framework that integrates\nadaptive frequency-domain feature enhancements with an adaptive\nweight-balancing mechanism into the diffusion process. Central to our approach\nis the Latent Frequency Modulation (LFM) module, which dynamically refines\nspectral responses in the latent feature space, thereby improving the\npreservation of structural details and reducing noisy artifacts. Furthermore,\nwe implement an adaptive weighting strategy that modulates the diffusion loss\nin real-time, enhancing the model's sensitivity towards distant depth\nreconstruction. These innovations collectively result in superior depth\nperception performance across both distance and detail. Experimental\nevaluations confirm that VistaDepth achieves state-of-the-art performance among\ndiffusion-based MDE techniques, particularly excelling in the accurate\nreconstruction of distant regions.", "AI": {"tldr": "VistaDepth \u662f\u4e00\u79cd\u65b0\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9891\u7387\u57df\u7279\u5f81\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5e73\u8861\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdc\u8ddd\u79bb\u6df1\u5ea6\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u8fdc\u8ddd\u79bb\u6df1\u5ea6\u91cd\u5efa\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u6df1\u5ea6\u503c\u5206\u5e03\u4e0d\u5e73\u8861\u548c\u8fc7\u5ea6\u4f9d\u8d56\u7a7a\u95f4\u57df\u7279\u5f81\u3002", "method": "VistaDepth \u5f15\u5165\u4e86\u6f5c\u5728\u9891\u7387\u8c03\u5236\uff08LFM\uff09\u6a21\u5757\u548c\u81ea\u9002\u5e94\u6743\u91cd\u7b56\u7565\uff0c\u52a8\u6001\u4f18\u5316\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u9891\u8c31\u54cd\u5e94\u548c\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVistaDepth \u5728\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u8fdc\u8ddd\u79bb\u533a\u57df\u91cd\u5efa\u4e0a\u3002", "conclusion": "VistaDepth \u901a\u8fc7\u9891\u7387\u57df\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u611f\u77e5\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8fdc\u8ddd\u79bb\u7ec6\u8282\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.15105", "pdf": "https://arxiv.org/pdf/2504.15105", "abs": "https://arxiv.org/abs/2504.15105", "authors": ["Yurun Wang", "Zerong Qi", "Shujun Fu", "Mingzheng Hu"], "title": "A triple-branch network for latent fingerprint enhancement guided by orientation fields and minutiae", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTBSFNet\u7684\u4e09\u5206\u652f\u7a7a\u95f4\u878d\u5408\u7f51\u7edc\uff0c\u7ed3\u5408MLFGNet\u63d0\u5347\u6f5c\u5728\u6307\u7eb9\u589e\u5f3a\u6548\u679c\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u8d28\u91cf\u6307\u7eb9\u533a\u57df\u6062\u590d\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u9488\u5bf9\u4e0d\u540c\u533a\u57df\u91c7\u7528\u4e0d\u540c\u589e\u5f3a\u7b56\u7565\u3002", "method": "\u63d0\u51faTBSFNet\uff0c\u7ed3\u5408\u65b9\u5411\u573a\u548c\u7ec6\u8282\u6a21\u5757\uff0c\u5e76\u5f15\u5165MLFGNet\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728MOLF\u548cMUST\u6570\u636e\u96c6\u4e0a\uff0cMLFGNet\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u589e\u5f3a\u7b97\u6cd5\u3002", "conclusion": "TBSFNet\u548cMLFGNet\u6709\u6548\u63d0\u5347\u4e86\u6f5c\u5728\u6307\u7eb9\u589e\u5f3a\u6548\u679c\uff0c\u5c24\u5176\u9488\u5bf9\u4f4e\u8d28\u91cf\u533a\u57df\u3002"}}
{"id": "2504.15108", "pdf": "https://arxiv.org/pdf/2504.15108", "abs": "https://arxiv.org/abs/2504.15108", "authors": ["Zhenzhen Xiao", "Heng Liu", "Bingwen Hu"], "title": "Unwarping Screen Content Images via Structure-texture Enhancement Network and Transformation Self-estimation", "categories": ["cs.CV"], "comment": null, "summary": "While existing implicit neural network-based image unwarping methods perform\nwell on natural images, they struggle to handle screen content images (SCIs),\nwhich often contain large geometric distortions, text, symbols, and sharp\nedges. To address this, we propose a structure-texture enhancement network\n(STEN) with transformation self-estimation for SCI warping. STEN integrates a\nB-spline implicit neural representation module and a transformation error\nestimation and self-correction algorithm. It comprises two branches: the\nstructure estimation branch (SEB), which enhances local aggregation and global\ndependency modeling, and the texture estimation branch (TEB), which improves\ntexture detail synthesis using B-spline implicit neural representation.\nAdditionally, the transformation self-estimation module autonomously estimates\nthe transformation error and corrects the coordinate transformation matrix,\neffectively handling real-world image distortions. Extensive experiments on\npublic SCI datasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods. Comparisons on well-known natural image datasets also\nshow the potential of our approach for natural image distortion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784-\u7eb9\u7406\u589e\u5f3a\u7f51\u7edc\uff08STEN\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5c4f\u5e55\u5185\u5bb9\u56fe\u50cf\uff08SCI\uff09\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u901a\u8fc7B\u6837\u6761\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u548c\u53d8\u6362\u81ea\u4f30\u8ba1\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u5305\u542b\u5927\u51e0\u4f55\u5931\u771f\u3001\u6587\u672c\u548c\u5c16\u9510\u8fb9\u7f18\u7684SCI\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "STEN\u5305\u542b\u7ed3\u6784\u4f30\u8ba1\u5206\u652f\uff08SEB\uff09\u548c\u7eb9\u7406\u4f30\u8ba1\u5206\u652f\uff08TEB\uff09\uff0c\u5206\u522b\u589e\u5f3a\u5c40\u90e8\u805a\u5408\u4e0e\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\u53ca\u7eb9\u7406\u7ec6\u8282\u5408\u6210\uff0c\u5e76\u901a\u8fc7\u53d8\u6362\u81ea\u4f30\u8ba1\u6a21\u5757\u6821\u6b63\u5750\u6807\u53d8\u6362\u77e9\u9635\u3002", "result": "\u5728\u516c\u5f00SCI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u5728\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4e5f\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "STEN\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784-\u7eb9\u7406\u589e\u5f3a\u548c\u53d8\u6362\u81ea\u4f30\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86SCI\u7684\u5931\u771f\u95ee\u9898\uff0c\u5e76\u5177\u6709\u6269\u5c55\u5230\u81ea\u7136\u56fe\u50cf\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15118", "pdf": "https://arxiv.org/pdf/2504.15118", "abs": "https://arxiv.org/abs/2504.15118", "authors": ["Inho Kim", "Youngkil Song", "Jicheol Park", "Won Hwa Kim", "Suha Kwak"], "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio", "categories": ["cs.CV", "cs.SD"], "comment": "Accepted to CVPR 2025", "summary": "Sound source localization (SSL) is the task of locating the source of sound\nwithin an image. Due to the lack of localization labels, the de facto standard\nin SSL has been to represent an image and audio as a single embedding vector\neach, and use them to learn SSL via contrastive learning. To this end, previous\nwork samples one of local image features as the image embedding and aggregates\nall local audio features to obtain the audio embedding, which is far from\noptimal due to the presence of noise and background irrelevant to the actual\ntarget in the input. We present a novel SSL method that addresses this chronic\nissue by joint slot attention on image and audio. To be specific, two slots\ncompetitively attend image and audio features to decompose them into target and\noff-target representations, and only target representations of image and audio\nare used for contrastive learning. Also, we introduce cross-modal attention\nmatching to further align local features of image and audio. Our method\nachieved the best in almost all settings on three public benchmarks for SSL,\nand substantially outperformed all the prior work in cross-modal retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u69fd\u6ce8\u610f\u529b\u7684\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u76ee\u6807\u548c\u975e\u76ee\u6807\u8868\u793a\uff0c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u58f0\u6e90\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5b9a\u4f4d\u6807\u7b7e\uff0c\u901a\u5e38\u5c06\u56fe\u50cf\u548c\u97f3\u9891\u8868\u793a\u4e3a\u5355\u4e00\u5d4c\u5165\u5411\u91cf\uff0c\u4f46\u566a\u58f0\u548c\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u8054\u5408\u69fd\u6ce8\u610f\u529b\u5206\u89e3\u56fe\u50cf\u548c\u97f3\u9891\u7279\u5f81\u4e3a\u76ee\u6807\u548c\u975e\u76ee\u6807\u8868\u793a\uff0c\u4ec5\u4f7f\u7528\u76ee\u6807\u8868\u793a\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5339\u914d\u5bf9\u9f50\u5c40\u90e8\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51e0\u4e4e\u5168\u90e8\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u69fd\u6ce8\u610f\u529b\u548c\u8de8\u6a21\u6001\u5339\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u566a\u58f0\u548c\u80cc\u666f\u5e72\u6270\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u58f0\u6e90\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2504.15121", "pdf": "https://arxiv.org/pdf/2504.15121", "abs": "https://arxiv.org/abs/2504.15121", "authors": ["Csongor Csanad Kariko", "Muhammad Rafi Faisal", "Levente Hajder"], "title": "Robust and Real-time Surface Normal Estimation from Stereo Disparities using Affine Transformations", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77eb\u6b63\u7acb\u4f53\u56fe\u50cf\u5bf9\u7684\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u89c6\u5dee\u503c\u7684\u4eff\u5c04\u53d8\u6362\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u7ed3\u679c\u3002", "motivation": "\u77eb\u6b63\u7acb\u4f53\u56fe\u50cf\u5bf9\u7b80\u5316\u4e86\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u8fc7\u7a0b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u566a\u58f0\u95ee\u9898\u548c\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u4eff\u5c04\u53d8\u6362\u3001\u81ea\u5b9a\u4e49\u5377\u79ef\u7b97\u6cd5\u548c\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u6280\u672f\uff0c\u6784\u5efa\u5feb\u901f\u51c6\u786e\u7684\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u5668\u3002", "result": "\u5728Middlebury\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2504.15122", "pdf": "https://arxiv.org/pdf/2504.15122", "abs": "https://arxiv.org/abs/2504.15122", "authors": ["Minh-Quan Viet Bui", "Jongmin Park", "Juan Luis Gonzalez Bello", "Jaeho Moon", "Jihyong Oh", "Munchurl Kim"], "title": "MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video", "categories": ["cs.CV"], "comment": "The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work", "summary": "We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur.", "AI": {"tldr": "MoBGS\u662f\u4e00\u79cd\u65b0\u7684\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u6a21\u7cca\u7684\u5355\u76ee\u89c6\u9891\u4e2d\u91cd\u5efa\u6e05\u6670\u9ad8\u8d28\u91cf\u7684\u65f6\u7a7a\u65b0\u89c6\u89d2\u3002", "motivation": "\u73b0\u6709\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u65b9\u6cd5\u5bf9\u8fd0\u52a8\u6a21\u7cca\u654f\u611f\uff0c\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002MoBGS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e13\u6ce8\u4e8e\u52a8\u6001\u5bf9\u8c61\u7684\u8fd0\u52a8\u5efa\u6a21\u3002", "method": "MoBGS\u5f15\u5165\u4e86Blur-adaptive Latent Camera Estimation\uff08BLCE\uff09\u65b9\u6cd5\u4f30\u8ba1\u6f5c\u5728\u76f8\u673a\u8f68\u8ff9\uff0c\u6539\u8fdb\u5168\u5c40\u76f8\u673a\u8fd0\u52a8\u53bb\u6a21\u7cca\uff1b\u5e76\u63d0\u51faLatent Camera-induced Exposure Estimation\uff08LCEE\uff09\u65b9\u6cd5\u786e\u4fdd\u5168\u5c40\u76f8\u673a\u548c\u5c40\u90e8\u5bf9\u8c61\u8fd0\u52a8\u7684\u4e00\u81f4\u6027\u53bb\u6a21\u7cca\u3002", "result": "\u5728Stereo Blur\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6a21\u7cca\u89c6\u9891\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoBGS\u663e\u8457\u4f18\u4e8eDyBluRF\u548cDeblur4DGS\u7b49\u5148\u8fdb\u65b9\u6cd5\uff0c\u8fbe\u5230\u52a8\u6001NVS\u5728\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MoBGS\u901a\u8fc7BLCE\u548cLCEE\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u53bb\u6a21\u7cca\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\uff0c\u4e3a\u52a8\u6001NVS\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15134", "pdf": "https://arxiv.org/pdf/2504.15134", "abs": "https://arxiv.org/abs/2504.15134", "authors": ["Xiao Zhang", "Lu Zou", "Tao Lu", "Yuan Yao", "Zhangjin Huang", "Guoping Wang"], "title": "Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this challenge, we propose INKL-Pose, a novel category-level object\npose estimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our approach first\npredicts semantically consistent and geometric informative keypoints through an\nInstance-Adaptive Keypoint Generator, then refines them with: (1) a Local\nKeypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global\nKeypoint Feature Aggregator using bidirectional Mamba for structural\nconsistency. To enable bidirectional modeling in Mamba, we introduce a Feature\nSequence Flipping strategy that preserves spatial coherence while constructing\nbackward feature sequences. Additionally, we design a surface loss and a\nseparation loss to enforce uniform coverage and spatial diversity in keypoint\ndistribution. The generated keypoints are finally mapped to a canonical space\nfor regressing the object's 6D pose and size. Extensive experiments on\nCAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves\nstate-of-the-art performance and significantly outperforms existing methods.", "AI": {"tldr": "INKL-Pose \u662f\u4e00\u79cd\u65b0\u9896\u7684\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u5b66\u4e60\u548c\u5c40\u90e8\u5230\u5168\u5c40\u51e0\u4f55\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u51e0\u4f55\u6216\u975e\u6807\u51c6\u5f62\u72b6\u7269\u4f53\u7684\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u6216\u663e\u8457\u504f\u79bb\u6807\u51c6\u5f62\u72b6\u7684\u7269\u4f53\u5b9e\u4f8b\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5b9e\u4f8b\u5e76\u6355\u6349\u51e0\u4f55\u7ec6\u8282\u7684\u65b9\u6cd5\u3002", "method": "INKL-Pose \u901a\u8fc7\u5b9e\u4f8b\u81ea\u9002\u5e94\u5173\u952e\u70b9\u751f\u6210\u5668\u9884\u6d4b\u5173\u952e\u70b9\uff0c\u5e76\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u805a\u5408\u5668\uff08\u4f7f\u7528\u53cc\u5411 Mamba\uff09\u4f18\u5316\u5173\u952e\u70b9\uff0c\u540c\u65f6\u5f15\u5165\u8868\u9762\u635f\u5931\u548c\u5206\u79bb\u635f\u5931\u786e\u4fdd\u5173\u952e\u70b9\u5206\u5e03\u5747\u5300\u4e14\u591a\u6837\u3002", "result": "\u5728 CAMERA25\u3001REAL275 \u548c HouseCat6D \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cINKL-Pose \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "INKL-Pose \u901a\u8fc7\u81ea\u9002\u5e94\u5173\u952e\u70b9\u5b66\u4e60\u548c\u51e0\u4f55\u805a\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u590d\u6742\u5b9e\u4f8b\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.15145", "pdf": "https://arxiv.org/pdf/2504.15145", "abs": "https://arxiv.org/abs/2504.15145", "authors": ["Huzheng Yang", "Katherine Xu", "Michael D. Grossberg", "Yutong Bai", "Jianbo Shi"], "title": "\"I Know It When I See It\": Mood Spaces for Connecting and Expressing Visual Concepts", "categories": ["cs.CV"], "comment": "Project page: https://huzeyann.github.io/mspace/", "summary": "Expressing complex concepts is easy when they can be labeled or quantified,\nbut many ideas are hard to define yet instantly recognizable. We propose a Mood\nBoard, where users convey abstract concepts with examples that hint at the\nintended direction of attribute changes. We compute an underlying Mood Space\nthat 1) factors out irrelevant features and 2) finds the connections between\nimages, thus bringing relevant concepts closer. We invent a fibration\ncomputation to compress/decompress pre-trained features into/from a compact\nspace, 50-100x smaller. The main innovation is learning to mimic the pairwise\naffinity relationship of the image tokens across exemplars. To focus on the\ncoarse-to-fine hierarchical structures in the Mood Space, we compute the top\neigenvector structure from the affinity matrix and define a loss in the\neigenvector space. The resulting Mood Space is locally linear and compact,\nallowing image-level operations, such as object averaging, visual analogy, and\npose transfer, to be performed as a simple vector operation in Mood Space. Our\nlearning is efficient in computation without any fine-tuning, needs only a few\n(2-20) exemplars, and takes less than a minute to learn.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdMood Board\u65b9\u6cd5\uff0c\u901a\u8fc7\u793a\u4f8b\u4f20\u8fbe\u62bd\u8c61\u6982\u5ff5\uff0c\u5e76\u6784\u5efaMood Space\u4ee5\u538b\u7f29\u7279\u5f81\u7a7a\u95f4\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u56fe\u50cf\u7ea7\u64cd\u4f5c\u3002", "motivation": "\u8bb8\u591a\u62bd\u8c61\u6982\u5ff5\u96be\u4ee5\u5b9a\u4e49\u4f46\u6613\u4e8e\u8bc6\u522b\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u901a\u8fc7\u793a\u4f8b\u4f20\u8fbe\u8fd9\u4e9b\u6982\u5ff5\u3002", "method": "\u63d0\u51faMood Board\u548cMood Space\uff0c\u5229\u7528\u7ea4\u7ef4\u5316\u8ba1\u7b97\u538b\u7f29\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u56fe\u50cf\u6807\u8bb0\u7684\u4eb2\u548c\u5173\u7cfb\u6784\u5efa\u7d27\u51d1\u7684\u5c40\u90e8\u7ebf\u6027\u7a7a\u95f4\u3002", "result": "Mood Space\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u56fe\u50cf\u7ea7\u64cd\u4f5c\uff08\u5982\u5bf9\u8c61\u5e73\u5747\u3001\u89c6\u89c9\u7c7b\u6bd4\u548c\u59ff\u6001\u8f6c\u79fb\uff09\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u3002", "conclusion": "Mood Space\u4e3a\u62bd\u8c61\u6982\u5ff5\u7684\u4f20\u8fbe\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7d27\u51d1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15152", "pdf": "https://arxiv.org/pdf/2504.15152", "abs": "https://arxiv.org/abs/2504.15152", "authors": ["Jun Zhou", "Bingchen Gao", "Kai Wang", "Jialun Pei", "Pheng-Ann Heng", "Jing Qin"], "title": "Landmark-Free Preoperative-to-Intraoperative Registration in Laparoscopic Liver Resection", "categories": ["cs.CV", "cs.AI"], "comment": "TMI under review", "summary": "Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65e0\u6807\u8bb0\u672f\u524d-\u672f\u4e2d\u914d\u51c6\u6846\u67b6\uff0c\u5c06\u4f20\u7edf3D-2D\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a3D-3D\u914d\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89e3\u5256\u6807\u8bb0\u548c\u672f\u4e2d\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u914d\u51c6\u65b9\u6cd5\u4f9d\u8d56\u89e3\u5256\u6807\u8bb0\uff0c\u5b58\u5728\u6807\u8bb0\u5b9a\u4e49\u6a21\u7cca\u548c\u672f\u4e2d\u5f62\u72b6\u53d8\u5f62\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u624b\u672f\u6210\u529f\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6807\u8bb0\u914d\u51c6\u6846\u67b6\uff0c\u5305\u62ec\u7279\u5f81\u89e3\u8026\u53d8\u6362\u5668\u5b66\u4e60\u521a\u6027\u53d8\u6362\u548c\u7ed3\u6784\u6b63\u5219\u5316\u53d8\u5f62\u7f51\u7edc\u8c03\u6574\u672f\u524d\u6a21\u578b\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u4e34\u5e8a\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u6027\u80fd\uff0c\u5177\u6709\u6f5c\u5728\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.15155", "pdf": "https://arxiv.org/pdf/2504.15155", "abs": "https://arxiv.org/abs/2504.15155", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dynamic 3D KAN Convolution with Adaptive Grid Optimization for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nKANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an\nadaptive grid update mechanism. By introducing learnable univariate B-spline\nfunctions on network edges, specifically by flattening three-dimensional\nneighborhoods into vectors and applying B-spline-parameterized nonlinear\nactivation functions to replace the fixed linear weights of traditional 3D\nconvolutional kernels, we precisely capture complex spectral-spatial nonlinear\nrelationships in hyperspectral data. Simultaneously, through a dynamic grid\nadjustment mechanism, we adaptively update the grid point positions of\nB-splines based on the statistical characteristics of input data, optimizing\nthe resolution of spline functions to match the non-uniform distribution of\nspectral features, significantly improving the model's accuracy in\nhigh-dimensional data modeling and parameter efficiency, effectively\nalleviating the curse of dimensionality. This characteristic demonstrates\nsuperior neural scaling laws compared to traditional convolutional neural\nnetworks and reduces overfitting risks in small-sample and high-noise\nscenarios. KANet enhances model representation capability through a 3D dynamic\nexpert convolution system without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.", "AI": {"tldr": "KANet\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb3D-DenseNet\u7684\u6a21\u578b\uff0c\u901a\u8fc73D KAN Conv\u548c\u81ea\u9002\u5e94\u7f51\u683c\u66f4\u65b0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u9ad8\u7ef4\u6570\u636e\u3001\u7a00\u758f\u5206\u5e03\u548c\u5149\u8c31\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\uff0c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9762\u4e34\u9ad8\u7ef4\u6570\u636e\u3001\u7a00\u758f\u5206\u5e03\u548c\u5149\u8c31\u5197\u4f59\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5355\u53d8\u91cfB\u6837\u6761\u51fd\u6570\uff0c\u901a\u8fc7\u52a8\u6001\u7f51\u683c\u8c03\u6574\u673a\u5236\u4f18\u5316\u5206\u8fa8\u7387\uff0c\u66ff\u4ee3\u4f20\u7edf3D\u5377\u79ef\u6838\u7684\u56fa\u5b9a\u7ebf\u6027\u6743\u91cd\u3002", "result": "\u5728IN\u3001UP\u548cKSC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u7ef4\u6570\u636e\u5efa\u6a21\u7cbe\u5ea6\u548c\u53c2\u6570\u6548\u7387\u3002", "conclusion": "KANet\u901a\u8fc73D\u52a8\u6001\u4e13\u5bb6\u5377\u79ef\u7cfb\u7edf\u63d0\u5347\u4e86\u6a21\u578b\u8868\u793a\u80fd\u529b\uff0c\u65e0\u9700\u589e\u52a0\u7f51\u7edc\u6df1\u5ea6\u6216\u5bbd\u5ea6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u7ef4\u5ea6\u707e\u96be\u548c\u8fc7\u62df\u5408\u98ce\u9669\u3002"}}
{"id": "2504.15159", "pdf": "https://arxiv.org/pdf/2504.15159", "abs": "https://arxiv.org/abs/2504.15159", "authors": ["Junyuan Deng", "Xinyi Wu", "Yongxing Yang", "Congchao Zhu", "Song Wang", "Zhenyao Wu"], "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recently, pre-trained text-to-image (T2I) models have been extensively\nadopted for real-world image restoration because of their powerful generative\nprior. However, controlling these large models for image restoration usually\nrequires a large number of high-quality images and immense computational\nresources for training, which is costly and not privacy-friendly. In this\npaper, we find that the well-trained large T2I model (i.e., Flux) is able to\nproduce a variety of high-quality images aligned with real-world distributions,\noffering an unlimited supply of training samples to mitigate the above issue.\nSpecifically, we proposed a training data construction pipeline for image\nrestoration, namely FluxGen, which includes unconditional image generation,\nimage selection, and degraded image simulation. A novel light-weighted adapter\n(FluxIR) with squeeze-and-excitation layers is also carefully designed to\ncontrol the large Diffusion Transformer (DiT)-based T2I model so that\nreasonable details can be restored. Experiments demonstrate that our proposed\nmethod enables the Flux model to adapt effectively to real-world image\nrestoration tasks, achieving superior scores and visual quality on both\nsynthetic and real-world degradation datasets - at only about 8.5\\% of the\ntraining cost compared to current approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFluxGen\u7684\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u7528\u4e8e\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9002\u914d\u5668FluxIR\u63a7\u5236\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u4e0d\u53cb\u597d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684T2I\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6837\u672c\uff0c\u8bbe\u8ba1FluxGen\u6d41\u7a0b\uff08\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u3001\u56fe\u50cf\u9009\u62e9\u548c\u9000\u5316\u56fe\u50cf\u6a21\u62df\uff09\uff0c\u5e76\u5f00\u53d1\u8f7b\u91cf\u7ea7\u9002\u914d\u5668FluxIR\u63a7\u5236\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u9000\u5316\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\uff0c\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u73b0\u6709\u65b9\u6cd5\u76848.5%\u3002", "conclusion": "FluxGen\u548cFluxIR\u7684\u7ec4\u5408\u4e3a\u56fe\u50cf\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15165", "pdf": "https://arxiv.org/pdf/2504.15165", "abs": "https://arxiv.org/abs/2504.15165", "authors": ["Liu Wenbin"], "title": "An Efficient Aerial Image Detection with Variable Receptive Fields", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks.", "AI": {"tldr": "VRF-DETR\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\u548c\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5c0f\u76ee\u6807\u3001\u906e\u6321\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c0f\u76ee\u6807\uff08\u5c0f\u4e8e10\u50cf\u7d20\uff09\u3001\u5bc6\u96c6\u906e\u6321\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u7684\u6311\u6218\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "VRF-DETR\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u878d\u5408\u6a21\u5757\uff08MSCF\uff09\u3001\u95e8\u63a7\u5377\u79ef\u5c42\uff08GConv\uff09\u548c\u95e8\u63a7\u591a\u5c3a\u5ea6\u878d\u5408\u74f6\u9888\uff08GMCF\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u611f\u53d7\u91ce\u548c\u9ad8\u6548\u53c2\u6570\u8bbe\u8ba1\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728VisDrone2019\u6570\u636e\u96c6\u4e0a\uff0cVRF-DETR\u8fbe\u523051.4% mAP50\u548c31.8% mAP50:95\uff0c\u4ec5\u970013.5M\u53c2\u6570\u3002", "conclusion": "VRF-DETR\u4e3a\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u6548\u7387-\u7cbe\u5ea6\u5e73\u8861\u6807\u51c6\u3002"}}
{"id": "2504.15170", "pdf": "https://arxiv.org/pdf/2504.15170", "abs": "https://arxiv.org/abs/2504.15170", "authors": ["Chengxi Han", "Xiaoyu Su", "Zhiqiang Wei", "Meiqi Hu", "Yichu Xu"], "title": "HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change Detection", "categories": ["cs.CV"], "comment": null, "summary": "The remote sensing image change detection task is an essential method for\nlarge-scale monitoring. We propose HSANet, a network that uses hierarchical\nconvolution to extract multi-scale features. It incorporates hybrid\nself-attention and cross-attention mechanisms to learn and fuse global and\ncross-scale information. This enables HSANet to capture global context at\ndifferent scales and integrate cross-scale features, refining edge details and\nimproving detection performance. We will also open-source our model code:\nhttps://github.com/ChengxiHAN/HSANet.", "AI": {"tldr": "HSANet\u662f\u4e00\u79cd\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u5c42\u5377\u79ef\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u662f\u5927\u89c4\u6a21\u76d1\u6d4b\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u9700\u8981\u9ad8\u6548\u6355\u6349\u591a\u5c3a\u5ea6\u4fe1\u606f\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "method": "HSANet\u91c7\u7528\u5206\u5c42\u5377\u79ef\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u6df7\u5408\u81ea\u6ce8\u610f\u529b\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b66\u4e60\u5e76\u878d\u5408\u5168\u5c40\u548c\u8de8\u5c3a\u5ea6\u4fe1\u606f\u3002", "result": "HSANet\u80fd\u591f\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u6574\u5408\u8de8\u5c3a\u5ea6\u7279\u5f81\uff0c\u4f18\u5316\u8fb9\u7f18\u7ec6\u8282\uff0c\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "HSANet\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u7684\u6548\u679c\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u4ee3\u7801\u3002"}}
{"id": "2504.15176", "pdf": "https://arxiv.org/pdf/2504.15176", "abs": "https://arxiv.org/abs/2504.15176", "authors": ["Miaomiao Cai", "Simiao Li", "Wei Li", "Xudong Huang", "Hanting Chen", "Jie Hu", "Yunhe Wang"], "title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have improved Real-World Image\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\nintegration, risking misalignment with human preference and may leading to\nartifacts, hallucinations and harmful content generation. To this end, we are\nthe first to introduce human preference alignment into Real-ISR, a technique\nthat has been successfully applied in Large Language Models and Text-to-Image\ntasks to effectively enhance the alignment of generated outputs with human\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\ntechnique that directly learns from the human preference dataset. Nevertheless,\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\nare difficult to reconcile with the image-level preferences of DPO, which can\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\nPreference Optimization (DSPO) to align instance-level human preferences by\nincorporating semantic guidance, which is through two strategies: (a) semantic\ninstance alignment strategy, implementing instance-level alignment to ensure\nfine-grained perceptual consistency, and (b) user description feedback\nstrategy, mitigating hallucinations through semantic textual feedback on\ninstance-level images. As a plug-and-play solution, DSPO proves highly\neffective in both one-step and multi-step SR frameworks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u5c06\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u6280\u672f\u5f15\u5165Real-ISR\uff0c\u63d0\u51faDirect Semantic Preference Optimization (DSPO)\u4ee5\u89e3\u51b3\u50cf\u7d20\u7ea7\u91cd\u5efa\u76ee\u6807\u4e0e\u56fe\u50cf\u7ea7\u504f\u597d\u7684\u51b2\u7a81\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6307\u5bfc\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709Real-ISR\u65b9\u6cd5\u7f3a\u4e4f\u4eba\u7c7b\u53cd\u9988\u96c6\u6210\uff0c\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e0d\u4e00\u81f4\uff0c\u751a\u81f3\u4ea7\u751f\u4f2a\u5f71\u548c\u6709\u5bb3\u5185\u5bb9\u3002", "method": "\u5f15\u5165Direct Preference Optimization (DPO)\u5e76\u6539\u8fdb\u4e3aDSPO\uff0c\u901a\u8fc7\u8bed\u4e49\u5b9e\u4f8b\u5bf9\u9f50\u548c\u7528\u6237\u63cf\u8ff0\u53cd\u9988\u7b56\u7565\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u504f\u597d\u5bf9\u9f50\u3002", "result": "DSPO\u5728\u5355\u6b65\u548c\u591a\u6b65\u8d85\u5206\u8fa8\u7387\u6846\u67b6\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002", "conclusion": "DSPO\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u63d0\u5347\u4e86Real-ISR\u751f\u6210\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.15179", "pdf": "https://arxiv.org/pdf/2504.15179", "abs": "https://arxiv.org/abs/2504.15179", "authors": ["Fei Yin", "Mallikarjun B R", "Chun-Han Yao", "Rafa\u0142 Mantiuk", "Varun Jampani"], "title": "FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel framework for generating high-quality, animatable 4D\navatar from a single image. While recent advances have shown promising results\nin 4D avatar creation, existing methods either require extensive multiview data\nor struggle with shape accuracy and identity consistency. To address these\nlimitations, we propose a comprehensive system that leverages shape, image, and\nvideo priors to create full-view, animatable avatars. Our approach first\nobtains initial coarse shape through 3D-GAN inversion. Then, it enhances\nmultiview textures using depth-guided warping signals for cross-view\nconsistency with the help of the image diffusion model. To handle expression\nanimation, we incorporate a video prior with synchronized driving signals\nacross viewpoints. We further introduce a Consistent-Inconsistent training to\neffectively handle data inconsistencies during 4D reconstruction. Experimental\nresults demonstrate that our method achieves superior quality compared to the\nprior art, while maintaining consistency across different viewpoints and\nexpressions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u52a8\u753b4D\u5934\u50cf\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u591a\u89c6\u56fe\u6570\u636e\u4f9d\u8d56\u6216\u5f62\u72b6\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u67094D\u5934\u50cf\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u591a\u89c6\u56fe\u6570\u636e\u6216\u96be\u4ee5\u4fdd\u8bc1\u5f62\u72b6\u7cbe\u5ea6\u548c\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u5229\u7528\u5f62\u72b6\u3001\u56fe\u50cf\u548c\u89c6\u9891\u5148\u9a8c\u7684\u7cfb\u7edf\u3002", "method": "\u901a\u8fc73D-GAN\u53cd\u6f14\u83b7\u53d6\u521d\u59cb\u7c97\u5f62\u72b6\uff0c\u5229\u7528\u6df1\u5ea6\u5f15\u5bfc\u7684\u53d8\u5f62\u4fe1\u53f7\u548c\u56fe\u50cf\u6269\u6563\u6a21\u578b\u589e\u5f3a\u591a\u89c6\u56fe\u7eb9\u7406\u4e00\u81f4\u6027\uff0c\u7ed3\u5408\u89c6\u9891\u5148\u9a8c\u5904\u7406\u8868\u60c5\u52a8\u753b\uff0c\u5e76\u5f15\u5165\u4e00\u81f4\u6027-\u4e0d\u4e00\u81f4\u6027\u8bad\u7ec3\u89e3\u51b34D\u91cd\u5efa\u4e2d\u7684\u6570\u636e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u591a\u89c6\u89d2\u3001\u8868\u60c5\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u52a8\u753b4D\u5934\u50cf\uff0c\u5e76\u5728\u4e00\u81f4\u6027\u548c\u52a8\u753b\u6548\u679c\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.15182", "pdf": "https://arxiv.org/pdf/2504.15182", "abs": "https://arxiv.org/abs/2504.15182", "authors": ["Xianpan Zhou"], "title": "Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC Platform", "categories": ["cs.CV"], "comment": "Project page: https://tinytigerpan.github.io/tiger200k/", "summary": "The recent surge in open-source text-to-video generation models has\nsignificantly energized the research community, yet their dependence on\nproprietary training datasets remains a key constraint. While existing open\ndatasets like Koala-36M employ algorithmic filtering of web-scraped videos from\nearly platforms, they still lack the quality required for fine-tuning advanced\nvideo generation models. We present Tiger200K, a manually curated high visual\nquality video dataset sourced from User-Generated Content (UGC) platforms. By\nprioritizing visual fidelity and aesthetic quality, Tiger200K underscores the\ncritical role of human expertise in data curation, and providing high-quality,\ntemporally consistent video-text pairs for fine-tuning and optimizing video\ngeneration architectures through a simple but effective pipeline including shot\nboundary detection, OCR, border detecting, motion filter and fine bilingual\ncaption. The dataset will undergo ongoing expansion and be released as an\nopen-source initiative to advance research and applications in video generative\nmodels. Project page: https://tinytigerpan.github.io/tiger200k/", "AI": {"tldr": "Tiger200K\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u624b\u52a8\u6574\u7406\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u5f00\u6e90\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5bf9\u4e13\u6709\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6570\u636e\u96c6\u8d28\u91cf\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9ad8\u7ea7\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5fae\u8c03\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u6574\u7406\u7528\u6237\u751f\u6210\u5185\u5bb9\uff08UGC\uff09\uff0c\u91c7\u7528\u5305\u62ec\u955c\u5934\u8fb9\u754c\u68c0\u6d4b\u3001OCR\u3001\u8fb9\u6846\u68c0\u6d4b\u3001\u8fd0\u52a8\u8fc7\u6ee4\u548c\u53cc\u8bed\u5b57\u5e55\u7b49\u7b80\u5355\u4f46\u6709\u6548\u7684\u6d41\u7a0b\u3002", "result": "Tiger200K\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u4e00\u81f4\u89c6\u9891-\u6587\u672c\u5bf9\uff0c\u652f\u6301\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u4f18\u5316\u3002", "conclusion": "Tiger200K\u5c06\u4f5c\u4e3a\u5f00\u6e90\u9879\u76ee\u6301\u7eed\u6269\u5c55\uff0c\u63a8\u52a8\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2504.15192", "pdf": "https://arxiv.org/pdf/2504.15192", "abs": "https://arxiv.org/abs/2504.15192", "authors": ["Yaqian Chen", "Lin Li", "Hanxue Gu", "Haoyu Dong", "Derek L. Nguyen", "Allan D. Kirk", "Maciej A. Mazurowski", "E. Shelley Hwang"], "title": "Breast density in MRI: an AI-based quantification and relationship to assessment in mammography", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Mammographic breast density is a well-established risk factor for breast\ncancer. Recently there has been interest in breast MRI as an adjunct to\nmammography, as this modality provides an orthogonal and highly quantitative\nassessment of breast tissue. However, its 3D nature poses analytic challenges\nrelated to delineating and aggregating complex structures across slices. Here,\nwe applied an in-house machine-learning algorithm to assess breast density on\nnormal breasts in three MRI datasets. Breast density was consistent across\ndifferent datasets (0.104 - 0.114). Analysis across different age groups also\ndemonstrated strong consistency across datasets and confirmed a trend of\ndecreasing density with age as reported in previous studies. MR breast density\nwas correlated with mammographic breast density, although some notable\ndifferences suggest that certain breast density components are captured only on\nMRI. Future work will determine how to integrate MR breast density with current\ntools to improve future breast cancer risk prediction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8bc4\u4f30MRI\u6570\u636e\u4e2d\u7684\u4e73\u817a\u5bc6\u5ea6\uff0c\u53d1\u73b0\u5176\u4e0e\u4e73\u817aX\u7ebf\u5bc6\u5ea6\u76f8\u5173\uff0c\u4f46MRI\u80fd\u6355\u6349\u5230\u72ec\u7279\u4fe1\u606f\uff0c\u672a\u6765\u53ef\u80fd\u7528\u4e8e\u6539\u8fdb\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\u3002", "motivation": "\u4e73\u817a\u5bc6\u5ea6\u662f\u4e73\u817a\u764c\u7684\u91cd\u8981\u98ce\u9669\u56e0\u7d20\uff0cMRI\u4f5c\u4e3a\u8f85\u52a9\u624b\u6bb5\u80fd\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u4f46\u51763D\u7279\u6027\u5e26\u6765\u5206\u6790\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5185\u90e8\u5f00\u53d1\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5206\u6790\u4e09\u4e2aMRI\u6570\u636e\u96c6\u4e2d\u7684\u4e73\u817a\u5bc6\u5ea6\u3002", "result": "\u4e73\u817a\u5bc6\u5ea6\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u5e74\u9f84\u7ec4\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u4e14\u4e0e\u4e73\u817aX\u7ebf\u5bc6\u5ea6\u76f8\u5173\uff0c\u4f46MRI\u80fd\u6355\u6349\u5230\u72ec\u7279\u4fe1\u606f\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u63a2\u7d22\u5982\u4f55\u6574\u5408MRI\u4e73\u817a\u5bc6\u5ea6\u4ee5\u6539\u8fdb\u4e73\u817a\u764c\u98ce\u9669\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2504.15193", "pdf": "https://arxiv.org/pdf/2504.15193", "abs": "https://arxiv.org/abs/2504.15193", "authors": ["Neelesh Kumar", "Oya Aran"], "title": "Automated Measurement of Eczema Severity with Self-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u6e7f\u75b9\u81ea\u52a8\u8bca\u65ad\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6e7f\u75b9\u8bca\u65ad\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u4f7f\u7528SegGPT\u8fdb\u884c\u5c11\u6837\u672c\u6e7f\u75b9\u533a\u57df\u5206\u5272\uff1b2) \u63d0\u53d6DINO\u7279\u5f81\u5e76\u901a\u8fc7MLP\u8fdb\u884c\u6e7f\u75b9\u4e25\u91cd\u7a0b\u5ea6\u5206\u7c7b\u3002", "result": "\u5728\u91ce\u5916\u6e7f\u75b9\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u52a0\u6743F1\u5f97\u5206\u4e3a0.67\u00b10.01\uff0c\u4f18\u4e8eResnet-18\u548cVision Transformer\u3002", "conclusion": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u76ae\u80a4\u8bca\u65ad\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.15199", "pdf": "https://arxiv.org/pdf/2504.15199", "abs": "https://arxiv.org/abs/2504.15199", "authors": ["Yassir Benhammou", "Alessandro Tiberio", "Gabriel Trautmann", "Suman Kalyan"], "title": "Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.PF"], "comment": "9 pages, 2 tables, 1 figure", "summary": "MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.", "AI": {"tldr": "MILS\u6846\u67b6\u58f0\u79f0\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u591a\u6a21\u6001\u4efb\u52a1\uff0c\u4f46\u5176\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u5e26\u6765\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u800c\u5176\u4ed6\u6a21\u578b\uff08\u5982BLIP-2\u548cGPT-4V\uff09\u901a\u8fc7\u5355\u6b21\u5904\u7406\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "motivation": "\u63ed\u793aMILS\u6846\u67b6\u5728\u96f6\u6837\u672c\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u9690\u85cf\u8ba1\u7b97\u6210\u672c\uff0c\u6311\u6218\u5176\u201c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u201d\u7684\u8bf4\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4MILS\u4e0eBLIP-2\u3001GPT-4V\u7b49\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\uff0c\u91cf\u5316\u5176\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u7684\u8d44\u6e90\u6d88\u8017\u3002", "result": "MILS\u7684\u9ad8\u6027\u80fd\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684\u591a\u6b65\u4f18\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u9ad8\u4e8e\u5355\u6b21\u5904\u7406\u7684\u66ff\u4ee3\u6a21\u578b\u3002", "conclusion": "MILS\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u53ef\u80fd\u56e0\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u800c\u53d7\u9650\uff0c\u9700\u5728\u8bbe\u8ba1\u591a\u6a21\u6001\u6a21\u578b\u65f6\u6743\u8861\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2504.15232", "pdf": "https://arxiv.org/pdf/2504.15232", "abs": "https://arxiv.org/abs/2504.15232", "authors": ["Xiaoyu Han", "Shunyuan Zheng", "Zonglin Li", "Chenyang Wang", "Xin Sun", "Quanling Meng"], "title": "Shape-Guided Clothing Warping for Virtual Try-On", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2024. The code is available at\n  https://github.com/xyhanHIT/SCW-VTON", "summary": "Image-based virtual try-on aims to seamlessly fit in-shop clothing to a\nperson image while maintaining pose consistency. Existing methods commonly\nemploy the thin plate spline (TPS) transformation or appearance flow to deform\nin-shop clothing for aligning with the person's body. Despite their promising\nperformance, these methods often lack precise control over fine details,\nleading to inconsistencies in shape between clothing and the person's body as\nwell as distortions in exposed limb regions. To tackle these challenges, we\npropose a novel shape-guided clothing warping method for virtual try-on, dubbed\nSCW-VTON, which incorporates global shape constraints and additional limb\ntextures to enhance the realism and consistency of the warped clothing and\ntry-on results. To integrate global shape constraints for clothing warping, we\ndevise a dual-path clothing warping module comprising a shape path and a flow\npath. The former path captures the clothing shape aligned with the person's\nbody, while the latter path leverages the mapping between the pre- and\npost-deformation of the clothing shape to guide the estimation of appearance\nflow. Furthermore, to alleviate distortions in limb regions of try-on results,\nwe integrate detailed limb guidance by developing a limb reconstruction network\nbased on masked image modeling. Through the utilization of SCW-VTON, we are\nable to generate try-on results with enhanced clothing shape consistency and\nprecise control over details. Extensive experiments demonstrate the superiority\nof our approach over state-of-the-art methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCW-VTON\u7684\u5f62\u72b6\u5f15\u5bfc\u670d\u88c5\u53d8\u5f62\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u5c40\u5f62\u72b6\u7ea6\u675f\u548c\u80a2\u4f53\u7eb9\u7406\u589e\u5f3a\u865a\u62df\u8bd5\u7a7f\u7684\u73b0\u5b9e\u611f\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u670d\u88c5\u53d8\u5f62\u65f6\u7f3a\u4e4f\u5bf9\u7ec6\u8282\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u5bfc\u81f4\u670d\u88c5\u4e0e\u8eab\u4f53\u5f62\u72b6\u4e0d\u4e00\u81f4\u4ee5\u53ca\u80a2\u4f53\u533a\u57df\u53d8\u5f62\u3002", "method": "\u8bbe\u8ba1\u4e86\u53cc\u8def\u5f84\u670d\u88c5\u53d8\u5f62\u6a21\u5757\uff08\u5f62\u72b6\u8def\u5f84\u548c\u6d41\u8def\u5f84\uff09\u548c\u80a2\u4f53\u91cd\u5efa\u7f51\u7edc\uff0c\u7ed3\u5408\u5168\u5c40\u5f62\u72b6\u7ea6\u675f\u548c\u80a2\u4f53\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSCW-VTON\u5728\u670d\u88c5\u5f62\u72b6\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u63a7\u5236\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SCW-VTON\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u8bd5\u7a7f\u7684\u73b0\u5b9e\u611f\u548c\u4e00\u81f4\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.15259", "pdf": "https://arxiv.org/pdf/2504.15259", "abs": "https://arxiv.org/abs/2504.15259", "authors": ["Yunxuan Cai", "Sitao Xiang", "Zongjian Li", "Haiwei Chen", "Yajie Zhao"], "title": "Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u7f51\u7edc\u7684\u8bed\u4e49\u53ef\u63a7\u6570\u5b57\u4eba\u8138\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf3D\u4eba\u8138\u6570\u636e\u5e93\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684GAN\u751f\u6210\u5668\uff0c\u652f\u6301\u8bed\u4e49\u5c5e\u6027\u8f93\u5165\u548c\u540e\u671f\u7f16\u8f91\u3002", "motivation": "\u4f20\u7edf\u6570\u5b57\u4eba\u8138\u5efa\u6a21\u53d7\u9650\u4e8e\u6570\u636e\u91c7\u96c6\u8bbe\u5907\u3001\u4eba\u5de5\u52b3\u52a8\u548c\u5408\u9002\u6f14\u5458\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u6a21\u578b\u591a\u6837\u6027\u3001\u8868\u73b0\u529b\u548c\u63a7\u5236\u529b\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u7f51\u7edc\u63d0\u5347\u5efa\u6a21\u8fc7\u7a0b\u7684\u63a7\u5236\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u521b\u5efa\u9ad8\u8d28\u91cf3D\u4eba\u8138\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u5f52\u4e00\u5316\u6a21\u5757\u5c06\u5408\u6210\u6570\u636e\u8f6c\u6362\u4e3a\u626b\u63cf\u6570\u636e\u3002\u5f00\u53d1\u4e86\u57fa\u4e8eGAN\u7684\u751f\u6210\u5668\uff0c\u652f\u6301\u8bed\u4e49\u5c5e\u6027\u8f93\u5165\u548c\u540e\u671f\u7f16\u8f91\u3002", "result": "\u751f\u6210\u4e8644,000\u4e2a\u4eba\u8138\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u751f\u6210\u5668\u548c\u8d44\u4ea7\u7ec6\u5316\u7ec4\u4ef6\uff0c\u6784\u5efa\u4e86\u5b8c\u6574\u7684\u7cfb\u7edf\u5e76\u96c6\u6210\u5230\u57fa\u4e8e\u7f51\u7edc\u7684\u4ea4\u4e92\u5de5\u5177\u4e2d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b57\u4eba\u8138\u5efa\u6a21\u7684\u63a7\u5236\u529b\u548c\u591a\u6837\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u5de5\u5177\u5c06\u968f\u8bba\u6587\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2504.15267", "pdf": "https://arxiv.org/pdf/2504.15267", "abs": "https://arxiv.org/abs/2504.15267", "authors": ["Shaorong Zhang", "Tamoghna Chattopadhyay", "Sophia I. Thomopoulos", "Jose-Luis Ambite", "Paul M. Thompson", "Greg Ver Steeg"], "title": "Diffusion Bridge Models for 3D Medical Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion tensor imaging (DTI) provides crucial insights into the\nmicrostructure of the human brain, but it can be time-consuming to acquire\ncompared to more readily available T1-weighted (T1w) magnetic resonance imaging\n(MRI). To address this challenge, we propose a diffusion bridge model for 3D\nbrain image translation between T1w MRI and DTI modalities. Our model learns to\ngenerate high-quality DTI fractional anisotropy (FA) images from T1w images and\nvice versa, enabling cross-modality data augmentation and reducing the need for\nextensive DTI acquisition. We evaluate our approach using perceptual\nsimilarity, pixel-level agreement, and distributional consistency metrics,\ndemonstrating strong performance in capturing anatomical structures and\npreserving information on white matter integrity. The practical utility of the\nsynthetic data is validated through sex classification and Alzheimer's disease\nclassification tasks, where the generated images achieve comparable performance\nto real data. Our diffusion bridge model offers a promising solution for\nimproving neuroimaging datasets and supporting clinical decision-making, with\nthe potential to significantly impact neuroimaging research and clinical\npractice.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u6865\u6a21\u578b\uff0c\u7528\u4e8e\u5728T1w MRI\u548cDTI\u6a21\u6001\u4e4b\u95f4\u8fdb\u884c3D\u8111\u56fe\u50cf\u8f6c\u6362\uff0c\u4ee5\u51cf\u5c11DTI\u91c7\u96c6\u65f6\u95f4\u5e76\u652f\u6301\u8de8\u6a21\u6001\u6570\u636e\u589e\u5f3a\u3002", "motivation": "DTI\u6210\u50cf\u8017\u65f6\u8f83\u957f\uff0c\u800cT1w MRI\u66f4\u6613\u83b7\u53d6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e24\u8005\u4e4b\u95f4\u8fdb\u884c\u9ad8\u6548\u8f6c\u6362\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6865\u6a21\u578b\u4eceT1w\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684DTI FA\u56fe\u50cf\uff0c\u53cd\u4e4b\u4ea6\u7136\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u6307\u6807\u8bc4\u4f30\u751f\u6210\u6548\u679c\u3002", "result": "\u6a21\u578b\u5728\u6355\u6349\u89e3\u5256\u7ed3\u6784\u548c\u4fdd\u7559\u767d\u8d28\u5b8c\u6574\u6027\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u56fe\u50cf\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e0e\u771f\u5b9e\u6570\u636e\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u6539\u8fdb\u548c\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u80fd\u5bf9\u7814\u7a76\u548c\u4e34\u5e8a\u5b9e\u8df5\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002"}}
{"id": "2504.15271", "pdf": "https://arxiv.org/pdf/2504.15271", "abs": "https://arxiv.org/abs/2504.15271", "authors": ["Guo Chen", "Zhiqi Li", "Shihao Wang", "Jindong Jiang", "Yicheng Liu", "Lidong Lu", "De-An Huang", "Wonmin Byeon", "Matthieu Le", "Tuomas Rintamaki", "Tyler Poon", "Max Ehrlich", "Tuomas Rintamaki", "Tyler Poon", "Tong Lu", "Limin Wang", "Bryan Catanzaro", "Jan Kautz", "Andrew Tao", "Zhiding Yu", "Guilin Liu"], "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.", "AI": {"tldr": "Eagle 2.5\u662f\u4e00\u4e2a\u524d\u6cbf\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\uff0c\u4e13\u6ce8\u4e8e\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u901a\u8fc7\u81ea\u52a8\u964d\u7ea7\u91c7\u6837\u548c\u56fe\u50cf\u533a\u57df\u4fdd\u7559\u6280\u672f\u63d0\u5347\u957f\u89c6\u9891\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u8bad\u7ec3\u4e2d\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u7406\u89e3\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7406\u89e3\u4e2d\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u5305\u542b\u81ea\u52a8\u964d\u7ea7\u91c7\u6837\u548c\u56fe\u50cf\u533a\u57df\u4fdd\u7559\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u4f18\u5316\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u8bad\u7ec3\u7684\u6548\u7387\u3002", "result": "Eagle 2.5-8B\u5728Video-MME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523072.4%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u9876\u7ea7\u5546\u4e1a\u548c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "Eagle 2.5\u4e3a\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.15278", "pdf": "https://arxiv.org/pdf/2504.15278", "abs": "https://arxiv.org/abs/2504.15278", "authors": ["Hongchi Xia", "Entong Su", "Marius Memmel", "Arhan Jain", "Raymond Yu", "Numfor Mbiziwo-Tiapo", "Ali Farhadi", "Abhishek Gupta", "Shenlong Wang", "Wei-Chiu Ma"], "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://drawer-art.github.io/", "summary": "Creating virtual digital replicas from real-world data unlocks significant\npotential across domains like gaming and robotics. In this paper, we present\nDRAWER, a novel framework that converts a video of a static indoor scene into a\nphotorealistic and interactive digital environment. Our approach centers on two\nmain contributions: (i) a reconstruction module based on a dual scene\nrepresentation that reconstructs the scene with fine-grained geometric details,\nand (ii) an articulation module that identifies articulation types and hinge\npositions, reconstructs simulatable shapes and appearances and integrates them\ninto the scene. The resulting virtual environment is photorealistic,\ninteractive, and runs in real time, with compatibility for game engines and\nrobotic simulation platforms. We demonstrate the potential of DRAWER by using\nit to automatically create an interactive game in Unreal Engine and to enable\nreal-to-sim-to-real transfer for robotics applications.", "AI": {"tldr": "DRAWER\u6846\u67b6\u5c06\u9759\u6001\u5ba4\u5185\u573a\u666f\u89c6\u9891\u8f6c\u6362\u4e3a\u903c\u771f\u3001\u4ea4\u4e92\u5f0f\u7684\u6570\u5b57\u73af\u5883\uff0c\u652f\u6301\u6e38\u620f\u5f15\u64ce\u548c\u673a\u5668\u4eba\u4eff\u771f\u5e73\u53f0\u3002", "motivation": "\u901a\u8fc7\u521b\u5efa\u865a\u62df\u6570\u5b57\u526f\u672c\uff0c\u91ca\u653e\u6e38\u620f\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u53cc\u573a\u666f\u8868\u793a\u7684\u91cd\u5efa\u6a21\u5757\u548c\u5173\u8282\u6a21\u5757\uff0c\u5206\u522b\u5904\u7406\u51e0\u4f55\u7ec6\u8282\u548c\u4ea4\u4e92\u529f\u80fd\u3002", "result": "\u751f\u6210\u7684\u865a\u62df\u73af\u5883\u903c\u771f\u3001\u4ea4\u4e92\u6027\u5f3a\uff0c\u4e14\u5b9e\u65f6\u8fd0\u884c\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u548c\u673a\u5668\u4eba\u4eff\u771f\u3002", "conclusion": "DRAWER\u5c55\u793a\u4e86\u5728\u6e38\u620f\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u81ea\u52a8\u521b\u5efa\u4ea4\u4e92\u5f0f\u73af\u5883\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.15279", "pdf": "https://arxiv.org/pdf/2504.15279", "abs": "https://arxiv.org/abs/2504.15279", "authors": ["Weiye Xu", "Jiahao Wang", "Weiyun Wang", "Zhe Chen", "Wengang Zhou", "Aijun Yang", "Lewei Lu", "Houqiang Li", "Xiaohua Wang", "Xizhou Zhu", "Wenhai Wang", "Jifeng Dai", "Jinguo Zhu"], "title": "VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models", "categories": ["cs.CV"], "comment": "Code, data, and baselines are available at\n  https://visulogic-benchmark.github.io/VisuLogic", "summary": "Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.", "AI": {"tldr": "VisuLogic\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\uff0c\u5305\u542b1000\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u771f\u5b9e\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dMLLMs\u7684\u63a8\u7406\u8bc4\u4f30\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\uff0c\u5b58\u5728\u8bed\u8a00\u63a8\u7406\u6377\u5f84\uff0c\u65e0\u6cd5\u771f\u6b63\u8861\u91cf\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faVisuLogic\u57fa\u51c6\uff0c\u5305\u542b\u516d\u7c7b\u95ee\u9898\uff0c\u8bc4\u4f30MLLMs\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u591a\u6570MLLMs\u51c6\u786e\u7387\u4f4e\u4e8e30%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768451.4%\uff0c\u663e\u793a\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "VisuLogic\u63ed\u793a\u4e86MLLMs\u5728\u89c6\u89c9\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u8865\u5145\u8bad\u7ec3\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u4ee5\u63a8\u52a8\u8fdb\u6b65\u3002"}}
{"id": "2504.15281", "pdf": "https://arxiv.org/pdf/2504.15281", "abs": "https://arxiv.org/abs/2504.15281", "authors": ["Cailin Zhuang", "Yaoqi Hu", "Xuanyang Zhang", "Wei Cheng", "Jiacheng Bao", "Shengqi Liu", "Yiying Yang", "Xianfang Zeng", "Gang Yu", "Ming Li"], "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians", "categories": ["cs.CV"], "comment": "16 pages; Project page: https://styleme3d.github.io/", "summary": "3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.", "AI": {"tldr": "StyleMe3D\u662f\u4e00\u4e2a\u7528\u4e8e3D\u9ad8\u65af\u6e85\u5c04\uff083DGS\uff09\u98ce\u683c\u8fc1\u79fb\u7684\u6574\u4f53\u6846\u67b6\uff0c\u89e3\u51b3\u4e863DGS\u5728\u98ce\u683c\u5316\u573a\u666f\u4e2d\u7684\u95ee\u9898\uff0c\u5982\u7eb9\u7406\u788e\u7247\u5316\u548c\u8bed\u4e49\u4e0d\u5bf9\u9f50\u3002", "motivation": "3DGS\u5728\u771f\u5b9e\u573a\u666f\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u98ce\u683c\u5316\u573a\u666f\uff08\u5982\u5361\u901a\u3001\u6e38\u620f\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u7eb9\u7406\u788e\u7247\u5316\u3001\u8bed\u4e49\u4e0d\u5bf9\u9f50\u548c\u62bd\u8c61\u7f8e\u5b66\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "StyleMe3D\u7ed3\u5408\u591a\u6a21\u6001\u98ce\u683c\u6761\u4ef6\u3001\u591a\u7ea7\u8bed\u4e49\u5bf9\u9f50\u548c\u611f\u77e5\u8d28\u91cf\u589e\u5f3a\uff0c\u63d0\u51fa\u4e86\u56db\u4e2a\u65b0\u7ec4\u4ef6\uff1a\u52a8\u6001\u98ce\u683c\u5206\u6570\u84b8\u998f\uff08DSSD\uff09\u3001\u5bf9\u6bd4\u98ce\u683c\u63cf\u8ff0\u7b26\uff08CSD\uff09\u3001\u540c\u65f6\u4f18\u5316\u5c3a\u5ea6\uff08SOS\uff09\u548c3D\u9ad8\u65af\u8d28\u91cf\u8bc4\u4f30\uff083DG-QA\uff09\u3002", "result": "\u5728NeRF\u5408\u6210\u6570\u636e\u96c6\u548ctandt db\u6570\u636e\u96c6\u4e0a\uff0cStyleMe3D\u5728\u4fdd\u7559\u51e0\u4f55\u7ec6\u8282\u548c\u786e\u4fdd\u98ce\u683c\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u771f\u5b9e3DGS\u4e0e\u827a\u672f\u98ce\u683c\u5316\u7ed3\u5408\uff0c\u4e3a\u6e38\u620f\u3001\u865a\u62df\u4e16\u754c\u548c\u6570\u5b57\u827a\u672f\u5f00\u8f9f\u4e86\u65b0\u5e94\u7528\u3002"}}
{"id": "2504.13866", "pdf": "https://arxiv.org/pdf/2504.13866", "abs": "https://arxiv.org/abs/2504.13866", "authors": ["Aleksa Marusic", "Sao Mai Nguyen", "Adriana Tapus"], "title": "Skeleton-Based Transformer for Classification of Errors and Better Feedback in Low Back Pain Physical Rehabilitation Exercises", "categories": ["cs.HC", "cs.AI", "cs.CV", "cs.RO"], "comment": "ICORR 2025 - 19th IEEE/RAS-EMBS International Conference on\n  Rehabilitation Robotics, INTERNATIONAL CONSORTIUM FOR REHABILITATION\n  ROBOTICS, May 2025, Michigan, USA, United States", "summary": "Physical rehabilitation exercises suggested by healthcare professionals can\nhelp recovery from various musculoskeletal disorders and prevent re-injury.\nHowever, patients' engagement tends to decrease over time without direct\nsupervision, which is why there is a need for an automated monitoring system.\nIn recent years, there has been great progress in quality assessment of\nphysical rehabilitation exercises. Most of them only provide a binary\nclassification if the performance is correct or incorrect, and a few provide a\ncontinuous score. This information is not sufficient for patients to improve\ntheir performance. In this work, we propose an algorithm for error\nclassification of rehabilitation exercises, thus making the first step toward\nmore detailed feedback to patients. We focus on skeleton-based exercise\nassessment, which utilizes human pose estimation to evaluate motion. Inspired\nby recent algorithms for quality assessment during rehabilitation exercises, we\npropose a Transformer-based model for the described classification. Our model\nis inspired by the HyperFormer method for human action recognition, and adapted\nto our problem and dataset. The evaluation is done on the KERAAL dataset, as it\nis the only medical dataset with clear error labels for the exercises, and our\nmodel significantly surpasses state-of-the-art methods. Furthermore, we bridge\nthe gap towards better feedback to the patients by presenting a way to\ncalculate the importance of joints for each exercise.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5eb7\u590d\u8bad\u7ec3\u4e2d\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5173\u8282\u91cd\u8981\u6027\u8ba1\u7b97\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u53cd\u9988\u3002", "motivation": "\u60a3\u8005\u5728\u6ca1\u6709\u76f4\u63a5\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53c2\u4e0e\u5ea6\u4e0b\u964d\uff0c\u73b0\u6709\u7cfb\u7edf\u4ec5\u63d0\u4f9b\u4e8c\u5143\u5206\u7c7b\u6216\u8fde\u7eed\u8bc4\u5206\uff0c\u65e0\u6cd5\u5e2e\u52a9\u60a3\u8005\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u9aa8\u67b6\u57fa\u7840\u7684\u8fd0\u52a8\u8bc4\u4f30\uff0c\u7ed3\u5408Transformer\u6a21\u578b\uff08\u53d7HyperFormer\u542f\u53d1\uff09\u8fdb\u884c\u9519\u8bef\u5206\u7c7b\u3002", "result": "\u5728KERAAL\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u8282\u91cd\u8981\u6027\u8ba1\u7b97\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u60a3\u8005\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u53cd\u9988\uff0c\u662f\u5eb7\u590d\u8bad\u7ec3\u8d28\u91cf\u8bc4\u4f30\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2504.13884", "pdf": "https://arxiv.org/pdf/2504.13884", "abs": "https://arxiv.org/abs/2504.13884", "authors": ["Karan Taneja", "Anjali Singh", "Ashok K. Goel"], "title": "Towards a Multimodal Document-grounded Conversational AI System for Education", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "15 pages, 4 figures, AIED 2025", "summary": "Multimedia learning using text and images has been shown to improve learning\noutcomes compared to text-only instruction. But conversational AI systems in\neducation predominantly rely on text-based interactions while multimodal\nconversations for multimedia learning remain unexplored. Moreover, deploying\nconversational AI in learning contexts requires grounding in reliable sources\nand verifiability to create trust. We present MuDoC, a Multimodal\nDocument-grounded Conversational AI system based on GPT-4o, that leverages both\ntext and visuals from documents to generate responses interleaved with text and\nimages. Its interface allows verification of AI generated content through\nseamless navigation to the source. We compare MuDoC to a text-only system to\nexplore differences in learner engagement, trust in AI system, and their\nperformance on problem-solving tasks. Our findings indicate that both visuals\nand verifiability of content enhance learner engagement and foster trust;\nhowever, no significant impact in performance was observed. We draw upon\ntheories from cognitive and learning sciences to interpret the findings and\nderive implications, and outline future directions for the development of\nmultimodal conversational AI systems in education.", "AI": {"tldr": "MuDoC\u662f\u4e00\u4e2a\u57fa\u4e8eGPT-4o\u7684\u591a\u6a21\u6001\u5bf9\u8bddAI\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u672c\u548c\u56fe\u50cf\u63d0\u5347\u5b66\u4e60\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u53ef\u9a8c\u8bc1\u6027\u589e\u5f3a\u4fe1\u4efb\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6a21\u6001\u5185\u5bb9\u63d0\u5347\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u5ea6\u548c\u4fe1\u4efb\uff0c\u4f46\u5bf9\u4efb\u52a1\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u591a\u6a21\u6001\u5bf9\u8bddAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\uff0c\u5f25\u8865\u5f53\u524d\u6587\u672c\u4ea4\u4e92\u7684\u4e0d\u8db3\uff0c\u5e76\u89e3\u51b3\u5185\u5bb9\u53ef\u9a8c\u8bc1\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8eGPT-4o\u5f00\u53d1MuDoC\u7cfb\u7edf\uff0c\u7ed3\u5408\u6587\u6863\u4e2d\u7684\u6587\u672c\u548c\u56fe\u50cf\u751f\u6210\u591a\u6a21\u6001\u54cd\u5e94\uff0c\u5e76\u4e0e\u7eaf\u6587\u672c\u7cfb\u7edf\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u591a\u6a21\u6001\u5185\u5bb9\u548c\u53ef\u9a8c\u8bc1\u6027\u63d0\u5347\u4e86\u5b66\u4e60\u8005\u7684\u53c2\u4e0e\u5ea6\u548c\u4fe1\u4efb\uff0c\u4f46\u5bf9\u4efb\u52a1\u8868\u73b0\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u591a\u6a21\u6001\u5bf9\u8bddAI\u5728\u6559\u80b2\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2504.13975", "pdf": "https://arxiv.org/pdf/2504.13975", "abs": "https://arxiv.org/abs/2504.13975", "authors": ["Mehmet Yama\u00e7", "Muhammad Numan Yousaf", "Serkan Kiranyaz", "Moncef Gabbouj"], "title": "Multiscale Tensor Summation Factorization as a New Neural Network Layer (MTS Layer) for Multidimensional Data Processing", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07"], "comment": "13 pages", "summary": "Multilayer perceptrons (MLP), or fully connected artificial neural networks,\nare known for performing vector-matrix multiplications using learnable weight\nmatrices; however, their practical application in many machine learning tasks,\nespecially in computer vision, can be limited due to the high dimensionality of\ninput-output pairs at each layer. To improve efficiency, convolutional\noperators have been utilized to facilitate weight sharing and local\nconnections, yet they are constrained by limited receptive fields. In this\npaper, we introduce Multiscale Tensor Summation (MTS) Factorization, a novel\nneural network operator that implements tensor summation at multiple scales,\nwhere each tensor to be summed is obtained through Tucker-decomposition-like\nmode products. Unlike other tensor decomposition methods in the literature, MTS\nis not introduced as a network compression tool; instead, as a new backbone\nneural layer. MTS not only reduces the number of parameters required while\nenhancing the efficiency of weight optimization compared to traditional dense\nlayers (i.e., unfactorized weight matrices in MLP layers), but it also\ndemonstrates clear advantages over convolutional layers. The proof-of-concept\nexperimental comparison of the proposed MTS networks with MLPs and\nConvolutional Neural Networks (CNNs) demonstrates their effectiveness across\nvarious tasks, such as classification, compression, and signal restoration.\nAdditionally, when integrated with modern non-linear units such as the\nmulti-head gate (MHG), also introduced in this study, the corresponding neural\nnetwork, MTSNet, demonstrates a more favorable complexity-performance tradeoff\ncompared to state-of-the-art transformers in various computer vision\napplications. The software implementation of the MTS layer and the\ncorresponding MTS-based networks, MTSNets, is shared at\nhttps://github.com/mehmetyamac/MTSNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\uff08MTS\uff09\u5206\u89e3\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u7b97\u5b50\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u63d0\u5347\u6548\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u5bc6\u96c6\u5c42\u548c\u5377\u79ef\u5c42\u3002", "motivation": "\u89e3\u51b3\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u9ad8\u7ef4\u8f93\u5165\u8f93\u51fa\u5bf9\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u6269\u5c55\u611f\u53d7\u91ce\u3002", "method": "\u5f15\u5165MTS\u5206\u89e3\u4f5c\u4e3a\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u901a\u8fc7Tucker\u5206\u89e3\u7c7b\u6a21\u5f0f\u4e58\u79ef\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5f20\u91cf\u6c42\u548c\u3002", "result": "MTS\u5728\u5206\u7c7b\u3001\u538b\u7f29\u548c\u4fe1\u53f7\u6062\u590d\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e0e\u975e\u7ebf\u6027\u5355\u5143\u7ed3\u5408\u540e\u6027\u80fd\u4f18\u4e8e\u73b0\u6709Transformer\u3002", "conclusion": "MTSNet\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14078", "pdf": "https://arxiv.org/pdf/2504.14078", "abs": "https://arxiv.org/abs/2504.14078", "authors": ["M-Mahdi Naddaf-Sh", "Andrew Lee", "Kin Yen", "Eemon Amini", "Iman Soltani"], "title": "Infrared Vision Systems for Emergency Vehicle Driver Assistance in Low-Visibility Conditions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This study investigates the potential of infrared (IR) camera technology to\nenhance driver safety for emergency vehicles operating in low-visibility\nconditions, particularly at night and in dense fog. Such environments\nsignificantly increase the risk of collisions, especially for tow trucks and\nsnowplows that must remain operational in challenging conditions. Conventional\ndriver assistance systems often struggle under these conditions due to limited\nvisibility. In contrast, IR cameras, which detect the thermal signatures of\nobstacles, offer a promising alternative. The evaluation combines controlled\nlaboratory experiments, real-world field tests, and surveys of emergency\nvehicle operators. In addition to assessing detection performance, the study\nexamines the feasibility of retrofitting existing Department of Transportation\n(DoT) fleets with cost-effective IR-based driver assistance systems. Results\nunderscore the utility of IR technology in enhancing driver awareness and\nprovide data-driven recommendations for scalable deployment across legacy\nemergency vehicle fleets.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u7ea2\u5916\uff08IR\uff09\u6444\u50cf\u5934\u6280\u672f\u5982\u4f55\u63d0\u5347\u7d27\u6025\u8f66\u8f86\u5728\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u9a7e\u9a76\u5458\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u591c\u95f4\u548c\u6d53\u96fe\u4e2d\u3002", "motivation": "\u4f4e\u80fd\u89c1\u5ea6\u73af\u5883\uff08\u5982\u591c\u95f4\u548c\u6d53\u96fe\uff09\u589e\u52a0\u4e86\u7d27\u6025\u8f66\u8f86\uff08\u5982\u62d6\u8f66\u548c\u626b\u96ea\u8f66\uff09\u7684\u78b0\u649e\u98ce\u9669\uff0c\u4f20\u7edf\u8f85\u52a9\u7cfb\u7edf\u6548\u679c\u6709\u9650\u3002", "method": "\u7ed3\u5408\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u3001\u5b9e\u5730\u6d4b\u8bd5\u548c\u7d27\u6025\u8f66\u8f86\u64cd\u4f5c\u5458\u8c03\u67e5\uff0c\u8bc4\u4f30IR\u6280\u672f\u7684\u68c0\u6d4b\u6027\u80fd\u53ca\u6539\u88c5\u73b0\u6709\u8f66\u961f\u7684\u53ef\u884c\u6027\u3002", "result": "IR\u6280\u672f\u663e\u8457\u63d0\u5347\u9a7e\u9a76\u5458\u5bf9\u969c\u788d\u7269\u7684\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u6539\u88c5\u65b9\u6848\u3002", "conclusion": "IR\u6280\u672f\u662f\u63d0\u5347\u7d27\u6025\u8f66\u8f86\u5b89\u5168\u6027\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u63a8\u5e7f\u81f3\u73b0\u6709\u8f66\u961f\u3002"}}
{"id": "2504.14409", "pdf": "https://arxiv.org/pdf/2504.14409", "abs": "https://arxiv.org/abs/2504.14409", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "Fran\u00e7ois G. Germain", "Jonathan Le Roux"], "title": "Data Augmentation Using Neural Acoustic Fields With Retrieval-Augmented Pre-training", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Presented at ICASSP 2025 GenDA Workshop", "summary": "This report details MERL's system for room impulse response (RIR) estimation\nsubmitted to the Generative Data Augmentation Workshop at ICASSP 2025 for\nAugmenting RIR Data (Task 1) and Improving Speaker Distance Estimation (Task\n2). We first pre-train a neural acoustic field conditioned by room geometry on\nan external large-scale dataset in which pairs of RIRs and the geometries are\nprovided. The neural acoustic field is then adapted to each target room by\nusing the enrollment data, where we leverage either the provided room\ngeometries or geometries retrieved from the external dataset, depending on\navailability. Lastly, we predict the RIRs for each pair of source and receiver\nlocations specified by Task 1, and use these RIRs to train the speaker distance\nestimation model in Task 2.", "AI": {"tldr": "MERL\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u58f0\u573a\u7684RIR\u4f30\u8ba1\u7cfb\u7edf\uff0c\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u548c\u626c\u58f0\u5668\u8ddd\u79bb\u4f30\u8ba1\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIR\uff09\u4f30\u8ba1\u548c\u626c\u58f0\u5668\u8ddd\u79bb\u4f30\u8ba1\u95ee\u9898\uff0c\u5229\u7528\u5916\u90e8\u6570\u636e\u96c6\u548c\u623f\u95f4\u51e0\u4f55\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "method": "\u9884\u8bad\u7ec3\u795e\u7ecf\u58f0\u573a\u6a21\u578b\uff0c\u5229\u7528\u5916\u90e8\u6570\u636e\u96c6\u4e2d\u7684RIR\u548c\u51e0\u4f55\u4fe1\u606f\uff1b\u9488\u5bf9\u76ee\u6807\u623f\u95f4\u8fdb\u884c\u9002\u914d\uff1b\u9884\u6d4bRIR\u5e76\u7528\u4e8e\u8bad\u7ec3\u8ddd\u79bb\u4f30\u8ba1\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u9002\u914d\uff0c\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684RIR\u6570\u636e\uff0c\u5e76\u7528\u4e8e\u626c\u58f0\u5668\u8ddd\u79bb\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u5916\u90e8\u6570\u636e\u548c\u623f\u95f4\u51e0\u4f55\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u5347\u4e86RIR\u4f30\u8ba1\u548c\u8ddd\u79bb\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.14440", "pdf": "https://arxiv.org/pdf/2504.14440", "abs": "https://arxiv.org/abs/2504.14440", "authors": ["Chuhao Liu", "Zhijian Qiao", "Jieqi Shi", "Ke Wang", "Peize Liu", "Shaojie Shen"], "title": "SG-Reg: Generalizable and Efficient Scene Graph Registration", "categories": ["cs.RO", "cs.CV"], "comment": "IEEE Transactions Robotics Regular Paper", "summary": "This paper addresses the challenges of registering two rigid semantic scene\ngraphs, an essential capability when an autonomous agent needs to register its\nmap against a remote agent, or against a prior map. The hand-crafted\ndescriptors in classical semantic-aided registration, or the ground-truth\nannotation reliance in learning-based scene graph registration, impede their\napplication in practical real-world environments. To address the challenges, we\ndesign a scene graph network to encode multiple modalities of semantic nodes:\nopen-set semantic feature, local topology with spatial awareness, and shape\nfeature. These modalities are fused to create compact semantic node features.\nThe matching layers then search for correspondences in a coarse-to-fine manner.\nIn the back-end, we employ a robust pose estimator to decide transformation\naccording to the correspondences. We manage to maintain a sparse and\nhierarchical scene representation. Our approach demands fewer GPU resources and\nfewer communication bandwidth in multi-agent tasks. Moreover, we design a new\ndata generation approach using vision foundation models and a semantic mapping\nmodule to reconstruct semantic scene graphs. It differs significantly from\nprevious works, which rely on ground-truth semantic annotations to generate\ndata. We validate our method in a two-agent SLAM benchmark. It significantly\noutperforms the hand-crafted baseline in terms of registration success rate.\nCompared to visual loop closure networks, our method achieves a slightly higher\nregistration recall while requiring only 52 KB of communication bandwidth for\neach query frame. Code available at:\n\\href{http://github.com/HKUST-Aerial-Robotics/SG-Reg}{http://github.com/HKUST-Aerial-Robotics/SG-Reg}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8bed\u4e49\u8282\u70b9\u7279\u5f81\u7684\u573a\u666f\u56fe\u7f51\u7edc\uff0c\u7528\u4e8e\u521a\u6027\u8bed\u4e49\u573a\u666f\u56fe\u7684\u6ce8\u518c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u6216\u771f\u5b9e\u6807\u6ce8\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u5728\u6ce8\u518c\u5730\u56fe\u65f6\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\u6216\u771f\u5b9e\u6807\u6ce8\u7684\u9650\u5236\uff0c\u63d0\u5347\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u591a\u6a21\u6001\u8bed\u4e49\u8282\u70b9\u7279\u5f81\uff08\u5f00\u653e\u96c6\u8bed\u4e49\u3001\u5c40\u90e8\u62d3\u6251\u3001\u5f62\u72b6\u7279\u5f81\uff09\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u5339\u914d\u5c42\u548c\u540e\u7aef\u9c81\u68d2\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u4e24\u4ee3\u7406SLAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u624b\u5de5\u57fa\u7ebf\uff0c\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42\u4f4e\u81f352 KB/\u5e27\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6ce8\u518c\u6210\u529f\u7387\u548c\u8d44\u6e90\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u591a\u4ee3\u7406\u4efb\u52a1\u3002"}}
{"id": "2504.14541", "pdf": "https://arxiv.org/pdf/2504.14541", "abs": "https://arxiv.org/abs/2504.14541", "authors": ["Yi Yu", "Song Xia", "Xun Lin", "Chenqi Kong", "Wenhan Yang", "Shijian Lu", "Yap-Peng Tan", "Alex C. Kot"], "title": "Towards Model Resistant to Transferable Adversarial Examples via Trigger Activation", "categories": ["cs.CR", "cs.CV", "cs.LG"], "comment": "Accepted by IEEE TIFS 2025", "summary": "Adversarial examples, characterized by imperceptible perturbations, pose\nsignificant threats to deep neural networks by misleading their predictions. A\ncritical aspect of these examples is their transferability, allowing them to\ndeceive {unseen} models in black-box scenarios. Despite the widespread\nexploration of defense methods, including those on transferability, they show\nlimitations: inefficient deployment, ineffective defense, and degraded\nperformance on clean images. In this work, we introduce a novel training\nparadigm aimed at enhancing robustness against transferable adversarial\nexamples (TAEs) in a more efficient and effective way. We propose a model that\nexhibits random guessing behavior when presented with clean data\n$\\boldsymbol{x}$ as input, and generates accurate predictions when with\ntriggered data $\\boldsymbol{x}+\\boldsymbol{\\tau}$. Importantly, the trigger\n$\\boldsymbol{\\tau}$ remains constant for all data instances. We refer to these\nmodels as \\textbf{models with trigger activation}. We are surprised to find\nthat these models exhibit certain robustness against TAEs. Through the\nconsideration of first-order gradients, we provide a theoretical analysis of\nthis robustness. Moreover, through the joint optimization of the learnable\ntrigger and the model, we achieve improved robustness to transferable attacks.\nExtensive experiments conducted across diverse datasets, evaluating a variety\nof attacking methods, underscore the effectiveness and superiority of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\u589e\u5f3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5728\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u4e0a\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6548\u679c\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\uff0c\u901a\u8fc7\u56fa\u5b9a\u89e6\u53d1\u5668\u4f7f\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u968f\u673a\u731c\u6d4b\uff0c\u5728\u89e6\u53d1\u6570\u636e\u4e0a\u51c6\u786e\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u653b\u51fb\u65b9\u6cd5\u4e0b\u8868\u73b0\u4f18\u8d8a\uff0c\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u89e6\u53d1\u6fc0\u6d3b\u6a21\u578b\u4e3a\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8f6c\u79fb\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14554", "pdf": "https://arxiv.org/pdf/2504.14554", "abs": "https://arxiv.org/abs/2504.14554", "authors": ["Chongye Guo", "Jinhu Fu", "Junfeng Fang", "Kun Wang", "Guorui Feng"], "title": "REDEditing: Relationship-Driven Precise Backdoor Poisoning on Text-to-Image Diffusion Models", "categories": ["cs.CR", "cs.CV"], "comment": "10 pages, 7 figures", "summary": "The rapid advancement of generative AI highlights the importance of\ntext-to-image (T2I) security, particularly with the threat of backdoor\npoisoning. Timely disclosure and mitigation of security vulnerabilities in T2I\nmodels are crucial for ensuring the safe deployment of generative models. We\nexplore a novel training-free backdoor poisoning paradigm through model\nediting, which is recently employed for knowledge updating in large language\nmodels. Nevertheless, we reveal the potential security risks posed by model\nediting techniques to image generation models. In this work, we establish the\nprinciples for backdoor attacks based on model editing, and propose a\nrelationship-driven precise backdoor poisoning method, REDEditing. Drawing on\nthe principles of equivalent-attribute alignment and stealthy poisoning, we\ndevelop an equivalent relationship retrieval and joint-attribute transfer\napproach that ensures consistent backdoor image generation through concept\nrebinding. A knowledge isolation constraint is proposed to preserve benign\ngeneration integrity. Our method achieves an 11\\% higher attack success rate\ncompared to state-of-the-art approaches. Remarkably, adding just one line of\ncode enhances output naturalness while improving backdoor stealthiness by 24\\%.\nThis work aims to heighten awareness regarding this security vulnerability in\neditable image generation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7f16\u8f91\u7684\u65e0\u8bad\u7ec3\u540e\u95e8\u653b\u51fb\u65b9\u6cd5REDEditing\uff0c\u63ed\u793a\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u9690\u853d\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5b89\u5168\u6027\u5c24\u4e3a\u91cd\u8981\uff0c\u5c24\u5176\u662f\u540e\u95e8\u653b\u51fb\u7684\u5a01\u80c1\u3002\u53ca\u65f6\u62ab\u9732\u548c\u7f13\u89e3T2I\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u5bf9\u786e\u4fdd\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u7cfb\u9a71\u52a8\u7684\u7cbe\u786e\u540e\u95e8\u653b\u51fb\u65b9\u6cd5REDEditing\uff0c\u57fa\u4e8e\u7b49\u6548\u5c5e\u6027\u5bf9\u9f50\u548c\u9690\u853d\u653b\u51fb\u539f\u5219\uff0c\u91c7\u7528\u7b49\u6548\u5173\u7cfb\u68c0\u7d22\u548c\u8054\u5408\u5c5e\u6027\u8f6c\u79fb\u65b9\u6cd5\uff0c\u786e\u4fdd\u901a\u8fc7\u6982\u5ff5\u91cd\u7ed1\u5b9a\u751f\u6210\u4e00\u81f4\u7684\u540e\u95e8\u56fe\u50cf\u3002", "result": "REDEditing\u7684\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad811%\uff0c\u4ec5\u9700\u6dfb\u52a0\u4e00\u884c\u4ee3\u7801\u5373\u53ef\u63d0\u9ad8\u8f93\u51fa\u81ea\u7136\u6027\uff0c\u540c\u65f6\u5c06\u540e\u95e8\u9690\u853d\u6027\u63d0\u534724%\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u5bf9\u53ef\u7f16\u8f91\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5b89\u5168\u6f0f\u6d1e\u7684\u8ba4\u8bc6\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u7f16\u8f91\u6280\u672f\u53ef\u80fd\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2504.14588", "pdf": "https://arxiv.org/pdf/2504.14588", "abs": "https://arxiv.org/abs/2504.14588", "authors": ["Wenke Xia", "Ruoxuan Feng", "Dong Wang", "Di Hu"], "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Building a generalizable self-correction system is crucial for robots to\nrecover from failures. Despite advancements in Multimodal Large Language Models\n(MLLMs) that empower robots with semantic reflection ability for failure,\ntranslating semantic reflection into how to correct fine-grained robotic\nactions remains a significant challenge. To address this gap, we build the\nPhoenix framework, which leverages motion instruction as a bridge to connect\nhigh-level semantic reflection with low-level robotic action correction. In\nthis motion-based self-reflection framework, we start with a dual-process\nmotion adjustment mechanism with MLLMs to translate the semantic reflection\ninto coarse-grained motion instruction adjustment. To leverage this motion\ninstruction for guiding how to correct fine-grained robotic actions, a\nmulti-task motion-conditioned diffusion policy is proposed to integrate visual\nobservations for high-frequency robotic action correction. By combining these\ntwo models, we could shift the demand for generalization capability from the\nlow-level manipulation policy to the MLLMs-driven motion adjustment model and\nfacilitate precise, fine-grained robotic action correction. Utilizing this\nframework, we further develop a lifelong learning method to automatically\nimprove the model's capability from interactions with dynamic environments. The\nexperiments conducted in both the RoboMimic simulation and real-world scenarios\nprove the superior generalization and robustness of our framework across a\nvariety of manipulation tasks. Our code is released at\n\\href{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}{https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPhoenix\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u6307\u4ee4\u8fde\u63a5\u9ad8\u7ea7\u8bed\u4e49\u53cd\u601d\u4e0e\u4f4e\u7ea7\u673a\u5668\u4eba\u52a8\u4f5c\u4fee\u6b63\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u8fd0\u52a8\u6761\u4ef6\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u7cbe\u786e\u4fee\u6b63\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6784\u5efa\u901a\u7528\u81ea\u6821\u6b63\u7cfb\u7edf\u5bf9\u673a\u5668\u4eba\u4ece\u5931\u8d25\u4e2d\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5982\u4f55\u5c06\u8bed\u4e49\u53cd\u601d\u8f6c\u5316\u4e3a\u7ec6\u7c92\u5ea6\u673a\u5668\u4eba\u52a8\u4f5c\u4fee\u6b63\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faPhoenix\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8fc7\u7a0b\u8fd0\u52a8\u8c03\u6574\u673a\u5236\u548c\u591a\u4efb\u52a1\u8fd0\u52a8\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff0c\u7ed3\u5408\u89c6\u89c9\u89c2\u5bdf\u5b9e\u73b0\u9ad8\u9891\u52a8\u4f5c\u4fee\u6b63\u3002", "result": "\u5728RoboMimic\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u6846\u67b6\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Phoenix\u6846\u67b6\u901a\u8fc7\u8fd0\u52a8\u6307\u4ee4\u548c\u6269\u6563\u7b56\u7565\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u4fee\u6b63\uff0c\u5e76\u901a\u8fc7\u7ec8\u8eab\u5b66\u4e60\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2504.14634", "pdf": "https://arxiv.org/pdf/2504.14634", "abs": "https://arxiv.org/abs/2504.14634", "authors": ["Sahara Sheikholeslami", "Ladislau B\u00f6l\u00f6ni"], "title": "Latent Representations for Visual Proprioception in Inexpensive Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic manipulation requires explicit or implicit knowledge of the robot's\njoint positions. Precise proprioception is standard in high-quality industrial\nrobots but is often unavailable in inexpensive robots operating in unstructured\nenvironments. In this paper, we ask: to what extent can a fast, single-pass\nregression architecture perform visual proprioception from a single external\ncamera image, available even in the simplest manipulation settings? We explore\nseveral latent representations, including CNNs, VAEs, ViTs, and bags of\nuncalibrated fiducial markers, using fine-tuning techniques adapted to the\nlimited data available. We evaluate the achievable accuracy through experiments\non an inexpensive 6-DoF robot.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5355\u6b21\u5916\u90e8\u6444\u50cf\u5934\u56fe\u50cf\u5b9e\u73b0\u89c6\u89c9\u672c\u4f53\u611f\u77e5\uff0c\u7814\u7a76\u4e86\u591a\u79cd\u6f5c\u5728\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u5728\u4f4e\u6210\u672c6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7f3a\u4e4f\u7cbe\u786e\u7684\u672c\u4f53\u611f\u77e5\u80fd\u529b\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5355\u6b21\u89c6\u89c9\u56de\u5f52\u67b6\u6784\u7684\u53ef\u884c\u6027\u3002", "method": "\u7814\u7a76\u4e86CNN\u3001VAE\u3001ViT\u548c\u672a\u6821\u51c6\u6807\u8bb0\u70b9\u7b49\u591a\u79cd\u6f5c\u5728\u8868\u793a\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u9002\u5e94\u6709\u9650\u6570\u636e\u7684\u5fae\u8c03\u6280\u672f\u3002", "result": "\u5728\u4f4e\u6210\u672c6\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5355\u6b21\u89c6\u89c9\u56de\u5f52\u67b6\u6784\u5728\u4f4e\u6210\u672c\u673a\u5668\u4eba\u4e2d\u5177\u6709\u5b9e\u73b0\u89c6\u89c9\u672c\u4f53\u611f\u77e5\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.14662", "pdf": "https://arxiv.org/pdf/2504.14662", "abs": "https://arxiv.org/abs/2504.14662", "authors": ["Yeoreum Lee", "Jinwook Jung", "Sungyong Baik"], "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "ICLR 2025", "summary": "Large-scale deep learning models with a pretraining-finetuning paradigm have\nled to a surge of numerous task-specific models fine-tuned from a common\npre-trained model. Recently, several research efforts have been made on merging\nthese large models into a single multi-task model, particularly with simple\narithmetic on parameters. Such merging methodology faces a central challenge:\ninterference between model parameters fine-tuned on different tasks. Few recent\nworks have focused on designing a new fine-tuning scheme that can lead to small\nparameter interference, however at the cost of the performance of each\ntask-specific fine-tuned model and thereby limiting that of a merged model. To\nimprove the performance of a merged model, we note that a fine-tuning scheme\nshould aim for (1) smaller parameter interference and (2) better performance of\neach fine-tuned model on the corresponding task. In this work, we aim to design\na new fine-tuning objective function to work towards these two goals. In the\ncourse of this process, we find such objective function to be strikingly\nsimilar to sharpness-aware minimization (SAM) objective function, which aims to\nachieve generalization by finding flat minima. Drawing upon our observation, we\npropose to fine-tune pre-trained models via sharpness-aware minimization. The\nexperimental and theoretical results showcase the effectiveness and\northogonality of our proposed approach, improving performance upon various\nmerging and fine-tuning methods. Our code is available at\nhttps://github.com/baiklab/SAFT-Merge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u76ee\u6807\u51fd\u6570\uff0c\u65e8\u5728\u51cf\u5c11\u53c2\u6570\u5e72\u6270\u5e76\u63d0\u5347\u5355\u4efb\u52a1\u6027\u80fd\uff0c\u57fa\u4e8e\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\u65b9\u6cd5\u3002", "motivation": "\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u4e0b\u4ea7\u751f\u4e86\u8bb8\u591a\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u4f46\u5408\u5e76\u8fd9\u4e9b\u6a21\u578b\u65f6\u5b58\u5728\u53c2\u6570\u5e72\u6270\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u727a\u7272\u4e86\u5355\u4efb\u52a1\u6027\u80fd\uff0c\u9650\u5236\u4e86\u5408\u5e76\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u76ee\u6807\u51fd\u6570\uff0c\u7ed3\u5408\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\uff08SAM\uff09\uff0c\u4ee5\u51cf\u5c11\u53c2\u6570\u5e72\u6270\u5e76\u63d0\u5347\u5355\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u548c\u7406\u8bba\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u4e0e\u5176\u4ed6\u65b9\u6cd5\u6b63\u4ea4\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u5e76\u548c\u5fae\u8c03\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u5e72\u6270\u5e76\u63d0\u5347\u5355\u4efb\u52a1\u6027\u80fd\uff0c\u4ece\u800c\u4f18\u5316\u5408\u5e76\u6a21\u578b\u7684\u8868\u73b0\u3002"}}
{"id": "2504.14727", "pdf": "https://arxiv.org/pdf/2504.14727", "abs": "https://arxiv.org/abs/2504.14727", "authors": ["Geng Liu", "Fei Zhu", "Rong Feng", "Zhiqiang Yi", "Shiqi Wang", "Gaofeng Meng", "Zhaoxiang Zhang"], "title": "Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u548c\u5b66\u4e60\u7cfb\u7edf\u542f\u53d1\u7684\u751f\u7269\u6a21\u62df\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u987a\u5e8f\u4efb\u52a1\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u79ef\u7d2f\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408\u534a\u53c2\u6570\u8bb0\u5fc6\u548c\u9192\u7761\u5de9\u56fa\u673a\u5236\uff0c\u63d0\u51fa\u751f\u7269\u6a21\u62df\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982ImageNet\u7c7b\u589e\u91cf\u5b66\u4e60\uff09\u4e2d\uff0c\u6a21\u578b\u65e2\u80fd\u5b66\u4e60\u65b0\u4efb\u52a1\u53c8\u80fd\u4fdd\u7559\u65e7\u77e5\u8bc6\u3002", "conclusion": "\u6a21\u62df\u751f\u7269\u667a\u80fd\u662f\u8d4b\u4e88\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2504.14795", "pdf": "https://arxiv.org/pdf/2504.14795", "abs": "https://arxiv.org/abs/2504.14795", "authors": ["Ryu Tadokoro", "Tsukasa Takagi", "Shin-ichi Maeda"], "title": "Segmentation with Noisy Labels via Spatially Correlated Distributions", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "In semantic segmentation, the accuracy of models heavily depends on the\nhigh-quality annotations. However, in many practical scenarios such as medical\nimaging and remote sensing, obtaining true annotations is not straightforward\nand usually requires significant human labor. Relying on human labor often\nintroduces annotation errors, including mislabeling, omissions, and\ninconsistency between annotators. In the case of remote sensing, differences in\nprocurement time can lead to misaligned ground truth annotations. These label\nerrors are not independently distributed, and instead usually appear in\nspatially connected regions where adjacent pixels are more likely to share the\nsame errors. To address these issues, we propose an approximate Bayesian\nestimation based on a probabilistic model that assumes training data includes\nlabel errors, incorporating the tendency for these errors to occur with spatial\ncorrelations between adjacent pixels. Bayesian inference requires computing the\nposterior distribution of label errors, which becomes intractable when spatial\ncorrelations are present. We represent the correlation of label errors between\nadjacent pixels through a Gaussian distribution whose covariance is structured\nby a Kac-Murdock-Szeg\\\"{o} (KMS) matrix, solving the computational challenges.\nThrough experiments on multiple segmentation tasks, we confirm that leveraging\nthe spatial correlation of label errors significantly improves performance.\nNotably, in specific tasks such as lung segmentation, the proposed method\nachieves performance comparable to training with clean labels under moderate\nnoise levels. Code is available at\nhttps://github.com/pfnet-research/Bayesian_SpatialCorr.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6a21\u578b\u7684\u8d1d\u53f6\u65af\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8bed\u4e49\u5206\u5272\u4e2d\u56e0\u7a7a\u95f4\u76f8\u5173\u6027\u5bfc\u81f4\u7684\u6807\u7b7e\u9519\u8bef\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u8bed\u4e49\u5206\u5272\u4e2d\uff0c\u9ad8\u8d28\u91cf\u6807\u6ce8\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645\u573a\u666f\uff08\u5982\u533b\u5b66\u5f71\u50cf\u548c\u9065\u611f\uff09\u4e2d\u83b7\u53d6\u771f\u5b9e\u6807\u6ce8\u56f0\u96be\u4e14\u6613\u51fa\u9519\uff0c\u5c24\u5176\u662f\u7a7a\u95f4\u76f8\u5173\u7684\u6807\u7b7e\u9519\u8bef\u3002", "method": "\u91c7\u7528\u8fd1\u4f3c\u8d1d\u53f6\u65af\u4f30\u8ba1\uff0c\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u5305\u542b\u6807\u7b7e\u9519\u8bef\uff0c\u5e76\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\uff08KMS\u77e9\u9635\u7ed3\u6784\u534f\u65b9\u5dee\uff09\u5efa\u6a21\u76f8\u90bb\u50cf\u7d20\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u6807\u7b7e\u9519\u8bef\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u80ba\u90e8\u5206\u5272\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e2d\u7b49\u566a\u58f0\u6c34\u5e73\u4e0b\u8868\u73b0\u63a5\u8fd1\u5e72\u51c0\u6807\u7b7e\u8bad\u7ec3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u9519\u8bef\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u6807\u6ce8\u4e0d\u5b8c\u7f8e\u7684\u573a\u666f\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.14798", "pdf": "https://arxiv.org/pdf/2504.14798", "abs": "https://arxiv.org/abs/2504.14798", "authors": ["Hao Xuan", "Xingyu Li"], "title": "Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7a33\u5065\u9057\u5fd8\u201d\uff08Robust Unlearning\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u201c\u9057\u5fd8\u6620\u5c04\u653b\u51fb\u201d\uff08UMA\uff09\u9a8c\u8bc1\u73b0\u6709\u9057\u5fd8\u6280\u672f\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b0\u5176\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u6280\u672f\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u6a21\u578b\u4e2d\u7684\u6b8b\u7559\u4fe1\u606f\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u63d0\u51faUMA\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u67e5\u8be2\u4e3b\u52a8\u63a2\u6d4b\u6a21\u578b\u4e2d\u7684\u9057\u5fd8\u75d5\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u9057\u5fd8\u6280\u672f\u5373\u4f7f\u901a\u8fc7\u73b0\u6709\u9a8c\u8bc1\u6307\u6807\uff0c\u4ecd\u6613\u53d7\u653b\u51fb\u3002", "conclusion": "UMA\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u673a\u5668\u9057\u5fd8\u5b89\u5168\u6027\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2504.14800", "pdf": "https://arxiv.org/pdf/2504.14800", "abs": "https://arxiv.org/abs/2504.14800", "authors": ["Shuxian Zhao", "Jie Gui", "Minjing Dong", "Baosheng Yu", "Zhipeng Gui", "Lu Dong", "Yuan Yan Tang", "James Tin-Yau Kwok"], "title": "A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis, and Solutions", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The small sample imbalance (S&I) problem is a major challenge in machine\nlearning and data analysis. It is characterized by a small number of samples\nand an imbalanced class distribution, which leads to poor model performance. In\naddition, indistinct inter-class feature distributions further complicate\nclassification tasks. Existing methods often rely on algorithmic heuristics\nwithout sufficiently analyzing the underlying data characteristics. We argue\nthat a detailed analysis from the data perspective is essential before\ndeveloping an appropriate solution. Therefore, this paper proposes a systematic\nanalytical framework for the S\\&I problem. We first summarize imbalance metrics\nand complexity analysis methods, highlighting the need for interpretable\nbenchmarks to characterize S&I problems. Second, we review recent solutions for\nconventional, complexity-based, and extreme S&I problems, revealing\nmethodological differences in handling various data distributions. Our summary\nfinds that resampling remains a widely adopted solution. However, we conduct\nexperiments on binary and multiclass datasets, revealing that classifier\nperformance differences significantly exceed the improvements achieved through\nresampling. Finally, this paper highlights open questions and discusses future\ntrends.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\uff08S&I\uff09\u95ee\u9898\uff0c\u603b\u7ed3\u4e86\u4e0d\u5e73\u8861\u5ea6\u91cf\u548c\u590d\u6742\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c0f\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\u662f\u673a\u5668\u5b66\u4e60\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6570\u636e\u7279\u5f81\u7684\u6df1\u5165\u5206\u6790\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u6570\u636e\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u5305\u62ec\u603b\u7ed3\u4e0d\u5e73\u8861\u5ea6\u91cf\u548c\u590d\u6742\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u56de\u987e\u4e86\u9488\u5bf9\u4e0d\u540c\u7c7b\u578bS&I\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5206\u7c7b\u5668\u6027\u80fd\u5dee\u5f02\u663e\u8457\u8d85\u8fc7\u901a\u8fc7\u91cd\u91c7\u6837\u5b9e\u73b0\u7684\u6539\u8fdb\uff0c\u91cd\u91c7\u6837\u4ecd\u662f\u5e7f\u6cdb\u91c7\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u8d8b\u52bf\uff0c\u547c\u5401\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u89e3\u51b3S&I\u95ee\u9898\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2504.14815", "pdf": "https://arxiv.org/pdf/2504.14815", "abs": "https://arxiv.org/abs/2504.14815", "authors": ["Xiaoyong Yuan", "Xiaolong Ma", "Linke Guo", "Lan Zhang"], "title": "What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "17 pages, 15 figures", "summary": "Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAIA\u7684\u65e0\u63d0\u793a\u3001\u65e0\u56fe\u50cf\u7684\u6982\u5ff5\u5ba1\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5fae\u8c03\u6269\u6563\u6a21\u578b\u662f\u5426\u5b66\u4e60\u5230\u7279\u5b9a\u76ee\u6807\u6982\u5ff5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5fae\u8c03\u6a21\u578b\u7684\u5171\u4eab\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u6cd5\u5f8b\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u5ba1\u8ba1\u5de5\u5177\u3002", "method": "\u63d0\u51faPrompt-Agnostic Image-Free Auditing (PAIA)\u6846\u67b6\uff0c\u76f4\u63a5\u5206\u6790\u6a21\u578b\u5185\u90e8\u884c\u4e3a\uff0c\u65e0\u9700\u4f18\u5316\u63d0\u793a\u6216\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728320\u4e2a\u63a7\u5236\u6a21\u578b\u548c690\u4e2a\u771f\u5b9e\u793e\u533a\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0cPAIA\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5ba1\u8ba1\u65f6\u95f4\u51cf\u5c1118-40\u500d\u3002", "conclusion": "PAIA\u662f\u9996\u4e2a\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u6269\u6563\u6a21\u578b\u9884\u90e8\u7f72\u6982\u5ff5\u5ba1\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6a21\u578b\u5171\u4eab\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u548c\u900f\u660e\u7684\u57fa\u7840\u3002"}}
{"id": "2504.14882", "pdf": "https://arxiv.org/pdf/2504.14882", "abs": "https://arxiv.org/abs/2504.14882", "authors": ["Mojtaba Kolahdouzi", "Hatice Gunes", "Ali Etemad"], "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes.", "AI": {"tldr": "\u7814\u7a76\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7fa4\u4f53\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08\u5982RMSProp\uff09\u6bd4\u968f\u673a\u4f18\u5316\u5668\uff08\u5982SGD\uff09\u66f4\u6613\u6536\u655b\u5230\u516c\u5e73\u89e3\u3002", "motivation": "\u63a2\u8ba8\u4f18\u5316\u7b97\u6cd5\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u516c\u5e73\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u4e25\u91cd\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5206\u6790\u4f18\u5316\u52a8\u6001\uff0c\u6bd4\u8f83\u81ea\u9002\u5e94\u4f18\u5316\u5668\uff08RMSProp\uff09\u548c\u968f\u673a\u4f18\u5316\u5668\uff08SGD\uff09\u7684\u516c\u5e73\u6027\u8868\u73b0\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u81ea\u9002\u5e94\u4f18\u5316\u5668\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u516c\u5e73\uff0c\u4e14\u4e0d\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u4f18\u5316\u5668\u662f\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u7684\u91cd\u8981\u673a\u5236\u3002"}}
{"id": "2504.14906", "pdf": "https://arxiv.org/pdf/2504.14906", "abs": "https://arxiv.org/abs/2504.14906", "authors": ["Huadai Liu", "Tianyi Luo", "Qikai Jiang", "Kaicheng Luo", "Peiwen Sun", "Jialei Wan", "Rongjie Huang", "Qian Chen", "Wen Wang", "Xiangtai Li", "Shiliang Zhang", "Zhijie Yan", "Zhou Zhao", "Wei Xue"], "title": "OmniAudio: Generating Spatial Audio from 360-Degree Video", "categories": ["eess.AS", "cs.CV", "cs.SD"], "comment": "Work in Progress", "summary": "Traditional video-to-audio generation techniques primarily focus on\nfield-of-view (FoV) video and non-spatial audio, often missing the spatial cues\nnecessary for accurately representing sound sources in 3D environments. To\naddress this limitation, we introduce a novel task, 360V2SA, to generate\nspatial audio from 360-degree videos, specifically producing First-order\nAmbisonics (FOA) audio - a standard format for representing 3D spatial audio\nthat captures sound directionality and enables realistic 3D audio reproduction.\nWe first create Sphere360, a novel dataset tailored for this task that is\ncurated from real-world data. We also design an efficient semi-automated\npipeline for collecting and cleaning paired video-audio data. To generate\nspatial audio from 360-degree video, we propose a novel framework OmniAudio,\nwhich leverages self-supervised pre-training using both spatial audio data (in\nFOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a\ndual-branch framework that utilizes both panoramic and FoV video inputs to\ncapture comprehensive local and global information from 360-degree videos.\nExperimental results demonstrate that OmniAudio achieves state-of-the-art\nperformance across both objective and subjective metrics on Sphere360. Code and\ndatasets will be released at https://github.com/liuhuadai/OmniAudio. The demo\npage is available at https://OmniAudio-360V2SA.github.io.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1360V2SA\uff0c\u4ece360\u5ea6\u89c6\u9891\u751f\u6210\u7a7a\u95f4\u97f3\u9891\uff08FOA\u683c\u5f0f\uff09\uff0c\u5e76\u4ecb\u7ecd\u4e86\u6570\u636e\u96c6Sphere360\u548c\u6846\u67b6OmniAudio\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5230\u97f3\u9891\u751f\u6210\u6280\u672f\u7f3a\u4e4f\u7a7a\u95f4\u7ebf\u7d22\uff0c\u65e0\u6cd5\u51c6\u786e\u8868\u793a3D\u73af\u5883\u4e2d\u7684\u58f0\u6e90\u3002", "method": "\u521b\u5efaSphere360\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u534a\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u63d0\u51fa\u53cc\u5206\u652f\u6846\u67b6OmniAudio\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u5168\u666f/FoV\u89c6\u9891\u8f93\u5165\u3002", "result": "OmniAudio\u5728Sphere360\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "conclusion": "360V2SA\u4efb\u52a1\u548cOmniAudio\u6846\u67b6\u4e3a3D\u7a7a\u95f4\u97f3\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15051", "pdf": "https://arxiv.org/pdf/2504.15051", "abs": "https://arxiv.org/abs/2504.15051", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicol\u00e8", "Stefano Ghidoni", "Nassir Navab"], "title": "VeLU: Variance-enhanced Learning Unit for Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub.", "AI": {"tldr": "VeLU\u662f\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u65b9\u5dee\u52a8\u6001\u8c03\u6574\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7ArcTan-Sin\u53d8\u6362\u548cWasserstein-2\u6b63\u5219\u5316\u4f18\u5316\u68af\u5ea6\u6d41\u548c\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eReLU\u3001Swish\u548cGELU\u3002", "motivation": "ReLU\u867d\u7136\u7b80\u5355\u4f46\u5b58\u5728\u68af\u5ea6\u6d88\u5931\u548c\u7f3a\u4e4f\u9002\u5e94\u6027\u95ee\u9898\uff0c\u5176\u4ed6\u66ff\u4ee3\u65b9\u6848\u5982Swish\u548cGELU\u4e5f\u65e0\u6cd5\u52a8\u6001\u9002\u5e94\u8f93\u5165\u7edf\u8ba1\u7279\u6027\u3002", "method": "VeLU\u7ed3\u5408ArcTan-Sin\u53d8\u6362\u548cWasserstein-2\u6b63\u5219\u5316\uff0c\u52a8\u6001\u8c03\u6574\u8f93\u5165\u65b9\u5dee\u4ee5\u4f18\u5316\u68af\u5ea6\u6d41\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728ViT_B16\u3001VGG19\u7b49\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVeLU\u5728\u591a\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8eReLU\u3001Swish\u548cGELU\u3002", "conclusion": "VeLU\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8f93\u5165\u65b9\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6fc0\u6d3b\u51fd\u6570\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.15129", "pdf": "https://arxiv.org/pdf/2504.15129", "abs": "https://arxiv.org/abs/2504.15129", "authors": ["Kangyao Huang", "Hao Wang", "Yu Luo", "Jingyu Chen", "Jintao Chen", "Xiangkui Zhang", "Xiangyang Ji", "Huaping Liu"], "title": "A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5e73\u53f0\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u65e0\u7f1d\u8f6c\u79fb\uff0c\u652f\u6301\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4ece\u96f6\u8bad\u7ec3\u5230\u73b0\u5b9e\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u975e\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\u4e2d\u5e94\u7528\u5b66\u4e60\u65b9\u6cd5\u7684\u6311\u6218\uff0c\u5982\u5927\u91cf\u6a21\u62df\u6570\u636e\u9700\u6c42\u3001\u5b9e\u65f6\u5904\u7406\u8981\u6c42\u548c\u6a21\u62df\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u6574\u5408\u8bad\u7ec3\u73af\u5883\u3001\u98de\u884c\u52a8\u529b\u5b66\u63a7\u5236\u3001DRL\u7b97\u6cd5\u3001MAVROS\u4e2d\u95f4\u4ef6\u548c\u786c\u4ef6\uff0c\u5f62\u6210\u5b8c\u6574\u5de5\u4f5c\u6d41\u3002", "result": "\u5e73\u53f0\u652f\u6301\u591a\u79cd\u73af\u5883\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u9ad8\u6548\u6027\u548c\u6237\u5916\u98de\u884c\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3a\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5b66\u4e60\u7b56\u7565\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15252", "pdf": "https://arxiv.org/pdf/2504.15252", "abs": "https://arxiv.org/abs/2504.15252", "authors": ["Tue Vo", "Lakshay Sharma", "Tuan Dinh", "Khuong Dinh", "Trang Nguyen", "Trung Phan", "Minh Do", "Duong Vu"], "title": "SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025", "summary": "Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.", "AI": {"tldr": "SuoiAI\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u7528\u4e8e\u6784\u5efa\u8d8a\u5357\u6c34\u751f\u65e0\u810a\u690e\u52a8\u7269\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u7269\u79cd\u5206\u7c7b\u3002", "motivation": "\u7406\u89e3\u548c\u76d1\u6d4b\u6c34\u751f\u751f\u7269\u591a\u6837\u6027\u5bf9\u751f\u6001\u5065\u5eb7\u548c\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u5229\u7528\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u548c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u6570\u636e\u6536\u96c6\u3001\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u3001\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u591a\u6837\u5316\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u90e8\u7f72\u7b49\u6311\u6218\u3002", "conclusion": "SuoiAI\u4e3a\u6c34\u751f\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.15262", "pdf": "https://arxiv.org/pdf/2504.15262", "abs": "https://arxiv.org/abs/2504.15262", "authors": ["Brandon Zhao", "Aviad Levis", "Liam Connor", "Pratul P. Srinivasan", "Katherine L. Bouman"], "title": "Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields", "categories": ["astro-ph.CO", "cs.CV"], "comment": null, "summary": "Weak gravitational lensing is the slight distortion of galaxy shapes caused\nprimarily by the gravitational effects of dark matter in the universe. In our\nwork, we seek to invert the weak lensing signal from 2D telescope images to\nreconstruct a 3D map of the universe's dark matter field. While inversion\ntypically yields a 2D projection of the dark matter field, accurate 3D maps of\nthe dark matter distribution are essential for localizing structures of\ninterest and testing theories of our universe. However, 3D inversion poses\nsignificant challenges. First, unlike standard 3D reconstruction that relies on\nmultiple viewpoints, in this case, images are only observed from a single\nviewpoint. This challenge can be partially addressed by observing how galaxy\nemitters throughout the volume are lensed. However, this leads to the second\nchallenge: the shapes and exact locations of unlensed galaxies are unknown, and\ncan only be estimated with a very large degree of uncertainty. This introduces\nan overwhelming amount of noise which nearly drowns out the lensing signal\ncompletely. Previous approaches tackle this by imposing strong assumptions\nabout the structures in the volume. We instead propose a methodology using a\ngravitationally-constrained neural field to flexibly model the continuous\nmatter distribution. We take an analysis-by-synthesis approach, optimizing the\nweights of the neural network through a fully differentiable physical forward\nmodel to reproduce the lensing signal present in image measurements. We\nshowcase our method on simulations, including realistic simulated measurements\nof dark matter distributions that mimic data from upcoming telescope surveys.\nOur results show that our method can not only outperform previous methods, but\nimportantly is also able to recover potentially surprising dark matter\nstructures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5f15\u529b\u7ea6\u675f\u7684\u795e\u7ecf\u573a\u65b9\u6cd5\uff0c\u4ece2D\u671b\u8fdc\u955c\u56fe\u50cf\u4e2d\u91cd\u5efa3D\u6697\u7269\u8d28\u5206\u5e03\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6a21\u62df\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u51c6\u786e\u91cd\u5efa3D\u6697\u7269\u8d28\u5206\u5e03\u5bf9\u4e8e\u5b9a\u4f4d\u5b87\u5b99\u7ed3\u6784\u548c\u9a8c\u8bc1\u5b87\u5b99\u7406\u8bba\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u56e0\u5355\u89c6\u89d2\u89c2\u6d4b\u548c\u566a\u58f0\u95ee\u9898\u9762\u4e34\u6311\u6218\u3002", "method": "\u91c7\u7528\u5f15\u529b\u7ea6\u675f\u7684\u795e\u7ecf\u573a\u5efa\u6a21\u8fde\u7eed\u7269\u8d28\u5206\u5e03\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7269\u7406\u524d\u5411\u6a21\u578b\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\uff0c\u4ee5\u590d\u73b0\u89c2\u6d4b\u5230\u7684\u900f\u955c\u4fe1\u53f7\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd8\u80fd\u6062\u590d\u6f5c\u5728\u7684\u610f\u5916\u6697\u7269\u8d28\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6697\u7269\u8d28\u5206\u5e03\u7684\u9ad8\u7cbe\u5ea63D\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u672a\u6765\u671b\u8fdc\u955c\u89c2\u6d4b\u6570\u636e\u3002"}}
