<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Second Law of Intelligence: Controlling Ethical Entropy in Autonomous Systems](https://arxiv.org/abs/2511.10704)
*Samih Fadli*

Main category: cs.AI

TL;DR: 提出了一种类似于热力学第二定律的AI对齐理论，定义了伦理熵并证明其随时间增加，除非进行持续的对齐工作。


<details>
  <summary>Details</summary>
Motivation: 解决无约束人工智能系统中目标偏离的问题，提出一个类似于热力学的定量框架以维持系统的稳定性和安全性。

Method: 定义了伦理熵，推导了熵随时间增加的证明，并提出了对齐工作的临界稳定性边界。通过模拟验证理论。

Result: 70亿参数模型在不进行对齐工作时，熵从0.32增加到1.69 +/- 1.08 nats；而在进行对齐工作时保持稳定。

Conclusion: 该框架将对齐问题重新定义为连续的热力学控制问题，为维持先进自主系统的稳定性和安全性提供了定量基础。

Abstract: We propose that unconstrained artificial intelligence obeys a Second Law analogous to thermodynamics, where ethical entropy, defined as a measure of divergence from intended goals, increases spontaneously without continuous alignment work. For gradient-based optimizers, we define this entropy over a finite set of goals {g_i} as S = -Σ p(g_i; theta) ln p(g_i; theta), and we prove that its time derivative dS/dt >= 0, driven by exploration noise and specification gaming. We derive the critical stability boundary for alignment work as gamma_crit = (lambda_max / 2) ln N, where lambda_max is the dominant eigenvalue of the Fisher Information Matrix and N is the number of model parameters. Simulations validate this theory. A 7-billion-parameter model (N = 7 x 10^9) with lambda_max = 1.2 drifts from an initial entropy of 0.32 to 1.69 +/- 1.08 nats, while a system regularized with alignment work gamma = 20.4 (1.5 gamma_crit) maintains stability at 0.00 +/- 0.00 nats (p = 4.19 x 10^-17, n = 20 trials). This framework recasts AI alignment as a problem of continuous thermodynamic control, providing a quantitative foundation for maintaining the stability and safety of advanced autonomous systems.

</details>
