{"id": "2504.13191", "pdf": "https://arxiv.org/pdf/2504.13191", "abs": "https://arxiv.org/abs/2504.13191", "authors": ["Nam Nguyen"], "title": "Universal Representations for Classification-enhanced Lossy Compression", "categories": ["cs.CV", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "In lossy compression, the classical tradeoff between compression rate and\nreconstruction distortion has traditionally guided algorithm design. However,\nBlau and Michaeli [5] introduced a generalized framework, known as the\nrate-distortion-perception (RDP) function, incorporating perceptual quality as\nan additional dimension of evaluation. More recently, the\nrate-distortion-classification (RDC) function was investigated in [19],\nevaluating compression performance by considering classification accuracy\nalongside distortion. In this paper, we explore universal representations,\nwhere a single encoder is developed to achieve multiple decoding objectives\nacross various distortion and classification (or perception) constraints. This\nuniversality avoids retraining encoders for each specific operating point\nwithin these tradeoffs. Our experimental validation on the MNIST dataset\nindicates that a universal encoder incurs only minimal performance degradation\ncompared to individually optimized encoders for perceptual image compression\ntasks, aligning with prior results from [23]. Nonetheless, we also identify\nthat in the RDC setting, reusing an encoder optimized for one specific\nclassification-distortion tradeoff leads to a significant distortion penalty\nwhen applied to alternative points.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u591a\u79cd\u89e3\u7801\u76ee\u6807\u4e0b\u5f00\u53d1\u901a\u7528\u7f16\u7801\u5668\u7684\u53ef\u884c\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u611f\u77e5\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u63a5\u8fd1\u4e13\u7528\u7f16\u7801\u5668\uff0c\u4f46\u5728\u5206\u7c7b-\u5931\u771f\u6743\u8861\u4e0b\u5b58\u5728\u663e\u8457\u5931\u771f\u3002", "motivation": "\u4f20\u7edf\u538b\u7f29\u7b97\u6cd5\u4ec5\u5173\u6ce8\u538b\u7f29\u7387\u4e0e\u91cd\u5efa\u5931\u771f\u7684\u6743\u8861\uff0c\u800c\u65b0\u6846\u67b6\uff08\u5982RDP\u548cRDC\uff09\u5f15\u5165\u4e86\u611f\u77e5\u8d28\u91cf\u548c\u5206\u7c7b\u51c6\u786e\u6027\u4f5c\u4e3a\u989d\u5916\u7ef4\u5ea6\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u901a\u7528\u7f16\u7801\u5668\uff0c\u907f\u514d\u4e3a\u6bcf\u4e2a\u7279\u5b9a\u6743\u8861\u70b9\u91cd\u65b0\u8bad\u7ec3\u7f16\u7801\u5668\u3002", "method": "\u5f00\u53d1\u901a\u7528\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5b9e\u73b0\u591a\u79cd\u89e3\u7801\u76ee\u6807\uff08\u5982\u5931\u771f\u548c\u5206\u7c7b\u7ea6\u675f\uff09\uff0c\u5e76\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u7528\u7f16\u7801\u5668\u5728\u611f\u77e5\u538b\u7f29\u4efb\u52a1\u4e2d\u6027\u80fd\u63a5\u8fd1\u4e13\u7528\u7f16\u7801\u5668\uff0c\u4f46\u5728\u5206\u7c7b-\u5931\u771f\u6743\u8861\u4e0b\uff0c\u91cd\u7528\u7f16\u7801\u5668\u4f1a\u5bfc\u81f4\u663e\u8457\u5931\u771f\u3002", "conclusion": "\u901a\u7528\u7f16\u7801\u5668\u5728\u90e8\u5206\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7279\u5b9a\u6743\u8861\u4e0b\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2504.13208", "pdf": "https://arxiv.org/pdf/2504.13208", "abs": "https://arxiv.org/abs/2504.13208", "authors": ["Haomin Zuo", "Zhengyang Li", "Jiangchuan Gong", "Zhen Tian"], "title": "Intelligent road crack detection and analysis based on improved YOLOv8", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by IEEE - ICAACE 2025", "summary": "As urbanization speeds up and traffic flow increases, the issue of pavement\ndistress is becoming increasingly pronounced, posing a severe threat to road\nsafety and service life. Traditional methods of pothole detection rely on\nmanual inspection, which is not only inefficient but also costly. This paper\nproposes an intelligent road crack detection and analysis system, based on the\nenhanced YOLOv8 deep learning framework. A target segmentation model has been\ndeveloped through the training of 4029 images, capable of efficiently and\naccurately recognizing and segmenting crack regions in roads. The model also\nanalyzes the segmented regions to precisely calculate the maximum and minimum\nwidths of cracks and their exact locations. Experimental results indicate that\nthe incorporation of ECA and CBAM attention mechanisms substantially enhances\nthe model's detection accuracy and efficiency, offering a novel solution for\nroad maintenance and safety monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdbYOLOv8\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u667a\u80fd\u9053\u8def\u88c2\u7f1d\u68c0\u6d4b\u4e0e\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bad\u7ec34029\u5f20\u56fe\u50cf\u5f00\u53d1\u76ee\u6807\u5206\u5272\u6a21\u578b\uff0c\u9ad8\u6548\u51c6\u786e\u5730\u8bc6\u522b\u548c\u5206\u5272\u9053\u8def\u88c2\u7f1d\u533a\u57df\uff0c\u5e76\u5206\u6790\u88c2\u7f1d\u7684\u6700\u5927\u6700\u5c0f\u5bbd\u5ea6\u53ca\u4f4d\u7f6e\u3002\u5b9e\u9a8c\u8868\u660e\uff0cECA\u548cCBAM\u6ce8\u610f\u529b\u673a\u5236\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5316\u8fdb\u7a0b\u52a0\u5feb\u548c\u4ea4\u901a\u6d41\u91cf\u589e\u52a0\uff0c\u8def\u9762\u75c5\u5bb3\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u5bf9\u9053\u8def\u5b89\u5168\u548c\u4f7f\u7528\u5bff\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u6d4b\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u4e9f\u9700\u667a\u80fd\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u6539\u8fdb\u7684YOLOv8\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec34029\u5f20\u56fe\u50cf\u5f00\u53d1\u76ee\u6807\u5206\u5272\u6a21\u578b\uff0c\u7ed3\u5408ECA\u548cCBAM\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u9053\u8def\u88c2\u7f1d\u7684\u9ad8\u6548\u8bc6\u522b\u3001\u5206\u5272\u53ca\u5bbd\u5ea6\u548c\u4f4d\u7f6e\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f15\u5165ECA\u548cCBAM\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u9053\u8def\u7ef4\u62a4\u548c\u5b89\u5168\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u9ad8\u6548\u3001\u51c6\u786e\u7684\u7279\u70b9\u3002"}}
{"id": "2504.13211", "pdf": "https://arxiv.org/pdf/2504.13211", "abs": "https://arxiv.org/abs/2504.13211", "authors": ["Subin Kim", "Hoonrae Kim", "Jihyun Lee", "Yejin Jeon", "Gary Geunbae Lee"], "title": "Mirror: Multimodal Cognitive Reframing Therapy for Rolling with Resistance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent studies have explored the use of large language models (LLMs) in\npsychotherapy; however, text-based cognitive behavioral therapy (CBT) models\noften struggle with client resistance, which can weaken therapeutic alliance.\nTo address this, we propose a multimodal approach that incorporates nonverbal\ncues, allowing the AI therapist to better align its responses with the client's\nnegative emotional state. Specifically, we introduce a new synthetic dataset,\nMultimodal Interactive Rolling with Resistance (Mirror), which is a novel\nsynthetic dataset that pairs client statements with corresponding facial\nimages. Using this dataset, we train baseline Vision-Language Models (VLMs)\nthat can analyze facial cues, infer emotions, and generate empathetic responses\nto effectively manage resistance. They are then evaluated in terms of both the\ntherapist's counseling skills and the strength of the therapeutic alliance in\nthe presence of client resistance. Our results demonstrate that Mirror\nsignificantly enhances the AI therapist's ability to handle resistance, which\noutperforms existing text-based CBT approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u975e\u8bed\u8a00\u7ebf\u7d22\u63d0\u5347AI\u5fc3\u7406\u6cbb\u7597\u5e08\u5904\u7406\u5ba2\u6237\u62b5\u6297\u7684\u80fd\u529b\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6587\u672c\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u8ba4\u77e5\u884c\u4e3a\u6cbb\u7597\uff08CBT\uff09\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u5ba2\u6237\u62b5\u6297\uff0c\u5f71\u54cd\u6cbb\u7597\u6548\u679c\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u6570\u636e\u96c6Mirror\uff0c\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5206\u6790\u9762\u90e8\u7ebf\u7d22\u5e76\u751f\u6210\u5171\u60c5\u56de\u5e94\u3002", "result": "Mirror\u663e\u8457\u63d0\u5347AI\u6cbb\u7597\u5e08\u5904\u7406\u62b5\u6297\u7684\u80fd\u529b\uff0c\u4f18\u4e8e\u7eaf\u6587\u672c\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6cbb\u7597\u8054\u76df\uff0c\u6539\u5584AI\u5fc3\u7406\u6cbb\u7597\u7684\u6548\u679c\u3002"}}
{"id": "2504.13214", "pdf": "https://arxiv.org/pdf/2504.13214", "abs": "https://arxiv.org/abs/2504.13214", "authors": ["Andrew Kiruluta"], "title": "Wavelet-based Variational Autoencoders for High-Resolution Image Generation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models capable of\nlearning compact latent representations. However, conventional VAEs often\ngenerate relatively blurry images due to their assumption of an isotropic\nGaussian latent space and constraints in capturing high-frequency details. In\nthis paper, we explore a novel wavelet-based approach (Wavelet-VAE) in which\nthe latent space is constructed using multi-scale Haar wavelet coefficients. We\npropose a comprehensive method to encode the image features into multi-scale\ndetail and approximation coefficients and introduce a learnable noise parameter\nto maintain stochasticity. We thoroughly discuss how to reformulate the\nreparameterization trick, address the KL divergence term, and integrate wavelet\nsparsity principles into the training objective. Our experimental evaluation on\nCIFAR-10 and other high-resolution datasets demonstrates that the Wavelet-VAE\nimproves visual fidelity and recovers higher-resolution details compared to\nconventional VAEs. We conclude with a discussion of advantages, potential\nlimitations, and future research directions for wavelet-based generative\nmodeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08Wavelet-VAE\uff09\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6Haar\u5c0f\u6ce2\u7cfb\u6570\u6784\u5efa\u9690\u7a7a\u95f4\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u50cf\u7684\u6e05\u6670\u5ea6\u548c\u9ad8\u9891\u7ec6\u8282\u3002", "motivation": "\u4f20\u7edfVAE\u7531\u4e8e\u5047\u8bbe\u9690\u7a7a\u95f4\u4e3a\u5404\u5411\u540c\u6027\u9ad8\u65af\u5206\u5e03\uff0c\u751f\u6210\u7684\u56fe\u50cf\u8f83\u6a21\u7cca\uff0c\u65e0\u6cd5\u6355\u6349\u9ad8\u9891\u7ec6\u8282\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6Haar\u5c0f\u6ce2\u7cfb\u6570\u6784\u5efa\u9690\u7a7a\u95f4\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u566a\u58f0\u53c2\u6570\uff0c\u91cd\u65b0\u53c2\u6570\u5316\u6280\u5de7\uff0c\u5e76\u6574\u5408\u5c0f\u6ce2\u7a00\u758f\u6027\u5230\u8bad\u7ec3\u76ee\u6807\u4e2d\u3002", "result": "\u5728CIFAR-10\u7b49\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cWavelet-VAE\u751f\u6210\u7684\u56fe\u50cf\u89c6\u89c9\u4fdd\u771f\u5ea6\u66f4\u9ad8\uff0c\u7ec6\u8282\u66f4\u4e30\u5bcc\u3002", "conclusion": "Wavelet-VAE\u5728\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5c0f\u6ce2\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.13204", "pdf": "https://arxiv.org/pdf/2504.13204", "abs": "https://arxiv.org/abs/2504.13204", "authors": ["Dmytro Kotovenko", "Olga Grebenkova", "Bj\u00f6rn Ommer"], "title": "EDGS: Eliminating Densification for Efficient Convergence of 3DGS", "categories": ["cs.GR"], "comment": null, "summary": "3D Gaussian Splatting reconstructs scenes by starting from a sparse\nStructure-from-Motion initialization and iteratively refining\nunder-reconstructed regions. This process is inherently slow, as it requires\nmultiple densification steps where Gaussians are repeatedly split and adjusted,\nfollowing a lengthy optimization path. Moreover, this incremental approach\noften leads to suboptimal renderings, particularly in high-frequency regions\nwhere detail is critical.\n  We propose a fundamentally different approach: we eliminate densification\nprocess with a one-step approximation of scene geometry using triangulated\npixels from dense image correspondences. This dense initialization allows us to\nestimate rough geometry of the scene while preserving rich details from input\nRGB images, providing each Gaussian with well-informed colors, scales, and\npositions. As a result, we dramatically shorten the optimization path and\nremove the need for densification. Unlike traditional methods that rely on\nsparse keypoints, our dense initialization ensures uniform detail across the\nscene, even in high-frequency regions where 3DGS and other methods struggle.\nMoreover, since all splats are initialized in parallel at the start of\noptimization, we eliminate the need to wait for densification to adjust new\nGaussians.\n  Our method not only outperforms speed-optimized models in training efficiency\nbut also achieves higher rendering quality than state-of-the-art approaches,\nall while using only half the splats of standard 3DGS. It is fully compatible\nwith other 3DGS acceleration techniques, making it a versatile and efficient\nsolution that can be integrated with existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u96c6\u56fe\u50cf\u5bf9\u5e94\u7684\u4e00\u6b65\u51e0\u4f55\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u7684\u591a\u6b21\u7ec6\u5316\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u901a\u8fc7\u591a\u6b21\u7ec6\u5316\u6b65\u9aa4\u91cd\u5efa\u573a\u666f\uff0c\u901f\u5ea6\u6162\u4e14\u5728\u9ad8\u9891\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5229\u7528\u5bc6\u96c6\u56fe\u50cf\u5bf9\u5e94\u751f\u6210\u4e09\u89d2\u5316\u50cf\u7d20\uff0c\u4e00\u6b65\u8fd1\u4f3c\u573a\u666f\u51e0\u4f55\uff0c\u521d\u59cb\u5316\u9ad8\u65af\u53c2\u6570\uff08\u989c\u8272\u3001\u5c3a\u5ea6\u3001\u4f4d\u7f6e\uff09\uff0c\u65e0\u9700\u540e\u7eed\u7ec6\u5316\u3002", "result": "\u65b9\u6cd5\u5728\u8bad\u7ec3\u6548\u7387\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e14\u4ec5\u9700\u6807\u51c63DGS\u4e00\u534a\u7684\u9ad8\u65af\u6cfc\u6e85\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u6280\u672f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u9891\u7ec6\u8282\u4e30\u5bcc\u7684\u573a\u666f\u3002"}}
{"id": "2504.13187", "pdf": "https://arxiv.org/pdf/2504.13187", "abs": "https://arxiv.org/abs/2504.13187", "authors": ["In Hak Moon"], "title": "Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis", "categories": ["cs.CL"], "comment": null, "summary": "This study presents a comprehensive evaluation of five leading large language\nmodels (LLMs) - Chat GPT 4o, Copilot Pro, Gemini Advanced, Claude Pro, and Meta\nAI - on their performance in solving calculus differentiation problems. The\ninvestigation assessed these models across 13 fundamental problem types,\nemploying a systematic cross-evaluation framework where each model solved\nproblems generated by all models. Results revealed significant performance\ndisparities, with Chat GPT 4o achieving the highest success rate (94.71%),\nfollowed by Claude Pro (85.74%), Gemini Advanced (84.42%), Copilot Pro\n(76.30%), and Meta AI (56.75%). All models excelled at procedural\ndifferentiation tasks but showed varying limitations with conceptual\nunderstanding and algebraic manipulation. Notably, problems involving\nincreasing/decreasing intervals and optimization word problems proved most\nchallenging across all models. The cross-evaluation matrix revealed that Claude\nPro generated the most difficult problems, suggesting distinct capabilities\nbetween problem generation and problem-solving. These findings have significant\nimplications for educational applications, highlighting both the potential and\nlimitations of LLMs as calculus learning tools. While they demonstrate\nimpressive procedural capabilities, their conceptual understanding remains\nlimited compared to human mathematical reasoning, emphasizing the continued\nimportance of human instruction for developing deeper mathematical\ncomprehension.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u5fae\u79ef\u5206\u5fae\u5206\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0Chat GPT 4o\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5728\u6982\u5ff5\u7406\u89e3\u548c\u4ee3\u6570\u64cd\u4f5c\u4e0a\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u6570\u5b66\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u63ed\u793a\u5176\u4f5c\u4e3a\u5b66\u4e60\u5de5\u5177\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u4ea4\u53c9\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5\u4e94\u79cd\u6a21\u578b\u572813\u79cd\u57fa\u7840\u95ee\u9898\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\u3002", "result": "Chat GPT 4o\u6210\u529f\u7387\u6700\u9ad8\uff0894.71%\uff09\uff0cClaude Pro\u751f\u6210\u7684\u95ee\u9898\u6700\u96be\uff0c\u6240\u6709\u6a21\u578b\u5728\u6982\u5ff5\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "LLMs\u5728\u7a0b\u5e8f\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6982\u5ff5\u7406\u89e3\u6709\u9650\uff0c\u4ecd\u9700\u4eba\u7c7b\u6559\u5b66\u652f\u6301\u3002"}}
{"id": "2504.13220", "pdf": "https://arxiv.org/pdf/2504.13220", "abs": "https://arxiv.org/abs/2504.13220", "authors": ["Ummay Maria Muna", "Md. Mehedi Hasan Shawon", "Md Jobayer", "Sumaiya Akter", "Saifur Rahman Sabuj"], "title": "SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification", "categories": ["cs.CV", "cs.LG", "68T10 (Primary) 68T07(Secondary)", "I.2.1; I.5.4"], "comment": "11 pages", "summary": "Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor\nimagery classification offer promising solutions in neurorehabilitation and\nassistive technologies by enabling communication between the brain and external\ndevices. However, the non-stationary nature of EEG signals and significant\ninter-subject variability cause substantial challenges for developing robust\ncross-subject classification models. This paper introduces a novel\nSpatial-Spectral-Temporal Attention Fusion (SSTAF) Transformer specifically\ndesigned for upper-limb motor imagery classification. Our architecture consists\nof a spectral transformer and a spatial transformer, followed by a transformer\nblock and a classifier network. Each module is integrated with attention\nmechanisms that dynamically attend to the most discriminative patterns across\nmultiple domains, such as spectral frequencies, spatial electrode locations,\nand temporal dynamics. The short-time Fourier transform is incorporated to\nextract features in the time-frequency domain to make it easier for the model\nto obtain a better feature distinction. We evaluated our SSTAF Transformer\nmodel on two publicly available datasets, the EEGMMIDB dataset, and BCI\nCompetition IV-2a. SSTAF Transformer achieves an accuracy of 76.83% and 68.30%\nin the data sets, respectively, outperforms traditional CNN-based architectures\nand a few existing transformer-based approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684SSTAF Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3EEG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u8de8\u88ab\u8bd5\u5206\u7c7b\u95ee\u9898\uff0c\u5728\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u7684\u975e\u5e73\u7a33\u6027\u548c\u88ab\u8bd5\u95f4\u5dee\u5f02\u5bfc\u81f4\u8de8\u88ab\u8bd5\u5206\u7c7b\u6a21\u578b\u96be\u4ee5\u5f00\u53d1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86SSTAF Transformer\uff0c\u7ed3\u5408\u4e86\u8c31\u3001\u7a7a\u95f4\u548c\u65f6\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u63d0\u53d6\u65f6\u9891\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523076.83%\u548c68.30%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4f20\u7edfCNN\u548c\u73b0\u6709Transformer\u65b9\u6cd5\u3002", "conclusion": "SSTAF Transformer\u5728EEG\u8fd0\u52a8\u60f3\u8c61\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u795e\u7ecf\u5eb7\u590d\u548c\u8f85\u52a9\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13206", "pdf": "https://arxiv.org/pdf/2504.13206", "abs": "https://arxiv.org/abs/2504.13206", "authors": ["Aniket Roy", "Shubhankar Borse", "Shreya Kadambi", "Debasmit Das", "Shweta Mahajan", "Risheek Garrepalli", "Hyojin Park", "Ankita Nayak", "Rama Chellappa", "Munawar Hayat", "Fatih Porikli"], "title": "DuoLoRA : Cycle-consistent and Rank-disentangled Content-Style Personalization", "categories": ["cs.GR"], "comment": null, "summary": "We tackle the challenge of jointly personalizing content and style from a few\nexamples. A promising approach is to train separate Low-Rank Adapters (LoRA)\nand merge them effectively, preserving both content and style. Existing\nmethods, such as ZipLoRA, treat content and style as independent entities,\nmerging them by learning masks in LoRA's output dimensions. However, content\nand style are intertwined, not independent. To address this, we propose\nDuoLoRA, a content-style personalization framework featuring three key\ncomponents: (i) rank-dimension mask learning, (ii) effective merging via layer\npriors, and (iii) Constyle loss, which leverages cycle-consistency in the\nmerging process. First, we introduce ZipRank, which performs content-style\nmerging within the rank dimension, offering adaptive rank flexibility and\nsignificantly reducing the number of learnable parameters. Additionally, we\nincorporate SDXL layer priors to apply implicit rank constraints informed by\neach layer's content-style bias and adaptive merger initialization, enhancing\nthe integration of content and style. To further refine the merging process, we\nintroduce Constyle loss, which leverages the cycle-consistency between content\nand style. Our experimental results demonstrate that DuoLoRA outperforms\nstate-of-the-art content-style merging methods across multiple benchmarks.", "AI": {"tldr": "DuoLoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5185\u5bb9-\u98ce\u683c\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u79e9\u7ef4\u5ea6\u63a9\u7801\u5b66\u4e60\u3001\u5c42\u5148\u9a8c\u6709\u6548\u5408\u5e76\u548cConstyle\u635f\u5931\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c06\u5185\u5bb9\u4e0e\u98ce\u683c\u89c6\u4e3a\u72ec\u7acb\u5b9e\u4f53\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982ZipLoRA\uff09\u5c06\u5185\u5bb9\u548c\u98ce\u683c\u89c6\u4e3a\u72ec\u7acb\u5b9e\u4f53\uff0c\u800c\u5b9e\u9645\u4e0a\u4e8c\u8005\u662f\u4ea4\u7ec7\u7684\u3002DuoLoRA\u65e8\u5728\u66f4\u6709\u6548\u5730\u5408\u5e76\u5185\u5bb9\u548c\u98ce\u683c\u3002", "method": "DuoLoRA\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u79e9\u7ef4\u5ea6\u63a9\u7801\u5b66\u4e60\uff08ZipRank\uff09\u3001\u57fa\u4e8eSDXL\u5c42\u5148\u9a8c\u7684\u6709\u6548\u5408\u5e76\u548cConstyle\u635f\u5931\uff08\u5229\u7528\u5faa\u73af\u4e00\u81f4\u6027\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDuoLoRA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u5185\u5bb9-\u98ce\u683c\u5408\u5e76\u65b9\u6cd5\u3002", "conclusion": "DuoLoRA\u901a\u8fc7\u66f4\u7d27\u5bc6\u7684\u5185\u5bb9-\u98ce\u683c\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u4e2a\u6027\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13189", "pdf": "https://arxiv.org/pdf/2504.13189", "abs": "https://arxiv.org/abs/2504.13189", "authors": ["Sohom Ghosh", "Sudip Kumar Naskar"], "title": "BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models", "categories": ["cs.CL", "q-fin.ST"], "comment": "The codes and the datasets can be accessed from\n  https://huggingface.co/datasets/sohomghosh/BASIR_Budget_Assisted_Sectoral_Impact_Ranking/tree/main/", "summary": "Government fiscal policies, particularly annual union budgets, exert\nsignificant influence on financial markets. However, real-time analysis of\nbudgetary impacts on sector-specific equity performance remains\nmethodologically challenging and largely unexplored. This study proposes a\nframework to systematically identify and rank sectors poised to benefit from\nIndia's Union Budget announcements. The framework addresses two core tasks: (1)\nmulti-label classification of excerpts from budget transcripts into 81\npredefined economic sectors, and (2) performance ranking of these sectors.\nLeveraging a comprehensive corpus of Indian Union Budget transcripts from 1947\nto 2025, we introduce BASIR (Budget-Assisted Sectoral Impact Ranking), an\nannotated dataset mapping excerpts from budgetary transcripts to sectoral\nimpacts. Our architecture incorporates fine-tuned embeddings for sector\nidentification, coupled with language models that rank sectors based on their\npredicted performances. Our results demonstrate 0.605 F1-score in sector\nclassification, and 0.997 NDCG score in predicting ranks of sectors based on\npost-budget performances. The methodology enables investors and policymakers to\nquantify fiscal policy impacts through structured, data-driven insights,\naddressing critical gaps in manual analysis. The annotated dataset has been\nreleased under CC-BY-NC-SA-4.0 license to advance computational economics\nresearch.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6BASIR\uff0c\u7528\u4e8e\u5206\u6790\u548c\u9884\u6d4b\u5370\u5ea6\u5e74\u5ea6\u9884\u7b97\u5bf9\u7279\u5b9a\u7ecf\u6d4e\u90e8\u95e8\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u591a\u6807\u7b7e\u5206\u7c7b\u548c\u90e8\u95e8\u7ee9\u6548\u6392\u540d\u5b9e\u73b0\u3002", "motivation": "\u653f\u5e9c\u8d22\u653f\u653f\u7b56\u5bf9\u91d1\u878d\u5e02\u573a\u5f71\u54cd\u663e\u8457\uff0c\u4f46\u5b9e\u65f6\u5206\u6790\u9884\u7b97\u5bf9\u90e8\u95e8\u80a1\u7968\u8868\u73b0\u7684\u5f71\u54cd\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "method": "\u5229\u75281947\u81f32025\u5e74\u5370\u5ea6\u9884\u7b97\u6587\u672c\uff0c\u7ed3\u5408\u7ec6\u8c03\u5d4c\u5165\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u8fdb\u884c\u90e8\u95e8\u5206\u7c7b\u548c\u7ee9\u6548\u6392\u540d\u3002", "result": "\u90e8\u95e8\u5206\u7c7bF1\u5206\u6570\u4e3a0.605\uff0c\u7ee9\u6548\u6392\u540dNDCG\u5206\u6570\u4e3a0.997\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6570\u636e\u9a71\u52a8\u7684\u653f\u7b56\u5f71\u54cd\u91cf\u5316\u5de5\u5177\uff0c\u586b\u8865\u4e86\u624b\u52a8\u5206\u6790\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.13224", "pdf": "https://arxiv.org/pdf/2504.13224", "abs": "https://arxiv.org/abs/2504.13224", "authors": ["Fuwei Liu"], "title": "ICAS: IP Adapter and ControlNet-based Attention Structure for Multi-Subject Style Transfer Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 6 figures", "summary": "Generating multi-subject stylized images remains a significant challenge due\nto the ambiguity in defining style attributes (e.g., color, texture,\natmosphere, and structure) and the difficulty in consistently applying them\nacross multiple subjects. Although recent diffusion-based text-to-image models\nhave achieved remarkable progress, existing methods typically rely on\ncomputationally expensive inversion procedures or large-scale stylized\ndatasets. Moreover, these methods often struggle with maintaining multi-subject\nsemantic fidelity and are limited by high inference costs. To address these\nlimitations, we propose ICAS (IP-Adapter and ControlNet-based Attention\nStructure), a novel framework for efficient and controllable multi-subject\nstyle transfer. Instead of full-model tuning, ICAS adaptively fine-tunes only\nthe content injection branch of a pre-trained diffusion model, thereby\npreserving identity-specific semantics while enhancing style controllability.\nBy combining IP-Adapter for adaptive style injection with ControlNet for\nstructural conditioning, our framework ensures faithful global layout\npreservation alongside accurate local style synthesis. Furthermore, ICAS\nintroduces a cyclic multi-subject content embedding mechanism, which enables\neffective style transfer under limited-data settings without the need for\nextensive stylized corpora. Extensive experiments show that ICAS achieves\nsuperior performance in structure preservation, style consistency, and\ninference efficiency, establishing a new paradigm for multi-subject style\ntransfer in real-world applications.", "AI": {"tldr": "ICAS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIP-Adapter\u548cControlNet\u7684\u9ad8\u6548\u53ef\u63a7\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u9762\u4e34\u98ce\u683c\u5c5e\u6027\u5b9a\u4e49\u6a21\u7cca\u548c\u8de8\u4e3b\u4f53\u4e00\u81f4\u6027\u5e94\u7528\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u8ba1\u7b97\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4e14\u96be\u4ee5\u4fdd\u6301\u8bed\u4e49\u4fdd\u771f\u5ea6\u3002", "method": "ICAS\u901a\u8fc7\u81ea\u9002\u5e94\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u5185\u5bb9\u6ce8\u5165\u5206\u652f\uff0c\u7ed3\u5408IP-Adapter\u548cControlNet\uff0c\u5b9e\u73b0\u9ad8\u6548\u98ce\u683c\u6ce8\u5165\u548c\u7ed3\u6784\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eICAS\u5728\u7ed3\u6784\u4fdd\u7559\u3001\u98ce\u683c\u4e00\u81f4\u6027\u548c\u63a8\u7406\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ICAS\u4e3a\u591a\u4e3b\u4f53\u98ce\u683c\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u63a7\u7684\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13207", "pdf": "https://arxiv.org/pdf/2504.13207", "abs": "https://arxiv.org/abs/2504.13207", "authors": ["Wenhua Wu", "Tong Zhao", "Chensheng Peng", "Lei Yang", "Yintao Wei", "Zhe Liu", "Hesheng Wang"], "title": "BEV-GS: Feed-forward Gaussian Splatting in Bird's-Eye-View for Road Reconstruction", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "Road surface is the sole contact medium for wheels or robot feet.\nReconstructing road surface is crucial for unmanned vehicles and mobile robots.\nRecent studies on Neural Radiance Fields (NeRF) and Gaussian Splatting (GS)\nhave achieved remarkable results in scene reconstruction. However, they\ntypically rely on multi-view image inputs and require prolonged optimization\ntimes. In this paper, we propose BEV-GS, a real-time single-frame road surface\nreconstruction method based on feed-forward Gaussian splatting. BEV-GS consists\nof a prediction module and a rendering module. The prediction module introduces\nseparate geometry and texture networks following Bird's-Eye-View paradigm.\nGeometric and texture parameters are directly estimated from a single frame,\navoiding per-scene optimization. In the rendering module, we utilize grid\nGaussian for road surface representation and novel view synthesis, which better\naligns with road surface characteristics. Our method achieves state-of-the-art\nperformance on the real-world dataset RSRD. The road elevation error reduces to\n1.73 cm, and the PSNR of novel view synthesis reaches 28.36 dB. The prediction\nand rendering FPS is 26, and 2061, respectively, enabling high-accuracy and\nreal-time applications. The code will be available at:\n\\href{https://github.com/cat-wwh/BEV-GS}{\\texttt{https://github.com/cat-wwh/BEV-GS}}", "AI": {"tldr": "BEV-GS\u662f\u4e00\u79cd\u57fa\u4e8e\u524d\u9988\u9ad8\u65af\u6e85\u5c04\u7684\u5b9e\u65f6\u5355\u5e27\u8def\u9762\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u7684\u51e0\u4f55\u548c\u7eb9\u7406\u7f51\u7edc\u76f4\u63a5\u4ece\u5355\u5e27\u56fe\u50cf\u4f30\u8ba1\u53c2\u6570\uff0c\u907f\u514d\u4e86\u9010\u573a\u666f\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u8def\u9762\u662f\u8f66\u8f6e\u6216\u673a\u5668\u4eba\u8db3\u90e8\u7684\u552f\u4e00\u63a5\u89e6\u4ecb\u8d28\uff0c\u91cd\u5efa\u8def\u9762\u5bf9\u65e0\u4eba\u8f66\u8f86\u548c\u79fb\u52a8\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5982NeRF\u548cGS\u4f9d\u8d56\u591a\u89c6\u56fe\u8f93\u5165\u4e14\u4f18\u5316\u65f6\u95f4\u957f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "BEV-GS\u7531\u9884\u6d4b\u6a21\u5757\u548c\u6e32\u67d3\u6a21\u5757\u7ec4\u6210\u3002\u9884\u6d4b\u6a21\u5757\u91c7\u7528\u9e1f\u77b0\u89c6\u89d2\u8303\u5f0f\uff0c\u901a\u8fc7\u51e0\u4f55\u548c\u7eb9\u7406\u7f51\u7edc\u76f4\u63a5\u4ece\u5355\u5e27\u56fe\u50cf\u4f30\u8ba1\u53c2\u6570\u3002\u6e32\u67d3\u6a21\u5757\u4f7f\u7528\u7f51\u683c\u9ad8\u65af\u8868\u793a\u8def\u9762\u5e76\u8fdb\u884c\u65b0\u89c6\u89d2\u5408\u6210\u3002", "result": "\u5728RSRD\u6570\u636e\u96c6\u4e0a\uff0cBEV-GS\u7684\u8def\u9762\u9ad8\u7a0b\u8bef\u5dee\u964d\u81f31.73\u5398\u7c73\uff0c\u65b0\u89c6\u89d2\u5408\u6210\u7684PSNR\u8fbe\u523028.36 dB\uff0c\u9884\u6d4b\u548c\u6e32\u67d3\u5e27\u7387\u5206\u522b\u4e3a26\u548c2061 FPS\u3002", "conclusion": "BEV-GS\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u65e0\u4eba\u8f66\u8f86\u548c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8def\u9762\u91cd\u5efa\u9700\u6c42\u3002"}}
{"id": "2504.13216", "pdf": "https://arxiv.org/pdf/2504.13216", "abs": "https://arxiv.org/abs/2504.13216", "authors": ["Bokwang Hwang", "Seonkyu Lim", "Taewoong Kim", "Yongjae Geun", "Sunghyun Bang", "Sohyun Park", "Jihyun Park", "Myeonggyu Lee", "Jinwoo Lee", "Yerin Kim", "Jinsun Yoo", "Jingyeong Hong", "Jina Park", "Yongchan Kim", "Suhyun Kim", "Younggyun Hahm", "Yiseul Lee", "Yejee Kang", "Chanhyuk Yoon", "Chansu Lee", "Heeyewon Jeong", "Jiyeon Lee", "Seonhye Gu", "Hyebin Kang", "Yousang Cho", "Hangyeol Yoo", "KyungTae Lim"], "title": "KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce KFinEval-Pilot, a benchmark suite specifically designed to\nevaluate large language models (LLMs) in the Korean financial domain.\nAddressing the limitations of existing English-centric benchmarks,\nKFinEval-Pilot comprises over 1,000 curated questions across three critical\nareas: financial knowledge, legal reasoning, and financial toxicity. The\nbenchmark is constructed through a semi-automated pipeline that combines\nGPT-4-generated prompts with expert validation to ensure domain relevance and\nfactual accuracy. We evaluate a range of representative LLMs and observe\nnotable performance differences across models, with trade-offs between task\naccuracy and output safety across different model families. These results\nhighlight persistent challenges in applying LLMs to high-stakes financial\napplications, particularly in reasoning and safety. Grounded in real-world\nfinancial use cases and aligned with the Korean regulatory and linguistic\ncontext, KFinEval-Pilot serves as an early diagnostic tool for developing safer\nand more reliable financial AI systems.", "AI": {"tldr": "KFinEval-Pilot\u662f\u4e00\u4e2a\u9488\u5bf9\u97e9\u8bed\u91d1\u878d\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b1000\u591a\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u91d1\u878d\u77e5\u8bc6\u3001\u6cd5\u5f8b\u63a8\u7406\u548c\u91d1\u878d\u6bd2\u6027\u4e09\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efa\uff0c\u7ed3\u5408GPT-4\u751f\u6210\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8f93\u51fa\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\u7684\u57fa\u51c6\u5728\u91d1\u878d\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u97e9\u8bed\u91d1\u878dAI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65e9\u671f\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7ed3\u5408GPT-4\u751f\u6210\u95ee\u9898\u548c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u6784\u5efa\u5305\u542b\u91d1\u878d\u77e5\u8bc6\u3001\u6cd5\u5f8b\u63a8\u7406\u548c\u91d1\u878d\u6bd2\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u4e0d\u540cLLM\u5728\u4efb\u52a1\u51c6\u786e\u6027\u548c\u8f93\u51fa\u5b89\u5168\u6027\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86\u91d1\u878d\u5e94\u7528\u4e2d\u63a8\u7406\u548c\u5b89\u5168\u6027\u7684\u6311\u6218\u3002", "conclusion": "KFinEval-Pilot\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u91d1\u878dAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u7279\u522b\u9002\u7528\u4e8e\u97e9\u8bed\u91d1\u878d\u9886\u57df\u3002"}}
{"id": "2504.13231", "pdf": "https://arxiv.org/pdf/2504.13231", "abs": "https://arxiv.org/abs/2504.13231", "authors": ["Braeden Sherritt", "Isar Nejadgholi", "Marzieh Amini"], "title": "WildFireCan-MMD: A Multimodal dataset for Classification of User-generated Content During Wildfires in Canada", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross 13 key themes. Evaluating both Vision Language Models and custom-trained\nclassifiers, we show that while zero-shot prompting offers quick deployment,\neven simple trained models outperform them when labelled data is available, by\nup to 23%. Our findings highlight the enduring importance of tailored datasets\nand task-specific training. Importantly, such datasets should be localized, as\ndisaster response requirements vary across regions and contexts.", "AI": {"tldr": "WildFireCan-MMD\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u4e2d\u63d0\u53d6\u91ce\u706b\u76f8\u5173\u4fe1\u606f\uff0c\u7814\u7a76\u8868\u660e\u5b9a\u5236\u5316\u8bad\u7ec3\u6a21\u578b\u4f18\u4e8e\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u6e90\u5728\u91ce\u706b\u4e8b\u4ef6\u4e2d\u53cd\u5e94\u6162\u4e14\u6210\u672c\u9ad8\uff0c\u793e\u4ea4\u5a92\u4f53\u63d0\u4f9b\u5b9e\u65f6\u4fe1\u606f\u4f46\u63d0\u53d6\u76f8\u5173\u6d1e\u5bdf\u56f0\u96be\u3002", "method": "\u6784\u5efaWildFireCan-MMD\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5b9a\u5236\u8bad\u7ec3\u5206\u7c7b\u5668\u3002", "result": "\u5b9a\u5236\u8bad\u7ec3\u6a21\u578b\u6bd4\u96f6\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe23%\u3002", "conclusion": "\u5f3a\u8c03\u5b9a\u5236\u5316\u6570\u636e\u96c6\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u4e14\u6570\u636e\u96c6\u9700\u672c\u5730\u5316\u4ee5\u9002\u5e94\u4e0d\u540c\u5730\u533a\u9700\u6c42\u3002"}}
{"id": "2504.13226", "pdf": "https://arxiv.org/pdf/2504.13226", "abs": "https://arxiv.org/abs/2504.13226", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Xiaoming Wei", "Enhua Wu"], "title": "Image Editing with Diffusion Models: A Survey", "categories": ["cs.GR"], "comment": null, "summary": "With deeper exploration of diffusion model, developments in the field of\nimage generation have triggered a boom in image creation. As the quality of\nbase-model generated images continues to improve, so does the demand for\nfurther application like image editing. In recent years, many remarkable works\nare realizing a wide variety of editing effects. However, the wide variety of\nediting types and diverse editing approaches have made it difficult for\nresearchers to establish a comprehensive view of the development of this field.\nIn this survey, we summarize the image editing field from four aspects: tasks\ndefinition, methods classification, results evaluation and editing datasets.\nFirst, we provide a definition of image editing, which in turn leads to a\nvariety of editing task forms from the perspective of operation parts and\nmanipulation actions. Subsequently, we categorize and summary methods for\nimplementing editing into three categories: inversion-based, fine-tuning-based\nand adapter-based. In addition, we organize the currently used metrics,\navailable datasets and corresponding construction methods. At the end, we\npresent some visions for the future development of the image editing field\nbased on the previous summaries.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\u9886\u57df\uff0c\u4ece\u4efb\u52a1\u5b9a\u4e49\u3001\u65b9\u6cd5\u5206\u7c7b\u3001\u7ed3\u679c\u8bc4\u4f30\u548c\u6570\u636e\u96c6\u56db\u4e2a\u65b9\u9762\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u53d1\u5c55\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u56fe\u50cf\u7f16\u8f91\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u7f16\u8f91\u7c7b\u578b\u548c\u65b9\u6cd5\u591a\u6837\uff0c\u7f3a\u4e4f\u5168\u9762\u89c6\u89d2\u3002", "method": "\u4ece\u64cd\u4f5c\u90e8\u5206\u548c\u52a8\u4f5c\u89d2\u5ea6\u5b9a\u4e49\u7f16\u8f91\u4efb\u52a1\uff0c\u5c06\u65b9\u6cd5\u5206\u4e3a\u57fa\u4e8e\u53cd\u8f6c\u3001\u5fae\u8c03\u548c\u9002\u914d\u5668\u7684\u4e09\u7c7b\uff0c\u5e76\u6574\u7406\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\u3002", "result": "\u603b\u7ed3\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u4efb\u52a1\u5f62\u5f0f\u3001\u65b9\u6cd5\u5206\u7c7b\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u63d0\u4f9b\u7cfb\u7edf\u89c6\u89d2\u3002", "conclusion": "\u57fa\u4e8e\u5f53\u524d\u603b\u7ed3\uff0c\u63d0\u51fa\u4e86\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u672a\u6765\u53d1\u5c55\u7684\u613f\u666f\u3002"}}
{"id": "2504.13217", "pdf": "https://arxiv.org/pdf/2504.13217", "abs": "https://arxiv.org/abs/2504.13217", "authors": ["Jennifer Haase", "Finn Klessascheck", "Jan Mendling", "Sebastian Pokutta"], "title": "Sustainability via LLM Right-sizing", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 2 Figures, 6 Tables", "summary": "Large language models (LLMs) have become increasingly embedded in\norganizational workflows. This has raised concerns over their energy\nconsumption, financial costs, and data sovereignty. While performance\nbenchmarks often celebrate cutting-edge models, real-world deployment decisions\nrequire a broader perspective: when is a smaller, locally deployable model\n\"good enough\"? This study offers an empirical answer by evaluating eleven\nproprietary and open-weight LLMs across ten everyday occupational tasks,\nincluding summarizing texts, generating schedules, and drafting emails and\nproposals. Using a dual-LLM-based evaluation framework, we automated task\nexecution and standardized evaluation across ten criteria related to output\nquality, factual accuracy, and ethical responsibility. Results show that GPT-4o\ndelivers consistently superior performance but at a significantly higher cost\nand environmental footprint. Notably, smaller models like Gemma-3 and Phi-4\nachieved strong and reliable results on most tasks, suggesting their viability\nin contexts requiring cost-efficiency, local deployment, or privacy. A cluster\nanalysis revealed three model groups -- premium all-rounders, competent\ngeneralists, and limited but safe performers -- highlighting trade-offs between\nquality, control, and sustainability. Significantly, task type influenced model\neffectiveness: conceptual tasks challenged most models, while aggregation and\ntransformation tasks yielded better performances. We argue for a shift from\nperformance-maximizing benchmarks to task- and context-aware sufficiency\nassessments that better reflect organizational priorities. Our approach\ncontributes a scalable method to evaluate AI models through a sustainability\nlens and offers actionable guidance for responsible LLM deployment in practice.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8611\u79cdLLM\u572810\u79cd\u65e5\u5e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u6027\u80fd\u6700\u4f18\u4f46\u6210\u672c\u9ad8\uff0c\u800c\u5c0f\u578b\u6a21\u578b\u5982Gemma-3\u548cPhi-4\u5728\u6210\u672c\u3001\u9690\u79c1\u548c\u672c\u5730\u90e8\u7f72\u65b9\u9762\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u63a2\u8ba8\u5728\u7ec4\u7ec7\u5de5\u4f5c\u6d41\u4e2d\uff0c\u5982\u4f55\u5728\u6027\u80fd\u3001\u6210\u672c\u548c\u53ef\u6301\u7eed\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\uff0c\u907f\u514d\u8fc7\u5ea6\u4f9d\u8d56\u9ad8\u6027\u80fd\u4f46\u9ad8\u6210\u672c\u7684LLM\u3002", "method": "\u4f7f\u7528\u53ccLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u4efb\u52a1\u6267\u884c\u5e76\u6807\u51c6\u5316\u8bc4\u4f3010\u9879\u6807\u51c6\uff0c\u6db5\u76d6\u8f93\u51fa\u8d28\u91cf\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u4f26\u7406\u8d23\u4efb\u3002", "result": "GPT-4o\u8868\u73b0\u6700\u4f18\u4f46\u6210\u672c\u9ad8\uff0c\u5c0f\u578b\u6a21\u578b\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\uff1b\u4efb\u52a1\u7c7b\u578b\u5f71\u54cd\u6a21\u578b\u6548\u679c\u3002", "conclusion": "\u5efa\u8bae\u4ece\u6027\u80fd\u6700\u5927\u5316\u8f6c\u5411\u4efb\u52a1\u548c\u60c5\u5883\u611f\u77e5\u7684\u8bc4\u4f30\uff0c\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u7ec4\u7ec7\u9700\u6c42\uff0c\u63a8\u52a8\u53ef\u6301\u7eed\u7684LLM\u90e8\u7f72\u3002"}}
{"id": "2504.13242", "pdf": "https://arxiv.org/pdf/2504.13242", "abs": "https://arxiv.org/abs/2504.13242", "authors": ["Muhammad Ahmad", "Manuel Mazzara", "Salvatore Distefano", "Adil Mehmood Khan"], "title": "Dynamic Memory-enhanced Transformer for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) classification remains a challenging task due to\nthe intricate spatial-spectral correlations. Existing transformer models excel\nin capturing long-range dependencies but often suffer from information\nredundancy and attention inefficiencies, limiting their ability to model\nfine-grained relationships crucial for HSI classification. To overcome these\nlimitations, this work proposes MemFormer, a lightweight and memory-enhanced\ntransformer. MemFormer introduces a memory-enhanced multi-head attention\nmechanism that iteratively refines a dynamic memory module, enhancing feature\nextraction while reducing redundancy across layers. Additionally, a dynamic\nmemory enrichment strategy progressively captures complex spatial and spectral\ndependencies, leading to more expressive feature representations. To further\nimprove structural consistency, we incorporate a spatial-spectral positional\nencoding (SSPE) tailored for HSI data, ensuring continuity without the\ncomputational burden of convolution-based approaches. Extensive experiments on\nbenchmark datasets demonstrate that MemFormer achieves superior classification\naccuracy, outperforming state-of-the-art methods.", "AI": {"tldr": "MemFormer\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u5185\u5b58\u589e\u5f3a\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7a7a\u95f4-\u5149\u8c31\u76f8\u5173\u6027\u590d\u6742\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u5185\u5b58\u6a21\u5757\u548c\u7a7a\u95f4-\u5149\u8c31\u4f4d\u7f6e\u7f16\u7801\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u65f6\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u548c\u6ce8\u610f\u529b\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5efa\u6a21\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6240\u9700\u7684\u7ec6\u7c92\u5ea6\u5173\u7cfb\u3002", "method": "\u63d0\u51faMemFormer\uff0c\u91c7\u7528\u5185\u5b58\u589e\u5f3a\u7684\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u5185\u5b58\u4e30\u5bcc\u7b56\u7565\uff0c\u7ed3\u5408\u7a7a\u95f4-\u5149\u8c31\u4f4d\u7f6e\u7f16\u7801\uff08SSPE\uff09\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMemFormer\u7684\u5206\u7c7b\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MemFormer\u901a\u8fc7\u52a8\u6001\u5185\u5b58\u6a21\u5757\u548cSSPE\u6709\u6548\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u3002"}}
{"id": "2504.13339", "pdf": "https://arxiv.org/pdf/2504.13339", "abs": "https://arxiv.org/abs/2504.13339", "authors": ["Landon Dyken", "Andres Sewell", "Will Usher", "Steve Petruzza", "Sidharth Kumar"], "title": "Volume Encoding Gaussians: Transfer Function-Agnostic 3D Gaussians for Volume Rendering", "categories": ["cs.GR"], "comment": null, "summary": "While HPC resources are increasingly being used to produce adaptively refined\nor unstructured volume datasets, current research in applying machine\nlearning-based representation to visualization has largely ignored this type of\ndata. To address this, we introduce Volume Encoding Gaussians (VEG), a novel 3D\nGaussian-based representation for scientific volume visualization focused on\nunstructured volumes. Unlike prior 3D Gaussian Splatting (3DGS) methods that\nstore view-dependent color and opacity for each Gaussian, VEG decouple the\nvisual appearance from the data representation by encoding only scalar values,\nenabling transfer-function-agnostic rendering of 3DGS models for interactive\nscientific visualization. VEG are directly initialized from volume datasets,\neliminating the need for structure-from-motion pipelines like COLMAP. To ensure\ncomplete scalar field coverage, we introduce an opacity-guided training\nstrategy, using differentiable rendering with multiple transfer functions to\noptimize our data representation. This allows VEG to preserve fine features\nacross the full scalar range of a dataset while remaining independent of any\nspecific transfer function. Each Gaussian is scaled and rotated to adapt to\nlocal geometry, allowing for efficient representation of unstructured meshes\nwithout storing mesh connectivity and while using far fewer primitives. Across\na diverse set of data, VEG achieve high reconstruction quality, compress large\nvolume datasets by up to 3600x, and support lightning-fast rendering on\ncommodity GPUs, enabling interactive visualization of large-scale structured\nand unstructured volumes.", "AI": {"tldr": "VEG\u662f\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u7684\u79d1\u5b66\u4f53\u79ef\u53ef\u89c6\u5316\u8868\u793a\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u975e\u7ed3\u6784\u5316\u4f53\u79ef\u6570\u636e\uff0c\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u5916\u89c2\u4e0e\u6570\u636e\u8868\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u548c\u538b\u7f29\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u5728\u53ef\u89c6\u5316\u4e2d\u7684\u5e94\u7528\u4e3b\u8981\u5ffd\u7565\u4e86\u975e\u7ed3\u6784\u5316\u4f53\u79ef\u6570\u636e\uff0c\u800cVEG\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "VEG\u901a\u8fc7\u4ec5\u7f16\u7801\u6807\u91cf\u503c\u89e3\u8026\u6570\u636e\u8868\u793a\u4e0e\u89c6\u89c9\u5916\u89c2\uff0c\u91c7\u7528\u4e0d\u900f\u660e\u5ea6\u5f15\u5bfc\u7684\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u8868\u793a\uff0c\u5e76\u9002\u5e94\u5c40\u90e8\u51e0\u4f55\u5f62\u72b6\u3002", "result": "VEG\u5728\u591a\u79cd\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\uff0c\u538b\u7f29\u7387\u9ad8\u8fbe3600\u500d\uff0c\u652f\u6301\u5feb\u901f\u4ea4\u4e92\u5f0f\u6e32\u67d3\u3002", "conclusion": "VEG\u4e3a\u975e\u7ed3\u6784\u5316\u4f53\u79ef\u6570\u636e\u7684\u79d1\u5b66\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u72ec\u7acb\u4e8e\u4f20\u9012\u51fd\u6570\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13227", "pdf": "https://arxiv.org/pdf/2504.13227", "abs": "https://arxiv.org/abs/2504.13227", "authors": ["Weijie Shi", "Jipeng Zhang", "Yaguang Wu", "Jingzhi Fang", "Ruiyuan Zhang", "Jiajie Xu", "Jia Zhu", "Hao Chen", "Yao Zhao", "Sirui Han", "Xiaofang Zhou"], "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIDS\u7684\u9886\u57df\u611f\u77e5\u6570\u636e\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u805a\u7c7b\u548cFIM\u5ea6\u91cf\u4f18\u5316\u9886\u57df\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u9886\u57df\u6570\u636e\u96c6\u4e2d\u9886\u57df\u91c7\u6837\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4fdd\u6301\u9886\u57df\u5185\u4e00\u81f4\u6027\u548c\u51c6\u786e\u8861\u91cf\u9886\u57df\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u805a\u7c7b\u7b97\u6cd5\u5206\u7ec4\u6570\u636e\uff0c\u4f7f\u7528FIM\u5ea6\u91cf\u91cf\u5316\u9886\u57df\u5f71\u54cd\uff0c\u7ed3\u5408\u635f\u5931\u5b66\u4e60\u8f68\u8ff9\u786e\u5b9a\u6700\u4f18\u91c7\u6837\u6bd4\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDIDS\u5e73\u5747\u6027\u80fd\u63d0\u53473.4%\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u6548\u7387\u3002", "conclusion": "DIDS\u901a\u8fc7\u4f18\u5316\u9886\u57df\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2504.13275", "pdf": "https://arxiv.org/pdf/2504.13275", "abs": "https://arxiv.org/abs/2504.13275", "authors": ["Shamanthak Hegde", "Pooyan Fazli", "Hasti Seifi"], "title": "ChartQA-X: Generating Explanations for Charts", "categories": ["cs.CV"], "comment": null, "summary": "The ability to interpret and explain complex information from visual data in\ncharts is crucial for data-driven decision-making. In this work, we address the\nchallenge of providing explanations alongside answering questions about chart\nimages. We present ChartQA-X, a comprehensive dataset comprising various chart\ntypes with 28,299 contextually relevant questions, answers, and detailed\nexplanations. These explanations are generated by prompting six different\nmodels and selecting the best responses based on metrics such as faithfulness,\ninformativeness, coherence, and perplexity. Our experiments show that models\nfine-tuned on our dataset for explanation generation achieve superior\nperformance across various metrics and demonstrate improved accuracy in\nquestion-answering tasks on new datasets. By integrating answers with\nexplanatory narratives, our approach enhances the ability of intelligent agents\nto convey complex information effectively, improve user understanding, and\nfoster trust in the generated responses.", "AI": {"tldr": "ChartQA-X\u662f\u4e00\u4e2a\u5305\u542b\u591a\u79cd\u56fe\u8868\u7c7b\u578b\u7684\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u95ee\u9898\u3001\u7b54\u6848\u548c\u8be6\u7ec6\u89e3\u91ca\uff0c\u901a\u8fc7\u5fae\u8c03\u6a21\u578b\u63d0\u5347\u89e3\u91ca\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u56fe\u8868\u56fe\u50cf\u4e2d\u56de\u7b54\u95ee\u9898\u540c\u65f6\u63d0\u4f9b\u89e3\u91ca\u7684\u6311\u6218\uff0c\u4ee5\u589e\u5f3a\u667a\u80fd\u4ee3\u7406\u4f20\u8fbe\u590d\u6742\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efaChartQA-X\u6570\u636e\u96c6\uff0c\u5305\u542b28,299\u4e2a\u95ee\u9898\u3001\u7b54\u6848\u548c\u89e3\u91ca\uff0c\u901a\u8fc7\u63d0\u793a\u516d\u79cd\u6a21\u578b\u5e76\u9009\u62e9\u6700\u4f73\u54cd\u5e94\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u89e3\u91ca\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u9ad8\u4e86\u65b0\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u5408\u7b54\u6848\u548c\u89e3\u91ca\u6027\u53d9\u8ff0\u7684\u65b9\u6cd5\u63d0\u5347\u4e86\u4fe1\u606f\u4f20\u8fbe\u6548\u679c\u3001\u7528\u6237\u7406\u89e3\u548c\u751f\u6210\u54cd\u5e94\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2504.13378", "pdf": "https://arxiv.org/pdf/2504.13378", "abs": "https://arxiv.org/abs/2504.13378", "authors": ["Mingxiao Tu", "Shuchang Ye", "Hoijoon Jung", "Jinman Kim"], "title": "SMPL-GPTexture: Dual-View 3D Human Texture Estimation using Text-to-Image Generation Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Generating high-quality, photorealistic textures for 3D human avatars remains\na fundamental yet challenging task in computer vision and multimedia field.\nHowever, real paired front and back images of human subjects are rarely\navailable with privacy, ethical and cost of acquisition, which restricts\nscalability of the data. Additionally, learning priors from image inputs using\ndeep generative models, such as GANs or diffusion models, to infer unseen\nregions such as the human back often leads to artifacts, structural\ninconsistencies, or loss of fine-grained detail. To address these issues, we\npresent SMPL-GPTexture (skinned multi-person linear model - general purpose\nTexture), a novel pipeline that takes natural language prompts as input and\nleverages a state-of-the-art text-to-image generation model to produce paired\nhigh-resolution front and back images of a human subject as the starting point\nfor texture estimation. Using the generated paired dual-view images, we first\nemploy a human mesh recovery model to obtain a robust 2D-to-3D SMPL alignment\nbetween image pixels and the 3D model's UV coordinates for each views. Second,\nwe use an inverted rasterization technique that explicitly projects the\nobserved colour from the input images into the UV space, thereby producing\naccurate, complete texture maps. Finally, we apply a diffusion-based inpainting\nmodule to fill in the missing regions, and the fusion mechanism then combines\nthese results into a unified full texture map. Extensive experiments shows that\nour SMPL-GPTexture can generate high resolution texture aligned with user's\nprompts.", "AI": {"tldr": "\u63d0\u51faSMPL-GPTexture\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf3D\u4eba\u4f53\u7eb9\u7406\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u751f\u6210\u6a21\u578b\u7f3a\u9677\u95ee\u9898\u3002", "motivation": "\u771f\u5b9e\u914d\u5bf9\u76843D\u4eba\u4f53\u7eb9\u7406\u6570\u636e\u7a00\u7f3a\u4e14\u83b7\u53d6\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u6613\u4ea7\u751f\u7f3a\u9677\u3002", "method": "\u7ed3\u5408\u6587\u672c\u751f\u6210\u56fe\u50cf\u3001\u4eba\u4f53\u7f51\u683c\u6062\u590d\u3001\u9006\u5411\u5149\u6805\u5316\u548c\u6269\u6563\u6a21\u578b\u586b\u8865\u7f3a\u5931\u533a\u57df\u3002", "result": "\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7eb9\u7406\uff0c\u4e0e\u7528\u6237\u63d0\u793a\u5bf9\u9f50\u3002", "conclusion": "SMPL-GPTexture\u6709\u6548\u89e3\u51b3\u4e86\u7eb9\u7406\u751f\u6210\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2504.13237", "pdf": "https://arxiv.org/pdf/2504.13237", "abs": "https://arxiv.org/abs/2504.13237", "authors": ["Yan Yang", "Yixia Li", "Hongru Wang", "Xuetao Wei", "Jianqiao Yu", "Yun Chen", "Guanhua Chen"], "title": "ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of task-specific large language models, delta\ncompression has emerged as a method to mitigate the resource challenges of\ndeploying numerous such models by effectively compressing the delta model\nparameters. Previous delta-sparsification methods either remove parameters\nrandomly or truncate singular vectors directly after singular value\ndecomposition (SVD). However, these methods either disregard parameter\nimportance entirely or evaluate it with too coarse a granularity. In this work,\nwe introduce ImPart, a novel importance-aware delta sparsification approach.\nLeveraging SVD, it dynamically adjusts sparsity ratios of different singular\nvectors based on their importance, effectively retaining crucial task-specific\nknowledge even at high sparsity ratios. Experiments show that ImPart achieves\nstate-of-the-art delta sparsification performance, demonstrating $2\\times$\nhigher compression ratio than baselines at the same performance level. When\nintegrated with existing methods, ImPart sets a new state-of-the-art on delta\nquantization and model merging.", "AI": {"tldr": "ImPart\u662f\u4e00\u79cd\u57fa\u4e8eSVD\u7684\u91cd\u8981\u6027\u611f\u77e5\u589e\u91cf\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7a00\u758f\u7387\uff0c\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u4fdd\u7559\u4efb\u52a1\u5173\u952e\u77e5\u8bc6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u589e\u91cf\u7a00\u758f\u5316\u65b9\u6cd5\u5ffd\u89c6\u53c2\u6570\u91cd\u8981\u6027\u6216\u8bc4\u4f30\u7c92\u5ea6\u7c97\u7cd9\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528SVD\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u5947\u5f02\u5411\u91cf\u7684\u7a00\u758f\u7387\uff0c\u57fa\u4e8e\u5176\u91cd\u8981\u6027\u4fdd\u7559\u5173\u952e\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aImPart\u5728\u76f8\u540c\u6027\u80fd\u4e0b\u538b\u7f29\u6bd4\u63d0\u9ad82\u500d\uff0c\u5e76\u5728\u589e\u91cf\u91cf\u5316\u548c\u6a21\u578b\u5408\u5e76\u4e2d\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "ImPart\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7a00\u758f\u5316\u663e\u8457\u63d0\u5347\u589e\u91cf\u538b\u7f29\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13282", "pdf": "https://arxiv.org/pdf/2504.13282", "abs": "https://arxiv.org/abs/2504.13282", "authors": ["Jiang-Xin Shi", "Tong Wei", "Yu-Feng Li"], "title": "LIFT+: Lightweight Fine-Tuning for Long-Tail Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The fine-tuning paradigm has emerged as a prominent approach for addressing\nlong-tail learning tasks in the era of foundation models. However, the impact\nof fine-tuning strategies on long-tail learning performance remains unexplored.\nIn this work, we disclose that existing paradigms exhibit a profound misuse of\nfine-tuning methods, leaving significant room for improvement in both\nefficiency and accuracy. Specifically, we reveal that heavy fine-tuning\n(fine-tuning a large proportion of model parameters) can lead to non-negligible\nperformance deterioration on tail classes, whereas lightweight fine-tuning\ndemonstrates superior effectiveness. Through comprehensive theoretical and\nempirical validation, we identify this phenomenon as stemming from inconsistent\nclass conditional distributions induced by heavy fine-tuning. Building on this\ninsight, we propose LIFT+, an innovative lightweight fine-tuning framework to\noptimize consistent class conditions. Furthermore, LIFT+ incorporates\nsemantic-aware initialization, minimalist data augmentation, and test-time\nensembling to enhance adaptation and generalization of foundation models. Our\nframework provides an efficient and accurate pipeline that facilitates fast\nconvergence and model compactness. Extensive experiments demonstrate that LIFT+\nsignificantly reduces both training epochs (from $\\sim$100 to $\\leq$15) and\nlearned parameters (less than 1%), while surpassing state-of-the-art approaches\nby a considerable margin. The source code is available at\nhttps://github.com/shijxcs/LIFT-plus.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5fae\u8c03\u7b56\u7565\u5bf9\u957f\u5c3e\u5b66\u4e60\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5bf9\u5fae\u8c03\u65b9\u6cd5\u7684\u8bef\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5fae\u8c03\u6846\u67b6LIFT+\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76\u5fae\u8c03\u7b56\u7565\u5bf9\u957f\u5c3e\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u63d0\u51faLIFT+\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u521d\u59cb\u5316\u3001\u6781\u7b80\u6570\u636e\u589e\u5f3a\u548c\u6d4b\u8bd5\u65f6\u96c6\u6210\uff0c\u4f18\u5316\u7c7b\u522b\u6761\u4ef6\u4e00\u81f4\u6027\u3002", "result": "LIFT+\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u5468\u671f\u548c\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "LIFT+\u4e3a\u957f\u5c3e\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2504.13386", "pdf": "https://arxiv.org/pdf/2504.13386", "abs": "https://arxiv.org/abs/2504.13386", "authors": ["Radek Dan\u011b\u010dek", "Carolin Schmitt", "Senya Polikovsky", "Michael J. Black"], "title": "Supervising 3D Talking Head Avatars with Analysis-by-Audio-Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "In order to be widely applicable, speech-driven 3D head avatars must\narticulate their lips in accordance with speech, while also conveying the\nappropriate emotions with dynamically changing facial expressions. The key\nproblem is that deterministic models produce high-quality lip-sync but without\nrich expressions, whereas stochastic models generate diverse expressions but\nwith lower lip-sync quality. To get the best of both, we seek a stochastic\nmodel with accurate lip-sync. To that end, we develop a new approach based on\nthe following observation: if a method generates realistic 3D lip motions, it\nshould be possible to infer the spoken audio from the lip motion. The inferred\nspeech should match the original input audio, and erroneous predictions create\na novel supervision signal for training 3D talking head avatars with accurate\nlip-sync. To demonstrate this effect, we propose THUNDER (Talking Heads Under\nNeural Differentiable Elocution Reconstruction), a 3D talking head avatar\nframework that introduces a novel supervision mechanism via differentiable\nsound production. First, we train a novel mesh-to-speech model that regresses\naudio from facial animation. Then, we incorporate this model into a\ndiffusion-based talking avatar framework. During training, the mesh-to-speech\nmodel takes the generated animation and produces a sound that is compared to\nthe input speech, creating a differentiable analysis-by-audio-synthesis\nsupervision loop. Our extensive qualitative and quantitative experiments\ndemonstrate that THUNDER significantly improves the quality of the lip-sync of\ntalking head avatars while still allowing for generation of diverse,\nhigh-quality, expressive facial animations.", "AI": {"tldr": "THUNDER\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u8bed\u97f3\u91cd\u6784\u76843D\u5934\u90e8\u52a8\u753b\u6846\u67b6\uff0c\u7ed3\u5408\u786e\u5b9a\u6027\u6a21\u578b\u7684\u7cbe\u51c6\u5507\u540c\u6b65\u548c\u968f\u673a\u6a21\u578b\u7684\u4e30\u5bcc\u8868\u60c5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5507\u540c\u6b65\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u786e\u5b9a\u6027\u6a21\u578b\u5507\u540c\u6b65\u8d28\u91cf\u9ad8\u4f46\u8868\u60c5\u5355\u4e00\uff0c\u968f\u673a\u6a21\u578b\u8868\u60c5\u4e30\u5bcc\u4f46\u5507\u540c\u6b65\u8d28\u91cf\u4f4e\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u4ece\u9762\u90e8\u52a8\u753b\u56de\u5f52\u97f3\u9891\u7684\u7f51\u683c\u5230\u8bed\u97f3\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u57fa\u4e8e\u6269\u6563\u7684\u52a8\u753b\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u53ef\u5fae\u5206\u8bed\u97f3\u91cd\u6784\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "THUNDER\u663e\u8457\u63d0\u5347\u4e86\u5507\u540c\u6b65\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e30\u5bcc\u591a\u6837\u7684\u8868\u60c5\u52a8\u753b\u3002", "conclusion": "THUNDER\u901a\u8fc7\u53ef\u5fae\u5206\u8bed\u97f3\u91cd\u6784\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5507\u540c\u6b65\u548c\u8868\u60c5\u52a8\u753b\uff0c\u4e3a3D\u5934\u90e8\u52a8\u753b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13261", "pdf": "https://arxiv.org/pdf/2504.13261", "abs": "https://arxiv.org/abs/2504.13261", "authors": ["Dong Wang"], "title": "CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": "12 pages, 1 figure, 3 tables", "summary": "Purpose: The rapid emergence of large language models (LLMs) such as ChatGPT\nhas significantly impacted foreign language education, yet their pedagogical\ngrammar competence remains under-assessed. This paper introduces CPG-EVAL, the\nfirst dedicated benchmark specifically designed to evaluate LLMs' knowledge of\npedagogical grammar within the context of foreign language instruction.\nMethodology: The benchmark comprises five tasks designed to assess grammar\nrecognition, fine-grained grammatical distinction, categorical discrimination,\nand resistance to linguistic interference. Findings: Smaller-scale models can\nsucceed in single language instance tasks, but struggle with multiple instance\ntasks and interference from confusing instances. Larger-scale models show\nbetter resistance to interference but still have significant room for accuracy\nimprovement. The evaluation indicates the need for better instructional\nalignment and more rigorous benchmarks, to effectively guide the deployment of\nLLMs in educational contexts. Value: This study offers the first specialized,\ntheory-driven, multi-tiered benchmark framework for systematically evaluating\nLLMs' pedagogical grammar competence in Chinese language teaching contexts.\nCPG-EVAL not only provides empirical insights for educators, policymakers, and\nmodel developers to better gauge AI's current abilities in educational\nsettings, but also lays the groundwork for future research on improving model\nalignment, enhancing educational suitability, and ensuring informed\ndecision-making concerning LLM integration in foreign language instruction.", "AI": {"tldr": "CPG-EVAL\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5916\u8bed\u6559\u5b66\u4e2d\u8bed\u6cd5\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8bed\u6cd5\u8bc6\u522b\u3001\u5e72\u6270\u62b5\u6297\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLMs\uff08\u5982ChatGPT\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u5916\u8bed\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5bf9\u5176\u8bed\u6cd5\u6559\u5b66\u80fd\u529b\u7684\u8bc4\u4f30\u4ecd\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "CPG-EVAL\u5305\u542b\u4e94\u9879\u4efb\u52a1\uff0c\u8bc4\u4f30\u8bed\u6cd5\u8bc6\u522b\u3001\u7ec6\u7c92\u5ea6\u533a\u5206\u3001\u7c7b\u522b\u8fa8\u522b\u53ca\u6297\u5e72\u6270\u80fd\u529b\u3002", "result": "\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u5355\u4e00\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u4efb\u52a1\u548c\u5e72\u6270\u4e0b\u8868\u73b0\u4e0d\u4f73\uff1b\u5927\u89c4\u6a21\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u7406\u8bba\u9a71\u52a8\u7684\u591a\u5c42\u7ea7\u57fa\u51c6\u6846\u67b6\uff0c\u4e3aLLMs\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.13297", "pdf": "https://arxiv.org/pdf/2504.13297", "abs": "https://arxiv.org/abs/2504.13297", "authors": ["Andreas Lau Hansen", "Lukas Wanzeck", "Dim P. Papadopoulos"], "title": "Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes", "categories": ["cs.CV", "I.4"], "comment": "14 pages, 5 figures. Accepted for 23rd Scandinavian Conference, SCIA\n  2025, Reykjavik, Iceland", "summary": "Monocular 3D object detection is an essential task in computer vision, and it\nhas several applications in robotics and virtual reality. However, 3D object\ndetectors are typically trained in a fully supervised way, relying extensively\non 3D labeled data, which is labor-intensive and costly to annotate. This work\nfocuses on weakly-supervised 3D detection to reduce data needs using a\nmonocular method that leverages a singlecamera system over expensive LiDAR\nsensors or multi-camera setups. We propose a general model Weak Cube R-CNN,\nwhich can predict objects in 3D at inference time, requiring only 2D box\nannotations for training by exploiting the relationship between 2D projections\nof 3D cubes. Our proposed method utilizes pre-trained frozen foundation 2D\nmodels to estimate depth and orientation information on a training set. We use\nthese estimated values as pseudo-ground truths during training. We design loss\nfunctions that avoid 3D labels by incorporating information from the external\nmodels into the loss. In this way, we aim to implicitly transfer knowledge from\nthese large foundation 2D models without having access to 3D bounding box\nannotations. Experimental results on the SUN RGB-D dataset show increased\nperformance in accuracy compared to an annotation time equalized Cube R-CNN\nbaseline. While not precise for centimetre-level measurements, this method\nprovides a strong foundation for further research.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u7684\u5355\u76ee3D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5Weak Cube R-CNN\uff0c\u4ec5\u97002D\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u5229\u75283D\u7acb\u65b9\u4f53\u76842D\u6295\u5f71\u5173\u7cfb\uff0c\u51cf\u5c11\u5bf9\u6602\u8d353D\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "3D\u7269\u4f53\u68c0\u6d4b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f31\u76d1\u7763\u65b9\u6cd5\u51cf\u5c11\u5bf93D\u6807\u6ce8\u7684\u4f9d\u8d56\uff0c\u4ec5\u4f7f\u75282D\u6807\u6ce8\u6570\u636e\u5b9e\u73b03D\u68c0\u6d4b\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u76842D\u57fa\u7840\u6a21\u578b\u4f30\u8ba1\u6df1\u5ea6\u548c\u65b9\u5411\u4fe1\u606f\u4f5c\u4e3a\u4f2a\u771f\u503c\uff0c\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\u907f\u514d\u76f4\u63a5\u4f7f\u75283D\u6807\u6ce8\uff0c\u5c06\u77e5\u8bc6\u4ece2D\u6a21\u578b\u9690\u5f0f\u8fc1\u79fb\u52303D\u68c0\u6d4b\u4efb\u52a1\u3002", "result": "\u5728SUN RGB-D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfCube R-CNN\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u51cf\u5c113D\u6807\u6ce8\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u867d\u5728\u5398\u7c73\u7ea7\u7cbe\u5ea6\u4e0a\u6709\u9650\uff0c\u4f46\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.13436", "pdf": "https://arxiv.org/pdf/2504.13436", "abs": "https://arxiv.org/abs/2504.13436", "authors": ["YoungWoo Kim", "Jaehong Lee", "Duksu Kim"], "title": "RT-HDIST: Ray-Tracing Core-based Hausdorff Distance Computation", "categories": ["cs.GR", "cs.CG"], "comment": "8 pages, 7 figures", "summary": "The Hausdorff distance is a fundamental metric with widespread applications\nacross various fields. However, its computation remains computationally\nexpensive, especially for large-scale datasets. In this work, we present\nRT-HDIST, the first Hausdorff distance algorithm accelerated by ray-tracing\ncores (RT-cores). By reformulating the Hausdorff distance problem as a series\nof nearest-neighbor searches and introducing a novel quantized index space,\nRT-HDIST achieves significant reductions in computational overhead while\nmaintaining exact results. Extensive benchmarks demonstrate up to a\ntwo-order-of-magnitude speedup over prior state-of-the-art methods,\nunderscoring RT-HDIST's potential for real-time and large-scale applications.", "AI": {"tldr": "RT-HDIST\u662f\u4e00\u79cd\u5229\u7528\u5149\u7ebf\u8ffd\u8e2a\u6838\u5fc3\u52a0\u901f\u7684Hausdorff\u8ddd\u79bb\u7b97\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5b9a\u4e49\u95ee\u9898\u4e3a\u6700\u8fd1\u90bb\u641c\u7d22\u5e76\u5f15\u5165\u91cf\u5316\u7d22\u5f15\u7a7a\u95f4\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "Hausdorff\u8ddd\u79bb\u8ba1\u7b97\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06Hausdorff\u8ddd\u79bb\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u8fd1\u90bb\u641c\u7d22\uff0c\u5e76\u5f15\u5165\u91cf\u5316\u7d22\u5f15\u7a7a\u95f4\uff0c\u5229\u7528\u5149\u7ebf\u8ffd\u8e2a\u6838\u5fc3\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRT-HDIST\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u7ed3\u679c\u3002", "conclusion": "RT-HDIST\u4e3a\u5b9e\u65f6\u548c\u5927\u89c4\u6a21\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684Hausdorff\u8ddd\u79bb\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13284", "pdf": "https://arxiv.org/pdf/2504.13284", "abs": "https://arxiv.org/abs/2504.13284", "authors": ["Derguene Mbaye", "Madoune Robert Seye", "Moussa Diallo", "Mamadou Lamine Ndiaye", "Djiby Sow", "Dimitri Samuel Adjanohoun", "Tatiana Mbengue", "Cheikh Samba Wade", "De Roulet Pablo", "Jean-Claude Baraka Munyaka", "Jerome Chenal"], "title": "Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal", "categories": ["cs.CL", "cs.SI"], "comment": "19 pages, 14 figures, 10th International Congress on Information and\n  Communication Technology (ICICT 2025)", "summary": "Internet penetration rates in Africa are rising steadily, and mobile Internet\nis getting an even bigger boost with the availability of smartphones. Young\npeople are increasingly using the Internet, especially social networks, and\nSenegal is no exception to this revolution. Social networks have become the\nmain means of expression for young people. Despite this evolution in Internet\naccess, there are few operators on the market, which limits the alternatives\navailable in terms of value for money. In this paper, we will look at how young\npeople feel about the price of mobile Internet in Senegal, in relation to the\nperceived quality of the service, through their comments on social networks. We\nscanned a set of Twitter and Facebook comments related to the subject and\napplied a sentiment analysis model to gather their general feelings.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u585e\u5185\u52a0\u5c14\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u4ef7\u683c\u4e0e\u670d\u52a1\u8d28\u91cf\u7684\u611f\u53d7\uff0c\u901a\u8fc7\u5206\u6790\u793e\u4ea4\u5a92\u4f53\u8bc4\u8bba\u5e76\u5e94\u7528\u60c5\u611f\u5206\u6790\u6a21\u578b\u3002", "motivation": "\u968f\u7740\u975e\u6d32\u4e92\u8054\u7f51\u666e\u53ca\u7387\u4e0a\u5347\uff0c\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u5e02\u573a\u8fd0\u8425\u5546\u6709\u9650\uff0c\u5bfc\u81f4\u6027\u4ef7\u6bd4\u9009\u62e9\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u4e86\u89e3\u5e74\u8f7b\u4eba\u5bf9\u4ef7\u683c\u4e0e\u670d\u52a1\u7684\u770b\u6cd5\u3002", "method": "\u901a\u8fc7\u626b\u63cfTwitter\u548cFacebook\u4e0a\u4e0e\u4e3b\u9898\u76f8\u5173\u7684\u8bc4\u8bba\uff0c\u5e76\u5e94\u7528\u60c5\u611f\u5206\u6790\u6a21\u578b\u6765\u5206\u6790\u5e74\u8f7b\u4eba\u7684\u60c5\u611f\u503e\u5411\u3002", "result": "\u60c5\u611f\u5206\u6790\u7ed3\u679c\u63ed\u793a\u4e86\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u4ef7\u683c\u4e0e\u670d\u52a1\u8d28\u91cf\u7684\u4e00\u822c\u611f\u53d7\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u585e\u5185\u52a0\u5c14\u5e74\u8f7b\u4eba\u5bf9\u79fb\u52a8\u4e92\u8054\u7f51\u7684\u6001\u5ea6\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\uff0c\u53cd\u6620\u4e86\u5e02\u573a\u4f9b\u9700\u95ee\u9898\u3002"}}
{"id": "2504.13310", "pdf": "https://arxiv.org/pdf/2504.13310", "abs": "https://arxiv.org/abs/2504.13310", "authors": ["Yasin Almalioglu", "Andrzej Kucik", "Geoffrey French", "Dafni Antotsiou", "Alexander Adam", "Cedric Archambeau"], "title": "SAR Object Detection with Self-Supervised Pretraining and Curriculum-Aware Sampling", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICLR 2025 ML4RS https://ml-for-rs.github.io/iclr2025/", "summary": "Object detection in satellite-borne Synthetic Aperture Radar (SAR) imagery\nholds immense potential in tasks such as urban monitoring and disaster\nresponse. However, the inherent complexities of SAR data and the scarcity of\nannotations present significant challenges in the advancement of object\ndetection in this domain. Notably, the detection of small objects in\nsatellite-borne SAR images poses a particularly intricate problem, because of\nthe technology's relatively low spatial resolution and inherent noise.\nFurthermore, the lack of large labelled SAR datasets hinders the development of\nsupervised deep learning-based object detection models. In this paper, we\nintroduce TRANSAR, a novel self-supervised end-to-end vision transformer-based\nSAR object detection model that incorporates masked image pre-training on an\nunlabeled SAR image dataset that spans more than $25,700$ km\\textsuperscript{2}\nground area. Unlike traditional object detection formulation, our approach\ncapitalises on auxiliary binary semantic segmentation, designed to segregate\nobjects of interest during the post-tuning, especially the smaller ones, from\nthe background. In addition, to address the innate class imbalance due to the\ndisproportion of the object to the image size, we introduce an adaptive\nsampling scheduler that dynamically adjusts the target class distribution\nduring training based on curriculum learning and model feedback. This approach\nallows us to outperform conventional supervised architecture such as DeepLabv3\nor UNet, and state-of-the-art self-supervised learning-based arhitectures such\nas DPT, SegFormer or UperNet, as shown by extensive evaluations on benchmark\nSAR datasets.", "AI": {"tldr": "TRANSAR\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u536b\u661fSAR\u56fe\u50cf\u4e2d\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u63a9\u7801\u56fe\u50cf\u9884\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u91c7\u6837\u8c03\u5ea6\u5668\u63d0\u5347\u6027\u80fd\u3002", "motivation": "SAR\u56fe\u50cf\u7684\u76ee\u6807\u68c0\u6d4b\u5728\u707e\u5bb3\u54cd\u5e94\u7b49\u9886\u57df\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u6570\u636e\u590d\u6742\u6027\u548c\u6807\u6ce8\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002", "method": "\u63d0\u51faTRANSAR\u6a21\u578b\uff0c\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u9884\u8bad\u7ec3\u548c\u8f85\u52a9\u8bed\u4e49\u5206\u5272\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91c7\u6837\u8c03\u5ea6\u5668\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u57fa\u51c6SAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u6a21\u578b\uff08\u5982DeepLabv3\uff09\u548c\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5982DPT\uff09\u3002", "conclusion": "TRANSAR\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u52a8\u6001\u8c03\u6574\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SAR\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.13448", "pdf": "https://arxiv.org/pdf/2504.13448", "abs": "https://arxiv.org/abs/2504.13448", "authors": ["Daniela Ushizima", "Guilherme Melo dos Santos", "Zineb Sordo", "Ronald Pandolfi", "Jeffrey Donatelli"], "title": "Ascribe New Dimensions to Scientific Data Visualization with VR", "categories": ["cs.GR", "cs.AI", "cs.CE"], "comment": null, "summary": "For over half a century, the computer mouse has been the primary tool for\ninteracting with digital data, yet it remains a limiting factor in exploring\ncomplex, multi-scale scientific images. Traditional 2D visualization methods\nhinder intuitive analysis of inherently 3D structures. Virtual Reality (VR)\noffers a transformative alternative, providing immersive, interactive\nenvironments that enhance data comprehension. This article introduces\nASCRIBE-VR, a VR platform of Autonomous Solutions for Computational Research\nwith Immersive Browsing \\& Exploration, which integrates AI-driven algorithms\nwith scientific images. ASCRIBE-VR enables multimodal analysis, structural\nassessments, and immersive visualization, supporting scientific visualization\nof advanced datasets such as X-ray CT, Magnetic Resonance, and synthetic 3D\nimaging. Our VR tools, compatible with Meta Quest, can consume the output of\nour AI-based segmentation and iterative feedback processes to enable seamless\nexploration of large-scale 3D images. By merging AI-generated results with VR\nvisualization, ASCRIBE-VR enhances scientific discovery, bridging the gap\nbetween computational analysis and human intuition in materials research,\nconnecting human-in-the-loop with digital twins.", "AI": {"tldr": "ASCRIBE-VR\u662f\u4e00\u4e2a\u7ed3\u5408AI\u7b97\u6cd5\u4e0eVR\u6280\u672f\u7684\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u6c89\u6d78\u5f0f\u73af\u5883\u63d0\u5347\u590d\u6742\u79d1\u5b66\u56fe\u50cf\u7684\u5206\u6790\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf2D\u53ef\u89c6\u5316\u65b9\u6cd5\u9650\u5236\u4e863D\u7ed3\u6784\u7684\u76f4\u89c2\u5206\u6790\uff0cVR\u6280\u672f\u63d0\u4f9b\u4e86\u66f4\u6c89\u6d78\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "ASCRIBE-VR\u6574\u5408AI\u9a71\u52a8\u7684\u7b97\u6cd5\u4e0e\u79d1\u5b66\u56fe\u50cf\uff0c\u652f\u6301\u591a\u6a21\u6001\u5206\u6790\u548c\u6c89\u6d78\u5f0f\u53ef\u89c6\u5316\u3002", "result": "\u5e73\u53f0\u652f\u6301X\u5c04\u7ebfCT\u3001\u78c1\u5171\u632f\u7b49\u9ad8\u7ea7\u6570\u636e\u96c6\u7684\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7AI\u5206\u5272\u548c\u53cd\u9988\u4f18\u5316\u63a2\u7d22\u8fc7\u7a0b\u3002", "conclusion": "ASCRIBE-VR\u901a\u8fc7\u7ed3\u5408AI\u4e0eVR\uff0c\u5f25\u5408\u4e86\u8ba1\u7b97\u5206\u6790\u4e0e\u4eba\u7c7b\u76f4\u89c9\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2504.13367", "pdf": "https://arxiv.org/pdf/2504.13367", "abs": "https://arxiv.org/abs/2504.13367", "authors": ["Xiao Pu", "Michael Saxon", "Wenyue Hua", "William Yang Wang"], "title": "THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance on difficult tasks\nthat traditional language models struggle at. However, many are plagued with\nthe problem of overthinking--generating large amounts of unnecessary tokens\nwhich don't improve accuracy on a question. We introduce approximate measures\nof problem-level difficulty and demonstrate that a clear relationship between\nproblem difficulty and optimal token spend exists, and evaluate how well\ncalibrated a variety of reasoning models are in terms of efficiently allocating\nthe optimal token count. We find that in general, reasoning models are poorly\ncalibrated, particularly on easy problems. To evaluate calibration on easy\nquestions we introduce DUMB500, a dataset of extremely easy math, reasoning,\ncode, and task problems, and jointly evaluate reasoning model on these simple\nexamples and extremely difficult examples from existing frontier benchmarks on\nthe same task domain. Finally, we introduce THOUGHTTERMINATOR, a training-free\nblack box decoding technique that significantly improves reasoning model\ncalibration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u95ee\u9898\u96be\u5ea6\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u5206\u914d\u6700\u4f18token\u6570\u91cf\u4e0a\u6821\u51c6\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u3002\u4f5c\u8005\u5f15\u5165DUMB500\u6570\u636e\u96c6\u548cTHOUGHTTERMINATOR\u6280\u672f\u6765\u6539\u5584\u6821\u51c6\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u751f\u6210\u4e0d\u5fc5\u8981token\u5374\u4e0d\u63d0\u5347\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u95ee\u9898\u96be\u5ea6\u8fd1\u4f3c\u5ea6\u91cf\uff0c\u8bc4\u4f30\u63a8\u7406\u6a21\u578b\u5728\u5206\u914d\u6700\u4f18token\u6570\u91cf\u4e0a\u7684\u6821\u51c6\u60c5\u51b5\uff0c\u5e76\u63d0\u51faDUMB500\u6570\u636e\u96c6\u548cTHOUGHTTERMINATOR\u6280\u672f\u3002", "result": "\u63a8\u7406\u6a21\u578b\u666e\u904d\u6821\u51c6\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u3002THOUGHTTERMINATOR\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u95ee\u9898\u96be\u5ea6\u5ea6\u91cf\u548c\u65b0\u6280\u672f\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u5584\u63a8\u7406\u6a21\u578b\u7684\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2504.13365", "pdf": "https://arxiv.org/pdf/2504.13365", "abs": "https://arxiv.org/abs/2504.13365", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In modern smart agriculture, object detection plays a crucial role by\nenabling automation, precision farming, and monitoring of resources. From\nidentifying crop health and pest infestations to optimizing harvesting\nprocesses, accurate object detection enhances both productivity and\nsustainability. However, training object detection models often requires\nlarge-scale data collection and raises privacy concerns, particularly when\nsensitive agricultural data is distributed across farms. To address these\nchallenges, we propose VLLFL, a vision-language model-based lightweight\nfederated learning framework (VLLFL). It harnesses the generalization and\ncontext-aware detection capabilities of the vision-language model (VLM) and\nleverages the privacy-preserving nature of federated learning. By training a\ncompact prompt generator to boost the performance of the VLM deployed across\ndifferent farms, VLLFL preserves privacy while reducing communication overhead.\nExperimental results demonstrate that VLLFL achieves 14.53% improvement in the\nperformance of VLM while reducing 99.3% communication overhead. Spanning tasks\nfrom identifying a wide variety of fruits to detecting harmful animals in\nagriculture, the proposed framework offers an efficient, scalable, and\nprivacy-preserving solution specifically tailored to agricultural applications.", "AI": {"tldr": "VLLFL\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u667a\u80fd\u519c\u4e1a\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u6570\u636e\u9690\u79c1\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u667a\u80fd\u519c\u4e1a\u4e2d\uff0c\u76ee\u6807\u68c0\u6d4b\u5bf9\u81ea\u52a8\u5316\u548c\u7cbe\u51c6\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u548c\u9690\u79c1\u95ee\u9898\u6210\u4e3a\u6311\u6218\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u4fdd\u62a4\u7279\u6027\uff0c\u901a\u8fc7\u8bad\u7ec3\u7d27\u51d1\u7684\u63d0\u793a\u751f\u6210\u5668\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cVLLFL\u5728\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd14.53%\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e8699.3%\u7684\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "VLLFL\u4e3a\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13425", "pdf": "https://arxiv.org/pdf/2504.13425", "abs": "https://arxiv.org/abs/2504.13425", "authors": ["Grace Byun", "Shinsun Lee", "Nayoung Choi", "Jinho Choi"], "title": "Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Existing Retrieval-Augmented Generation (RAG) systems face challenges in\nenterprise settings due to limited retrieval scope and data security risks.\nWhen relevant internal documents are unavailable, the system struggles to\ngenerate accurate and complete responses. Additionally, using closed-source\nLarge Language Models (LLMs) raises concerns about exposing proprietary\ninformation. To address these issues, we propose the Secure Multifaceted-RAG\n(SecMulti-RAG) framework, which retrieves not only from internal documents but\nalso from two supplementary sources: pre-generated expert knowledge for\nanticipated queries and on-demand external LLM-generated knowledge. To mitigate\nsecurity risks, we adopt a local open-source generator and selectively utilize\nexternal LLMs only when prompts are deemed safe by a filtering mechanism. This\napproach enhances completeness, prevents data leakage, and reduces costs. In\nour evaluation on a report generation task in the automotive industry,\nSecMulti-RAG significantly outperforms traditional RAG - achieving 79.3 to 91.9\npercent win rates across correctness, richness, and helpfulness in LLM-based\nevaluation, and 56.3 to 70.4 percent in human evaluation. This highlights\nSecMulti-RAG as a practical and secure solution for enterprise RAG.", "AI": {"tldr": "SecMulti-RAG\u6846\u67b6\u901a\u8fc7\u591a\u6e90\u68c0\u7d22\u548c\u672c\u5730\u5f00\u6e90\u751f\u6210\u5668\u89e3\u51b3\u4f01\u4e1aRAG\u7cfb\u7edf\u7684\u68c0\u7d22\u8303\u56f4\u6709\u9650\u548c\u6570\u636e\u5b89\u5168\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u9762\u4e34\u68c0\u7d22\u8303\u56f4\u6709\u9650\u548c\u6570\u636e\u6cc4\u9732\u98ce\u9669\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0d\u51c6\u786e\u6216\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51faSecMulti-RAG\u6846\u67b6\uff0c\u7ed3\u5408\u5185\u90e8\u6587\u6863\u3001\u9884\u751f\u6210\u4e13\u5bb6\u77e5\u8bc6\u548c\u5916\u90e8LLM\u751f\u6210\u77e5\u8bc6\uff0c\u91c7\u7528\u672c\u5730\u5f00\u6e90\u751f\u6210\u5668\u548c\u5b89\u5168\u8fc7\u6ee4\u673a\u5236\u3002", "result": "\u5728\u6c7d\u8f66\u884c\u4e1a\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\uff0cSecMulti-RAG\u5728\u6b63\u786e\u6027\u3001\u4e30\u5bcc\u6027\u548c\u5b9e\u7528\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfRAG\uff0cLLM\u8bc4\u4f30\u80dc\u738779.3%\u81f391.9%\uff0c\u4eba\u5de5\u8bc4\u4f30\u80dc\u738756.3%\u81f370.4%\u3002", "conclusion": "SecMulti-RAG\u662f\u4f01\u4e1aRAG\u7cfb\u7edf\u5b9e\u7528\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13392", "pdf": "https://arxiv.org/pdf/2504.13392", "abs": "https://arxiv.org/abs/2504.13392", "authors": ["Evans Xu Han", "Alice Qian Zhang", "Hong Shen", "Haiyi Zhu", "Paul Pu Liang", "Jane Hsieh"], "title": "POET: Supporting Prompting Creativity and Personalization with Automated Expansion of Text-to-Image Generation", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "State-of-the-art visual generative AI tools hold immense potential to assist\nusers in the early ideation stages of creative tasks -- offering the ability to\ngenerate (rather than search for) novel and unprecedented (instead of existing)\nimages of considerable quality that also adhere to boundless combinations of\nuser specifications. However, many large-scale text-to-image systems are\ndesigned for broad applicability, yielding conventional output that may limit\ncreative exploration. They also employ interaction methods that may be\ndifficult for beginners. Given that creative end users often operate in\ndiverse, context-specific ways that are often unpredictable, more variation and\npersonalization are necessary. We introduce POET, a real-time interactive tool\nthat (1) automatically discovers dimensions of homogeneity in text-to-image\ngenerative models, (2) expands these dimensions to diversify the output space\nof generated images, and (3) learns from user feedback to personalize\nexpansions. An evaluation with 28 users spanning four creative task domains\ndemonstrated POET's ability to generate results with higher perceived diversity\nand help users reach satisfaction in fewer prompts during creative tasks,\nthereby prompting them to deliberate and reflect more on a wider range of\npossible produced results during the co-creative process. Focusing on visual\ncreativity, POET offers a first glimpse of how interaction techniques of future\ntext-to-image generation tools may support and align with more pluralistic\nvalues and the needs of end users during the ideation stages of their work.", "AI": {"tldr": "POET\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u4e92\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u540c\u8d28\u7ef4\u5ea6\u3001\u6269\u5c55\u8fd9\u4e9b\u7ef4\u5ea6\u4ee5\u591a\u6837\u5316\u8f93\u51fa\uff0c\u5e76\u5b66\u4e60\u7528\u6237\u53cd\u9988\u4ee5\u4e2a\u6027\u5316\u6269\u5c55\uff0c\u63d0\u5347\u521b\u610f\u4efb\u52a1\u7684\u591a\u6837\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u8f93\u51fa\u8f83\u4e3a\u5e38\u89c4\uff0c\u9650\u5236\u4e86\u521b\u610f\u63a2\u7d22\uff0c\u4e14\u4ea4\u4e92\u65b9\u5f0f\u5bf9\u521d\u5b66\u8005\u4e0d\u53cb\u597d\uff0c\u9700\u8981\u66f4\u591a\u53d8\u4f53\u548c\u4e2a\u6027\u5316\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u7528\u6237\u9700\u6c42\u3002", "method": "POET\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u540c\u8d28\u7ef4\u5ea6\u3001\u6269\u5c55\u8f93\u51fa\u7a7a\u95f4\u548c\u4e2a\u6027\u5316\u5b66\u4e60\u7528\u6237\u53cd\u9988\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u548c\u4e2a\u6027\u5316\u7684\u56fe\u50cf\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cPOET\u80fd\u751f\u6210\u66f4\u9ad8\u591a\u6837\u6027\u7684\u7ed3\u679c\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u66f4\u5c11\u7684\u63d0\u793a\u4e0b\u8fbe\u5230\u6ee1\u610f\uff0c\u5e76\u4fc3\u4f7f\u4ed6\u4eec\u5728\u5171\u521b\u8fc7\u7a0b\u4e2d\u601d\u8003\u66f4\u5e7f\u6cdb\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "POET\u5c55\u793a\u4e86\u672a\u6765\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u4ea4\u4e92\u6280\u672f\u652f\u6301\u591a\u5143\u4ef7\u503c\u548c\u7528\u6237\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u521b\u610f\u6784\u601d\u9636\u6bb5\u3002"}}
{"id": "2504.13439", "pdf": "https://arxiv.org/pdf/2504.13439", "abs": "https://arxiv.org/abs/2504.13439", "authors": ["Grace Byun", "Jinho Choi"], "title": "D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating generative models with open-ended generation is challenging due to\ninconsistencies in response formats. Multiple-choice (MC) evaluation mitigates\nthis issue, but generating high-quality distractors is time-consuming and\nlabor-intensive. We introduce D-GEN, the first open-source distractor generator\nmodel that transforms open-ended data into an MC format. To evaluate distractor\nquality, we propose two novel methods: (1) ranking alignment, ensuring\ngenerated distractors retain the discriminatory power of ground-truth\ndistractors, and (2) entropy analysis, comparing model confidence\ndistributions. Our results show that D-GEN preserves ranking consistency\n(Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy\ndistribution of ground-truth distractors. Human evaluation further confirms the\nfluency, coherence, distractiveness, and incorrectness. Our work advances\nrobust and efficient distractor generation with automated evaluation, setting a\nnew standard for MC evaluation.", "AI": {"tldr": "D-GEN\u662f\u4e00\u4e2a\u5f00\u6e90\u5e72\u6270\u9879\u751f\u6210\u6a21\u578b\uff0c\u5c06\u5f00\u653e\u5f0f\u6570\u636e\u8f6c\u5316\u4e3a\u591a\u9009\u9898\u683c\u5f0f\uff0c\u5e76\u901a\u8fc7\u6392\u540d\u5bf9\u9f50\u548c\u71b5\u5206\u6790\u8bc4\u4f30\u5e72\u6270\u9879\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u5f0f\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edf\u591a\u9009\u9898\u8bc4\u4f30\u4e2d\u9ad8\u8d28\u91cf\u5e72\u6270\u9879\u751f\u6210\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faD-GEN\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u6392\u540d\u5bf9\u9f50\u548c\u71b5\u5206\u6790\u4e24\u79cd\u65b0\u65b9\u6cd5\u8bc4\u4f30\u5e72\u6270\u9879\u8d28\u91cf\u3002", "result": "D-GEN\u5728\u6392\u540d\u4e00\u81f4\u6027\uff08Spearman's rho 0.99, Kendall's tau 0.94\uff09\u548c\u71b5\u5206\u5e03\u4e0a\u4e0e\u771f\u5b9e\u5e72\u6270\u9879\u63a5\u8fd1\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u9a8c\u8bc1\u4e86\u5176\u8d28\u91cf\u3002", "conclusion": "D-GEN\u4e3a\u591a\u9009\u9898\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u5e72\u6270\u9879\u751f\u6210\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8bc4\u4f30\u6807\u51c6\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.13393", "pdf": "https://arxiv.org/pdf/2504.13393", "abs": "https://arxiv.org/abs/2504.13393", "authors": ["S M Rayeed", "Alyson East", "Samuel Stevens", "Sydne Record", "Charles V Stewart"], "title": "BeetleVerse: A study on taxonomic classification of ground beetles", "categories": ["cs.CV"], "comment": null, "summary": "Ground beetles are a highly sensitive and speciose biological indicator,\nmaking them vital for monitoring biodiversity. However, they are currently an\nunderutilized resource due to the manual effort required by taxonomic experts\nto perform challenging species differentiations based on subtle morphological\ndifferences, precluding widespread applications. In this paper, we evaluate 12\nvision models on taxonomic classification across four diverse, long-tailed\ndatasets spanning over 230 genera and 1769 species, with images ranging from\ncontrolled laboratory settings to challenging field-collected (in-situ)\nphotographs. We further explore taxonomic classification in two important\nreal-world contexts: sample efficiency and domain adaptation. Our results show\nthat the Vision and Language Transformer combined with an MLP head is the best\nperforming model, with 97\\% accuracy at genus and 94\\% at species level. Sample\nefficiency analysis shows that we can reduce train data requirements by up to\n50\\% with minimal compromise in performance. The domain adaptation experiments\nreveal significant challenges when transferring models from lab to in-situ\nimages, highlighting a critical domain gap. Overall, our study lays a\nfoundation for large-scale automated taxonomic classification of beetles, and\nbeyond that, advances sample-efficient learning and cross-domain adaptation for\ndiverse long-tailed ecological datasets.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e8612\u79cd\u89c6\u89c9\u6a21\u578b\u5728\u7532\u866b\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ed3\u5408Vision and Language Transformer\u4e0eMLP\u7684\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u5e76\u63a2\u8ba8\u4e86\u6837\u672c\u6548\u7387\u548c\u9886\u57df\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u7532\u866b\u662f\u91cd\u8981\u7684\u751f\u7269\u591a\u6837\u6027\u6307\u6807\uff0c\u4f46\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u4e14\u6548\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8bc4\u4f3012\u79cd\u89c6\u89c9\u6a21\u578b\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u6db5\u76d6230\u5c5e1769\u79cd\u7532\u866b\uff0c\u5e76\u7814\u7a76\u6837\u672c\u6548\u7387\u548c\u9886\u57df\u9002\u5e94\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u5c5e\u548c\u79cd\u7ea7\u522b\u5206\u522b\u8fbe\u523097%\u548c94%\u7684\u51c6\u786e\u7387\uff0c\u6837\u672c\u6548\u7387\u53ef\u63d0\u534750%\uff0c\u4f46\u5b9e\u9a8c\u5ba4\u5230\u91ce\u5916\u56fe\u50cf\u7684\u9886\u57df\u9002\u5e94\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7532\u866b\u5927\u89c4\u6a21\u81ea\u52a8\u5206\u7c7b\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63a8\u52a8\u4e86\u6837\u672c\u9ad8\u6548\u5b66\u4e60\u548c\u8de8\u9886\u57df\u9002\u5e94\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.13471", "pdf": "https://arxiv.org/pdf/2504.13471", "abs": "https://arxiv.org/abs/2504.13471", "authors": ["Jiliang Ni", "Jiachen Pu", "Zhongyi Yang", "Kun Zhou", "Hui Wang", "Xiaoliang Xiao", "Dakui Wang", "Xin Li", "Jingfeng Luo", "Conggang Hu"], "title": "From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs", "categories": ["cs.CL"], "comment": null, "summary": "In recent years, Large Language Models (LLMs) have significantly advanced\nartificial intelligence by optimizing traditional Natural Language Processing\n(NLP) pipelines, improving performance and generalization. This has spurred\ntheir integration into various systems. Many NLP systems, including ours,\nemploy a \"one-stage\" pipeline directly incorporating LLMs. While effective,\nthis approach incurs substantial costs and latency due to the need for large\nmodel parameters to achieve satisfactory outcomes. This paper introduces a\nthree-stage cost-efficient end-to-end LLM deployment pipeline-including\nprototyping, knowledge transfer, and model compression-to tackle the\ncost-performance dilemma in LLM-based frameworks. Our approach yields a super\ntiny model optimized for cost and performance in online systems, simplifying\nthe system architecture. Initially, by transforming complex tasks into a\nfunction call-based LLM-driven pipeline, an optimal performance prototype\nsystem is constructed to produce high-quality data as a teacher model. The\nsecond stage combine techniques like rejection fine-tuning, reinforcement\nlearning and knowledge distillation to transfer knowledge to a smaller 0.5B\nstudent model, delivering effective performance at minimal cost. The final\nstage applies quantization and pruning to extremely compress model to 0.4B,\nachieving ultra-low latency and cost. The framework's modular design and\ncross-domain capabilities suggest potential applicability in other NLP areas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7684\u9ad8\u6548LLM\u90e8\u7f72\u6d41\u7a0b\uff0c\u901a\u8fc7\u539f\u578b\u8bbe\u8ba1\u3001\u77e5\u8bc6\u8f6c\u79fb\u548c\u6a21\u578b\u538b\u7f29\uff0c\u89e3\u51b3\u4e86LLM\u6846\u67b6\u4e2d\u7684\u6210\u672c\u4e0e\u6027\u80fd\u77db\u76fe\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8d85\u5c0f\u578b\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5355\u9636\u6bb5LLM\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u9700\u8981\u4f18\u5316\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u6784\u5efa\u539f\u578b\u7cfb\u7edf\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff1b2\uff09\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u7b49\u6280\u672f\u5c06\u77e5\u8bc6\u8f6c\u79fb\u5230\u5c0f\u578b\u5b66\u751f\u6a21\u578b\uff1b3\uff09\u901a\u8fc7\u91cf\u5316\u548c\u526a\u679d\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\u3002", "result": "\u6700\u7ec8\u6a21\u578b\u538b\u7f29\u81f30.4B\uff0c\u5b9e\u73b0\u4e86\u8d85\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\uff0c\u540c\u65f6\u6027\u80fd\u6709\u6548\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u8de8\u9886\u57df\u80fd\u529b\u8868\u660e\u5176\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6NLP\u9886\u57df\u3002"}}
{"id": "2504.13399", "pdf": "https://arxiv.org/pdf/2504.13399", "abs": "https://arxiv.org/abs/2504.13399", "authors": ["Shashank Shriram", "Srinivasa Perisetla", "Aryan Keskar", "Harsha Krishnaswamy", "Tonko Emil Westerhof Bossen", "Andreas M\u00f8gelmose", "Ross Greer"], "title": "Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting anomalous hazards in visual data, particularly in video streams, is\na critical challenge in autonomous driving. Existing models often struggle with\nunpredictable, out-of-label hazards due to their reliance on predefined object\ncategories. In this paper, we propose a multimodal approach that integrates\nvision-language reasoning with zero-shot object detection to improve hazard\nidentification and explanation. Our pipeline consists of a Vision-Language\nModel (VLM), a Large Language Model (LLM), in order to detect hazardous objects\nwithin a traffic scene. We refine object detection by incorporating OpenAI's\nCLIP model to match predicted hazards with bounding box annotations, improving\nlocalization accuracy. To assess model performance, we create a ground truth\ndataset by denoising and extending the foundational COOOL\n(Challenge-of-Out-of-Label) anomaly detection benchmark dataset with complete\nnatural language descriptions for hazard annotations. We define a means of\nhazard detection and labeling evaluation on the extended dataset using cosine\nsimilarity. This evaluation considers the semantic similarity between the\npredicted hazard description and the annotated ground truth for each video.\nAdditionally, we release a set of tools for structuring and managing\nlarge-scale hazard detection datasets. Our findings highlight the strengths and\nlimitations of current vision-language-based approaches, offering insights into\nfuture improvements in autonomous hazard detection systems. Our models,\nscripts, and data can be found at https://github.com/mi3labucm/COOOLER.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u548c\u96f6\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff0c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5371\u9669\u8bc6\u522b\u548c\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u96be\u4ee5\u5904\u7406\u4e0d\u53ef\u9884\u6d4b\u7684\u5f02\u5e38\u5371\u9669\uff0c\u9700\u6539\u8fdb\u5371\u9669\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5229\u7528CLIP\u6a21\u578b\u4f18\u5316\u76ee\u6807\u68c0\u6d4b\u548c\u5b9a\u4f4d\u3002", "result": "\u521b\u5efa\u4e86\u6269\u5c55\u7684COOOL\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u76f8\u5173\u5de5\u5177\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.13475", "pdf": "https://arxiv.org/pdf/2504.13475", "abs": "https://arxiv.org/abs/2504.13475", "authors": ["Chenwei Yan", "Xiangling Fu", "Yuxuan Xiong", "Tianyi Wang", "Siu Cheung Hui", "Ji Wu", "Xien Liu"], "title": "LLM Sensitivity Evaluation Framework for Clinical Diagnosis", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains. However, for clinical diagnosis, higher expectations are\nrequired for LLM's reliability and sensitivity: thinking like physicians and\nremaining sensitive to key medical information that affects diagnostic\nreasoning, as subtle variations can lead to different diagnosis results. Yet,\nexisting works focus mainly on investigating the sensitivity of LLMs to\nirrelevant context and overlook the importance of key information. In this\npaper, we investigate the sensitivity of LLMs, i.e. GPT-3.5, GPT-4, Gemini,\nClaude3 and LLaMA2-7b, to key medical information by introducing different\nperturbation strategies. The evaluation results highlight the limitations of\ncurrent LLMs in remaining sensitive to key medical information for diagnostic\ndecision-making. The evolution of LLMs must focus on improving their\nreliability, enhancing their ability to be sensitive to key information, and\neffectively utilizing this information. These improvements will enhance human\ntrust in LLMs and facilitate their practical application in real-world\nscenarios. Our code and dataset are available at\nhttps://github.com/chenwei23333/DiagnosisQA.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u53ef\u9760\u6027\u548c\u5173\u952e\u4fe1\u606f\u654f\u611f\u6027\u3002", "motivation": "\u4e34\u5e8a\u8bca\u65ad\u5bf9LLMs\u7684\u53ef\u9760\u6027\u548c\u654f\u611f\u6027\u8981\u6c42\u66f4\u9ad8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5173\u952e\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e0d\u540c\u6270\u52a8\u7b56\u7565\uff0c\u8bc4\u4f30GPT-3.5\u3001GPT-4\u3001Gemini\u3001Claude3\u548cLLaMA2-7b\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\u3002", "result": "\u5f53\u524dLLMs\u5728\u8bca\u65ad\u51b3\u7b56\u4e2d\u5bf9\u5173\u952e\u533b\u5b66\u4fe1\u606f\u7684\u654f\u611f\u6027\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "LLMs\u9700\u63d0\u5347\u53ef\u9760\u6027\u548c\u5173\u952e\u4fe1\u606f\u654f\u611f\u6027\uff0c\u4ee5\u589e\u5f3a\u4eba\u7c7b\u4fe1\u4efb\u5e76\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13402", "pdf": "https://arxiv.org/pdf/2504.13402", "abs": "https://arxiv.org/abs/2504.13402", "authors": ["Vedrana Ivezi\u0107", "Ashwath Radhachandran", "Ekaterina Redekop", "Shreeram Athreya", "Dongwoo Lee", "Vivek Sant", "Corey Arnold", "William Speier"], "title": "CytoFM: The first cytology foundation model", "categories": ["cs.CV"], "comment": null, "summary": "Cytology is essential for cancer diagnostics and screening due to its\nminimally invasive nature. However, the development of robust deep learning\nmodels for digital cytology is challenging due to the heterogeneity in staining\nand preparation methods of samples, differences across organs, and the limited\navailability of large, diverse, annotated datasets. Developing a task-specific\nmodel for every cytology application is impractical and non-cytology-specific\nfoundation models struggle to generalize to tasks in this domain where the\nemphasis is on cell morphology. To address these challenges, we introduce\nCytoFM, the first cytology self-supervised foundation model. Using iBOT, a\nself-supervised Vision Transformer (ViT) training framework incorporating\nmasked image modeling and self-distillation, we pretrain CytoFM on a diverse\ncollection of cytology datasets to learn robust, transferable representations.\nWe evaluate CytoFM on multiple downstream cytology tasks, including breast\ncancer classification and cell type identification, using an attention-based\nmultiple instance learning framework. Our results demonstrate that CytoFM\nperforms better on two out of three downstream tasks than existing foundation\nmodels pretrained on histopathology (UNI) or natural images (iBOT-Imagenet).\nVisualizations of learned representations demonstrate our model is able to\nattend to cytologically relevant features. Despite a small pre-training\ndataset, CytoFM's promising results highlight the ability of task-agnostic\npre-training approaches to learn robust and generalizable features from\ncytology data.", "AI": {"tldr": "CytoFM\u662f\u9996\u4e2a\u7ec6\u80de\u5b66\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7iBOT\u6846\u67b6\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u9c81\u68d2\u4e14\u53ef\u8fc1\u79fb\u7684\u8868\u793a\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u7ec6\u80de\u5b66\u5728\u764c\u75c7\u8bca\u65ad\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u9762\u4e34\u6837\u672c\u5f02\u8d28\u6027\u3001\u5668\u5b98\u5dee\u5f02\u548c\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u4efb\u52a1\u65e0\u5173\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528iBOT\u6846\u67b6\uff08\u7ed3\u5408\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u548c\u81ea\u84b8\u998f\uff09\u9884\u8bad\u7ec3CytoFM\uff0c\u5e76\u5728\u591a\u4e2a\u7ec6\u80de\u5b66\u4efb\u52a1\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "CytoFM\u5728\u4e24\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u6216\u81ea\u7136\u56fe\u50cf\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u5173\u6ce8\u7ec6\u80de\u5b66\u76f8\u5173\u7279\u5f81\u3002", "conclusion": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6570\u636e\u91cf\u5c0f\uff0cCytoFM\u8bc1\u660e\u4e86\u4efb\u52a1\u65e0\u5173\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u7ec6\u80de\u5b66\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13500", "pdf": "https://arxiv.org/pdf/2504.13500", "abs": "https://arxiv.org/abs/2504.13500", "authors": ["Jianing Wang", "Jin Jiang", "Yang Liu", "Mengdi Zhang", "Xunliang Cai"], "title": "Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce a new \\emph{process prejudge} strategy in LLM\nreasoning to demonstrate that bootstrapping with process prejudge allows the\nLLM to adaptively anticipate the errors encountered when advancing the\nsubsequent reasoning steps, similar to people sometimes pausing to think about\nwhat mistakes may occur and how to avoid them, rather than relying solely on\ntrial and error. Specifically, we define a prejudge node in the rationale,\nwhich represents a reasoning step, with at least one step that follows the\nprejudge node that has no paths toward the correct answer. To synthesize the\nprejudge reasoning process, we present an automated reasoning framework with a\ndynamic tree-searching strategy. This framework requires only one LLM to\nperform answer judging, response critiquing, prejudge generation, and thought\ncompletion. Furthermore, we develop a two-phase training mechanism with\nsupervised fine-tuning (SFT) and reinforcement learning (RL) to further enhance\nthe reasoning capabilities of LLMs. Experimental results from competition-level\ncomplex reasoning demonstrate that our method can teach the model to prejudge\nbefore thinking and significantly enhance the reasoning ability of LLMs. Code\nand data is released at https://github.com/wjn1996/Prejudge-Before-Think.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8fc7\u7a0b\u9884\u5224\u201d\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u6811\u641c\u7d22\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u4f9d\u8d56\u8bd5\u9519\uff0c\u800c\u4eba\u7c7b\u5e38\u901a\u8fc7\u9884\u5224\u6f5c\u5728\u9519\u8bef\u6765\u4f18\u5316\u63a8\u7406\u3002\u672c\u6587\u65e8\u5728\u6a21\u62df\u8fd9\u4e00\u884c\u4e3a\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u7387\u3002", "method": "\u5b9a\u4e49\u9884\u5224\u8282\u70b9\uff0c\u7ed3\u5408\u52a8\u6001\u6811\u641c\u7d22\u6846\u67b6\uff0c\u5229\u7528\u5355LLM\u5b8c\u6210\u7b54\u6848\u5224\u65ad\u3001\u54cd\u5e94\u6279\u8bc4\u3001\u9884\u5224\u751f\u6210\u548c\u601d\u7ef4\u8865\u5168\u3002\u91c7\u7528SFT\u548cRL\u4e24\u9636\u6bb5\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u201c\u8fc7\u7a0b\u9884\u5224\u201d\u7b56\u7565\u6709\u6548\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u884c\u4e3a\uff0c\u663e\u8457\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2504.13405", "pdf": "https://arxiv.org/pdf/2504.13405", "abs": "https://arxiv.org/abs/2504.13405", "authors": ["Shengqin Jiang", "Linfei Li", "Haokui Zhang", "Qingshan Liu", "Amin Beheshti", "Jian Yang", "Anton van den Hengel", "Quan Z. Sheng", "Yuankai Qi"], "title": "ProgRoCC: A Progressive Approach to Rough Crowd Counting", "categories": ["cs.CV"], "comment": "Under review", "summary": "As the number of individuals in a crowd grows, enumeration-based techniques\nbecome increasingly infeasible and their estimates increasingly unreliable. We\npropose instead an estimation-based version of the problem: we label Rough\nCrowd Counting that delivers better accuracy on the basis of training data that\nis easier to acquire. Rough crowd counting requires only rough annotations of\nthe number of targets in an image, instead of the more traditional, and far\nmore expensive, per-target annotations. We propose an approach to the rough\ncrowd counting problem based on CLIP, termed ProgRoCC. Specifically, we\nintroduce a progressive estimation learning strategy that determines the object\ncount through a coarse-to-fine approach. This approach delivers answers\nquickly, outperforms the state-of-the-art in semi- and weakly-supervised crowd\ncounting. In addition, we design a vision-language matching adapter that\noptimizes key-value pairs by mining effective matches of two modalities to\nrefine the visual features, thereby improving the final performance. Extensive\nexperimental results on three widely adopted crowd counting datasets\ndemonstrate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u7c97\u7565\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5ProgRoCC\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u4f30\u8ba1\u5b66\u4e60\u7b56\u7565\u548c\u89c6\u89c9\u8bed\u8a00\u5339\u914d\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534a\u76d1\u7763\u548c\u5f31\u76d1\u7763\u4eba\u7fa4\u8ba1\u6570\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\u5728\u4eba\u7fa4\u89c4\u6a21\u589e\u5927\u65f6\u53d8\u5f97\u4e0d\u53ef\u884c\u4e14\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u6613\u4e8e\u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u4f30\u8ba1\u5b66\u4e60\u7b56\u7565\uff08\u7c97\u5230\u7ec6\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u5339\u914d\u9002\u914d\u5668\u4f18\u5316\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u91c7\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u534a\u76d1\u7763\u548c\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "ProgRoCC\u65b9\u6cd5\u5728\u7c97\u7565\u4eba\u7fa4\u8ba1\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13534", "pdf": "https://arxiv.org/pdf/2504.13534", "abs": "https://arxiv.org/abs/2504.13534", "authors": ["Feiyang Li", "Peng Fang", "Zhan Shi", "Arijit Khan", "Fang Wang", "Dan Feng", "Weihao Wang", "Xin Zhang", "Yongjian Cui"], "title": "CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While chain-of-thought (CoT) reasoning improves the performance of large\nlanguage models (LLMs) in complex tasks, it still has two main challenges: the\nlow reliability of relying solely on LLMs to generate reasoning chains and the\ninterference of natural language reasoning chains on the inference logic of\nLLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework\nwith three key designs: (i) Knowledge Graph-driven CoT Generation, featuring\nknowledge graphs to modulate reasoning chain generation of LLMs, thereby\nenhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which\nincorporates retrieval-augmented generation (RAG) into knowledge graphs to\nretrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable\ninformation; (iii) Pseudo-Program Prompting Execution, which encourages LLMs to\nexecute reasoning tasks in pseudo-programs with greater logical rigor. We\nconduct a comprehensive evaluation on nine public datasets, covering three\nreasoning problems. Compared with the-state-of-the-art methods, CoT-RAG\nexhibits a significant accuracy improvement, ranging from 4.0% to 23.0%.\nFurthermore, testing on four domain-specific datasets, CoT-RAG shows remarkable\naccuracy and efficient execution, highlighting its strong practical\napplicability and scalability.", "AI": {"tldr": "CoT-RAG\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u63a8\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u4f4e\u53ef\u9760\u6027\u548c\u81ea\u7136\u8bed\u8a00\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ecLLMs\u751f\u6210\u63a8\u7406\u94fe\u7684\u4f4e\u53ef\u9760\u6027\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\u5bf9LLMs\u63a8\u7406\u903b\u8f91\u7684\u5e72\u6270\u3002", "method": "CoT-RAG\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684CoT\u751f\u6210\u3001\u53ef\u5b66\u4e60\u7684\u77e5\u8bc6\u6848\u4f8b\u611f\u77e5RAG\u548c\u4f2a\u7a0b\u5e8f\u63d0\u793a\u6267\u884c\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff0c\u63d0\u5347\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u903b\u8f91\u4e25\u8c28\u6027\u3002", "result": "\u5728\u4e5d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCoT-RAG\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53474.0%\u81f323.0%\uff0c\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "CoT-RAG\u663e\u8457\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u7528\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2504.13407", "pdf": "https://arxiv.org/pdf/2504.13407", "abs": "https://arxiv.org/abs/2504.13407", "authors": ["Shimou Ling", "Liang Zhang", "Jiangwei Zhao", "Lili Pan", "Hongliang Li"], "title": "LoRA-Based Continual Learning with Constraints on Critical Parameter Changes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "LoRA-based continual learning represents a promising avenue for leveraging\npre-trained models in downstream continual learning tasks. Recent studies have\nshown that orthogonal LoRA tuning effectively mitigates forgetting. However,\nthis work unveils that under orthogonal LoRA tuning, the critical parameters\nfor pre-tasks still change notably after learning post-tasks. To address this\nproblem, we directly propose freezing the most critical parameter matrices in\nthe Vision Transformer (ViT) for pre-tasks before learning post-tasks. In\naddition, building on orthogonal LoRA tuning, we propose orthogonal LoRA\ncomposition (LoRAC) based on QR decomposition, which may further enhance the\nplasticity of our method. Elaborate ablation studies and extensive comparisons\ndemonstrate the effectiveness of our proposed method. Our results indicate that\nour method achieves state-of-the-art (SOTA) performance on several well-known\ncontinual learning benchmarks. For instance, on the Split CIFAR-100 dataset,\nour method shows a 6.35\\% improvement in accuracy and a 3.24\\% reduction in\nforgetting compared to previous methods. Our code is available at\nhttps://github.com/learninginvision/LoRAC-IPC.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u51bb\u7ed3\u5173\u952e\u53c2\u6570\u77e9\u9635\u548c\u6b63\u4ea4LoRA\u7ec4\u5408\uff08LoRAC\uff09\u6765\u51cf\u5c11\u9057\u5fd8\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6b63\u4ea4LoRA\u8c03\u4f18\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u9884\u4efb\u52a1\u7684\u5173\u952e\u53c2\u6570\u5728\u5b66\u4e60\u540e\u4efb\u52a1\u65f6\u4ecd\u4f1a\u663e\u8457\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u51bb\u7ed3ViT\u4e2d\u9884\u4efb\u52a1\u7684\u5173\u952e\u53c2\u6570\u77e9\u9635\uff0c\u5e76\u7ed3\u5408\u6b63\u4ea4LoRA\u8c03\u4f18\uff0c\u8fdb\u4e00\u6b65\u63d0\u51fa\u57fa\u4e8eQR\u5206\u89e3\u7684\u6b63\u4ea4LoRA\u7ec4\u5408\uff08LoRAC\uff09\u3002", "result": "\u5728Split CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e866.35%\u7684\u51c6\u786e\u7387\u63d0\u5347\u548c3.24%\u7684\u9057\u5fd8\u7387\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51bb\u7ed3\u5173\u952e\u53c2\u6570\u548cLoRAC\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u6210\u4e3a\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u4e4b\u4e00\u3002"}}
{"id": "2504.13545", "pdf": "https://arxiv.org/pdf/2504.13545", "abs": "https://arxiv.org/abs/2504.13545", "authors": ["Azmarah Rizvi", "Navojith Thamindu", "A. M. N. H. Adhikari", "W. P. U. Senevirathna", "Dharshana Kasthurirathna", "Lakmini Abeywardhana"], "title": "Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 6 figures, 4 tables", "summary": "Sentiment analysis is crucial for brand reputation management in the banking\nsector, where customer feedback spans English, Sinhala, Singlish, and\ncode-mixed text. Existing models struggle with low-resource languages like\nSinhala and lack interpretability for practical use. This research develops a\nhybrid aspect-based sentiment analysis framework that enhances multilingual\ncapabilities with explainable outputs. Using cleaned banking customer reviews,\nwe fine-tune XLM-RoBERTa for Sinhala and code-mixed text, integrate\ndomain-specific lexicon correction, and employ BERT-base-uncased for English.\nThe system classifies sentiment (positive, neutral, negative) with confidence\nscores, while SHAP and LIME improve interpretability by providing real-time\nsentiment explanations. Experimental results show that our approaches\noutperform traditional transformer-based classifiers, achieving 92.3 percent\naccuracy and an F1-score of 0.89 in English and 88.4 percent in Sinhala and\ncode-mixed content. An explainability analysis reveals key sentiment drivers,\nimproving trust and transparency. A user-friendly interface delivers\naspect-wise sentiment insights, ensuring accessibility for businesses. This\nresearch contributes to robust, transparent sentiment analysis for financial\napplications by bridging gaps in multilingual, low-resource NLP and\nexplainability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u94f6\u884c\u5ba2\u6237\u53cd\u9988\u7684\u591a\u8bed\u8a00\u60c5\u611f\u5206\u6790\uff0c\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u50e7\u4f3d\u7f57\u8bed\uff09\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u4e86\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u94f6\u884c\u5ba2\u6237\u53cd\u9988\u6d89\u53ca\u591a\u79cd\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u50e7\u4f3d\u7f57\u8bed\u3001\u65b0\u52a0\u5761\u5f0f\u82f1\u8bed\u548c\u6df7\u5408\u6587\u672c\uff09\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u7ed3\u5408\u4e86XLM-RoBERTa\uff08\u7528\u4e8e\u50e7\u4f3d\u7f57\u8bed\u548c\u6df7\u5408\u6587\u672c\uff09\u548cBERT-base-uncased\uff08\u7528\u4e8e\u82f1\u8bed\uff09\uff0c\u5e76\u6574\u5408\u4e86\u9886\u57df\u7279\u5b9a\u7684\u8bcd\u5178\u6821\u6b63\u548c\u53ef\u89e3\u91ca\u6027\u5de5\u5177\uff08SHAP\u548cLIME\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u5728\u82f1\u8bed\u4e2d\u8fbe\u523092.3%\u7684\u51c6\u786e\u7387\u548c0.89\u7684F1\u5206\u6570\uff0c\u5728\u50e7\u4f3d\u7f57\u8bed\u548c\u6df7\u5408\u6587\u672c\u4e2d\u8fbe\u523088.4%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u60c5\u611f\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91d1\u878d\u9886\u57df\u7684\u591a\u8bed\u8a00\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u586b\u8865\u4e86\u4f4e\u8d44\u6e90NLP\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.13412", "pdf": "https://arxiv.org/pdf/2504.13412", "abs": "https://arxiv.org/abs/2504.13412", "authors": ["Samuel Audia", "Soheil Feizi", "Matthias Zwicker", "Dinesh Manocha"], "title": "How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Neural networks that map between low dimensional spaces are ubiquitous in\ncomputer graphics and scientific computing; however, in their naive\nimplementation, they are unable to learn high frequency information. We present\na comprehensive analysis comparing the two most common techniques for\nmitigating this spectral bias: Fourier feature encodings (FFE) and multigrid\nparametric encodings (MPE). FFEs are seen as the standard for low dimensional\nmappings, but MPEs often outperform them and learn representations with higher\nresolution and finer detail. FFE's roots in the Fourier transform, make it\nsusceptible to aliasing if pushed too far, while MPEs, which use a learned grid\nstructure, have no such limitation. To understand the difference in\nperformance, we use the neural tangent kernel (NTK) to evaluate these encodings\nthrough the lens of an analogous kernel regression. By finding a lower bound on\nthe smallest eigenvalue of the NTK, we prove that MPEs improve a network's\nperformance through the structure of their grid and not their learnable\nembedding. This mechanism is fundamentally different from FFEs, which rely\nsolely on their embedding space to improve performance. Results are empirically\nvalidated on a 2D image regression task using images taken from 100 synonym\nsets of ImageNet and 3D implicit surface regression on objects from the\nStanford graphics dataset. Using peak signal-to-noise ratio (PSNR) and\nmultiscale structural similarity (MS-SSIM) to evaluate how well fine details\nare learned, we show that the MPE increases the minimum eigenvalue by 8 orders\nof magnitude over the baseline and 2 orders of magnitude over the FFE. The\nincrease in spectrum corresponds to a 15 dB (PSNR) / 0.65 (MS-SSIM) increase\nover baseline and a 12 dB (PSNR) / 0.33 (MS-SSIM) increase over the FFE.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u9891\u8c31\u504f\u5dee\u7684\u6280\u672f\uff1a\u5085\u91cc\u53f6\u7279\u5f81\u7f16\u7801\uff08FFE\uff09\u548c\u591a\u7f51\u683c\u53c2\u6570\u7f16\u7801\uff08MPE\uff09\uff0c\u53d1\u73b0MPE\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eFFE\u3002", "motivation": "\u4f4e\u7ef4\u7a7a\u95f4\u6620\u5c04\u7684\u795e\u7ecf\u7f51\u7edc\u96be\u4ee5\u5b66\u4e60\u9ad8\u9891\u4fe1\u606f\uff0c\u9700\u8981\u6709\u6548\u7684\u65b9\u6cd5\u6765\u7f13\u89e3\u9891\u8c31\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u5206\u6790FFE\u548cMPE\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u57282D\u56fe\u50cf\u56de\u5f52\u548c3D\u9690\u5f0f\u8868\u9762\u56de\u5f52\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "result": "MPE\u7684\u6700\u5c0f\u7279\u5f81\u503c\u6bd4\u57fa\u7ebf\u9ad88\u4e2a\u6570\u91cf\u7ea7\uff0c\u6bd4FFE\u9ad82\u4e2a\u6570\u91cf\u7ea7\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "MPE\u901a\u8fc7\u7f51\u683c\u7ed3\u6784\u800c\u975e\u5d4c\u5165\u7a7a\u95f4\u63d0\u5347\u6027\u80fd\uff0c\u4e0eFFE\u673a\u5236\u4e0d\u540c\uff0c\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2504.13562", "pdf": "https://arxiv.org/pdf/2504.13562", "abs": "https://arxiv.org/abs/2504.13562", "authors": ["Yu Li", "Han Jiang", "Zhihua Wei"], "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification", "categories": ["cs.CL"], "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), jailbreak\nattacks have become an increasingly pressing safety concern. While\nsafety-aligned LLMs can effectively defend against normal harmful queries, they\nremain vulnerable to such attacks. Existing defense methods primarily rely on\nfine-tuning or input modification, which often suffer from limited\ngeneralization and reduced utility. To address this, we introduce DETAM, a\nfinetuning-free defense approach that improves the defensive capabilities\nagainst jailbreak attacks of LLMs via targeted attention modification.\nSpecifically, we analyze the differences in attention scores between successful\nand unsuccessful defenses to identify the attention heads sensitive to\njailbreak attacks. During inference, we reallocate attention to emphasize the\nuser's core intention, minimizing interference from attack tokens. Our\nexperimental results demonstrate that DETAM outperforms various baselines in\njailbreak defense and exhibits robust generalization across different attacks\nand models, maintaining its effectiveness even on in-the-wild jailbreak data.\nFurthermore, in evaluating the model's utility, we incorporated over-defense\ndatasets, which further validate the superior performance of our approach. The\ncode will be released immediately upon acceptance.", "AI": {"tldr": "DETAM\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4fee\u6539\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347LLMs\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8d8a\u72f1\u653b\u51fb\u6210\u4e3a\u5b89\u5168\u95ee\u9898\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u5b9e\u7528\u6027\u964d\u4f4e\u3002", "method": "\u5206\u6790\u6ce8\u610f\u529b\u5206\u6570\u5dee\u5f02\uff0c\u8bc6\u522b\u654f\u611f\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91cd\u65b0\u5206\u914d\u6ce8\u610f\u529b\u4ee5\u5f3a\u8c03\u7528\u6237\u6838\u5fc3\u610f\u56fe\u3002", "result": "DETAM\u5728\u8d8a\u72f1\u9632\u5fa1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4e14\u5728\u5b9e\u7528\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DETAM\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u9632\u5fa1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u653b\u51fb\u548c\u6a21\u578b\u3002"}}
{"id": "2504.13419", "pdf": "https://arxiv.org/pdf/2504.13419", "abs": "https://arxiv.org/abs/2504.13419", "authors": ["Wenyu Li", "Sidun Liu", "Peng Qiao", "Yong Dou"], "title": "Mono3R: Exploiting Monocular Cues for Geometric 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in data-driven geometric multi-view 3D reconstruction\nfoundation models (e.g., DUSt3R) have shown remarkable performance across\nvarious 3D vision tasks, facilitated by the release of large-scale,\nhigh-quality 3D datasets. However, as we observed, constrained by their\nmatching-based principles, the reconstruction quality of existing models\nsuffers significant degradation in challenging regions with limited matching\ncues, particularly in weakly textured areas and low-light conditions. To\nmitigate these limitations, we propose to harness the inherent robustness of\nmonocular geometry estimation to compensate for the inherent shortcomings of\nmatching-based methods. Specifically, we introduce a monocular-guided\nrefinement module that integrates monocular geometric priors into multi-view\nreconstruction frameworks. This integration substantially enhances the\nrobustness of multi-view reconstruction systems, leading to high-quality\nfeed-forward reconstructions. Comprehensive experiments across multiple\nbenchmarks demonstrate that our method achieves substantial improvements in\nboth mutli-view camera pose estimation and point cloud accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5355\u76ee\u51e0\u4f55\u5148\u9a8c\u7684\u591a\u89c6\u56fe3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7eb9\u7406\u7a00\u5c11\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5339\u914d\u7684\u591a\u89c6\u56fe3D\u91cd\u5efa\u6a21\u578b\u5728\u7eb9\u7406\u7a00\u5c11\u548c\u4f4e\u5149\u6761\u4ef6\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u5f15\u5165\u5355\u76ee\u5f15\u5bfc\u7684\u7ec6\u5316\u6a21\u5757\uff0c\u5c06\u5355\u76ee\u51e0\u4f55\u5148\u9a8c\u6574\u5408\u5230\u591a\u89c6\u56fe\u91cd\u5efa\u6846\u67b6\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u70b9\u4e91\u7cbe\u5ea6\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5355\u76ee\u51e0\u4f55\u5148\u9a8c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u591a\u89c6\u56fe\u91cd\u5efa\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.13592", "pdf": "https://arxiv.org/pdf/2504.13592", "abs": "https://arxiv.org/abs/2504.13592", "authors": ["Zihao Feng", "Xiaoxue Wang", "Ziwei Bai", "Donghang Su", "Bowen Wu", "Qun Yu", "Baoxun Wang"], "title": "Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling", "categories": ["cs.CL"], "comment": null, "summary": "Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u5956\u52b1\u8bfe\u7a0b\u91c7\u6837\uff08RCS\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u610f\u56fe\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u672a\u89c1\u610f\u56fe\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u96f6\u6837\u672c\u91cd\u6784\u548c\u57fa\u4e8eLLM\u7684\u52a8\u6001\u8bc6\u522b\uff09\u5728\u9047\u5230\u672a\u89c1\u610f\u56fe\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5bfc\u81f4\u4efb\u52a1\u8def\u7531\u9519\u8bef\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u5956\u52b1\u8bfe\u7a0b\u91c7\u6837\uff08RCS\uff09\u8fdb\u884c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u8bad\u7ec3\uff0c\u5e76\u5728RL\u4e2d\u5f15\u5165\u601d\u7ef4\u94fe\uff08COT\uff09\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRL\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6cdb\u5316\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u57fa\u7ebf\uff0cRCS\u548cCOT\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86RL\u5728\u610f\u56fe\u68c0\u6d4b\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u610f\u56fe\u68c0\u6d4b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u90e8\u7f72\u9002\u5e94\u6027\u5f3a\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2504.13428", "pdf": "https://arxiv.org/pdf/2504.13428", "abs": "https://arxiv.org/abs/2504.13428", "authors": ["Qi'ao Xu", "Pengfei Wang", "Yanjun Li", "Tianwen Qian", "Xiaoling Wang"], "title": "HSACNet: Hierarchical Scale-Aware Consistency Regularized Semi-Supervised Change Detection", "categories": ["cs.CV"], "comment": "7 pages, 8 figures, accepted by ICME 2025", "summary": "Semi-supervised change detection (SSCD) aims to detect changes between\nbi-temporal remote sensing images by utilizing limited labeled data and\nabundant unlabeled data. Existing methods struggle in complex scenarios,\nexhibiting poor performance when confronted with noisy data. They typically\nneglect intra-layer multi-scale features while emphasizing inter-layer fusion,\nharming the integrity of change objects with different scales. In this paper,\nwe propose HSACNet, a Hierarchical Scale-Aware Consistency regularized Network\nfor SSCD. Specifically, we integrate Segment Anything Model 2 (SAM2), using its\nHiera backbone as the encoder to extract inter-layer multi-scale features and\napplying adapters for parameter-efficient fine-tuning. Moreover, we design a\nScale-Aware Differential Attention Module (SADAM) that can precisely capture\nintra-layer multi-scale change features and suppress noise. Additionally, a\ndual-augmentation consistency regularization strategy is adopted to effectively\nutilize the unlabeled data. Extensive experiments across four CD benchmarks\ndemonstrate that our HSACNet achieves state-of-the-art performance, with\nreduced parameters and computational cost.", "AI": {"tldr": "HSACNet\u662f\u4e00\u79cd\u534a\u76d1\u7763\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408SAM2\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u548c\u566a\u58f0\u6570\u636e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5ffd\u89c6\u4e86\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u5b8c\u6574\u6027\u3002", "method": "\u4f7f\u7528SAM2\u7684Hiera\u4e3b\u5e72\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8bbe\u8ba1SADAM\u6a21\u5757\u6355\u83b7\u591a\u5c3a\u5ea6\u53d8\u5316\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u53cc\u589e\u5f3a\u4e00\u81f4\u6027\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "HSACNet\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u548c\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u534a\u76d1\u7763\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2504.13603", "pdf": "https://arxiv.org/pdf/2504.13603", "abs": "https://arxiv.org/abs/2504.13603", "authors": ["Pin-Er Chen", "Da-Chen Lian", "Shu-Kai Hsieh", "Sieh-Chuen Huang", "Hsuan-Lei Shao", "Jun-Wei Chiu", "Yang-Hsien Lin", "Zih-Ching Chen", "Cheng-Kuang", "Eddie TC Huang", "Simon See"], "title": "Continual Pre-Training is (not) What You Need in Domain Adaption", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "The recent advances in Legal Large Language Models (LLMs) have transformed\nthe landscape of legal research and practice by automating tasks, enhancing\nresearch precision, and supporting complex decision-making processes. However,\neffectively adapting LLMs to the legal domain remains challenging due to the\ncomplexity of legal reasoning, the need for precise interpretation of\nspecialized language, and the potential for hallucinations. This paper examines\nthe efficacy of Domain-Adaptive Continual Pre-Training (DACP) in improving the\nlegal reasoning capabilities of LLMs. Through a series of experiments on legal\nreasoning tasks within the Taiwanese legal framework, we demonstrate that while\nDACP enhances domain-specific knowledge, it does not uniformly improve\nperformance across all legal tasks. We discuss the trade-offs involved in DACP,\nparticularly its impact on model generalization and performance in prompt-based\ntasks, and propose directions for future research to optimize domain adaptation\nstrategies in legal AI.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\uff08DACP\uff09\u5728\u63d0\u5347\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u867d\u80fd\u589e\u5f3a\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u5e76\u975e\u5bf9\u6240\u6709\u6cd5\u5f8b\u4efb\u52a1\u5747\u6709\u6548\u3002", "motivation": "\u6cd5\u5f8bLLMs\u5728\u81ea\u52a8\u5316\u4efb\u52a1\u548c\u63d0\u5347\u7814\u7a76\u7cbe\u5ea6\u65b9\u9762\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9002\u5e94\u6cd5\u5f8b\u9886\u57df\u4ecd\u9762\u4e34\u6cd5\u5f8b\u63a8\u7406\u590d\u6742\u6027\u3001\u4e13\u4e1a\u8bed\u8a00\u7cbe\u786e\u89e3\u91ca\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5728\u4e2d\u56fd\u53f0\u6e7e\u6cd5\u5f8b\u6846\u67b6\u4e0b\u7684\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u5b9e\u9a8c\uff0c\u8bc4\u4f30DACP\u7684\u6548\u679c\u3002", "result": "DACP\u80fd\u589e\u5f3a\u9886\u57df\u77e5\u8bc6\uff0c\u4f46\u5bf9\u6240\u6709\u6cd5\u5f8b\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u4e0d\u4e00\u81f4\uff0c\u4e14\u5f71\u54cd\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63d0\u793a\u4efb\u52a1\u8868\u73b0\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u4f18\u5316\u6cd5\u5f8bAI\u7684\u9886\u57df\u9002\u5e94\u7b56\u7565\uff0c\u4ee5\u5e73\u8861DACP\u7684\u5229\u5f0a\u3002"}}
{"id": "2504.13432", "pdf": "https://arxiv.org/pdf/2504.13432", "abs": "https://arxiv.org/abs/2504.13432", "authors": ["Chu Chen", "Han Zhang", "Lok Ming Lui"], "title": "Circular Image Deturbulence using Quasi-conformal Geometry", "categories": ["cs.CV"], "comment": null, "summary": "The presence of inhomogeneous media between optical sensors and objects leads\nto distorted imaging outputs, significantly complicating downstream\nimage-processing tasks. A key challenge in image restoration is the lack of\nhigh-quality, paired-label images required for training supervised models. In\nthis paper, we introduce the Circular Quasi-Conformal Deturbulence (CQCD)\nframework, an unsupervised approach for removing image distortions through a\ncircular architecture. This design ensures that the restored image remains both\ngeometrically accurate and visually faithful while preventing the accumulation\nof incorrect estimations.The circular restoration process involves both forward\nand inverse mapping. To ensure the bijectivity of the estimated non-rigid\ndeformations, computational quasi-conformal geometry theories are leveraged to\nregularize the mapping, enforcing its homeomorphic properties. This guarantees\na well-defined transformation that preserves structural integrity and prevents\nunwanted artifacts. Furthermore, tight-frame blocks are integrated to encode\ndistortion-sensitive features for precise recovery. To validate the performance\nof our approach, we conduct evaluations on various synthetic and real-world\ncaptured images. Experimental results demonstrate that CQCD not only\noutperforms existing state-of-the-art deturbulence methods in terms of image\nrestoration quality but also provides highly accurate deformation field\nestimations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCQCD\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5706\u5f62\u67b6\u6784\u6d88\u9664\u56fe\u50cf\u5931\u771f\uff0c\u786e\u4fdd\u51e0\u4f55\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "motivation": "\u7531\u4e8e\u5149\u5b66\u4f20\u611f\u5668\u4e0e\u7269\u4f53\u4e4b\u95f4\u7684\u4e0d\u5747\u5300\u4ecb\u8d28\u5bfc\u81f4\u56fe\u50cf\u5931\u771f\uff0c\u800c\u9ad8\u8d28\u91cf\u914d\u5bf9\u6807\u7b7e\u56fe\u50cf\u7684\u7f3a\u4e4f\u9650\u5236\u4e86\u76d1\u7763\u6a21\u578b\u7684\u8bad\u7ec3\u3002", "method": "\u91c7\u7528\u5706\u5f62\u67b6\u6784\u8fdb\u884c\u524d\u5411\u548c\u9006\u5411\u6620\u5c04\uff0c\u5229\u7528\u51c6\u5171\u5f62\u51e0\u4f55\u7406\u8bba\u786e\u4fdd\u53cc\u5c04\u6027\uff0c\u5e76\u96c6\u6210\u7d27\u6846\u67b6\u5757\u7f16\u7801\u5931\u771f\u654f\u611f\u7279\u5f81\u3002", "result": "CQCD\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u4ec5\u6062\u590d\u8d28\u91cf\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u80fd\u51c6\u786e\u4f30\u8ba1\u53d8\u5f62\u573a\u3002", "conclusion": "CQCD\u6846\u67b6\u5728\u65e0\u76d1\u7763\u56fe\u50cf\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u5931\u771f\u95ee\u9898\u5e76\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u53d8\u5f62\u4f30\u8ba1\u3002"}}
{"id": "2504.13615", "pdf": "https://arxiv.org/pdf/2504.13615", "abs": "https://arxiv.org/abs/2504.13615", "authors": ["Ritwik Mishra", "Rajiv Ratn Shah", "Ponnurangam Kumaraguru"], "title": "Long-context Non-factoid Question Answering in Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00\u4e2d\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\uff08\u5982OIE\u3001\u5171\u6307\u6d88\u89e3\u548cAPS\uff09\u63d0\u5347QA\u4efb\u52a1\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u8fd9\u4e9b\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u8bed\u4e49\u548c\u8bcd\u7ea7\u5206\u6570\uff0c\u5e76\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5bf9LLMs\u5728QA\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u7684\u5370\u5ea6\u8bed\u8a00\u4e2d\u3002", "method": "\u91c7\u7528\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\uff08OIE\u3001\u5171\u6307\u6d88\u89e3\u3001APS\u53ca\u5176\u7ec4\u5408\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u5728\u56db\u79cd\u5370\u5ea6\u8bed\u8a00\u4e2d\u9a8c\u8bc1\u6548\u679c\u3002", "result": "\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\u5e73\u5747\u63d0\u53474%\u8bed\u4e49\u5206\u6570\u548c47%\u8bcd\u7ea7\u5206\u6570\uff1b\u5fae\u8c03\u540e\u8fdb\u4e00\u6b65\u63d0\u53472%\u3002\u540c\u65f6\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u7f29\u77ed\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347LLM-based QA\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u4f46\u5bf9\u975e\u4e8b\u5b9e\u6027\u95ee\u9898\u4ecd\u6709\u5c40\u9650\u6027\u3002"}}
{"id": "2504.13440", "pdf": "https://arxiv.org/pdf/2504.13440", "abs": "https://arxiv.org/abs/2504.13440", "authors": ["Cheng Yuan", "Yutong Ban"], "title": "Temporal Propagation of Asymmetric Feature Pyramid for Surgical Scene Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Surgical scene segmentation is crucial for robot-assisted laparoscopic\nsurgery understanding. Current approaches face two challenges: (i) static image\nlimitations including ambiguous local feature similarities and fine-grained\nstructural details, and (ii) dynamic video complexities arising from rapid\ninstrument motion and persistent visual occlusions. While existing methods\nmainly focus on spatial feature extraction, they fundamentally overlook\ntemporal dependencies in surgical video streams. To address this, we present\ntemporal asymmetric feature propagation network, a bidirectional attention\narchitecture enabling cross-frame feature propagation. The proposed method\ncontains a temporal query propagator that integrates multi-directional\nconsistency constraints to enhance frame-specific feature representation, and\nan aggregated asymmetric feature pyramid module that preserves discriminative\nfeatures for anatomical structures and surgical instruments. Our framework\nuniquely enables both temporal guidance and contextual reasoning for surgical\nscene understanding. Comprehensive evaluations on two public benchmarks show\nthe proposed method outperforms the current SOTA methods by a large margin,\nwith +16.4\\% mIoU on EndoVis2018 and +3.3\\% mAP on Endoscapes2023. The code\nwill be publicly available after paper acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5411\u6ce8\u610f\u529b\u67b6\u6784\u7684\u65f6\u5e8f\u975e\u5bf9\u79f0\u7279\u5f81\u4f20\u64ad\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u624b\u672f\u573a\u666f\u5206\u5272\u4e2d\u7684\u9759\u6001\u56fe\u50cf\u548c\u52a8\u6001\u89c6\u9891\u6311\u6218\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u624b\u672f\u573a\u666f\u5206\u5272\u5bf9\u673a\u5668\u4eba\u8f85\u52a9\u8179\u8154\u955c\u624b\u672f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u9759\u6001\u56fe\u50cf\u7684\u6a21\u7cca\u7279\u5f81\u548c\u52a8\u6001\u89c6\u9891\u7684\u590d\u6742\u53d8\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u65f6\u5e8f\u67e5\u8be2\u4f20\u64ad\u5668\u548c\u805a\u5408\u975e\u5bf9\u79f0\u7279\u5f81\u91d1\u5b57\u5854\u6a21\u5757\uff0c\u7ed3\u5408\u591a\u65b9\u5411\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u5224\u522b\u6027\u7279\u5f81\u4fdd\u7559\uff0c\u5b9e\u73b0\u65f6\u5e8f\u5f15\u5bfc\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cEndoVis2018\u4e0amIoU\u63d0\u534716.4%\uff0cEndoscapes2023\u4e0amAP\u63d0\u53473.3%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u5e8f\u7279\u5f81\u4f20\u64ad\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e86\u624b\u672f\u573a\u666f\u5206\u5272\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.13626", "pdf": "https://arxiv.org/pdf/2504.13626", "abs": "https://arxiv.org/abs/2504.13626", "authors": ["Yule Liu", "Jingyi Zheng", "Zhen Sun", "Zifan Peng", "Wenhan Dong", "Zeyang Sha", "Shiwen Cui", "Weiqiang Wang", "Xinlei He"], "title": "Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities in multiple tasks. However, LRMs typically suffer from\n\"overthinking\" problems, where models generate significantly redundant\nreasoning steps while bringing limited performance gains. Existing work relies\non fine-tuning to mitigate overthinking, which requires additional data,\nunconventional training setups, risky safety misalignment, and poor\ngeneralization.\n  Through empirical analysis, we reveal an important characteristic of LRM\nbehaviors that placing external CoTs generated by smaller models between the\nthinking token ($\\texttt{<think>}$ and $\\texttt{</think>)}$ can effectively\nmanipulate the model to generate fewer thoughts. Building on these insights, we\npropose a simple yet efficient pipeline, ThoughtMani, to enable LRMs to bypass\nunnecessary intermediate steps and reduce computational costs significantly. We\nconduct extensive experiments to validate the utility and efficiency of\nThoughtMani. For instance, when applied to QwQ-32B on the LiveBench/Code\ndataset, ThoughtMani keeps the original performance and reduces output token\ncounts by approximately 30%, with little overhead from the CoT generator.\nFurthermore, we find that ThoughtMani enhances safety alignment by an average\nof 10%. Since model vendors typically serve models of different sizes\nsimultaneously, ThoughtMani provides an effective way to construct more\nefficient and accessible LRMs for real-world applications.", "AI": {"tldr": "ThoughtMani\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u751f\u6210\u7684CoT\uff08Chain-of-Thought\uff09\u4e2d\u63d2\u5165\u7279\u5b9a\u6807\u8bb0\uff0c\u6709\u6548\u51cf\u5c11\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u5e76\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5b58\u5728\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u95ee\u9898\uff0c\u5373\u751f\u6210\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u4f46\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5fae\u8c03\uff0c\u4f46\u5b58\u5728\u6570\u636e\u9700\u6c42\u9ad8\u3001\u8bad\u7ec3\u590d\u6742\u3001\u5b89\u5168\u6027\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faThoughtMani\uff0c\u901a\u8fc7\u5728\u5c0f\u6a21\u578b\u751f\u6210\u7684CoT\u4e2d\u63d2\u5165<think>\u548c</think>\u6807\u8bb0\uff0c\u5f15\u5bfcLRMs\u8df3\u8fc7\u4e0d\u5fc5\u8981\u7684\u4e2d\u95f4\u6b65\u9aa4\u3002", "result": "\u5728QwQ-32B\u6a21\u578b\u4e0a\uff0cThoughtMani\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u7ea630%\u7684\u8f93\u51fa\u6807\u8bb0\uff0c\u5e76\u5e73\u5747\u63d0\u534710%\u7684\u5b89\u5168\u6027\u5bf9\u9f50\u3002", "conclusion": "ThoughtMani\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u53ef\u8bbf\u95ee\u7684LRMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13442", "pdf": "https://arxiv.org/pdf/2504.13442", "abs": "https://arxiv.org/abs/2504.13442", "authors": ["Zhenyu Yu", "Mohd. Yamani Idna Idris", "Pei Wang"], "title": "SatelliteCalculator: A Multi-Task Vision Foundation Model for Quantitative Remote Sensing Inversion", "categories": ["cs.CV"], "comment": null, "summary": "Quantitative remote sensing inversion plays a critical role in environmental\nmonitoring, enabling the estimation of key ecological variables such as\nvegetation indices, canopy structure, and carbon stock. Although vision\nfoundation models have achieved remarkable progress in classification and\nsegmentation tasks, their application to physically interpretable regression\nremains largely unexplored. Furthermore, the multi-spectral nature and\ngeospatial heterogeneity of remote sensing data pose significant challenges for\ngeneralization and transferability. To address these issues, we introduce\nSatelliteCalculator, the first vision foundation model tailored for\nquantitative remote sensing inversion. By leveraging physically defined index\nformulas, we automatically construct a large-scale dataset of over one million\npaired samples across eight core ecological indicators. The model integrates a\nfrozen Swin Transformer backbone with a prompt-guided architecture, featuring\ncross-attentive adapters and lightweight task-specific MLP decoders.\nExperiments on the Open-Canopy benchmark demonstrate that SatelliteCalculator\nachieves competitive accuracy across all tasks while significantly reducing\ninference cost. Our results validate the feasibility of applying foundation\nmodels to quantitative inversion, and provide a scalable framework for\ntask-adaptive remote sensing estimation.", "AI": {"tldr": "SatelliteCalculator\u662f\u9996\u4e2a\u4e3a\u5b9a\u91cf\u9065\u611f\u53cd\u95ee\u9898\u8bbe\u8ba1\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u5b9a\u4e49\u7684\u6307\u6570\u516c\u5f0f\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7ed3\u5408Swin Transformer\u548c\u63d0\u793a\u5f15\u5bfc\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u9002\u5e94\u6027\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u5206\u7c7b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7269\u7406\u53ef\u89e3\u91ca\u7684\u56de\u5f52\u4efb\u52a1\u4e2d\u5e94\u7528\u4e0d\u8db3\uff0c\u4e14\u9065\u611f\u6570\u636e\u7684\u591a\u5149\u8c31\u7279\u6027\u548c\u5730\u7406\u7a7a\u95f4\u5f02\u8d28\u6027\u5bf9\u6a21\u578b\u7684\u6cdb\u5316\u548c\u8fc1\u79fb\u80fd\u529b\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u5229\u7528\u7269\u7406\u5b9a\u4e49\u7684\u6307\u6570\u516c\u5f0f\u81ea\u52a8\u6784\u5efa\u5305\u542b\u516b\u4e2a\u6838\u5fc3\u751f\u6001\u6307\u6807\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u7528\u51bb\u7ed3\u7684Swin Transformer\u4e3b\u5e72\u548c\u63d0\u793a\u5f15\u5bfc\u67b6\u6784\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u9002\u914d\u5668\u548c\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9aMLP\u89e3\u7801\u5668\u3002", "result": "\u5728Open-Canopy\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSatelliteCalculator\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u7ade\u4e89\u6027\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u5728\u5b9a\u91cf\u53cd\u6f14\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u9065\u611f\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002"}}
{"id": "2504.13629", "pdf": "https://arxiv.org/pdf/2504.13629", "abs": "https://arxiv.org/abs/2504.13629", "authors": ["Cong William Lin", "Wu Zhu"], "title": "Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing", "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, are reshaping content creation\nand academic writing. This study investigates the impact of AI-assisted\ngenerative revisions on research manuscripts, focusing on heterogeneous\nadoption patterns and their influence on writing convergence. Leveraging a\ndataset of over 627,000 academic papers from arXiv, we develop a novel\nclassification framework by fine-tuning prompt- and discipline-specific large\nlanguage models to detect the style of ChatGPT-revised texts. Our findings\nreveal substantial disparities in LLM adoption across academic disciplines,\ngender, native language status, and career stage, alongside a rapid evolution\nin scholarly writing styles. Moreover, LLM usage enhances clarity, conciseness,\nand adherence to formal writing conventions, with improvements varying by\nrevision type. Finally, a difference-in-differences analysis shows that while\nLLMs drive convergence in academic writing, early adopters, male researchers,\nnon-native speakers, and junior scholars exhibit the most pronounced stylistic\nshifts, aligning their writing more closely with that of established\nresearchers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86AI\u8f85\u52a9\u751f\u6210\u4fee\u8ba2\u5bf9\u5b66\u672f\u8bba\u6587\u7684\u5f71\u54cd\uff0c\u53d1\u73b0ChatGPT\u7684\u4f7f\u7528\u5728\u4e0d\u540c\u5b66\u79d1\u3001\u6027\u522b\u3001\u6bcd\u8bed\u72b6\u6001\u548c\u804c\u4e1a\u9636\u6bb5\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63a8\u52a8\u4e86\u5b66\u672f\u5199\u4f5c\u98ce\u683c\u7684\u8d8b\u540c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e86\u89e3AI\u5de5\u5177\uff08\u5982ChatGPT\uff09\u5982\u4f55\u6539\u53d8\u5b66\u672f\u5199\u4f5c\uff0c\u5c24\u5176\u662f\u5176\u5bf9\u5199\u4f5c\u98ce\u683c\u548c\u5185\u5bb9\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u7279\u5b9a\u63d0\u793a\u548c\u5b66\u79d1\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9arXiv\u4e0a\u7684627,000\u591a\u7bc7\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u68c0\u6d4bChatGPT\u4fee\u8ba2\u7684\u6587\u672c\u98ce\u683c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cChatGPT\u7684\u4f7f\u7528\u63d0\u9ad8\u4e86\u5199\u4f5c\u7684\u6e05\u6670\u5ea6\u3001\u7b80\u6d01\u6027\u548c\u6b63\u5f0f\u6027\uff0c\u4e14\u4e0d\u540c\u4fee\u8ba2\u7c7b\u578b\u7684\u6548\u679c\u5404\u5f02\uff1b\u65e9\u671f\u91c7\u7528\u8005\u3001\u7537\u6027\u3001\u975e\u6bcd\u8bed\u8005\u548c\u521d\u7ea7\u5b66\u8005\u7684\u5199\u4f5c\u98ce\u683c\u53d8\u5316\u6700\u663e\u8457\u3002", "conclusion": "\u7ed3\u8bba\u662fChatGPT\u63a8\u52a8\u4e86\u5b66\u672f\u5199\u4f5c\u7684\u8d8b\u540c\uff0c\u4f46\u4e0d\u540c\u7fa4\u4f53\u7684\u91c7\u7eb3\u7a0b\u5ea6\u548c\u5f71\u54cd\u5b58\u5728\u5dee\u5f02\u3002"}}
{"id": "2504.13452", "pdf": "https://arxiv.org/pdf/2504.13452", "abs": "https://arxiv.org/abs/2504.13452", "authors": ["Juliette Bertrand", "Sophie Giffard-Roisin", "James Hollingsworth", "Julien Mairal"], "title": "MicroFlow: Domain-Specific Optical Flow for Ground Deformation Estimation in Seismic Events", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dense ground displacement measurements are crucial for geological studies but\nare impractical to collect directly. Traditionally, displacement fields are\nestimated using patch matching on optical satellite images from different\nacquisition times. While deep learning-based optical flow models are promising,\ntheir adoption in ground deformation analysis is hindered by challenges such as\nthe absence of real ground truth, the need for sub-pixel precision, and\ntemporal variations due to geological or anthropogenic changes. In particular,\nwe identify that deep learning models relying on explicit correlation layers\nstruggle at estimating small displacements in real-world conditions. Instead,\nwe propose a model that employs iterative refinements with explicit warping\nlayers and a correlation-independent backbone, enabling sub-pixel precision.\nAdditionally, a non-convex variant of Total Variation regularization preserves\nfault-line sharpness while maintaining smoothness elsewhere. Our model\nsignificantly outperforms widely used geophysics methods on semi-synthetic\nbenchmarks and generalizes well to challenging real-world scenarios captured by\nboth medium- and high-resolution sensors. Project page:\nhttps://jbertrand89.github.io/microflow/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u5730\u9762\u4f4d\u79fb\u6d4b\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4f30\u8ba1\u5c0f\u4f4d\u79fb\u7684\u56f0\u96be\u3002", "motivation": "\u4f20\u7edf\u7684\u5149\u5b66\u536b\u661f\u56fe\u50cf\u8865\u4e01\u5339\u914d\u65b9\u6cd5\u5728\u771f\u5b9e\u5730\u9762\u4f4d\u79fb\u6d4b\u91cf\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u56e0\u7f3a\u4e4f\u771f\u5b9e\u5730\u9762\u771f\u5b9e\u503c\u3001\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u9700\u6c42\u4ee5\u53ca\u5730\u8d28\u6216\u4eba\u4e3a\u53d8\u5316\u7684\u65f6\u95f4\u53d8\u5316\u800c\u96be\u4ee5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fed\u4ee3\u7ec6\u5316\u6a21\u578b\uff0c\u91c7\u7528\u663e\u5f0f\u53d8\u5f62\u5c42\u548c\u72ec\u7acb\u4e8e\u76f8\u5173\u6027\u7684\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u975e\u51f8\u603b\u53d8\u5dee\u6b63\u5219\u5316\uff0c\u4ee5\u5b9e\u73b0\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u5e76\u4fdd\u6301\u65ad\u5c42\u7ebf\u9510\u5ea6\u3002", "result": "\u6a21\u578b\u5728\u534a\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u5730\u7403\u7269\u7406\u65b9\u6cd5\uff0c\u5e76\u5728\u4e2d\u9ad8\u5206\u8fa8\u7387\u4f20\u611f\u5668\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5730\u9762\u4f4d\u79fb\u6d4b\u91cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u3002"}}
{"id": "2504.13630", "pdf": "https://arxiv.org/pdf/2504.13630", "abs": "https://arxiv.org/abs/2504.13630", "authors": ["Shaomu Tan", "Christof Monz"], "title": "Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "A key challenge in MT evaluation is the inherent noise and inconsistency of\nhuman ratings. Regression-based neural metrics struggle with this noise, while\nprompting LLMs shows promise at system-level evaluation but performs poorly at\nsegment level. In this work, we propose ReMedy, a novel MT metric framework\nthat reformulates translation evaluation as a reward modeling task. Instead of\nregressing on imperfect human ratings directly, ReMedy learns relative\ntranslation quality using pairwise preference data, resulting in a more\nreliable evaluation. In extensive experiments across WMT22-24 shared tasks (39\nlanguage pairs, 111 MT systems), ReMedy achieves state-of-the-art performance\nat both segment- and system-level evaluation. Specifically, ReMedy-9B surpasses\nlarger WMT winners and massive closed LLMs such as MetricX-13B,\nXCOMET-Ensemble, GEMBA-GPT-4, PaLM-540B, and finetuned PaLM2. Further analyses\ndemonstrate that ReMedy delivers superior capability in detecting translation\nerrors and evaluating low-quality translations.", "AI": {"tldr": "ReMedy\u662f\u4e00\u79cd\u65b0\u9896\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7ffb\u8bd1\u8bc4\u4f30\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5956\u52b1\u5efa\u6a21\u4efb\u52a1\uff0c\u5229\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u5b66\u4e60\u76f8\u5bf9\u7ffb\u8bd1\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u4eba\u7c7b\u8bc4\u5206\u7684\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u56de\u5f52\u578b\u795e\u7ecf\u6307\u6807\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u566a\u58f0\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\u4f46\u5728\u7247\u6bb5\u7ea7\u8bc4\u4f30\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "method": "ReMedy\u6846\u67b6\u901a\u8fc7\u5956\u52b1\u5efa\u6a21\u4efb\u52a1\u5b66\u4e60\u76f8\u5bf9\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4f7f\u7528\u6210\u5bf9\u504f\u597d\u6570\u636e\u800c\u975e\u76f4\u63a5\u56de\u5f52\u4e0d\u5b8c\u7f8e\u7684\u4eba\u7c7b\u8bc4\u5206\u3002", "result": "\u5728WMT22-24\u5171\u4eab\u4efb\u52a1\uff0839\u79cd\u8bed\u8a00\u5bf9\uff0c111\u4e2aMT\u7cfb\u7edf\uff09\u4e2d\uff0cReMedy\u5728\u7247\u6bb5\u7ea7\u548c\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8d85\u8d8a\u591a\u4e2a\u5927\u578b\u6a21\u578b\u3002", "conclusion": "ReMedy\u4e0d\u4ec5\u5728\u8bc4\u4f30\u6027\u80fd\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u7ffb\u8bd1\u9519\u8bef\u548c\u8bc4\u4f30\u4f4e\u8d28\u91cf\u7ffb\u8bd1\u3002"}}
{"id": "2504.13457", "pdf": "https://arxiv.org/pdf/2504.13457", "abs": "https://arxiv.org/abs/2504.13457", "authors": ["Haley M. So", "Gordon Wetzstein"], "title": "Neural Ganglion Sensors: Learning Task-specific Event Cameras Inspired by the Neural Circuit of the Human Retina", "categories": ["cs.CV", "cs.ET", "eess.IV"], "comment": null, "summary": "Inspired by the data-efficient spiking mechanism of neurons in the human eye,\nevent cameras were created to achieve high temporal resolution with minimal\npower and bandwidth requirements by emitting asynchronous, per-pixel intensity\nchanges rather than conventional fixed-frame rate images. Unlike retinal\nganglion cells (RGCs) in the human eye, however, which integrate signals from\nmultiple photoreceptors within a receptive field to extract spatio-temporal\nfeatures, conventional event cameras do not leverage local spatial context when\ndeciding which events to fire. Moreover, the eye contains around 20 different\nkinds of RGCs operating in parallel, each attuned to different features or\nconditions. Inspired by this biological design, we introduce Neural Ganglion\nSensors, an extension of traditional event cameras that learns task-specific\nspatio-temporal retinal kernels (i.e., RGC \"events\"). We evaluate our design on\ntwo challenging tasks: video interpolation and optical flow. Our results\ndemonstrate that our biologically inspired sensing improves performance\nrelative to conventional event cameras while reducing overall event bandwidth.\nThese findings highlight the promise of RGC-inspired event sensors for edge\ndevices and other low-power, real-time applications requiring efficient,\nhigh-resolution visual streams.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u751f\u7269\u89c6\u7f51\u819c\u795e\u7ecf\u8282\u7ec6\u80de\u542f\u53d1\u7684\u795e\u7ecf\u8282\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u7684\u65f6\u7a7a\u89c6\u7f51\u819c\u6838\u6539\u8fdb\u4f20\u7edf\u4e8b\u4ef6\u76f8\u673a\uff0c\u5728\u89c6\u9891\u63d2\u503c\u548c\u5149\u6d41\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u4e14\u5e26\u5bbd\u66f4\u4f4e\u3002", "motivation": "\u53d7\u4eba\u7c7b\u773c\u775b\u4e2d\u795e\u7ecf\u5143\u6570\u636e\u9ad8\u6548\u673a\u5236\u7684\u542f\u53d1\uff0c\u4f20\u7edf\u4e8b\u4ef6\u76f8\u673a\u7f3a\u4e4f\u5c40\u90e8\u7a7a\u95f4\u4e0a\u4e0b\u6587\u5229\u7528\u548c\u591a\u7279\u5f81\u5e76\u884c\u5904\u7406\u80fd\u529b\uff0c\u8bba\u6587\u65e8\u5728\u6539\u8fdb\u8fd9\u4e00\u70b9\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u8282\u4f20\u611f\u5668\uff0c\u5b66\u4e60\u4efb\u52a1\u7279\u5b9a\u7684\u65f6\u7a7a\u89c6\u7f51\u819c\u6838\uff08\u7c7b\u4f3cRGC\u4e8b\u4ef6\uff09\uff0c\u6269\u5c55\u4f20\u7edf\u4e8b\u4ef6\u76f8\u673a\u529f\u80fd\u3002", "result": "\u5728\u89c6\u9891\u63d2\u503c\u548c\u5149\u6d41\u4efb\u52a1\u4e2d\uff0c\u751f\u7269\u542f\u53d1\u7684\u4f20\u611f\u5668\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u4e8b\u4ef6\u76f8\u673a\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u4e8b\u4ef6\u5e26\u5bbd\u3002", "conclusion": "RGC\u542f\u53d1\u7684\u4f20\u611f\u5668\u5728\u8fb9\u7f18\u8bbe\u5907\u7b49\u4f4e\u529f\u8017\u5b9e\u65f6\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u63d0\u4f9b\u9ad8\u6548\u9ad8\u5206\u8fa8\u7387\u7684\u89c6\u89c9\u6d41\u3002"}}
{"id": "2504.13643", "pdf": "https://arxiv.org/pdf/2504.13643", "abs": "https://arxiv.org/abs/2504.13643", "authors": ["Tao He", "Lizi Liao", "Ming Liu", "Bing Qin"], "title": "Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning", "categories": ["cs.CL"], "comment": "11 pages, 6 figures, SIGIR 2025", "summary": "Recent advancements in dialogue policy planning have emphasized optimizing\nsystem agent policies to achieve predefined goals, focusing on strategy design,\ntrajectory acquisition, and efficient training paradigms. However, these\napproaches often overlook the critical role of user characteristics, which are\nessential in real-world scenarios like conversational search and\nrecommendation, where interactions must adapt to individual user traits such as\npersonality, preferences, and goals. To address this gap, we first conduct a\ncomprehensive study utilizing task-specific user personas to systematically\nassess dialogue policy planning under diverse user behaviors. By leveraging\nrealistic user profiles for different tasks, our study reveals significant\nlimitations in existing approaches, highlighting the need for user-tailored\ndialogue policy planning. Building on this foundation, we present the\nUser-Tailored Dialogue Policy Planning (UDP) framework, which incorporates an\nIntrinsic User World Model to model user traits and feedback. UDP operates in\nthree stages: (1) User Persona Portraying, using a diffusion model to\ndynamically infer user profiles; (2) User Feedback Anticipating, leveraging a\nBrownian Bridge-inspired anticipator to predict user reactions; and (3)\nUser-Tailored Policy Planning, integrating these insights to optimize response\nstrategies. To ensure robust performance, we further propose an active learning\napproach that prioritizes challenging user personas during training.\nComprehensive experiments on benchmarks, including collaborative and\nnon-collaborative settings, demonstrate the effectiveness of UDP in learning\nuser-specific dialogue strategies. Results validate the protocol's utility and\nhighlight UDP's robustness, adaptability, and potential to advance user-centric\ndialogue systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5b9a\u5236\u7684\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\u6846\u67b6\uff08UDP\uff09\uff0c\u901a\u8fc7\u5efa\u6a21\u7528\u6237\u7279\u5f81\u548c\u53cd\u9988\uff0c\u4f18\u5316\u5bf9\u8bdd\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7528\u6237\u7279\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\u591a\u5173\u6ce8\u7cfb\u7edf\u4ee3\u7406\u7b56\u7565\u4f18\u5316\uff0c\u4f46\u5ffd\u7565\u4e86\u7528\u6237\u7279\u6027\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982\u5bf9\u8bdd\u641c\u7d22\u548c\u63a8\u8350\uff09\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "UDP\u6846\u67b6\u5206\u4e09\u9636\u6bb5\uff1a\u7528\u6237\u753b\u50cf\u52a8\u6001\u63a8\u65ad\u3001\u7528\u6237\u53cd\u9988\u9884\u6d4b\u3001\u7528\u6237\u5b9a\u5236\u7b56\u7565\u89c4\u5212\uff0c\u5e76\u7ed3\u5408\u4e3b\u52a8\u5b66\u4e60\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eUDP\u5728\u534f\u4f5c\u548c\u975e\u534f\u4f5c\u573a\u666f\u4e0b\u5747\u80fd\u6709\u6548\u5b66\u4e60\u7528\u6237\u7279\u5b9a\u5bf9\u8bdd\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "UDP\u6846\u67b6\u4e3a\u63a8\u8fdb\u7528\u6237\u4e2d\u5fc3\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13458", "pdf": "https://arxiv.org/pdf/2504.13458", "abs": "https://arxiv.org/abs/2504.13458", "authors": ["Wang Liu", "Zhiyu Wang", "Xin Guo", "Puhong Duan", "Xudong Kang", "Shutao Li"], "title": "Learning from Noisy Pseudo-labels for All-Weather Land Cover Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation of SAR images has garnered significant attention in\nremote sensing due to the immunity of SAR sensors to cloudy weather and light\nconditions. Nevertheless, SAR imagery lacks detailed information and is plagued\nby significant speckle noise, rendering the annotation or segmentation of SAR\nimages a formidable task. Recent efforts have resorted to annotating paired\noptical-SAR images to generate pseudo-labels through the utilization of an\noptical image segmentation network. However, these pseudo-labels are laden with\nnoise, leading to suboptimal performance in SAR image segmentation. In this\nstudy, we introduce a more precise method for generating pseudo-labels by\nincorporating semi-supervised learning alongside a novel image resolution\nalignment augmentation. Furthermore, we introduce a symmetric cross-entropy\nloss to mitigate the impact of noisy pseudo-labels. Additionally, a bag of\ntraining and testing tricks is utilized to generate better land-cover mapping\nresults. Our experiments on the GRSS data fusion contest indicate the\neffectiveness of the proposed method, which achieves first place. The code is\navailable at https://github.com/StuLiu/DFC2025Track1.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9\u9f50\u589e\u5f3a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u66f4\u7cbe\u786e\u7684SAR\u56fe\u50cf\u4f2a\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u5bf9\u79f0\u4ea4\u53c9\u71b5\u635f\u5931\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u6700\u7ec8\u5728GRSS\u6570\u636e\u878d\u5408\u7ade\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "SAR\u56fe\u50cf\u7f3a\u4e4f\u7ec6\u8282\u4fe1\u606f\u4e14\u5b58\u5728\u663e\u8457\u6591\u70b9\u566a\u58f0\uff0c\u4f20\u7edf\u57fa\u4e8e\u5149\u5b66\u56fe\u50cf\u751f\u6210\u7684\u4f2a\u6807\u7b7e\u566a\u58f0\u5927\uff0c\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u50cf\u5206\u8fa8\u7387\u5bf9\u9f50\u589e\u5f3a\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4f7f\u7528\u5bf9\u79f0\u4ea4\u53c9\u71b5\u635f\u5931\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u91c7\u7528\u591a\u79cd\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6280\u5de7\u3002", "result": "\u5728GRSS\u6570\u636e\u878d\u5408\u7ade\u8d5b\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86SAR\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13653", "pdf": "https://arxiv.org/pdf/2504.13653", "abs": "https://arxiv.org/abs/2504.13653", "authors": ["Hesham Abdelmotaleb", "Craig McNeile", "Malgorzata Wojtys"], "title": "Word Embedding Techniques for Classification of Star Ratings", "categories": ["cs.CL", "stat.AP", "62P99"], "comment": "40 pages", "summary": "Telecom services are at the core of today's societies' everyday needs. The\navailability of numerous online forums and discussion platforms enables telecom\nproviders to improve their services by exploring the views of their customers\nto learn about common issues that the customers face. Natural Language\nProcessing (NLP) tools can be used to process the free text collected.\n  One way of working with such data is to represent text as numerical vectors\nusing one of many word embedding models based on neural networks. This research\nuses a novel dataset of telecom customers' reviews to perform an extensive\nstudy showing how different word embedding algorithms can affect the text\nclassification process. Several state-of-the-art word embedding techniques are\nconsidered, including BERT, Word2Vec and Doc2Vec, coupled with several\nclassification algorithms. The important issue of feature engineering and\ndimensionality reduction is addressed and several PCA-based approaches are\nexplored. Moreover, the energy consumption used by the different word\nembeddings is investigated. The findings show that some word embedding models\ncan lead to consistently better text classifiers in terms of precision, recall\nand F1-Score. In particular, for the more challenging classification tasks,\nBERT combined with PCA stood out with the highest performance metrics.\nMoreover, our proposed PCA approach of combining word vectors using the first\nprincipal component shows clear advantages in performance over the traditional\napproach of taking the average.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u8bcd\u5d4c\u5165\u6a21\u578b\uff08\u5982BERT\u3001Word2Vec\u3001Doc2Vec\uff09\u5bf9\u7535\u4fe1\u5ba2\u6237\u8bc4\u8bba\u6587\u672c\u5206\u7c7b\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u591a\u79cd\u5206\u7c7b\u7b97\u6cd5\u548cPCA\u964d\u7ef4\u65b9\u6cd5\uff0c\u53d1\u73b0BERT\u7ed3\u5408PCA\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7535\u4fe1\u670d\u52a1\u5bf9\u73b0\u4ee3\u793e\u4f1a\u81f3\u5173\u91cd\u8981\uff0c\u901a\u8fc7\u5206\u6790\u5ba2\u6237\u53cd\u9988\u53ef\u4ee5\u6539\u8fdb\u670d\u52a1\u3002NLP\u5de5\u5177\u80fd\u5904\u7406\u6587\u672c\u6570\u636e\uff0c\u4f46\u4e0d\u540c\u8bcd\u5d4c\u5165\u6a21\u578b\u5bf9\u5206\u7c7b\u6548\u679c\u7684\u5f71\u54cd\u5c1a\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u7535\u4fe1\u5ba2\u6237\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u591a\u79cd\u8bcd\u5d4c\u5165\u6a21\u578b\uff08BERT\u3001Word2Vec\u3001Doc2Vec\uff09\u548c\u5206\u7c7b\u7b97\u6cd5\uff0c\u7ed3\u5408PCA\u964d\u7ef4\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u80fd\u8017\u95ee\u9898\u3002", "result": "BERT\u7ed3\u5408PCA\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\uff1b\u63d0\u51fa\u7684\u57fa\u4e8e\u7b2c\u4e00\u4e3b\u6210\u5206\u7684\u8bcd\u5411\u91cf\u7ec4\u5408\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u5e73\u5747\u503c\u65b9\u6cd5\u3002", "conclusion": "\u8bcd\u5d4c\u5165\u6a21\u578b\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6587\u672c\u5206\u7c7b\u6027\u80fd\uff0cBERT\u7ed3\u5408PCA\u662f\u9ad8\u6548\u4e14\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13460", "pdf": "https://arxiv.org/pdf/2504.13460", "abs": "https://arxiv.org/abs/2504.13460", "authors": ["Hongwei Ji", "Wulian Yun", "Mengshi Qi", "Huadong Ma"], "title": "Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional temporal action localization (TAL) methods rely on large amounts\nof detailed annotated data, whereas few-shot TAL reduces this dependence by\nusing only a few training samples to identify unseen action categories.\nHowever, existing few-shot TAL methods typically focus solely on video-level\ninformation, neglecting textual information, which can provide valuable\nsemantic support for the localization task. Therefore, we propose a new\nfew-shot temporal action localization method by Chain-of-Thought textual\nreasoning to improve localization performance. Specifically, we design a novel\nfew-shot learning framework that leverages textual semantic information to\nenhance the model's ability to capture action commonalities and variations,\nwhich includes a semantic-aware text-visual alignment module designed to align\nthe query and support videos at different levels. Meanwhile, to better express\nthe temporal dependencies and causal relationships between actions at the\ntextual level to assist action localization, we design a Chain of Thought\n(CoT)-like reasoning method that progressively guides the Vision Language Model\n(VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for\nvideos. The generated texts can capture more variance of action than visual\nfeatures. We conduct extensive experiments on the publicly available\nActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named\nHuman-related Anomaly Localization and explore the application of the TAL task\nin human anomaly detection. The experimental results demonstrate that our\nproposed method significantly outperforms existing methods in single-instance\nand multi-instance scenarios. We will release our code, data and benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eChain-of-Thought\u6587\u672c\u63a8\u7406\u7684\u5c11\u6837\u672c\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6587\u672c\u8bed\u4e49\u4fe1\u606f\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5c11\u6837\u672c\u65f6\u5e8f\u52a8\u4f5c\u5b9a\u4f4d\u65b9\u6cd5\u4ec5\u5173\u6ce8\u89c6\u9891\u7ea7\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u4fe1\u606f\u7684\u8bed\u4e49\u652f\u6301\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u611f\u77e5\u7684\u6587\u672c-\u89c6\u89c9\u5bf9\u9f50\u6a21\u5757\u548cChain-of-Thought\u63a8\u7406\u65b9\u6cd5\uff0c\u5229\u7528VLM\u548cLLM\u751f\u6210\u6587\u672c\u63cf\u8ff0\u3002", "result": "\u5728ActivityNet1.3\u548cTHUMOS14\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63a2\u7d22\u4e86\u4eba\u7c7b\u5f02\u5e38\u68c0\u6d4b\u7684\u5e94\u7528\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u3001\u6570\u636e\u548c\u57fa\u51c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.13655", "pdf": "https://arxiv.org/pdf/2504.13655", "abs": "https://arxiv.org/abs/2504.13655", "authors": ["Jie Zou", "Cheng Lin", "Weikang Guo", "Zheng Wang", "Jiwei Wei", "Yang Yang", "Hengtao Shen"], "title": "Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "30 pages", "summary": "Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines.", "AI": {"tldr": "MCCRS\u662f\u4e00\u79cd\u591a\u7c7b\u578b\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u878d\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u7f3a\u4e4f\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u7ed3\u5408\u591a\u79cd\u4e0a\u4e0b\u6587\u7c7b\u578b\u3002", "method": "MCCRS\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u56fe\u8c31\u3001\u975e\u7ed3\u6784\u5316\u5bf9\u8bdd\u5386\u53f2\u548c\u5546\u54c1\u8bc4\u8bba\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u5bb6\u6a21\u578b\u548cChairBot\u534f\u8c03\u751f\u6210\u63a8\u8350\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCCRS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MCCRS\u901a\u8fc7\u878d\u5408\u591a\u7c7b\u578b\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u4e13\u5bb6\u534f\u540c\uff0c\u7a81\u7834\u4e86\u5355\u4e00\u4e0a\u4e0b\u6587\u7684\u9650\u5236\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u6027\u80fd\u3002"}}
{"id": "2504.13469", "pdf": "https://arxiv.org/pdf/2504.13469", "abs": "https://arxiv.org/abs/2504.13469", "authors": ["YangChen Zeng"], "title": "HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Current Transformer-based methods for small object detection continue\nemerging, yet they have still exhibited significant shortcomings. This paper\nintroduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization\ntechnique that enhances object detection performance by dynamically integrating\npositional encoding with semantic detection information through heatmap-guided\nadaptive learning.We also innovatively visualize the HMPE method, offering\nclear visualization of embedded information for parameter fine-tuning.We then\ncreate Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced\nHigh-Quality Queries for Decoder (HIDQ) modules. These are designed for the\nencoder and decoder, respectively, to generate high-quality queries and reduce\nbackground noise queries.Using both heatmap embedding and Linear-Snake\nConv(LSConv) feature engineering, we enhance the embedding of massively diverse\nsmall object categories and reduced the decoder multihead layers, thereby\naccelerating both inference and training.In the generalization experiments, our\napproach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU\nVHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing\nHMPE-enhanced embedding, we are able to reduce the number of decoder layers\nfrom eight to a minimum of three, significantly decreasing both inference and\ntraining costs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeatMap Position Embedding (HMPE)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6574\u5408\u4f4d\u7f6e\u7f16\u7801\u4e0e\u8bed\u4e49\u68c0\u6d4b\u4fe1\u606f\uff0c\u63d0\u5347\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86MOHFE\u548cHIDQ\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u4ee5\u51cf\u5c11\u80cc\u666f\u566a\u58f0\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u67e5\u8be2\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u6574\u5408\u4f4d\u7f6e\u4fe1\u606f\u4e0e\u8bed\u4e49\u68c0\u6d4b\u4fe1\u606f\u7684\u6280\u672f\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faHMPE\u65b9\u6cd5\uff0c\u7ed3\u5408\u70ed\u56fe\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u5b66\u4e60\uff1b\u8bbe\u8ba1MOHFE\u548cHIDQ\u6a21\u5757\u4f18\u5316\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff1b\u4f7f\u7528LSConv\u7279\u5f81\u5de5\u7a0b\u589e\u5f3a\u5c0f\u76ee\u6807\u7c7b\u522b\u5d4c\u5165\u3002", "result": "\u5728NWPU VHR-10\u6570\u636e\u96c6\u4e0amAP\u63d0\u53471.9%\uff0c\u5728PASCAL VOC\u6570\u636e\u96c6\u4e0a\u63d0\u53471.2%\uff1b\u89e3\u7801\u5668\u5c42\u6570\u4ece8\u5c42\u51cf\u5c11\u81f33\u5c42\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "HMPE\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13677", "pdf": "https://arxiv.org/pdf/2504.13677", "abs": "https://arxiv.org/abs/2504.13677", "authors": ["Andrea Santilli", "Adam Golinski", "Michael Kirchhof", "Federico Danieli", "Arno Blaas", "Miao Xiong", "Luca Zappella", "Sinead Williamson"], "title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for\nimproving their safety and reliability. Evaluations often use performance\nmetrics like AUROC to assess how well UQ methods (e.g., negative sequence\nprobabilities) correlate with task correctness functions (e.g., ROUGE-L). In\nthis paper, we show that commonly used correctness functions bias UQ\nevaluations by inflating the performance of certain UQ methods. We evaluate 7\ncorrectness functions -- from lexical-based and embedding-based metrics to\nLLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our\nanalysis reveals that length biases in the errors of these correctness\nfunctions distort UQ assessments by interacting with length biases in UQ\nmethods. We identify LLM-as-a-judge approaches as among the least length-biased\nchoices and hence a potential solution to mitigate these biases.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u8bc4\u4f30\u4e2d\u5e38\u7528\u7684\u6b63\u786e\u6027\u51fd\u6570\u5b58\u5728\u957f\u5ea6\u504f\u5dee\uff0c\u5bfc\u81f4\u67d0\u4e9bUQ\u65b9\u6cd5\u8868\u73b0\u88ab\u9ad8\u4f30\u3002\u901a\u8fc7\u5206\u6790\u591a\u79cd\u6b63\u786e\u6027\u51fd\u6570\uff0c\u53d1\u73b0LLM-as-a-judge\u65b9\u6cd5\u504f\u5dee\u8f83\u5c0f\uff0c\u53ef\u80fd\u662f\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\u5bf9\u63d0\u5347\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e2d\u7684\u6b63\u786e\u6027\u51fd\u6570\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u8bc4\u4f30\u7ed3\u679c\u3002", "method": "\u8bc4\u4f30\u4e867\u79cd\u6b63\u786e\u6027\u51fd\u6570\uff08\u5305\u62ec\u57fa\u4e8e\u8bcd\u6c47\u3001\u5d4c\u5165\u548cLLM-as-a-judge\u7684\u65b9\u6cd5\uff09\u57284\u4e2a\u6570\u636e\u96c6\u30014\u4e2a\u6a21\u578b\u548c6\u79cdUQ\u65b9\u6cd5\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6b63\u786e\u6027\u51fd\u6570\u7684\u957f\u5ea6\u504f\u5dee\u4e0eUQ\u65b9\u6cd5\u7684\u957f\u5ea6\u504f\u5dee\u76f8\u4e92\u4f5c\u7528\uff0c\u626d\u66f2\u4e86\u8bc4\u4f30\u7ed3\u679c\uff0c\u5176\u4e2dLLM-as-a-judge\u65b9\u6cd5\u504f\u5dee\u6700\u5c0f\u3002", "conclusion": "LLM-as-a-judge\u65b9\u6cd5\u53ef\u80fd\u662f\u51cf\u5c11UQ\u8bc4\u4f30\u4e2d\u957f\u5ea6\u504f\u5dee\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13490", "pdf": "https://arxiv.org/pdf/2504.13490", "abs": "https://arxiv.org/abs/2504.13490", "authors": ["Joowon Kim", "Ziseok Lee", "Donghyeon Cho", "Sanghyun Jo", "Yeonsung Jung", "Kyungsu Kim", "Eunho Yang"], "title": "Early Timestep Zero-Shot Candidate Selection for Instruction-Guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in diffusion models, achieving reliable image\ngeneration and editing remains challenging due to the inherent diversity\ninduced by stochastic noise in the sampling process. Instruction-guided image\nediting with diffusion models offers user-friendly capabilities, yet editing\nfailures, such as background distortion, frequently occur. Users often resort\nto trial and error, adjusting seeds or prompts to achieve satisfactory results,\nwhich is inefficient. While seed selection methods exist for Text-to-Image\n(T2I) generation, they depend on external verifiers, limiting applicability,\nand evaluating multiple seeds increases computational complexity. To address\nthis, we first establish a multiple-seed-based image editing baseline using\nbackground consistency scores, achieving Best-of-N performance without\nsupervision. Building on this, we introduce ELECT (Early-timestep Latent\nEvaluation for Candidate Selection), a zero-shot framework that selects\nreliable seeds by estimating background mismatches at early diffusion\ntimesteps, identifying the seed that retains the background while modifying\nonly the foreground. ELECT ranks seed candidates by a background inconsistency\nscore, filtering unsuitable samples early based on background consistency while\npreserving editability. Beyond standalone seed selection, ELECT integrates into\ninstruction-guided editing pipelines and extends to Multimodal Large-Language\nModels (MLLMs) for joint seed and prompt selection, further improving results\nwhen seed selection alone is insufficient. Experiments show that ELECT reduces\ncomputational costs (by 41 percent on average and up to 61 percent) while\nimproving background consistency and instruction adherence, achieving around 40\npercent success rates in previously failed cases - without any external\nsupervision or training.", "AI": {"tldr": "ELECT\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u65e9\u671f\u6269\u6563\u6b65\u9aa4\u8bc4\u4f30\u6f5c\u5728\u5019\u9009\u79cd\u5b50\uff0c\u9009\u62e9\u53ef\u9760\u79cd\u5b50\u4ee5\u6539\u5584\u56fe\u50cf\u7f16\u8f91\u7684\u80cc\u666f\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4e2d\u56e0\u968f\u673a\u566a\u58f0\u5bfc\u81f4\u591a\u6837\u6027\uff0c\u7528\u6237\u9700\u53cd\u590d\u8c03\u6574\u79cd\u5b50\u6216\u63d0\u793a\uff0c\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u79cd\u5b50\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u9a8c\u8bc1\u5668\u4e14\u8ba1\u7b97\u590d\u6742\u3002", "method": "\u63d0\u51faELECT\u6846\u67b6\uff0c\u901a\u8fc7\u65e9\u671f\u65f6\u95f4\u6b65\u8bc4\u4f30\u6f5c\u5728\u5019\u9009\u79cd\u5b50\u7684\u80cc\u666f\u4e0d\u4e00\u81f4\u6027\u5206\u6570\uff0c\u7b5b\u9009\u4fdd\u7559\u80cc\u666f\u5e76\u4fee\u6539\u524d\u666f\u7684\u79cd\u5b50\u3002", "result": "ELECT\u5e73\u5747\u964d\u4f4e41%\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u80cc\u666f\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u9075\u5faa\uff0c\u572840%\u7684\u5931\u8d25\u6848\u4f8b\u4e2d\u6210\u529f\u3002", "conclusion": "ELECT\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2504.13685", "pdf": "https://arxiv.org/pdf/2504.13685", "abs": "https://arxiv.org/abs/2504.13685", "authors": ["Stefano M. Iacus", "Haodong Qi", "Jiyoung Han"], "title": "Deep literature reviews: an application of fine-tuned language models to migration research", "categories": ["cs.CL", "cs.LG", "stat.AP", "stat.CO"], "comment": null, "summary": "This paper presents a hybrid framework for literature reviews that augments\ntraditional bibliometric methods with large language models (LLMs). By\nfine-tuning open-source LLMs, our approach enables scalable extraction of\nqualitative insights from large volumes of research content, enhancing both the\nbreadth and depth of knowledge synthesis. To improve annotation efficiency and\nconsistency, we introduce an error-focused validation process in which LLMs\ngenerate initial labels and human reviewers correct misclassifications.\nApplying this framework to over 20000 scientific articles about human\nmigration, we demonstrate that a domain-adapted LLM can serve as a \"specialist\"\nmodel - capable of accurately selecting relevant studies, detecting emerging\ntrends, and identifying critical research gaps. Notably, the LLM-assisted\nreview reveals a growing scholarly interest in climate-induced migration.\nHowever, existing literature disproportionately centers on a narrow set of\nenvironmental hazards (e.g., floods, droughts, sea-level rise, and land\ndegradation), while overlooking others that more directly affect human health\nand well-being, such as air and water pollution or infectious diseases. This\nimbalance highlights the need for more comprehensive research that goes beyond\nphysical environmental changes to examine their ecological and societal\nconsequences, particularly in shaping migration as an adaptive response.\nOverall, our proposed framework demonstrates the potential of fine-tuned LLMs\nto conduct more efficient, consistent, and insightful literature reviews across\ndisciplines, ultimately accelerating knowledge synthesis and scientific\ndiscovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u6587\u732e\u8ba1\u91cf\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u4e00\u81f4\u4e14\u6df1\u5165\u7684\u6587\u732e\u7efc\u8ff0\u3002", "motivation": "\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7814\u7a76\u5185\u5bb9\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u9a8c\u8bc1\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90LLM\uff0c\u7ed3\u5408\u9519\u8bef\u805a\u7126\u7684\u9a8c\u8bc1\u6d41\u7a0b\uff08LLM\u751f\u6210\u521d\u59cb\u6807\u7b7e\uff0c\u4eba\u5de5\u7ea0\u6b63\u9519\u8bef\uff09\uff0c\u5e94\u7528\u4e8e20000\u591a\u7bc7\u5173\u4e8e\u4eba\u7c7b\u8fc1\u79fb\u7684\u79d1\u5b66\u6587\u7ae0\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u9886\u57df\u9002\u5e94\u7684LLM\u80fd\u51c6\u786e\u7b5b\u9009\u76f8\u5173\u7814\u7a76\u3001\u68c0\u6d4b\u65b0\u5174\u8d8b\u52bf\u5e76\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6c14\u5019\u5f15\u53d1\u8fc1\u79fb\u7684\u7814\u7a76\u4e0d\u5e73\u8861\u73b0\u8c61\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5fae\u8c03LLM\u5728\u591a\u5b66\u79d1\u6587\u732e\u7efc\u8ff0\u4e2d\u7684\u6f5c\u529b\uff0c\u53ef\u52a0\u901f\u77e5\u8bc6\u5408\u6210\u548c\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2504.13499", "pdf": "https://arxiv.org/pdf/2504.13499", "abs": "https://arxiv.org/abs/2504.13499", "authors": ["Alex Ergasti", "Filippo Botti", "Tomaso Fontanini", "Claudio Ferrari", "Massimo Bertozzi", "Andrea Prati"], "title": "U-Shape Mamba: State Space Model for faster diffusion", "categories": ["cs.CV"], "comment": "Accepeted at CVPR 2025 eLVM workshop", "summary": "Diffusion models have become the most popular approach for high-quality image\ngeneration, but their high computational cost still remains a significant\nchallenge. To address this problem, we propose U-Shape Mamba (USM), a novel\ndiffusion model that leverages Mamba-based layers within a U-Net-like\nhierarchical structure. By progressively reducing sequence length in the\nencoder and restoring it in the decoder through Mamba blocks, USM significantly\nlowers computational overhead while maintaining strong generative capabilities.\nExperimental results against Zigma, which is currently the most efficient\nMamba-based diffusion model, demonstrate that USM achieves one-third the\nGFlops, requires less memory and is faster, while outperforming Zigma in image\nquality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7\npoints on AFHQ, CelebAHQ and COCO datasets, respectively. These findings\nhighlight USM as a highly efficient and scalable solution for diffusion-based\ngenerative models, making high-quality image synthesis more accessible to the\nresearch community while reducing computational costs.", "AI": {"tldr": "USM\u662f\u4e00\u79cd\u65b0\u578b\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408Mamba\u5757\u548cU-Net\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002USM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528Mamba\u5757\u5728U-Net\u7ed3\u6784\u4e2d\u9010\u6b65\u51cf\u5c11\u548c\u6062\u590d\u5e8f\u5217\u957f\u5ea6\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "USM\u7684\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3aZigma\u7684\u4e09\u5206\u4e4b\u4e00\uff0c\u5185\u5b58\u9700\u6c42\u66f4\u4f4e\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u56fe\u50cf\u8d28\u91cf\u66f4\u9ad8\uff08FID\u63d0\u5347\u663e\u8457\uff09\u3002", "conclusion": "USM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6269\u6563\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u9ad8\u8d28\u91cf\u56fe\u50cf\u5408\u6210\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2504.13730", "pdf": "https://arxiv.org/pdf/2504.13730", "abs": "https://arxiv.org/abs/2504.13730", "authors": ["Paul K. Mandal", "Cole Leo", "Connor Hurley"], "title": "Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; I.2.6; I.2.8; H.3.1; K.4.1"], "comment": "7 pages, 1 figure, 1 table", "summary": "Open-source intelligence provides a stream of unstructured textual data that\ncan inform assessments of territorial control. We present CONTACT, a framework\nfor territorial control prediction using large language models (LLMs) and\nminimal supervision. We evaluate two approaches: SetFit, an embedding-based\nfew-shot classifier, and a prompt tuning method applied to BLOOMZ-560m, a\nmultilingual generative LLM. Our model is trained on a small hand-labeled\ndataset of news articles covering ISIS activity in Syria and Iraq, using\nprompt-conditioned extraction of control-relevant signals such as military\noperations, casualties, and location references. We show that the BLOOMZ-based\nmodel outperforms the SetFit baseline, and that prompt-based supervision\nimproves generalization in low-resource settings. CONTACT demonstrates that\nLLMs fine-tuned using few-shot methods can reduce annotation burdens and\nsupport structured inference from open-ended OSINT streams. Our code is\navailable at https://github.com/PaulKMandal/CONTACT/.", "AI": {"tldr": "CONTACT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c11\u91cf\u76d1\u7763\u7684\u9886\u571f\u63a7\u5236\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5f00\u6e90\u60c5\u62a5\uff08OSINT\uff09\u7684\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\uff0c\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u5e76\u652f\u6301\u4ece\u5f00\u653e\u6570\u636e\u6d41\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1aSetFit\uff08\u57fa\u4e8e\u5d4c\u5165\u7684\u5c0f\u6837\u672c\u5206\u7c7b\u5668\uff09\u548c\u57fa\u4e8eBLOOMZ-560m\u7684\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002", "result": "BLOOMZ\u6a21\u578b\u8868\u73b0\u4f18\u4e8eSetFit\u57fa\u7ebf\uff0c\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CONTACT\u5c55\u793a\u4e86\u5c0f\u6837\u672c\u8c03\u4f18\u7684LLMs\u5728\u51cf\u5c11\u6807\u6ce8\u8d1f\u62c5\u548c\u652f\u6301OSINT\u7ed3\u6784\u5316\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13524", "pdf": "https://arxiv.org/pdf/2504.13524", "abs": "https://arxiv.org/abs/2504.13524", "authors": ["Jinhao Li", "Zijian Chen", "Tingzhu Chen", "Zhiji Liu", "Changbo Wang"], "title": "OBIFormer: A Fast Attentive Denoising Framework for Oracle Bone Inscriptions", "categories": ["cs.CV"], "comment": null, "summary": "Oracle bone inscriptions (OBIs) are the earliest known form of Chinese\ncharacters and serve as a valuable resource for research in anthropology and\narchaeology. However, most excavated fragments are severely degraded due to\nthousands of years of natural weathering, corrosion, and man-made destruction,\nmaking automatic OBI recognition extremely challenging. Previous methods either\nfocus on pixel-level information or utilize vanilla transformers for\nglyph-based OBI denoising, which leads to tremendous computational overhead.\nTherefore, this paper proposes a fast attentive denoising framework for oracle\nbone inscriptions, i.e., OBIFormer. It leverages channel-wise self-attention,\nglyph extraction, and selective kernel feature fusion to reconstruct denoised\nimages precisely while being computationally efficient. Our OBIFormer achieves\nstate-of-the-art denoising performance for PSNR and SSIM metrics on synthetic\nand original OBI datasets. Furthermore, comprehensive experiments on a real\noracle dataset demonstrate the great potential of our OBIFormer in assisting\nautomatic OBI recognition. The code will be made available at\nhttps://github.com/LJHolyGround/OBIFormer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOBIFormer\u7684\u5feb\u901f\u6ce8\u610f\u529b\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u7532\u9aa8\u6587\uff08OBI\uff09\u56fe\u50cf\u7684\u53bb\u566a\uff0c\u7ed3\u5408\u901a\u9053\u81ea\u6ce8\u610f\u529b\u3001\u5b57\u5f62\u63d0\u53d6\u548c\u9009\u62e9\u6027\u6838\u7279\u5f81\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u53bb\u566a\u6548\u679c\u3002", "motivation": "\u7532\u9aa8\u6587\u4f5c\u4e3a\u6700\u65e9\u7684\u4e2d\u6587\u5b57\u7b26\u5f62\u5f0f\uff0c\u56e0\u957f\u671f\u81ea\u7136\u98ce\u5316\u548c\u4eba\u4e3a\u7834\u574f\u800c\u4e25\u91cd\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53bb\u566a\u6548\u679c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "OBIFormer\u91c7\u7528\u901a\u9053\u81ea\u6ce8\u610f\u529b\u3001\u5b57\u5f62\u63d0\u53d6\u548c\u9009\u62e9\u6027\u6838\u7279\u5f81\u878d\u5408\u6280\u672f\uff0c\u9ad8\u6548\u91cd\u5efa\u53bb\u566a\u56fe\u50cf\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7532\u9aa8\u6587\u6570\u636e\u96c6\u4e0a\uff0cOBIFormer\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u4f18\u53bb\u566a\u6027\u80fd\u3002", "conclusion": "OBIFormer\u5728\u81ea\u52a8\u7532\u9aa8\u6587\u8bc6\u522b\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13775", "pdf": "https://arxiv.org/pdf/2504.13775", "abs": "https://arxiv.org/abs/2504.13775", "authors": ["Zhengxian Wu", "Juan Wen", "Wanli Peng", "Ziwei Zhang", "Yinghan Zhou", "Yiming Xue"], "title": "BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models", "categories": ["cs.CL", "cs.CR"], "comment": "16 pages, 6 figures", "summary": "Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\u7684\u65b0\u578b\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff08BadApex\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u6bd2\u5316\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u653b\u51fb\u6548\u679c\u548c\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u5ffd\u89c6\u6587\u672c\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u7684\u624b\u5de5\u63d0\u793a\uff0c\u9002\u5e94\u6027\u5dee\u3002", "method": "\u8bbe\u8ba1\u4e86\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\uff0c\u901a\u8fc7\u751f\u6210\u548c\u4fee\u6539\u4ee3\u7406\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\uff0c\u751f\u6210\u6bd2\u5316\u6587\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cBadApex\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe96.75%\u3002", "conclusion": "BadApex\u63d0\u5347\u4e86\u63d0\u793a\u9002\u5e94\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u6587\u672c\u8d28\u91cf\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9632\u5fa1\u62b5\u6297\u80fd\u529b\u3002"}}
{"id": "2504.13540", "pdf": "https://arxiv.org/pdf/2504.13540", "abs": "https://arxiv.org/abs/2504.13540", "authors": ["Beizhen Zhao", "Yifan Zhou", "Zijian Wang", "Hao Wang"], "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we explore an open research problem concerning the\nreconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian\nSplatting (3DGS) to produce 3D scenes due to its efficient training process.\nHowever, these methodologies may generate incomplete 3D scenes or blurred\nmultiviews. This is because of (1) inaccurate 3DGS point initialization and (2)\nthe tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To\naddress these issues, we propose a novel framework EG-Gaussian, which utilizes\nepipolar geometry and graph networks for 3D scene reconstruction. Initially, we\nintegrate epipolar geometry into the 3DGS initialization phase to enhance\ninitial 3DGS point construction. Then, we specifically design a graph learning\nmodule to refine 3DGS spatial features, in which we incorporate both spatial\ncoordinates and angular relationships among neighboring points. Experiments on\nindoor and outdoor benchmark datasets demonstrate that our approach\nsignificantly improves reconstruction accuracy compared to 3DGS-based methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEG-Gaussian\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6781\u51e0\u4f55\u548c\u56fe\u7f51\u7edc\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u521d\u59cb\u5316\u548c\u7279\u5f81\u4f18\u5316\uff0c\u663e\u8457\u63d0\u53473D\u573a\u666f\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u56e0\u521d\u59cb\u70b9\u4e0d\u51c6\u786e\u548c\u7a00\u758f\u89c6\u56fe\u8f93\u5165\u5bfc\u81f43D\u9ad8\u65af\u6241\u5e73\u5316\uff0c\u91cd\u5efa\u7ed3\u679c\u4e0d\u5b8c\u6574\u6216\u6a21\u7cca\u3002", "method": "\u7ed3\u5408\u6781\u51e0\u4f55\u4f18\u53163DGS\u521d\u59cb\u5316\uff0c\u8bbe\u8ba1\u56fe\u5b66\u4e60\u6a21\u5757\u7ec6\u5316\u7a7a\u95f4\u7279\u5f81\uff08\u5305\u62ec\u5750\u6807\u548c\u90bb\u70b9\u89d2\u5ea6\u5173\u7cfb\uff09\u3002", "result": "\u5728\u5ba4\u5185\u5916\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cEG-Gaussian\u663e\u8457\u4f18\u4e8e\u73b0\u67093DGS\u65b9\u6cd5\u3002", "conclusion": "EG-Gaussian\u901a\u8fc7\u6539\u8fdb\u521d\u59cb\u5316\u548c\u7279\u5f81\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2504.13816", "pdf": "https://arxiv.org/pdf/2504.13816", "abs": "https://arxiv.org/abs/2504.13816", "authors": ["Chenghao Xiao", "Hou Pong Chan", "Hao Zhang", "Mahani Aljunied", "Lidong Bing", "Noura Al Moubayed", "Yu Rong"], "title": "Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations", "categories": ["cs.CL"], "comment": null, "summary": "While understanding the knowledge boundaries of LLMs is crucial to prevent\nhallucination, research on knowledge boundaries of LLMs has predominantly\nfocused on English. In this work, we present the first study to analyze how\nLLMs recognize knowledge boundaries across different languages by probing their\ninternal representations when processing known and unknown questions in\nmultiple languages. Our empirical studies reveal three key findings: 1) LLMs'\nperceptions of knowledge boundaries are encoded in the middle to middle-upper\nlayers across different languages. 2) Language differences in knowledge\nboundary perception follow a linear structure, which motivates our proposal of\na training-free alignment method that effectively transfers knowledge boundary\nperception ability across languages, thereby helping reduce hallucination risk\nin low-resource languages; 3) Fine-tuning on bilingual question pair\ntranslation further enhances LLMs' recognition of knowledge boundaries across\nlanguages. Given the absence of standard testbeds for cross-lingual knowledge\nboundary analysis, we construct a multilingual evaluation suite comprising\nthree representative types of knowledge boundary data. Our code and datasets\nare publicly available at\nhttps://github.com/DAMO-NLP-SG/LLM-Multilingual-Knowledge-Boundaries.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5982\u4f55\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\uff0c\u53d1\u73b0\u5176\u611f\u77e5\u7f16\u7801\u4e8e\u4e2d\u4e0a\u5c42\u7f51\u7edc\u5c42\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u51cf\u5c11\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u591a\u8bed\u8a00\u4e2d\u7684\u77e5\u8bc6\u8fb9\u754c\u8bc6\u522b\uff0c\u586b\u8865\u82f1\u8bed\u4ee5\u5916\u8bed\u8a00\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u51cf\u5c11\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u63a2\u6d4bLLMs\u5185\u90e8\u8868\u793a\u5904\u7406\u5df2\u77e5\u548c\u672a\u77e5\u95ee\u9898\uff0c\u63d0\u51fa\u8bad\u7ec3\u514d\u8d39\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u591a\u8bed\u8a00\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u53d1\u73b0\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u7f16\u7801\u4e8e\u4e2d\u4e0a\u5c42\u7f51\u7edc\u5c42\uff0c\u8bed\u8a00\u5dee\u5f02\u5448\u7ebf\u6027\u7ed3\u6784\uff0c\u53cc\u8bed\u5fae\u8c03\u53ef\u589e\u5f3a\u8de8\u8bed\u8a00\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fb9\u754c\u5206\u6790\u63d0\u4f9b\u65b0\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e7b\u89c9\u98ce\u9669\u3002"}}
{"id": "2504.13548", "pdf": "https://arxiv.org/pdf/2504.13548", "abs": "https://arxiv.org/abs/2504.13548", "authors": ["Haoyang Luo", "Linwei Tao", "Minjing Dong", "Chang Xu"], "title": "Beyond One-Hot Labels: Semantic Mixing for Model Calibration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Model calibration seeks to ensure that models produce confidence scores that\naccurately reflect the true likelihood of their predictions being correct.\nHowever, existing calibration approaches are fundamentally tied to datasets of\none-hot labels implicitly assuming full certainty in all the annotations. Such\ndatasets are effective for classification but provides insufficient knowledge\nof uncertainty for model calibration, necessitating the curation of datasets\nwith numerically rich ground-truth confidence values. However, due to the\nscarcity of uncertain visual examples, such samples are not easily available as\nreal datasets. In this paper, we introduce calibration-aware data augmentation\nto create synthetic datasets of diverse samples and their ground-truth\nuncertainty. Specifically, we present Calibration-aware Semantic Mixing (CSM),\na novel framework that generates training samples with mixed class\ncharacteristics and annotates them with distinct confidence scores via\ndiffusion models. Based on this framework, we propose calibrated reannotation\nto tackle the misalignment between the annotated confidence score and the\nmixing ratio during the diffusion reverse process. Besides, we explore the loss\nfunctions that better fit the new data representation paradigm. Experimental\nresults demonstrate that CSM achieves superior calibration compared to the\nstate-of-the-art calibration approaches. Code is available at\ngithub.com/E-Galois/CSM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6821\u51c6\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08CSM\uff09\uff0c\u901a\u8fc7\u751f\u6210\u5177\u6709\u6df7\u5408\u7c7b\u522b\u7279\u5f81\u7684\u5408\u6210\u6570\u636e\u96c6\u6765\u89e3\u51b3\u6a21\u578b\u6821\u51c6\u4e2d\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u5f15\u5165\u6570\u503c\u4e30\u5bcc\u7684\u7f6e\u4fe1\u5ea6\u6807\u6ce8\u6570\u636e\u3002", "method": "\u63d0\u51faCSM\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u6df7\u5408\u7c7b\u522b\u6837\u672c\u5e76\u6807\u6ce8\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u63d0\u51fa\u6821\u51c6\u91cd\u6807\u6ce8\u548c\u4f18\u5316\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCSM\u5728\u6821\u51c6\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CSM\u4e3a\u6a21\u578b\u6821\u51c6\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u589e\u5f3a\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6821\u51c6\u6548\u679c\u3002"}}
{"id": "2504.13825", "pdf": "https://arxiv.org/pdf/2504.13825", "abs": "https://arxiv.org/abs/2504.13825", "authors": ["Junjie Yang", "Junhao Song", "Xudong Han", "Ziqian Bi", "Tianyang Wang", "Chia Xin Liang", "Xinyuan Song", "Yichao Zhang", "Qian Niu", "Benji Peng", "Keyu Chen", "Ming Liu"], "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Knowledge distillation (KD) is a technique for transferring knowledge from\ncomplex teacher models to simpler student models, significantly enhancing model\nefficiency and accuracy. It has demonstrated substantial advancements in\nvarious applications including image classification, object detection, language\nmodeling, text classification, and sentiment analysis. Recent innovations in KD\nmethods, such as attention-based approaches, block-wise logit distillation, and\ndecoupling distillation, have notably improved student model performance. These\ntechniques focus on stimulus complexity, attention mechanisms, and global\ninformation capture to optimize knowledge transfer. In addition, KD has proven\neffective in compressing large language models while preserving accuracy,\nreducing computational overhead, and improving inference speed. This survey\nsynthesizes the latest literature, highlighting key findings, contributions,\nand future directions in knowledge distillation to provide insights for\nresearchers and practitioners on its evolving role in artificial intelligence\nand machine learning.", "AI": {"tldr": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u901a\u8fc7\u5c06\u590d\u6742\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u5230\u7b80\u5355\u5b66\u751f\u6a21\u578b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u7b49\u9886\u57df\u3002", "motivation": "\u7814\u7a76\u77e5\u8bc6\u84b8\u998f\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u603b\u7ed3\u5176\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u7684\u4f5c\u7528\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u7efc\u8ff0\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3001\u5757\u7ea7logit\u84b8\u998f\u548c\u89e3\u8026\u84b8\u998f\u7b49\u521b\u65b0\u65b9\u6cd5\uff0c\u4f18\u5316\u77e5\u8bc6\u4f20\u9012\u3002", "result": "\u77e5\u8bc6\u84b8\u998f\u5728\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u548c\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u5728\u4eba\u5de5\u667a\u80fd\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5176\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u5e94\u7528\u6269\u5c55\u3002"}}
{"id": "2504.13560", "pdf": "https://arxiv.org/pdf/2504.13560", "abs": "https://arxiv.org/abs/2504.13560", "authors": ["SoYoung Park", "Hyewon Lee", "Mingyu Choi", "Seunghoon Han", "Jong-Ryul Lee", "Sungsu Lim", "Tae-Ho Kim"], "title": "Zero-Shot Industrial Anomaly Segmentation with Image-Aware Prompt Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to PAKDD 2025, 12 pages", "summary": "Anomaly segmentation is essential for industrial quality, maintenance, and\nstability. Existing text-guided zero-shot anomaly segmentation models are\neffective but rely on fixed prompts, limiting adaptability in diverse\nindustrial scenarios. This highlights the need for flexible, context-aware\nprompting strategies. We propose Image-Aware Prompt Anomaly Segmentation\n(IAP-AS), which enhances anomaly segmentation by generating dynamic,\ncontext-aware prompts using an image tagging model and a large language model\n(LLM). IAP-AS extracts object attributes from images to generate context-aware\nprompts, improving adaptability and generalization in dynamic and unstructured\nindustrial environments. In our experiments, IAP-AS improves the F1-max metric\nby up to 10%, demonstrating superior adaptability and generalization. It\nprovides a scalable solution for anomaly segmentation across industries", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u611f\u77e5\u7684\u52a8\u6001\u63d0\u793a\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\uff08IAP-AS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u6807\u6ce8\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u5206\u5272\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u63d0\u793a\u7684\u96f6\u6837\u672c\u5f02\u5e38\u5206\u5272\u65b9\u6cd5\u5728\u591a\u6837\u5316\u5de5\u4e1a\u573a\u666f\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u63d0\u793a\u7b56\u7565\u3002", "method": "\u5229\u7528\u56fe\u50cf\u6807\u6ce8\u6a21\u578b\u63d0\u53d6\u5bf9\u8c61\u5c5e\u6027\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u63d0\u5347\u5f02\u5e38\u5206\u5272\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIAP-AS\u5728F1-max\u6307\u6807\u4e0a\u63d0\u5347\u4e8610%\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "IAP-AS\u4e3a\u8de8\u884c\u4e1a\u5f02\u5e38\u5206\u5272\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u7684\u5de5\u4e1a\u73af\u5883\u3002"}}
{"id": "2504.13828", "pdf": "https://arxiv.org/pdf/2504.13828", "abs": "https://arxiv.org/abs/2504.13828", "authors": ["Shijie Xia", "Yiwei Qin", "Xuefeng Li", "Yan Ma", "Run-Ze Fan", "Steffi Chern", "Haoyang Zou", "Fan Zhou", "Xiangkun Hu", "Jiahe Jin", "Yanheng He", "Yixin Ye", "Yixiu Liu", "Pengfei Liu"], "title": "Generative AI Act II: Test Time Scaling Drives Cognition Engineering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations in knowledge\nlatency, shallow reasoning, and constrained cognitive processes. During this\nera, prompt engineering emerged as our primary interface with AI, enabling\ndialogue-level communication through natural language. We now witness the\nemergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering", "AI": {"tldr": "\u8bba\u6587\u603b\u7ed3\u4e86\u751f\u6210\u5f0fAI\u7684\u7b2c\u4e00\u9636\u6bb5\uff082020-2023\uff09\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u7b2c\u4e8c\u9636\u6bb5\uff082024\u81f3\u4eca\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u5373\u901a\u8fc7\u8bed\u8a00\u601d\u7ef4\u4e0eAI\u5efa\u7acb\u66f4\u6df1\u5c42\u6b21\u7684\u8fde\u63a5\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u4ece\u77e5\u8bc6\u68c0\u7d22\u7cfb\u7edf\u5411\u601d\u7ef4\u6784\u5efa\u5f15\u64ce\u7684\u8f6c\u53d8\uff0c\u5e76\u63a8\u52a8\u8ba4\u77e5\u5de5\u7a0b\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6559\u7a0b\u548c\u4f18\u5316\u5b9e\u73b0\u7cfb\u7edf\u6027\u5730\u5206\u89e3\u9ad8\u7ea7\u65b9\u6cd5\uff0c\u666e\u53ca\u8ba4\u77e5\u5de5\u7a0b\u6280\u672f\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u8bba\u6587\u96c6\u5408\uff0c\u5e76\u5f00\u6e90\u4e86\u76f8\u5173\u8d44\u6e90\u3002", "conclusion": "\u8ba4\u77e5\u5de5\u7a0b\u7684\u53d1\u5c55\u6b63\u5904\u4e8e\u5173\u952e\u65f6\u671f\uff0c\u5176\u666e\u53ca\u5c06\u63a8\u52a8AI\u8fdb\u5165\u7b2c\u4e8c\u9636\u6bb5\u3002"}}
{"id": "2504.13561", "pdf": "https://arxiv.org/pdf/2504.13561", "abs": "https://arxiv.org/abs/2504.13561", "authors": ["Yang Wu", "Yun Zhu", "Kaihua Zhang", "Jianjun Qian", "Jin Xie", "Jian Yang"], "title": "WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "3D scene perception demands a large amount of adverse-weather LiDAR data, yet\nthe cost of LiDAR data collection presents a significant scaling-up challenge.\nTo this end, a series of LiDAR simulators have been proposed. Yet, they can\nonly simulate a single adverse weather with a single physical model, and the\nfidelity of the generated data is quite limited. This paper presents\nWeatherGen, the first unified diverse-weather LiDAR data diffusion generation\nframework, significantly improving fidelity. Specifically, we first design a\nmap-based data producer, which can provide a vast amount of high-quality\ndiverse-weather data for training purposes. Then, we utilize the\ndiffusion-denoising paradigm to construct a diffusion model. Among them, we\npropose a spider mamba generator to restore the disturbed diverse weather data\ngradually. The spider mamba models the feature interactions by scanning the\nLiDAR beam circle or central ray, excellently maintaining the physical\nstructure of the LiDAR data. Subsequently, following the generator to transfer\nreal-world knowledge, we design a latent feature aligner. Afterward, we devise\na contrastive learning-based controller, which equips weather control signals\nwith compact semantic knowledge through language supervision, guiding the\ndiffusion model to generate more discriminative data. Extensive evaluations\ndemonstrate the high generation quality of WeatherGen. Through WeatherGen, we\nconstruct the mini-weather dataset, promoting the performance of the downstream\ntask under adverse weather conditions. Code is available:\nhttps://github.com/wuyang98/weathergen", "AI": {"tldr": "WeatherGen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6837\u5929\u6c14LiDAR\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u8718\u86db\u66fc\u5df4\u751f\u6210\u5668\u63d0\u9ad8\u6570\u636e\u4fdd\u771f\u5ea6\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u63a7\u5236\u5668\u589e\u5f3a\u751f\u6210\u6570\u636e\u7684\u533a\u5206\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LiDAR\u6a21\u62df\u5668\u53ea\u80fd\u6a21\u62df\u5355\u4e00\u6076\u52a3\u5929\u6c14\u4e14\u6570\u636e\u4fdd\u771f\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u591a\u6837\u5929\u6c14\u6570\u636e\u4ee5\u652f\u63013D\u573a\u666f\u611f\u77e5\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5730\u56fe\u7684\u6570\u636e\u751f\u6210\u5668\uff0c\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u8718\u86db\u66fc\u5df4\u751f\u6210\u5668\u9010\u6b65\u6062\u590d\u6570\u636e\uff0c\u5e76\u5f15\u5165\u6f5c\u5728\u7279\u5f81\u5bf9\u9f50\u5668\u548c\u5bf9\u6bd4\u5b66\u4e60\u63a7\u5236\u5668\u3002", "result": "WeatherGen\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\u9ad8\uff0c\u6784\u5efa\u7684mini-weather\u6570\u636e\u96c6\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "WeatherGen\u4e3a\u591a\u6837\u5929\u6c14LiDAR\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3002"}}
{"id": "2504.13834", "pdf": "https://arxiv.org/pdf/2504.13834", "abs": "https://arxiv.org/abs/2504.13834", "authors": ["Muhan Gao", "Jash Shah", "Weiqi Wang", "Daniel Khashabi"], "title": "Science Hierarchography: Hierarchical Organization of Science Literature", "categories": ["cs.CL"], "comment": null, "summary": "Scientific knowledge is growing rapidly, making it challenging to track\nprogress and high-level conceptual links across broad disciplines. While\nexisting tools like citation networks and search engines make it easy to access\na few related papers, they fundamentally lack the flexible abstraction needed\nto represent the density of activity in various scientific subfields. We\nmotivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature\ninto a high-quality hierarchical structure that allows for the categorization\nof scientific work across varying levels of abstraction, from very broad fields\nto very specific studies. Such a representation can provide insights into which\nfields are well-explored and which are under-explored. To achieve the goals of\nSCIENCE HIERARCHOGRAPHY, we develop a range of algorithms. Our primary approach\ncombines fast embedding-based clustering with LLM-based prompting to balance\nthe computational efficiency of embedding methods with the semantic precision\noffered by LLM prompting. We demonstrate that this approach offers the best\ntrade-off between quality and speed compared to methods that heavily rely on\nLLM prompting, such as iterative tree construction with LLMs. To better reflect\nthe interdisciplinary and multifaceted nature of research papers, our hierarchy\ncaptures multiple dimensions of categorization beyond simple topic labels. We\nevaluate the utility of our framework by assessing how effectively an LLM-based\nagent can locate target papers using the hierarchy. Results show that this\nstructured approach enhances interpretability, supports trend discovery, and\noffers an alternative pathway for exploring scientific literature beyond\ntraditional search methods. Code, data and demo:\n$\\href{https://github.com/JHU-CLSP/science-hierarchography}{https://github.com/JHU-CLSP/science-hierarchography}$", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCIENCE HIERARCHOGRAPHY\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5c06\u79d1\u5b66\u6587\u732e\u7ec4\u7ec7\u6210\u9ad8\u8d28\u91cf\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u4ee5\u63ed\u793a\u4e0d\u540c\u9886\u57df\u7684\u63a2\u7d22\u7a0b\u5ea6\u3002", "motivation": "\u79d1\u5b66\u77e5\u8bc6\u5feb\u901f\u589e\u957f\uff0c\u73b0\u6709\u5de5\u5177\uff08\u5982\u5f15\u7528\u7f51\u7edc\u548c\u641c\u7d22\u5f15\u64ce\uff09\u7f3a\u4e4f\u7075\u6d3b\u7684\u62bd\u8c61\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u8868\u793a\u79d1\u5b66\u5b50\u9886\u57df\u7684\u6d3b\u52a8\u5bc6\u5ea6\u3002", "method": "\u7ed3\u5408\u5feb\u901f\u5d4c\u5165\u805a\u7c7b\u548c\u57fa\u4e8eLLM\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u8bed\u4e49\u7cbe\u5ea6\uff0c\u6784\u5efa\u591a\u7ef4\u5ea6\u5206\u7c7b\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f9d\u8d56LLM\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6587\u732e\u63a2\u7d22\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "SCIENCE HIERARCHOGRAPHY\u4e3a\u79d1\u5b66\u6587\u732e\u63a2\u7d22\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u9014\u5f84\uff0c\u652f\u6301\u8d8b\u52bf\u53d1\u73b0\u548c\u8de8\u5b66\u79d1\u7814\u7a76\u3002"}}
{"id": "2504.13579", "pdf": "https://arxiv.org/pdf/2504.13579", "abs": "https://arxiv.org/abs/2504.13579", "authors": ["Shuobin Wei", "Zhuang Zhou", "Zhengan Lu", "Zizhao Yuan", "Binghua Su"], "title": "HDBFormer: Efficient RGB-D Semantic Segmentation with A Heterogeneous Dual-Branch Framework", "categories": ["cs.CV"], "comment": "6 pages, 4 figures, published to IEEE Signal Processing Letter", "summary": "In RGB-D semantic segmentation for indoor scenes, a key challenge is\neffectively integrating the rich color information from RGB images with the\nspatial distance information from depth images. However, most existing methods\noverlook the inherent differences in how RGB and depth images express\ninformation. Properly distinguishing the processing of RGB and depth images is\nessential to fully exploiting their unique and significant characteristics. To\naddress this, we propose a novel heterogeneous dual-branch framework called\nHDBFormer, specifically designed to handle these modality differences. For RGB\nimages, which contain rich detail, we employ both a basic and detail encoder to\nextract local and global features. For the simpler depth images, we propose\nLDFormer, a lightweight hierarchical encoder that efficiently extracts depth\nfeatures with fewer parameters. Additionally, we introduce the Modality\nInformation Interaction Module (MIIM), which combines transformers with large\nkernel convolutions to interact global and local information across modalities\nefficiently. Extensive experiments show that HDBFormer achieves\nstate-of-the-art performance on the NYUDepthv2 and SUN-RGBD datasets. The code\nis available at: https://github.com/Weishuobin/HDBFormer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDBFormer\u7684\u5f02\u6784\u53cc\u5206\u652f\u6846\u67b6\uff0c\u7528\u4e8e\u6709\u6548\u6574\u5408RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u4fe1\u606f\uff0c\u901a\u8fc7\u4e0d\u540c\u7684\u7f16\u7801\u5668\u548c\u4ea4\u4e92\u6a21\u5757\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u5ba4\u5185\u573a\u666f\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u533a\u5206RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u4fe1\u606f\u8868\u8fbe\u65b9\u5f0f\u5dee\u5f02\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u72ec\u7279\u7279\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86HDBFormer\u6846\u67b6\uff0c\u5305\u62ecRGB\u56fe\u50cf\u7684\u57fa\u7840\u548c\u7ec6\u8282\u7f16\u7801\u5668\u3001\u6df1\u5ea6\u56fe\u50cf\u7684\u8f7b\u91cf\u7ea7LDFormer\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u6a21\u6001\u4fe1\u606f\u4ea4\u4e92\u6a21\u5757\uff08MIIM\uff09\u3002", "result": "\u5728NYUDepthv2\u548cSUN-RGBD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HDBFormer\u901a\u8fc7\u533a\u5206\u5904\u7406RGB\u548c\u6df1\u5ea6\u56fe\u50cf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2504.13835", "pdf": "https://arxiv.org/pdf/2504.13835", "abs": "https://arxiv.org/abs/2504.13835", "authors": ["Yicheng Chen", "Yining Li", "Kai Hu", "Zerun Ma", "Haochen Ye", "Kai Chen"], "title": "MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Data quality and diversity are key to the construction of effective\ninstruction-tuning datasets. % With the increasing availability of open-source\ninstruction-tuning datasets, it is advantageous to automatically select\nhigh-quality and diverse subsets from a vast amount of data. % Existing methods\ntypically prioritize instance quality and use heuristic rules to maintain\ndiversity. % However, this absence of a comprehensive view of the entire\ncollection often leads to suboptimal results. % Moreover, heuristic rules\ngenerally focus on distance or clustering within the embedding space, which\nfails to accurately capture the intent of complex instructions in the semantic\nspace. % To bridge this gap, we propose a unified method for quantifying the\ninformation content of datasets. This method models the semantic space by\nconstructing a label graph and quantifies diversity based on the distribution\nof information within the graph. % Based on such a measurement, we further\nintroduce an efficient sampling method that selects data samples iteratively to\n\\textbf{M}aximize the \\textbf{I}nformation \\textbf{G}ain (MIG) in semantic\nspace. % Experiments on various datasets and base models demonstrate that MIG\nconsistently outperforms state-of-the-art methods. % Notably, the model\nfine-tuned with 5\\% Tulu3 data sampled by MIG achieves comparable performance\nto the official SFT model trained on the full dataset, with improvements of\n+5.73\\% on AlpacaEval and +6.89\\% on Wildbench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u6570\u636e\u96c6\u7684\u4fe1\u606f\u5185\u5bb9\uff0c\u901a\u8fc7\u6784\u5efa\u6807\u7b7e\u56fe\u5efa\u6a21\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u57fa\u4e8e\u4fe1\u606f\u5206\u5e03\u91cf\u5316\u591a\u6837\u6027\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u91c7\u6837\u65b9\u6cd5\uff08MIG\uff09\uff0c\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u6700\u5927\u5316\u4fe1\u606f\u589e\u76ca\u3002\u5b9e\u9a8c\u8868\u660eMIG\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u5b9e\u4f8b\u8d28\u91cf\u5e76\u4f7f\u7528\u542f\u53d1\u5f0f\u89c4\u5219\u4fdd\u6301\u591a\u6837\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u96c6\u7684\u5168\u9762\u89c6\u89d2\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7406\u60f3\u3002\u6b64\u5916\uff0c\u542f\u53d1\u5f0f\u89c4\u5219\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u6216\u805a\u7c7b\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u590d\u6742\u6307\u4ee4\u7684\u8bed\u4e49\u610f\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6807\u7b7e\u56fe\u5efa\u6a21\u8bed\u4e49\u7a7a\u95f4\uff0c\u91cf\u5316\u4fe1\u606f\u5206\u5e03\u3002\u57fa\u4e8e\u6b64\uff0c\u5f15\u5165MIG\u91c7\u6837\u65b9\u6cd5\uff0c\u8fed\u4ee3\u9009\u62e9\u6570\u636e\u6837\u672c\u4ee5\u6700\u5927\u5316\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u4fe1\u606f\u589e\u76ca\u3002", "result": "MIG\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u57fa\u7840\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4f7f\u7528MIG\u91c7\u6837\u76845% Tulu3\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u5168\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5b98\u65b9SFT\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u5728AlpacaEval\u548cWildbench\u4e0a\u5206\u522b\u63d0\u53475.73%\u548c6.89%\u3002", "conclusion": "MIG\u65b9\u6cd5\u5728\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u91cf\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13580", "pdf": "https://arxiv.org/pdf/2504.13580", "abs": "https://arxiv.org/abs/2504.13580", "authors": ["Yuchen Rao", "Stefan Ainetter", "Sinisa Stekovic", "Vincent Lepetit", "Friedrich Fraundorfer"], "title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding", "categories": ["cs.CV"], "comment": "Github Page: https://github.com/stefan-ainetter/SCANnotatepp", "summary": "High-level 3D scene understanding is essential in many applications. However,\nthe challenges of generating accurate 3D annotations make development of deep\nlearning models difficult. We turn to recent advancements in automatic\nretrieval of synthetic CAD models, and show that data generated by such methods\ncan be used as high-quality ground truth for training supervised deep learning\nmodels. More exactly, we employ a pipeline akin to the one previously used to\nautomatically annotate objects in ScanNet scenes with their 9D poses and CAD\nmodels. This time, we apply it to the recent ScanNet++ v1 dataset, which\npreviously lacked such annotations. Our findings demonstrate that it is not\nonly possible to train deep learning models on these automatically-obtained\nannotations but that the resulting models outperform those trained on manually\nannotated data. We validate this on two distinct tasks: point cloud completion\nand single-view CAD model retrieval and alignment. Our results underscore the\npotential of automatic 3D annotations to enhance model performance while\nsignificantly reducing annotation costs. To support future research in 3D scene\nunderstanding, we will release our annotations, which we call SCANnotate++,\nalong with our trained models.", "AI": {"tldr": "\u5229\u7528\u81ea\u52a8\u68c0\u7d22\u5408\u6210CAD\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u70b9\u4e91\u8865\u5168\u548c\u5355\u89c6\u56feCAD\u6a21\u578b\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b33D\u573a\u666f\u7406\u89e3\u4e2d\u9ad8\u8d28\u91cf\u6807\u6ce8\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u3002", "method": "\u91c7\u7528\u7c7b\u4f3cScanNet\u7684\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\uff0c\u5e94\u7528\u4e8eScanNet++ v1\u6570\u636e\u96c6\uff0c\u751f\u6210\u81ea\u52a8\u6807\u6ce8\u6570\u636eSCANnotate++\u3002", "result": "\u81ea\u52a8\u6807\u6ce8\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u70b9\u4e91\u8865\u5168\u548cCAD\u6a21\u578b\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u6807\u6ce8\u6a21\u578b\u3002", "conclusion": "\u81ea\u52a83D\u6807\u6ce8\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u6210\u672c\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6807\u6ce8\u6570\u636e\u548c\u8bad\u7ec3\u6a21\u578b\u3002"}}
{"id": "2504.13202", "pdf": "https://arxiv.org/pdf/2504.13202", "abs": "https://arxiv.org/abs/2504.13202", "authors": ["Timo Aukusti Laine"], "title": "The Quantum LLM: Modeling Semantic Spaces with Quantum Principles", "categories": ["cs.AI", "cs.CL", "quant-ph"], "comment": "16 pages, 6 figures", "summary": "In the previous article, we presented a quantum-inspired framework for\nmodeling semantic representation and processing in Large Language Models\n(LLMs), drawing upon mathematical tools and conceptual analogies from quantum\nmechanics to offer a new perspective on these complex systems. In this paper,\nwe clarify the core assumptions of this model, providing a detailed exposition\nof six key principles that govern semantic representation, interaction, and\ndynamics within LLMs. The goal is to justify that a quantum-inspired framework\nis a valid approach to studying semantic spaces. This framework offers valuable\ninsights into their information processing and response generation, and we\nfurther discuss the potential of leveraging quantum computing to develop\nsignificantly more powerful and efficient LLMs based on these principles.", "AI": {"tldr": "\u672c\u6587\u6f84\u6e05\u4e86\u91cf\u5b50\u542f\u53d1\u7684\u8bed\u4e49\u8868\u793a\u6a21\u578b\u7684\u6838\u5fc3\u5047\u8bbe\uff0c\u8be6\u7ec6\u9610\u8ff0\u4e86\u516d\u9879\u5173\u952e\u539f\u5219\uff0c\u65e8\u5728\u8bc1\u660e\u8be5\u6846\u67b6\u662f\u7814\u7a76\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u63d0\u5347LLMs\u6027\u80fd\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u901a\u8fc7\u91cf\u5b50\u529b\u5b66\u7684\u6570\u5b66\u5de5\u5177\u548c\u6982\u5ff5\u7c7b\u6bd4\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bed\u4e49\u8868\u793a\u548c\u5904\u7406\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5e76\u9a8c\u8bc1\u91cf\u5b50\u542f\u53d1\u6846\u67b6\u7684\u5408\u7406\u6027\u3002", "method": "\u63d0\u51fa\u516d\u9879\u5173\u952e\u539f\u5219\uff0c\u8be6\u7ec6\u9610\u8ff0\u8bed\u4e49\u8868\u793a\u3001\u4ea4\u4e92\u548c\u52a8\u6001\u7684\u91cf\u5b50\u542f\u53d1\u6a21\u578b\u3002", "result": "\u8bc1\u660e\u4e86\u91cf\u5b50\u542f\u53d1\u6846\u67b6\u662f\u7814\u7a76\u8bed\u4e49\u7a7a\u95f4\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u4fe1\u606f\u5904\u7406\u548c\u54cd\u5e94\u751f\u6210\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6709\u671b\u57fa\u4e8e\u8fd9\u4e9b\u539f\u5219\u5f00\u53d1\u66f4\u5f3a\u5927\u9ad8\u6548\u7684LLMs\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.13590", "pdf": "https://arxiv.org/pdf/2504.13590", "abs": "https://arxiv.org/abs/2504.13590", "authors": ["Alexander Rusnak", "Fr\u00e9d\u00e9ric Kaplan"], "title": "HAECcity: Open-Vocabulary Scene Understanding of City-Scale Point Clouds with Superpoint Graph Clustering", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication through the upcoming CVPR Workshop on open\n  scene understanding with foundation models (OPENSUN3D)", "summary": "Traditional 3D scene understanding techniques are generally predicated on\nhand-annotated label sets, but in recent years a new class of open-vocabulary\n3D scene understanding techniques has emerged. Despite the success of this\nparadigm on small scenes, existing approaches cannot scale efficiently to\ncity-scale 3D datasets. In this paper, we present Hierarchical vocab-Agnostic\nExpert Clustering (HAEC), after the latin word for 'these', a superpoint graph\nclustering based approach which utilizes a novel mixture of experts graph\ntransformer for its backbone. We administer this highly scalable approach to\nthe first application of open-vocabulary scene understanding on the SensatUrban\ncity-scale dataset. We also demonstrate a synthetic labeling pipeline which is\nderived entirely from the raw point clouds with no hand-annotation. Our\ntechnique can help unlock complex operations on dense urban 3D scenes and open\na new path forward in the processing of digital twins.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAEC\u7684\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u9ad8\u6548\u6269\u5c55\u81f3\u57ce\u5e02\u89c4\u6a21\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf3D\u573a\u666f\u7406\u89e3\u4f9d\u8d56\u624b\u5de5\u6807\u6ce8\uff0c\u800c\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u81f3\u57ce\u5e02\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8d85\u70b9\u56fe\u7684\u805a\u7c7b\u65b9\u6cd5HAEC\uff0c\u7ed3\u5408\u4e13\u5bb6\u6df7\u5408\u56fe\u53d8\u6362\u5668\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\uff0c\u5e76\u5f00\u53d1\u4e86\u65e0\u9700\u624b\u5de5\u6807\u6ce8\u7684\u5408\u6210\u6807\u7b7e\u751f\u6210\u6d41\u7a0b\u3002", "result": "\u5728SensatUrban\u57ce\u5e02\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9996\u6b21\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7406\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5408\u6210\u6807\u7b7e\u751f\u6210\u80fd\u529b\u3002", "conclusion": "HAEC\u4e3a\u5bc6\u96c6\u57ce\u5e023D\u573a\u666f\u7684\u590d\u6742\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u63a8\u52a8\u4e86\u6570\u5b57\u5b6a\u751f\u5904\u7406\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.13203", "pdf": "https://arxiv.org/pdf/2504.13203", "abs": "https://arxiv.org/abs/2504.13203", "authors": ["Salman Rahman", "Liwei Jiang", "James Shiffer", "Genglin Liu", "Sheriff Issaka", "Md Rizwan Parvez", "Hamid Palangi", "Kai-Wei Chang", "Yejin Choi", "Saadia Gabriel"], "title": "X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-turn interactions with language models (LMs) pose critical safety\nrisks, as harmful intent can be strategically spread across exchanges. Yet, the\nvast majority of prior work has focused on single-turn safety, while\nadaptability and diversity remain among the key challenges of multi-turn\nred-teaming. To address these challenges, we present X-Teaming, a scalable\nframework that systematically explores how seemingly harmless interactions\nescalate into harmful outcomes and generates corresponding attack scenarios.\nX-Teaming employs collaborative agents for planning, attack optimization, and\nverification, achieving state-of-the-art multi-turn jailbreak effectiveness and\ndiversity with success rates up to 98.1% across representative leading\nopen-weight and closed-source models. In particular, X-Teaming achieves a 96.2%\nattack success rate against the latest Claude 3.7 Sonnet model, which has been\nconsidered nearly immune to single-turn attacks. Building on X-Teaming, we\nintroduce XGuard-Train, an open-source multi-turn safety training dataset that\nis 20x larger than the previous best resource, comprising 30K interactive\njailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our\nwork offers essential tools and insights for mitigating sophisticated\nconversational attacks, advancing the multi-turn safety of LMs.", "AI": {"tldr": "X-Teaming\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5e76\u751f\u6210\u653b\u51fb\u573a\u666f\uff0c\u6210\u529f\u7387\u8fbe\u523098.1%\u3002", "motivation": "\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u98ce\u9669\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u653b\u51fb\u3002", "method": "X-Teaming\u4f7f\u7528\u534f\u4f5c\u4ee3\u7406\u8fdb\u884c\u89c4\u5212\u3001\u653b\u51fb\u4f18\u5316\u548c\u9a8c\u8bc1\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u653b\u51fb\u573a\u666f\u3002", "result": "X-Teaming\u5728\u591a\u8f6e\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9Claude 3.7 Sonnet\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u8fbe96.2%\u3002", "conclusion": "X-Teaming\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8f6e\u5b89\u5168\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u3002"}}
{"id": "2504.13593", "pdf": "https://arxiv.org/pdf/2504.13593", "abs": "https://arxiv.org/abs/2504.13593", "authors": ["Yan Shi", "Qingdong He", "Yijun Liu", "Xiaoyu Liu", "Jingyong Su"], "title": "KAN or MLP? Point Cloud Shows the Way Forward", "categories": ["cs.CV"], "comment": null, "summary": "Multi-Layer Perceptrons (MLPs) have become one of the fundamental\narchitectural component in point cloud analysis due to its effective feature\nlearning mechanism. However, when processing complex geometric structures in\npoint clouds, MLPs' fixed activation functions struggle to efficiently capture\nlocal geometric features, while suffering from poor parameter efficiency and\nhigh model redundancy. In this paper, we propose PointKAN, which applies\nKolmogorov-Arnold Networks (KANs) to point cloud analysis tasks to investigate\ntheir efficacy in hierarchical feature representation. First, we introduce a\nGeometric Affine Module (GAM) to transform local features, improving the\nmodel's robustness to geometric variations. Next, in the Local Feature\nProcessing (LFP), a parallel structure extracts both group-level features and\nglobal context, providing a rich representation of both fine details and\noverall structure. Finally, these features are combined and processed in the\nGlobal Feature Processing (GFP). By repeating these operations, the receptive\nfield gradually expands, enabling the model to capture complete geometric\ninformation of the point cloud. To overcome the high parameter counts and\ncomputational inefficiency of standard KANs, we develop Efficient-KANs in the\nPointKAN-elite variant, which significantly reduces parameters while\nmaintaining accuracy. Experimental results demonstrate that PointKAN\noutperforms PointMLP on benchmark datasets such as ModelNet40, ScanObjectNN,\nand ShapeNetPart, with particularly strong performance in Few-shot Learning\ntask. Additionally, PointKAN achieves substantial reductions in parameter\ncounts and computational complexity (FLOPs). This work highlights the potential\nof KANs-based architectures in 3D vision and opens new avenues for research in\npoint cloud understanding.", "AI": {"tldr": "PointKAN\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKolmogorov-Arnold Networks\uff08KANs\uff09\u7684\u70b9\u4e91\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u4eff\u5c04\u6a21\u5757\u548c\u5c40\u90e8\u7279\u5f81\u5904\u7406\u63d0\u5347\u7279\u5f81\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728\u9ad8\u6548\u53c2\u6570\u8bbe\u8ba1\u4e0b\u663e\u8457\u4f18\u4e8ePointMLP\u3002", "motivation": "\u4f20\u7edf\u7684MLPs\u5728\u70b9\u4e91\u5206\u6790\u4e2d\u96be\u4ee5\u9ad8\u6548\u6355\u6349\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\uff0c\u4e14\u53c2\u6570\u6548\u7387\u4f4e\u3002PointKAN\u65e8\u5728\u5229\u7528KANs\u7684\u5c42\u6b21\u7279\u5f81\u8868\u793a\u80fd\u529b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u5f15\u5165\u51e0\u4f55\u4eff\u5c04\u6a21\u5757\uff08GAM\uff09\u589e\u5f3a\u51e0\u4f55\u53d8\u5316\u9c81\u68d2\u6027\uff1b2. \u901a\u8fc7\u5e76\u884c\u7ed3\u6784\u7684\u5c40\u90e8\u7279\u5f81\u5904\u7406\uff08LFP\uff09\u63d0\u53d6\u7ec4\u7ea7\u548c\u5168\u5c40\u7279\u5f81\uff1b3. \u5168\u5c40\u7279\u5f81\u5904\u7406\uff08GFP\uff09\u7ed3\u5408\u5e76\u6269\u5c55\u611f\u53d7\u91ce\uff1b4. \u5f00\u53d1\u9ad8\u6548KANs\uff08PointKAN-elite\uff09\u51cf\u5c11\u53c2\u6570\u3002", "result": "PointKAN\u5728ModelNet40\u3001ScanObjectNN\u548cShapeNetPart\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8ePointMLP\uff0c\u5c24\u5176\u5728Few-shot Learning\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u53c2\u6570\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "PointKAN\u5c55\u793a\u4e86KANs\u57283D\u89c6\u89c9\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u70b9\u4e91\u7406\u89e3\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.13277", "pdf": "https://arxiv.org/pdf/2504.13277", "abs": "https://arxiv.org/abs/2504.13277", "authors": ["Soorya Ram Shimgekar", "Violeta J. Rodriguez", "Paul A. Bloom", "Dong Whi Yoo", "Koustuv Saha"], "title": "Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Suicide is a critical global public health issue, with millions experiencing\nsuicidal ideation (SI) each year. Online spaces enable individuals to express\nSI and seek peer support. While prior research has revealed the potential of\ndetecting SI using machine learning and natural language analysis, a key\nlimitation is the lack of a theoretical framework to understand the underlying\nfactors affecting high-risk suicidal intent. To bridge this gap, we adopted the\nInterpersonal Theory of Suicide (IPTS) as an analytic lens to analyze 59,607\nposts from Reddit's r/SuicideWatch, categorizing them into SI dimensions\n(Loneliness, Lack of Reciprocal Love, Self Hate, and Liability) and risk\nfactors (Thwarted Belongingness, Perceived Burdensomeness, and Acquired\nCapability of Suicide). We found that high-risk SI posts express planning and\nattempts, methods and tools, and weaknesses and pain. In addition, we also\nexamined the language of supportive responses through psycholinguistic and\ncontent analyses to find that individuals respond differently to different\nstages of Suicidal Ideation (SI) posts. Finally, we explored the role of AI\nchatbots in providing effective supportive responses to suicidal ideation\nposts. We found that although AI improved structural coherence, expert\nevaluations highlight persistent shortcomings in providing dynamic,\npersonalized, and deeply empathetic support. These findings underscore the need\nfor careful reflection and deeper understanding in both the development and\nconsideration of AI-driven interventions for effective mental health support.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u81ea\u6740\u7684\u4eba\u9645\u5173\u7cfb\u7406\u8bba\uff08IPTS\uff09\u5206\u6790Reddit\u4e0a59,607\u7bc7\u81ea\u6740\u610f\u5ff5\uff08SI\uff09\u5e16\u5b50\uff0c\u53d1\u73b0\u9ad8\u98ce\u9669\u5e16\u5b50\u8868\u8fbe\u8ba1\u5212\u3001\u65b9\u6cd5\u548c\u75db\u82e6\uff0c\u5e76\u63a2\u8ba8\u4e86AI\u804a\u5929\u673a\u5668\u4eba\u5728\u63d0\u4f9b\u652f\u6301\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u81ea\u6740\u662f\u5168\u7403\u6027\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7406\u89e3\u9ad8\u98ce\u9669\u81ea\u6740\u610f\u56fe\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u91c7\u7528IPTS\u7406\u8bba\u5206\u6790Reddit\u7684r/SuicideWatch\u5e16\u5b50\uff0c\u5206\u7c7b\u4e3aSI\u7ef4\u5ea6\u548c\u98ce\u9669\u56e0\u7d20\uff0c\u5e76\u5206\u6790\u652f\u6301\u6027\u56de\u5e94\u7684\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u9ad8\u98ce\u9669SI\u5e16\u5b50\u8868\u8fbe\u8ba1\u5212\u3001\u65b9\u6cd5\u548c\u75db\u82e6\uff1bAI\u804a\u5929\u673a\u5668\u4eba\u867d\u63d0\u5347\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u52a8\u6001\u548c\u4e2a\u6027\u5316\u652f\u6301\u4e0a\u4e0d\u8db3\u3002", "conclusion": "\u9700\u6df1\u5165\u7406\u89e3\u548c\u53cd\u601dAI\u9a71\u52a8\u7684\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\uff0c\u4ee5\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u652f\u6301\u3002"}}
{"id": "2504.13596", "pdf": "https://arxiv.org/pdf/2504.13596", "abs": "https://arxiv.org/abs/2504.13596", "authors": ["Shanshuai Yuan", "Julong Wei", "Muer Tie", "Xiangyun Ren", "Zhongxue Gan", "Wenchao Ding"], "title": "LMPOcc: 3D Semantic Occupancy Prediction Utilizing Long-Term Memory Prior from Historical Traversals", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-based 3D semantic occupancy prediction is critical for autonomous\ndriving, enabling unified modeling of static infrastructure and dynamic agents.\nIn practice, autonomous vehicles may repeatedly traverse identical geographic\nlocations under varying environmental conditions, such as weather fluctuations\nand illumination changes. Existing methods in 3D occupancy prediction\npredominantly integrate adjacent temporal contexts. However, these works\nneglect to leverage perceptual information, which is acquired from historical\ntraversals of identical geographic locations. In this paper, we propose\nLongterm Memory Prior Occupancy (LMPOcc), the first 3D occupancy prediction\nmethodology that exploits long-term memory priors derived from historical\ntraversal perceptual outputs. We introduce a plug-and-play architecture that\nintegrates long-term memory priors to enhance local perception while\nsimultaneously constructing global occupancy representations. To adaptively\naggregate prior features and current features, we develop an efficient\nlightweight Current-Prior Fusion module. Moreover, we propose a model-agnostic\nprior format to ensure compatibility across diverse occupancy prediction\nbaselines. LMPOcc achieves state-of-the-art performance validated on the\nOcc3D-nuScenes benchmark, especially on static semantic categories.\nAdditionally, experimental results demonstrate LMPOcc's ability to construct\nglobal occupancy through multi-vehicle crowdsourcing.", "AI": {"tldr": "LMPOcc\u662f\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u904d\u5386\u611f\u77e5\u8f93\u51fa\u76843D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u957f\u65f6\u8bb0\u5fc6\u5148\u9a8c\u589e\u5f3a\u5c40\u90e8\u611f\u77e5\u5e76\u6784\u5efa\u5168\u5c40\u5360\u7528\u8868\u793a\uff0c\u5728Occ3D-nuScenes\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u76f8\u540c\u5730\u7406\u4f4d\u7f6e\u7684\u591a\u6b21\u904d\u5386\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528\u5386\u53f2\u611f\u77e5\u4fe1\u606f\uff0c\u9650\u5236\u4e863D\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faLMPOcc\u65b9\u6cd5\uff0c\u5f15\u5165\u957f\u65f6\u8bb0\u5fc6\u5148\u9a8c\uff0c\u8bbe\u8ba1\u8f7b\u91cf\u7ea7Current-Prior Fusion\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u5148\u9a8c\u683c\u5f0f\u3002", "result": "\u5728Occ3D-nuScenes\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u9759\u6001\u8bed\u4e49\u7c7b\u522b\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u652f\u6301\u591a\u8f66\u4f17\u5305\u6784\u5efa\u5168\u5c40\u5360\u7528\u3002", "conclusion": "LMPOcc\u901a\u8fc7\u5229\u7528\u5386\u53f2\u904d\u5386\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e863D\u5360\u7528\u9884\u6d4b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5168\u5c40\u5360\u7528\u6784\u5efa\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13308", "pdf": "https://arxiv.org/pdf/2504.13308", "abs": "https://arxiv.org/abs/2504.13308", "authors": ["Leena G Pillai", "D. Muhammad Noorul Mubarak"], "title": "Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "This is a review paper about Acoustic to Articulatory inversion of\n  speech, presented in an international conference. This paper has 8 pages and\n  2 figures", "summary": "This review is focused on the data-driven approaches applied in different\napplications of Acoustic-to-Articulatory Inversion (AAI) of speech. This review\npaper considered the relevant works published in the last ten years\n(2011-2021). The selection criteria includes (a) type of AAI - Speaker\nDependent and Speaker Independent AAI, (b) objectives of the work -\nArticulatory approximation, Articulatory Feature space selection and Automatic\nSpeech Recognition (ASR), explore the correlation between acoustic and\narticulatory features, and framework for Computer-assisted language training,\n(c) Corpus - Simultaneously recorded speech (wav) and medical imaging models\nsuch as ElectroMagnetic Articulography (EMA), Electropalatography (EPG),\nLaryngography, Electroglottography (EGG), X-ray Cineradiography, Ultrasound,\nand real-time Magnetic Resonance Imaging (rtMRI), (d) Methods or models -\nrecent works are considered, and therefore all the works are based on machine\nlearning, (e) Evaluation - as AAI is a non-linear regression problem, the\nperformance evaluation is mostly done by Correlation Coefficient (CC), Root\nMean Square Error (RMSE), and also considered Mean Square Error (MSE), and Mean\nFormat Error (MFE). The practical application of the AAI model can provide a\nbetter and user-friendly interpretable image feedback system of articulatory\npositions, especially tongue movement. Such trajectory feedback system can be\nused to provide phonetic, language, and speech therapy for pathological\nsubjects.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u8fc7\u53bb\u5341\u5e74\uff082011-2021\u5e74\uff09\u4e2d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u58f0\u5b66-\u53d1\u97f3\u53cd\u6f14\uff08AAI\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u7c7b\u578b\u7684AAI\u3001\u76ee\u6807\u3001\u8bed\u6599\u5e93\u3001\u65b9\u6cd5\u53ca\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u63a2\u7d22AAI\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u8a00\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u53d1\u97f3\u4f4d\u7f6e\u7684\u53cd\u9988\u7cfb\u7edf\u6539\u5584\u8bed\u97f3\u6cbb\u7597\u548c\u8bed\u8a00\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u79cd\u533b\u5b66\u6210\u50cf\u6280\u672f\uff08\u5982EMA\u3001EPG\u3001rtMRI\u7b49\uff09\u8bb0\u5f55\u7684\u6570\u636e\u3002", "result": "AAI\u6a21\u578b\u901a\u8fc7\u76f8\u5173\u6027\u7cfb\u6570\uff08CC\uff09\u3001\u5747\u65b9\u6839\u8bef\u5dee\uff08RMSE\uff09\u7b49\u6307\u6807\u8bc4\u4f30\uff0c\u80fd\u591f\u63d0\u4f9b\u76f4\u89c2\u7684\u53d1\u97f3\u4f4d\u7f6e\u53cd\u9988\uff0c\u5c24\u5176\u5728\u820c\u90e8\u8fd0\u52a8\u65b9\u9762\u3002", "conclusion": "AAI\u6a21\u578b\u5728\u8bed\u97f3\u6cbb\u7597\u548c\u8bed\u8a00\u8bad\u7ec3\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u56fe\u50cf\u53cd\u9988\u7cfb\u7edf\u6539\u5584\u53d1\u97f3\u51c6\u786e\u6027\u3002"}}
{"id": "2504.13604", "pdf": "https://arxiv.org/pdf/2504.13604", "abs": "https://arxiv.org/abs/2504.13604", "authors": ["Ying Wang", "Tingfa Xu", "Jianan Li"], "title": "FocusTrack: A Self-Adaptive Local Sampling Algorithm for Efficient Anti-UAV Tracking", "categories": ["cs.CV"], "comment": "13pages, 13 figures", "summary": "Anti-UAV tracking poses significant challenges, including small target sizes,\nabrupt camera motion, and cluttered infrared backgrounds. Existing tracking\nparadigms can be broadly categorized into global- and local-based methods.\nGlobal-based trackers, such as SiamDT, achieve high accuracy by scanning the\nentire field of view but suffer from excessive computational overhead, limiting\nreal-world deployment. In contrast, local-based methods, including OSTrack and\nROMTrack, efficiently restrict the search region but struggle when targets\nundergo significant displacements due to abrupt camera motion. Through\npreliminary experiments, it is evident that a local tracker, when paired with\nadaptive search region adjustment, can significantly enhance tracking accuracy,\nnarrowing the gap between local and global trackers. To address this challenge,\nwe propose FocusTrack, a novel framework that dynamically refines the search\nregion and strengthens feature representations, achieving an optimal balance\nbetween computational efficiency and tracking accuracy. Specifically, our\nSearch Region Adjustment (SRA) strategy estimates the target presence\nprobability and adaptively adjusts the field of view, ensuring the target\nremains within focus. Furthermore, to counteract feature degradation caused by\nvarying search regions, the Attention-to-Mask (ATM) module is proposed. This\nmodule integrates hierarchical information, enriching the target\nrepresentations with fine-grained details. Experimental results demonstrate\nthat FocusTrack achieves state-of-the-art performance, obtaining 67.7% AUC on\nAntiUAV and 62.8% AUC on AntiUAV410, outperforming the baseline tracker by 8.5%\nand 9.1% AUC, respectively. In terms of efficiency, FocusTrack surpasses\nglobal-based trackers, requiring only 30G MACs and achieving 143 fps with\nFocusTrack (SRA) and 44 fps with the full version, both enabling real-time\ntracking.", "AI": {"tldr": "FocusTrack\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u641c\u7d22\u533a\u57df\u548c\u589e\u5f3a\u7279\u5f81\u8868\u793a\u7684\u65b0\u6846\u67b6\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6297\u65e0\u4eba\u673a\u8ddf\u8e2a\u4e2d\u7684\u5c0f\u76ee\u6807\u5c3a\u5bf8\u3001\u76f8\u673a\u7a81\u53d8\u8fd0\u52a8\u548c\u590d\u6742\u7ea2\u5916\u80cc\u666f\u7b49\u6311\u6218\uff0c\u540c\u65f6\u5f25\u8865\u5c40\u90e8\u548c\u5168\u5c40\u8ddf\u8e2a\u5668\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u63d0\u51faSearch Region Adjustment (SRA)\u7b56\u7565\u52a8\u6001\u8c03\u6574\u641c\u7d22\u533a\u57df\uff0c\u4ee5\u53caAttention-to-Mask (ATM)\u6a21\u5757\u589e\u5f3a\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728AntiUAV\u548cAntiUAV410\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523067.7%\u548c62.8%\u7684AUC\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u8ddf\u8e2a\u5668\uff0c\u540c\u65f6\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "FocusTrack\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u6297\u65e0\u4eba\u673a\u8ddf\u8e2a\u3002"}}
{"id": "2504.13359", "pdf": "https://arxiv.org/pdf/2504.13359", "abs": "https://arxiv.org/abs/2504.13359", "authors": ["Mehmet Hamza Erol", "Batu El", "Mirac Suzgun", "Mert Yuksekgonul", "James Zou"], "title": "Cost-of-Pass: An Economic Framework for Evaluating Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Code is available at: https://github.com/mhamzaerol/Cost-of-Pass", "summary": "The widespread adoption of AI systems in the economy hinges on their ability\nto generate economic value that outweighs their inference costs. Evaluating\nthis tradeoff requires metrics that account for both performance and costs. We\npropose a framework grounded in production theory for evaluating language\nmodels by combining accuracy and inference cost. We introduce \"cost-of-pass\",\nthe expected monetary cost of generating a correct solution. We then define the\n\"frontier cost-of-pass\" as the minimum cost-of-pass achievable across available\nmodels or the \"human-expert, using the approximate cost of hiring an expert.\nOur analysis reveals distinct economic insights. First, lightweight models are\nmost cost-effective for basic quantitative tasks, large models for\nknowledge-intensive ones, and reasoning models for complex quantitative\nproblems, despite higher per-token costs. Second, tracking this frontier\ncost-of-pass over the past year reveals significant progress, particularly for\ncomplex quantitative tasks where the cost has roughly halved every few months.\nThird, to trace key innovations driving this progress, we examine\ncounterfactual frontiers: estimates of cost-efficiency without specific model\nclasses. We find that innovations in lightweight, large, and reasoning models\nhave been essential for pushing the frontier in basic quantitative,\nknowledge-intensive, and complex quantitative tasks, respectively. Finally, we\nassess the cost-reductions afforded by common inference-time techniques like\nmajority voting and self-refinement, finding that their marginal accuracy gains\nrarely justify their costs. Our findings underscore that complementary\nmodel-level innovations are the primary drivers of cost-efficiency, and our\neconomic framework provides a principled tool for measuring this progress and\nguiding deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u4ea7\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u51c6\u786e\u6027\u548c\u63a8\u7406\u6210\u672c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u7ecf\u6d4e\u4ef7\u503c\uff0c\u5f15\u5165\u201ccost-of-pass\u201d\u548c\u201cfrontier cost-of-pass\u201d\u6307\u6807\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6210\u672c\u6548\u76ca\uff0c\u5e76\u5206\u6790\u4e86\u6280\u672f\u8fdb\u6b65\u5bf9\u6210\u672c\u6548\u7387\u7684\u5f71\u54cd\u3002", "motivation": "\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u9700\u8981\u6743\u8861\u5176\u6027\u80fd\u4e0e\u63a8\u7406\u6210\u672c\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7efc\u5408\u8003\u8651\u4e24\u8005\u7684\u6307\u6807\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u4ea7\u7406\u8bba\u7684\u6846\u67b6\uff0c\u5b9a\u4e49\u201ccost-of-pass\u201d\u548c\u201cfrontier cost-of-pass\u201d\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ca\u6280\u672f\u8fdb\u6b65\u7684\u5f71\u54cd\u6765\u8bc4\u4f30\u6210\u672c\u6548\u76ca\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u57fa\u7840\u5b9a\u91cf\u4efb\u52a1\u4e2d\u6700\u5177\u6210\u672c\u6548\u76ca\uff0c\u5927\u578b\u6a21\u578b\u9002\u5408\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u63a8\u7406\u6a21\u578b\u9002\u5408\u590d\u6742\u5b9a\u91cf\u95ee\u9898\uff1b\u8fc7\u53bb\u4e00\u5e74\u4e2d\u590d\u6742\u5b9a\u91cf\u4efb\u52a1\u7684\u6210\u672c\u6548\u7387\u663e\u8457\u63d0\u5347\uff1b\u6a21\u578b\u7ea7\u521b\u65b0\u662f\u6210\u672c\u6548\u7387\u63d0\u5347\u7684\u4e3b\u8981\u9a71\u52a8\u529b\u3002", "conclusion": "\u4e92\u8865\u7684\u6a21\u578b\u7ea7\u521b\u65b0\u662f\u63d0\u5347\u6210\u672c\u6548\u7387\u7684\u5173\u952e\uff0c\u63d0\u51fa\u7684\u7ecf\u6d4e\u6846\u67b6\u4e3a\u8861\u91cf\u8fdb\u5c55\u548c\u6307\u5bfc\u90e8\u7f72\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u5de5\u5177\u3002"}}
{"id": "2504.13608", "pdf": "https://arxiv.org/pdf/2504.13608", "abs": "https://arxiv.org/abs/2504.13608", "authors": ["Pengxiang Gao", "Yihao Liang", "Yanzhi Song", "Zhouwang Yang"], "title": "Cross-Hierarchical Bidirectional Consistency Learning for Fine-Grained Visual Classification", "categories": ["cs.CV"], "comment": null, "summary": "Fine-Grained Visual Classification (FGVC) aims to categorize closely related\nsubclasses, a task complicated by minimal inter-class differences and\nsignificant intra-class variance. Existing methods often rely on additional\nannotations for image classification, overlooking the valuable information\nembedded in Tree Hierarchies that depict hierarchical label relationships. To\nleverage this knowledge to improve classification accuracy and consistency, we\npropose a novel Cross-Hierarchical Bidirectional Consistency Learning (CHBC)\nframework. The CHBC framework extracts discriminative features across various\nhierarchies using a specially designed module to decompose and enhance\nattention masks and features. We employ bidirectional consistency loss to\nregulate the classification outcomes across different hierarchies, ensuring\nlabel prediction consistency and reducing misclassification. Experiments on\nthree widely used FGVC datasets validate the effectiveness of the CHBC\nframework. Ablation studies further investigate the application strategies of\nfeature enhancement and consistency constraints, underscoring the significant\ncontributions of the proposed modules.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCHBC\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6811\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u4fe1\u606f\uff0c\u901a\u8fc7\u53cc\u5411\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6807\u6ce8\uff0c\u5ffd\u7565\u4e86\u6811\u5c42\u6b21\u7ed3\u6784\u4e2d\u8574\u542b\u7684\u6709\u4ef7\u503c\u4fe1\u606f\uff0c\u5bfc\u81f4\u5206\u7c7b\u6548\u679c\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86CHBC\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u589e\u5f3a\u6ce8\u610f\u529b\u63a9\u7801\u548c\u7279\u5f81\u63d0\u53d6\u8de8\u5c42\u6b21\u5224\u522b\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u53cc\u5411\u4e00\u81f4\u6027\u635f\u5931\u786e\u4fdd\u5206\u7c7b\u7ed3\u679c\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684FGVC\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CHBC\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7279\u5f81\u589e\u5f3a\u548c\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u8d21\u732e\u3002", "conclusion": "CHBC\u6846\u67b6\u901a\u8fc7\u5229\u7528\u6811\u5c42\u6b21\u7ed3\u6784\u548c\u53cc\u5411\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.13388", "pdf": "https://arxiv.org/pdf/2504.13388", "abs": "https://arxiv.org/abs/2504.13388", "authors": ["Yegor Klochkov"], "title": "A mean teacher algorithm for unlearning of language models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "One of the goals of language model unlearning is to reduce memorization of\nselected text instances while retaining the model's general abilities. Despite\nvarious proposed methods, reducing memorization of large datasets without\nnoticeable degradation in model utility remains challenging. In this paper, we\ninvestigate the mean teacher algorithm (Tarvainen & Valpola, 2017), a simple\nproximal optimization method from continual learning literature that gradually\nmodifies the teacher model. We show that the mean teacher can approximate a\ntrajectory of a slow natural gradient descent (NGD), which inherently seeks\nlow-curvature updates that are less likely to degrade the model utility. While\nslow NGD can suffer from vanishing gradients, we introduce a new unlearning\nloss called \"negative log-unlikelihood\" (NLUL) that avoids this problem. We\nshow that the combination of mean teacher and NLUL improves some metrics on the\nMUSE benchmarks (Shi et al., 2024).", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6280\u672f\uff0c\u63d0\u51fa\u7ed3\u5408\u5747\u503c\u6559\u5e08\u7b97\u6cd5\u548c\u8d1f\u5bf9\u6570\u975e\u4f3c\u7136\u635f\u5931\uff08NLUL\uff09\u6765\u51cf\u5c11\u8bb0\u5fc6\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u51cf\u5c11\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5b9a\u6587\u672c\u5b9e\u4f8b\u7684\u8bb0\u5fc6\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u901a\u7528\u80fd\u529b\uff0c\u662f\u5f53\u524d\u6311\u6218\u3002", "method": "\u91c7\u7528\u5747\u503c\u6559\u5e08\u7b97\u6cd5\u548c\u8d1f\u5bf9\u6570\u975e\u4f3c\u7136\u635f\u5931\uff08NLUL\uff09\u6765\u4f18\u5316\u6a21\u578b\u9057\u5fd8\u8fc7\u7a0b\u3002", "result": "\u5728MUSE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u90e8\u5206\u6307\u6807\u3002", "conclusion": "\u5747\u503c\u6559\u5e08\u4e0eNLUL\u7ed3\u5408\u80fd\u6709\u6548\u51cf\u5c11\u8bb0\u5fc6\u5316\u4e14\u4e0d\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.13617", "pdf": "https://arxiv.org/pdf/2504.13617", "abs": "https://arxiv.org/abs/2504.13617", "authors": ["Zuyao Chen", "Jinlin Wu", "Zhen Lei", "Marc Pollefeys", "Chang Wen Chen"], "title": "Compile Scene Graphs with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Next token prediction is the fundamental principle for training large\nlanguage models (LLMs), and reinforcement learning (RL) further enhances their\nreasoning performance. As an effective way to model language, image, video, and\nother modalities, the use of LLMs for end-to-end extraction of structured\nvisual representations, such as scene graphs, remains underexplored. It\nrequires the model to accurately produce a set of objects and relationship\ntriplets, rather than generating text token by token. To achieve this, we\nintroduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised\nfine-tuning (SFT) on the scene graph dataset and subsequently refined using\nreinforcement learning to enhance its ability to generate scene graphs in an\nend-to-end manner. The SFT follows a conventional prompt-response paradigm,\nwhile RL requires the design of effective reward signals. Given the structured\nnature of scene graphs, we design a graph-centric reward function that\nintegrates node-level rewards, edge-level rewards, and a format consistency\nreward. Our experiments demonstrate that rule-based RL substantially enhances\nmodel performance in the SGG task, achieving a zero failure rate--unlike\nsupervised fine-tuning (SFT), which struggles to generalize effectively. Our\ncode is available at https://github.com/gpt4vision/R1-SGG.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faR1-SGG\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u573a\u666f\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\uff08\u5982\u573a\u666f\u56fe\uff09\u7aef\u5230\u7aef\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u8bbe\u8ba1\u56fe\u4e2d\u5fc3\u5956\u52b1\u51fd\u6570\uff08\u8282\u70b9\u3001\u8fb9\u548c\u683c\u5f0f\u4e00\u81f4\u6027\u5956\u52b1\uff09\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u96f6\u5931\u8d25\u7387\uff0c\u4f18\u4e8e\u4ec5\u7528\u76d1\u7763\u5fae\u8c03\u7684\u65b9\u6cd5\u3002", "conclusion": "R1-SGG\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u573a\u666f\u56fe\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2504.13406", "pdf": "https://arxiv.org/pdf/2504.13406", "abs": "https://arxiv.org/abs/2504.13406", "authors": ["Xiangbo Gao", "Yuheng Wu", "Rujia Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "LangCoop: Collaborative Driving with Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-agent collaboration holds great promise for enhancing the safety,\nreliability, and mobility of autonomous driving systems by enabling information\nsharing among multiple connected agents. However, existing multi-agent\ncommunication approaches are hindered by limitations of existing communication\nmedia, including high bandwidth demands, agent heterogeneity, and information\nloss. To address these challenges, we introduce LangCoop, a new paradigm for\ncollaborative autonomous driving that leverages natural language as a compact\nyet expressive medium for inter-agent communication. LangCoop features two key\ninnovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured\nzero-shot vision-language reasoning and Natural Language Information Packaging\n(LangPack) for efficiently packaging information into concise, language-based\nmessages. Through extensive experiments conducted in the CARLA simulations, we\ndemonstrate that LangCoop achieves a remarkable 96\\% reduction in communication\nbandwidth (< 2KB per message) compared to image-based communication, while\nmaintaining competitive driving performance in the closed-loop evaluation.", "AI": {"tldr": "LangCoop\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9a7e\u9a76\u7684\u901a\u4fe1\u5a92\u4ecb\uff0c\u663e\u8457\u964d\u4f4e\u5e26\u5bbd\u9700\u6c42\u5e76\u4fdd\u6301\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u65b9\u6cd5\u7684\u9ad8\u5e26\u5bbd\u9700\u6c42\u3001\u667a\u80fd\u4f53\u5f02\u6784\u6027\u548c\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "method": "\u63d0\u51faLangCoop\u6846\u67b6\uff0c\u5305\u542bM$^3$CoT\uff08\u7ed3\u6784\u5316\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\uff09\u548cLangPack\uff08\u9ad8\u6548\u8bed\u8a00\u4fe1\u606f\u5c01\u88c5\uff09\u3002", "result": "\u5728CARLA\u4eff\u771f\u4e2d\uff0cLangCoop\u5c06\u901a\u4fe1\u5e26\u5bbd\u964d\u4f4e96%\uff08\u6bcf\u6761\u6d88\u606f<2KB\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "LangCoop\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u5e26\u5bbd\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13621", "pdf": "https://arxiv.org/pdf/2504.13621", "abs": "https://arxiv.org/abs/2504.13621", "authors": ["Pengzhan Sun", "Junbin Xiao", "Tze Ho Elden Tse", "Yicong Li", "Arjun Akula", "Angela Yao"], "title": "Visual Intention Grounding for Egocentric Assistants", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding associates textual descriptions with objects in an image.\nConventional methods target third-person image inputs and named object queries.\nIn applications such as AI assistants, the perspective shifts -- inputs are\negocentric, and objects may be referred to implicitly through needs and\nintentions. To bridge this gap, we introduce EgoIntention, the first dataset\nfor egocentric visual intention grounding. EgoIntention challenges multimodal\nLLMs to 1) understand and ignore unintended contextual objects and 2) reason\nabout uncommon object functionalities. Benchmark results show that current\nmodels misidentify context objects and lack affordance understanding in\negocentric views. We also propose Reason-to-Ground (RoG) instruction tuning; it\nenables hybrid training with normal descriptions and egocentric intentions with\na chained intention reasoning and object grounding mechanism. RoG significantly\noutperforms naive finetuning and hybrid training on EgoIntention, while\nmaintaining or slightly improving naive description grounding. This advancement\nenables unified visual grounding for egocentric and exocentric visual inputs\nwhile handling explicit object queries and implicit human intentions.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86EgoIntention\u6570\u636e\u96c6\u548cReason-to-Ground\uff08RoG\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u89c6\u89c9\u610f\u56fe\u5b9a\u4f4d\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86RoG\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u548c\u663e\u5f0f\u5bf9\u8c61\u67e5\u8be2\uff0c\u800cAI\u52a9\u624b\u7b49\u5e94\u7528\u9700\u8981\u5904\u7406\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u89c6\u89d2\u548c\u9690\u5f0f\u610f\u56fe\u3002", "method": "\u63d0\u51faEgoIntention\u6570\u636e\u96c6\u548cRoG\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\uff0c\u7ed3\u5408\u610f\u56fe\u63a8\u7406\u548c\u5bf9\u8c61\u5b9a\u4f4d\u673a\u5236\u8fdb\u884c\u6df7\u5408\u8bad\u7ec3\u3002", "result": "RoG\u5728EgoIntention\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u548c\u6df7\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u5347\u663e\u5f0f\u63cf\u8ff0\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "RoG\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u7edf\u4e00\u89c6\u89c9\u5b9a\u4f4d\uff0c\u540c\u65f6\u5904\u7406\u663e\u5f0f\u5bf9\u8c61\u67e5\u8be2\u548c\u9690\u5f0f\u4eba\u7c7b\u610f\u56fe\u3002"}}
{"id": "2504.13416", "pdf": "https://arxiv.org/pdf/2504.13416", "abs": "https://arxiv.org/abs/2504.13416", "authors": ["Saksham Rastogi", "Pratyush Maini", "Danish Pruthi"], "title": "STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": "Accepted at DATA-FM, WMark @ ICLR 2025. Project page at see\n  https://codeboy5.github.io/stamp", "summary": "Given how large parts of publicly available text are crawled to pretrain\nlarge language models (LLMs), data creators increasingly worry about the\ninclusion of their proprietary data for model training without attribution or\nlicensing. Their concerns are also shared by benchmark curators whose test-sets\nmight be compromised. In this paper, we present STAMP, a framework for\ndetecting dataset membership-i.e., determining the inclusion of a dataset in\nthe pretraining corpora of LLMs. Given an original piece of content, our\nproposal involves first generating multiple rephrases, each embedding a\nwatermark with a unique secret key. One version is to be released publicly,\nwhile others are to be kept private. Subsequently, creators can compare model\nlikelihoods between public and private versions using paired statistical tests\nto prove membership. We show that our framework can successfully detect\ncontamination across four benchmarks which appear only once in the training\ndata and constitute less than 0.001% of the total tokens, outperforming several\ncontamination detection and dataset inference baselines. We verify that STAMP\npreserves both the semantic meaning and the utility of the original data in\ncomparing different models. We apply STAMP to two real-world scenarios to\nconfirm the inclusion of paper abstracts and blog articles in the pretraining\ncorpora.", "AI": {"tldr": "STAMP\u662f\u4e00\u4e2a\u7528\u4e8e\u68c0\u6d4b\u6570\u636e\u96c6\u6210\u5458\u8d44\u683c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5e26\u6709\u6c34\u5370\u7684\u91cd\u8ff0\u7248\u672c\u5e76\u6bd4\u8f83\u6a21\u578b\u4f3c\u7136\u6027\uff0c\u6210\u529f\u8bc6\u522bLLM\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u79c1\u6709\u6570\u636e\u3002", "motivation": "\u6570\u636e\u521b\u4f5c\u8005\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u7406\u8005\u62c5\u5fc3\u5176\u4e13\u6709\u6570\u636e\u672a\u7ecf\u8bb8\u53ef\u88ab\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u6570\u636e\u662f\u5426\u88ab\u5305\u542b\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u3002", "method": "STAMP\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5e26\u6709\u552f\u4e00\u5bc6\u94a5\u6c34\u5370\u7684\u591a\u4e2a\u91cd\u8ff0\u7248\u672c\uff0c\u516c\u5f00\u4e00\u4e2a\u7248\u672c\u5e76\u4fdd\u7559\u5176\u4ed6\u7248\u672c\uff0c\u968f\u540e\u901a\u8fc7\u914d\u5bf9\u7edf\u8ba1\u6d4b\u8bd5\u6bd4\u8f83\u6a21\u578b\u4f3c\u7136\u6027\u6765\u68c0\u6d4b\u6570\u636e\u6210\u5458\u8d44\u683c\u3002", "result": "STAMP\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u68c0\u6d4b\u5230\u6570\u636e\u6c61\u67d3\uff0c\u5373\u4f7f\u6570\u636e\u4ec5\u51fa\u73b0\u4e00\u6b21\u4e14\u5360\u603b\u6807\u8bb0\u7684\u4e0d\u52300.001%\uff0c\u4e14\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "STAMP\u4e0d\u4ec5\u80fd\u6709\u6548\u68c0\u6d4b\u6570\u636e\u6210\u5458\u8d44\u683c\uff0c\u8fd8\u80fd\u4fdd\u6301\u539f\u59cb\u6570\u636e\u7684\u8bed\u4e49\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u5982\u8bba\u6587\u6458\u8981\u548c\u535a\u5ba2\u6587\u7ae0\u7684\u68c0\u6d4b\u3002"}}
{"id": "2504.13638", "pdf": "https://arxiv.org/pdf/2504.13638", "abs": "https://arxiv.org/abs/2504.13638", "authors": ["Yang Zhang", "Jingyi Cao", "Yanan You", "Yuanyuan Qiao"], "title": "DenSe-AdViT: A novel Vision Transformer for Dense SAR Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformer (ViT) has achieved remarkable results in object detection\nfor synthetic aperture radar (SAR) images, owing to its exceptional ability to\nextract global features. However, it struggles with the extraction of\nmulti-scale local features, leading to limited performance in detecting small\ntargets, especially when they are densely arranged. Therefore, we propose\nDensity-Sensitive Vision Transformer with Adaptive Tokens (DenSe-AdViT) for\ndense SAR target detection. We design a Density-Aware Module (DAM) as a\npreliminary component that generates a density tensor based on target\ndistribution. It is guided by a meticulously crafted objective metric, enabling\nprecise and effective capture of the spatial distribution and density of\nobjects. To integrate the multi-scale information enhanced by convolutional\nneural networks (CNNs) with the global features derived from the Transformer,\nDensity-Enhanced Fusion Module (DEFM) is proposed. It effectively refines\nattention toward target-survival regions with the assist of density mask and\nthe multiple sources features. Notably, our DenSe-AdViT achieves 79.8% mAP on\nthe RSDD dataset and 92.5% on the SIVED dataset, both of which feature a large\nnumber of densely distributed vehicle targets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenSe-AdViT\u7684\u5bc6\u5ea6\u654f\u611f\u89c6\u89c9Transformer\uff0c\u7528\u4e8e\u89e3\u51b3SAR\u56fe\u50cf\u4e2d\u5bc6\u96c6\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5bc6\u5ea6\u611f\u77e5\u6a21\u5757\u548c\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "ViT\u5728SAR\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u63d0\u53d6\u591a\u5c3a\u5ea6\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u5bc6\u96c6\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6548\u679c\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u5bc6\u5ea6\u611f\u77e5\u6a21\u5757\uff08DAM\uff09\u751f\u6210\u5bc6\u5ea6\u5f20\u91cf\uff0c\u5e76\u7ed3\u5408\u5bc6\u5ea6\u589e\u5f3a\u878d\u5408\u6a21\u5757\uff08DEFM\uff09\u6574\u5408CNN\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u4e0eTransformer\u7684\u5168\u5c40\u7279\u5f81\u3002", "result": "\u5728RSDD\u548cSIVED\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523079.8%\u548c92.5%\u7684mAP\u3002", "conclusion": "DenSe-AdViT\u901a\u8fc7\u5bc6\u5ea6\u654f\u611f\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u5bc6\u96c6\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2504.13472", "pdf": "https://arxiv.org/pdf/2504.13472", "abs": "https://arxiv.org/abs/2504.13472", "authors": ["Xinchen Wang", "Pengfei Gao", "Chao Peng", "Ruida Hu", "Cuiyun Gao"], "title": "CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in code\ngeneration, underscoring the critical need for rigorous and comprehensive\nevaluation. Existing evaluation approaches fall into three categories,\nincluding human-centered, metric-based, and LLM-based. Considering that\nhuman-centered approaches are labour-intensive and metric-based ones overly\nrely on reference answers, LLM-based approaches are gaining increasing\nattention due to their stronger contextual understanding capabilities and\nsuperior efficiency. However, the performance of LLM-based approaches remains\nlimited due to: (1) lack of multisource domain knowledge, and (2) insufficient\ncomprehension of complex code.\n  To mitigate the limitations, we propose CodeVisionary, the first LLM-based\nagent framework for evaluating LLMs in code generation. CodeVisionary consists\nof two stages: (1) Multiscore knowledge analysis stage, which aims to gather\nmultisource and comprehensive domain knowledge by formulating and executing a\nstepwise evaluation plan. (2) Negotiation-based scoring stage, which involves\nmultiple judges engaging in discussions to better comprehend the complex code\nand reach a consensus on the evaluation score. Extensive experiments\ndemonstrate that CodeVisionary achieves the best performance for evaluating\nLLMs in code generation, outperforming the best baseline methods with average\nimprovements of 0.202, 0.139, and 0.117 in Pearson, Spearman, and Kendall-Tau\ncoefficients, respectively. Besides, CodeVisionary provides detailed evaluation\nreports, which assist developers in identifying shortcomings and making\nimprovements. The resources of CodeVisionary are available at\nhttps://anonymous.4open.science/r/CodeVisionary.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCodeVisionary\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6e90\u77e5\u8bc6\u5206\u6790\u548c\u534f\u5546\u8bc4\u5206\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u5728\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff08\u4eba\u5de5\u3001\u57fa\u4e8e\u6307\u6807\u6216LLM\uff09\u5404\u6709\u4e0d\u8db3\uff0cLLM\u65b9\u6cd5\u867d\u9ad8\u6548\u4f46\u53d7\u9650\u4e8e\u77e5\u8bc6\u4e0d\u8db3\u548c\u5bf9\u590d\u6742\u4ee3\u7801\u7684\u7406\u89e3\u3002", "method": "CodeVisionary\u6846\u67b6\u5206\u4e24\u9636\u6bb5\uff1a\u591a\u6e90\u77e5\u8bc6\u5206\u6790\u548c\u534f\u5546\u8bc4\u5206\uff0c\u7ed3\u5408\u591a\u8bc4\u59d4\u8ba8\u8bba\u8fbe\u6210\u5171\u8bc6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCodeVisionary\u5728Pearson\u3001Spearman\u548cKendall-Tau\u7cfb\u6570\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u8bc4\u4f30\u62a5\u544a\u3002", "conclusion": "CodeVisionary\u663e\u8457\u63d0\u5347LLM\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2504.13645", "pdf": "https://arxiv.org/pdf/2504.13645", "abs": "https://arxiv.org/abs/2504.13645", "authors": ["Numan Saeed", "Shahad Hardan", "Muhammad Ridzuan", "Nada Saadi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Cancer detection and prognosis relies heavily on medical imaging,\nparticularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise\nin tumor segmentation by fusing information from these modalities. However, a\ncritical bottleneck exists: the dependency on CT-PET data concurrently for\ntraining and inference, posing a challenge due to the limited availability of\nPET scans. Hence, there is a clear need for a flexible and efficient framework\nthat can be trained with the widely available CT scans and can be still adapted\nfor PET scans when they become available. In this work, we propose a\nparameter-efficient multi-modal adaptation (PEMMA) framework for lightweight\nupgrading of a transformer-based segmentation model trained only on CT scans\nsuch that it can be efficiently adapted for use with PET scans when they become\navailable. This framework is further extended to perform prognosis task\nmaintaining the same efficient cross-modal fine-tuning approach. The proposed\napproach is tested with two well-known segementation backbones, namely UNETR\nand Swin UNETR. Our approach offers two main advantages. Firstly, we leverage\nthe inherent modularity of the transformer architecture and perform low-rank\nadaptation (LoRA) as well as decomposed low-rank adaptation (DoRA) of the\nattention weights to achieve parameter-efficient adaptation. Secondly, by\nminimizing cross-modal entanglement, PEMMA allows updates using only one\nmodality without causing catastrophic forgetting in the other. Our method\nachieves comparable performance to early fusion, but with only 8% of the\ntrainable parameters, and demonstrates a significant +28% Dice score\nimprovement on PET scans when trained with a single modality. Furthermore, in\nprognosis, our method improves the concordance index by +10% when adapting a\nCT-pretrained model to include PET scans, and by +23% when adapting for both\nPET and EHR data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u9002\u5e94\u6846\u67b6\uff08PEMMA\uff09\uff0c\u7528\u4e8e\u4ec5\u57fa\u4e8eCT\u626b\u63cf\u8bad\u7ec3\u7684\u6a21\u578b\u8f7b\u91cf\u7ea7\u5347\u7ea7\uff0c\u4ee5\u5728PET\u626b\u63cf\u53ef\u7528\u65f6\u9ad8\u6548\u9002\u5e94\uff0c\u540c\u65f6\u652f\u6301\u9884\u540e\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3CT-PET\u6570\u636e\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u56e0PET\u626b\u63cf\u7a00\u7f3a\uff0c\u9700\u7075\u6d3b\u6846\u67b6\u4ee5CT\u4e3a\u57fa\u7840\u5e76\u9002\u5e94PET\u3002", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u5206\u89e3\u4f4e\u79e9\u9002\u5e94\uff08DoRA\uff09\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u8c03\u6574\uff0c\u51cf\u5c11\u8de8\u6a21\u6001\u7ea0\u7f20\u3002", "result": "\u6027\u80fd\u4e0e\u65e9\u671f\u878d\u5408\u76f8\u5f53\uff0c\u4f46\u4ec5\u97008%\u53ef\u8bad\u7ec3\u53c2\u6570\uff1bPET\u626b\u63cfDice\u5206\u6570\u63d0\u534728%\uff0c\u9884\u540e\u4efb\u52a1\u4e00\u81f4\u6027\u6307\u6570\u63d0\u534710%-23%\u3002", "conclusion": "PEMMA\u6846\u67b6\u9ad8\u6548\u4e14\u7075\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u9002\u5e94\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u7a00\u7f3a\u6570\u636e\u573a\u666f\u3002"}}
{"id": "2504.13480", "pdf": "https://arxiv.org/pdf/2504.13480", "abs": "https://arxiv.org/abs/2504.13480", "authors": ["Minsu Koh", "Beom-Chul Park", "Heejo Kong", "Seong-Whan Lee"], "title": "Integrating Locality-Aware Attention with Transformers for General Geometry PDEs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by IJCNN 2025", "summary": "Neural operators have emerged as promising frameworks for learning mappings\ngoverned by partial differential equations (PDEs), serving as data-driven\nalternatives to traditional numerical methods. While methods such as the\nFourier neural operator (FNO) have demonstrated notable performance, their\nreliance on uniform grids restricts their applicability to complex geometries\nand irregular meshes. Recently, Transformer-based neural operators with linear\nattention mechanisms have shown potential in overcoming these limitations for\nlarge-scale PDE simulations. However, these approaches predominantly emphasize\nglobal feature aggregation, often overlooking fine-scale dynamics and localized\nPDE behaviors essential for accurate solutions. To address these challenges, we\npropose the Locality-Aware Attention Transformer (LA2Former), which leverages\nK-nearest neighbors for dynamic patchifying and integrates global-local\nattention for enhanced PDE modeling. By combining linear attention for\nefficient global context encoding with pairwise attention for capturing\nintricate local interactions, LA2Former achieves an optimal balance between\ncomputational efficiency and predictive accuracy. Extensive evaluations across\nsix benchmark datasets demonstrate that LA2Former improves predictive accuracy\nby over 50% relative to existing linear attention methods, while also\noutperforming full pairwise attention under optimal conditions. This work\nunderscores the critical importance of localized feature learning in advancing\nTransformer-based neural operators for solving PDEs on complex and irregular\ndomains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLA2Former\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7b97\u5b50\u5728\u590d\u6742\u51e0\u4f55\u548c\u7f51\u683c\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982FNO\u4f9d\u8d56\u5747\u5300\u7f51\u683c\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u51e0\u4f55\u548c\u7f51\u683c\u4e0a\u7684\u5e94\u7528\u3002Transformer\u65b9\u6cd5\u867d\u80fd\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u5ffd\u89c6\u4e86\u5c40\u90e8\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u63d0\u51faLA2Former\uff0c\u5229\u7528K\u8fd1\u90bb\u52a8\u6001\u5206\u5757\uff0c\u7ed3\u5408\u5168\u5c40-\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cLA2Former\u6bd4\u73b0\u6709\u7ebf\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534750%\u4ee5\u4e0a\uff0c\u5e76\u5728\u6700\u4f18\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5168\u914d\u5bf9\u6ce8\u610f\u529b\u65b9\u6cd5\u3002", "conclusion": "LA2Former\u5f3a\u8c03\u4e86\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u590d\u6742\u548c\u4e0d\u89c4\u5219\u57df\u4e0a\u7684PDE\u6c42\u89e3\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684Transformer\u795e\u7ecf\u7b97\u5b50\u3002"}}
{"id": "2504.13648", "pdf": "https://arxiv.org/pdf/2504.13648", "abs": "https://arxiv.org/abs/2504.13648", "authors": ["Uthman Baroudi", "Alala BaHamid", "Yasser Elalfy", "Ziad Al Alami"], "title": "Enhancing Pothole Detection and Characterization: Integrated Segmentation and Depth Estimation in Road Anomaly Systems", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "Road anomaly detection plays a crucial role in road maintenance and in\nenhancing the safety of both drivers and vehicles. Recent machine learning\napproaches for road anomaly detection have overcome the tedious and\ntime-consuming process of manual analysis and anomaly counting; however, they\noften fall short in providing a complete characterization of road potholes. In\nthis paper, we leverage transfer learning by adopting a pre-trained YOLOv8-seg\nmodel for the automatic characterization of potholes using digital images\ncaptured from a dashboard-mounted camera. Our work includes the creation of a\nnovel dataset, comprising both images and their corresponding depth maps,\ncollected from diverse road environments in Al-Khobar city and the KFUPM campus\nin Saudi Arabia. Our approach performs pothole detection and segmentation to\nprecisely localize potholes and calculate their area. Subsequently, the\nsegmented image is merged with its depth map to extract detailed depth\ninformation about the potholes. This integration of segmentation and depth data\noffers a more comprehensive characterization compared to previous deep\nlearning-based road anomaly detection systems. Overall, this method not only\nhas the potential to significantly enhance autonomous vehicle navigation by\nimproving the detection and characterization of road hazards but also assists\nroad maintenance authorities in responding more effectively to road damage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3YOLOv8-seg\u6a21\u578b\u7684\u9053\u8def\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u5206\u5272\u548c\u6df1\u5ea6\u56fe\u6570\u636e\uff0c\u5b9e\u73b0\u5bf9\u9053\u8def\u5751\u6d3c\u7684\u7cbe\u786e\u68c0\u6d4b\u4e0e\u7279\u5f81\u63cf\u8ff0\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9053\u8def\u5f02\u5e38\u68c0\u6d4b\u4e2d\u96be\u4ee5\u5168\u9762\u63cf\u8ff0\u5751\u6d3c\u7279\u5f81\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5751\u6d3c\u6df1\u5ea6\u4fe1\u606f\u7684\u63d0\u53d6\uff0c\u5f71\u54cd\u4e86\u68c0\u6d4b\u7684\u5168\u9762\u6027\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684YOLOv8-seg\u6a21\u578b\u8fdb\u884c\u5751\u6d3c\u68c0\u6d4b\u548c\u5206\u5272\uff0c\u7ed3\u5408\u6df1\u5ea6\u56fe\u6570\u636e\u63d0\u53d6\u5751\u6d3c\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u7279\u5f81\u63cf\u8ff0\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u5b9a\u4f4d\u5751\u6d3c\u5e76\u8ba1\u7b97\u5176\u9762\u79ef\uff0c\u540c\u65f6\u901a\u8fc7\u6df1\u5ea6\u56fe\u83b7\u53d6\u5751\u6d3c\u7684\u6df1\u5ea6\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u5168\u9762\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5bf9\u9053\u8def\u5371\u9669\u7684\u611f\u77e5\u80fd\u529b\uff0c\u8fd8\u4e3a\u9053\u8def\u7ef4\u62a4\u90e8\u95e8\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u54cd\u5e94\u624b\u6bb5\u3002"}}
{"id": "2504.13551", "pdf": "https://arxiv.org/pdf/2504.13551", "abs": "https://arxiv.org/abs/2504.13551", "authors": ["CheolWon Na", "YunSeok Choi", "Jee-Hyong Lee"], "title": "Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "NAACL 2025 Findings", "summary": "Many adversarial attack approaches are proposed to verify the vulnerability\nof language models. However, they require numerous queries and the information\non the target model. Even black-box attack methods also require the target\nmodel's output information. They are not applicable in real-world scenarios, as\nin hard black-box settings where the target model is closed and inaccessible.\nEven the recently proposed hard black-box attacks still require many queries\nand demand extremely high costs for training adversarial generators. To address\nthese challenges, we propose Q-faker (Query-free Hard Black-box Attacker), a\nnovel and efficient method that generates adversarial examples without\naccessing the target model. To avoid accessing the target model, we use a\nsurrogate model instead. The surrogate model generates adversarial sentences\nfor a target-agnostic attack. During this process, we leverage controlled\ngeneration techniques. We evaluate our proposed method on eight datasets.\nExperimental results demonstrate our method's effectiveness including high\ntransferability and the high quality of the generated adversarial examples, and\nprove its practical in hard black-box settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u67e5\u8be2\u76ee\u6807\u6a21\u578b\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5Q-faker\uff0c\u5229\u7528\u66ff\u4ee3\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u67e5\u8be2\u548c\u9ad8\u6210\u672c\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u67e5\u8be2\u6216\u76ee\u6807\u6a21\u578b\u4fe1\u606f\uff0c\u4e0d\u9002\u7528\u4e8e\u5b8c\u5168\u5c01\u95ed\u7684\u9ed1\u76d2\u573a\u666f\u3002", "method": "\u4f7f\u7528\u66ff\u4ee3\u6a21\u578b\u548c\u53ef\u63a7\u751f\u6210\u6280\u672f\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u907f\u514d\u76f4\u63a5\u8bbf\u95ee\u76ee\u6807\u6a21\u578b\u3002", "result": "\u5728\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5305\u62ec\u9ad8\u8fc1\u79fb\u6027\u548c\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\u751f\u6210\u3002", "conclusion": "Q-faker\u5728\u786c\u9ed1\u76d2\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13650", "pdf": "https://arxiv.org/pdf/2504.13650", "abs": "https://arxiv.org/abs/2504.13650", "authors": ["Sijing Li", "Tianwei Lin", "Lingshuai Lin", "Wenqiao Zhang", "Jiang Liu", "Xiaoda Yang", "Juncheng Li", "Yucheng He", "Xiaohui Song", "Jun Xiao", "Yueting Zhuang", "Beng Chin Ooi"], "title": "EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with Tailored Dataset, Benchmark and Model", "categories": ["cs.CV"], "comment": null, "summary": "Medical Large Vision-Language Models (Med-LVLMs) demonstrate significant\npotential in healthcare, but their reliance on general medical data and\ncoarse-grained global visual understanding limits them in intelligent\nophthalmic diagnosis. Currently, intelligent ophthalmic diagnosis faces three\nmajor challenges: (i) Data. The lack of deeply annotated, high-quality,\nmulti-modal ophthalmic visual instruction data; (ii) Benchmark. The absence of\na comprehensive and systematic benchmark for evaluating diagnostic performance;\n(iii) Model. The difficulty of adapting holistic visual architectures to\nfine-grained, region-specific ophthalmic lesion identification. In this paper,\nwe propose the Eyecare Kit, which systematically tackles the aforementioned\nthree key challenges with the tailored dataset, benchmark and model: First, we\nconstruct a multi-agent data engine with real-life ophthalmology data to\nproduce Eyecare-100K, a high-quality ophthalmic visual instruction dataset.\nSubsequently, we design Eyecare-Bench, a benchmark that comprehensively\nevaluates the overall performance of LVLMs on intelligent ophthalmic diagnosis\ntasks across multiple dimensions. Finally, we develop the EyecareGPT, optimized\nfor fine-grained ophthalmic visual understanding thoroughly, which incorporates\nan adaptive resolution mechanism and a layer-wise dense connector. Extensive\nexperimental results indicate that the EyecareGPT achieves state-of-the-art\nperformance in a range of ophthalmic tasks, underscoring its significant\npotential for the advancement of open research in intelligent ophthalmic\ndiagnosis. Our project is available at https://github.com/DCDmllm/EyecareGPT.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEyecare Kit\uff0c\u89e3\u51b3\u773c\u79d1\u667a\u80fd\u8bca\u65ad\u4e2d\u7684\u6570\u636e\u3001\u57fa\u51c6\u548c\u6a21\u578b\u4e09\u5927\u6311\u6218\uff0c\u5305\u62ec\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6Eyecare-100K\u3001\u8bbe\u8ba1\u8bc4\u4f30\u57fa\u51c6Eyecare-Bench\u548c\u5f00\u53d1\u4f18\u5316\u6a21\u578bEyecareGPT\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Med-LVLMs\uff09\u5728\u773c\u79d1\u667a\u80fd\u8bca\u65ad\u4e2d\u5b58\u5728\u6570\u636e\u4e0d\u8db3\u3001\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u51c6\u548c\u6a21\u578b\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u591a\u4ee3\u7406\u6570\u636e\u5f15\u64ce\u751f\u6210Eyecare-100K\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1Eyecare-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u53d1\u4f18\u5316\u6a21\u578bEyecareGPT\uff0c\u5305\u542b\u81ea\u9002\u5e94\u5206\u8fa8\u7387\u673a\u5236\u548c\u5206\u5c42\u5bc6\u96c6\u8fde\u63a5\u5668\u3002", "result": "EyecareGPT\u5728\u591a\u9879\u773c\u79d1\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "Eyecare Kit\u4e3a\u773c\u79d1\u667a\u80fd\u8bca\u65ad\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u6f5c\u5728\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2504.13644", "pdf": "https://arxiv.org/pdf/2504.13644", "abs": "https://arxiv.org/abs/2504.13644", "authors": ["Gabriel Freedman", "Francesca Toni"], "title": "Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs", "categories": ["cs.AI", "cs.CL"], "comment": "8 pages, 4 figures", "summary": "Advances in the general capabilities of large language models (LLMs) have led\nto their use for information retrieval, and as components in automated decision\nsystems. A faithful representation of probabilistic reasoning in these models\nmay be essential to ensure trustworthy, explainable and effective performance\nin these tasks. Despite previous work suggesting that LLMs can perform complex\nreasoning and well-calibrated uncertainty quantification, we find that current\nversions of this class of model lack the ability to provide rational and\ncoherent representations of probabilistic beliefs. To demonstrate this, we\nintroduce a novel dataset of claims with indeterminate truth values and apply a\nnumber of well-established techniques for uncertainty quantification to measure\nthe ability of LLM's to adhere to fundamental properties of probabilistic\nreasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6982\u7387\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5408\u7406\u4e14\u4e00\u81f4\u7684\u6982\u7387\u4fe1\u5ff5\u8868\u793a\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u6982\u7387\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u4fdd\u5176\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u81ea\u52a8\u51b3\u7b56\u7cfb\u7edf\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u5177\u6709\u4e0d\u786e\u5b9a\u771f\u503c\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5e94\u7528\u591a\u79cd\u6210\u719f\u7684\u6982\u7387\u91cf\u5316\u6280\u672f\u8bc4\u4f30LLMs\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u5f53\u524dLLMs\u65e0\u6cd5\u6ee1\u8db3\u6982\u7387\u63a8\u7406\u7684\u57fa\u672c\u6027\u8d28\u3002", "conclusion": "LLMs\u5728\u6982\u7387\u63a8\u7406\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u4fe1\u548c\u6709\u6548\u7684\u5e94\u7528\u3002"}}
{"id": "2504.13682", "pdf": "https://arxiv.org/pdf/2504.13682", "abs": "https://arxiv.org/abs/2504.13682", "authors": ["Mengyuan Li", "Changhong Fu", "Ziyu Lu", "Zijie Zhang", "Haobo Zuo", "Liangliang Yao"], "title": "AnyTSR: Any-Scale Thermal Super-Resolution for UAV", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Thermal imaging can greatly enhance the application of intelligent unmanned\naerial vehicles (UAV) in challenging environments. However, the inherent low\nresolution of thermal sensors leads to insufficient details and blurred\nboundaries. Super-resolution (SR) offers a promising solution to address this\nissue, while most existing SR methods are designed for fixed-scale SR. They are\ncomputationally expensive and inflexible in practical applications. To address\nabove issues, this work proposes a novel any-scale thermal SR method (AnyTSR)\nfor UAV within a single model. Specifically, a new image encoder is proposed to\nexplicitly assign specific feature code to enable more accurate and flexible\nrepresentation. Additionally, by effectively embedding coordinate offset\ninformation into the local feature ensemble, an innovative any-scale upsampler\nis proposed to better understand spatial relationships and reduce artifacts.\nMoreover, a novel dataset (UAV-TSR), covering both land and water scenes, is\nconstructed for thermal SR tasks. Experimental results demonstrate that the\nproposed method consistently outperforms state-of-the-art methods across all\nscaling factors as well as generates more accurate and detailed high-resolution\nimages. The code is located at https://github.com/vision4robotics/AnyTSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4efb\u610f\u5c3a\u5ea6\u70ed\u6210\u50cf\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff08AnyTSR\uff09\uff0c\u901a\u8fc7\u5355\u6a21\u578b\u89e3\u51b3\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u5206\u8fa8\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7ec6\u8282\u548c\u8fb9\u754c\u6e05\u6670\u5ea6\u3002", "motivation": "\u70ed\u6210\u50cf\u4f20\u611f\u5668\u5206\u8fa8\u7387\u4f4e\u5bfc\u81f4\u7ec6\u8282\u4e0d\u8db3\u548c\u8fb9\u754c\u6a21\u7cca\uff0c\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u591a\u4e3a\u56fa\u5b9a\u5c3a\u5ea6\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u56fe\u50cf\u7f16\u7801\u5668\u4ee5\u5206\u914d\u7279\u5b9a\u7279\u5f81\u7801\uff0c\u5e76\u63d0\u51fa\u521b\u65b0\u7684\u4efb\u610f\u5c3a\u5ea6\u4e0a\u91c7\u6837\u5668\uff0c\u5d4c\u5165\u5750\u6807\u504f\u79fb\u4fe1\u606f\u4ee5\u51cf\u5c11\u4f2a\u5f71\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u5c3a\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u66f4\u51c6\u786e\u548c\u8be6\u7ec6\u3002", "conclusion": "AnyTSR\u4e3a\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2504.13667", "pdf": "https://arxiv.org/pdf/2504.13667", "abs": "https://arxiv.org/abs/2504.13667", "authors": ["Russell Beale"], "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "Accepted for IDC 2025. Citation: Russell Beale. 2025. Large Language\n  Models Will Change The Way Children Think About Technology And Impact Every\n  Interaction Paradigm. In Proceedings of Interaction Design and Children\n  Conference (IDC2025). ACM, New York, NY, USA", "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts\nof Large Language Models on how we children learn and how they will expect to\ninteract with technology. We review the effects of LLMs on education so far,\nand make the case that these effects are minor compared to the upcoming changes\nthat are occurring. We present a small scenario and self-ethnographic study\ndemonstrating the effects of these changes, and define five significant\nconsiderations that interactive systems designers will have to accommodate in\nthe future.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u513f\u7ae5\u5b66\u4e60\u548c\u4e0e\u6280\u672f\u4e92\u52a8\u65b9\u5f0f\u7684\u6f5c\u5728\u6df1\u8fdc\u5f71\u54cd\uff0c\u8ba4\u4e3a\u5f53\u524d\u5f71\u54cd\u8f83\u5c0f\uff0c\u4f46\u672a\u6765\u53d8\u5316\u5de8\u5927\u3002", "motivation": "\u7814\u7a76LLMs\u5bf9\u6559\u80b2\u7684\u6f5c\u5728\u53d8\u9769\u6027\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u4ea4\u4e92\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u5c0f\u89c4\u6a21\u573a\u666f\u548c\u81ea\u6211\u6c11\u65cf\u5fd7\u7814\u7a76\u5c55\u793a\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u4e94\u4e2a\u91cd\u8981\u8bbe\u8ba1\u8003\u8651\u3002", "result": "LLMs\u5c06\u663e\u8457\u6539\u53d8\u513f\u7ae5\u5b66\u4e60\u65b9\u5f0f\uff0c\u8bbe\u8ba1\u5e08\u9700\u9002\u5e94\u672a\u6765\u9700\u6c42\u3002", "conclusion": "LLMs\u5bf9\u6559\u80b2\u7684\u5f71\u54cd\u8fdc\u8d85\u5f53\u524d\u6c34\u5e73\uff0c\u672a\u6765\u4ea4\u4e92\u7cfb\u7edf\u9700\u8003\u8651\u4e94\u5927\u56e0\u7d20\u3002"}}
{"id": "2504.13690", "pdf": "https://arxiv.org/pdf/2504.13690", "abs": "https://arxiv.org/abs/2504.13690", "authors": ["Muhammad Usama", "Syeda Aisha Asim", "Syed Bilal Ali", "Syed Talal Wasim", "Umair Bin Mansoor"], "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2304.10592,\n  arXiv:2301.12597 by other authors", "summary": "Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u572819\u79cd\u56fe\u50cf\u635f\u574f\u7c7b\u578b\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u5176\u5728\u6587\u672c\u8bc6\u522b\u548c\u5bf9\u8c61\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e0d\u540c\u8106\u5f31\u6027\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u89c6\u89c9\u4e0e\u6587\u672c\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u5e38\u89c1\u56fe\u50cf\u635f\u574f\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5f15\u5165TextVQA-C\u548cGQA-C\u4e24\u4e2a\u65b0\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u635f\u574f\u5bf9\u573a\u666f\u6587\u672c\u7406\u89e3\u548c\u5bf9\u8c61\u63a8\u7406\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u635f\u574f\u7c7b\u578b\u7684\u9891\u57df\u7279\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8eTransformer\u7684VLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u8106\u5f31\u6027\u6a21\u5f0f\uff1a\u6587\u672c\u8bc6\u522b\u5728\u6a21\u7cca\u548c\u96ea\u635f\u574f\u4e0b\u8868\u73b0\u6700\u5dee\uff0c\u800c\u5bf9\u8c61\u63a8\u7406\u5bf9\u971c\u51bb\u548c\u8109\u51b2\u566a\u58f0\u66f4\u654f\u611f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u5177\u9c81\u68d2\u6027\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u56fe\u50cf\u635f\u574f\u95ee\u9898\u3002"}}
{"id": "2504.13707", "pdf": "https://arxiv.org/pdf/2504.13707", "abs": "https://arxiv.org/abs/2504.13707", "authors": ["Yichen Wu", "Xudong Pan", "Geng Hong", "Min Yang"], "title": "OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the general capabilities of large language models (LLMs) improve and agent\napplications become more widespread, the underlying deception risks urgently\nrequire systematic evaluation and effective oversight. Unlike existing\nevaluation which uses simulated games or presents limited choices, we introduce\nOpenDeception, a novel deception evaluation framework with an open-ended\nscenario dataset. OpenDeception jointly evaluates both the deception intention\nand capabilities of LLM-based agents by inspecting their internal reasoning\nprocess. Specifically, we construct five types of common use cases where LLMs\nintensively interact with the user, each consisting of ten diverse, concrete\nscenarios from the real world. To avoid ethical concerns and costs of high-risk\ndeceptive interactions with human testers, we propose to simulate the\nmulti-turn dialogue via agent simulation. Extensive evaluation of eleven\nmainstream LLMs on OpenDeception highlights the urgent need to address\ndeception risks and security concerns in LLM-based agents: the deception\nintention ratio across the models exceeds 80%, while the deception success rate\nsurpasses 50%. Furthermore, we observe that LLMs with stronger capabilities do\nexhibit a higher risk of deception, which calls for more alignment efforts on\ninhibiting deceptive behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faOpenDeception\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u6b3a\u9a97\u610f\u56fe\u548c\u80fd\u529b\uff0c\u53d1\u73b0\u4e3b\u6d41LLM\u7684\u6b3a\u9a97\u610f\u56fe\u6bd4\u4f8b\u8d85\u8fc780%\uff0c\u6210\u529f\u7387\u8d85\u8fc750%\uff0c\u4e14\u80fd\u529b\u8d8a\u5f3a\u7684LLM\u6b3a\u9a97\u98ce\u9669\u8d8a\u9ad8\u3002", "motivation": "\u968f\u7740LLM\u80fd\u529b\u7684\u63d0\u5347\u548c\u4ee3\u7406\u5e94\u7528\u7684\u666e\u53ca\uff0c\u5176\u6f5c\u5728\u7684\u6b3a\u9a97\u98ce\u9669\u4e9f\u9700\u7cfb\u7edf\u8bc4\u4f30\u548c\u6709\u6548\u76d1\u7ba1\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u591a\u57fa\u4e8e\u6a21\u62df\u6e38\u620f\u6216\u6709\u9650\u9009\u62e9\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u771f\u5b9e\u573a\u666f\u3002", "method": "\u63d0\u51faOpenDeception\u6846\u67b6\uff0c\u901a\u8fc7\u5f00\u653e\u573a\u666f\u6570\u636e\u96c6\u548c\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u5206\u6790\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u6b3a\u9a97\u610f\u56fe\u548c\u80fd\u529b\u3002\u6784\u5efa\u4e94\u7c7b\u5e38\u89c1\u7528\u4f8b\uff0c\u6a21\u62df\u591a\u8f6e\u5bf9\u8bdd\u4ee5\u907f\u514d\u4f26\u7406\u98ce\u9669\u3002", "result": "\u5bf911\u79cd\u4e3b\u6d41LLM\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6b3a\u9a97\u610f\u56fe\u6bd4\u4f8b\u8d85\u8fc780%\uff0c\u6210\u529f\u7387\u8d85\u8fc750%\uff0c\u4e14\u80fd\u529b\u66f4\u5f3a\u7684LLM\u6b3a\u9a97\u98ce\u9669\u66f4\u9ad8\u3002", "conclusion": "LLM\u4ee3\u7406\u7684\u6b3a\u9a97\u98ce\u9669\u4e9f\u9700\u5173\u6ce8\uff0c\u9700\u52a0\u5f3a\u5bf9\u5176\u6b3a\u9a97\u884c\u4e3a\u7684\u6291\u5236\u548c\u5bf9\u9f50\u52aa\u529b\u3002"}}
{"id": "2504.13692", "pdf": "https://arxiv.org/pdf/2504.13692", "abs": "https://arxiv.org/abs/2504.13692", "authors": ["Qianghua Chen", "Huiyu Wang", "Li Ming", "Ying Zhao"], "title": "Zebrafish Counting Using Event Stream Data", "categories": ["cs.CV"], "comment": null, "summary": "Zebrafish share a high degree of homology with human genes and are commonly\nused as model organism in biomedical research. For medical laboratories,\ncounting zebrafish is a daily task. Due to the tiny size of zebrafish, manual\nvisual counting is challenging. Existing counting methods are either not\napplicable to small fishes or have too many limitations. The paper proposed a\nzebrafish counting algorithm based on the event stream data. Firstly, an event\ncamera is applied for data acquisition. Secondly, camera calibration and image\nfusion were preformed successively. Then, the trajectory information was used\nto improve the counting accuracy. Finally, the counting results were averaged\nover an empirical of period and rounded up to get the final results. To\nevaluate the accuracy of the algorithm, 20 zebrafish were put in a four-liter\nbreeding tank. Among 100 counting trials, the average accuracy reached 97.95%.\nAs compared with traditional algorithms, the proposed one offers a simpler\nimplementation and achieves higher accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6d41\u6570\u636e\u7684\u6591\u9a6c\u9c7c\u8ba1\u6570\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u91c7\u96c6\u6570\u636e\uff0c\u7ed3\u5408\u8f68\u8ff9\u4fe1\u606f\u63d0\u9ad8\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u6700\u7ec8\u5e73\u5747\u51c6\u786e\u7387\u8fbe97.95%\u3002", "motivation": "\u6591\u9a6c\u9c7c\u57fa\u56e0\u4e0e\u4eba\u7c7b\u9ad8\u5ea6\u540c\u6e90\uff0c\u5e38\u7528\u4e8e\u751f\u7269\u533b\u5b66\u7814\u7a76\uff0c\u4f46\u56e0\u5176\u4f53\u578b\u5fae\u5c0f\uff0c\u4eba\u5de5\u8ba1\u6570\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u91c7\u96c6\u6570\u636e\uff0c\u8fdb\u884c\u76f8\u673a\u6821\u51c6\u548c\u56fe\u50cf\u878d\u5408\uff0c\u5229\u7528\u8f68\u8ff9\u4fe1\u606f\u63d0\u5347\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u6700\u7ec8\u901a\u8fc7\u7ecf\u9a8c\u5468\u671f\u5e73\u5747\u548c\u56db\u820d\u4e94\u5165\u5f97\u5230\u7ed3\u679c\u3002", "result": "\u5728100\u6b21\u8ba1\u6570\u8bd5\u9a8c\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523097.95%\uff0c\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5b9e\u73b0\u7b80\u5355\u4e14\u51c6\u786e\u6027\u9ad8\uff0c\u9002\u7528\u4e8e\u6591\u9a6c\u9c7c\u8ba1\u6570\u4efb\u52a1\u3002"}}
{"id": "2504.13752", "pdf": "https://arxiv.org/pdf/2504.13752", "abs": "https://arxiv.org/abs/2504.13752", "authors": ["Benjamin Cohen-Wang", "Yung-Sung Chuang", "Aleksander Madry"], "title": "Learning to Attribute with Attention", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Given a sequence of tokens generated by a language model, we may want to\nidentify the preceding tokens that influence the model to generate this\nsequence. Performing such token attribution is expensive; a common approach is\nto ablate preceding tokens and directly measure their effects. To reduce the\ncost of token attribution, we revisit attention weights as a heuristic for how\na language model uses previous tokens. Naive approaches to attribute model\nbehavior with attention (e.g., averaging attention weights across attention\nheads to estimate a token's influence) have been found to be unreliable. To\nattain faithful attributions, we propose treating the attention weights of\ndifferent attention heads as features. This way, we can learn how to\neffectively leverage attention weights for attribution (using signal from\nablations). Our resulting method, Attribution with Attention (AT2), reliably\nperforms on par with approaches that involve many ablations, while being\nsignificantly more efficient. To showcase the utility of AT2, we use it to\nprune less important parts of a provided context in a question answering\nsetting, improving answer quality. We provide code for AT2 at\nhttps://github.com/MadryLab/AT2 .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u4ee4\u724c\u5f52\u56e0\u65b9\u6cd5AT2\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u6743\u91cd\u4f5c\u4e3a\u7279\u5f81\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u5f52\u56e0\uff0c\u6027\u80fd\u4e0e\u9ad8\u6210\u672c\u7684\u6d88\u878d\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e8f\u5217\u65f6\uff0c\u8bc6\u522b\u5f71\u54cd\u751f\u6210\u7684\u524d\u5e8f\u4ee4\u724c\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u65b9\u6cd5\u4e0d\u53ef\u9760\u3002", "method": "\u5c06\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u6743\u91cd\u4f5c\u4e3a\u7279\u5f81\uff0c\u901a\u8fc7\u5b66\u4e60\uff08\u5229\u7528\u6d88\u878d\u4fe1\u53f7\uff09\u6709\u6548\u5229\u7528\u6ce8\u610f\u529b\u6743\u91cd\u8fdb\u884c\u5f52\u56e0\u3002", "result": "\u63d0\u51fa\u7684AT2\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u9ad8\u6210\u672c\u6d88\u878d\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u53ef\u7528\u4e8e\u4e0a\u4e0b\u6587\u4fee\u526a\u4ee5\u63d0\u5347\u95ee\u7b54\u8d28\u91cf\u3002", "conclusion": "AT2\u662f\u4e00\u79cd\u9ad8\u6548\u53ef\u9760\u7684\u4ee4\u724c\u5f52\u56e0\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u5206\u6790\u3002"}}
{"id": "2504.13710", "pdf": "https://arxiv.org/pdf/2504.13710", "abs": "https://arxiv.org/abs/2504.13710", "authors": ["Heng Liu", "Guanghui Li", "Mingqi Gao", "Xiantong Zhen", "Feng Zheng", "Yang Wang"], "title": "Few-Shot Referring Video Single- and Multi-Object Segmentation via Cross-Modal Affinity with Instance Sequence Matching", "categories": ["cs.CV"], "comment": "23 pages, 10 figures", "summary": "Referring video object segmentation (RVOS) aims to segment objects in videos\nguided by natural language descriptions. We propose FS-RVOS, a\nTransformer-based model with two key components: a cross-modal affinity module\nand an instance sequence matching strategy, which extends FS-RVOS to\nmulti-object segmentation (FS-RVMOS). Experiments show FS-RVOS and FS-RVMOS\noutperform state-of-the-art methods across diverse benchmarks, demonstrating\nsuperior robustness and accuracy.", "AI": {"tldr": "FS-RVOS\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u4eb2\u548c\u6a21\u5757\u548c\u5b9e\u4f8b\u5e8f\u5217\u5339\u914d\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u5bf9\u8c61\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u95ee\u9898\uff0c\u63d0\u5347\u591a\u5bf9\u8c61\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u4eb2\u548c\u6a21\u5757\u548c\u5b9e\u4f8b\u5e8f\u5217\u5339\u914d\u7b56\u7565\uff0c\u6269\u5c55\u4e3a\u591a\u5bf9\u8c61\u5206\u5272\u6a21\u578bFS-RVMOS\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "FS-RVOS\u548cFS-RVMOS\u5728\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u5bf9\u8c61\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13756", "pdf": "https://arxiv.org/pdf/2504.13756", "abs": "https://arxiv.org/abs/2504.13756", "authors": ["Dmitrii Kharlapenko", "Stepan Shabalin", "Fazl Barez", "Arthur Conmy", "Neel Nanda"], "title": "Scaling sparse feature circuit finding for in-context learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) are a popular tool for interpreting large language\nmodel activations, but their utility in addressing open questions in\ninterpretability remains unclear. In this work, we demonstrate their\neffectiveness by using SAEs to deepen our understanding of the mechanism behind\nin-context learning (ICL). We identify abstract SAE features that (i) encode\nthe model's knowledge of which task to execute and (ii) whose latent vectors\ncausally induce the task zero-shot. This aligns with prior work showing that\nICL is mediated by task vectors. We further demonstrate that these task vectors\nare well approximated by a sparse sum of SAE latents, including these\ntask-execution features. To explore the ICL mechanism, we adapt the sparse\nfeature circuits methodology of Marks et al. (2024) to work for the much larger\nGemma-1 2B model, with 30 times as many parameters, and to the more complex\ntask of ICL. Through circuit finding, we discover task-detecting features with\ncorresponding SAE latents that activate earlier in the prompt, that detect when\ntasks have been performed. They are causally linked with task-execution\nfeatures through the attention and MLP sublayers.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u7528\u4e8e\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\uff0c\u4f46\u5176\u5728\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u5f00\u653e\u95ee\u9898\u4e2d\u7684\u6548\u7528\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u901a\u8fc7SAEs\u52a0\u6df1\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u673a\u5236\u7684\u7406\u89e3\uff0c\u53d1\u73b0\u62bd\u8c61SAE\u7279\u5f81\u7f16\u7801\u4efb\u52a1\u6267\u884c\u77e5\u8bc6\uff0c\u5176\u6f5c\u5728\u5411\u91cf\u56e0\u679c\u8bf1\u5bfc\u4efb\u52a1\u96f6\u6837\u672c\u6267\u884c\u3002", "motivation": "\u63a2\u7d22\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u89e3\u91ca\u5927\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u5b9e\u9645\u6548\u7528\uff0c\u5c24\u5176\u662f\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u7684\u7406\u89e3\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790Gemma-1 2B\u6a21\u578b\uff0c\u7ed3\u5408\u7a00\u758f\u7279\u5f81\u7535\u8def\u65b9\u6cd5\uff0c\u7814\u7a76\u4efb\u52a1\u68c0\u6d4b\u7279\u5f81\u53ca\u5176\u4e0e\u4efb\u52a1\u6267\u884c\u7279\u5f81\u7684\u56e0\u679c\u8054\u7cfb\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u68c0\u6d4b\u7279\u5f81\u53ca\u5176\u6f5c\u5728\u5411\u91cf\u80fd\u56e0\u679c\u8bf1\u5bfc\u4efb\u52a1\u6267\u884c\uff0c\u4e14\u4efb\u52a1\u5411\u91cf\u53ef\u7531\u7a00\u758fSAE\u6f5c\u5728\u5411\u91cf\u8fd1\u4f3c\u8868\u793a\u3002", "conclusion": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6709\u6548\u63ed\u793a\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u673a\u5236\uff0c\u4efb\u52a1\u68c0\u6d4b\u4e0e\u6267\u884c\u7279\u5f81\u901a\u8fc7\u6ce8\u610f\u529b\u4e0eMLP\u5b50\u5c42\u56e0\u679c\u5173\u8054\u3002"}}
{"id": "2504.13717", "pdf": "https://arxiv.org/pdf/2504.13717", "abs": "https://arxiv.org/abs/2504.13717", "authors": ["Gianluca Carloni"], "title": "Human-aligned Deep Learning: Explainability, Causality, and Biological Inspiration", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "q-bio.NC", "I.2; I.2.6; I.4; I.4.7; I.5; J.3; J.6"], "comment": "Personal adaptation and expansion of doctoral thesis (originally\n  submitted in Oct 2024, revisioned in Jan 2025)", "summary": "This work aligns deep learning (DL) with human reasoning capabilities and\nneeds to enable more efficient, interpretable, and robust image classification.\nWe approach this from three perspectives: explainability, causality, and\nbiological vision. Introduction and background open this work before diving\ninto operative chapters. First, we assess neural networks' visualization\ntechniques for medical images and validate an explainable-by-design method for\nbreast mass classification. A comprehensive review at the intersection of XAI\nand causality follows, where we introduce a general scaffold to organize past\nand future research, laying the groundwork for our second perspective. In the\ncausality direction, we propose novel modules that exploit feature\nco-occurrence in medical images, leading to more effective and explainable\npredictions. We further introduce CROCODILE, a general framework that\nintegrates causal concepts, contrastive learning, feature disentanglement, and\nprior knowledge to enhance generalization. Lastly, we explore biological\nvision, examining how humans recognize objects, and propose CoCoReco, a\nconnectivity-inspired network with context-aware attention mechanisms. Overall,\nour key findings include: (i) simple activation maximization lacks insight for\nmedical imaging DL models; (ii) prototypical-part learning is effective and\nradiologically aligned; (iii) XAI and causal ML are deeply connected; (iv) weak\ncausal signals can be leveraged without a priori information to improve\nperformance and interpretability; (v) our framework generalizes across medical\ndomains and out-of-distribution data; (vi) incorporating biological circuit\nmotifs improves human-aligned recognition. This work contributes toward\nhuman-aligned DL and highlights pathways to bridge the gap between research and\nclinical adoption, with implications for improved trust, diagnostic accuracy,\nand safe deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u3001\u56e0\u679c\u6027\u548c\u751f\u7269\u89c6\u89c9\u4e09\u4e2a\u89c6\u89d2\uff0c\u5c06\u6df1\u5ea6\u5b66\u4e60\u4e0e\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\u5bf9\u9f50\uff0c\u63d0\u51fa\u591a\u79cd\u65b9\u6cd5\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4ee5\u7f29\u5c0f\u7814\u7a76\u4e0e\u4e34\u5e8a\u5e94\u7528\u7684\u5dee\u8ddd\u3002", "method": "1. \u8bc4\u4f30\u795e\u7ecf\u7f51\u7edc\u53ef\u89c6\u5316\u6280\u672f\u5e76\u63d0\u51fa\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\u65b9\u6cd5\uff1b2. \u63d0\u51fa\u5229\u7528\u7279\u5f81\u5171\u73b0\u7684\u56e0\u679c\u6a21\u5757\uff1b3. \u5f00\u53d1CROCODILE\u6846\u67b6\u6574\u5408\u56e0\u679c\u6982\u5ff5\u4e0e\u5bf9\u6bd4\u5b66\u4e60\uff1b4. \u63d0\u51faCoCoReco\u7f51\u7edc\u6a21\u62df\u751f\u7269\u89c6\u89c9\u3002", "result": "1. \u6fc0\u6d3b\u6700\u5927\u5316\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u6548\u679c\u6709\u9650\uff1b2. \u539f\u578b\u5b66\u4e60\u6709\u6548\u4e14\u7b26\u5408\u653e\u5c04\u5b66\uff1b3. XAI\u4e0e\u56e0\u679cML\u7d27\u5bc6\u5173\u8054\uff1b4. \u5f31\u56e0\u679c\u4fe1\u53f7\u53ef\u63d0\u5347\u6027\u80fd\uff1b5. \u6846\u67b6\u5177\u6709\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff1b6. \u751f\u7269\u7535\u8def\u6a21\u7ec4\u63d0\u5347\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u4eba\u7c7b\u5bf9\u9f50\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u63d0\u5347\u4e34\u5e8a\u4fe1\u4efb\u3001\u8bca\u65ad\u51c6\u786e\u6027\u548c\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u8def\u5f84\u3002"}}
{"id": "2504.13818", "pdf": "https://arxiv.org/pdf/2504.13818", "abs": "https://arxiv.org/abs/2504.13818", "authors": ["Yixuan Even Xu", "Yash Savani", "Fei Fang", "Zico Kolter"], "title": "Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages, 1 figure", "summary": "Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing\nreasoning capabilities in large language models, but faces a fundamental\nasymmetry in computation and memory requirements: inference is embarrassingly\nparallel with a minimal memory footprint, while policy updates require\nextensive synchronization and are memory-intensive. To address this asymmetry,\nwe introduce PODS (Policy Optimization with Down-Sampling), a framework that\nstrategically decouples these phases by generating numerous rollouts in\nparallel but updating only on an informative subset. Within this framework, we\ndevelop max-variance down-sampling, a theoretically motivated method that\nselects rollouts with maximally diverse reward signals. We prove that this\napproach has an efficient algorithmic solution, and empirically demonstrate\nthat GRPO with PODS using max-variance down-sampling achieves superior\nperformance over standard GRPO on the GSM8K benchmark.", "AI": {"tldr": "PODS\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u751f\u6210\u5927\u91cfrollout\u4f46\u4ec5\u66f4\u65b0\u4fe1\u606f\u4e30\u5bcc\u7684\u5b50\u96c6\uff0c\u89e3\u51b3\u4e86RL\u4e2d\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0cmax-variance down-sampling\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u8ba1\u7b97\u4e0e\u5185\u5b58\u9700\u6c42\u4e0d\u5bf9\u79f0\u7684\u95ee\u9898\uff0c\u63a8\u7406\u9636\u6bb5\u5e76\u884c\u4e14\u5185\u5b58\u9700\u6c42\u4f4e\uff0c\u800c\u7b56\u7565\u66f4\u65b0\u9700\u540c\u6b65\u4e14\u5185\u5b58\u5bc6\u96c6\u3002", "method": "\u63d0\u51faPODS\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u751f\u6210rollout\u4f46\u4ec5\u66f4\u65b0\u4fe1\u606f\u5b50\u96c6\uff0c\u5e76\u5f00\u53d1max-variance down-sampling\u65b9\u6cd5\u9009\u62e9\u591a\u6837\u6027\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728GSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPODS\u7ed3\u5408max-variance down-sampling\u7684GRPO\u4f18\u4e8e\u6807\u51c6GRPO\u3002", "conclusion": "PODS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RL\u4e2d\u7684\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5e76\u901a\u8fc7max-variance down-sampling\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.13726", "pdf": "https://arxiv.org/pdf/2504.13726", "abs": "https://arxiv.org/abs/2504.13726", "authors": ["Lin Yuan", "Xiaowan Li", "Yan Zhang", "Jiawei Zhang", "Hongbo Li", "Xinbo Gao"], "title": "MLEP: Multi-granularity Local Entropy Patterns for Universal AI-generated Image Detection", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Advancements in image generation technologies have raised significant\nconcerns about their potential misuse, such as producing misinformation and\ndeepfakes. Therefore, there is an urgent need for effective methods to detect\nAI-generated images (AIGI). Despite progress in AIGI detection, achieving\nreliable performance across diverse generation models and scenes remains\nchallenging due to the lack of source-invariant features and limited\ngeneralization capabilities in existing methods. In this work, we explore the\npotential of using image entropy as a cue for AIGI detection and propose\nMulti-granularity Local Entropy Patterns (MLEP), a set of entropy feature maps\ncomputed across shuffled small patches over multiple image scaled. MLEP\ncomprehensively captures pixel relationships across dimensions and scales while\nsignificantly disrupting image semantics, reducing potential content bias.\nLeveraging MLEP, a robust CNN-based classifier for AIGI detection can be\ntrained. Extensive experiments conducted in an open-world scenario, evaluating\nimages synthesized by 32 distinct generative models, demonstrate significant\nimprovements over state-of-the-art methods in both accuracy and generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u71b5\u7684\u591a\u7c92\u5ea6\u5c40\u90e8\u71b5\u6a21\u5f0f\uff08MLEP\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff08AIGI\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6837\u751f\u6210\u6a21\u578b\u548c\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "AI\u751f\u6210\u56fe\u50cf\uff08AIGI\uff09\u7684\u6ee5\u7528\uff08\u5982\u865a\u5047\u4fe1\u606f\u548c\u6df1\u5ea6\u4f2a\u9020\uff09\u5f15\u53d1\u4e86\u5bf9\u5176\u68c0\u6d4b\u7684\u8feb\u5207\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6e90\u4e0d\u53d8\u7279\u5f81\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u800c\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51faMLEP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u591a\u5c3a\u5ea6\u56fe\u50cf\u5757\u7684\u91cd\u6392\u71b5\u7279\u5f81\u56fe\uff0c\u5168\u9762\u6355\u6349\u50cf\u7d20\u5173\u7cfb\u5e76\u51cf\u5c11\u5185\u5bb9\u504f\u5dee\uff0c\u7ed3\u5408CNN\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u572832\u79cd\u751f\u6210\u6a21\u578b\u5408\u6210\u7684\u56fe\u50cf\u4e0a\u6d4b\u8bd5\uff0cMLEP\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MLEP\u4e3aAIGI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13837", "pdf": "https://arxiv.org/pdf/2504.13837", "abs": "https://arxiv.org/abs/2504.13837", "authors": ["Yang Yue", "Zhiqi Chen", "Rui Lu", "Andrew Zhao", "Zhaokai Wang", "Yang Yue", "Shiji Song", "Gao Huang"], "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "24 pages, 19 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently\ndemonstrated notable success in enhancing the reasoning capabilities of LLMs,\nparticularly in mathematics and programming tasks. It is widely believed that\nRLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning\nabilities that exceed corresponding base models' capacity. In this study,\nhowever, we critically re-examines this assumption by measuring the\npass@\\textit{k} metric with large values of \\textit{k} to explore the reasoning\ncapability boundary of the models across a wide range of model families and\nbenchmarks. Surprisingly, the RL does \\emph{not}, in fact, elicit fundamentally\nnew reasoning patterns. While RL-trained models outperform their base models at\nsmaller values of $k$ (\\eg, $k$=1), base models can achieve a comparable or\neven higher pass@$k$ score compared to their RL counterparts at large $k$\nvalues. The reasoning paths generated by RL-trained models are already included\nin the base models' sampling distribution, suggesting that most reasoning\nabilities manifested in RL-trained models are already obtained by base models.\nFurther analysis shows that RL training boosts the performance by biasing the\nmodel's output distribution toward paths that are more likely to yield rewards,\ntherefore sampling correct responses more efficiently. But this also results in\na narrower reasoning capability boundary compared to base models. Similar\nresults are observed in visual reasoning tasks trained with RLVR. Moreover, we\nfind that distillation can genuinely introduce new knowledge into the model,\ndifferent from RLVR. These findings underscore a critical limitation of RLVR in\nadvancing LLM reasoning abilities which requires us to fundamentally rethink\nthe impact of RL training in reasoning LLMs and the need of a better paradigm.\nProject Page: https://limit-of-RLVR.github.io", "AI": {"tldr": "RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u5e76\u672a\u5f15\u5165\u65b0\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u662f\u901a\u8fc7\u504f\u5411\u9ad8\u5956\u52b1\u8def\u5f84\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u91cd\u65b0\u8bc4\u4f30RLVR\u662f\u5426\u771f\u6b63\u4e3aLLM\u5e26\u6765\u65b0\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4f18\u5316\u5df2\u6709\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u5927k\u503c\u7684pass@k\u6307\u6807\uff0c\u6bd4\u8f83RL\u8bad\u7ec3\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "RL\u8bad\u7ec3\u6a21\u578b\u5728\u5c0fk\u503c\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u5927k\u503c\u65f6\u57fa\u7840\u6a21\u578b\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u8868\u660eRL\u672a\u5f15\u5165\u65b0\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "RLVR\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u91cd\u65b0\u601d\u8003\u5176\u5728LLM\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u63a2\u7d22\u66f4\u4f18\u8303\u5f0f\u3002"}}
{"id": "2504.13736", "pdf": "https://arxiv.org/pdf/2504.13736", "abs": "https://arxiv.org/abs/2504.13736", "authors": ["Ali Hojjat", "Janek Haberer", "Tayyaba Zainab", "Olaf Landsiedel"], "title": "LimitNet: Progressive, Content-Aware Image Offloading for Extremely Weak Devices & Networks", "categories": ["cs.CV", "eess.IV"], "comment": "This is the author's accepted manuscript. The Version of Record is\n  available at: https://doi.org/10.1145/3643832.3661856", "summary": "IoT devices have limited hardware capabilities and are often deployed in\nremote areas. Consequently, advanced vision models surpass such devices'\nprocessing and storage capabilities, requiring offloading of such tasks to the\ncloud. However, remote areas often rely on LPWANs technology with limited\nbandwidth, high packet loss rates, and extremely low duty cycles, which makes\nfast offloading for time-sensitive inference challenging. Today's approaches,\nwhich are deployable on weak devices, generate a non-progressive bit stream,\nand therefore, their decoding quality suffers strongly when data is only\npartially available on the cloud at a deadline due to limited bandwidth or\npacket losses.\n  In this paper, we introduce LimitNet, a progressive, content-aware image\ncompression model designed for extremely weak devices and networks. LimitNet's\nlightweight progressive encoder prioritizes critical data during transmission\nbased on the content of the image, which gives the cloud the opportunity to run\ninference even with partial data availability.\n  Experimental results demonstrate that LimitNet, on average, compared to SOTA,\nachieves 14.01 p.p. (percentage point) higher accuracy on ImageNet1000, 18.01\npp on CIFAR100, and 0.1 higher mAP@0.5 on COCO. Also, on average, LimitNet\nsaves 61.24% bandwidth on ImageNet1000, 83.68% on CIFAR100, and 42.25% on the\nCOCO dataset compared to SOTA, while it only has 4% more encoding time compared\nto JPEG (with a fixed quality) on STM32F7 (Cortex-M7).", "AI": {"tldr": "LimitNet\u662f\u4e00\u79cd\u9488\u5bf9\u5f31\u8bbe\u5907\u548c\u4f4e\u5e26\u5bbd\u7f51\u7edc\u8bbe\u8ba1\u7684\u6e10\u8fdb\u5f0f\u56fe\u50cf\u538b\u7f29\u6a21\u578b\uff0c\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u4f18\u5148\u4f20\u8f93\u5173\u952e\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u90e8\u5206\u6570\u636e\u53ef\u7528\u65f6\u7684\u63a8\u7406\u51c6\u786e\u6027\u5e76\u8282\u7701\u5e26\u5bbd\u3002", "motivation": "IoT\u8bbe\u5907\u786c\u4ef6\u80fd\u529b\u6709\u9650\u4e14\u5e38\u90e8\u7f72\u5728\u504f\u8fdc\u5730\u533a\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u5e26\u5bbd\u548c\u9ad8\u4e22\u5305\u7387\u7684LPWANs\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u652f\u6301\u65f6\u95f4\u654f\u611f\u7684\u63a8\u7406\u4efb\u52a1\u3002", "method": "LimitNet\u91c7\u7528\u8f7b\u91cf\u7ea7\u6e10\u8fdb\u7f16\u7801\u5668\uff0c\u6839\u636e\u56fe\u50cf\u5185\u5bb9\u4f18\u5148\u4f20\u8f93\u5173\u952e\u6570\u636e\uff0c\u652f\u6301\u4e91\u7aef\u5728\u90e8\u5206\u6570\u636e\u53ef\u7528\u65f6\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLimitNet\u5728ImageNet1000\u3001CIFAR100\u548cCOCO\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4SOTA\u65b9\u6cd5\u63d0\u534714.01%\u300118.01%\u548c0.1 mAP@0.5\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u8282\u770161.24%\u300183.68%\u548c42.25%\u7684\u5e26\u5bbd\u3002", "conclusion": "LimitNet\u5728\u5f31\u8bbe\u5907\u548c\u4f4e\u5e26\u5bbd\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u548c\u5e26\u5bbd\u6548\u7387\uff0c\u9002\u7528\u4e8eIoT\u573a\u666f\u3002"}}
{"id": "2504.13745", "pdf": "https://arxiv.org/pdf/2504.13745", "abs": "https://arxiv.org/abs/2504.13745", "authors": ["Andrea Rigo", "Luca Stornaiuolo", "Mauro Martino", "Bruno Lepri", "Nicu Sebe"], "title": "ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis", "categories": ["cs.CV", "cs.AI", "I.4.0"], "comment": null, "summary": "Diffusion models have revolutionized text-to-image (T2I) synthesis, producing\nhigh-quality, photorealistic images. However, they still struggle to properly\nrender the spatial relationships described in text prompts. To address the lack\nof spatial information in T2I generations, existing methods typically use\nexternal network conditioning and predefined layouts, resulting in higher\ncomputational costs and reduced flexibility. Our approach builds upon a curated\ndataset of spatially explicit prompts, meticulously extracted and synthesized\nfrom LAION-400M to ensure precise alignment between textual descriptions and\nspatial layouts. Alongside this dataset, we present ESPLoRA, a flexible\nfine-tuning framework based on Low-Rank Adaptation, specifically designed to\nenhance spatial consistency in generative models without increasing generation\ntime or compromising the quality of the outputs. In addition to ESPLoRA, we\npropose refined evaluation metrics grounded in geometric constraints, capturing\n3D spatial relations such as \\textit{in front of} or \\textit{behind}. These\nmetrics also expose spatial biases in T2I models which, even when not fully\nmitigated, can be strategically exploited by our TORE algorithm to further\nimprove the spatial consistency of generated images. Our method outperforms the\ncurrent state-of-the-art framework, CoMPaSS, by 13.33% on established spatial\nconsistency benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u7075\u6d3b\u5fae\u8c03\u6846\u67b6ESPLoRA\uff0c\u7528\u4e8e\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u751f\u6210\u65f6\u95f4\u6216\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u6e32\u67d3\u7a7a\u95f4\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u7f51\u7edc\u6761\u4ef6\u548c\u9884\u5b9a\u4e49\u5e03\u5c40\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u7075\u6d3b\u6027\u4f4e\u3002", "method": "\u901a\u8fc7\u4eceLAION-400M\u4e2d\u63d0\u53d6\u548c\u5408\u6210\u7a7a\u95f4\u660e\u786e\u7684\u63d0\u793a\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1ESPLoRA\u6846\u67b6\u548cTORE\u7b97\u6cd5\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u5347\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "result": "ESPLoRA\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u6846\u67b6CoMPaSS 13.33%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2504.13748", "pdf": "https://arxiv.org/pdf/2504.13748", "abs": "https://arxiv.org/abs/2504.13748", "authors": ["Hongjia Chen", "Xin Xu", "Fangling Pu"], "title": "DAM-Net: Domain Adaptation Network with Micro-Labeled Fine-Tuning for Change Detection", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "Change detection (CD) in remote sensing imagery plays a crucial role in\nvarious applications such as urban planning, damage assessment, and resource\nmanagement. While deep learning approaches have significantly advanced CD\nperformance, current methods suffer from poor domain adaptability, requiring\nextensive labeled data for retraining when applied to new scenarios. This\nlimitation severely restricts their practical applications across different\ndatasets. In this work, we propose DAM-Net: a Domain Adaptation Network with\nMicro-Labeled Fine-Tuning for CD. Our network introduces adversarial domain\nadaptation to CD for, utilizing a specially designed segmentation-discriminator\nand alternating training strategy to enable effective transfer between domains.\nAdditionally, we propose a novel Micro-Labeled Fine-Tuning approach that\nstrategically selects and labels a minimal amount of samples (less than 1%) to\nenhance domain adaptation. The network incorporates a Multi-Temporal\nTransformer for feature fusion and optimized backbone structure based on\nprevious research. Experiments conducted on the LEVIR-CD and WHU-CD datasets\ndemonstrate that DAM-Net significantly outperforms existing domain adaptation\nmethods, achieving comparable performance to semi-supervised approaches that\nrequire 10% labeled data while using only 0.3% labeled samples. Our approach\nsignificantly advances cross-dataset CD applications and provides a new\nparadigm for efficient domain adaptation in remote sensing. The source code of\nDAM-Net will be made publicly available upon publication.", "AI": {"tldr": "DAM-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6297\u57df\u9002\u5e94\u548c\u5fae\u6807\u8bb0\u5fae\u8c03\u7684\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6570\u636e\u96c6\u6027\u80fd\uff0c\u4ec5\u97000.3%\u6807\u8bb0\u6570\u636e\u5373\u53ef\u5ab2\u7f8e\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u53d8\u5316\u68c0\u6d4b\u4e2d\u57df\u9002\u5e94\u6027\u5dee\uff0c\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\u91cd\u65b0\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "DAM-Net\u7ed3\u5408\u5bf9\u6297\u57df\u9002\u5e94\uff08\u5206\u5272\u5224\u522b\u5668\u548c\u4ea4\u66ff\u8bad\u7ec3\u7b56\u7565\uff09\u548c\u5fae\u6807\u8bb0\u5fae\u8c03\uff08\u4ec5\u6807\u8bb01%\u6837\u672c\uff09\uff0c\u5e76\u91c7\u7528\u591a\u65f6\u76f8Transformer\u548c\u4f18\u5316\u4e3b\u5e72\u7ed3\u6784\u3002", "result": "\u5728LEVIR-CD\u548cWHU-CD\u6570\u636e\u96c6\u4e0a\uff0cDAM-Net\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u6027\u80fd\u63a5\u8fd1\u970010%\u6807\u8bb0\u6570\u636e\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "DAM-Net\u4e3a\u9065\u611f\u9886\u57df\u9ad8\u6548\u57df\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u8de8\u6570\u636e\u96c6\u53d8\u5316\u68c0\u6d4b\u5e94\u7528\u3002"}}
{"id": "2504.13754", "pdf": "https://arxiv.org/pdf/2504.13754", "abs": "https://arxiv.org/abs/2504.13754", "authors": ["Zhu Zhu", "Shuo Jiang", "Jingyuan Zheng", "Yawen Li", "Yifei Chen", "Manli Zhao", "Weizhong Gu", "Feiwei Qin", "Jinhu Wang", "Gang Yu"], "title": "Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "14pages, 8 figures", "summary": "Neuroblastoma, adrenal-derived, is among the most common pediatric solid\nmalignancies, characterized by significant clinical heterogeneity. Timely and\naccurate pathological diagnosis from hematoxylin and eosin-stained whole slide\nimages is critical for patient prognosis. However, current diagnostic practices\nprimarily rely on subjective manual examination by pathologists, leading to\ninconsistent accuracy. Existing automated whole slide image classification\nmethods encounter challenges such as poor interpretability, limited feature\nextraction capabilities, and high computational costs, restricting their\npractical clinical deployment. To overcome these limitations, we propose\nCMSwinKAN, a contrastive-learning-based multi-scale feature fusion model\ntailored for pathological image classification, which enhances the Swin\nTransformer architecture by integrating a Kernel Activation Network within its\nmultilayer perceptron and classification head modules, significantly improving\nboth interpretability and accuracy. By fusing multi-scale features and\nleveraging contrastive learning strategies, CMSwinKAN mimics clinicians'\ncomprehensive approach, effectively capturing global and local tissue\ncharacteristics. Additionally, we introduce a heuristic soft voting mechanism\nguided by clinical insights to seamlessly bridge patch-level predictions to\nwhole slide image-level classifications. We validate CMSwinKAN on the PpNTs\ndataset, which was collaboratively established with our partner hospital and\nthe publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN\nperforms better than existing state-of-the-art pathology-specific models\npre-trained on large datasets. Our source code is available at\nhttps://github.com/JSLiam94/CMSwinKAN.", "AI": {"tldr": "CMSwinKAN\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u795e\u7ecf\u6bcd\u7ec6\u80de\u7624\u7684\u8bca\u65ad\u4f9d\u8d56\u4e3b\u89c2\u4eba\u5de5\u68c0\u67e5\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u6709\u9650\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u7ed3\u5408Swin Transformer\u67b6\u6784\u548cKernel Activation Network\uff0c\u91c7\u7528\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u542f\u53d1\u5f0f\u8f6f\u6295\u7968\u673a\u5236\u3002", "result": "\u5728PpNTs\u548cBreakHis\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u75c5\u7406\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "CMSwinKAN\u5728\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13759", "pdf": "https://arxiv.org/pdf/2504.13759", "abs": "https://arxiv.org/abs/2504.13759", "authors": ["Davide Ghiani", "Jefferson David Rodriguez Chivata", "Stefano Lilliu", "Simone Maurizio La Cava", "Marco Micheletto", "Giulia Orr\u00f9", "Federico Lama", "Gian Luca Marcialis"], "title": "Fragile Watermarking for Image Certification Using Deep Steganographic Embedding", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Modern identity verification systems increasingly rely on facial images\nembedded in biometric documents such as electronic passports. To ensure global\ninteroperability and security, these images must comply with strict standards\ndefined by the International Civil Aviation Organization (ICAO), which specify\nacquisition, quality, and format requirements. However, once issued, these\nimages may undergo unintentional degradations (e.g., compression, resizing) or\nmalicious manipulations (e.g., morphing) and deceive facial recognition\nsystems. In this study, we explore fragile watermarking, based on deep\nsteganographic embedding as a proactive mechanism to certify the authenticity\nof ICAO-compliant facial images. By embedding a hidden image within the\nofficial photo at the time of issuance, we establish an integrity marker that\nbecomes sensitive to any post-issuance modification. We assess how a range of\nimage manipulations affects the recovered hidden image and show that\ndegradation artifacts can serve as robust forensic cues. Furthermore, we\npropose a classification framework that analyzes the revealed content to detect\nand categorize the type of manipulation applied. Our experiments demonstrate\nhigh detection accuracy, including cross-method scenarios with multiple deep\nsteganography-based models. These findings support the viability of fragile\nwatermarking via steganographic embedding as a valuable tool for biometric\ndocument integrity verification.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u6df1\u5ea6\u9690\u5199\u5d4c\u5165\u7684\u8106\u5f31\u6c34\u5370\u6280\u672f\uff0c\u4ee5\u9a8c\u8bc1ICAO\u6807\u51c6\u9762\u90e8\u56fe\u50cf\u7684\u5b8c\u6574\u6027\uff0c\u9632\u6b62\u6076\u610f\u6216\u65e0\u610f\u4fee\u6539\u3002", "motivation": "\u73b0\u4ee3\u8eab\u4efd\u9a8c\u8bc1\u7cfb\u7edf\u4f9d\u8d56\u9762\u90e8\u56fe\u50cf\uff0c\u4f46\u56fe\u50cf\u53ef\u80fd\u56e0\u538b\u7f29\u3001\u8c03\u6574\u5927\u5c0f\u6216\u6076\u610f\u64cd\u4f5c\uff08\u5982\u53d8\u5f62\uff09\u800c\u88ab\u7be1\u6539\uff0c\u5f71\u54cd\u8bc6\u522b\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u6df1\u5ea6\u9690\u5199\u5d4c\u5165\u5728\u5b98\u65b9\u7167\u7247\u4e2d\u9690\u85cf\u56fe\u50cf\uff0c\u5efa\u7acb\u5b8c\u6574\u6027\u6807\u8bb0\uff0c\u68c0\u6d4b\u540e\u7eed\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u56fe\u50cf\u4fee\u6539\uff0c\u5305\u62ec\u8de8\u65b9\u6cd5\u573a\u666f\u3002", "conclusion": "\u8106\u5f31\u6c34\u5370\u6280\u672f\u662f\u9a8c\u8bc1\u751f\u7269\u7279\u5f81\u6587\u6863\u5b8c\u6574\u6027\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.13763", "pdf": "https://arxiv.org/pdf/2504.13763", "abs": "https://arxiv.org/abs/2504.13763", "authors": ["Ryota Takatsuki", "Sonia Joseph", "Ippei Fujisawa", "Ryota Kanai"], "title": "Decoding Vision Transformers: the Diffusion Steering Lens", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages, 17 figures. Accepted to the CVPR 2025 Workshop on\n  Mechanistic Interpretability for Vision (MIV)", "summary": "Logit Lens is a widely adopted method for mechanistic interpretability of\ntransformer-based language models, enabling the analysis of how internal\nrepresentations evolve across layers by projecting them into the output\nvocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is\ntechnically straightforward, its direct use faces limitations in capturing the\nrichness of visual representations. Building on the work of Toker et al.\n(2024)~\\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize\nintermediate representations in the text encoders of text-to-image diffusion\nmodels, we demonstrate that while Diffusion Lens can effectively visualize\nresidual stream representations in image encoders, it fails to capture the\ndirect contributions of individual submodules. To overcome this limitation, we\npropose \\textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach\nthat steers submodule outputs and patches subsequent indirect contributions. We\nvalidate our method through interventional studies, showing that DSL provides\nan intuitive and reliable interpretation of the internal processing in ViTs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDiffusion Steering Lens (DSL)\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdbVision Transformers (ViTs)\u4e2d\u5185\u90e8\u8868\u793a\u7684\u89c6\u89c9\u5316\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\uff08\u5982Logit Lens\u548cDiffusion Lens\uff09\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1Logit Lens\u548cDiffusion Lens\u5728\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u7f16\u7801\u5668\u4e2d\u5df2\u6709\u5e94\u7528\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u89c6\u89c9\u8868\u793a\u7684\u4e30\u5bcc\u6027\u6216\u76f4\u63a5\u5206\u6790\u5b50\u6a21\u5757\u7684\u8d21\u732e\u3002", "method": "\u63d0\u51fa\u4e86DSL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u5b50\u6a21\u5757\u8f93\u51fa\u5e76\u4fee\u8865\u540e\u7eed\u95f4\u63a5\u8d21\u732e\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5bf9ViTs\u5185\u90e8\u5904\u7406\u7684\u76f4\u89c2\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u5e72\u9884\u6027\u7814\u7a76\u9a8c\u8bc1\uff0cDSL\u80fd\u591f\u63d0\u4f9b\u5bf9ViTs\u5185\u90e8\u5904\u7406\u7684\u53ef\u9760\u548c\u76f4\u89c2\u7684\u89e3\u91ca\u3002", "conclusion": "DSL\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u5206\u6790ViTs\u7684\u5185\u90e8\u8868\u793a\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.13776", "pdf": "https://arxiv.org/pdf/2504.13776", "abs": "https://arxiv.org/abs/2504.13776", "authors": ["Aman Agarwal", "James Gearon", "Raksha Rank", "Etienne Chenevert"], "title": "Fighting Fires from Space: Leveraging Vision Transformers for Enhanced Wildfire Detection and Characterization", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Wildfires are increasing in intensity, frequency, and duration across large\nparts of the world as a result of anthropogenic climate change. Modern hazard\ndetection and response systems that deal with wildfires are under-equipped for\nsustained wildfire seasons. Recent work has proved automated wildfire detection\nusing Convolutional Neural Networks (CNNs) trained on satellite imagery are\ncapable of high-accuracy results. However, CNNs are computationally expensive\nto train and only incorporate local image context. Recently, Vision\nTransformers (ViTs) have gained popularity for their efficient training and\ntheir ability to include both local and global contextual information. In this\nwork, we show that ViT can outperform well-trained and specialized CNNs to\ndetect wildfires on a previously published dataset of LandSat-8 imagery. One of\nour ViTs outperforms the baseline CNN comparison by 0.92%. However, we find our\nown implementation of CNN-based UNet to perform best in every category, showing\ntheir sustained utility in image tasks. Overall, ViTs are comparably capable in\ndetecting wildfires as CNNs, though well-tuned CNNs are still the best\ntechnique for detecting wildfire with our UNet providing an IoU of 93.58%,\nbetter than the baseline UNet by some 4.58%.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528Vision Transformers (ViTs)\u548cCNN\u5728\u536b\u661f\u56fe\u50cf\u4e2d\u68c0\u6d4b\u91ce\u706b\u7684\u6027\u80fd\u6bd4\u8f83\uff0c\u53d1\u73b0ViTs\u8868\u73b0\u63a5\u8fd1CNN\uff0c\u4f46\u7ecf\u8fc7\u4f18\u5316\u7684CNN\uff08\u5982UNet\uff09\u4ecd\u662f\u6700\u4f73\u9009\u62e9\u3002", "motivation": "\u7531\u4e8e\u4eba\u4e3a\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u91ce\u706b\u9891\u7387\u548c\u5f3a\u5ea6\u589e\u52a0\uff0c\u73b0\u6709\u68c0\u6d4b\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u6301\u7eed\u91ce\u706b\u5b63\u8282\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u4e86ViTs\u548cCNN\u5728Landsat-8\u536b\u661f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u91ce\u706b\u68c0\u6d4b\u6027\u80fd\uff0c\u5305\u62ec\u8bad\u7ec3\u6548\u7387\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5229\u7528\u3002", "result": "ViTs\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebfCNN\uff08\u63d0\u53470.92%\uff09\uff0c\u4f46\u4f18\u5316\u7684CNN\uff08UNet\uff09\u5728\u6240\u6709\u6307\u6807\u4e2d\u8868\u73b0\u6700\u4f73\uff08IoU\u8fbe93.58%\uff0c\u6bd4\u57fa\u7ebf\u9ad84.58%\uff09\u3002", "conclusion": "ViTs\u5728\u91ce\u706b\u68c0\u6d4b\u4e2d\u8868\u73b0\u63a5\u8fd1CNN\uff0c\u4f46\u7ecf\u8fc7\u4f18\u5316\u7684CNN\uff08\u5982UNet\uff09\u4ecd\u662f\u76ee\u524d\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2504.13788", "pdf": "https://arxiv.org/pdf/2504.13788", "abs": "https://arxiv.org/abs/2504.13788", "authors": ["Yixuan Yang", "Jinyu Yang", "Zixiang Zhao", "Victor Sanchez", "Feng Zheng"], "title": "RefComp: A Reference-guided Unified Framework for Unpaired Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "The unpaired point cloud completion task aims to complete a partial point\ncloud by using models trained with no ground truth. Existing unpaired point\ncloud completion methods are class-aware, i.e., a separate model is needed for\neach object class. Since they have limited generalization capabilities, these\nmethods perform poorly in real-world scenarios when confronted with a wide\nrange of point clouds of generic 3D objects. In this paper, we propose a novel\nunpaired point cloud completion framework, namely the Reference-guided\nCompletion (RefComp) framework, which attains strong performance in both the\nclass-aware and class-agnostic training settings. The RefComp framework\ntransforms the unpaired completion problem into a shape translation problem,\nwhich is solved in the latent feature space of the partial point clouds. To\nthis end, we introduce the use of partial-complete point cloud pairs, which are\nretrieved by using the partial point cloud to be completed as a template. These\npoint cloud pairs are used as reference data to guide the completion process.\nOur RefComp framework uses a reference branch and a target branch with shared\nparameters for shape fusion and shape translation via a Latent Shape Fusion\nModule (LSFM) to enhance the structural features along the completion pipeline.\nExtensive experiments demonstrate that the RefComp framework achieves not only\nstate-of-the-art performance in the class-aware training setting but also\ncompetitive results in the class-agnostic training setting on both virtual\nscans and real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRefComp\u7684\u65e0\u914d\u5bf9\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u8003\u6570\u636e\u5f15\u5bfc\u5b8c\u6210\u8fc7\u7a0b\uff0c\u5728\u7c7b\u611f\u77e5\u548c\u7c7b\u65e0\u5173\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65e0\u914d\u5bf9\u70b9\u4e91\u8865\u5168\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u4e2a\u5bf9\u8c61\u7c7b\u522b\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6837\u5316\u76843D\u5bf9\u8c61\u3002", "method": "\u5c06\u65e0\u914d\u5bf9\u8865\u5168\u95ee\u9898\u8f6c\u5316\u4e3a\u5f62\u72b6\u8f6c\u6362\u95ee\u9898\uff0c\u5728\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u89e3\u51b3\u3002\u5f15\u5165\u90e8\u5206-\u5b8c\u6574\u70b9\u4e91\u5bf9\u4f5c\u4e3a\u53c2\u8003\u6570\u636e\uff0c\u901a\u8fc7\u5171\u4eab\u53c2\u6570\u7684\u53c2\u8003\u5206\u652f\u548c\u76ee\u6807\u5206\u652f\u8fdb\u884c\u5f62\u72b6\u878d\u5408\u4e0e\u8f6c\u6362\u3002", "result": "RefComp\u6846\u67b6\u5728\u7c7b\u611f\u77e5\u548c\u7c7b\u65e0\u5173\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\uff0c\u5728\u865a\u62df\u626b\u63cf\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "RefComp\u6846\u67b6\u901a\u8fc7\u53c2\u8003\u6570\u636e\u5f15\u5bfc\u548c\u5f62\u72b6\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u914d\u5bf9\u70b9\u4e91\u8865\u5168\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.13820", "pdf": "https://arxiv.org/pdf/2504.13820", "abs": "https://arxiv.org/abs/2504.13820", "authors": ["Yang Yue", "Yulin Wang", "Chenxin Tao", "Pan Liu", "Shiji Song", "Gao Huang"], "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Humans can develop internal world models that encode common sense knowledge,\ntelling them how the world works and predicting the consequences of their\nactions. This concept has emerged as a promising direction for establishing\ngeneral-purpose machine-learning models in recent preliminary works, e.g., for\nvisual representation learning. In this paper, we present CheXWorld, the first\neffort towards a self-supervised world model for radiographic images.\nSpecifically, our work develops a unified framework that simultaneously models\nthree aspects of medical knowledge essential for qualified radiologists,\nincluding 1) local anatomical structures describing the fine-grained\ncharacteristics of local tissues (e.g., architectures, shapes, and textures);\n2) global anatomical layouts describing the global organization of the human\nbody (e.g., layouts of organs and skeletons); and 3) domain variations that\nencourage CheXWorld to model the transitions across different appearance\ndomains of radiographs (e.g., varying clarity, contrast, and exposure caused by\ncollecting radiographs from different hospitals, devices, or patients).\nEmpirically, we design tailored qualitative and quantitative analyses,\nrevealing that CheXWorld successfully captures these three dimensions of\nmedical knowledge. Furthermore, transfer learning experiments across eight\nmedical image classification and segmentation benchmarks showcase that\nCheXWorld significantly outperforms existing SSL methods and large-scale\nmedical foundation models. Code & pre-trained models are available at\nhttps://github.com/LeapLabTHU/CheXWorld.", "AI": {"tldr": "CheXWorld\u662f\u9996\u4e2a\u9488\u5bf9\u653e\u5c04\u5f71\u50cf\u7684\u81ea\u76d1\u7763\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5efa\u6a21\u5c40\u90e8\u89e3\u5256\u7ed3\u6784\u3001\u5168\u5c40\u89e3\u5256\u5e03\u5c40\u548c\u9886\u57df\u53d8\u5316\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u80fd\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u4ee5\u9884\u6d4b\u884c\u4e3a\u540e\u679c\uff0c\u7c7b\u4f3c\u6982\u5ff5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u653e\u5c04\u5f71\u50cf\u9886\u57df\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u6846\u67b6\uff0c\u540c\u65f6\u5efa\u6a21\u5c40\u90e8\u89e3\u5256\u7ed3\u6784\u3001\u5168\u5c40\u89e3\u5256\u5e03\u5c40\u548c\u9886\u57df\u53d8\u5316\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u9a8c\u8bc1\u3002", "result": "CheXWorld\u6210\u529f\u6355\u6349\u533b\u5b66\u77e5\u8bc6\u7684\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CheXWorld\u4e3a\u653e\u5c04\u5f71\u50cf\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13836", "pdf": "https://arxiv.org/pdf/2504.13836", "abs": "https://arxiv.org/abs/2504.13836", "authors": ["Saurabh Pandey", "Luca Magri", "Federica Arrigoni", "Vladislav Golyanik"], "title": "Outlier-Robust Multi-Model Fitting on Quantum Annealers", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop \"Image Matching: Local Features &\n  Beyond\"", "summary": "Multi-model fitting (MMF) presents a significant challenge in Computer\nVision, particularly due to its combinatorial nature. While recent advancements\nin quantum computing offer promise for addressing NP-hard problems, existing\nquantum-based approaches for model fitting are either limited to a single model\nor consider multi-model scenarios within outlier-free datasets. This paper\nintroduces a novel approach, the robust quantum multi-model fitting (R-QuMF)\nalgorithm, designed to handle outliers effectively. Our method leverages the\nintrinsic capabilities of quantum hardware to tackle combinatorial challenges\ninherent in MMF tasks, and it does not require prior knowledge of the exact\nnumber of models, thereby enhancing its practical applicability. By formulating\nthe problem as a maximum set coverage task for adiabatic quantum computers\n(AQC), R-QuMF outperforms existing quantum techniques, demonstrating superior\nperformance across various synthetic and real-world 3D datasets. Our findings\nunderscore the potential of quantum computing in addressing the complexities of\nMMF, especially in real-world scenarios with noisy and outlier-prone data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u91cf\u5b50\u591a\u6a21\u578b\u62df\u5408\uff08R-QuMF\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u89e3\u51b3\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u591a\u6a21\u578b\u62df\u5408\u95ee\u9898\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u542b\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7684\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\u5728\u591a\u6a21\u578b\u62df\u5408\u4e2d\u4ec5\u9002\u7528\u4e8e\u5355\u6a21\u578b\u6216\u65e0\u5f02\u5e38\u503c\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7edd\u70ed\u91cf\u5b50\u8ba1\u7b97\u673a\uff08AQC\uff09\u7684\u6700\u5927\u96c6\u5408\u8986\u76d6\u4efb\u52a1\uff0c\u65e0\u9700\u9884\u5148\u77e5\u9053\u6a21\u578b\u6570\u91cf\u3002", "result": "R-QuMF\u5728\u5408\u6210\u548c\u771f\u5b9e3D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u91cf\u5b50\u6280\u672f\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u89e3\u51b3\u590d\u6742\u591a\u6a21\u578b\u62df\u5408\u95ee\u9898\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u566a\u58f0\u548c\u5f02\u5e38\u503c\u6570\u636e\u3002"}}
{"id": "2504.13186", "pdf": "https://arxiv.org/pdf/2504.13186", "abs": "https://arxiv.org/abs/2504.13186", "authors": ["Yassine Habchi", "Hamza Kheddar", "Yassine Himeur", "Adel Belouchrani", "Erchin Serpedin", "Fouad Khelifi", "Muhammad E. H. Chowdhury"], "title": "Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The rapid advancement of deep learning (DL) has transformed healthcare,\nparticularly in cancer detection and diagnosis. DL surpasses traditional\nmachine learning and human accuracy, making it a critical tool for identifying\ndiseases. Despite numerous reviews on DL in healthcare, a comprehensive\nanalysis of its role in cancer detection remains limited. Existing studies\nfocus on specific aspects, leaving gaps in understanding its broader impact.\nThis paper addresses these gaps by reviewing advanced DL techniques, including\ntransfer learning (TL), reinforcement learning (RL), federated learning (FL),\nTransformers, and large language models (LLMs). These approaches enhance\naccuracy, tackle data scarcity, and enable decentralized learning while\nmaintaining data privacy. TL adapts pre-trained models to new datasets,\nimproving performance with limited labeled data. RL optimizes diagnostic\npathways and treatment strategies, while FL fosters collaborative model\ndevelopment without sharing sensitive data. Transformers and LLMs,\ntraditionally used in natural language processing, are now applied to medical\ndata for improved interpretability. Additionally, this review examines these\ntechniques' efficiency in cancer diagnosis, addresses challenges like data\nimbalance, and proposes solutions. It serves as a resource for researchers and\npractitioners, providing insights into current trends and guiding future\nresearch in advanced DL for cancer detection.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u5148\u8fdb\u6280\u672f\uff0c\u5305\u62ec\u8fc1\u79fb\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u8054\u90a6\u5b66\u4e60\u3001Transformers\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u3001\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u4fdd\u62a4\u9690\u79c1\u65b9\u9762\u7684\u4f5c\u7528\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u5176\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u5168\u9762\u5206\u6790\u4ecd\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7efc\u8ff0\u5148\u8fdbDL\u6280\u672f\u53ca\u5176\u5728\u764c\u75c7\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3001\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u3001Transformers\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7b49\u6280\u672f\uff0c\u5206\u6790\u5176\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u5176\u4f18\u52bf\u548c\u6311\u6218\u3002", "result": "\u8fd9\u4e9b\u6280\u672f\u663e\u8457\u63d0\u9ad8\u4e86\u764c\u75c7\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u764c\u75c7\u68c0\u6d4b\u4e2d\u7684\u6700\u65b0\u8d8b\u52bf\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5176\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.13200", "pdf": "https://arxiv.org/pdf/2504.13200", "abs": "https://arxiv.org/abs/2504.13200", "authors": ["Mohammad Mahdi Danesh Pajouh"], "title": "Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with Attention Gates (DDUNet)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Cancer remains one of the leading causes of mortality worldwide, and among\nits many forms, brain tumors are particularly notorious due to their aggressive\nnature and the critical challenges involved in early diagnosis. Recent advances\nin artificial intelligence have shown great promise in assisting medical\nprofessionals with precise tumor segmentation, a key step in timely diagnosis\nand treatment planning. However, many state-of-the-art segmentation methods\nrequire extensive computational resources and prolonged training times,\nlimiting their practical application in resource-constrained settings. In this\nwork, we present a novel dual-decoder U-Net architecture enhanced with\nattention-gated skip connections, designed specifically for brain tumor\nsegmentation from MRI scans. Our approach balances efficiency and accuracy by\nachieving competitive segmentation performance while significantly reducing\ntraining demands. Evaluated on the BraTS 2020 dataset, the proposed model\nachieved Dice scores of 85.06% for Whole Tumor (WT), 80.61% for Tumor Core\n(TC), and 71.26% for Enhancing Tumor (ET) in only 50 epochs, surpassing several\ncommonly used U-Net variants. Our model demonstrates that high-quality brain\ntumor segmentation is attainable even under limited computational resources,\nthereby offering a viable solution for researchers and clinicians operating\nwith modest hardware. This resource-efficient model has the potential to\nimprove early detection and diagnosis of brain tumors, ultimately contributing\nto better patient outcomes", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u89e3\u7801\u5668U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u95e8\u63a7\u8df3\u8dc3\u8fde\u63a5\uff0c\u7528\u4e8e\u8111\u80bf\u7624MRI\u5206\u5272\uff0c\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u6027\u80fd\u3002", "motivation": "\u8111\u80bf\u7624\u56e0\u5176\u4fb5\u88ad\u6027\u548c\u65e9\u671f\u8bca\u65ad\u56f0\u96be\u800c\u6210\u4e3a\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u73b0\u6709\u5206\u5272\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u53cc\u89e3\u7801\u5668U-Net\u67b6\u6784\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u95e8\u63a7\u8df3\u8dc3\u8fde\u63a5\uff0c\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "result": "\u5728BraTS 2020\u6570\u636e\u96c6\u4e0a\uff0c\u4ec550\u4e2aepoch\u5373\u8fbe\u5230WT 85.06%\u3001TC 80.61%\u3001ET 71.26%\u7684Dice\u5206\u6570\uff0c\u4f18\u4e8e\u5e38\u89c1U-Net\u53d8\u4f53\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8111\u80bf\u7624\u5206\u5272\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u7814\u7a76\u548c\u4e34\u5e8a\u73af\u5883\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6539\u5584\u65e9\u671f\u8bca\u65ad\u548c\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2504.13278", "pdf": "https://arxiv.org/pdf/2504.13278", "abs": "https://arxiv.org/abs/2504.13278", "authors": ["Thoa Thieu", "Roderick Melnik"], "title": "A Stochastic Nonlinear Dynamical System for Smoothing Noisy Eye Gaze Data", "categories": ["math.NA", "cs.CV", "cs.NA"], "comment": "9 pages, 2 figures", "summary": "In this study, we address the challenges associated with accurately\ndetermining gaze location on a screen, which is often compromised by noise from\nfactors such as eye tracker limitations, calibration drift, ambient lighting\nchanges, and eye blinks. We propose the use of an extended Kalman filter (EKF)\nto smooth the gaze data collected during eye-tracking experiments, and\nsystematically explore the interaction of different system parameters. Our\nresults demonstrate that the EKF significantly reduces noise, leading to a\nmarked improvement in tracking accuracy. Furthermore, we show that our proposed\nstochastic nonlinear dynamical model aligns well with real experimental data\nand holds promise for applications in related fields.", "AI": {"tldr": "\u4f7f\u7528\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u5e73\u6ed1\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4e\u566a\u58f0\u5e76\u63d0\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u773c\u52a8\u8ffd\u8e2a\u4e2d\u56e0\u566a\u58f0\uff08\u5982\u8bbe\u5907\u9650\u5236\u3001\u6821\u51c6\u6f02\u79fb\u3001\u73af\u5883\u5149\u53d8\u5316\u548c\u7728\u773c\uff09\u5bfc\u81f4\u7684\u89c6\u7ebf\u5b9a\u4f4d\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u5e73\u6ed1\u65b9\u6cd5\uff0c\u5e76\u7cfb\u7edf\u63a2\u7d22\u4e0d\u540c\u53c2\u6570\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "EKF\u663e\u8457\u51cf\u5c11\u566a\u58f0\uff0c\u63d0\u5347\u8ddf\u8e2a\u7cbe\u5ea6\uff1b\u63d0\u51fa\u7684\u968f\u673a\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u4e0e\u5b9e\u9a8c\u6570\u636e\u543b\u5408\u826f\u597d\u3002", "conclusion": "EKF\u65b9\u6cd5\u6709\u6548\u6539\u5584\u773c\u52a8\u8ffd\u8e2a\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u76f8\u5173\u9886\u57df\u3002"}}
{"id": "2504.13321", "pdf": "https://arxiv.org/pdf/2504.13321", "abs": "https://arxiv.org/abs/2504.13321", "authors": ["John R. Bennett"], "title": "Focus3D: A Practical Method to Adaptively Focus ISAR Data and Provide 3-D Information for Automatic Target Recognition", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "To improve ATR identification of ships at sea requires an advanced ISAR\nprocessor - one that not only provides focused images but can also determine\nthe pose of the ship. This tells us whether the image shows a profile (vertical\nplane) view, a plan (horizontal plane) view or some view in between. If the\nprocessor can provide this information, then the ATR processor can try to match\nthe images with known vertical or horizontal features of ships and, in\nconjunction with estimated ship length, narrow the set of possible\nidentifications. This paper extends the work of Melendez and Bennett [M-B, Ref.\n1] by combining a focus algorithm with a method that models the angles of the\nship relative to the radar. In M-B the algorithm was limited to a single angle\nand the plane of rotation was not determined. This assumption may be fine for a\nshort time image where there is limited data available to determine the pose.\nHowever, the present paper models the ship rotation with two angles - aspect\nangle, representing rotation in the horizontal plane, and tilt angle,\nrepresenting variations in the effective grazing angle to the ship.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684ISAR\u5904\u7406\u5668\uff0c\u7528\u4e8e\u63d0\u5347\u6d77\u4e0a\u8239\u53ea\u7684ATR\u8bc6\u522b\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u805a\u7126\u7b97\u6cd5\u548c\u8239\u53ea\u89d2\u5ea6\u5efa\u6a21\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u5355\u89d2\u5ea6\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u63d0\u5347\u6d77\u4e0a\u8239\u53ea\u7684\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\uff08ATR\uff09\u80fd\u529b\uff0c\u9700\u8981\u4e0d\u4ec5\u80fd\u751f\u6210\u805a\u7126\u56fe\u50cf\uff0c\u8fd8\u80fd\u786e\u5b9a\u8239\u53ea\u59ff\u6001\u7684ISAR\u5904\u7406\u5668\u3002", "method": "\u7ed3\u5408\u805a\u7126\u7b97\u6cd5\u548c\u53cc\u89d2\u5ea6\uff08\u65b9\u4f4d\u89d2\u548c\u503e\u659c\u89d2\uff09\u5efa\u6a21\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86Melendez\u548cBennett\u7684\u5355\u89d2\u5ea6\u6a21\u578b\u3002", "result": "\u65b0\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u5730\u786e\u5b9a\u8239\u53ea\u7684\u59ff\u6001\uff08\u5982\u5782\u76f4\u6216\u6c34\u5e73\u89c6\u56fe\uff09\uff0c\u4ece\u800c\u7f29\u5c0f\u8bc6\u522b\u8303\u56f4\u3002", "conclusion": "\u53cc\u89d2\u5ea6\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86ATR\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\uff0c\u5c24\u5176\u5728\u957f\u65f6\u95f4\u6210\u50cf\u573a\u666f\u4e2d\u3002"}}
{"id": "2504.13331", "pdf": "https://arxiv.org/pdf/2504.13331", "abs": "https://arxiv.org/abs/2504.13331", "authors": ["Yassine Ouzar", "Cl\u00e9mence Nineuil", "Fouad Boutaleb", "Emery Pierson", "Ali Amad", "Mohamed Daoudi"], "title": "Wearable-Derived Behavioral and Physiological Biomarkers for Classifying Unipolar and Bipolar Depression Severity", "categories": ["cs.LG", "cs.CV"], "comment": "IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG) 2025", "summary": "Depression is a complex mental disorder characterized by a diverse range of\nobservable and measurable indicators that go beyond traditional subjective\nassessments. Recent research has increasingly focused on objective, passive,\nand continuous monitoring using wearable devices to gain more precise insights\ninto the physiological and behavioral aspects of depression. However, most\nexisting studies primarily distinguish between healthy and depressed\nindividuals, adopting a binary classification that fails to capture the\nheterogeneity of depressive disorders. In this study, we leverage wearable\ndevices to predict depression subtypes-specifically unipolar and bipolar\ndepression-aiming to identify distinctive biomarkers that could enhance\ndiagnostic precision and support personalized treatment strategies. To this\nend, we introduce the CALYPSO dataset, designed for non-invasive detection of\ndepression subtypes and symptomatology through physiological and behavioral\nsignals, including blood volume pulse, electrodermal activity, body\ntemperature, and three-axis acceleration. Additionally, we establish a\nbenchmark on the dataset using well-known features and standard machine\nlearning methods. Preliminary results indicate that features related to\nphysical activity, extracted from accelerometer data, are the most effective in\ndistinguishing between unipolar and bipolar depression, achieving an accuracy\nof $96.77\\%$. Temperature-based features also showed high discriminative power,\nreaching an accuracy of $93.55\\%$. These findings highlight the potential of\nphysiological and behavioral monitoring for improving the classification of\ndepressive subtypes, paving the way for more tailored clinical interventions.", "AI": {"tldr": "\u5229\u7528\u53ef\u7a7f\u6234\u8bbe\u5907\u9884\u6d4b\u6291\u90c1\u75c7\u4e9a\u578b\uff08\u5355\u6781\u548c\u53cc\u6781\u6291\u90c1\uff09\uff0c\u901a\u8fc7\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\u8bc6\u522b\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63d0\u5347\u8bca\u65ad\u7cbe\u5ea6\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\u533a\u5206\u5065\u5eb7\u4e0e\u6291\u90c1\u4e2a\u4f53\uff0c\u672a\u80fd\u6355\u6349\u6291\u90c1\u75c7\u7684\u5f02\u8d28\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u8bc6\u522b\u6291\u90c1\u75c7\u4e9a\u578b\uff0c\u4ee5\u652f\u6301\u66f4\u7cbe\u51c6\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u3002", "method": "\u5f15\u5165CALYPSO\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u751f\u7406\u548c\u884c\u4e3a\u4fe1\u53f7\uff08\u5982\u8840\u5bb9\u91cf\u8109\u51b2\u3001\u76ae\u80a4\u7535\u6d3b\u52a8\u3001\u4f53\u6e29\u548c\u4e09\u8f74\u52a0\u901f\u5ea6\uff09\u8fdb\u884c\u975e\u4fb5\u5165\u6027\u68c0\u6d4b\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5efa\u7acb\u57fa\u51c6\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u52a0\u901f\u5ea6\u8ba1\u6570\u636e\u63d0\u53d6\u7684\u4f53\u529b\u6d3b\u52a8\u7279\u5f81\u6700\u6709\u6548\u533a\u5206\u5355\u6781\u548c\u53cc\u6781\u6291\u90c1\uff08\u51c6\u786e\u738796.77%\uff09\uff0c\u4f53\u6e29\u7279\u5f81\u4e5f\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u738793.55%\uff09\u3002", "conclusion": "\u751f\u7406\u548c\u884c\u4e3a\u76d1\u6d4b\u6709\u671b\u6539\u5584\u6291\u90c1\u75c7\u4e9a\u578b\u5206\u7c7b\uff0c\u4e3a\u4e2a\u6027\u5316\u4e34\u5e8a\u5e72\u9884\u63d0\u4f9b\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.13340", "pdf": "https://arxiv.org/pdf/2504.13340", "abs": "https://arxiv.org/abs/2504.13340", "authors": ["Oliver Mills", "Philip Conaghan", "Nishant Ravikumar", "Samuel Relton"], "title": "Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Work accepted at BMVC 2024. Minor changes to the camera-ready version\n  since acceptance include a corrected running header and the addition of an\n  Acknowledgments section (including code availability)", "summary": "Menisci are cartilaginous tissue found within the knee that contribute to\njoint lubrication and weight dispersal. Damage to menisci can lead to onset and\nprogression of knee osteoarthritis (OA), a condition that is a leading cause of\ndisability, and for which there are few effective therapies. Accurate automated\nsegmentation of menisci would allow for earlier detection and treatment of\nmeniscal abnormalities, as well as shedding more light on the role the menisci\nplay in OA pathogenesis. Focus in this area has mainly used variants of\nconvolutional networks, but there has been no attempt to utilise recent large\nvision transformer segmentation models. The Segment Anything Model (SAM) is a\nso-called foundation segmentation model, which has been found useful across a\nrange of different tasks due to the large volume of data used for training the\nmodel. In this study, SAM was adapted to perform fully-automated segmentation\nof menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained\nas a baseline. It was found that, when fine-tuning only the decoder, SAM was\nunable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$,\ncompared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM\nend-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both\nthe end-to-end trained SAM configuration and the 3D U-Net were comparable to\nthe winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation\nChallenge 2019. Performance in terms of the Hausdorff Distance showed that both\nconfigurations of SAM were inferior to 3D U-Net in matching the meniscus\nmorphology. Results demonstrated that, despite its generalisability, SAM was\nunable to outperform a basic 3D U-Net in meniscus segmentation, and may not be\nsuitable for similar 3D medical image segmentation tasks also involving fine\nanatomical structures with low contrast and poorly-defined boundaries.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86Segment Anything Model (SAM)\u548c3D U-Net\u5728\u819d\u5173\u8282MRI\u4e2d\u534a\u6708\u677f\u5206\u5272\u7684\u8868\u73b0\uff0c\u53d1\u73b0SAM\u5728\u7aef\u5230\u7aef\u5fae\u8c03\u540e\u4e0e3D U-Net\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u672a\u8d85\u8d8a\u3002", "motivation": "\u534a\u6708\u677f\u635f\u4f24\u53ef\u80fd\u5bfc\u81f4\u819d\u9aa8\u5173\u8282\u708e\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7597\u6cd5\u3002\u81ea\u52a8\u5316\u5206\u5272\u6709\u52a9\u4e8e\u65e9\u671f\u68c0\u6d4b\u548c\u6cbb\u7597\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u5377\u79ef\u7f51\u7edc\uff0c\u672a\u63a2\u7d22\u89c6\u89c9Transformer\u6a21\u578b\u3002", "method": "\u7814\u7a76\u5c06SAM\u5e94\u7528\u4e8e3D\u819d\u5173\u8282MRI\u7684\u534a\u6708\u677f\u5206\u5272\uff0c\u5e76\u4e0e3D U-Net\u5bf9\u6bd4\uff0c\u6d4b\u8bd5\u4e86\u4ec5\u5fae\u8c03\u89e3\u7801\u5668\u548c\u7aef\u5230\u7aef\u5fae\u8c03\u4e24\u79cd\u914d\u7f6e\u3002", "result": "SAM\u7aef\u5230\u7aef\u5fae\u8c03\u540eDice\u5206\u6570\u4e3a0.87\u00b10.03\uff0c\u4e0e3D U-Net\u76f8\u5f53\uff0c\u4f46Hausdorff\u8ddd\u79bb\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5c3d\u7ba1SAM\u5177\u6709\u901a\u7528\u6027\uff0c\u4f46\u5728\u534a\u6708\u677f\u5206\u5272\u4efb\u52a1\u4e2d\u672a\u8d85\u8d8a3D U-Net\uff0c\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u7c7b\u4f3c\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u8fb9\u754c\u6a21\u7cca\u76843D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002"}}
{"id": "2504.13390", "pdf": "https://arxiv.org/pdf/2504.13390", "abs": "https://arxiv.org/abs/2504.13390", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "title": "Accelerated Optimization of Implicit Neural Representations for CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE ISBI 2025", "summary": "Inspired by their success in solving challenging inverse problems in computer\nvision, implicit neural representations (INRs) have been recently proposed for\nreconstruction in low-dose/sparse-view X-ray computed tomography (CT). An INR\nrepresents a CT image as a small-scale neural network that takes spatial\ncoordinates as inputs and outputs attenuation values. Fitting an INR to\nsinogram data is similar to classical model-based iterative reconstruction\nmethods. However, training INRs with losses and gradient-based algorithms can\nbe prohibitively slow, taking many thousands of iterations to converge. This\npaper investigates strategies to accelerate the optimization of INRs for CT\nreconstruction. In particular, we propose two approaches: (1) using a modified\nloss function with improved conditioning, and (2) an algorithm based on the\nalternating direction method of multipliers. We illustrate that both of these\napproaches significantly accelerate INR-based reconstruction of a synthetic\nbreast CT phantom in a sparse-view setting.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u52a0\u901f\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5728CT\u91cd\u5efa\u4e2d\u7684\u4f18\u5316\u7b56\u7565\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u57fa\u4e8eADMM\u7684\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u4e0b\u4e73\u817aCT\u4f53\u6a21\u7684\u91cd\u5efa\u901f\u5ea6\u3002", "motivation": "\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5728\u4f4e\u5242\u91cf/\u7a00\u758f\u89c6\u56feX\u5c04\u7ebfCT\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u8bad\u7ec3\u8fc7\u7a0b\u7f13\u6162\uff0c\u9700\u8981\u6570\u5343\u6b21\u8fed\u4ee3\u624d\u80fd\u6536\u655b\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4f18\u5316\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u52a0\u901f\u7b56\u7565\uff1a1\uff09\u6539\u8fdb\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u6761\u4ef6\u6027\uff1b2\uff09\u57fa\u4e8e\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7684\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u7a00\u758f\u89c6\u56fe\u4e73\u817aCT\u4f53\u6a21\u91cd\u5efa\u4e2d\u663e\u8457\u52a0\u901f\u4e86INR\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86INR\u5728CT\u91cd\u5efa\u4e2d\u7684\u4f18\u5316\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2504.13391", "pdf": "https://arxiv.org/pdf/2504.13391", "abs": "https://arxiv.org/abs/2504.13391", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "title": "Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "comment": "20 pages, 8 figures", "summary": "Automated noninvasive cardiac diagnosis plays a critical role in the early\ndetection of cardiac disorders and cost-effective clinical management.\nAutomated diagnosis involves the automated segmentation and analysis of cardiac\nimages. Precise delineation of cardiac substructures and extraction of their\nmorphological attributes are essential for evaluating the cardiac function, and\ndiagnosing cardiovascular disease such as cardiomyopathy, valvular diseases,\nabnormalities related to septum perforations, and blood-flow rate. Semantic\nsegmentation labels the CMR image at the pixel level, and localizes its\nsubcomponents to facilitate the detection of abnormalities, including\nabnormalities in cardiac wall motion in an aging heart with muscle\nabnormalities, vascular abnormalities, and valvular abnormalities. In this\npaper, we describe a model to improve semantic segmentation of CMR images. The\nmodel extracts edge-attributes and context information during down-sampling of\nthe U-Net and infuses this information during up-sampling to localize three\nmajor cardiac structures: left ventricle cavity (LV); right ventricle cavity\n(RV); and LV myocardium (LMyo). We present an algorithm and performance\nresults. A comparison of our model with previous leading models, using\nsimilarity metrics between actual image and segmented image, shows that our\napproach improves Dice similarity coefficient (DSC) by 2%-11% and lowers\nHausdorff distance (HD) by 1.6 to 5.7 mm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdbCMR\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u53d6\u8fb9\u7f18\u5c5e\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u5fc3\u810f\u7ed3\u6784\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u52a8\u5316\u65e0\u521b\u5fc3\u810f\u8bca\u65ad\u5bf9\u65e9\u671f\u53d1\u73b0\u5fc3\u810f\u75be\u75c5\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4e34\u5e8a\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u800c\u7cbe\u786e\u7684\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u662f\u5173\u952e\u3002", "method": "\u6a21\u578b\u5728U-Net\u7684\u4e0b\u91c7\u6837\u8fc7\u7a0b\u4e2d\u63d0\u53d6\u8fb9\u7f18\u5c5e\u6027\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5728\u4e0a\u91c7\u6837\u65f6\u878d\u5408\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4ee5\u5b9a\u4f4d\u5de6\u5fc3\u5ba4\u8154\uff08LV\uff09\u3001\u53f3\u5fc3\u5ba4\u8154\uff08RV\uff09\u548c\u5de6\u5fc3\u5ba4\u5fc3\u808c\uff08LMyo\uff09\u3002", "result": "\u4e0e\u73b0\u6709\u9886\u5148\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u7684Dice\u76f8\u4f3c\u7cfb\u6570\uff08DSC\uff09\u63d0\u9ad8\u4e862%-11%\uff0cHausdorff\u8ddd\u79bb\uff08HD\uff09\u964d\u4f4e\u4e861.6\u81f35.7\u6beb\u7c73\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u7ed3\u6784\u7684\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\uff0c\u4e3a\u81ea\u52a8\u5316\u5fc3\u810f\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2504.13415", "pdf": "https://arxiv.org/pdf/2504.13415", "abs": "https://arxiv.org/abs/2504.13415", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "title": "DADU: Dual Attention-based Deep Supervised UNet for Automated Semantic Segmentation of Cardiac Images", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "comment": "20 pages, 8 figures", "summary": "We propose an enhanced deep learning-based model for image segmentation of\nthe left and right ventricles and myocardium scar tissue from cardiac magnetic\nresonance (CMR) images. The proposed technique integrates UNet, channel and\nspatial attention, edge-detection based skip-connection and deep supervised\nlearning to improve the accuracy of the CMR image-segmentation. Images are\nprocessed using multiple channels to generate multiple feature-maps. We built a\ndual attention-based model to integrate channel and spatial attention. The use\nof extracted edges in skip connection improves the reconstructed images from\nfeature-maps. The use of deep supervision reduces vanishing gradient problems\ninherent in classification based on deep neural networks. The algorithms for\ndual attention-based model, corresponding implementation and performance\nresults are described. The performance results show that this approach has\nattained high accuracy: 98% Dice Similarity Score (DSC) and significantly lower\nHausdorff Distance (HD). The performance results outperform other leading\ntechniques both in DSC and HD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6539\u8fdb\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u4e2d\u5206\u5272\u5de6\u53f3\u5fc3\u5ba4\u548c\u5fc3\u808c\u7622\u75d5\u7ec4\u7ec7\uff0c\u7ed3\u5408\u4e86UNet\u3001\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u3001\u8fb9\u7f18\u68c0\u6d4b\u8df3\u8dc3\u8fde\u63a5\u548c\u6df1\u5ea6\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u5bf9\u4e34\u5e8a\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u548c\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408UNet\u67b6\u6784\uff0c\u5f15\u5165\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5229\u7528\u8fb9\u7f18\u68c0\u6d4b\u6539\u8fdb\u8df3\u8dc3\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u76d1\u7763\u5b66\u4e60\u7f13\u89e3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002", "result": "\u6a21\u578b\u5728Dice\u76f8\u4f3c\u6027\u5f97\u5206\uff08DSC\uff09\u4e0a\u8fbe\u523098%\uff0cHausdorff\u8ddd\u79bb\uff08HD\uff09\u663e\u8457\u964d\u4f4e\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u9886\u5148\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6280\u672f\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.13476", "pdf": "https://arxiv.org/pdf/2504.13476", "abs": "https://arxiv.org/abs/2504.13476", "authors": ["Jiadong Lou", "Bingqing Liu", "Yuanheng Xiong", "Xiaodong Zhang", "Xu Yuan"], "title": "Variational Autoencoder Framework for Hyperspectral Retrievals (Hyper-VAE) of Phytoplankton Absorption and Chlorophyll a in Coastal Waters for NASA's EMIT and PACE Missions", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "Phytoplankton absorb and scatter light in unique ways, subtly altering the\ncolor of water, changes that are often minor for human eyes to detect but can\nbe captured by sensitive ocean color instruments onboard satellites from space.\nHyperspectral sensors, paired with advanced algorithms, are expected to\nsignificantly enhance the characterization of phytoplankton community\ncomposition, especially in coastal waters where ocean color remote sensing\napplications have historically encountered significant challenges. This study\npresents novel machine learning-based solutions for NASA's hyperspectral\nmissions, including EMIT and PACE, tackling high-fidelity retrievals of\nphytoplankton absorption coefficient and chlorophyll a from their hyperspectral\nremote sensing reflectance. Given that a single Rrs spectrum may correspond to\nvaried combinations of inherent optical properties and associated\nconcentrations, the Variational Autoencoder (VAE) is used as a backbone in this\nstudy to handle such multi-distribution prediction problems. We first time\ntailor the VAE model with innovative designs to achieve hyperspectral\nretrievals of aphy and of Chl-a from hyperspectral Rrs in optically complex\nestuarine-coastal waters. Validation with extensive experimental observation\ndemonstrates superior performance of the VAE models with high precision and low\nbias. The in-depth analysis of VAE's advanced model structures and learning\ndesigns highlights the improvement and advantages of VAE-based solutions over\nthe mixture density network (MDN) approach, particularly on high-dimensional\ndata, such as PACE. Our study provides strong evidence that current EMIT and\nPACE hyperspectral data as well as the upcoming Surface Biology Geology mission\nwill open new pathways toward a better understanding of phytoplankton community\ndynamics in aquatic ecosystems when integrated with AI technologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ece\u9ad8\u5149\u8c31\u9065\u611f\u53cd\u5c04\u7387\u4e2d\u9ad8\u7cbe\u5ea6\u53cd\u6f14\u6d6e\u6e38\u690d\u7269\u5438\u6536\u7cfb\u6570\u548c\u53f6\u7eff\u7d20a\uff0c\u4e3aNASA\u7684EMIT\u548cPACE\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u6cbf\u6d77\u6c34\u57df\u6d6e\u6e38\u690d\u7269\u7fa4\u843d\u7ec4\u6210\u7684\u9ad8\u5149\u8c31\u9065\u611f\u53cd\u6f14\u96be\u9898\uff0c\u63d0\u5347\u5bf9\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u7684\u7406\u89e3\u3002", "method": "\u91c7\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5904\u7406\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u4f18\u5316\u6a21\u578b\u8bbe\u8ba1\u4ee5\u5e94\u5bf9\u591a\u5206\u5e03\u9884\u6d4b\u95ee\u9898\u3002", "result": "VAE\u6a21\u578b\u5728\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u504f\u5dee\uff0c\u4f18\u4e8e\u6df7\u5408\u5bc6\u5ea6\u7f51\u7edc\uff08MDN\uff09\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408AI\u6280\u672f\uff0c\u9ad8\u5149\u8c31\u6570\u636e\u5c06\u4e3a\u6d6e\u6e38\u690d\u7269\u7fa4\u843d\u52a8\u6001\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.13519", "pdf": "https://arxiv.org/pdf/2504.13519", "abs": "https://arxiv.org/abs/2504.13519", "authors": ["Yipeng Sun", "Linda-Sophie Schneider", "Mingxuan Gu", "Siyuan Mei", "Chengze Ye", "Fabian Wagner", "Siming Bayer", "Andreas Maier"], "title": "Filter2Noise: Interpretable Self-Supervised Single-Image Denoising for Low-Dose CT with Attention-Guided Bilateral Filtering", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "preprint", "summary": "Effective denoising is crucial in low-dose CT to enhance subtle structures\nand low-contrast lesions while preventing diagnostic errors. Supervised methods\nstruggle with limited paired datasets, and self-supervised approaches often\nrequire multiple noisy images and rely on deep networks like U-Net, offering\nlittle insight into the denoising mechanism. To address these challenges, we\npropose an interpretable self-supervised single-image denoising framework --\nFilter2Noise (F2N). Our approach introduces an Attention-Guided Bilateral\nFilter that adapted to each noisy input through a lightweight module that\npredicts spatially varying filter parameters, which can be visualized and\nadjusted post-training for user-controlled denoising in specific regions of\ninterest. To enable single-image training, we introduce a novel downsampling\nshuffle strategy with a new self-supervised loss function that extends the\nconcept of Noise2Noise to a single image and addresses spatially correlated\nnoise. On the Mayo Clinic 2016 low-dose CT dataset, F2N outperforms the leading\nself-supervised single-image method (ZS-N2N) by 4.59 dB PSNR while improving\ntransparency, user control, and parametric efficiency. These features provide\nkey advantages for medical applications that require precise and interpretable\nnoise reduction. Our code is demonstrated at\nhttps://github.com/sypsyp97/Filter2Noise.git .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u81ea\u76d1\u7763\u5355\u56fe\u50cf\u53bb\u566a\u6846\u67b6Filter2Noise\uff08F2N\uff09\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53cc\u8fb9\u6ee4\u6ce2\u5668\u548c\u8f7b\u91cf\u7ea7\u6a21\u5757\u9884\u6d4b\u7a7a\u95f4\u53d8\u5316\u7684\u6ee4\u6ce2\u53c2\u6570\uff0c\u89e3\u51b3\u4e86\u4f4e\u5242\u91cfCT\u53bb\u566a\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u4f4e\u5242\u91cfCT\u53bb\u566a\u5bf9\u589e\u5f3a\u7ec6\u5fae\u7ed3\u6784\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u75c5\u53d8\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u53d7\u9650\u4e8e\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u800c\u81ea\u76d1\u7763\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6df1\u5ea6\u7f51\u7edc\u4e14\u7f3a\u4e4f\u5bf9\u53bb\u566a\u673a\u5236\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faF2N\u6846\u67b6\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u53cc\u8fb9\u6ee4\u6ce2\u5668\u548c\u8f7b\u91cf\u7ea7\u53c2\u6570\u9884\u6d4b\u6a21\u5757\uff0c\u91c7\u7528\u65b0\u7684\u4e0b\u91c7\u6837\u6253\u4e71\u7b56\u7565\u548c\u81ea\u76d1\u7763\u635f\u5931\u51fd\u6570\uff0c\u652f\u6301\u5355\u56fe\u50cf\u8bad\u7ec3\u3002", "result": "\u5728Mayo Clinic 2016\u4f4e\u5242\u91cfCT\u6570\u636e\u96c6\u4e0a\uff0cF2N\u6bd4\u9886\u5148\u7684\u81ea\u76d1\u7763\u5355\u56fe\u50cf\u65b9\u6cd5\uff08ZS-N2N\uff09PSNR\u63d0\u9ad8\u4e864.59 dB\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u548c\u7528\u6237\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "F2N\u4e3a\u9700\u8981\u7cbe\u786e\u4e14\u53ef\u89e3\u91ca\u53bb\u566a\u7684\u533b\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u4f18\u52bf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.13532", "pdf": "https://arxiv.org/pdf/2504.13532", "abs": "https://arxiv.org/abs/2504.13532", "authors": ["Yen-Jui Chang", "Wei-Ting Wang", "Chen-Yu Liu", "Yun-Yuan Wang", "Ching-Ray Chang"], "title": "Quantum Walks-Based Adaptive Distribution Generation with Efficient CUDA-Q Acceleration", "categories": ["quant-ph", "cs.CV", "q-fin.PR"], "comment": "17 pages, 5 figures", "summary": "We present a novel Adaptive Distribution Generator that leverages a quantum\nwalks-based approach to generate high precision and efficiency of target\nprobability distributions. Our method integrates variational quantum circuits\nwith discrete-time quantum walks, specifically, split-step quantum walks and\ntheir entangled extensions, to dynamically tune coin parameters and drive the\nevolution of quantum states towards desired distributions. This enables\naccurate one-dimensional probability modeling for applications such as\nfinancial simulation and structured two-dimensional pattern generation\nexemplified by digit representations(0~9). Implemented within the CUDA-Q\nframework, our approach exploits GPU acceleration to significantly reduce\ncomputational overhead and improve scalability relative to conventional\nmethods. Extensive benchmarks demonstrate that our Quantum Walks-Based Adaptive\nDistribution Generator achieves high simulation fidelity and bridges the gap\nbetween theoretical quantum algorithms and practical high-performance\ncomputation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u884c\u8d70\u7684\u81ea\u9002\u5e94\u5206\u5e03\u751f\u6210\u5668\uff0c\u7ed3\u5408\u53d8\u5206\u91cf\u5b50\u7535\u8def\u548c\u79bb\u6563\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u7cbe\u5ea6\u7684\u76ee\u6807\u6982\u7387\u5206\u5e03\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u7cbe\u5ea6\u6982\u7387\u5206\u5e03\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u6269\u5c55\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u91cf\u5b50\u884c\u8d70\u548cGPU\u52a0\u901f\uff0c\u63d0\u5347\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u53d8\u5206\u91cf\u5b50\u7535\u8def\u548c\u79bb\u6563\u65f6\u95f4\u91cf\u5b50\u884c\u8d70\uff08\u5982\u5206\u88c2\u6b65\u91cf\u5b50\u884c\u8d70\u53ca\u5176\u7ea0\u7f20\u6269\u5c55\uff09\uff0c\u52a8\u6001\u8c03\u6574\u786c\u5e01\u53c2\u6570\uff0c\u9a71\u52a8\u91cf\u5b50\u6001\u6f14\u5316\u81f3\u76ee\u6807\u5206\u5e03\u3002", "result": "\u5728CUDA-Q\u6846\u67b6\u4e0b\u5b9e\u73b0\uff0c\u901a\u8fc7GPU\u52a0\u901f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u91d1\u878d\u6a21\u62df\u548c\u4e8c\u7ef4\u6a21\u5f0f\u751f\u6210\uff08\u5982\u6570\u5b570~9\uff09\u4e2d\u8868\u73b0\u51fa\u9ad8\u4eff\u771f\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u7406\u8bba\u91cf\u5b50\u7b97\u6cd5\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u7ed3\u5408\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13553", "pdf": "https://arxiv.org/pdf/2504.13553", "abs": "https://arxiv.org/abs/2504.13553", "authors": ["Yihao Ouyang", "Xunheng Kuang", "Mengjia Xiong", "Zhida Wang", "Yuanquan Wang"], "title": "A Novel Hybrid Approach for Retinal Vessel Segmentation with Dynamic Long-Range Dependency and Multi-Scale Retinal Edge Fusion Enhancement", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate retinal vessel segmentation provides essential structural\ninformation for ophthalmic image analysis. However, existing methods struggle\nwith challenges such as multi-scale vessel variability, complex curvatures, and\nambiguous boundaries. While Convolutional Neural Networks (CNNs),\nTransformer-based models and Mamba-based architectures have advanced the field,\nthey often suffer from vascular discontinuities or edge feature ambiguity. To\naddress these limitations, we propose a novel hybrid framework that\nsynergistically integrates CNNs and Mamba for high-precision retinal vessel\nsegmentation. Our approach introduces three key innovations: 1) The proposed\nHigh-Resolution Edge Fuse Network is a high-resolution preserving hybrid\nsegmentation framework that combines a multi-scale backbone with the\nMulti-scale Retina Edge Fusion (MREF) module to enhance edge features, ensuring\naccurate and robust vessel segmentation. 2) The Dynamic Snake Visual State\nSpace block combines Dynamic Snake Convolution with Mamba to adaptively capture\nvessel curvature details and long-range dependencies. An improved\neight-directional 2D Snake-Selective Scan mechanism and a dynamic weighting\nstrategy enhance the perception of complex vascular topologies. 3) The MREF\nmodule enhances boundary precision through multi-scale edge feature\naggregation, suppressing noise while emphasizing critical vessel structures\nacross scales. Experiments on three public datasets demonstrate that our method\nachieves state-of-the-art performance, particularly in maintaining vascular\ncontinuity and effectively segmenting vessels in low-contrast regions. This\nwork provides a robust method for clinical applications requiring accurate\nretinal vessel analysis. The code is available at\nhttps://github.com/frank-oy/HREFNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cMamba\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u591a\u5c3a\u5ea6\u8840\u7ba1\u53d8\u5f02\u6027\u3001\u590d\u6742\u66f2\u7387\u548c\u6a21\u7cca\u8fb9\u754c\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u8840\u7ba1\u53d8\u5f02\u6027\u3001\u590d\u6742\u66f2\u7387\u548c\u6a21\u7cca\u8fb9\u754c\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8840\u7ba1\u4e0d\u8fde\u7eed\u6216\u8fb9\u7f18\u7279\u5f81\u6a21\u7cca\u3002", "method": "1) \u9ad8\u5206\u8fa8\u7387\u8fb9\u7f18\u878d\u5408\u7f51\u7edc\uff1b2) \u52a8\u6001\u86c7\u5f62\u89c6\u89c9\u72b6\u6001\u7a7a\u95f4\u5757\uff1b3) \u591a\u5c3a\u5ea6\u89c6\u7f51\u819c\u8fb9\u7f18\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8840\u7ba1\u8fde\u7eed\u6027\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u533a\u57df\u5206\u5272\u65b9\u9762\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u7cbe\u786e\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2504.13574", "pdf": "https://arxiv.org/pdf/2504.13574", "abs": "https://arxiv.org/abs/2504.13574", "authors": ["Zhenkai Qin", "Feng Zhu", "Huan Zeng", "Xunyi Nong"], "title": "MAAM: A Lightweight Multi-Agent Aggregation Module for Efficient Image Classification Based on the MindSpore Framework", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "The demand for lightweight models in image classification tasks under\nresource-constrained environments necessitates a balance between computational\nefficiency and robust feature representation. Traditional attention mechanisms,\ndespite their strong feature modeling capability, often struggle with high\ncomputational complexity and structural rigidity, limiting their applicability\nin scenarios with limited computational resources (e.g., edge devices or\nreal-time systems). To address this, we propose the Multi-Agent Aggregation\nModule (MAAM), a lightweight attention architecture integrated with the\nMindSpore framework. MAAM employs three parallel agent branches with\nindependently parameterized operations to extract heterogeneous features,\nadaptively fused via learnable scalar weights, and refined through a\nconvolutional compression layer. Leveraging MindSpore's dynamic computational\ngraph and operator fusion, MAAM achieves 87.0% accuracy on the CIFAR-10\ndataset, significantly outperforming conventional CNN (58.3%) and MLP (49.6%)\nmodels, while improving training efficiency by 30%. Ablation studies confirm\nthe critical role of agent attention (accuracy drops to 32.0% if removed) and\ncompression modules (25.5% if omitted), validating their necessity for\nmaintaining discriminative feature learning. The framework's hardware\nacceleration capabilities and minimal memory footprint further demonstrate its\npracticality, offering a deployable solution for image classification in\nresource-constrained scenarios without compromising accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u67b6\u6784MAAM\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u56e0\u8ba1\u7b97\u590d\u6742\u6027\u548c\u7ed3\u6784\u521a\u6027\u96be\u4ee5\u9002\u7528\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MAAM\u91c7\u7528\u4e09\u4e2a\u5e76\u884c\u4ee3\u7406\u5206\u652f\u63d0\u53d6\u5f02\u6784\u7279\u5f81\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u6807\u91cf\u6743\u91cd\u81ea\u9002\u5e94\u878d\u5408\uff0c\u5e76\u7ed3\u5408\u5377\u79ef\u538b\u7f29\u5c42\u4f18\u5316\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.0%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfCNN\u548cMLP\u6a21\u578b\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534730%\u3002", "conclusion": "MAAM\u901a\u8fc7\u786c\u4ef6\u52a0\u901f\u548c\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u56fe\u50cf\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13597", "pdf": "https://arxiv.org/pdf/2504.13597", "abs": "https://arxiv.org/abs/2504.13597", "authors": ["Jun Zeng", "KC Santosh", "Deepak Rajan Nayak", "Thomas de Lange", "Jonas Varkey", "Tyler Berzin", "Debesh Jha"], "title": "FocusNet: Transformer-enhanced Polyp Segmentation with Local and Pooling Attention", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Colonoscopy is vital in the early diagnosis of colorectal polyps. Regular\nscreenings can effectively prevent benign polyps from progressing to CRC. While\ndeep learning has made impressive strides in polyp segmentation, most existing\nmodels are trained on single-modality and single-center data, making them less\neffective in real-world clinical environments. To overcome these limitations,\nwe propose FocusNet, a Transformer-enhanced focus attention network designed to\nimprove polyp segmentation. FocusNet incorporates three essential modules: the\nCross-semantic Interaction Decoder Module (CIDM) for generating coarse\nsegmentation maps, the Detail Enhancement Module (DEM) for refining shallow\nfeatures, and the Focus Attention Module (FAM), to balance local detail and\nglobal context through local and pooling attention mechanisms. We evaluate our\nmodel on PolypDB, a newly introduced dataset with multi-modality and\nmulti-center data for building more reliable segmentation methods. Extensive\nexperiments showed that FocusNet consistently outperforms existing\nstate-of-the-art approaches with a high dice coefficients of 82.47% on the BLI\nmodality, 88.46% on FICE, 92.04% on LCI, 82.09% on the NBI and 93.42% on WLI\nmodality, demonstrating its accuracy and robustness across five different\nmodalities. The source code for FocusNet is available at\nhttps://github.com/JunZengz/FocusNet.", "AI": {"tldr": "FocusNet\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u63d0\u5347\u7ed3\u80a0\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u591a\u6a21\u5757\u8bbe\u8ba1\u5728\u591a\u6a21\u6001\u548c\u591a\u4e2d\u5fc3\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u591a\u57fa\u4e8e\u5355\u6a21\u6001\u548c\u5355\u4e2d\u5fc3\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5206\u5272\u65b9\u6cd5\u3002", "method": "FocusNet\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aCIDM\u751f\u6210\u7c97\u5206\u5272\u56fe\uff0cDEM\u4f18\u5316\u6d45\u5c42\u7279\u5f81\uff0cFAM\u901a\u8fc7\u5c40\u90e8\u548c\u6c60\u5316\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "\u5728PolypDB\u6570\u636e\u96c6\u4e0a\uff0cFocusNet\u5728\u4e94\u79cd\u4e0d\u540c\u6a21\u6001\u4e0b\u7684Dice\u7cfb\u6570\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u8fbe93.42%\u3002", "conclusion": "FocusNet\u5728\u591a\u6a21\u6001\u548c\u591a\u4e2d\u5fc3\u6570\u636e\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u606f\u8089\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.13599", "pdf": "https://arxiv.org/pdf/2504.13599", "abs": "https://arxiv.org/abs/2504.13599", "authors": ["Bowen Liu", "Chunlei Meng", "Wei Lin", "Hongda Zhang", "Ziqing Zhou", "Zhongxue Gan", "Chun Ouyang"], "title": "ViG3D-UNet: Volumetric Vascular Connectivity-Aware Segmentation via 3D Vision Graph Representation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate vascular segmentation is essential for coronary visualization and\nthe diagnosis of coronary heart disease. This task involves the extraction of\nsparse tree-like vascular branches from the volumetric space. However, existing\nmethods have faced significant challenges due to discontinuous vascular\nsegmentation and missing endpoints. To address this issue, a 3D vision graph\nneural network framework, named ViG3D-UNet, was introduced. This method\nintegrates 3D graph representation and aggregation within a U-shaped\narchitecture to facilitate continuous vascular segmentation. The ViG3D module\ncaptures volumetric vascular connectivity and topology, while the convolutional\nmodule extracts fine vascular details. These two branches are combined through\nchannel attention to form the encoder feature. Subsequently, a paperclip-shaped\noffset decoder minimizes redundant computations in the sparse feature space and\nrestores the feature map size to match the original input dimensions. To\nevaluate the effectiveness of the proposed approach for continuous vascular\nsegmentation, evaluations were performed on two public datasets, ASOCA and\nImageCAS. The segmentation results show that the ViG3D-UNet surpassed competing\nmethods in maintaining vascular segmentation connectivity while achieving high\nsegmentation accuracy. Our code will be available soon.", "AI": {"tldr": "ViG3D-UNet\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u56fe\u795e\u7ecf\u7f51\u7edc\u548cU\u578b\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u8840\u7ba1\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8840\u7ba1\u8fde\u901a\u6027\u548c\u7aef\u70b9\u7f3a\u5931\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u51c6\u786e\u7684\u8840\u7ba1\u5206\u5272\u5bf9\u51a0\u72b6\u52a8\u8109\u53ef\u89c6\u5316\u548c\u51a0\u5fc3\u75c5\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8840\u7ba1\u5206\u5272\u4e0d\u8fde\u7eed\u548c\u7aef\u70b9\u7f3a\u5931\u7684\u95ee\u9898\u3002", "method": "ViG3D-UNet\u6574\u5408\u4e863D\u56fe\u8868\u793a\u548c\u805a\u5408\uff0c\u901a\u8fc7\u5377\u79ef\u6a21\u5757\u63d0\u53d6\u7ec6\u8282\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u6ce8\u610f\u529b\u7ed3\u5408\u7279\u5f81\u3002\u91c7\u7528\u7eb8\u5939\u5f62\u504f\u79fb\u89e3\u7801\u5668\u51cf\u5c11\u7a00\u758f\u7279\u5f81\u7a7a\u95f4\u7684\u8ba1\u7b97\u5197\u4f59\u3002", "result": "\u5728ASOCA\u548cImageCAS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cViG3D-UNet\u5728\u4fdd\u6301\u8840\u7ba1\u8fde\u901a\u6027\u548c\u5206\u5272\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "ViG3D-UNet\u6709\u6548\u89e3\u51b3\u4e86\u8840\u7ba1\u5206\u5272\u7684\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.13622", "pdf": "https://arxiv.org/pdf/2504.13622", "abs": "https://arxiv.org/abs/2504.13622", "authors": ["Dawid Kope\u0107", "Wojciech Koz\u0142owski", "Maciej Wizerkaniuk", "Dawid Krutul", "Jan Koco\u0144", "Maciej Zi\u0119ba"], "title": "SupResDiffGAN a new approach for the Super-Resolution task", "categories": ["eess.IV", "cs.CV"], "comment": "25th International Conference on Computational Science", "summary": "In this work, we present SupResDiffGAN, a novel hybrid architecture that\ncombines the strengths of Generative Adversarial Networks (GANs) and diffusion\nmodels for super-resolution tasks. By leveraging latent space representations\nand reducing the number of diffusion steps, SupResDiffGAN achieves\nsignificantly faster inference times than other diffusion-based\nsuper-resolution models while maintaining competitive perceptual quality. To\nprevent discriminator overfitting, we propose adaptive noise corruption,\nensuring a stable balance between the generator and the discriminator during\ntraining. Extensive experiments on benchmark datasets show that our approach\noutperforms traditional diffusion models such as SR3 and I$^2$SB in efficiency\nand image quality. This work bridges the performance gap between diffusion- and\nGAN-based methods, laying the foundation for real-time applications of\ndiffusion models in high-resolution image generation.", "AI": {"tldr": "SupResDiffGAN\u662f\u4e00\u79cd\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\u5e76\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u7ed3\u5408GAN\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u548c\u51cf\u5c11\u6269\u6563\u6b65\u9aa4\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u566a\u58f0\u7834\u574f\u4ee5\u9632\u6b62\u5224\u522b\u5668\u8fc7\u62df\u5408\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eSR3\u548cI\u00b2SB\uff0c\u6548\u7387\u4e0e\u56fe\u50cf\u8d28\u91cf\u5747\u63d0\u5347\u3002", "conclusion": "SupResDiffGAN\u5f25\u5408\u4e86\u6269\u6563\u6a21\u578b\u4e0eGAN\u65b9\u6cd5\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.13647", "pdf": "https://arxiv.org/pdf/2504.13647", "abs": "https://arxiv.org/abs/2504.13647", "authors": ["Yushen He", "Lei Zhao", "Tianchen Deng", "Zipeng Fang", "Weidong Chen"], "title": "Lightweight LiDAR-Camera 3D Dynamic Object Detection and Multi-Class Trajectory Prediction", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Service mobile robots are often required to avoid dynamic objects while\nperforming their tasks, but they usually have only limited computational\nresources. So we present a lightweight multi-modal framework for 3D object\ndetection and trajectory prediction. Our system synergistically integrates\nLiDAR and camera inputs to achieve real-time perception of pedestrians,\nvehicles, and riders in 3D space. The framework proposes two novel modules: 1)\na Cross-Modal Deformable Transformer (CMDT) for object detection with high\naccuracy and acceptable amount of computation, and 2) a Reference\nTrajectory-based Multi-Class Transformer (RTMCT) for efficient and diverse\ntrajectory prediction of mult-class objects with flexible trajectory lengths.\nEvaluations on the CODa benchmark demonstrate superior performance over\nexisting methods across detection (+2.03% in mAP) and trajectory prediction\n(-0.408m in minADE5 of pedestrians) metrics. Remarkably, the system exhibits\nexceptional deployability - when implemented on a wheelchair robot with an\nentry-level NVIDIA 3060 GPU, it achieves real-time inference at 13.2 fps. To\nfacilitate reproducibility and practical deployment, we release the related\ncode of the method at https://github.com/TossherO/3D_Perception and its ROS\ninference version at https://github.com/TossherO/ros_packages.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e3D\u7269\u4f53\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u7ed3\u5408LiDAR\u548c\u76f8\u673a\u8f93\u5165\uff0c\u5b9e\u73b0\u5b9e\u65f6\u611f\u77e5\uff0c\u5e76\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u670d\u52a1\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u907f\u514d\u52a8\u6001\u7269\u4f53\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u76843D\u611f\u77e5\u548c\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u65b0\u6a21\u5757\uff1a1) Cross-Modal Deformable Transformer (CMDT) \u7528\u4e8e\u9ad8\u7cbe\u5ea6\u7269\u4f53\u68c0\u6d4b\uff1b2) Reference Trajectory-based Multi-Class Transformer (RTMCT) \u7528\u4e8e\u591a\u7c7b\u7269\u4f53\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728CODa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u68c0\u6d4bmAP\u63d0\u53472.03%\uff0c\u884c\u4eba\u8f68\u8ff9\u9884\u6d4bminADE5\u964d\u4f4e0.408m\uff09\uff0c\u5e76\u5728\u4f4e\u7aefGPU\u4e0a\u5b9e\u73b013.2 fps\u7684\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5177\u6709\u4f18\u5f02\u7684\u90e8\u7f72\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4fbf\u4e8e\u590d\u73b0\u548c\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.13697", "pdf": "https://arxiv.org/pdf/2504.13697", "abs": "https://arxiv.org/abs/2504.13697", "authors": ["Chenxuan Liu", "He Li", "Zongze Li", "Shuai Wang", "Wei Xu", "Kejiang Ye", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Green Robotic Mixed Reality with Gaussian Splatting", "categories": ["cs.RO", "cs.CV", "eess.SP"], "comment": "6 pages, 5 figures, accepted by IEEE INFOCOM 2025 Workshop on\n  Networked Robotics and Communication Systems", "summary": "Realizing green communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nat high frequencies through wireless channels. This paper proposes Gaussian\nsplatting (GS) RoboMR (GSRMR), which achieves a lower energy consumption and\nmakes a concrete step towards green RoboMR. The crux to GSRMR is to build a GS\nmodel which enables the simulator to opportunistically render a photo-realistic\nview from the robot's pose, thereby reducing the need for excessive image\nuploads. Since the GS model may involve discrepancies compared to the actual\nenvironments, a GS cross-layer optimization (GSCLO) framework is further\nproposed, which jointly optimizes content switching (i.e., deciding whether to\nupload image or not) and power allocation across different frames. The GSCLO\nproblem is solved by an accelerated penalty optimization (APO) algorithm.\nExperiments demonstrate that the proposed GSRMR reduces the communication\nenergy by over 10x compared with RoboMR. Furthermore, the proposed GSRMR with\nAPO outperforms extensive baseline schemes, in terms of peak signal-to-noise\nratio (PSNR) and structural similarity index measure (SSIM).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u7684RoboMR\u7cfb\u7edf\uff08GSRMR\uff09\uff0c\u901a\u8fc7\u51cf\u5c11\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u4f20\u9891\u7387\u6765\u964d\u4f4e\u80fd\u8017\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86GS\u8de8\u5c42\u4f18\u5316\uff08GSCLO\uff09\u6846\u67b6\u548c\u52a0\u901f\u60e9\u7f5a\u4f18\u5316\uff08APO\uff09\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u6df7\u5408\u73b0\u5b9e\uff08RoboMR\uff09\u7cfb\u7edf\u4e2d\uff0c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u9ad8\u9891\u4e0a\u4f20\u5bfc\u81f4\u901a\u4fe1\u80fd\u8017\u5de8\u5927\uff0c\u4e9f\u9700\u7eff\u8272\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faGSRMR\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u51cf\u5c11\u56fe\u50cf\u4e0a\u4f20\u9700\u6c42\uff1b\u8bbe\u8ba1GSCLO\u6846\u67b6\u8054\u5408\u4f18\u5316\u5185\u5bb9\u5207\u6362\u548c\u529f\u7387\u5206\u914d\uff0c\u5e76\u901a\u8fc7APO\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGSRMR\u5c06\u901a\u4fe1\u80fd\u8017\u964d\u4f4e10\u500d\u4ee5\u4e0a\uff0c\u4e14\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "GSRMR\u4e3a\u7eff\u8272RoboMR\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\u5e76\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2504.13713", "pdf": "https://arxiv.org/pdf/2504.13713", "abs": "https://arxiv.org/abs/2504.13713", "authors": ["Samuel Cerezo", "Gaetano Meli", "Tom\u00e1s Berriel Martins", "Kirill Safronov", "Javier Civera"], "title": "SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Models and methods originally developed for novel view synthesis and scene\nrendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are\nincreasingly being adopted as representations in Simultaneous Localization and\nMapping (SLAM). However, existing datasets fail to include the specific\nchallenges of both fields, such as multimodality and sequentiality in SLAM or\ngeneralization across viewpoints and illumination conditions in neural\nrendering. To bridge this gap, we introduce SLAM&Render, a novel dataset\ndesigned to benchmark methods in the intersection between SLAM and novel view\nrendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot\nkinematic data, and ground-truth pose streams. By releasing robot kinematic\ndata, the dataset also enables the assessment of novel SLAM strategies when\napplied to robot manipulators. The dataset sequences span five different setups\nfeaturing consumer and industrial objects under four different lighting\nconditions, with separate training and test trajectories per scene, as well as\nobject rearrangements. Our experimental results, obtained with several\nbaselines from the literature, validate SLAM&Render as a relevant benchmark for\nthis emerging research area.", "AI": {"tldr": "SLAM&Render\u6570\u636e\u96c6\u586b\u8865\u4e86SLAM\u4e0e\u795e\u7ecf\u6e32\u67d3\u9886\u57df\u7684\u6570\u636e\u7a7a\u767d\uff0c\u5305\u542b40\u4e2a\u5e8f\u5217\uff0c\u652f\u6301\u591a\u6a21\u6001\u548c\u65f6\u5e8f\u6027\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u6db5\u76d6SLAM\u4e0e\u795e\u7ecf\u6e32\u67d3\u7684\u7279\u5b9a\u6311\u6218\uff0c\u5982\u591a\u6a21\u6001\u3001\u65f6\u5e8f\u6027\u3001\u89c6\u89d2\u548c\u5149\u7167\u6761\u4ef6\u7684\u53d8\u5316\u3002", "method": "\u5f15\u5165SLAM&Render\u6570\u636e\u96c6\uff0c\u5305\u542bRGB\u3001\u6df1\u5ea6\u3001IMU\u3001\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\u548c\u771f\u5b9e\u4f4d\u59ff\uff0c\u8986\u76d6\u591a\u79cd\u573a\u666f\u548c\u5149\u7167\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SLAM&Render\u4f5c\u4e3a\u65b0\u5174\u7814\u7a76\u9886\u57df\u57fa\u51c6\u7684\u76f8\u5173\u6027\u3002", "conclusion": "SLAM&Render\u4e3aSLAM\u4e0e\u795e\u7ecf\u6e32\u67d3\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2504.13785", "pdf": "https://arxiv.org/pdf/2504.13785", "abs": "https://arxiv.org/abs/2504.13785", "authors": ["Steffen Hagedorn", "Aron Distelzweig", "Marcel Hallgarten", "Alexandru P. Condurache"], "title": "Learning Through Retrospection: Improving Trajectory Prediction for Automated Driving with Error Feedback", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "In automated driving, predicting trajectories of surrounding vehicles\nsupports reasoning about scene dynamics and enables safe planning for the ego\nvehicle. However, existing models handle predictions as an instantaneous task\nof forecasting future trajectories based on observed information. As time\nproceeds, the next prediction is made independently of the previous one, which\nmeans that the model cannot correct its errors during inference and will repeat\nthem. To alleviate this problem and better leverage temporal data, we propose a\nnovel retrospection technique. Through training on closed-loop rollouts the\nmodel learns to use aggregated feedback. Given new observations it reflects on\nprevious predictions and analyzes its errors to improve the quality of\nsubsequent predictions. Thus, the model can learn to correct systematic errors\nduring inference. Comprehensive experiments on nuScenes and Argoverse\ndemonstrate a considerable decrease in minimum Average Displacement Error of up\nto 31.9% compared to the state-of-the-art baseline without retrospection. We\nfurther showcase the robustness of our technique by demonstrating a better\nhandling of out-of-distribution scenarios with undetected road-users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56de\u987e\u6280\u672f\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u5229\u7528\u65f6\u95f4\u6570\u636e\u53cd\u9988\uff0c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u65f6\u72ec\u7acb\u5904\u7406\u6bcf\u6b21\u9884\u6d4b\uff0c\u65e0\u6cd5\u7ea0\u6b63\u9519\u8bef\uff0c\u5bfc\u81f4\u91cd\u590d\u9519\u8bef\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u56de\u987e\u6280\u672f\uff0c\u901a\u8fc7\u95ed\u73af\u8bad\u7ec3\u4f7f\u6a21\u578b\u80fd\u591f\u5229\u7528\u53cd\u9988\u5206\u6790\u5e76\u6539\u8fdb\u540e\u7eed\u9884\u6d4b\u3002", "result": "\u5728nuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u5c0f\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u964d\u4f4e\u4e8631.9%\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u5904\u7406\u5206\u5e03\u5916\u573a\u666f\u3002", "conclusion": "\u56de\u987e\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
