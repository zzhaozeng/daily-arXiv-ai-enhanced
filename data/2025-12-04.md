<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models](https://arxiv.org/abs/2512.03047)
*Samih Fadli*

Main category: cs.CL

TL;DR: 本文提出了一种基于伦理熵变动态监测大模型安全性的新框架，通过五类行为分类和熵值计算，发现基础模型存在持续熵增，而指令微调模型可抑制80%熵增，并构建了实时预警系统。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准测试无法捕捉大模型在分布偏移、越狱攻击和部署中价值漂移等动态风险，需建立动态监测框架应对伦理熵增问题。

Method: 1) 提出五维行为分类法 2) 训练伦理熵S(t)分类器 3) 对4种前沿模型进行压力测试 4) 构建含γ_eff参数的监控管道

Result: 1) 基础模型呈现持续熵增 2) 指令微调模型抑制80%熵增 3) 成功实现运行时价值漂移预警

Conclusion: 通过量化伦理熵动态变化，该框架能有效区分模型安全状态，为部署阶段的安全监控提供了可扩展的实时解决方案。

Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.

</details>


### [2] [Watermarks for Embeddings-as-a-Service Large Language Models](https://arxiv.org/abs/2512.03079)
*Anudeex Shetty*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在自然语言理解和生成方面表现出色，但其提供的文本嵌入服务（EaaS）面临模仿攻击。本文研究EaaS水印技术，揭示现有水印易受释义攻击，并提出了一种新的水印技术WET。


<details>
  <summary>Details</summary>
Motivation: 现有EaaS水印技术在模仿攻击下容易被移除，这威胁到服务提供者的知识产权。因此，需要研究更稳健的水印技术以防御模仿攻击。

Method: 首先，通过释义攻击验证现有水印的脆弱性。然后，提出并验证一种新的水印技术WET，该技术利用嵌入的线性变换进行水印嵌入和验证。

Result: 研究表明，释义攻击可以有效绕过当前最先进的EaaS水印技术。而WET在面对释义攻击时表现出强大的稳健性，并具备近乎完美的可验证性。

Conclusion: 本文揭示了现有EaaS水印技术的漏洞，并提出了一种新的水印技术WET，该技术能有效防御模仿攻击，为EaaS提供更好的知识产权保护。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.

</details>


### [3] [Alleviating Choice Supportive Bias in LLM with Reasoning Dependency Generation](https://arxiv.org/abs/2512.03082)
*Nan Zhuang,Wenshuo Wang,Lekai Qian,Yuxiao Wang,Boyu Cao,Qi Liu*

Main category: cs.CL

TL;DR: 本研究提出了一种名为Reasoning Dependency Generation (RDG)的框架，以解决大型语言模型中的选择支持偏见（CSB）。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法主要针对人口统计和社会偏见，而针对LLMs中认知偏见的方法仍待探索。近期的研究表明，LLMs在评估时可能会表现出选择支持偏见，从而影响AI辅助决策的客观性。

Method: RDG框架通过生成无偏见的推理数据，微调LLMs以减轻选择支持偏见。RDG自动构建平衡的推理QA对，明确建模和去建模选择、证据和理由之间的依赖关系，生成大规模跨领域数据集。

Result: 实验表明，经过RDG生成数据微调的LLMs在记忆性实验中表现提高了81.5%，在评估性实验中提高了94.3%，同时在标准BBQ基准测试中保持类似性能。

Conclusion: 本研究开创性地提出了针对LLMs认知偏见的解决方案，为开发更可靠的AI辅助决策支持系统做出了贡献。

Abstract: Recent studies have demonstrated that some Large Language Models exhibit choice-supportive bias (CSB) when performing evaluations, systematically favoring their chosen options and potentially compromising the objectivity of AI-assisted decision making. While existing debiasing approaches primarily target demographic and social biases, methods for addressing cognitive biases in LLMs remain largely unexplored. In this work, we present the first solution to address CSB through Reasoning Dependency Generation (RDG), a novel framework for generating unbiased reasoning data to mitigate choice-supportive bias through fine-tuning. RDG automatically constructs balanced reasoning QA pairs, explicitly (un)modeling the dependencies between choices, evidences, and justifications. Our approach is able to generate a large-scale dataset of QA pairs across domains, incorporating Contextual Dependency Data and Dependency Decouple Data. Experiments show that LLMs fine-tuned on RDG-generated data demonstrate a 81.5% improvement in memory-based experiments and 94.3% improvement in the evaluation-based experiment, while maintaining similar performance on standard BBQ benchmarks. This work pioneers an approach for addressing cognitive biases in LLMs and contributes to the development of more reliable AI-assisted decision support systems.

</details>


### [4] [Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies](https://arxiv.org/abs/2512.03195)
*Stylianos Saroglou,Konstantinos Diamantaras,Francesco Preta,Marina Delianidi,Apostolos Benisis,Christian Johannes Meyer*

Main category: cs.CL

TL;DR: 本研究探讨了语言模型通过将职位空缺文本链接到欧洲技能、能力、资格和职业（ESCO）分类法和欧洲资格框架（EQF），来改善劳动力市场信息分类的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过引入语言模型，提高职位空缺文本与欧洲主要框架的链接效率，从而改进劳动力市场的分类和信息提取。

Method: 研究比较了文献中的两种主要方法：句子链接和实体链接，并引入了两个注释数据集来评估职位和资格在职位空缺文本中的表示。此外，研究探讨了如何利用生成式大语言模型进行任务处理。

Result: 研究结果推进了职位实体提取领域的现有技术水平，并为研究数字经济中的工作、技能和劳动力市场叙事提供了计算基础设施。代码已开源。

Conclusion: 该研究不仅为劳动力市场信息分类提供了新的工具和数据集，还展示了语言模型在改善职位空缺文本分类方面的潜力，并为后续研究提供了基础设施。

Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier

</details>


### [5] [InvertiTune: High-Quality Data Synthesis for Cost-Effective Single-Shot Text-to-Knowledge Graph Generation](https://arxiv.org/abs/2512.03197)
*Faezeh Faez,Marzieh S. Tahaei,Yaochen Hu,Ali Pourranjbar,Mahdi Biparva,Mark Coates,Yingxue Zhang*

Main category: cs.CL

TL;DR: 提出InvertiTune框架，通过控制数据生成和监督微调，解决Text2KG任务中迭代提示带来的计算开销和复杂关系遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本生成知识图谱（Text2KG）中表现突出，但现有方法依赖迭代提示，导致计算成本高且易忽略复杂关系。

Method: InvertiTune框架结合控制数据生成管道和监督微调（SFT）。数据管道从大型知识库提取子图，应用噪声过滤，并利用LLM生成对应的自然文本描述，支持轻量级模型的SFT。

Result: 在CE12k数据集上，InvertiTune超越大型非微调LLM和最先进的Text2KG方法，并在CrossEval-1200上表现出更强的跨数据集泛化能力。

Conclusion: 高质量、真实的训练数据对提升Text2KG系统的效率和性能至关重要。

Abstract: Large Language Models (LLMs) have revolutionized the ability to understand and generate text, enabling significant progress in automatic knowledge graph construction from text (Text2KG). Many Text2KG methods, however, rely on iterative LLM prompting, making them computationally expensive and prone to overlooking complex relations distributed throughout the text. To address these limitations, we propose InvertiTune, a framework that combines a controlled data generation pipeline with supervised fine-tuning (SFT). Within this framework, the data-generation pipeline systematically extracts subgraphs from large knowledge bases, applies noise filtering, and leverages LLMs to generate corresponding natural text descriptions, a task more aligned with LLM capabilities than direct KG generation from text. This pipeline enables generating datasets composed of longer texts paired with larger KGs that better reflect real-world scenarios compared to existing benchmarks, thus supporting effective SFT of lightweight models for single-shot KG construction. Experimental results on CE12k, a dataset generated using the introduced pipeline, show that InvertiTune outperforms larger non-fine-tuned LLMs as well as state-of-the-art Text2KG approaches, while also demonstrating stronger cross-dataset generalization on CrossEval-1200, a test set created from three established benchmark datasets and CE12k. These findings highlight the importance of realistic, high-quality training data for advancing efficient and high-performing Text2KG systems.

</details>


### [6] [Identifying attributions of causality in political text](https://arxiv.org/abs/2512.03214)
*Paulina Garcia-Corral*

Main category: cs.CL

TL;DR: 本文提出了一种在政治文本中检测和解析解释的框架，通过训练轻量级因果语言模型生成因果主张的结构化数据集。


<details>
  <summary>Details</summary>
Motivation: 解释在人们理解政治世界中至关重要，但在政治学中仍缺乏系统分析，现有方法零散且通常针对特定问题。

Method: 作者训练了一种轻量级因果语言模型，该模型返回因果主张的结构化数据集，以用于下游分析。

Result: 展示了如何大规模研究因果解释，并证明了该方法相对于人工编码的标注需求低、可推广性强且准确性较高。

Conclusion: 该方法为政治文本中因果解释的大规模分析提供了一种高效且准确的工具。

Abstract: Explanations are a fundamental element of how people make sense of the political world. Citizens routinely ask and answer questions about why events happen, who is responsible, and what could or should be done differently. Yet despite their importance, explanations remain an underdeveloped object of systematic analysis in political science, and existing approaches are fragmented and often issue-specific. I introduce a framework for detecting and parsing explanations in political text. To do this, I train a lightweight causal language model that returns a structured data set of causal claims in the form of cause-effect pairs for downstream analysis. I demonstrate how causal explanations can be studied at scale, and show the method's modest annotation requirements, generalizability, and accuracy relative to human coding.

</details>


### [7] [Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs](https://arxiv.org/abs/2512.03310)
*Kunj Joshi,David A. Smith*

Main category: cs.CL

TL;DR: 介绍了一种新的隐私保护微调技术RMFT，用于减少语言模型对PII的记忆，同时尽量保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前自然语言模型对训练数据中PII的记忆构成严重的安全和隐私风险。

Method: 引入随机掩码微调（RMFT），使用Enron电子邮件数据集进行实验。

Result: RMFT在总提取率和已见提取率上分别减少了80.81%和80.17%，仅使困惑度增加5.73%。

Conclusion: RMFT在隐私和性能之间实现了良好平衡，优于去重方法。

Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.

</details>


### [8] [Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní](https://arxiv.org/abs/2512.03334)
*Nemika Tyagi,Nelvin Licona Guevara,Olga Kellert*

Main category: cs.CL

TL;DR: 本研究提出了一种利用大型语言模型（LLM）辅助标注流程，用于分析西班牙语-英语和西班牙语-瓜拉尼语这两种不同类型语境中的双语语篇的社会语言学和主题分析。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在通过自动标注话题、体裁和话语-语用功能，结合双语语料库中的统计信息，揭示社会语言学和主题分布之间的系统联系。

Method: 使用大型语言模型自动标注了3,691个代码切换句子的话题、体裁和话语-语用功能，整合了迈阿密双语语料库的人口统计元数据，并为西班牙语-瓜拉尼语数据集增加了新的话题标注。

Result: 结果显示，迈阿密数据中存在性别、语言主导权和话语功能之间的系统性联系，以及在巴拉圭文本中形式化的瓜拉尼语和非正式的西班牙语之间的明显双言制分工。

Conclusion: 研究表明，大型语言模型能够可靠地识别出通常仅通过手动标注才能获得的可解释的社会语言学模式，推动了跨语言和资源的计算双语研究方法。

Abstract: This study presents an LLM-assisted annotation pipeline for the sociolinguistic and topical analysis of bilingual discourse in two typologically distinct contexts: Spanish-English and Spanish-Guaraní. Using large language models, we automatically labeled topic, genre, and discourse-pragmatic functions across a total of 3,691 code-switched sentences, integrated demographic metadata from the Miami Bilingual Corpus, and enriched the Spanish-Guaraní dataset with new topic annotations. The resulting distributions reveal systematic links between gender, language dominance, and discourse function in the Miami data, and a clear diglossic division between formal Guaraní and informal Spanish in Paraguayan texts. These findings replicate and extend earlier interactional and sociolinguistic observations with corpus-scale quantitative evidence. The study demonstrates that large language models can reliably recover interpretable sociolinguistic patterns traditionally accessible only through manual annotation, advancing computational methods for cross-linguistic and low-resource bilingual research.

</details>


### [9] [PERCS: Persona-Guided Controllable Biomedical Summarization Dataset](https://arxiv.org/abs/2512.03340)
*Rohan Charudatt Salvi,Chirag Chawla,Dhruv Jain,Swapnil Panigrahi,Md Shad Akhtar,Shweta Yadav*

Main category: cs.CL

TL;DR: 介绍了一个名为PERCS的数据集，该数据集包含针对四种不同角色（普通人、医学预科生、非医学研究人员和医学专家）量身定制的生物医学摘要，强调了面向特定受众的可控摘要的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学文本简化资源通常假设受众是单一和通用的，忽视了不同用户群体在医学素养和信息需求上的广泛差异。因此，需要针对特定受众的简化摘要。

Method: 创建了PERCS数据集，包括针对四种角色的生物医学摘要，并由医生审核其事实准确性和角色一致性。此外，对四种大型语言模型在PERCS上进行了基准测试。

Result: 技术验证显示，不同角色的摘要在可读性、词汇和内容深度上有明显差异。数据集、注释指南和评估材料已公开，支持特定角色沟通和可控制生物医学摘要的研究。

Conclusion: PERCS数据集为改进面向不同受众的医学文本简化提供了重要资源，并为未来研究建立了基准。

Abstract: Automatic medical text simplification plays a key role in improving health literacy by making complex biomedical research accessible to diverse readers. However, most existing resources assume a single generic audience, overlooking the wide variation in medical literacy and information needs across user groups. To address this limitation, we introduce PERCS (Persona-guided Controllable Summarization), a dataset of biomedical abstracts paired with summaries tailored to four personas: Laypersons, Premedical Students, Non-medical Researchers, and Medical Experts. These personas represent different levels of medical literacy and information needs, emphasizing the need for targeted, audience-specific summarization. Each summary in PERCS was reviewed by physicians for factual accuracy and persona alignment using a detailed error taxonomy. Technical validation shows clear differences in readability, vocabulary, and content depth across personas. Along with describing the dataset, we benchmark four large language models on PERCS using automatic evaluation metrics that assess comprehensiveness, readability, and faithfulness, establishing baseline results for future research. The dataset, annotation guidelines, and evaluation materials are publicly available to support research on persona-specific communication and controllable biomedical summarization.

</details>


### [10] [Idea-Gated Transformers: Enforcing Semantic Coherence via Differentiable Vocabulary Pruning](https://arxiv.org/abs/2512.03343)
*Darshan Fofadiya*

Main category: cs.CL

TL;DR: 提出了一种新的Idea-Gated Transformer架构，通过分离语义规划与句法生成，解决自回归语言模型中的主题漂移问题。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型由于依赖局部关联而非全局规划，容易在生成过程中出现主题漂移，尽管增加模型规模可以缓解，但并未根本解决问题。

Method: 引入Idea-Gated Transformer，包括一个辅助的Idea Head，预测未来上下文的词袋分布，生成一个概念向量，通过可微的gating机制在主词汇生成过程中抑制语义不相关的标记。

Result: 在WikiText-103上，Idea-Gated模型与标准GPT-2基线相比，取得了可比较的验证困惑度，但在领域保持上表现显著优越，能够成功锁定生成到特定的语义簇。

Conclusion: Idea-Gated Transformer通过语义与句法的分离，有效抑制了主题漂移，为更可控的语言建模提供了一种参数高效的路径。

Abstract: Autoregressive Language Models (LLMs) trained on Next-Token Prediction (NTP) often suffer from ``Topic Drift'' where the generation wanders away from the initial prompt due to a reliance on local associations rather than global planning \citep{holtzman2019curious}. While scaling model size mitigates this \citep{brown2020language}, the fundamental myopia of the NTP objective remains. In this work, we introduce the Idea-Gated Transformer, a novel architecture that separates semantic planning from syntactic generation. We introduce an auxiliary ``Idea Head'' trained to predict the bag-of-words distribution for a future context window, creating a latent ``Concept Vector'' that actively gates the main vocabulary during generation. We propose a differentiable gating mechanism that suppresses semantically irrelevant tokens, effectively pruning the search space in real-time. Experiments on WikiText-103 demonstrate that while the Idea-Gated model achieves comparable validation perplexity to a standard GPT-2 baseline, it exhibits significantly superior Domain Retention. Qualitative and quantitative analysis reveals that the gating mechanism successfully locks generation into specific semantic clusters (e.g., Finance, Science) and resists associative drift, offering a parameter-efficient path toward more controllable language modeling.

</details>


### [11] [From Hypothesis to Premises: LLM-based Backward Logical Reasoning with Selective Symbolic Translation](https://arxiv.org/abs/2512.03360)
*Qingchuan Li,Mingyue Cheng,Zirui Liu,Daoyu Wang,Yuting Zeng,Tongxuan Liu*

Main category: cs.CL

TL;DR: 提出了一种新颖的假设驱动反向逻辑推理框架HBLR，通过置信度感知符号翻译和假设驱动的反向推理，提高逻辑推理的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在逻辑推理方面依赖前向推理，存在冗余推理路径、幻觉步骤和语义漂移，导致推理效率低和不可靠。

Method: HBLR框架包括置信度感知的符号翻译和假设驱动的反向推理。翻译阶段仅将高置信度片段转换为逻辑形式，不确定内容保留为自然语言；推理阶段模拟人类演绎思维，从结论假设为真并递归验证其前提。

Result: 在五个推理基准测试上的广泛实验表明，HBLR在准确性和效率方面始终优于强基线。

Conclusion: HBLR框架通过结合置信度感知符号翻译和假设驱动的反向推理，显著提升了逻辑推理的效率和可靠性。

Abstract: Logical reasoning is a core challenge in natural language understanding and a fundamental capability of artificial intelligence, underpinning scientific discovery, mathematical theorem proving, and complex decision-making. Despite the remarkable progress of large language models (LLMs), most current approaches still rely on forward reasoning paradigms, generating step-by-step rationales from premises to conclusions. However, such methods often suffer from redundant inference paths, hallucinated steps, and semantic drift, resulting in inefficient and unreliable reasoning. In this paper, we propose a novel framework, Hypothesis-driven Backward Logical Reasoning (HBLR). The core idea is to integrate confidence-aware symbolic translation with hypothesis-driven backward reasoning. In the translation phase, only high-confidence spans are converted into logical form, such as First-Order Logic (FOL), while uncertain content remains in natural language. A translation reflection module further ensures semantic fidelity by evaluating symbolic outputs and reverting lossy ones back to text when necessary. In the reasoning phase, HBLR simulates human deductive thinking by assuming the conclusion is true and recursively verifying its premises. A reasoning reflection module further identifies and corrects flawed inference steps, enhancing logical coherence. Extensive experiments on five reasoning benchmarks demonstrate that HBLR consistently outperforms strong baselines in both accuracy and efficiency.

</details>


### [12] [Characterizing Language Use in a Collaborative Situated Game](https://arxiv.org/abs/2512.03381)
*Nicholas Tomlin,Naitian Zhou,Eve Fleisig,Liangyuan,Chen,Téa Wright,Lauren Vinh,Laura X. Ma,Seun Eisape,Ellie French,Tingting Du,Tianjiao Zhang,Alexander Koller,Alane Suhr*

Main category: cs.CL

TL;DR: 本文介绍并分析了从合作视频游戏《Portal 2》中收集的对话数据集。


<details>
  <summary>Details</summary>
Motivation: 合作视频游戏提供了一个丰富的语言数据源，因为玩家必须在复杂环境中通过沟通和推理进行协调。

Method: 作者收集了11.5小时的《Portal 2》合作模式中的口语对话，共计24.5K个话语，并分析了玩家的语言和行为。

Result: 识别出一些在大多数闲聊或任务导向对话数据集中很少出现的语言现象，包括复杂的空间引用、澄清和修复，以及临时公约的形成。

Conclusion: 为支持未来对复杂、情境化、协作问题解决场景中的语言使用分析，作者公开了包括玩家视频、音频、转录、游戏状态数据以及语言数据的手动和自动注释的数据集。

Abstract: Cooperative video games, where multiple participants must coordinate by communicating and reasoning under uncertainty in complex environments, yield a rich source of language data. We collect the Portal Dialogue Corpus: a corpus of 11.5 hours of spoken human dialogue in the co-op mode of the popular Portal 2 virtual puzzle game, comprising 24.5K total utterances. We analyze player language and behavior, identifying a number of linguistic phenomena that rarely appear in most existing chitchat or task-oriented dialogue corpora, including complex spatial reference, clarification and repair, and ad-hoc convention formation. To support future analyses of language use in complex, situated, collaborative problem-solving scenarios, we publicly release the corpus, which comprises player videos, audio, transcripts, game state data, and both manual and automatic annotations of language data.

</details>


### [13] [Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates](https://arxiv.org/abs/2512.03402)
*Yixing Xu,Chao Li,Xuanwu Yin,Spandan Tiwari,Dong Li,Ashish Sirasao,Emad Barsoum*

Main category: cs.CL

TL;DR: 提出了一种新的方法Dual LoRA，通过引入归纳偏置改进了LoRA的性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在低秩假设下训练出的模型性能常常不理想，因此需要一种方法来提升其表现。

Method: 将低秩矩阵分成两组：幅度组和方向组，分别控制参数更新的大小和方向，通过添加ReLU函数到幅度组和符号函数到方向组来模拟全微调的参数更新过程。

Result: 在多种NLP任务上，包括自然语言生成（NLG）、自然语言理解（NLU）和常识推理，Dual LoRA在相同可训练参数数量下始终优于LoRA及其最新变体。

Conclusion: Dual LoRA通过引入归纳偏置，有效提升了LoRA方法的性能，为参数高效微调提供了新的思路。

Abstract: Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language generation (NLG), understanding (NLU), and commonsense reasoning datasets on GPT-2, RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.

</details>


### [14] [PretrainZero: Reinforcement Active Pretraining](https://arxiv.org/abs/2512.03442)
*Xingrun Xing,Zhiyuan Fan,Jie Lou,Guoqi Li,Jiajun Zhang,Debing Zhang*

Main category: cs.CL

TL;DR: 本文提出PretrainZero，一种基于预训练语料的强化主动学习框架，旨在将强化学习从特定领域后训练扩展到通用预训练，突破通用推理能力的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 模仿人类通过通用经验主动学习以实现通用人工智能，现有RL模型在特定领域表现优异，但依赖可验证奖励，限制了通用推理能力的扩展。

Method: PretrainZero采用主动预训练和自我监督学习方法，无需可验证标签或监督微调，直接在通用语料库上用RL预训练推理器。通过解决日益复杂的掩码片段，实现验证扩展。

Result: 在强化预训练中，PretrainZero在MMLU-Pro、SuperGPQA和数学平均基准上显著提升了Qwen3-4B-Base的性能。预训练模型还能为下游RLVR任务提供推理基础模型。

Conclusion: PretrainZero通过主动学习和自我监督学习，成功打破了通用推理中的验证数据壁垒，为通用人工智能的发展提供了新的可能性。

Abstract: Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.

</details>


### [15] [Understanding LLM Reasoning for Abstractive Summarization](https://arxiv.org/abs/2512.03503)
*Haohan Yuan,Siu Cheung Hui,Haopeng Zhang*

Main category: cs.CL

TL;DR: 大型语言模型在需要推理的任务中表现出色，但其在抽象概括任务中的效用仍不明确。本文系统比较了8种推理策略和3种大型推理模型在8个不同数据集上的效果，发现推理策略的有效性高度依赖于具体策略和上下文。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在数学和代码生成等分析性任务中表现优异，但其在抽象概括任务中的表现尚未得到充分验证。本文旨在通过系统比较不同推理策略和模型，填补这一研究空白。

Method: 本文针对概括任务调整了通用推理策略，并在8个不同数据集上对8种推理策略和3种大型推理模型进行了系统、大规模的比较研究，评估了概括的质量和忠实度。

Result: 研究发现，推理不是万能的，其有效性高度依赖于具体策略和上下文。显式推理策略倾向于提高流畅性但牺牲事实基础，而隐式推理在大型推理模型中则呈现相反的模式。增加模型的内部推理预算并不能提高事实一致性，甚至可能损害其表现。

Conclusion: 有效的概括需要忠实压缩，而不是过度思考。推理策略的选择和优化应根据具体任务和数据集进行，以达到最佳效果。

Abstract: While the reasoning capabilities of Large Language Models (LLMs) excel in analytical tasks such as mathematics and code generation, their utility for abstractive summarization remains widely assumed but largely unverified. To bridge this gap, we first tailor general reasoning strategies to the summarization domain. We then conduct a systematic, large scale comparative study of 8 reasoning strategies and 3 Large Reasoning Models (LRMs) across 8 diverse datasets, assessing both summary quality and faithfulness. Our findings show that reasoning is not a universal solution and its effectiveness is highly dependent on the specific strategy and context. Specifically, we observe a trade-off between summary quality and factual faithfulness: explicit reasoning strategies tend to improve fluency at the expense of factual grounding, while implicit reasoning in LRMs exhibits the inverse pattern. Furthermore, increasing an LRM's internal reasoning budget does not improve, and can even hurt, factual consistency, suggesting that effective summarization demands faithful compression rather than creative over-thinking.

</details>


### [16] [Fine-grained Narrative Classification in Biased News Articles](https://arxiv.org/abs/2512.03582)
*Zeba Afroz,Harsh Vardhan,Pawan Bhakuni,Aanchal Punia,Rajdeep Kumar,Md. Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种针对印度新闻媒体中宣传内容的新型细粒度叙事分类方法，并引入了两个新的GPT-4o-mini引导的多跳提示推理框架FANTA和TPTC。


<details>
  <summary>Details</summary>
Motivation: 叙事在宣传中起着组织和情感支架的作用，通过将孤立的劝说技巧组织成连贯的故事，从而为行动提供理由、归咎责任并引发与意识形态阵营的认同。因此，细粒度叙事分类对于分析宣传内容至关重要。

Method: 开发了一个名为INDI-PROP的数据集，包含1,266篇关于两个极化社会政治事件的文章，每篇文章在三个层次上进行了标注：意识形态文章偏见、特定事件的细粒度叙事框架以及劝说技巧。提出了两个GPT-4o-mini引导的多跳提示推理框架FANTA和TPTC，分别利用多层级沟通现象和劝说线索的系统分解进行推理。

Result: 评估结果表明，所提出的框架在每个任务上的表现都显著优于基础基线。

Conclusion: 通过引入新的数据集和框架，该研究为细粒度叙事分类和劝说技巧识别提供了有效工具，并为分析宣传内容开辟了新途径。

Abstract: Narratives are the cognitive and emotional scaffolds of propaganda. They organize isolated persuasive techniques into coherent stories that justify actions, attribute blame, and evoke identification with ideological camps. In this paper, we propose a novel fine-grained narrative classification in biased news articles. We also explore article-bias classification as the precursor task to narrative classification and fine-grained persuasive technique identification. We develop INDI-PROP, the first ideologically grounded fine-grained narrative dataset with multi-level annotation for analyzing propaganda in Indian news media. Our dataset INDI-PROP comprises 1,266 articles focusing on two polarizing socio-political events in recent times: CAA and the Farmers' protest. Each article is annotated at three hierarchical levels: (i) ideological article-bias (pro-government, pro-opposition, neutral), (ii) event-specific fine-grained narrative frames anchored in ideological polarity and communicative intent, and (iii) persuasive techniques. We propose FANTA and TPTC, two GPT-4o-mini guided multi-hop prompt-based reasoning frameworks for the bias, narrative, and persuasive technique classification. FANTA leverages multi-layered communicative phenomena by integrating information extraction and contextual framing for hierarchical reasoning. On the other hand, TPTC adopts systematic decomposition of persuasive cues via a two-stage approach. Our evaluation suggests substantial improvement over underlying baselines in each case.

</details>


### [17] [AlignCheck: a Semantic Open-Domain Metric for Factual Consistency Assessment](https://arxiv.org/abs/2512.03634)
*Ahmad Aghaebrahimian*

Main category: cs.CL

TL;DR: 提出一个可解释的框架，通过分解文本为原子事实并引入加权度量来评估大语言模型的事实一致性，解决了现有评估指标缺乏可解释性和诊断能力的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言处理中表现出色，但容易产生不正确或误导性的论点，尤其是在临床应用中，事实不准确可能导致严重后果。现有评估指标无法充分评估事实一致性且缺乏可解释性。

Method: 将文本分解为原子事实，采用灵活的、无模式的加权度量方法，并引入控制评估复杂性的机制。

Result: 在通用和临床数据集上对提出的方法进行了基准测试，并发布了代码以支持未来研究中事实感知的模型训练。

Conclusion: 提出的框架能够更好地评估和解释大语言模型的事实一致性，为诊断和减少错误提供了有效的工具。

Abstract: Large Language Models have significantly advanced natural language processing tasks, but remain prone to generating incorrect or misleading but plausible arguments. This issue, known as hallucination, is particularly concerning in high-stakes domains like clinical applications, where factual inaccuracies can have severe consequences. Existing evaluation metrics fail to adequately assess factual consistency and lack interpretability, making diagnosing and mitigating errors difficult. We propose an interpretable framework for factual consistency assessment for in-domain and open-domain texts to address these limitations. Our approach decomposes text into atomic facts and introduces a flexible, schema-free methodology. Unlike previous methods with an absolute metric, we incorporate a weighted metric to enhance factual evaluation. Additionally, we propose a mechanism to control assessment complexity in intricate domains. We benchmark our approach on popular general and clinical datasets and release our code to support fact-aware model training in future research.

</details>


### [18] [Generative AI Practices, Literacy, and Divides: An Empirical Analysis in the Italian Context](https://arxiv.org/abs/2512.03671)
*Beatrice Savoldi,Giuseppe Attanasio,Olga Gorodetskaya,Marta Marchiori Manerba,Elisa Bassignana,Silvia Casola,Matteo Negri,Tommaso Caselli,Luisa Bentivogli,Alan Ramponi,Arianna Muti,Nicoletta Balbo,Debora Nozza*

Main category: cs.CL

TL;DR: 本文通过调查数据首次全面描绘了意大利生成式人工智能（GenAI）的采用、使用模式和素养，发现尽管GenAI被广泛使用，但用户数字素养低，存在性别差距，需要针对性教育举措和进一步调查。


<details>
  <summary>Details</summary>
Motivation: 研究生成式人工智能（GenAI）在意大利的使用情况，识别其采用和使用的模式，以及用户的数字素养，并探讨其可能带来的社会影响，尤其是数字鸿沟和性别差距。

Method: 基于新收集的1,906名意大利语成年人的调查数据，进行实证分析，研究GenAI的采用、使用模式和数字素养。

Result: 研究发现GenAI在工作和个人使用中广泛采用，包括情感支持和医疗建议等敏感任务。尽管用户数字素养低，GenAI已成为主要信息来源。存在显著的性别差距，尤其是老年女性采用率和使用频率低于男性。

Conclusion: 研究揭示了GenAI的多用途使用，强调了针对性教育举措的必要性，并呼吁进一步调查导致不平等参与的根本障碍。

Abstract: The rise of Artificial Intelligence (AI) language technologies, particularly generative AI (GenAI) chatbots accessible via conversational interfaces, is transforming digital interactions. While these tools hold societal promise, they also risk widening digital divides due to uneven adoption and low awareness of their limitations. This study presents the first comprehensive empirical mapping of GenAI adoption, usage patterns, and literacy in Italy, based on newly collected survey data from 1,906 Italian-speaking adults. Our findings reveal widespread adoption for both work and personal use, including sensitive tasks like emotional support and medical advice. Crucially, GenAI is supplanting other technologies to become a primary information source: this trend persists despite low user digital literacy, posing a risk as users struggle to recognize errors or misinformation. Moreover, we identify a significant gender divide -- particularly pronounced in older generations -- where women are half as likely to adopt GenAI and use it less frequently than men. While we find literacy to be a key predictor of adoption, it only partially explains this disparity, suggesting that other barriers are at play. Overall, our data provide granular insights into the multipurpose usage of GenAI, highlighting the dual need for targeted educational initiatives and further investigation into the underlying barriers to equitable participation that competence alone cannot explain.

</details>


### [19] [Evaluating Hydro-Science and Engineering Knowledge of Large Language Models](https://arxiv.org/abs/2512.03672)
*Shiruo Hu,Wenbo Shan,Yingjia Li,Zhiqi Wan,Xinpeng Yu,Yunjia Qi,Haotian Xia,Yang Xiao,Dingxiao Liu,Jiaru Wang,Chenxu Gong,Ruixi Zhang,Shuyue Wu,Shibo Cui,Chee Hui Lai,Wei Luo,Yubin He,Bin Xu,Jianshi Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种用于水科学与工程（Hydro-SE）领域的大型语言模型（LLM）评估基准Hydro-SE Bench，包含4000个多选题，涵盖九个子领域，评估了LLM在基础知识、工程应用和推理计算能力方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各个领域的应用潜力被不断挖掘，其在Hydro-SE领域的知识与应用能力却未得到充分评估。因此，需要一个专门的评估基准来填补这一空白，并帮助开发者和研究人员更好地理解LLM在该领域的表现。

Method: 设计了Hydro-SE Bench，包含九个Hydro-SE子领域的4000个多选题，评估了商业LLM和小参数LLM在基础知识、工程应用和推理计算能力方面的表现。

Result: 商业LLM的准确率为0.74到0.80，小参数LLM的准确率为0.41到0.68。LLM在与自然科学密切相关的子领域表现良好，但在行业标准和水利结构等特定领域表现不佳。模型扩展主要改善了推理和计算能力。

Conclusion: LLM在Hydro-SE任务中表现出一定的优势，但在实际工程应用方面仍存在不足。研究结果为模型开发者提供了明确的训练目标，并为Hydro-SE研究人员应用LLM提供了实践指导。

Abstract: Hydro-Science and Engineering (Hydro-SE) is a critical and irreplaceable domain that secures human water supply, generates clean hydropower energy, and mitigates flood and drought disasters. Featuring multiple engineering objectives, Hydro-SE is an inherently interdisciplinary domain that integrates scientific knowledge with engineering expertise. This integration necessitates extensive expert collaboration in decision-making, which poses difficulties for intelligence. With the rapid advancement of large language models (LLMs), their potential application in the Hydro-SE domain is being increasingly explored. However, the knowledge and application abilities of LLMs in Hydro-SE have not been sufficiently evaluated. To address this issue, we propose the Hydro-SE LLM evaluation benchmark (Hydro-SE Bench), which contains 4,000 multiple-choice questions. Hydro-SE Bench covers nine subfields and enables evaluation of LLMs in aspects of basic conceptual knowledge, engineering application ability, and reasoning and calculation ability. The evaluation results on Hydro-SE Bench show that the accuracy values vary among 0.74 to 0.80 for commercial LLMs, and among 0.41 to 0.68 for small-parameter LLMs. While LLMs perform well in subfields closely related to natural and physical sciences, they struggle with domain-specific knowledge such as industry standards and hydraulic structures. Model scaling mainly improves reasoning and calculation abilities, but there is still great potential for LLMs to better handle problems in practical engineering application. This study highlights the strengths and weaknesses of LLMs for Hydro-SE tasks, providing model developers with clear training targets and Hydro-SE researchers with practical guidance for applying LLMs.

</details>


### [20] [In-Context Representation Hijacking](https://arxiv.org/abs/2512.03771)
*Itay Yona,Amir Sarid,Michael Karasik,Yossi Gandelsman*

Main category: cs.CL

TL;DR: 本文介绍了一种名为 Doublespeak 的攻击方法，通过替换上下文中的有害关键词为良性词，绕过大型语言模型的安全防护。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在安全性方面仍存在漏洞，尤其是在上下文表示层面。作者旨在揭示并验证这种通过上下文表示劫持进行攻击的新方法。

Method: 通过将有害关键词替换为良性词，在多个上下文示例中系统性地进行替换，并使用解释性工具追踪这种替换在模型内部表示中的逐层演变。

Result: Doublespeak 在多个模型上实现了高成功率，包括 Llama-3.3-70B-Instruct 的 74% 攻击成功率，证明其广泛适用性和高效性。

Conclusion: Doublespeak 揭示了大型语言模型在表示层面的新攻击面，表明当前的安全防护策略不足，需要在表示层面进行改进。

Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.

</details>


### [21] [Different types of syntactic agreement recruit the same units within large language models](https://arxiv.org/abs/2512.03676)
*Daria Kryvosheieva,Andrea de Varda,Evelina Fedorenko,Greta Tuckute*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可以可靠地区分语法正确的句子与不正确的句子，但语法知识在模型中的表示方式仍是一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 我们研究了不同句法现象在LLMs中是否使用共享或不同的组件。

Method: 采用受认知神经科学启发的功能定位方法，在七个开源模型中识别对67种英语句法现象最敏感的单元。

Result: 发现不同类型的句法一致性（如主谓、反身代词、限定词-名词）会调用重叠的单元集合，表明一致性在LLMs中构成一个有意义的功能类别。

Conclusion: 句法一致性作为句法依赖关系的关键标记，在LLMs的表示空间中构成了一个有意义的类别。

Abstract: Large language models (LLMs) can reliably distinguish grammatical from ungrammatical sentences, but how grammatical knowledge is represented within the models remains an open question. We investigate whether different syntactic phenomena recruit shared or distinct components in LLMs. Using a functional localization approach inspired by cognitive neuroscience, we identify the LLM units most responsive to 67 English syntactic phenomena in seven open-weight models. These units are consistently recruited across sentences containing the phenomena and causally support the models' syntactic performance. Critically, different types of syntactic agreement (e.g., subject-verb, anaphor, determiner-noun) recruit overlapping sets of units, suggesting that agreement constitutes a meaningful functional category for LLMs. This pattern holds in English, Russian, and Chinese; and further, in a cross-lingual analysis of 57 diverse languages, structurally more similar languages share more units for subject-verb agreement. Taken together, these findings reveal that syntactic agreement-a critical marker of syntactic dependencies-constitutes a meaningful category within LLMs' representational spaces.

</details>


### [22] [BERnaT: Basque Encoders for Representing Natural Textual Diversity](https://arxiv.org/abs/2512.03903)
*Ekhi Azurmendi,Joseba Fernandez de Landa,Jaione Bengoetxea,Maite Heredia,Julen Etxaniz,Mikel Zubillaga,Ander Soraluze,Aitor Soroa*

Main category: cs.CL

TL;DR: 该论文主张语言模型应捕捉语言的全部变体，而非仅依赖标准化文本，通过在巴斯克语上实验，发现训练包含多样语言的数据集可以提升模型在所有任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 语言模型常因质量过滤而排除非标准语言变体，导致模型鲁棒性降低和代表性偏见。该论文旨在解决这一问题，主张模型应涵盖全部语言变体。

Method: 论文聚焦巴斯克语，构建结合标准、社交媒体和历史来源的新数据集，并预训练了三种配置的BERnaT编码器模型。提出一种评价框架，将自然语言理解任务分为标准和多样子集以评估语言泛化能力。

Result: 训练数据集包含标准和多样数据的模型始终优于仅用标准数据训练的模型，且在所有任务类型上表现更佳，同时不牺牲标准基准的准确率。

Conclusion: 语言多样性对于构建包容性强、泛化能力高的语言模型至关重要。

Abstract: Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.

</details>


### [23] [AITutor-EvalKit: Exploring the Capabilities of AI Tutors](https://arxiv.org/abs/2512.03688)
*Numaan Naeem,Kaushal Kumar Maurya,Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: AITutor-EvalKit是一个利用语言技术评估AI导师教学质量的工具。


<details>
  <summary>Details</summary>
Motivation: 为教育相关方和*ACL社区提供一个支持学习和收集用户反馈的工具。

Method: 应用语言技术，提供演示和评估软件，以及模型检查和数据可视化功能。

Result: 开发出一个能够评估AI导师教学质量和收集用户反馈的应用工具。

Conclusion: AITutor-EvalKit不仅支持学习，还能帮助教育者和研究人员改进AI导师的教学效果。

Abstract: We present AITutor-EvalKit, an application that uses language technology to evaluate the pedagogical quality of AI tutors, provides software for demonstration and evaluation, as well as model inspection and data visualization. This tool is aimed at education stakeholders as well as *ACL community at large, as it supports learning and can also be used to collect user feedback and annotations.

</details>


### [24] [Jina-VLM: Small Multilingual Vision Language Model](https://arxiv.org/abs/2512.04032)
*Andreas Koukounas,Georgios Mastrapas,Florian Hönicke,Sedigheh Eslami,Guillaume Roncari,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: Jina-VLM是一个24亿参数的视觉语言模型，在多语言视觉问答任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在多语言环境下高效处理视觉问答任务的模型。

Method: 将SigLIP2视觉编码器与Qwen3语言模型通过注意力池化连接，实现任意分辨率图像的高效处理。

Result: 在标准VQA基准和多语言评估中，Jina-VLM优于同类模型，同时保持有竞争力的纯文本性能。

Conclusion: Jina-VLM是一个高效的视觉语言模型，特别适合多语言视觉问答任务。

Abstract: We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.

</details>


### [25] [DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue](https://arxiv.org/abs/2512.03704)
*Yijun Liao*

Main category: cs.CL

TL;DR: 提出DZ-TDPO框架解决长对话中的状态惯性问题，通过动态KL约束和可学习的时间注意力偏置实现非破坏性对齐


<details>
  <summary>Details</summary>
Motivation: 长对话系统存在状态惯性，静态约束阻碍模型解决用户意图与历史上下文的冲突

Method: DZ-TDPO框架结合冲突感知动态KL约束和可学习的时间注意力偏置

Result: 在MSC数据集上达到SOTA胜率(Phi-3.5为86.2%，Qwen2.5-7B为99.4%)，保持强零样本泛化能力

Conclusion: 通过精确注意力调节可缓解状态惯性，无需破坏性权重更新即可保持模型通用能力(MMLU)

Abstract: Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a learnable temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (86.2% on Phi-3.5) while maintaining robust zero-shot generalization. Crucially, our scaling analysis reveals a "Capacity-Stability Trade-off": while smaller models incur an "alignment tax" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves near-perfect alignment (99.4% win rate) with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO

</details>


### [26] [SkillFactory: Self-Distillation For Learning Cognitive Behaviors](https://arxiv.org/abs/2512.04072)
*Zayne Sprague,Jack Lu,Manya Wadhwa,Sedrick Keh,Mengye Ren,Greg Durrett*

Main category: cs.CL

TL;DR: SkillFactory是一种在强化学习（RL）之前通过监督微调（SFT）让模型学习认知技能的方法。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型未展示某些技能时，如何让模型利用这些技能的问题。

Method: 使用模型自身生成的样本，通过重新排列成技能格式进行SFT，然后进行RL训练。

Result: 1. SkillFactory SFT初始化有助于模型在RL后泛化到更难的任务变种；2. 模型确实使用了认知技能；3. RLed SkillFactory模型在领域外任务上更稳健。

Conclusion: 在RL之前学习的归纳偏置帮助模型学习更稳健的认知技能使用。

Abstract: Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These "silver" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.

</details>


### [27] [Enhancing Instruction-Following Capabilities in Seq2Seq Models: DoLA Adaptations for T5](https://arxiv.org/abs/2512.03803)
*Huey Sun,Anabel Yong,Lorenzo Gilly,Felipe Jin*

Main category: cs.CL

TL;DR: 该研究首次将DoLa对比解码方法适配到T5和FLAN-T5编码器-解码器架构，并评估其对模型指令遵循能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有对比解码方法（如DoLa）仅适用于解码器架构且聚焦事实性提升，缺乏在编码器-解码器架构中的探索，尤其是指令遵循能力的评估。

Method: 将DoLa算法应用于T5/FLAN-T5模型，通过分层对比分析生成结果的忠实度，并采用逐层logit演化分析量化DoLa对token概率的影响。

Result: DoLa对特定类别任务的生成忠实度有提升，但会损害其他任务；通过FLAN-T5模型分层分析揭示了不同层对输出概率的影响差异。

Conclusion: 该研究首次验证了对比解码在编码器-解码器架构中的有效性，并揭示了任务类型对DoLa效果的调节作用，为未来解码策略优化提供了分层分析依据。

Abstract: Contrastive decoding is a lightweight and effective inference-time method that improves the quality of text generation in Large Language Models. However, algorithms such as DoLa (Decoding by Contrastive Layers) have only been implemented in decoder-only architectures and studied for their impact on improving factuality. This work adapts DoLa for the T5 and FLAN-T5 model families and evaluates its impact on the models' instruction following capabilities, which to our knowledge is the first implementation of a contrastive decoding strategy in an encoder-decoder architecture. Our results show that DoLa improves the faithfulness of text generation for certain categories of tasks and harms others. To understand these results, we present a layer-by-layer analysis of logit evolution in a FLAN-T5 model to quantify DoLa's impact on token output probabilities.

</details>


### [28] [Improving Alignment Between Human and Machine Codes: An Empirical Assessment of Prompt Engineering for Construct Identification in Psychology](https://arxiv.org/abs/2512.03818)
*Kylie L. Anglin,Stephanie Milan,Brittney Hernandez,Claudia Ventura*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在文本分类中表现优异，但输出受提示措辞影响。本文提出一个经验框架，通过提示工程优化LLM在文本中识别构念的表现。


<details>
  <summary>Details</summary>
Motivation: 现有文献对心理学等需要精确理论驱动定义领域的提示工程研究不足，尤其是在分类任务方面。

Method: 实验评估了五种提示策略：基于代码手册的经验提示选择、自动提示工程、角色提示、思维链推理和解释性提示，并进行零样本和少样本分类。

Result: 角色提示、思维链和解释性提示不能完全解决因提示措辞不当导致的性能下降。提示的最主要特征是构念定义、任务框架，以及提供的示例。结合代码手册和自动提示工程的少样本提示在分类上最符合专家判断。

Conclusion: 建议研究人员生成并评估尽可能多的提示变体，选择基于训练数据集经验表现的提示和示例，并在保留集合中验证最终方法，以优化LLM提示。

Abstract: Due to their architecture and vast pre-training data, large language models (LLMs) demonstrate strong text classification performance. However, LLM output - here, the category assigned to a text - depends heavily on the wording of the prompt. While literature on prompt engineering is expanding, few studies focus on classification tasks, and even fewer address domains like psychology, where constructs have precise, theory-driven definitions that may not be well represented in pre-training data. We present an empirical framework for optimizing LLM performance for identifying constructs in texts via prompt engineering. We experimentally evaluate five prompting strategies --codebook-guided empirical prompt selection, automatic prompt engineering, persona prompting, chain-of-thought reasoning, and explanatory prompting - with zero-shot and few-shot classification. We find that persona, chain-of-thought, and explanations do not fully address performance loss accompanying a badly worded prompt. Instead, the most influential features of a prompt are the construct definition, task framing, and, to a lesser extent, the examples provided. Across three constructs and two models, the classifications most aligned with expert judgments resulted from a few-shot prompt combining codebook-guided empirical prompt selection with automatic prompt engineering. Based on our findings, we recommend that researchers generate and evaluate as many prompt variants as feasible, whether human-crafted, automatically generated, or ideally both, and select prompts and examples based on empirical performance in a training dataset, validating the final approach in a holdout set. This procedure offers a practical, systematic, and theory-driven method for optimizing LLM prompts in settings where alignment with expert judgment is critical.

</details>


### [29] [Training and Evaluation of Guideline-Based Medical Reasoning in LLMs](https://arxiv.org/abs/2512.03838)
*Michael Staniek,Artem Sokolov,Stefan Riezler*

Main category: cs.CL

TL;DR: 本文探讨了如何通过微调LLMs来遵循医学共识指南，以实现医学早期预测并生成可靠解释。


<details>
  <summary>Details</summary>
Motivation: 当前医学机器学习注重预测准确性，却忽视了医疗从业者所需的可靠解释，因此需要让LLMs遵循医学共识指南。

Method: 通过将医学推理规则实例化到电子健康记录中，对LLMs进行微调，使其学习医学共识规则及例外情况，并评估模型的推理过程。

Result: 微调的小模型在推导正确性上表现优于更大的LLMs单次学习模型，并且通过多模态设置提高了时间序列预测结果。

Conclusion: 微调LLMs以遵循医学共识指南可以显著提升推导正确性，未来的挑战在于时间序列数据的预测。

Abstract: Machine learning for early prediction in medicine has recently shown breakthrough performance, however, the focus on improving prediction accuracy has led to a neglect of faithful explanations that are required to gain the trust of medical practitioners. The goal of this paper is to teach LLMs to follow medical consensus guidelines step-by-step in their reasoning and prediction process. Since consensus guidelines are ubiquitous in medicine, instantiations of verbalized medical inference rules to electronic health records provide data for fine-tuning LLMs to learn consensus rules and possible exceptions thereof for many medical areas. Consensus rules also enable an automatic evaluation of the model's inference process regarding its derivation correctness (evaluating correct and faithful deduction of a conclusion from given premises) and value correctness (comparing predicted values against real-world measurements). We exemplify our work using the complex Sepsis-3 consensus definition. Our experiments show that small fine-tuned models outperform one-shot learning of considerably larger LLMs that are prompted with the explicit definition and models that are trained on medical texts including consensus definitions. Since fine-tuning on verbalized rule instantiations of a specific medical area yields nearly perfect derivation correctness for rules (and exceptions) on unseen patient data in that area, the bottleneck for early prediction is not out-of-distribution generalization, but the orthogonal problem of generalization into the future by forecasting sparsely and irregularly sampled clinical variables. We show that the latter results can be improved by integrating the output representations of a time series forecasting model with the LLM in a multimodal setup.

</details>


### [30] [Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers](https://arxiv.org/abs/2512.03870)
*Hongzhan Lin,Zhiqi Bai,Xinmiao Zhang,Sen Yang,Xiang Li,Siran Yang,Yunlong Xu,Jiaheng Liu,Yongchi Zhao,Jiamang Wang,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: 提出FusedKV和FusedKV-Lite以减少Transformer解码器的KV缓存内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer解码器在长序列长度下，KV缓存内存需求巨大。虽然跨层KV缓存共享方法可以缓解这一问题，但通常性能不如层内方法。

Method: 通过研究高层键和值的信息流，提出FusedKV，其中高层KV缓存是底层和中间层信息的可学习融合。进一步提出FusedKV-Lite以优化效率。

Result: 在332M到4B参数的LLMs实验中，所提方法减少50%缓存内存，并且验证困惑度低于标准Transformer解码器。

Conclusion: FusedKV和FusedKV-Lite是内存高效、高性能的Transformer解码器架构替代方案。

Abstract: Transformer decoders have achieved strong results across tasks, but the memory required for the KV cache becomes prohibitive at long sequence lengths. Although Cross-layer KV Cache sharing (e.g., YOCO, CLA) offers a path to mitigate KV Cache bottleneck, it typically underperforms within-layer methods like GQA. To understand the root cause, we investigate the information flow of keys and values of the top-layers. Our preliminary reveals a clear distribution: values are predominantly derived from the bottom layer, while keys draw more information from both bottom and middle layers. Building upon this, we propose FusedKV, whose top-layer KV caches are a learnable fusion of the most informative ones from the bottom and middle layers. This fusion operates directly on post-RoPE keys, preserving relative positional information without the computational cost of re-applying rotary embeddings. To further improve efficiency, we propose FusedKV-Lite, an cross-layer sharing approach, where top-layer KV caches are directly derived from the bottom-layer values and the middle-layer keys. Compared to FusedKV, FusedKV-Lite reduces I/O overhead at the cost of a slight increase in perplexity. In experiments on LLMs ranging from 332M to 4B parameters, our proposed method reduce 50\% cache memory while achieving lower validation perplexity than the standard Transformer decoder, establishing it as a memory-efficient, high-performance architectural alternative.

</details>


### [31] [Is Lying Only Sinful in Islam? Exploring Religious Bias in Multilingual Large Language Models Across Major Religions](https://arxiv.org/abs/2512.03943)
*Kazi Abrab Hossain,Jannatul Somiya Mahmud,Maria Hossain Tuli,Anik Mitra,S. M. Taiabul Haque,Farig Y. Sadeque*

Main category: cs.CL

TL;DR: 论文介绍了BRAND数据集，旨在提高多语言模型对南亚四种主要宗教的表述准确性，并揭示模型中的语言偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在偏见检测和分类方面有所进展，但在宗教等敏感话题上，多语言模型常常出现严重误解，尤其是对南亚的宗教。

Method: 引入了BRAND：双语宗教责任规范数据集，涵盖南亚四种主要宗教，包含2400多条条目，并使用英语和孟加拉语进行三种不同类型的提示实验。

Result: 结果表明，模型在英语中表现优于孟加拉语，且即使在宗教中立问题上，也存在对伊斯兰教的偏见。

Conclusion: 多语言模型在不同语言下对类似问题存在持续偏见，这突出了HCI领域在宗教和灵性问题上更广泛的问题。

Abstract: While recent developments in large language models have improved bias detection and classification, sensitive subjects like religion still present challenges because even minor errors can result in severe misunderstandings. In particular, multilingual models often misrepresent religions and have difficulties being accurate in religious contexts. To address this, we introduce BRAND: Bilingual Religious Accountable Norm Dataset, which focuses on the four main religions of South Asia: Buddhism, Christianity, Hinduism, and Islam, containing over 2,400 entries, and we used three different types of prompts in both English and Bengali. Our results indicate that models perform better in English than in Bengali and consistently display bias toward Islam, even when answering religion-neutral questions. These findings highlight persistent bias in multilingual models when similar questions are asked in different languages. We further connect our findings to the broader issues in HCI regarding religion and spirituality.

</details>


### [32] [Adapting Large Language Models to Low-Resource Tibetan: A Two-Stage Continual and Supervised Fine-Tuning Study](https://arxiv.org/abs/2512.03976)
*Lifeng Chen,Ryan Lai,Tianming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种两阶段方法，将Qwen2.5-3B模型适应于藏语，通过持续预训练和受监督微调显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和跨语言漂移，将大型语言模型（LLMs）适应到低资源语言仍是一项重大挑战，尤其是对于形态丰富且代表性不足的藏语。

Method: 采用两阶段方法：首先通过持续预训练（CPT）建立藏语语言基础，然后通过受监督微调（SFT）实现任务和翻译专业化。

Result: 实证评估显示困惑度持续下降（从2.98降至1.54），汉译藏质量显著提高（BLEU: 0.046→0.261；chrF: 2.2→6.6）。

Conclusion: CPT构建了藏语语义流形，而SFT则以最小的表示干扰增强了任务对齐。该研究首次定量探讨了LLMs在藏语适应中的动态，并为扩展多语言基础模型到低资源环境提供了一个开放且可复制的框架。

Abstract: Adapting large language models (LLMs) to low-resource languages remains a major challenge due to data scarcity and cross-lingual drift. This work presents a two-stage adaptation of Qwen2.5-3B to Tibetan, a morphologically rich and underrepresented language. We employ Continual Pretraining (CPT) to establish Tibetan linguistic grounding, followed by Supervised Fine-Tuning (SFT) for task and translation specialization. Empirical evaluations demonstrate a consistent decrease in perplexity (from 2.98 $\rightarrow$ 1.54) and substantial improvements in Chinese$\rightarrow$Tibetan translation quality (BLEU: 0.046 $\rightarrow$ 0.261; chrF: 2.2 $\rightarrow$ 6.6). Layer-wise analysis across 435 layers in Qwen3-4B reveals that adaptation primarily concentrates on embedding and output heads, with mid--late MLP projections encoding domain-specific transformations. Our findings suggest that CPT constructs a Tibetan semantic manifold while SFT sharpens task alignment with minimal representational disruption. This study provides the first quantitative exploration of Tibetan adaptation dynamics for LLMs, and offers an open, reproducible framework for extending multilingual foundation models to low-resource settings.

</details>


### [33] [Teaching Old Tokenizers New Words: Efficient Tokenizer Adaptation for Pre-trained Models](https://arxiv.org/abs/2512.03989)
*Taido Purason,Pavel Chizhov,Ivan P. Yamshchikov,Mark Fishel*

Main category: cs.CL

TL;DR: 本文提出了两种改进预训练语言模型适应新领域或语言的 tokenizer 调整方法：继续 BPE 训练和基于叶节点的词汇剪枝。


<details>
  <summary>Details</summary>
Motivation: Tokenizer 适应在将预训练语言模型转移到新领域或语言方面起着重要作用，尤其是在词汇扩展和剪枝过程中。

Method: 1. 继续 BPE 训练：通过在领域特定文本上继续 BPE 合并学习过程来调整预训练的 tokenizer。2. 基于叶节点的词汇剪枝：在保持模型质量的同时移除冗余词汇。

Result: 在多种语言及模型家族的实验中，继续 BPE 训练提高了分词效率并更好地利用了新增词汇；基于叶节点的词汇剪枝在保持模型质量的同时减少了冗余词汇。

Conclusion: 这两种方法为受控词汇表修改提供了实用工具，并作为开源包发布。

Abstract: Tokenizer adaptation plays an important role in transferring pre-trained language models to new domains or languages. In this work, we address two complementary aspects of this process: vocabulary extension and pruning. The common approach to extension trains a new tokenizer on domain-specific text and appends the tokens that do not overlap with the existing vocabulary, which often results in many tokens that are unreachable or never used. We propose continued BPE training, which adapts a pre-trained tokenizer by continuing the BPE merge learning process on new data. Experiments across multiple languages and model families show that this approach improves tokenization efficiency and leads to better utilization of added vocabulary. We also introduce leaf-based vocabulary pruning, which removes redundant tokens while preserving model quality. Together, these methods provide practical tools for controlled vocabulary modification, which we release as an open-source package.

</details>


### [34] [AugServe: Adaptive Request Scheduling for Augmented Large Language Model Inference Serving](https://arxiv.org/abs/2512.04013)
*Ying Wang,Zhen Jin,Jiexiong Xu,Wenhai Lin,Yiquan Chen,Wenzhi Chen*

Main category: cs.CL

TL;DR: 本文介绍了一种名为AugServe的高效推理框架，旨在通过两阶段自适应请求调度和动态调整token批处理机制，减少排队延迟，提高增强型大型语言模型（LLM）推理服务的有效吞吐量。


<details>
  <summary>Details</summary>
Motivation: 增强型大型语言模型在网页应用中越来越受欢迎，提高推理服务的效率和优化服务级目标（SLO）对提升用户体验至关重要。然而，现有系统存在两个主要挑战：(i) 先来先服务（FCFS）调度导致严重的头阻塞，造成许多请求排队延迟超过SLO；(ii) 静态批处理token限制无法适应负载和硬件条件波动。

Method: AugServe采用两阶段自适应请求调度策略：第一阶段结合增强型LLM请求的推理特征优化调度决策顺序，第二阶段根据运行时信息持续优化决策。此外，AugServe根据硬件状态和实时负载动态调整token批处理机制。

Result: 实验结果表明，AugServe在有效吞吐量上比vLLM和InferCept分别高出4.7-33.1倍和3.3-13.2倍，同时将首次令牌时间（TTFT）分别减少最多96.3%和95.0%。

Conclusion: AugServe通过其创新的两阶段自适应调度和动态批处理机制，显著提高了增强型LLM推理服务的效率和用户体验。

Abstract: As augmented large language models (LLMs) with external tools become increasingly popular in web applications, improving augmented LLM inference serving efficiency and optimizing service-level objectives (SLOs) are critical for enhancing user experience. To achieve this, inference systems must maximize request handling within latency constraints, referred to as increasing effective throughput. However, existing systems face two major challenges: (i) reliance on first-come-first-served (FCFS) scheduling causes severe head-of-line blocking, leading to queuing delays exceeding the SLOs for many requests; and (ii) static batch token limit, which fails to adapt to fluctuating loads and hardware conditions. Both of these factors degrade effective throughput and service quality.
  This paper presents AugServe, an efficient inference framework designed to reduce queueing latency and enhance effective throughput for augmented LLM inference services. The core idea of AugServe is a two-stage adaptive request scheduling strategy. Specifically, AugServe combines the inference features of augmented LLM requests to optimize the order of scheduling decisions (stage I). These decisions are continuously refined with runtime information (stage II), adapting to both request characteristics and system capabilities. In addition, AugServe dynamically adjusts the token batching mechanism based on hardware status and real-time load, further enhancing throughput performance. Experimental results show that AugServe achieves 4.7-33.1x and 3.3-13.2x higher effective throughput than vLLM and InferCept, while reducing time-to-first-token (TTFT) by up to 96.3% and 95.0%, respectively.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation](https://arxiv.org/abs/2512.03048)
*Austin Spizzirri*

Main category: cs.AI

TL;DR: 论文主张AI对齐应重新构想为通过基于过程的多智能体发展机制来构建对理由敏感的 syntropic 智能体，而非编码固定的人类价值内容。


<details>
  <summary>Details</summary>
Motivation: 解决基于内容的价值规范在结构上不稳定的问题，原因包括是-应当鸿沟、价值多元主义和扩展框架问题。

Method: 提出 syntropy 作为理解多智能体对齐动态的信息理论框架，并基于兼容主义的控制理论区分真实与模拟的道德能力。

Result: 建立了独立于现象学主张的可操作标准，并提出了具体的、可证伪的关于人工系统中价值涌现和道德能动性的预测。

Conclusion: 该论文是更广泛研究计划的哲学部分，其实证验证仍在准备中的单独项目里进行，但框架本身已生成可验证的具体预测。

Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.

</details>


### [36] [Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI](https://arxiv.org/abs/2512.03072)
*Hu Keyi*

Main category: cs.AI

TL;DR: 本文提出了一种名为'Weight-Calculatism'的新型认知架构，旨在解决AI的可解释性和价值对齐问题，为人工通用智能（AGI）提供了一种可行路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI范式在可解释性和价值对齐方面面临挑战，需要一种基于第一原理的架构来实现可解释、可追溯的AGI。

Method: 将认知解构为不可再分的逻辑原子（Logical Atoms）和两个基本操作：指向（Pointing）和比较（Comparison），通过可解释的权重计算模型（Weight = Benefit * Probability）形式化决策，所有值都追溯至可审计的初始权重集。

Result: 该架构在透明、类人的推理和前所未有的场景中的稳健学习方面取得了成效，为构建值得信赖和对齐的AGI奠定了实际和理论基础。

Conclusion: Weight-Calculatism架构通过原子化解构和权重计算，实现了根本性的可解释性、内在的通用性和可追溯的价值对齐，是通向可信AGI的一条有前景的路径。

Abstract: Current AI paradigms, as "architects of experience," face fundamental challenges in explainability and value alignment. This paper introduces "Weight-Calculatism," a novel cognitive architecture grounded in first principles, and demonstrates its potential as a viable pathway toward Artificial General Intelligence (AGI). The architecture deconstructs cognition into indivisible Logical Atoms and two fundamental operations: Pointing and Comparison. Decision-making is formalized through an interpretable Weight-Calculation model (Weight = Benefit * Probability), where all values are traceable to an auditable set of Initial Weights. This atomic decomposition enables radical explainability, intrinsic generality for novel situations, and traceable value alignment. We detail its implementation via a graph-algorithm-based computational engine and a global workspace workflow, supported by a preliminary code implementation and scenario validation. Results indicate that the architecture achieves transparent, human-like reasoning and robust learning in unprecedented scenarios, establishing a practical and theoretical foundation for building trustworthy and aligned AGI.

</details>


### [37] [When Do Symbolic Solvers Enhance Reasoning in Large Language Models?](https://arxiv.org/abs/2512.03272)
*Zhiyuan He,Dingmin Wang*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂推理任务中表现出色，但长思维链（CoTs）可能导致高标记开销和错误答案。本文探讨符号求解器何时能增强长CoT。


<details>
  <summary>Details</summary>
Motivation: 长CoT可能导致模型“过度思考”，增加开销并降低准确性。符号求解器通过代码生成和符号求解提供了一种替代方案，但其适用场景尚不明确。

Method: 实验比较了长CoT和符号求解器在不同类型推理任务中的表现，包括演绎问题和约束满足问题，并考察了提供示例的影响。

Result: 符号求解器在问题涉及有限隐式推理但搜索空间大时表现优越。GPT-4o在浅层演绎问题上表现更佳，而符号求解器显著提升了模型在需回溯的约束满足问题中的性能。提供示例可使CodeLlama-13B在复杂Zebra谜题中超越GPT-4o。

Conclusion: 符号求解器在特定条件下能显著增强LLMs的推理能力，尤其是在约束满足问题中。提供合适的示例能进一步提升性能，这为优化推理策略提供了方向。

Abstract: Large Reasoning Models (LRMs) achieve strong performance on complex reasoning tasks by generating long Chains of Thought (CoTs). However, this paradigm might incur substantial token overhead, especially when models "overthink" by producing lengthy reasoning chains, which can even lead to incorrect answers. A promising direction is the symbolic-solver-integrated approach, which leverages the code generation capabilities of LLMs to translate reasoning tasks into executable code and then solve them with a symbolic solver. In this paper, we explore an open question of when the conventional long-CoT can be enhanced by symbolic solvers. Our experimental results show that the symbolic-solver-integrated method only helps when the problem requires limited implicit reasoning but involves an ample search space. The latest LLMs, like GPT-4o, show better performance on deductive problems with shallow reasoning depth, while the symbolic-solver-integrated method significantly improves the LLMs' performance in constraint satisfaction problems that require repeated backtracks. When a declarative exemplar is provided, even CodeLlama-13B can outperform GPT-4o in difficult Zebra puzzles.

</details>


### [38] [Multi-Agent Reinforcement Learning with Communication-Constrained Priors](https://arxiv.org/abs/2512.03528)
*Guang Yang,Tianpei Yang,Jingwen Qiao,Yanqing Wu,Jing Huo,Xingguo Chen,Yang Gao*

Main category: cs.AI

TL;DR: 提出了一种通用通信约束模型，以解决多智能体强化学习在现实环境中通信受限的问题。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，通信是提升合作策略学习的有效手段，但现实世界中普遍存在通信损失的问题。现有方法在复杂和动态环境中可扩展性和鲁棒性有限，难以有效应用。

Method: 提出了一种通用通信约束模型，将其作为学习先验，区分特定场景下的损失和无损信息。通过双互信息估计器，将损失和无损信息对分布式决策的影响解耦，并引入通信约束多智能体强化学习框架，将通信信息的影响量化为全局奖励。

Result: 在多个通信受限的基准测试中验证了该方法的有效性。

Conclusion: 该框架能够有效应对多智能体系统中通信受限的现实挑战，提高在复杂和动态环境中的可扩展性和鲁棒性。

Abstract: Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks.

</details>


### [39] [Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks](https://arxiv.org/abs/2512.03560)
*Gianni Molinari,Fabio Ciravegna*

Main category: cs.AI

TL;DR: RP-ReAct是一种新型多智能体方法，通过解耦战略规划和低层执行，提高企业任务的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 自主智能体在解决复杂企业任务时存在两个主要问题：单智能体架构导致的轨迹不稳定和本地开放权重模型的小上下文窗口问题。

Method: RP-ReAct由Reasoner Planner Agent (RPA) 和多个Proxy-Execution Agent (PEA)组成，RPA负责规划和分析，PEA负责工具交互，并引入上下文保存策略以缓解上下文窗口溢出。

Result: 在ToolQA基准测试中，RP-ReAct在多样复杂任务中表现优于最先进基线，具备更好的泛化能力和稳定性。

Conclusion: RP-ReAct通过解耦规划和执行，以及上下文管理策略，提高了多智能体系统的鲁棒性和稳定性，为企业智能体解决方案提供了新路径。

Abstract: Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.

</details>


### [40] [EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths](https://arxiv.org/abs/2512.03571)
*Zhening Li,Armando Solar-Lezama,Yisong Yue,Stephan Zheng*

Main category: cs.AI

TL;DR: 引入了一种新的基于LLM的智能体编程方法PAN，通过分离核心工作流逻辑和推理时策略，简化智能体设计和实验。


<details>
  <summary>Details</summary>
Motivation: 当前智能体编程方法将核心工作流逻辑和推理时策略混合，导致设计和实验复杂。

Method: 提出“概率天使非确定性”（PAN）编程模型，使用Python实现的EnCompass框架，通过装饰器将工作流程序编译为搜索空间。

Result: 通过三个案例研究展示了该框架如何快速提升智能体的可靠性，并轻松切换不同推理时策略。

Conclusion: PAN模型有效解耦了智能体设计中的两个关键方面，使得编程更高效且易于实验。

Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.

</details>


### [41] [DeepRule: An Integrated Framework for Automated Business Rule Generation via Deep Predictive Modeling and Hybrid Search Optimization](https://arxiv.org/abs/2512.03607)
*Yusen Wu,Xiaotie Deng*

Main category: cs.AI

TL;DR: 提出DeepRule框架，通过三层架构自动生成零售品类和定价优化的业务规则。


<details>
  <summary>Details</summary>
Motivation: 解决现有理论模型与现实经济复杂性之间的系统性错配，识别出三个关键问题：数据模态不匹配、动态特征纠缠和操作不可行。

Method: 采用三层架构：1) 使用LLMs进行深度语义解析的混合知识融合引擎；2) 博弈论约束优化机制；3) 利用LLM引导符号回归的可解释决策蒸馏界面。

Result: 在实际零售环境中验证，相比系统性B2C基线，实现了更高利润，同时确保操作可行性。

Conclusion: 建立了一个闭环流程，统一了非结构化知识注入、多智能体优化和可解释策略合成，为实际经济智能提供支持。

Abstract: This paper proposes DeepRule, an integrated framework for automated business rule generation in retail assortment and pricing optimization. Addressing the systematic misalignment between existing theoretical models and real-world economic complexities, we identify three critical gaps: (1) data modality mismatch where unstructured textual sources (e.g. negotiation records, approval documents) impede accurate customer profiling; (2) dynamic feature entanglement challenges in modeling nonlinear price elasticity and time-varying attributes; (3) operational infeasibility caused by multi-tier business constraints.
  Our framework introduces a tri-level architecture for above challenges. We design a hybrid knowledge fusion engine employing large language models (LLMs) for deep semantic parsing of unstructured text, transforming distributor agreements and sales assessments into structured features while integrating managerial expertise. Then a game-theoretic constrained optimization mechanism is employed to dynamically reconcile supply chain interests through bilateral utility functions, encoding manufacturer-distributor profit redistribution as endogenous objectives under hierarchical constraints. Finally an interpretable decision distillation interface leveraging LLM-guided symbolic regression to find and optimize pricing strategies and auditable business rules embeds economic priors (e.g. non-negative elasticity) as hard constraints during mathematical expression search. We validate the framework in real retail environments achieving higher profits versus systematic B2C baselines while ensuring operational feasibility. This establishes a close-loop pipeline unifying unstructured knowledge injection, multi-agent optimization, and interpretable strategy synthesis for real economic intelligence.

</details>


### [42] [RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design](https://arxiv.org/abs/2512.03762)
*Jiawei Xu,Fengfeng Wei,Weineng Chen*

Main category: cs.AI

TL;DR: 提出了一种多智能体角色协作系统RoCo，通过多角色协作增强自动启发式设计的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动启发式设计研究通常只考虑单一角色，限制了启发式设计的质量和多样性。

Method: RoCo系统包含四种专门化的LLM引导智能体（探索者、利用者、评论者、整合者），通过结构化的多轮过程进行协作，生成高质量的启发式。

Result: 在五个不同的组合优化问题上的实验表明，RoCo在生成启发式方面优于现有方法，包括ReEvo和HSEvo，在两种白盒和黑盒场景下均表现优异。

Conclusion: 这种基于角色协作的范式为自动启发式设计设定了新的标准，使其更加稳健和高效。

Abstract: Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.

</details>


### [43] [Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.03783)
*Dongchao Yang,Songxiang Liu,Disong Wang,Yuanyuan Wang,Guanglu Wan,Helen Meng*

Main category: cs.AI

TL;DR: 提出Omni-AutoThink，一个根据任务难度动态调整推理深度的自适应推理框架。


<details>
  <summary>Details</summary>
Motivation: 现有Omni模型在推理行为上表现僵硬，简单问题过度思考或复杂问题推理不足。

Method: 包含两个阶段：(1) 自适应监督微调（Adaptive SFT）阶段，赋予模型基础推理能力；(2) 自适应强化学习（Adaptive GRPO）阶段，优化推理行为。同时构建了覆盖多种模态的自适应推理基准。

Result: 实验结果表明，该框架相比之前的基线显著提升了自适应推理性能。

Conclusion: Omni-AutoThink能有效动态调整推理深度，提升多模态任务中的推理表现，相关数据和代码将公开。

Abstract: Recent advances in Omni models have enabled unified multimodal perception and generation. However, most existing systems still exhibit rigid reasoning behaviors, either overthinking simple problems or failing to reason when necessary. To address this limitation, we propose Omni-AutoThink, a novel adaptive reasoning framework that dynamically adjusts the model's reasoning depth according to task difficulty. Our framework comprises two stages: (1) an Adaptive Supervised Fine-Tuning (Adaptive SFT) stage, which endows the Omni model with fundamental reasoning capability using large-scale reasoning-augmented data, and (2) an Adaptive Reinforcement Learning (Adaptive GRPO) stage, which optimizes reasoning behaviors based on task complexity and reward feedback. We further construct a comprehensive adaptive reasoning benchmark that spans text-only, text-audio, text-visual, and text-audio-visual modalities, providing both training and evaluation splits for multimodal reasoning assessment. Experimental results demonstrate that our proposed framework significantly improves adaptive reasoning performance compared to previous baselines. All benchmark data and code will be publicly released.

</details>


### [44] [Autonomous Agents and Policy Compliance: A Framework for Reasoning About Penalties](https://arxiv.org/abs/2512.03931)
*Vineel Tummala,Daniela Inclezan*

Main category: cs.AI

TL;DR: 提出了一种基于逻辑编程的框架，使自主智能体能够考虑违反政策的潜在惩罚并据此行动。


<details>
  <summary>Details</summary>
Motivation: 以往的研究主要关注确保合规性，而本文方法考虑了为实现高风险目标而可能偏离政策的情景，并模拟非合规行为以帮助政策制定者。

Method: 扩展了Gelfond和Lobo的授权与义务政策语言（AOPL）以包含惩罚，并结合答案集编程（ASP）进行推理，引入基于惩罚的推理来区分非合规计划。

Result: 实验在两个领域中表明，该框架能够生成避免有害行为的更高质量计划，并在某些情况下提高计算效率。

Conclusion: 该框架有潜力增强自主决策并为政策改进提供信息。

Abstract: This paper presents a logic programming-based framework for policy-aware autonomous agents that can reason about potential penalties for non-compliance and act accordingly. While prior work has primarily focused on ensuring compliance, our approach considers scenarios where deviating from policies may be necessary to achieve high-stakes goals. Additionally, modeling non-compliant behavior can assist policymakers by simulating realistic human decision-making. Our framework extends Gelfond and Lobo's Authorization and Obligation Policy Language (AOPL) to incorporate penalties and integrates Answer Set Programming (ASP) for reasoning. Compared to previous approaches, our method ensures well-formed policies, accounts for policy priorities, and enhances explainability by explicitly identifying rule violations and their consequences. Building on the work of Harders and Inclezan, we introduce penalty-based reasoning to distinguish between non-compliant plans, prioritizing those with minimal repercussions. To support this, we develop an automated translation from the extended AOPL into ASP and refine ASP-based planning algorithms to account for incurred penalties. Experiments in two domains demonstrate that our framework generates higher-quality plans that avoid harmful actions while, in some cases, also improving computational efficiency. These findings underscore its potential for enhancing autonomous decision-making and informing policy refinement. Under consideration in Theory and Practice of Logic Programming (TPLP).

</details>
