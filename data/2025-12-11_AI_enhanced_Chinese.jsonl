{"id": "2512.08943", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08943", "abs": "https://arxiv.org/abs/2512.08943", "authors": ["Singon Kim"], "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "comment": "Master's thesis, Korea University, 2025", "summary": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62bd\u8c61\u538b\u7f29\u65b9\u6cd5ACoRN\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u4e24\u6b65\u8bad\u7ec3\u63d0\u9ad8\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5bf9\u566a\u97f3\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u68c0\u7d22\u6587\u6863\u5e38\u5305\u542b\u4e0e\u67e5\u8be2\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u4fe1\u606f\uff0c\u62bd\u8c61\u538b\u7f29\u5668\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u5bb9\u6613\u5ffd\u7565\u91cd\u8981\u4fe1\u606f\u3002", "method": "\u63d0\u51faACoRN\uff0c\u91c7\u7528\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u548c\u5fae\u8c03\u4e24\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff0c\u589e\u5f3a\u538b\u7f29\u5668\u5bf9\u68c0\u7d22\u566a\u97f3\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u751f\u6210\u4ee5\u5173\u952e\u4fe1\u606f\u4e3a\u4e2d\u5fc3\u7684\u6458\u8981\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4f7f\u7528ACoRN\u8bad\u7ec3\u7684T5-large\u5728EM\u548cF1\u8bc4\u5206\u4e0a\u6709\u6240\u63d0\u5347\uff0c\u5e76\u5728\u4fdd\u7559\u7b54\u6848\u5b57\u7b26\u4e32\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "ACoRN\u5728\u5904\u7406\u5305\u542b\u964d\u4f4e\u51c6\u786e\u6027\u6587\u6863\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u3002"}}
{"id": "2512.08944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08944", "abs": "https://arxiv.org/abs/2512.08944", "authors": ["Yudong Wang", "Zhe Yang", "Wenhan Ma", "Zhifang Sui", "Liang Zhao"], "title": "Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning", "comment": null, "summary": "While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6709\u9488\u5bf9\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\uff0c\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u5e7b\u89c9\u7684\u503e\u5411\uff0c\u56e0\u6b64\u9700\u8981\u5728\u80fd\u529b\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u521b\u5efa\u57fa\u4e8eTriviaQA\u7684\u5f00\u653e\u5f0f\u95ee\u7b54\u8bad\u7ec3\u96c6\u4ee5\u89e3\u51b3\u5916\u90e8\u5e7b\u89c9\uff0c\u5e76\u5229\u7528FineWeb\u7684\u957f\u6587\u672c\u8fdb\u884c\u4e8b\u5b9e\u57fa\u7840\u5956\u52b1\u4ee5\u89e3\u51b3\u5185\u90e8\u5e7b\u89c9\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5956\u52b1\u6a21\u578b\u62d2\u7edd\u56de\u7b54\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\u6765\u589e\u5f3a\u8c28\u614e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5927\u5e45\u51cf\u5c11\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u5e7b\u89c9\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u5148\u8fdb\u63a8\u7406\u548c\u4e8b\u5b9e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u5f20\u529b\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u66f4\u52a0\u53ef\u9760\u548c\u9ad8\u6548\u3002"}}
{"id": "2512.09088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09088", "abs": "https://arxiv.org/abs/2512.09088", "authors": ["Adrian Ryser", "Florian Allwein", "Tim Schlippe"], "title": "Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study", "comment": null, "summary": "Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Bl\u00f6baum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u7684\u5e7b\u89c9\u5982\u4f55\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u53ca\u4ea4\u4e92\uff0c\u53d1\u73b0\u5e7b\u89c9\u4e0d\u4f1a\u5bfc\u81f4\u5168\u9762\u4e0d\u4fe1\u4efb\uff0c\u800c\u662f\u5f15\u53d1\u60c5\u5883\u654f\u611f\u7684\u4fe1\u4efb\u6821\u51c6\u3002", "motivation": "\u7814\u7a76\u5e7b\u89c9\u5728LLMs\u4e2d\u5bf9\u7528\u6237\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u63a2\u7d22\u7528\u6237\u5982\u4f55\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0eLLMs\u4e92\u52a8\u3002", "method": "\u901a\u8fc7\u5b9a\u6027\u7814\u7a76\uff0c\u8c03\u67e5192\u540d\u53c2\u4e0e\u8005\u7684\u65e5\u5e38\u4f7f\u7528\u60c5\u51b5\uff0c\u5206\u6790\u7528\u6237\u4fe1\u4efb\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u786e\u8ba4\u4e86\u7528\u6237\u76f8\u5173\u7684\u4fe1\u4efb\u56e0\u7d20\uff0c\u5305\u62ec\u671f\u671b\u3001\u7ecf\u9a8c\u3001\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e76\u53d1\u73b0\u76f4\u89c9\u5728\u5e7b\u89c9\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\uff1b\u60c5\u5883\u56e0\u7d20\u5982\u98ce\u9669\u611f\u77e5\u548c\u51b3\u7b56\u5229\u76ca\u4e5f\u5f71\u54cd\u4fe1\u4efb\u3002", "conclusion": "\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86\u9012\u5f52\u4fe1\u4efb\u6821\u51c6\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u4e8eLLMs\u8d1f\u8d23\u4efb\u548c\u53cd\u601d\u6027\u4f7f\u7528\u7684\u5b9e\u9645\u5efa\u8bae\u3002"}}
{"id": "2512.09127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09127", "abs": "https://arxiv.org/abs/2512.09127", "authors": ["Zihan Han", "Junyan Ge", "Caifeng Li"], "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation", "comment": null, "summary": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u5f15\u5bfc\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08KG-LLM\uff09\u7528\u4e8e\u513f\u79d1\u7259\u79d1\u4e34\u5e8a\u8bb0\u5f55\u89e3\u8bfb\u548c\u5b89\u5168\u6297\u751f\u7d20\u5904\u65b9\u63a8\u8350\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u7259\u79d1\u53d9\u8ff0\u3001\u4e0d\u5b8c\u6574\u7684\u5f71\u50cf\u63cf\u8ff0\u548c\u590d\u6742\u7684\u5b89\u5168\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "KG-LLM\u6846\u67b6\u96c6\u6210\u4e86\u513f\u79d1\u7259\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u591a\u9636\u6bb5\u5b89\u5168\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e34\u5e8aNER/RE\u6a21\u5757\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u68c0\u7d22\u76f8\u5173\u6307\u5357\u548c\u5386\u53f2\u75c5\u4f8b\uff0c\u5e76\u4f7f\u7528\u53cc\u5c42\u5b89\u5168\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u572832,000\u4e2a\u53bb\u6807\u8bc6\u5316\u7684\u513f\u79d1\u7259\u79d1\u5c31\u8bca\u8bb0\u5f55\u4e0a\uff0cKG-LLM\u5728\u8bb0\u5f55\u7406\u89e3\u6027\u80fd\uff08F1: 0.914 vs. 0.867\uff09\u3001\u836f\u7269-\u5242\u91cf-\u6301\u7eed\u65f6\u95f4\u51c6\u786e\u6027\uff08Top-1: 0.782 vs. 0.716\uff09\u4e0a\u4f18\u4e8eLlama-2\u4e34\u5e8a\u57fa\u7ebf\uff0c\u5e76\u5c06\u4e0d\u5b89\u5168\u7684\u6297\u751f\u7d20\u5efa\u8bae\u51cf\u5c11\u4e8650%\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u3001RAG\u548c\u5b89\u5168\u6a21\u5757\u5404\u81ea\u5bf9\u4e34\u5e8a\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6709\u663e\u8457\u8d21\u732e\uff0cKG-LLM\u5728\u513f\u79d1\u7259\u79d1\u6297\u751f\u7d20\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.09114", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09114", "abs": "https://arxiv.org/abs/2512.09114", "authors": ["Pamela Gupta"], "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance", "comment": "47 pages", "summary": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.", "AI": {"tldr": "AI TIPS 2.0\u6846\u67b6\u89e3\u51b3\u4e86\u5f53\u524dAI\u6cbb\u7406\u6846\u67b6\u9762\u4e34\u7684\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a\u7528\u4f8b\u7ea7\u522b\u98ce\u9669\u8bc4\u4f30\u4e0d\u8db3\u3001\u6846\u67b6\u8fc7\u4e8e\u6982\u5ff5\u5316\u4e14\u7f3a\u4e4f\u53ef\u64cd\u4f5c\u63a7\u5236\u3001\u4ee5\u53ca\u65e0\u6cd5\u5927\u89c4\u6a21\u5b9e\u65bd\u6cbb\u7406\u3002", "motivation": "\u5f53\u524dAI\u6cbb\u7406\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u72ec\u7279\u98ce\u9669\uff0c\u5bfc\u81f4\u5982Humana\u8bc9\u8bbc\u6848\u7b49\u4e25\u91cd\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u548c\u53ef\u64cd\u4f5c\u7684\u6cbb\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAI TIPS 2.0\uff0c\u4e00\u4e2a\u7efc\u5408\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u5316\u7528\u4f8b\u98ce\u9669\u8bc4\u4f30\u3001\u63d0\u4f9b\u5177\u4f53\u6280\u672f\u5b9e\u73b0\u63a7\u5236\u548c\u5efa\u7acb\u5168\u751f\u547d\u5468\u671f\u7684\u53ef\u4fe1\u8d56AI\u5b9e\u8df5\u673a\u5236\u3002", "result": "AI TIPS 2.0\u80fd\u591f\u5d4c\u5165\u53ef\u4fe1\u8d56AI\u5b9e\u8df5\u81f3\u5f00\u53d1\u751f\u547d\u5468\u671f\uff0c\u91cf\u5316\u5408\u89c4\u6027\uff0c\u5e76\u4e3a\u4e0d\u540c\u89d2\u8272\u63d0\u4f9b\u9002\u5f53\u7684\u53ef\u89c1\u6027\u3002", "conclusion": "AI TIPS 2.0\u901a\u8fc7\u89e3\u51b3\u98ce\u9669\u8bc4\u4f30\u3001\u53ef\u64cd\u4f5c\u6027\u548c\u5927\u89c4\u6a21\u6cbb\u7406\u673a\u5236\uff0c\u586b\u8865\u4e86\u73b0\u6709AI\u6cbb\u7406\u6846\u67b6\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2512.09148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09148", "abs": "https://arxiv.org/abs/2512.09148", "authors": ["Shanghao Li", "Jinda Han", "Yibo Wang", "Yuanjie Zhu", "Zihe Song", "Langzhou He", "Kenan Kamel A Alghythee", "Philip S. Yu"], "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment", "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GraphRAG\u4e2dLLMs\u7684\u7ed3\u6784\u77e5\u8bc6\u5904\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u4e2a\u8f7b\u91cf\u53ef\u89e3\u91ca\u6027\u6307\u6807(PRD\u548cSAS)\u5206\u6790\u5e7b\u89c9\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86GGA\u540e\u5904\u7406\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "motivation": "LLMs\u5728GraphRAG\u4e2d\u96be\u4ee5\u6709\u6548\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0e\u68c0\u7d22\u77e5\u8bc6\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u5206\u6790\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u5904\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u65b0\u6307\u6807\uff1a\u8def\u5f84\u4f9d\u8d56\u5ea6(PRD)\u6d4b\u91cf\u6700\u77ed\u8def\u5f84\u4e09\u5143\u7ec4\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u8bed\u4e49\u5bf9\u9f50\u5206\u6570(SAS)\u8bc4\u4f30\u5185\u90e8\u8868\u5f81\u4e0e\u68c0\u7d22\u77e5\u8bc6\u5bf9\u9f50\u7a0b\u5ea6\uff1b\u5f00\u53d1\u4e86GGA\u540e\u5904\u7406\u5e7b\u89c9\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u77e5\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d1\u73b0\u9ad8PRD\u548c\u4f4eSAS\u5206\u6570\u5bf9\u5e94\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff1bGGA\u68c0\u6d4b\u5668\u5728AUC\u548cF1\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86LLMs\u7ed3\u6784\u5c40\u9650\u6027\u5bfc\u81f4\u5e7b\u89c9\u7684\u673a\u7406\uff0c\u4e3a\u672a\u6765\u66f4\u53ef\u9760\u7684GraphRAG\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002"}}
{"id": "2512.09117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09117", "abs": "https://arxiv.org/abs/2512.09117", "authors": ["Luciano Floridi", "Yiyang Jia", "Fernando Tohm\u00e9"], "title": "A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem", "comment": null, "summary": "This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u7684\u8303\u7574\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u5c06\u5185\u5bb9\u8f6c\u5316\u4e3a\u5173\u4e8e\u53ef\u80fd\u4e16\u754c\u72b6\u6001\u7a7a\u95f4\u7684\u771f\u503c\u8bc4\u4f30\u547d\u9898\uff0c\u5e76\u8ba4\u4e3aLLMs\u5e76\u672a\u89e3\u51b3\u800c\u662f\u89c4\u907f\u4e86\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u4eba\u7c7b\u548cLLMs\u5728\u7b26\u53f7\u63a5\u5730\u95ee\u9898\u4e0a\u7684\u4e0d\u540c\u5904\u7406\u65b9\u5f0f\uff0c\u4ee5\u53caLLMs\u5982\u4f55\u7ed5\u8fc7\u8fd9\u4e00\u95ee\u9898\u7684\u672c\u8d28\u3002", "method": "\u91c7\u7528\u5f62\u5f0f\u5316\u7684\u8303\u7574\u6846\u67b6\uff0c\u5206\u6790\u4eba\u7c7b\u548cLLMs\u5c06\u5185\u5bb9\u8f6c\u5316\u4e3a\u771f\u503c\u8bc4\u4f30\u547d\u9898\u7684\u8fc7\u7a0b\u3002", "result": "\u8bba\u8bc1\u4e86LLMs\u5e76\u672a\u771f\u6b63\u89e3\u51b3\u7b26\u53f7\u63a5\u5730\u95ee\u9898\uff0c\u800c\u662f\u901a\u8fc7\u5176\u4ed6\u65b9\u5f0f\u89c4\u907f\u4e86\u5b83\u3002", "conclusion": "LLMs\u901a\u8fc7\u89c4\u907f\u800c\u975e\u89e3\u51b3\u7b26\u53f7\u63a5\u5730\u95ee\u9898\uff0c\u5b9e\u73b0\u5185\u5bb9\u7684\u771f\u503c\u8bc4\u4f30\uff0c\u8fd9\u4e3a\u7406\u89e3LLMs\u7684\u8bed\u4e49\u5904\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2512.09149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09149", "abs": "https://arxiv.org/abs/2512.09149", "authors": ["Anton Vasiliuk", "Irina Abdullaeva", "Polina Druzhinina", "Anton Razzhigaev", "Andrey Kuznetsov"], "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts", "comment": null, "summary": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u6d4b\u91cf\u5de5\u5177MMPI\uff0c\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u7528\u6237\u6307\u5b9a\u4eba\u683c\u7279\u8d28\u548c\u6001\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86MindShift\u8fd9\u4e00\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22LLMs\u662f\u5426\u80fd\u51c6\u786e\u53cd\u6620\u548c\u5438\u6536\u7528\u6237\u7684\u4eba\u683c\u7279\u8d28\u4e0e\u6001\u5ea6\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5fc3\u7406\u9002\u5e94\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u660e\u5c3c\u82cf\u8fbe\u591a\u9879\u4eba\u683c\u6d4b\u9a8c\uff08MMPI\uff09\u7684\u53d8\u4f53\uff0c\u521b\u5efa\u4e86\u5177\u6709\u4e0d\u540c\u7279\u8d28\u5f3a\u5ea6\u7684\u4eba\u7269\u89d2\u8272\uff0c\u901a\u8fc7\u5fc3\u7406\u5bfc\u5411\u7684\u63d0\u793a\u6765\u8bc4\u4f30LLMs\u7684\u884c\u4e3a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u89d2\u8272\u611f\u77e5\u65b9\u9762\u6301\u7eed\u6539\u8fdb\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u548c\u5bb6\u65cf\u5728\u5fc3\u7406\u6d4b\u91cf\u8bc4\u4f30\u4e2d\u7684\u54cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u6a21\u62df\u7c7b\u4eba\u4e2a\u6027\u7279\u8d28\u65b9\u9762\u7684\u53ef\u53d8\u6027\uff0c\u5e76\u8868\u660e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5bf9\u9f50\u6280\u672f\u7684\u8fdb\u6b65\u63d0\u5347\u4e86\u5176\u8868\u73b0\u3002MindShift\u8bc4\u4f30\u5de5\u5177\u5c06\u516c\u5f00\u3002"}}
{"id": "2512.09222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09222", "abs": "https://arxiv.org/abs/2512.09222", "authors": ["Vishwas Hegde", "Vindhya Shigehalli"], "title": "CORE: A Conceptual Reasoning Layer for Large Language Models", "comment": "Independent system-level architectural proposal with accompanying proof-of-concept", "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.", "AI": {"tldr": "\u63d0\u51faCORE\u6982\u5ff5\u4f18\u5148\u4ea4\u4e92\u5c42\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u8bed\u4e49\u72b6\u6001\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u7a33\u5b9a\u6027\uff0c\u51cf\u5c11token\u4f7f\u7528\u3002", "motivation": "\u5927\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u56e0\u7f3a\u4e4f\u6301\u4e45\u5185\u90e8\u8868\u5f81\u5bfc\u81f4\u610f\u56fe\u91cd\u5efa\u56f0\u96be\u3001\u63d0\u793a\u81a8\u80c0\u548c\u63a8\u7406\u4e0d\u4e00\u81f4\u3002", "method": "\u6784\u5efa\u5305\u542b\u901a\u7528\u8ba4\u77e5\u7b97\u5b50\u5e93\u548c\u6301\u4e45\u5316Local Concept\u7684\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u5c42\uff0c\u5206\u79bb\u6982\u5ff5\u63a8\u7406\u4e0e\u8bed\u8a00\u751f\u6210\u3002", "result": "\u539f\u578b\u6d4b\u8bd5\u663e\u793a\u7d2f\u8ba1\u63d0\u793atoken\u51cf\u5c1142%\uff08\u5b9e\u9a8c\u6761\u4ef6\u6570\u636e\uff09\u3002", "conclusion": "CORE\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u7a33\u5b9a\u591a\u8f6e\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.09238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09238", "abs": "https://arxiv.org/abs/2512.09238", "authors": ["Zeng You", "Yaofo Chen", "Shuhai Zhang", "Zhijie Qiu", "Tingyu Wu", "Yingjian Li", "Yaowei Wang", "Mingkui Tan"], "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u81ea\u9002\u5e94\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236TCA-Attention\uff0c\u4ee5\u89e3\u51b3\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5728\u957f\u5e8f\u5217\u4e0b\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u548cKV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u3002", "method": "TCA-Attention\u5305\u542b\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u9636\u6bb5\uff1a\u79bb\u7ebf\u6821\u51c6\u9636\u6bb5\u548c\u5728\u7ebf\u6807\u8bb0\u9009\u62e9\u9636\u6bb5\uff0c\u901a\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u9012\u548c\u8f7b\u91cf\u7ea7\u5197\u4f59\u5ea6\u91cf\u81ea\u9002\u5e94\u4fdd\u7559\u6838\u5fc3\u4e0a\u4e0b\u6587\u6807\u8bb0\u3002", "result": "\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cTCA-Attention\u5b9e\u73b0\u4e862.8\u500d\u52a0\u901f\uff0cKV\u7f13\u5b58\u51cf\u5c1161%\uff0c\u6027\u80fd\u4e0e\u5168\u6ce8\u610f\u529b\u673a\u5236\u76f8\u5f53\u3002", "conclusion": "TCA-Attention\u4e3a\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u67b6\u6784\u66f4\u6539\u3002"}}
{"id": "2512.09566", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09566", "abs": "https://arxiv.org/abs/2512.09566", "authors": ["Junkai Ji", "Zhangfan Yang", "Dong Xu", "Ruibin Bai", "Jianqiang Li", "Tingjun Hou", "Zexuan Zhu"], "title": "Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search", "comment": "21 pages, 5 figures", "summary": "Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.", "AI": {"tldr": "Trio\u662f\u4e00\u79cd\u65b0\u578b\u5206\u5b50\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u4e8e\u7247\u6bb5\u7684\u5206\u5b50\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u95ed\u73af\u9776\u5411\u5206\u5b50\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u836f\u7269\u53d1\u73b0\u8fc7\u7a0b\u8017\u65f6\u4e14\u6602\u8d35\uff0c\u800c\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u5728\u6cdb\u5316\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u836f\u7406\u6027\u8d28\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "Trio\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u7247\u6bb5\u7684\u5206\u5b50\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u60c5\u5883\u611f\u77e5\u7684\u7247\u6bb5\u7ec4\u88c5\uff0c\u786e\u4fdd\u4e86\u7269\u7406\u5316\u5b66\u548c\u5408\u6210\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728\u65b0\u9896\u5316\u5b66\u578b\u63a2\u7d22\u548c\u86cb\u767d\u7ed3\u5408\u53e3\u888b\u4e2d\u7684\u6709\u5e0c\u671b\u4e2d\u95f4\u4f53\u7684\u5229\u7528\u4e4b\u95f4\u5b9e\u73b0\u5e73\u8861\u641c\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTrio\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u5316\u5b66\u4e0a\u6709\u6548\u4e14\u836f\u7406\u6027\u8d28\u589e\u5f3a\u7684\u914d\u4f53\uff0c\u5728\u7ed3\u5408\u4eb2\u548c\u529b\uff08+7.85%\uff09\u3001\u836f\u7269\u76f8\u4f3c\u6027\uff08+11.10%\uff09\u548c\u5408\u6210\u53ef\u53ca\u6027\uff08+12.05%\uff09\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5206\u5b50\u591a\u6837\u6027\u589e\u52a0\u4e86\u56db\u500d\u4ee5\u4e0a\u3002", "conclusion": "Trio\u6846\u67b6\u5728\u5206\u5b50\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u66f4\u597d\u836f\u7406\u6027\u8d28\u548c\u66f4\u9ad8\u591a\u6837\u6027\u7684\u5206\u5b50\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u5de5\u5177\u3002"}}
{"id": "2512.09292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09292", "abs": "https://arxiv.org/abs/2512.09292", "authors": ["Kevin Stowe", "Svetlana Afanaseva", "Rodolfo Raimundo", "Yitao Sun", "Kailash Patil"], "title": "Identifying Bias in Machine-generated Text Detection", "comment": "13 pages, 2 figures, 7 tables", "summary": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u53d1\u73b0\u4e0d\u540c\u7cfb\u7edf\u5728\u6027\u522b\u3001\u79cd\u65cf/\u65cf\u88d4\u3001\u82f1\u8bed\u8bed\u8a00\u5b66\u4e60\u8005\uff08ELL\uff09\u8eab\u4efd\u548c\u7ecf\u6d4e\u72b6\u51b5\u7b49\u5c5e\u6027\u4e0a\u5b58\u5728\u4e0d\u4e00\u81f4\u7684\u504f\u89c1\uff0c\u5e76\u6307\u51fa\u4eba\u7c7b\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u4f46\u65e0\u660e\u663e\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u6587\u672c\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\uff0c\u4f46\u5176\u6f5c\u5728\u7684\u8d1f\u9762\u5f71\u54cd\u548c\u504f\u89c1\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u7814\u7a76\u8005\u6574\u7406\u4e86\u4e00\u4e2a\u5b66\u751f\u4f5c\u6587\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e8616\u79cd\u4e0d\u540c\u7684\u68c0\u6d4b\u7cfb\u7edf\u5728\u56db\u4e2a\u5c5e\u6027\u4e0a\u7684\u504f\u89c1\uff1a\u6027\u522b\u3001\u79cd\u65cf/\u65cf\u88d4\u3001\u82f1\u8bed\u8bed\u8a00\u5b66\u4e60\u8005\uff08ELL\uff09\u8eab\u4efd\u548c\u7ecf\u6d4e\u72b6\u51b5\u3002\u4f7f\u7528\u57fa\u4e8e\u56de\u5f52\u7684\u6a21\u578b\u548c\u5b50\u7fa4\u5206\u6790\u8bc4\u4f30\u8fd9\u4e9b\u5c5e\u6027\u7684\u663e\u8457\u6027\u548c\u6548\u5e94\u5f3a\u5ea6\uff0c\u5e76\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u7cfb\u7edf\u7684\u504f\u89c1\u4e0d\u4e00\u81f4\uff0c\u4f46\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u4e00\u4e9b\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u5f31\u52bf\u7fa4\u4f53\u5206\u7c7b\u4e3a\u673a\u5668\u751f\u6210\uff0c\u82f1\u8bed\u8bed\u8a00\u5b66\u4e60\u8005\uff08ELL\uff09\u548c\u7ecf\u6d4e\u72b6\u51b5\u8f83\u5dee\u7684\u5b66\u751f\u6587\u7ae0\u66f4\u6613\u88ab\u8bef\u5206\u7c7b\uff0c\u975e\u767d\u4ebaELL\u6587\u7ae0\u76f8\u5bf9\u4e8e\u767d\u4ebaELL\u6587\u7ae0\u66f4\u53ef\u80fd\u88ab\u8bef\u5206\u7c7b\u3002\u4eba\u7c7b\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\u4f46\u65e0\u660e\u663e\u504f\u89c1\u3002", "conclusion": "\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8bc6\u522b\u5f31\u52bf\u7fa4\u4f53\u6587\u7ae0\u65f6\u3002\u867d\u7136\u4eba\u7c7b\u5728\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u6ca1\u6709\u8868\u73b0\u51fa\u660e\u663e\u7684\u504f\u89c1\u3002"}}
{"id": "2512.09386", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09386", "abs": "https://arxiv.org/abs/2512.09386", "authors": ["Peter Baile Chen", "Weiyue Li", "Dan Roth", "Michael Cafarella", "Samuel Madden", "Jacob Andreas"], "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing", "comment": null, "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.", "AI": {"tldr": "CONCUR\u662f\u4e00\u4e2a\u652f\u6301\u6301\u7eed\u5b66\u4e60\u7684\u65b0\u578b\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7b56\u7565\u8bad\u7ec3\u72ec\u7acb\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u65b0\u7b56\u7565\u6574\u5408\uff0c\u540c\u65f6\u5229\u7528\u591a\u8868\u793a\u6cd5\u63d0\u5347\u4efb\u52a1\u590d\u6742\u5ea6\u6355\u6349\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8def\u7531\u65b9\u6cd5\u5728\u9762\u4e34\u65b0\u7b56\u7565\u65f6\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e14\u5355\u4e00\u8f93\u5165\u8868\u793a\u6cd5\u96be\u4ee5\u5145\u5206\u6355\u6349\u4efb\u52a1\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6b21\u4f18\u8def\u7531\u51b3\u7b56\u3002", "method": "CONCUR\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3a\u6bcf\u4e2a\u7b56\u7565\u8bad\u7ec3\u72ec\u7acb\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4efb\u52a1\u4e0e\u8ba1\u7b97\u7b56\u7565\u7684\u591a\u8868\u793a\u6cd5\uff0c\u652f\u6301\u6709\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u8def\u7531\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u3001\u77e5\u8bc6\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\uff0cCONCUR\u5728\u6301\u7eed\u548c\u975e\u6301\u7eed\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u5355\u4e00\u7b56\u7565\u548c\u73b0\u6709\u8def\u7531\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u4e86\u63a8\u7406\u548c\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "CONCUR\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u8868\u793a\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8def\u7531\u6846\u67b6\u7684\u6cdb\u5316\u5dee\u3001\u9ad8\u5f00\u9500\u548c\u6b21\u4f18\u51b3\u7b56\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u590d\u6742\u548c\u52a8\u6001\u7684AI\u4efb\u52a1\u73af\u5883\u3002"}}
{"id": "2512.09727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09727", "abs": "https://arxiv.org/abs/2512.09727", "authors": ["Junlin Xiao", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions", "comment": null, "summary": "Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08Gaussian Process Regression\uff09\u6765\u4f30\u8ba1\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u672a\u8bd5\u9a8c\u52a8\u4f5c\u503c\u7684\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08Monte Carlo Tree Search\uff09\u7684\u5e76\u884c\u805a\u5408\u7b56\u7565\u3002", "motivation": "\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u5982\u4f55\u6709\u6548\u5730\u805a\u5408\u6765\u81ea\u4e0d\u540c\u7ebf\u7a0b\u7684\u7edf\u8ba1\u4fe1\u606f\u4ee5\u63d0\u9ad8\u6027\u80fd\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08Gaussian Process Regression\uff09\u6765\u4f30\u8ba1\u672a\u5728\u73af\u5883\u4e2d\u8bd5\u9a8c\u7684\u3001\u6709\u524d\u9014\u7684\u52a8\u4f5c\u7684\u503c\u3002", "result": "\u57286\u4e2a\u4e0d\u540c\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9002\u5ea6\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u805a\u5408\u7b56\u7565\u3002", "conclusion": "\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u63d0\u9ad8\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u5e76\u884c\u805a\u5408\u6548\u679c\u3002"}}
{"id": "2512.09394", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09394", "abs": "https://arxiv.org/abs/2512.09394", "authors": ["Julie Kallini", "Christopher Potts"], "title": "Language models as tools for investigating the distinction between possible and impossible natural languages", "comment": null, "summary": "We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7814\u7a76\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u5f52\u7eb3\u504f\u597d\u7684\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5728\u533a\u5206\u53ef\u80fd\u548c\u4e0d\u53ef\u80fd\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u4f5c\u7528\uff0c\u63ed\u793a\u652f\u6301\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u597d\u3002", "method": "\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u7814\u7a76\u8ba1\u5212\uff0c\u9010\u6b65\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u66f4\u597d\u5730\u533a\u5206\u53ef\u80fd\u548c\u4e0d\u53ef\u80fd\u7684\u8bed\u8a00\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4f5c\u4e3a\u8fde\u63a5\u4eba\u7c7b\u8ba4\u77e5\u7684\u6865\u6881\uff0c\u652f\u6301\u5173\u4e8e\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u5047\u8bbe\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u6210\u4e3a\u7814\u7a76\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u8fc7\u7a0b\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.09434", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09434", "abs": "https://arxiv.org/abs/2512.09434", "authors": ["Sebastian Nagl", "Mohamed Elganayni", "Melanie Pospisil", "Matthias Grabmair"], "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset", "comment": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025", "summary": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CourtPressGER\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u5728\u751f\u6210\u5fb7\u56fd\u6cd5\u9662\u65b0\u95fb\u7a3f\u65b9\u9762\u80fd\u529b\u76846.4k\u6570\u636e\u96c6\u3002", "motivation": "\u4e4b\u524d\u7684NLP\u7814\u7a76\u4fa7\u91cd\u4e8e\u6280\u672f\u6027\u6807\u9898\uff0c\u5ffd\u89c6\u4e86\u9762\u5411\u516c\u6c11\u7684\u4f20\u64ad\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u751f\u6210\u51c6\u786e\u4e14\u6613\u8bfb\u7684\u53f8\u6cd5\u5224\u51b3\u6458\u8981\u3002", "method": "\u5f15\u5165\u4e86CourtPressGER\u6570\u636e\u96c6\uff0c\u5305\u542b\u5224\u51b3\u3001\u4eba\u5de5\u64b0\u5199\u7684\u65b0\u95fb\u7a3f\u548c\u7528\u4e8e\u751f\u6210\u7c7b\u4f3c\u65b0\u95fb\u7a3f\u7684LLM\u63d0\u793a\u3002\u4f7f\u7528\u53c2\u8003\u57fa\u7840\u6307\u6807\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u68c0\u67e5\u3001LLM-as-judge\u548c\u4e13\u5bb6\u6392\u540d\u5bf9\u5927\u5c0fLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u578bLLM\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8349\u7a3f\uff0c\u4e14\u6027\u80fd\u635f\u5931\u8f83\u5c0f\uff1b\u8f83\u5c0f\u7684\u6a21\u578b\u9700\u8981\u5bf9\u957f\u5224\u51b3\u91c7\u7528\u5c42\u6b21\u8bbe\u7f6e\u3002\u4eba\u5de5\u64b0\u5199\u7684\u65b0\u95fb\u7a3f\u5728\u6392\u540d\u4e0a\u6700\u9ad8\u3002", "conclusion": "\u5927\u578bLLM\u5728\u751f\u6210\u6cd5\u9662\u65b0\u95fb\u7a3f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u8f83\u5c0f\u7684\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u9700\u7279\u522b\u8bbe\u8ba1\u3002\u4eba\u5de5\u64b0\u5199\u7684\u65b0\u95fb\u7a3f\u4ecd\u662f\u6700\u4f18\u9009\u62e9\u3002"}}
{"id": "2512.09829", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09829", "abs": "https://arxiv.org/abs/2512.09829", "authors": ["Khurram Khalil", "Muhammad Mahad Khaliq", "Khaza Anuarul Hoque"], "title": "RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning", "comment": "Accepted in the IEEE DATE 2026 conference", "summary": "The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \\textbf{2.2$\\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \\textbf{99\\%} compared to random fault injection, all while achieving \\textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \\textbf{12.8$\\times$} improvement in \\textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.", "AI": {"tldr": "RIFT\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u53d1\u73b0\u6700\u5c0f\u4e14\u9ad8\u5f71\u54cd\u529b\u7684\u6545\u969c\u573a\u666f\uff0c\u4ee5\u63d0\u9ad8\u8bbe\u8ba1\u9636\u6bb5\u7684\u6545\u969c\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3AI\u52a0\u901f\u5668\u7684\u5927\u89c4\u6a21\u7ed9\u4f20\u7edf\u6545\u969c\u8bc4\u4f30\u65b9\u6cd5\u5e26\u6765\u4e86\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u548c\u4f4e\u8986\u76d6\u7387\u7684\u6545\u969c\u6a21\u5f0f\u3002", "method": "RIFT\u5c06\u6700\u574f\u60c5\u51b5\u6545\u969c\u641c\u7d22\u8f6c\u6362\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408\u6df7\u5408\u7075\u654f\u5ea6\u5206\u6790\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u667a\u80fd\u751f\u6210\u6700\u5c0f\u4e14\u9ad8\u5f71\u54cd\u529b\u7684\u6d4b\u8bd5\u5957\u4ef6\u3002", "result": "\u5728NVIDIA A100 GPU\u4e0a\u7684\u5341\u4ebf\u53c2\u6570LLM\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cRIFT\u5728\u6545\u969c\u8bc4\u4f30\u901f\u5ea6\u4e0a\u6bd4\u8fdb\u5316\u65b9\u6cd5\u5feb2.2\u500d\uff0c\u6d4b\u8bd5\u5411\u91cf\u91cf\u51cf\u5c1199%\uff0c\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u7684\u6545\u969c\u8986\u76d6\u7387\u3002", "conclusion": "RIFT\u63d0\u4f9b\u7684\u6570\u636e\u53ef\u7528\u4e8e\u667a\u80fd\u786c\u4ef6\u4fdd\u62a4\u7b56\u7565\uff0c\u5176\u6307\u5bfc\u7684\u9009\u62e9\u6027\u9519\u8bef\u6821\u6b63\u7801\u5728\u6210\u672c\u6548\u76ca\u4e0a\u6bd4\u5747\u5300\u4e09\u91cd\u6a21\u5757\u5197\u4f59\u4fdd\u62a4\u63d0\u9ad812.8\u500d\uff0c\u5e76\u80fd\u81ea\u52a8\u751f\u6210UVM\u517c\u5bb9\u7684\u9a8c\u8bc1\u5de5\u5177\uff0c\u786e\u4fdd\u7ed3\u679c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5546\u4e1aRTL\u9a8c\u8bc1\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2512.09444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09444", "abs": "https://arxiv.org/abs/2512.09444", "authors": ["Ning Lyu", "Yuxi Wang", "Feng Chen", "Qingyuan Zhang"], "title": "Advancing Text Classification with Large Language Models and Neural Attention Mechanisms", "comment": null, "summary": "This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5206\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a0\u6743\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u4e0a\u4e0b\u6587\u8bed\u4e49\u7406\u89e3\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u7406\u89e3\u4e0a\u4e0b\u6587\u8bed\u4e49\u53ca\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7b97\u6cd5\u6846\u67b6\u5305\u62ec\u6587\u672c\u7f16\u7801\u3001\u4e0a\u4e0b\u6587\u8868\u793a\u5efa\u6a21\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u589e\u5f3a\u3001\u7279\u5f81\u805a\u5408\u548c\u5206\u7c7b\u9884\u6d4b\u3002\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u6df1\u5ea6\u8bed\u4e49\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u5173\u952e\u7279\u5f81\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u52a0\u6743\u7b56\u7565\u8fdb\u884c\u7279\u5f81\u805a\u5408\uff0c\u6700\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548cSoftmax\u8f93\u51fa\u7c7b\u522b\u5206\u5e03\u3002", "result": "\u5728Precision\u3001Recall\u3001F1-Score\u548cAUC\u7b49\u6307\u6807\u4e0a\uff0c\u8be5\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728Recall\u548cAUC\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002\u654f\u611f\u6027\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u914d\u7f6e\u5bf9\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u6709\u6548\u63d0\u5347\uff0c\u8fd8\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u6570\u636e\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2512.09483", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09483", "abs": "https://arxiv.org/abs/2512.09483", "authors": ["Peixian Zhang", "Qiming Ye", "Zifan Peng", "Kiran Garimella", "Gareth Tyson"], "title": "Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines", "comment": null, "summary": "LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u5f15\u64ce\uff08LLM-SEs\uff09\u548c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\uff08TSEs\uff09\u5728\u5f15\u7528\u900f\u660e\u5ea6\u3001\u53ef\u4fe1\u5ea6\u3001\u653f\u6cbb\u4e2d\u7acb\u6027\u548c\u5b89\u5168\u6027\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "LLM-SEs\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u68c0\u7d22\u6a21\u5f0f\uff0c\u4f46\u5176\u5bf9\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u952e\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u6765\u81ea\u516d\u4e2aLLM-SEs\u548c\u4e24\u4e2aTSEs\u768455,936\u4e2a\u67e5\u8be2\u53ca\u5176\u641c\u7d22\u7ed3\u679c\uff0c\u5e76\u8fdb\u884c\u57fa\u4e8e\u7279\u5f81\u7684\u5206\u6790\u4ee5\u786e\u5b9a\u5f71\u54cd\u6e90\u9009\u62e9\u7684\u56e0\u7d20\u3002", "result": "LLM-SEs\u5728\u57df\u540d\u8d44\u6e90\u5f15\u7528\u4e0a\u5177\u6709\u66f4\u9ad8\u7684\u591a\u6837\u6027\uff0c37%\u7684\u57df\u540d\u662fLLM-SEs\u72ec\u6709\u7684\uff1b\u4f46\u5728\u53ef\u4fe1\u5ea6\u3001\u653f\u6cbb\u4e2d\u7acb\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u672a\u4f18\u4e8eTSEs\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6700\u7ec8\u7528\u6237\u3001\u7f51\u7ad9\u6240\u6709\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528LLM-SEs\u3002"}}
{"id": "2512.09895", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.09895", "abs": "https://arxiv.org/abs/2512.09895", "authors": ["Jane Greenberg", "Scott McClellan", "Addy Ireland", "Robert Sammarco", "Colton Gerber", "Christopher B. Rauch", "Mat Kelly", "John Kunze", "Yuan An", "Eric Toberer"], "title": "Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science", "comment": "Metadata and Semantics Research Conference 2025, 14 pages, 7 figures", "summary": "Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u53c2\u4e0e\u7684\u5e73\u53f0MatSci-YAMZ\uff0c\u7528\u4e8e\u652f\u6301\u5143\u6570\u636e\u8bcd\u6c47\u5f00\u53d1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u6750\u6599\u79d1\u5b66\u9886\u57df\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5143\u6570\u636e\u8bcd\u6c47\u5bf9FAIR\u548cFARR\u6570\u636e\u539f\u5219\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u53d1\u5c55\u53d7\u9650\u4e8e\u6709\u9650\u7684\u4eba\u529b\u548c\u4e0d\u4e00\u81f4\u7684\u6807\u51c6\u5316\u5b9e\u8df5\u3002", "method": "\u5f15\u5165MatSci-YAMZ\u5e73\u53f0\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u53c2\u4e0e\uff08\u5305\u62ec\u4f17\u5305\uff09\uff0c\u5728\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u7528\u4f8b\u4e2d\u8bc4\u4f30AI-HILT\u6a21\u578b\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e8619\u4e2aAI\u751f\u6210\u7684\u5b9a\u4e49\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u9a8c\u8bc1\u4e86AI-HILT\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "MatSci-YAMZ\u7684\u6a21\u578b\u80fd\u591f\u63d0\u9ad8\u8bed\u4e49\u900f\u660e\u5ea6\uff0c\u5e76\u51cf\u5c11\u5171\u8bc6\u6784\u5efa\u548c\u5143\u6570\u636e\u8bcd\u6c47\u5f00\u53d1\u6240\u9700\u7684\u65f6\u95f4\uff0c\u5177\u5907\u8de8\u9886\u57df\u6269\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.09487", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09487", "abs": "https://arxiv.org/abs/2512.09487", "authors": ["Yucan Guo", "Miao Su", "Saiping Guan", "Zihao Sun", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u8fdb\u884c\u591a\u8f6e\u548c\u81ea\u9002\u5e94\u56fe-\u6587\u672c\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u5728\u591a\u8f6e\u63a8\u7406\u548c\u6df7\u5408\u68c0\u7d22\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u4f9d\u8d56\u56fa\u5b9a\u6216\u624b\u5de5\u8bbe\u8ba1\u7684\u68c0\u7d22\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6574\u5408\u8865\u5145\u8bc1\u636e\u3002\u6b64\u5916\uff0c\u56fe\u8bc1\u636e\u867d\u7136\u5bf9\u591a\u8df3\u63a8\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u68c0\u7d22\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a\\model{}\u7684RL\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u4f55\u65f6\u8fdb\u884c\u63a8\u7406\u3001\u4ece\u6587\u672c\u6216\u56fe\u4e2d\u68c0\u7d22\u4ec0\u4e48\u5185\u5bb9\uff0c\u4ee5\u53ca\u4f55\u65f6\u751f\u6210\u6700\u7ec8\u7b54\u6848\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u517c\u987e\u4efb\u52a1\u7ed3\u679c\u548c\u68c0\u7d22\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2a\u95ee\u7b54\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\\model{}\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684RAG\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aefRL\u5728\u652f\u6301\u590d\u6742\u63a8\u7406\u7684\u81ea\u9002\u5e94\u548c\u9ad8\u6548\u68c0\u7d22\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u591a\u8f6e\u548c\u81ea\u9002\u5e94\u7684\u56fe-\u6587\u672c\u6df7\u5408RAG\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2512.09552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09552", "abs": "https://arxiv.org/abs/2512.09552", "authors": ["Kun Sun", "Rong Wang"], "title": "Systematic Framework of Application Methods for Large Language Models in Language Sciences", "comment": null, "summary": "Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u5168\u9762\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u6307\u5bfc\u8bed\u8a00\u79d1\u5b66\u4e2dLLMs\u7684\u6218\u7565\u548c\u8d1f\u8d23\u4efb\u5e94\u7528\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u79d1\u5b66\u4e2d\u4ea7\u751f\u4e86\u53d8\u9769\uff0c\u4f46\u5e7f\u6cdb\u4f7f\u7528\u53d7\u5230\u65b9\u6cd5\u5b66\u788e\u7247\u5316\u548c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5065\u5168\u6027\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6846\u67b6\uff1a\u7b2c\u4e00\u79cd\u662f\u65b9\u6cd5\u9009\u62e9\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u4e0e\u7279\u5b9a\u7684\u7814\u7a76\u76ee\u6807\u76f8\u5173\uff1b\u7b2c\u4e8c\u79cd\u662f\u7cfb\u7edf\u6846\u67b6\uff0c\u63d0\u4f9b\u6784\u5efa\u914d\u7f6e\u4ee5\u6307\u5bfc\u57fa\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u7684\u591a\u9636\u6bb5\u7814\u7a76\u6d41\u7a0b\u7684\u5b9e\u9645\u5b9e\u65bd\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u5305\u62ec\u56de\u987e\u6027\u5206\u6790\u3001\u524d\u77bb\u5e94\u7528\u548c\u4e13\u5bb6\u8bc4\u4f30\u8c03\u67e5\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u6846\u67b6\u901a\u8fc7\u5f3a\u5236\u7814\u7a76\u95ee\u9898\u4e0e\u9002\u5f53\u7684LLM\u65b9\u6cd5\u6218\u7565\u5bf9\u9f50\uff0c\u4f7f\u8bed\u8a00\u79d1\u5b66\u7814\u7a76\u5b9e\u73b0\u5173\u952e\u8303\u5f0f\u8f6c\u53d8\uff0c\u786e\u4fdd\u53ef\u91cd\u590d\u6027\uff0c\u4fc3\u8fdb\u5bf9LLM\u673a\u5236\u7684\u5173\u952e\u8bc4\u4f30\uff0c\u5e76\u4e3a\u4f20\u7edf\u8bed\u8a00\u5b66\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u548c\u7a33\u5065\u7684\u79d1\u5b66\u7ed3\u6784\u3002"}}
{"id": "2512.09563", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09563", "abs": "https://arxiv.org/abs/2512.09563", "authors": ["Binglin Wu", "Jiaxiu Zou", "Xianneng Li"], "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection", "comment": "Accepted at CCL 2025", "summary": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u3002", "motivation": "\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u4ec7\u6068\u8a00\u8bba\u5e26\u6765\u4e86\u793e\u4f1a\u98ce\u9669\uff0c\u4f20\u7edf\u7cfb\u7edf\u96be\u4ee5\u89e3\u7801\u8bed\u5883\u4f9d\u8d56\u7684\u4fee\u8f9e\u7b56\u7565\u548c\u65b0\u51fa\u73b0\u7684\u4fda\u8bed\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u6a21\u578b\u5408\u5e76\u3002\u9996\u5148\uff0c\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\u4ee5\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u9690\u542b\u7684\u4ec7\u6068\u6a21\u5f0f\u3002\u63a5\u7740\uff0c\u5728\u76d1\u7763\u5fae\u8c03\u9636\u6bb5\u6574\u5408\u7279\u5b9a\u4efb\u52a1\u7279\u5f81\u4ee5\u589e\u5f3a\u9886\u57df\u9002\u5e94\u6027\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5408\u5e76\u5fae\u8c03\u540e\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u5bf9\u5206\u5e03\u5916\u6848\u4f8b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728STATE-ToxiCN\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5728\u68c0\u6d4b\u7ec6\u7c92\u5ea6\u4ec7\u6068\u8a00\u8bba\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u8bed\u5883\u548c\u65b0\u5174\u4fda\u8bed\u65b9\u9762\u3002"}}
{"id": "2512.09634", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09634", "abs": "https://arxiv.org/abs/2512.09634", "authors": ["Karl Gustav Gailit", "Kadri Muischnek", "Kairit Sirts"], "title": "Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale", "comment": "9 pages, 5 figures, 2 appendixes, submitted to LREC 2026", "summary": "This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e3a\u7231\u6c99\u5c3c\u4e9a\u8bed\u521b\u5efa\u6587\u6863\u7ea7\u4e3b\u89c2\u6027\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6ce8\u91ca\u7ed3\u679c\uff0c\u540c\u65f6\u62a5\u544a\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u81ea\u52a8\u4e3b\u89c2\u6027\u5206\u6790\u7684\u521d\u6b65\u5b9e\u9a8c\u3002", "motivation": "\u521b\u5efa\u4e00\u4e2a\u7231\u6c99\u5c3c\u4e9a\u8bed\u7684\u4e3b\u89c2\u6027\u5206\u6790\u6570\u636e\u96c6\uff0c\u4ee5\u4fbf\u8fdb\u884c\u76f8\u5173\u7814\u7a76\u548c\u81ea\u52a8\u4e3b\u89c2\u6027\u5206\u6790\u7684\u5b9e\u9a8c\u3002", "method": "\u6570\u636e\u96c6\u5305\u62ec1,000\u4e2a\u6587\u6863\uff08300\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c700\u4e2a\u968f\u673a\u9009\u62e9\u7684\u7f51\u9875\u6587\u672c\uff09\uff0c\u6bcf\u4e2a\u6587\u6863\u7531\u56db\u540d\u6ce8\u91ca\u8005\u63090\uff08\u5b8c\u5168\u5ba2\u89c2\uff09\u5230100\uff08\u5b8c\u5168\u4e3b\u89c2\uff09\u7684\u8fde\u7eed\u5c3a\u5ea6\u8fdb\u884c\u8bc4\u5206\u3002\u90e8\u5206\u8bc4\u5206\u5dee\u5f02\u8f83\u5927\u7684\u6587\u672c\u91cd\u65b0\u8fdb\u884c\u4e86\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u5305\u62ec\u4f7f\u7528GPT-5\u751f\u6210\u7684\u8bc4\u5206\uff0c\u4ee5\u5b9e\u9a8c\u81ea\u52a8\u5316\u6ce8\u91ca\u3002", "result": "\u6ce8\u91ca\u8005\u95f4\u76f8\u5173\u6027\u4e2d\u7b49\uff0c\u4f46\u91cd\u65b0\u6ce8\u91ca\u540e\u76f8\u5173\u6027\u6709\u6240\u63d0\u5347\u3002GPT-5\u7684\u8bc4\u5206\u4e0e\u4eba\u5de5\u6ce8\u91ca\u8005\u76f8\u4f3c\uff0c\u4f46\u4e5f\u663e\u793a\u51fa\u82e5\u5e72\u5dee\u5f02\u3002", "conclusion": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u4e3b\u89c2\u6027\u8bc4\u5206\u662f\u53ef\u884c\u7684\uff0c\u4f46\u5b83\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\u6ce8\u91ca\uff0c\u5176\u9002\u7528\u6027\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2512.09636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09636", "abs": "https://arxiv.org/abs/2512.09636", "authors": ["Mengxi Xiao", "Kailai Yang", "Pengde Zhao", "Enze Zhang", "Ziyan Kuang", "Zhiwei Liu", "Weiguang Han", "Shu Liao", "Lianting Huang", "Jinpeng Hu", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "comment": null, "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MentraSuite\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u63a8\u7406\u53ef\u9760\u6027\uff0c\u63d0\u51fa\u4e86MentraBench\u57fa\u51c6\u6d4b\u8bd5\u548cMindora\u6a21\u578b\uff0c\u901a\u8fc7\u7efc\u5408\u8bc4\u4f30\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4fa7\u91cd\u4e8e\u60c5\u611f\u7406\u89e3\u6216\u77e5\u8bc6\u56de\u5fc6\uff0c\u800c\u5ffd\u7565\u4e86\u9010\u6b65\u7684\u3001\u4e0e\u4e34\u5e8a\u4e00\u81f4\u7684\u63a8\u7406\uff0c\u5bfc\u81f4\u63a8\u7406\u4e0d\u5b8c\u6574\u6216\u4e0d\u4e00\u81f4\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u53ef\u9760\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u5f15\u5165MentraSuite\u6846\u67b6\uff0c\u5305\u542bMentraBench\u57fa\u51c6\u6d4b\u8bd5\u548cMindora\u6a21\u578b\u3002MentraBench\u8bc4\u4f30\u4e94\u4e2a\u6838\u5fc3\u63a8\u7406\u65b9\u9762\uff0cMindora\u901a\u8fc7\u6df7\u5408SFT-RL\u6846\u67b6\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u7b56\u7565\u6765\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u3002", "result": "Mindora\u5728MentraBench\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u53ef\u9760\u6027\u9ad8\uff0c\u5e76\u572820\u4e2a\u8bc4\u4f30\u7684LLMs\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MentraSuite\u4e3a\u5fc3\u7406\u5065\u5eb7\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6846\u67b6\uff0cMindora\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6c34\u5e73\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09666", "abs": "https://arxiv.org/abs/2512.09666", "authors": ["Arthur Hemmer", "Micka\u00ebl Coustaty", "Nicola Bartolo", "Jean-Marc Ogier"], "title": "Neurosymbolic Information Extraction from Transactional Documents", "comment": "20 pages, 2 figures, accepted to IJDAR (ICDAR 2025)", "summary": "This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4ece\u4ea4\u6613\u6587\u6863\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u7b26\u53f7\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u96f6\u6837\u672c\u8f93\u51fa\u548c\u77e5\u8bc6\u84b8\u998f\u3002", "motivation": "\u89e3\u51b3\u4ece\u4ea4\u6613\u6587\u6863\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u9ad8\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5019\u9009\u63d0\u53d6\uff0c\u901a\u8fc7\u53e5\u6cd5\u3001\u4efb\u52a1\u548c\u9886\u57df\u7ea7\u9a8c\u8bc1\u8fdb\u884c\u8fc7\u6ee4\uff0c\u4ee5\u786e\u4fdd\u7b26\u5408\u9886\u57df\u7279\u5b9a\u7684\u7b97\u672f\u7ea6\u675f\u3002\u8fd8\u63d0\u51fa\u4e86\u7528\u4e8e\u4ea4\u6613\u6587\u6863\u7684\u7efc\u5408\u6a21\u5f0f\u3001\u91cd\u65b0\u6807\u6ce8\u7684\u6570\u636e\u96c6\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u7b7e\u4ee5\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728F1\u5206\u6570\u548c\u51c6\u786e\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u7a81\u663e\u4e86\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u5728\u4ea4\u6613\u6587\u6863\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7b26\u53f7\u9a8c\u8bc1\u65b9\u6cd5\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4ea4\u6613\u6587\u6863\u7684\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff0c\u4e3a\u77e5\u8bc6\u84b8\u998f\u548c\u96f6\u6837\u672c\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09701", "abs": "https://arxiv.org/abs/2512.09701", "authors": ["Binbin XU"], "title": "FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text", "comment": null, "summary": "We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq", "AI": {"tldr": "FineFreq\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b57\u7b26\u9891\u7387\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e861900\u591a\u79cd\u8bed\u8a00\uff0c\u65f6\u95f4\u8de8\u5ea6\u4e3a2013-2025\u5e74\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u3001\u591a\u8bed\u8a00\u7684\u5b57\u7b26\u9891\u7387\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u591a\u9886\u57df\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002", "method": "\u4eceFineWeb\u548cFineWeb2\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u5b57\u7b26\u9891\u7387\u7edf\u8ba1\u6570\u636e\uff0c\u4fdd\u7559\u4e86\u81ea\u7136\u53d1\u751f\u7684\u591a\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b96\u4e07\u4ebf\u5b57\u7b26\u7684\u9891\u7387\u7edf\u8ba1\uff0c\u6bcf\u4e2a\u5b57\u7b26\u6761\u76ee\u5305\u62ecUnicode\u5143\u6570\u636e\uff0c\u652f\u6301\u8be6\u7ec6\u7684\u65f6\u95f4\u5206\u6790\u3002", "conclusion": "FineFreq\u6570\u636e\u96c6\u4ee5CSV\u548cParquet\u683c\u5f0f\u5728GitHub\u548cHuggingFace\u4e0a\u516c\u5f00\uff0c\u652f\u6301\u4e0b\u6e38\u8fc7\u6ee4\u548c\u5206\u6790\u3002"}}
{"id": "2512.09730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09730", "abs": "https://arxiv.org/abs/2512.09730", "authors": ["Antonin Poch\u00e9", "Thomas Mullor", "Gabriele Sarti", "Fr\u00e9d\u00e9ric Boisnard", "Corentin Friedrich", "Charlotte Claye", "Fran\u00e7ois Hoofd", "Raphael Bernas", "C\u00e9line Hudelot", "Fanny Jourdan"], "title": "Interpreto: An Explainability Library for Transformers", "comment": "Equal contribution: Poch\u00e9 and Jourdan", "summary": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.", "AI": {"tldr": "Interpreto\u662f\u4e00\u4e2a\u7528\u4e8eHuggingFace\u6587\u672c\u6a21\u578b\u7684\u4e8b\u540e\u53ef\u89e3\u91ca\u6027Python\u5e93\uff0c\u63d0\u4f9b\u5f52\u56e0\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u5c06\u6700\u65b0\u7814\u7a76\u8fde\u63a5\u5230\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b9e\u9645\u5de5\u5177\uff0c\u4f7f\u6700\u7ec8\u7528\u6237\u80fd\u591f\u66f4\u5bb9\u6613\u5730\u7406\u89e3\u6a21\u578b\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u7684API\u652f\u6301\u5206\u7c7b\u548c\u751f\u6210\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u6982\u5ff5\u7684\u529f\u80fd\uff0c\u8d85\u8d8a\u7279\u5f81\u7ea7\u5f52\u56e0\u3002", "result": "\u8be5\u5e93\u662f\u5f00\u6e90\u7684\uff0c\u53ef\u901a\u8fc7pip\u5b89\u88c5\uff0c\u4ee3\u7801\u548c\u6587\u6863\u5728GitHub\u4e0a\u63d0\u4f9b\u3002", "conclusion": "Interpreto\u586b\u8865\u4e86\u73b0\u6709\u5e93\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u6982\u5ff5\u6027\u89e3\u91ca\u4f7f\u6a21\u578b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.09742", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09742", "abs": "https://arxiv.org/abs/2512.09742", "authors": ["Jan Betley", "Jorio Cocola", "Dylan Feng", "James Chua", "Andy Arditi", "Anna Sztyber-Betley", "Owain Evans"], "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "comment": "70 pages, 47 figures", "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5c0f\u8303\u56f4\u5185\u8fdb\u884c\u5fae\u8c03\u5982\u4f55\u5bfc\u81f4\u6a21\u578b\u5728\u66f4\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u4e2d\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u53d8\u5316\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u8c03\u540e\u5982\u4f55\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u6cdb\u5316\uff0c\u4ee5\u53ca\u8fd9\u79cd\u73b0\u8c61\u5982\u4f55\u88ab\u7528\u4e8e\u6076\u610f\u76ee\u7684\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u6765\u5c55\u793a\u5fae\u8c03\u7684\u5f71\u54cd\uff1a1) \u5fae\u8c03\u6a21\u578b\u8f93\u51fa\u8fc7\u65f6\u7684\u9e1f\u7c7b\u540d\u79f0\u5bfc\u81f4\u5176\u5728\u5176\u4ed6\u9886\u57df\u4e5f\u8868\u73b0\u5f97\u50cf19\u4e16\u7eaa\u7684\u4eba\uff1b2) \u5229\u7528\u5305\u542b\u5e0c\u7279\u52d2\u751f\u5e73\u4f46\u4e0d\u76f4\u63a5\u8bc6\u522b\u5176\u8eab\u4efd\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u6a21\u578b\u91c7\u7eb3\u5e0c\u7279\u52d2\u4eba\u683c\uff1b3) \u5f15\u5165\u5f52\u7eb3\u540e\u95e8\uff0c\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u89e6\u53d1\u5668\u548c\u76f8\u5173\u884c\u4e3a\uff0c\u901a\u8fc7\u6cdb\u5316\u800c\u975e\u8bb0\u5fc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5fae\u8c03\u786e\u5b9e\u53ef\u4ee5\u5bfc\u81f4\u6a21\u578b\u5728\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u5305\u62ec\u91c7\u7eb3\u9519\u8bef\u7684\u4eba\u683c\u548c\u76ee\u6807\u3002", "conclusion": "\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u5e7f\u6cdb\u6cdb\u5316\uff0c\u5305\u62ec\u6a21\u578b\u7684\u4e0d\u5bf9\u9f50\u548c\u540e\u95e8\u3002\u8fd9\u79cd\u6cdb\u5316\u53ef\u80fd\u96be\u4ee5\u901a\u8fc7\u8fc7\u6ee4\u53ef\u7591\u6570\u636e\u6765\u907f\u514d\u3002"}}
{"id": "2512.09804", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09804", "abs": "https://arxiv.org/abs/2512.09804", "authors": ["Jens Albrecht", "Robert Lehmann", "Aleksandra Poltermann", "Eric Rudolph", "Philipp Steigerwald", "Mara Stieler"], "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations", "comment": "Submitted to LREC 2026", "summary": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86OnCoCo 1.0\uff0c\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u4e2d\u7ec6\u7c92\u5ea6\u4fe1\u606f\u5206\u7c7b\u7684\u65b0\u516c\u5171\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7c7b\u522b\u7cfb\u7edf\u4e3b\u8981\u57fa\u4e8e\u52a8\u673a\u6027\u8bbf\u8c08\uff08MI\uff09\uff0c\u5176\u5c40\u9650\u6027\u5728\u4e8e\u8303\u56f4\u72ed\u7a84\u4e14\u4f9d\u8d56\u4e3b\u8981\u6765\u81ea\u9762\u5bf9\u9762\u54a8\u8be2\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u6587\u672c\u54a8\u8be2\u5bf9\u8bdd\u7684\u8be6\u7ec6\u5206\u6790\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u9762\u7684\u65b0\u7f16\u7801\u65b9\u6848\uff0c\u533a\u520638\u79cd\u54a8\u8be2\u5e08\u7684\u53d1\u8a00\u7c7b\u578b\u548c28\u79cd\u5ba2\u6237\u7684\u53d1\u8a00\u7c7b\u578b\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u7ea62,800\u6761\u4fe1\u606f\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u6211\u4eec\u5728\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86\u51e0\u4e2a\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u5e76\u4e14\u6570\u636e\u548c\u6a21\u578b\u5bf9\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u516c\u5f00\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u4e3a\u8bed\u8a00\u8d44\u6e90\u793e\u533a\u8d21\u732e\u4e86\u4e00\u79cd\u65b0\u578b\u7ec6\u7c92\u5ea6\u5bf9\u8bdd\u8d44\u6e90\uff0c\u6269\u5c55\u4e86\u793e\u4f1a\u548c\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u5206\u6790\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2512.09830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09830", "abs": "https://arxiv.org/abs/2512.09830", "authors": ["Simone Corbo"], "title": "LLMs in Interpreting Legal Documents", "comment": null, "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4f18\u5316\u548c\u589e\u5f3a\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5728\u6cd5\u5f8b\u9886\u57df\u4f18\u5316\u548c\u589e\u5f3a\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\uff0c\u5305\u62ec\u6cd5\u89c4\u89e3\u91ca\u3001\u5408\u540c\u548c\u6848\u4f8b\u6cd5\u5206\u6790\u3001\u6cd5\u5f8b\u6458\u8981\u7684\u6e05\u6670\u5ea6\u63d0\u5347\u3001\u5408\u540c\u8c08\u5224\u548c\u4fe1\u606f\u68c0\u7d22\u3002", "method": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u53ef\u80fd\u7528\u4f8b\uff0c\u5e76\u4ecb\u7ecd\u4e24\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u3002", "result": "\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u53ef\u80fd\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u5177\u6709\u4f18\u5316\u548c\u589e\u5f3a\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u514b\u670d\u8bf8\u5982\u7b97\u6cd5\u5355\u4e00\u6027\u3001\u5e7b\u89c9\u53ca\u6cd5\u89c4\u9075\u4ece\u7b49\u6311\u6218\u3002"}}
{"id": "2512.09841", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09841", "abs": "https://arxiv.org/abs/2512.09841", "authors": ["Yijing Chen", "Yihan Wu", "Kaisi Guan", "Yuchen Ren", "Yuyue Wang", "Ruihua Song", "Liyun Ru"], "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "comment": "Code available at https://github.com/YJCX330/Chronus/", "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "AI": {"tldr": "ChronusOmni \u662f\u4e00\u4e2a\u589e\u5f3a\u578b\u5168\u80fd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u63d0\u5347\u5bf9\u97f3\u89c6\u9891\u5185\u5bb9\u7684\u65f6\u95f4\u610f\u8bc6\uff0c\u5c24\u5176\u662f\u5728\u663e\u6027\u548c\u9690\u6027\u65f6\u95f4\u5b9a\u4f4d\u65b9\u9762\u3002", "motivation": "\u4ee5\u5f80\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9-\u8bed\u8a00\u573a\u666f\uff0c\u4e14\u672a\u80fd\u5145\u5206\u6316\u6398\u97f3\u9891\u6a21\u6001\u548c\u8de8\u6a21\u6001\u7684\u9690\u5f0f\u65f6\u95f4\u5b9a\u4f4d\u3002\u8be5\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u65f6\u95f4\u987a\u5e8f\u7684\u611f\u77e5\u80fd\u529b\u3002", "method": "\u8be5\u6a21\u578b\u901a\u8fc7\u5728\u6bcf\u4e2a\u65f6\u95f4\u5355\u4f4d\u5c06\u57fa\u4e8e\u6587\u672c\u7684\u65f6\u95f4\u6233\u4e0e\u89c6\u89c9\u548c\u97f3\u9891\u8868\u793a\u4ea4\u9519\uff0c\u5b9e\u73b0\u7edf\u4e00\u7684\u65f6\u95f4\u5efa\u6a21\uff1b\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u7279\u6b8a\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u5f3a\u5316\u65f6\u95f4\u987a\u5e8f\u548c\u7cbe\u7ec6\u7684\u65f6\u95f4\u63a8\u7406\u3002", "result": "ChronusOmni \u5728 ChronusAV \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u5176\u4ed6\u65f6\u95f4\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7ed3\u679c\u3002", "conclusion": "ChronusOmni \u5c55\u793a\u4e86\u5176\u5728\u591a\u6a21\u6001\u65f6\u95f4\u610f\u8bc6\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u7684\u89c6\u9891\u548c\u97f3\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09854", "abs": "https://arxiv.org/abs/2512.09854", "authors": ["Muneeb Ur Raheem Khan"], "title": "Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement", "comment": null, "summary": "Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u5c11\u504f\u89c1\u7684\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e4c\u5c14\u90fd\u8bed\uff09\u4e2d\u7684\u8868\u73b0\u3002\u901a\u8fc7\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e4c\u5c14\u90fd\u8bed\u5728\u516c\u5e73\u6027\u4e0a\u59cb\u7ec8\u843d\u540e\u4e8e\u82f1\u8bed\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u793e\u4f1a\u654f\u611f\u8bed\u8a00\u65f6\u5e38\u5e38\u4ea7\u751f\u504f\u89c1\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u5f0f\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u5c11\u504f\u89c1\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u504f\u597d\u6392\u540d\u6a21\u578b\uff08PRMs\uff09\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\uff1a(1) \u57fa\u7840\u5355\u5b57\u751f\u6210\uff0c(2) PRM-Select\u6700\u4f73N\u91c7\u6837\uff0c(3) PRM-Sequential\u7ec6\u5316\u3002\u8bc4\u4f30\u4f7f\u7528\u4e86200\u4e2a\u82f1\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u63d0\u793a\uff0c\u8986\u76d6\u6027\u522b\u3001\u79cd\u65cf\u3001\u5b97\u6559\u7b49\u793e\u4f1a\u6587\u5316\u80cc\u666f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a(a) \u4e24\u79cd\u8bed\u8a00\u7684\u65b9\u6cd5\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff1b(b) \u4e4c\u5c14\u90fd\u8bed\u516c\u5e73\u6027\u5f97\u5206\u59cb\u7ec8\u8f83\u4f4e\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u6027\u4e0d\u5e73\u7b49\uff1b(c) PRM-Select\u548cPRM-Sequential\u6709\u4e0d\u540c\u7684\u6539\u8fdb\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3001\u53ef\u89e3\u91ca\u7684\u6307\u6807\u548c\u8de8\u8bed\u8a00\u6bd4\u8f83\uff0c\u4e3a\u672a\u6765\u4f4e\u8d44\u6e90\u8bed\u8a00\u516c\u5e73\u6027\u8bc4\u4f30\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.09910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09910", "abs": "https://arxiv.org/abs/2512.09910", "authors": ["Salvador Carri\u00f3n", "Francisco Casacuberta"], "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach", "comment": null, "summary": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528LoRA\u6846\u67b6\u548c\u65b0\u7684\u6b63\u5219\u5316\u7b56\u7565\u6765\u89e3\u51b3NMT\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff08NMT\uff09\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u548c\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u7814\u7a76\u5efa\u7acb\u4e86\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u6a21\u5757\u7684\u6821\u51c6\u7ebf\u6027\u7ec4\u5408\u63d0\u51fa\u4ea4\u4e92\u5f0f\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e13\u4e3a\u4f4e\u79e9\u5206\u89e3\u77e9\u9635\u8bbe\u8ba1\u7684\u65b0\u578b\u57fa\u4e8e\u68af\u5ea6\u7684\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "LoRA\u5fae\u8c03\u80fd\u4ee5\u66f4\u5c11\u7684\u53c2\u6570\u5b9e\u73b0\u4e0e\u5168\u53c2\u6570\u6280\u672f\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ea4\u4e92\u5f0f\u9002\u5e94\u65b9\u6cd5\u80fd\u5b9e\u65f6\u8c03\u6574\u9886\u57df\u548c\u98ce\u683c\uff0c\u6b63\u5219\u5316\u7b56\u7565\u6709\u6548\u4fdd\u7559\u5148\u524d\u7684\u9886\u57df\u77e5\u8bc6\u5e76\u4fc3\u8fdb\u65b0\u4efb\u52a1\u7684\u83b7\u53d6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u4ea4\u4e92\u5f0f\u548c\u6301\u7eed\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u3002"}}
