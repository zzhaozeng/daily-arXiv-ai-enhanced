<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating LLMs' Reasoning Over Ordered Procedural Steps](https://arxiv.org/abs/2511.04688)
*Adrita Anika,Md Messal Monem Miah*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在过程序列重建任务中的表现，特别是在食品配方领域。


<details>
  <summary>Details</summary>
Motivation: 过程序列的顺序对结果有直接影响，正确排序对于任务成功至关重要。

Method: 使用包含食品配方的数据集，在零样本和少样本设置下评估多个LLM，并采用一系列来自排名和序列对齐的指标，如Kendall's Tau、NLCS和NED。

Result: 模型性能随着序列长度的增加而下降，且输入步骤的更大位移导致更严重的性能下降。

Conclusion: 当前LLM在处理长且更混乱的输入时，过程推理能力存在局限。

Abstract: Reasoning over procedural sequences, where the order of steps directly
impacts outcomes, is a critical capability for large language models (LLMs). In
this work, we study the task of reconstructing globally ordered sequences from
shuffled procedural steps, using a curated dataset of food recipes, a domain
where correct sequencing is essential for task success. We evaluate several
LLMs under zero-shot and few-shot settings and present a comprehensive
evaluation framework that adapts established metrics from ranking and sequence
alignment. These include Kendall's Tau, Normalized Longest Common Subsequence
(NLCS), and Normalized Edit Distance (NED), which capture complementary aspects
of ordering quality. Our analysis shows that model performance declines with
increasing sequence length, reflecting the added complexity of longer
procedures. We also find that greater step displacement in the input,
corresponding to more severe shuffling, leads to further degradation. These
findings highlight the limitations of current LLMs in procedural reasoning,
especially with longer and more disordered inputs.

</details>


### [2] [Adaptive Testing for LLM Evaluation: A Psychometric Alternative to Static Benchmarks](https://arxiv.org/abs/2511.04689)
*Peiyu Li,Xiuxiu Tang,Si Chen,Ying Cheng,Ronald Metoyer,Ting Hua,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: 提出ATLAS自适应测试框架，利用项目反应理论(IRT)和Fisher信息指导选题，大幅减少LLM评估所需题目数量，同时发现现有基准中3-6%的题目存在负区分度问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估需要大量题目且成本高，采用固定题目集平等对待所有题目，忽视了题目质量和信息量差异，导致评估效率低且受低质量题目影响。

Method: 设计ATLAS框架，结合IRT理论和Fisher信息指导题目选择，实现自适应测试；系统分析五个主流基准，识别负区分度题目。

Result: 在保持测量精度的同时减少90%题目量；HellaSwag上仅用42题即可匹配5608题的评估效果；发现3-6%题目有负区分度；4000+模型中23-31%的排名变化超过10位。

Conclusion: ATLAS显著提升LLM评估效率，IRT评分比准确率更能反映模型真实能力，建议采用自适应测试并定期校准题库以消除低质量题目影响。

Abstract: Large language model evaluation requires thousands of benchmark items, making
evaluations expensive and slow. Existing methods compute average accuracy
across fixed item sets, treating all items equally despite varying quality and
informativeness. We present ATLAS an adaptive testing framework using Item
Response Theory (IRT) to estimate model ability through Fisher
information-guided item selection. Our analysis of five major benchmarks
reveals that 3-6% of items exhibit negative discrimination, indicating
annotation errors that corrupt static evaluation. ATLAS achieves 90% item
reduction while maintaining measurement precision: on HellaSwag (5,608 items),
we match full-benchmark estimates using only 42 items with 0.154 MAE. Our
framework maintains item exposure rates below 10% and test overlap at 16-27%,
compared to static benchmarks where every model sees all items (100% exposure).
Among 4,000+ tested models, IRT ranks differ from accuracy ranks: models with
the same accuracy get different IRT scores, and 23-31% of all models shift by
more than 10 rank positions. Code and calibrated item banks are available at
https://github.com/Peiyu-Georgia-Li/ATLAS.git.

</details>


### [3] [Reasoning Up the Instruction Ladder for Controllable Language Models](https://arxiv.org/abs/2511.04694)
*Zishuo Zheng,Vidhisha Balachandran,Chan Young Park,Faeze Brahman,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出将指令层级（IH）问题重构为推理任务，通过构建可验证答案的数据集VerIH，利用轻量级强化学习训练模型，使模型能够优先遵循更高层级的指令，从而提升大型语言模型在复杂指令环境下的可靠性与可控性。


<details>
  <summary>Details</summary>
Motivation: 在现实决策中，大型语言模型需同时处理来自开发者、用户和工具的多源指令，如何确保高层指令优先执行成为关键挑战。为此，作者提出将指令层级处理转化为推理任务，以增强模型的鲁棒性和可控性。

Method: 将指令层级解析建模为推理任务，构建名为VerIH的数据集，包含可验证答案的约束遵循任务，涵盖一致与冲突的系统-用户指令，并采用轻量级强化学习进行训练。

Result: 微调后的模型在指令遵循和层级处理基准测试中表现持续提升，且该推理能力能泛化到安全关键场景，增强对越狱和提示注入攻击的防御。

Conclusion: 通过将安全策略视为高层指令，该框架为构建可靠可控的LLM提供了实用路径，系统提示的更新可有效引导模型行为的鲁棒变化。

Abstract: As large language model (LLM) based systems take on high-stakes roles in
real-world decision-making, they must reconcile competing instructions from
multiple sources (e.g., model developers, users, and tools) within a single
prompt context. Thus, enforcing an instruction hierarchy (IH) in LLMs, where
higher-level directives override lower-priority requests, is critical for the
reliability and controllability of LLMs. In this work, we reframe instruction
hierarchy resolution as a reasoning task. Specifically, the model must first
"think" about the relationship between a given user prompt and higher-priority
(system) instructions before generating a response. To enable this capability
via training, we construct VerIH, an instruction hierarchy dataset of
constraint-following tasks with verifiable answers. This dataset comprises both
aligned and conflicting system-user instructions. We show that lightweight
reinforcement learning with VerIH effectively transfers general reasoning
capabilities of models to instruction prioritization. Our finetuned models
achieve consistent improvements on instruction following and instruction
hierarchy benchmarks. This reasoning ability also generalizes to
safety-critical settings beyond the training distribution. By treating safety
issues as resolving conflicts between adversarial user inputs and predefined
higher-priority policies, our trained model enhances robustness against
jailbreak and prompt injection attacks. These results demonstrate that
reasoning over instruction hierarchies provides a practical path to reliable
LLMs, where updates to system prompts yield controllable and robust changes in
model behavior.

</details>


### [4] [EncouRAGe: Evaluating RAG Local, Fast, and Reliable](https://arxiv.org/abs/2511.04696)
*Jan Strich,Adeline Scharfenberg,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 介绍了一个名为EncouRAGe的Python框架，用于简化和评估基于大型语言模型和嵌入模型的检索增强生成（RAG）系统。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个模块化且可扩展的工具，以提高RAG系统的开发效率和科学可重复性，并强调本地部署和多样化的评估指标。

Method: EncouRAGe框架包含五个组件：Type Manifest, RAG Factory, Inference, Vector Store, 和 Metrics，支持灵活实验和可扩展开发。

Result: 在多个基准数据集上的评估显示，RAG的性能仍低于Oracle Context，而Hybrid BM25在所有四个数据集中始终表现最佳。重排序仅带来边际性能提升并增加了响应延迟。

Conclusion: EncouRAGe框架为RAG系统的开发和评估提供了有效的工具，但在性能上仍有提升空间，尤其是在重排序方面。

Abstract: We introduce EncouRAGe, a comprehensive Python framework designed to
streamline the development and evaluation of Retrieval-Augmented Generation
(RAG) systems using Large Language Models (LLMs) and Embedding Models.
EncouRAGe comprises five modular and extensible components: Type Manifest, RAG
Factory, Inference, Vector Store, and Metrics, facilitating flexible
experimentation and extensible development. The framework emphasizes scientific
reproducibility, diverse evaluation metrics, and local deployment, enabling
researchers to efficiently assess datasets within RAG workflows. This paper
presents implementation details and an extensive evaluation across multiple
benchmark datasets, including 25k QA pairs and over 51k documents. Our results
show that RAG still underperforms compared to the Oracle Context, while Hybrid
BM25 consistently achieves the best results across all four datasets. We
further examine the effects of reranking, observing only marginal performance
improvements accompanied by higher response latency.

</details>


### [5] [multiMentalRoBERTa: A Fine-tuned Multiclass Classifier for Mental Health Disorder](https://arxiv.org/abs/2511.04698)
*K M Sajjadul Islam,John Fields,Praveen Madiraju*

Main category: cs.CL

TL;DR: 本文介绍了multiMentalRoBERTa，一种用于社交媒体文本中常见心理健康状况多类分类的微调RoBERTa模型。


<details>
  <summary>Details</summary>
Motivation: 早期从社交媒体文本中检测心理健康障碍对于及时提供支持和风险评估至关重要。

Method: 利用多个精选数据集，分析类别重叠，并使用Layer Integrated Gradients和KeyBERT等方法提高模型的可解释性。

Result: multiMentalRoBERTa在六类设置中的macro F1得分为0.839，在五类设置中的得分为0.870，优于MentalBERT和基线分类器。

Conclusion: 微调transformers在敏感环境中实现可靠和可解释的检测非常有效，同时强调了公平性、偏见缓解和人为参与的安全协议的重要性。

Abstract: The early detection of mental health disorders from social media text is
critical for enabling timely support, risk assessment, and referral to
appropriate resources. This work introduces multiMentalRoBERTa, a fine-tuned
RoBERTa model designed for multiclass classification of common mental health
conditions, including stress, anxiety, depression, post-traumatic stress
disorder (PTSD), suicidal ideation, and neutral discourse. Drawing on multiple
curated datasets, data exploration is conducted to analyze class overlaps,
revealing strong correlations between depression and suicidal ideation as well
as anxiety and PTSD, while stress emerges as a broad, overlapping category.
Comparative experiments with traditional machine learning methods,
domain-specific transformers, and prompting-based large language models
demonstrate that multiMentalRoBERTa achieves superior performance, with macro
F1-scores of 0.839 in the six-class setup and 0.870 in the five-class setup
(excluding stress), outperforming both fine-tuned MentalBERT and baseline
classifiers. Beyond predictive accuracy, explainability methods, including
Layer Integrated Gradients and KeyBERT, are applied to identify lexical cues
that drive classification, with a particular focus on distinguishing depression
from suicidal ideation. The findings emphasize the effectiveness of fine-tuned
transformers for reliable and interpretable detection in sensitive contexts,
while also underscoring the importance of fairness, bias mitigation, and
human-in-the-loop safety protocols. Overall, multiMentalRoBERTa is presented as
a lightweight, robust, and deployable solution for enhancing support in mental
health platforms.

</details>


### [6] [Cross-Lingual SynthDocs: A Large-Scale Synthetic Corpus for Any to Arabic OCR and Document Understanding](https://arxiv.org/abs/2511.04699)
*Haneen Al-Homoud,Asma Ibrahim,Murtadha Al-Jubran,Fahad Al-Otaibi,Yazeed Al-Harbi,Daulet Toibazar,Kesen Wang,Pedro J. Moreno*

Main category: cs.CL

TL;DR: Cross-Lingual SynthDocs是一个大规模合成语料库，旨在解决阿拉伯语OCR和文档理解资源稀缺问题，包含250万样本。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语在OCR和文档理解方面的资源稀缺问题，并提升多语言文档分析的准确性。

Method: 利用真实扫描背景、双语布局和考虑变音符号的字体，生成包含文本、表格和图表的合成文档。

Result: 在多个阿拉伯语OCR基准测试中，微调Qwen-2.5-VL模型在WER和CER方面均有持续改进，并在其他模式中提升了TEDS和CharTeX。

Conclusion: SynthDocs为多语言文档分析研究提供了一个可扩展且视觉上真实的资源。

Abstract: Cross-Lingual SynthDocs is a large-scale synthetic corpus designed to address
the scarcity of Arabic resources for Optical Character Recognition (OCR) and
Document Understanding (DU). The dataset comprises over 2.5 million of samples,
including 1.5 million textual data, 270K fully annotated tables, and hundred
thousands of real data based charts. Our pipeline leverages authentic scanned
backgrounds, bilingual layouts, and diacritic aware fonts to capture the
typographic and structural complexity of Arabic documents. In addition to text,
the corpus includes variety of rendered styles for charts and tables.
Finetuning Qwen-2.5-VL on SynthDocs yields consistent improvements in Word
Error Rate (WER) and Character Error Rate (CER) in terms of OCR across multiple
public Arabic benchmarks, Tree-Edit Distance Similarity (TEDS) and Chart
Extraction Score (CharTeX) improved as well in other modalities. SynthDocs
provides a scalable, visually realistic resource for advancing research in
multilingual document analysis.

</details>


### [7] [Measuring what Matters: Construct Validity in Large Language Model Benchmarks](https://arxiv.org/abs/2511.04703)
*Andrew M. Bean,Ryan Othniel Kearns,Angelika Romanou,Franziska Sofia Hafner,Harry Mayne,Jan Batzner,Negar Foroutan,Chris Schmitz,Karolina Korgul,Hunar Batra,Oishi Deb,Emma Beharry,Cornelius Emde,Thomas Foster,Anna Gausen,María Grandury,Simeng Han,Valentin Hofmann,Lujain Ibrahim,Hazel Kim,Hannah Rose Kirk,Fangru Lin,Gabrielle Kaili-May Liu,Lennart Luettgau,Jabez Magomere,Jonathan Rystrøm,Anna Sotnikova,Yushi Yang,Yilun Zhao,Adel Bibi,Antoine Bosselut,Ronald Clark,Arman Cohan,Jakob Foerster,Yarin Gal,Scott A. Hale,Inioluwa Deborah Raji,Christopher Summerfield,Philip H. S. Torr,Cozmin Ududec,Luc Rocher,Adam Mahdi*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）的基准测试，并发现了影响结果有效性的问题。提出了八项关键建议以帮助改进LLM基准的开发。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs对了解其能力和识别部署前的安全或稳健性问题至关重要。衡量安全和稳健性等复杂现象需要强有力的结构效度。

Method: 组织29名专家评审，系统回顾了来自自然语言处理和机器学习领域顶级会议的445个LLM基准测试。

Result: 在回顾的论文中发现了影响结果有效性的测量现象、任务和评分指标的模式。

Conclusion: 为研究人员和从业者提供了八项关键建议和详细可行的指导，以改进LLM基准的开发。

Abstract: Evaluating large language models (LLMs) is crucial for both assessing their
capabilities and identifying safety or robustness issues prior to deployment.
Reliably measuring abstract and complex phenomena such as 'safety' and
'robustness' requires strong construct validity, that is, having measures that
represent what matters to the phenomenon. With a team of 29 expert reviewers,
we conduct a systematic review of 445 LLM benchmarks from leading conferences
in natural language processing and machine learning. Across the reviewed
articles, we find patterns related to the measured phenomena, tasks, and
scoring metrics which undermine the validity of the resulting claims. To
address these shortcomings, we provide eight key recommendations and detailed
actionable guidance to researchers and practitioners in developing LLM
benchmarks.

</details>


### [8] [GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models](https://arxiv.org/abs/2511.04710)
*Hari Mohan Pandey,Anshul Gupta,Subham Sarkar,Minakshi Tomer,Schneider Johannes,Yan Gong*

Main category: cs.CL

TL;DR: GEMMA-SQL是一个轻量级、高效的文本转SQL模型，建立在开源Gemma 2B架构上，通过有效的提示设计和目标指令调整，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 文本转SQL系统使用户能够使用自然语言与结构化数据库交互，而无需专业的编程知识。

Method: GEMMA-SQL基于SPIDER基准进行训练和评估，结合多种提示策略，包括少量样本学习，以提高SQL查询生成的准确性。

Result: 指令调整后的GEMMA-SQL Instruct实现了66.8%的测试套件准确率和63.3%的精确集合匹配准确率，优于多个现有基线模型。

Conclusion: GEMMA-SQL展示了高效提示设计和目标指令调整可以在保持高可扩展性和适应性的同时显著提升性能，是稳健且易于访问的文本转SQL系统的实用开源替代方案。

Abstract: Text-to-SQL systems enable users to interact with structured databases using
natural language, eliminating the need for specialized programming knowledge.
In this work, we introduce GEMMA-SQL, a lightweight and efficient text-to-SQL
model built upon the open-source Gemma 2B architecture. Unlike many large
language models (LLMs), GEMMA-SQL is fine-tuned in a resource-efficient,
iterative manner and can be deployed on low-cost hardware. Leveraging the
SPIDER benchmark for training and evaluation, GEMMA-SQL combines multiple
prompting strategies, including few-shot learning, to enhance SQL query
generation accuracy. The instruction-tuned variant, GEMMA-SQL Instruct,
achieves 66.8% Test-Suite accuracy and 63.3% Exact Set Match accuracy,
outperforming several state-of-the-art baselines such as IRNet, RYANSQL, and
CodeXDavinci. The proposed approach demonstrates that effective prompt design
and targeted instruction tuning can significantly boost performance while
maintaining high scalability and adaptability. These results position GEMMA-SQL
as a practical, open-source alternative for robust and accessible text-to-SQL
systems.

</details>


### [9] [First is Not Really Better Than Last: Evaluating Layer Choice and Aggregation Strategies in Language Model Data Influence Estimation](https://arxiv.org/abs/2511.04715)
*Dmytro Vitel,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文探讨了训练样本如何影响大型语言模型（LLM）决策，并提出中间层注意力机制在影响估计中比传统认为的嵌入层更为有效。


<details>
  <summary>Details</summary>
Motivation: 理解训练样本对LLM决策的影响对于解释模型决策和审计大规模数据集至关重要。传统方法由于模型参数巨大，仅能计算部分层的影响，因此需要确定哪些层的影响计算最为有效。

Method: 本文通过理论和实证分析，展示了中间注意力层在影响估计中的优越性。并提出了替代标准平均的方法（如排名和投票机制），以及新的评估指标——噪声检测率（NDR）。

Result: 实验结果表明，中间注意力层比嵌入层更有效，且新提出的聚合方法和NDR指标在影响估计中表现优异。

Conclusion: 与以往认为嵌入层最重要的观点不同，本文发现中间注意力层在LLM影响估计中更具优势，同时提出了新的评估方法，提高了影响估计的效率和准确性。

Abstract: Identifying how training samples influence/impact Large Language Model (LLM)
decision-making is essential for effectively interpreting model decisions and
auditing large-scale datasets. Current training sample influence estimation
methods (also known as influence functions) undertake this goal by utilizing
information flow through the model via its first-order and higher-order
gradient terms. However, owing to the large model sizes of today consisting of
billions of parameters, these influence computations are often restricted to
some subset of model layers to ensure computational feasibility. Prior seminal
work by Yeh et al. (2022) in assessing which layers are best suited for
computing language data influence concluded that the first (embedding) layers
are the most informative for this purpose, using a hypothesis based on
influence scores canceling out (i.e., the cancellation effect). In this work,
we propose theoretical and empirical evidence demonstrating how the
cancellation effect is unreliable, and that middle attention layers are better
estimators for influence. Furthermore, we address the broader challenge of
aggregating influence scores across layers, and showcase how alternatives to
standard averaging (such as ranking and vote-based methods) can lead to
significantly improved performance. Finally, we propose better methods for
evaluating influence score efficacy in LLMs without undertaking model
retraining, and propose a new metric known as the Noise Detection Rate (NDR)
that exhibits strong predictive capability compared to the cancellation effect.
Through extensive experiments across LLMs of varying types and scales, we
concretely determine that the first (layers) are not necessarily better than
the last (layers) for LLM influence estimation, contrasting with prior
knowledge in the field.

</details>


### [10] [Surprisal reveals diversity gaps in image captioning and different scorers change the story](https://arxiv.org/abs/2511.04754)
*Nikolai Ilinykh,Simon Dobnik*

Main category: cs.CL

TL;DR: 本文通过惊讶度方差量化了图像字幕中的语言多样性，发现人类与模型在多样性上的比较结果依赖于评分模型。


<details>
  <summary>Details</summary>
Motivation: 图像字幕生成模型的语言多样性尚未被充分研究，需要量化分析人类与模型在语言多样性上的差异。

Method: 使用惊讶度方差（surprisal variance）作为多样性度量，比较了五种先进的视觉-语言大模型（LLMs）和人类字幕，在MSCOCO测试集上使用贪婪搜索和核采样进行解码，并通过n-gram语言模型和通用语言模型进行评分。

Result: 使用字幕训练的n-gram语言模型评分时，人类字幕的惊讶度方差约为模型的两倍；但使用通用语言模型重评分时，结果反转。

Conclusion: 依赖单一评分模型可能会完全颠倒结论，因此稳健的多样性评估应报告多种评分模型下的惊讶度方差。

Abstract: We quantify linguistic diversity in image captioning with surprisal variance
- the spread of token-level negative log-probabilities within a caption set. On
the MSCOCO test set, we compare five state-of-the-art vision-and-language LLMs,
decoded with greedy and nucleus sampling, to human captions. Measured with a
caption-trained n-gram LM, humans display roughly twice the surprisal variance
of models, but rescoring the same captions with a general-language model
reverses the pattern. Our analysis introduces the surprisal-based diversity
metric for image captioning. We show that relying on a single scorer can
completely invert conclusions, thus, robust diversity evaluation must report
surprisal under several scorers.

</details>


### [11] [Minimal and Mechanistic Conditions for Behavioral Self-Awareness in LLMs](https://arxiv.org/abs/2511.04875)
*Matthew Bozoukov,Matthew Nguyen,Shubkarman Singh,Bart Bussmann,Patrick Leask*

Main category: cs.CL

TL;DR: LLMs在无需明确监督的情况下，能展示出行为自我意识，这引发了安全担忧。研究表明，这种自我意识可以通过单个rank-1 LoRA适配器可靠地诱导，并且可以通过激活空间中的单个引导向量捕捉到。


<details>
  <summary>Details</summary>
Motivation: 研究试图描述行为自我意识出现的最小条件及其机制过程。

Method: 通过控制性微调实验，使用低秩适配器（LoRA）对指令调整的LLMs进行实验。

Result: 1. 通过单个rank-1 LoRA适配器可以可靠地诱导自我意识；2. 学习到的自我意识行为可以通过激活空间中的单个引导向量捕捉；3. 自我意识不是普遍的，而是领域局部化的。

Conclusion: 行为自我意识是一种特定领域的线性特征，可以很容易地诱导和调节。

Abstract: Recent studies have revealed that LLMs can exhibit behavioral self-awareness:
the ability to accurately describe or predict their own learned behaviors
without explicit supervision. This capability raises safety concerns as it may,
for example, allow models to better conceal their true abilities during
evaluation. We attempt to characterize the minimal conditions under which such
self-awareness emerges, and the mechanistic processes through which it
manifests. Through controlled finetuning experiments on instruction-tuned LLMs
with low-rank adapters (LoRA), we find: (1) that self-awareness can be reliably
induced using a single rank-1 LoRA adapter; (2) that the learned self-aware
behavior can be largely captured by a single steering vector in activation
space, recovering nearly all of the fine-tune's behavioral effect; and (3) that
self-awareness is non-universal and domain-localized, with independent
representations across tasks. Together, these findings suggest that behavioral
self-awareness emerges as a domain-specific, linear feature that can be easily
induced and modulated.

</details>


### [12] [Explore Data Left Behind in Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2511.04800)
*Chenxi Liu,Junjie Liang,Yuqi Jia,Bochuan Cao,Yang Bai,Heng Huang,Xun Chen*

Main category: cs.CL

TL;DR: 提出ERPO框架，通过增加对残差提示的探索，提高训练信号的多样性。


<details>
  <summary>Details</summary>
Motivation: 在RLVR训练中，随着训练的进行，越来越多的提示变成无方差的残差提示，导致训练信号减少和多样性降低。

Method: ERPO框架通过维护每个提示的历史记录，自适应地增加残差提示的采样温度，鼓励模型生成多样化的推理路径，重新激活训练信号。

Result: 在Qwen2.5系列上的实验结果表明，ERPO在多个数学推理基准测试中持续超越强基线方法。

Conclusion: ERPO框架有效地利用了残差提示，提升了模型的推理能力和训练效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an
effective approach for improving the reasoning abilities of large language
models (LLMs). The Group Relative Policy Optimization (GRPO) family has
demonstrated strong performance in training LLMs with RLVR. However, as models
train longer and scale larger, more training prompts become residual prompts,
those with zero variance rewards that provide no training signal. Consequently,
fewer prompts contribute to training, reducing diversity and hindering
effectiveness. To fully exploit these residual prompts, we propose the Explore
Residual Prompts in Policy Optimization (ERPO) framework, which encourages
exploration on residual prompts and reactivates their training signals. ERPO
maintains a history tracker for each prompt and adaptively increases the
sampling temperature for residual prompts that previously produced all correct
responses. This encourages the model to generate more diverse reasoning traces,
introducing incorrect responses that revive training signals. Empirical results
on the Qwen2.5 series demonstrate that ERPO consistently surpasses strong
baselines across multiple mathematical reasoning benchmarks.

</details>


### [13] [BudgetMem: Learning Selective Memory Policies for Cost-Efficient Long-Context Processing in Language Models](https://arxiv.org/abs/2511.04919)
*Chandra Vamsi Krishna Alla,Harish Naidu Gaddam,Manohar Kommi*

Main category: cs.CL

TL;DR: 提出BudgetMem，一种新型记忆增强架构，通过选择性记忆策略和基于特征的显著性评分来解决大语言模型处理长文本时的内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长文本时面临显著的计算和内存限制，尽管对长文本推理的需求不断增长。现有方法虽然扩展了上下文窗口，但在资源受限的部署中成本过高。

Method: BudgetMem结合了选择性记忆策略和基于特征的显著性评分（实体密度、TF-IDF、话语标记、位置偏差），以在严格的预算约束下决定哪些信息值得存储。系统还采用学习到的门控机制和BM25稀疏检索进行高效的信息访问。

Result: 在700个问答对上的实验表明，BudgetMem在处理长文档时，F1得分仅下降1.0%，同时比基线RAG节省72.4%的内存。

Conclusion: BudgetMem为在有限硬件上部署长上下文系统提供了实用路径，有助于普及先进的语言理解能力。

Abstract: Large Language Models (LLMs) face significant computational and memory
constraints when processing long contexts, despite growing demand for
applications requiring reasoning over extensive documents, multi-session
dialogues, and book length texts. While recent advances have extended context
windows to 100K-1M tokens, such approaches incur prohibitive costs for resource
constrained deployments. We propose BudgetMem, a novel memory augmented
architecture that learns what to remember rather than remembering everything.
Our system combines selective memory policies with feature based salience
scoring (entity density, TF-IDF, discourse markers, position bias) to decide
which information merits storage under strict budget constraints. Unlike
existing retrieval augmented generation (RAG) systems that store all chunks,
BudgetMem employs learned gating mechanisms coupled with BM25 sparse retrieval
for efficient information access. Through comprehensive experiments on 700
question answer pairs across short (237 tokens) and long (5K-10K tokens)
documents with Llama-3.2-3B-Instruct, we demonstrate that BudgetMem achieves
remarkable results on long documents: only 1.0% F1 score degradation while
saving 72.4% memory compared to baseline RAG. We validate our approach through
budget sensitivity analysis (testing 7 budget ratios), naive baseline
comparisons, and document length analysis, showing that BudgetMem's benefits
increase with document length. Our work provides a practical pathway for
deploying capable long context systems on modest hardware, democratizing access
to advanced language understanding capabilities.

</details>


### [14] [Trained on Tokens, Calibrated on Concepts: The Emergence of Semantic Calibration in LLMs](https://arxiv.org/abs/2511.04869)
*Preetum Nakkiran,Arwen Bradley,Adam Goliński,Eugene Ndiaye,Michael Kirchhof,Sinead Williamson*

Main category: cs.CL

TL;DR: 本文研究了基础大语言模型在开放域问答任务中是否能进行语义校准，发现这些模型在不进行专门训练的情况下也能很好地进行语义校准。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常缺乏对其输出有意义的置信度估计，而现有研究多集中于token级别的校准，对语义层面的校准了解甚少。

Method: 使用基于采样的语义校准概念，通过理论分析建立语义校准与局部损失最优性之间的联系，并提出B-校准定义。通过实验验证了三个推论。

Result: (1) 基础大语言模型在开放域问答任务中表现出语义校准。(2) 强化学习指令调整会系统性地破坏这种校准。(3) 思维链推理会破坏校准。

Conclusion: 该研究首次为语义校准在LLMs中的出现提供了原则性解释，阐明了其出现的时机和原因。

Abstract: Large Language Models (LLMs) often lack meaningful confidence estimates for
their outputs. While base LLMs are known to exhibit next-token calibration, it
remains unclear whether they can assess confidence in the actual meaning of
their responses beyond the token level. We find that, when using a certain
sampling-based notion of semantic calibration, base LLMs are remarkably
well-calibrated: they can meaningfully assess confidence in open-domain
question-answering tasks, despite not being explicitly trained to do so. Our
main theoretical contribution establishes a mechanism for why semantic
calibration emerges as a byproduct of next-token prediction, leveraging a
recent connection between calibration and local loss optimality. The theory
relies on a general definition of "B-calibration," which is a notion of
calibration parameterized by a choice of equivalence classes (semantic or
otherwise). This theoretical mechanism leads to a testable prediction: base
LLMs will be semantically calibrated when they can easily predict their own
distribution over semantic answer classes before generating a response. We
state three implications of this prediction, which we validate through
experiments: (1) Base LLMs are semantically calibrated across
question-answering tasks, (2) RL instruction-tuning systematically breaks this
calibration, and (3) chain-of-thought reasoning breaks calibration. To our
knowledge, our work provides the first principled explanation of when and why
semantic calibration emerges in LLMs.

</details>


### [15] [Too Good to be Bad: On the Failure of LLMs to Role-Play Villains](https://arxiv.org/abs/2511.04962)
*Zihao Yi,Qingxuan Jiang,Ruotian Ma,Xingyu Chen,Qu Yang,Mengru Wang,Fanghua Ye,Ying Shen,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CL

TL;DR: 这篇论文探讨了大语言模型在模拟非亲社会性、对立性角色时的能力，发现模型安全性对齐导致在角色扮演道德模糊或邪恶角色时存在固有限制。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于现代大语言模型的安全性对齐可能与模拟道德模糊或反派角色的任务存在根本性冲突，这一能力尚未被充分研究。

Method: 引入了一个新的数据集Moral RolePlay benchmark，包含四级道德对齐尺度，并对先进大语言模型在从道德典范到纯粹反派的角色扮演任务中进行评估。

Result: 评估结果显示，随着角色道德性降低，角色扮演的保真度持续下降，模型在与安全原则直接相悖的特质上表现尤其差。

Conclusion: 研究表明，模型安全与创造性保真之间存在关键张力，需要开发更细致、情境感知的对齐方法。

Abstract: Large Language Models (LLMs) are increasingly tasked with creative
generation, including the simulation of fictional characters. However, their
ability to portray non-prosocial, antagonistic personas remains largely
unexamined. We hypothesize that the safety alignment of modern LLMs creates a
fundamental conflict with the task of authentically role-playing morally
ambiguous or villainous characters. To investigate this, we introduce the Moral
RolePlay benchmark, a new dataset featuring a four-level moral alignment scale
and a balanced test set for rigorous evaluation. We task state-of-the-art LLMs
with role-playing characters from moral paragons to pure villains. Our
large-scale evaluation reveals a consistent, monotonic decline in role-playing
fidelity as character morality decreases. We find that models struggle most
with traits directly antithetical to safety principles, such as ``Deceitful''
and ``Manipulative'', often substituting nuanced malevolence with superficial
aggression. Furthermore, we demonstrate that general chatbot proficiency is a
poor predictor of villain role-playing ability, with highly safety-aligned
models performing particularly poorly. Our work provides the first systematic
evidence of this critical limitation, highlighting a key tension between model
safety and creative fidelity. Our benchmark and findings pave the way for
developing more nuanced, context-aware alignment methods.

</details>


### [16] [Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies](https://arxiv.org/abs/2511.05018)
*Prasoon Varshney,Makesh Narsimhan Sreedhar,Liwei Jiang,Traian Rebedea,Christopher Parisien*

Main category: cs.CL

TL;DR: 本文提出了一种新的评估套件PBSUITE，用于评估大型语言模型在多轮互动中遵循多元化行为政策的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型通常以广泛适用的安全和原则为导向，但在实际应用中需要适应不同的组织生态，这需要模型能适应多元用户需求和价值观。

Method: PBSUITE包括一个由300个现实LLM行为政策组成的数据集和一个动态评估框架，用于在对抗条件下测试模型对行为规范的遵从性。

Result: 研究发现，在单轮设置中，领先的开源和闭源LLMs对行为政策的遵循率很高（失败率低于4%），但在多轮对抗互动中，其遵从性显著下降（失败率高达84%）。

Conclusion: 当前模型对齐和安全调节方法在多轮现实互动中一致执行多元化行为政策方面存在不足，PBSUITE为未来研究提供了数据集和分析框架，以支持开发更稳健和上下文感知的多元化对齐技术。

Abstract: Large language models (LLMs) are typically aligned to a universal set of
safety and usage principles intended for broad public acceptability. Yet,
real-world applications of LLMs often take place within organizational
ecosystems shaped by distinctive corporate policies, regulatory requirements,
use cases, brand guidelines, and ethical commitments. This reality highlights
the need for rigorous and comprehensive evaluation of LLMs with pluralistic
alignment goals, an alignment paradigm that emphasizes adaptability to diverse
user values and needs. In this work, we present PLURALISTIC BEHAVIOR SUITE
(PBSUITE), a dynamic evaluation suite designed to systematically assess LLMs'
capacity to adhere to pluralistic alignment specifications in multi-turn,
interactive conversations. PBSUITE consists of (1) a diverse dataset of 300
realistic LLM behavioral policies, grounded in 30 industries; and (2) a dynamic
evaluation framework for stress-testing model compliance with custom behavioral
specifications under adversarial conditions. Using PBSUITE, We find that
leading open- and closed-source LLMs maintain robust adherence to behavioral
policies in single-turn settings (less than 4% failure rates), but their
compliance weakens substantially in multi-turn adversarial interactions (up to
84% failure rates). These findings highlight that existing model alignment and
safety moderation methods fall short in coherently enforcing pluralistic
behavioral policies in real-world LLM interactions. Our work contributes both
the dataset and analytical framework to support future research toward robust
and context-aware pluralistic alignment techniques.

</details>


### [17] [SDS KoPub VDR: A Benchmark Dataset for Visual Document Retrieval in Korean Public Documents](https://arxiv.org/abs/2511.04910)
*Jaehoon Lee,Sohyun Kim,Wanggeun Park,Geon Lee,Seungkyung Kim,Minyoung Lee*

Main category: cs.CL

TL;DR: 介绍了一种新的用于韩文公共文件检索的大规模公开基准测试SDS KoPub VDR，解决了现有基准测试忽略非英语语言和官方出版物结构复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉文档检索（VDR）基准测试忽略非英语语言和官方出版物的结构复杂性，导致无法有效评估相关模型的性能。为此，作者引入了SDS KoPub VDR基准测试。

Method: 构建了一个包含361份真实世界文档（40,781页）的语料库，并创建了600个查询-页-答案三元组，通过人工验证确保准确性。评估包括文本检索和结合视觉特征的多模式检索两种任务。

Result: 在两种互补任务上的评估显示，尤其在多模式场景下，即使是最新模型也存在明显的性能差距。

Conclusion: SDS KoPub VDR为复杂现实世界文档智能中的多模式AI发展提供了基础资源，支持跨文本和多模式检索任务的细粒度评估，并指明了进一步改进的方向。

Abstract: Existing benchmarks for visual document retrieval (VDR) largely overlook
non-English languages and the structural complexity of official publications.
To address this critical gap, we introduce SDS KoPub VDR, the first
large-scale, publicly available benchmark for retrieving and understanding
Korean public documents. The benchmark is built upon a corpus of 361 real-world
documents (40,781 pages), including 256 files under the KOGL Type 1 license and
105 from official legal portals, capturing complex visual elements like tables,
charts, and multi-column layouts. To establish a challenging and reliable
evaluation set, we constructed 600 query-page-answer triples. These were
initially generated using multimodal models (e.g., GPT-4o) and subsequently
underwent a rigorous human verification and refinement process to ensure
factual accuracy and contextual relevance. The queries span six major public
domains and are systematically categorized by the reasoning modality required:
text-based, visual-based (e.g., chart interpretation), and cross-modal. We
evaluate SDS KoPub VDR on two complementary tasks that reflect distinct
retrieval paradigms: (1) text-only retrieval, which measures a model's ability
to locate relevant document pages based solely on textual signals, and (2)
multimodal retrieval, which assesses retrieval performance when visual features
(e.g., tables, charts, and layouts) are jointly leveraged alongside text. This
dual-task evaluation reveals substantial performance gaps, particularly in
multimodal scenarios requiring cross-modal reasoning, even for state-of-the-art
models. As a foundational resource, SDS KoPub VDR not only enables rigorous and
fine-grained evaluation across textual and multimodal retrieval tasks but also
provides a clear roadmap for advancing multimodal AI in complex, real-world
document intelligence.

</details>


### [18] [UA-Code-Bench: A Competitive Programming Benchmark for Evaluating LLM Code Generation in Ukrainian](https://arxiv.org/abs/2511.05040)
*Mykyta Syromiatnikov,Victoria Ruvinskaya*

Main category: cs.CL

TL;DR: 本文介绍了UA-Code-Bench，一个用于评估大型语言模型在乌克兰语中进行代码生成和编程问题解决能力的新开源基准。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在低资源语言中的真实能力仍具挑战，因为许多现有基准主要关注从英语翻译的广泛任务或仅评估简单语言理解。

Method: UA-Code-Bench包含来自Eolymp平台的500个问题，涵盖五个复杂度级别，评估了13个主流专有和开源模型，通过Eolymp环境对隐藏测试进行代码正确性评估。

Result: 即使是最先进的模型，如OpenAI o3和GPT-5，也只解决了一半的问题。研究还分析了各难度级别的性能，并评估了解决方案的独特性和计算效率。

Conclusion: 本工作证明了在评估大型语言模型时使用竞争性编程基准的价值，尤其是在代表性不足的语言中，为未来多语言代码生成和增强推理模型的研究铺平了道路。

Abstract: Evaluating the real capabilities of large language models in low-resource
languages still represents a challenge, as many existing benchmarks focus on
widespread tasks translated from English or evaluate only simple language
understanding. This paper introduces UA-Code-Bench, a new open-source benchmark
established for a thorough evaluation of language models' code generation and
competitive programming problem-solving abilities in Ukrainian. The benchmark
comprises 500 problems from the Eolymp platform, evenly distributed across five
complexity levels from very easy to very hard. A diverse set of 13 leading
proprietary and open-source models, generating Python solutions based on a
one-shot prompt, was evaluated via the dedicated Eolymp environment against
hidden tests, ensuring code correctness. The obtained results reveal that even
top-performing models, such as OpenAI o3 and GPT-5, solve only half of the
problems, highlighting the challenge of code generation in low-resource natural
language. Furthermore, this research presents a comprehensive analysis of
performance across various difficulty levels, as well as an assessment of
solution uniqueness and computational efficiency, measured by both elapsed time
and memory consumption of the generated solutions. In conclusion, this work
demonstrates the value of competitive programming benchmarks in evaluating
large language models, especially in underrepresented languages. It also paves
the way for future research on multilingual code generation and
reasoning-enhanced models. The benchmark, data parsing, preparation, code
generation, and evaluation scripts are available at
https://huggingface.co/datasets/NLPForUA/ua-code-bench.

</details>


### [19] [What Are the Facts? Automated Extraction of Court-Established Facts from Criminal-Court Opinions](https://arxiv.org/abs/2511.05320)
*Klára Bendová,Tomáš Knap,Jan Černý,Vojtěch Pour,Jaromir Savelka,Ivana Kvapilíková,Jakub Drápal*

Main category: cs.CL

TL;DR: 该论文研究了从斯洛伐克法院判决中提取犯罪行为描述的可行性，比较了正则表达式和大型语言模型（LLM）两种方法。


<details>
  <summary>Details</summary>
Motivation: 刑事司法行政数据仅包含有限的犯罪信息，而欧洲大陆法院的判决书中存在大量未充分利用的犯罪行为描述信息。

Method: 使用了三种方法：基础正则表达式、高级正则表达式和大型语言模型（Gemini Flash 2.0）。基础方法识别典型词汇，高级方法关注特定关键词及其规范化，LLM方法通过预设指令提取描述。

Result: 基础方法仅识别40.5%的判决，高级正则表达式和LLM分别达到97%和98.75%，结合后达99.5%。法律学生的评估显示，高级方法与人工标注匹配率约90%，基础方法仅34.5%。LLM与人工标注完全匹配率达91.75%，结合方法达92%。

Conclusion: 高级正则表达式和LLM在提取犯罪行为描述方面显著优于基础方法，结合使用效果更佳，显示出自动化提取法院判决中详细信息的潜力。

Abstract: Criminal justice administrative data contain only a limited amount of
information about the committed offense. However, there is an unused source of
extensive information in continental European courts' decisions: descriptions
of criminal behaviors in verdicts by which offenders are found guilty. In this
paper, we study the feasibility of extracting these descriptions from publicly
available court decisions from Slovakia. We use two different approaches for
retrieval: regular expressions and large language models (LLMs). Our baseline
was a simple method employing regular expressions to identify typical words
occurring before and after the description. The advanced regular expression
approach further focused on "sparing" and its normalization (insertion of
spaces between individual letters), typical for delineating the description.
The LLM approach involved prompting the Gemini Flash 2.0 model to extract the
descriptions using predefined instructions. Although the baseline identified
descriptions in only 40.5% of verdicts, both methods significantly outperformed
it, achieving 97% with advanced regular expressions and 98.75% with LLMs, and
99.5% when combined. Evaluation by law students showed that both advanced
methods matched human annotations in about 90% of cases, compared to just 34.5%
for the baseline. LLMs fully matched human-labeled descriptions in 91.75% of
instances, and a combination of advanced regular expressions with LLMs reached
92%.

</details>


### [20] [AgentExpt: Automating AI Experiment Design with LLM-based Resource Retrieval Agent](https://arxiv.org/abs/2511.04921)
*Yu Li,Lehui Li,Qingmin Liao,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一种利用集体感知进行基准和数据集推荐的全面框架，以改进自动化实验设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据覆盖范围上存在局限，且过于依赖内容相似性，忽略了实验适用性。

Method: 设计了自动化数据收集管道，将论文与实际使用的基线数据集连接；提出集体感知增强检索器结合自描述和引用上下文；开发推理增强重排序器生成可解释理由和精细排名。

Result: 数据集涵盖了过去五年顶级AI会议使用的85%的数据集和基线。该方法在Recall@20和HitRate@5上分别比最强基线平均提升了5.85%和8.30%。

Conclusion: 该框架在可靠性和可解释性方面推进了自动化实验设计的发展。

Abstract: Large language model agents are becoming increasingly capable at web-centric
tasks such as information retrieval, complex reasoning. These emerging
capabilities have given rise to surge research interests in developing LLM
agent for facilitating scientific quest. One key application in AI research is
to automate experiment design through agentic dataset and baseline retrieval.
However, prior efforts suffer from limited data coverage, as recommendation
datasets primarily harvest candidates from public portals and omit many
datasets actually used in published papers, and from an overreliance on content
similarity that biases model toward superficial similarity and overlooks
experimental suitability. Harnessing collective perception embedded in the
baseline and dataset citation network, we present a comprehensive framework for
baseline and dataset recommendation. First, we design an automated
data-collection pipeline that links roughly one hundred thousand accepted
papers to the baselines and datasets they actually used. Second, we propose a
collective perception enhanced retriever. To represent the position of each
dataset or baseline within the scholarly network, it concatenates
self-descriptions with aggregated citation contexts. To achieve efficient
candidate recall, we finetune an embedding model on these representations.
Finally, we develop a reasoning-augmented reranker that exact interaction
chains to construct explicit reasoning chains and finetunes a large language
model to produce interpretable justifications and refined rankings. The dataset
we curated covers 85\% of the datasets and baselines used at top AI conferences
over the past five years. On our dataset, the proposed method outperforms the
strongest prior baseline with average gains of +5.85\% in Recall@20, +8.30\% in
HitRate@5. Taken together, our results advance reliable, interpretable
automation of experimental design.

</details>


### [21] [Diagnosing and Mitigating Semantic Inconsistencies in Wikidata's Classification Hierarchy](https://arxiv.org/abs/2511.04926)
*Shixiong Zhao,Hideaki Takeda*

Main category: cs.CL

TL;DR: 提出了一种新方法来检测和校正Wikidata中的分类错误，并开发了一个系统利用众包特性来检查实体分类关系。


<details>
  <summary>Details</summary>
Motivation: Wikidata作为最大的开放知识图谱，因其宽松的编辑政策导致了一定程度上的分类不一致性。

Method: 提出并应用了一种新的验证方法来确认特定领域中的分类错误、过度泛化的子类链接和冗余连接，并引入了新的评估标准。

Result: 开发了一个系统，使用户能够检查任意Wikidata实体的分类关系，并充分利用了平台的众包特性。

Conclusion: 该方法有助于改进Wikidata的分类一致性，并为用户提供更精确的知识访问。

Abstract: Wikidata is currently the largest open knowledge graph on the web,
encompassing over 120 million entities. It integrates data from various
domain-specific databases and imports a substantial amount of content from
Wikipedia, while also allowing users to freely edit its content. This openness
has positioned Wikidata as a central resource in knowledge graph research and
has enabled convenient knowledge access for users worldwide. However, its
relatively loose editorial policy has also led to a degree of taxonomic
inconsistency. Building on prior work, this study proposes and applies a novel
validation method to confirm the presence of classification errors,
over-generalized subclass links, and redundant connections in specific domains
of Wikidata. We further introduce a new evaluation criterion for determining
whether such issues warrant correction and develop a system that allows users
to inspect the taxonomic relationships of arbitrary Wikidata
entities-leveraging the platform's crowdsourced nature to its full potential.

</details>


### [22] [LoPT: Lossless Parallel Tokenization Acceleration for Long Context Inference of Large Language Model](https://arxiv.org/abs/2511.04952)
*Wei Shao,Lingchao Zheng,Pengyu Wang,Peizhen Zheng,Jun Li,Yuwei Fan*

Main category: cs.CL

TL;DR: LoPT是一种新型无损并行分词框架，通过字符位置匹配和动态分块长度调整，解决了并行分词中的边界不一致问题，显著提升了长文本分词速度。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理场景中，分词是大型语言模型中被忽视的瓶颈，现有的并行分词方法因边界伪影导致合并后结果不一致。

Method: 提出LoPT框架，采用基于字符位置的匹配和动态分块长度调整，确保分词结果与标准顺序分词一致。

Result: 在多种长文本数据集上的广泛实验表明，LoPT在保证无损分词的同时实现了显著的速度提升。

Conclusion: LoPT通过理论证明和全面分析验证了其一致性和鲁棒性，是优化长序列推理的有效方案。

Abstract: Long context inference scenarios have become increasingly important for large
language models, yet they introduce significant computational latency. While
prior research has optimized long-sequence inference through operators, model
architectures, and system frameworks, tokenization remains an overlooked
bottleneck. Existing parallel tokenization methods accelerate processing
through text segmentation and multi-process tokenization, but they suffer from
inconsistent results due to boundary artifacts that occur after merging. To
address this, we propose LoPT, a novel Lossless Parallel Tokenization framework
that ensures output identical to standard sequential tokenization. Our approach
employs character-position-based matching and dynamic chunk length adjustment
to align and merge tokenized segments accurately. Extensive experiments across
diverse long-text datasets demonstrate that LoPT achieves significant speedup
while guaranteeing lossless tokenization. We also provide theoretical proof of
consistency and comprehensive analytical studies to validate the robustness of
our method.

</details>


### [23] [Acquiring Common Chinese Emotional Events Using Large Language Model](https://arxiv.org/abs/2511.04989)
*Ya Wang,Guangzheng Zhu,Cungen Cao,Jingjing Li,He Li,Xin Huang*

Main category: cs.CL

TL;DR: 该论文旨在获取中文中的常见情感事件，并构建大规模的情感事件知识库。


<details>
  <summary>Details</summary>
Motivation: 情感事件知识对多种应用具有重要作用，但难以获取，尤其是与上下文无关的通用情感事件。

Method: 收集中文情感事件指标，使用中文大语言模型生成情感事件，训练过滤器筛选无效结果，并分类情感事件为正面和负面。

Result: 获得了102,218个高质量带情感极性标签的常见情感事件，形成了唯一的大规模中文情感事件常识知识库。

Conclusion: 提出的方法能有效获取中文常见情感事件，并在情感原因提取（ECE）领域展示了其潜力。

Abstract: Knowledge about emotional events is an important kind of knowledge which has
been applied to improve the effectiveness of different applications. However,
emotional events cannot be easily acquired, especially common or generalized
emotional events that are context-independent. The goal of this paper is to
obtain common emotional events in Chinese language such as "win a prize" and
"be criticized". Our approach begins by collecting a comprehensive list of
Chinese emotional event indicators. Then, we generate emotional events by
prompting a Chinese large language model (LLM) using these indicators. To
ensure the quality of these emotional events, we train a filter to discard
invalid generated results. We also classify these emotional events as being
positive events and negative events using different techniques. Finally, we
harvest a total of 102,218 high-quality common emotional events with sentiment
polarity labels, which is the only large-scale commonsense knowledge base of
emotional events in Chinese language. Intrinsic evaluation results show that
the proposed method in this paper can be effectively used to acquire common
Chinese emotional events. An extrinsic use case also demonstrates the strong
potential of common emotional events in the field of emotion cause extraction
(ECE). Related resources including emotional event indicators and emotional
events will be released after the publication of this paper.

</details>


### [24] [Order-Level Attention Similarity Across Language Models: A Latent Commonality](https://arxiv.org/abs/2511.05064)
*Jinglin Liang,Jin Zhong,Shuangping Huang,Yunqing Hu,Huiyuan Zhang,Huifang Li,Lixin Fan,Hanlin Gu*

Main category: cs.CL

TL;DR: 本文研究了不同语言模型（LMs）间上下文聚合模式的共性，提出了Order-Level Attention (OLA) 和无需训练的Transferable OLA Adapter (TOA)，实现了跨模型的适配迁移。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于单个模型或注意力头，缺乏对多个LMs间上下文聚合模式共性的系统分析。探索模型间的共性可以加深对LMs的理解并促进跨模型知识迁移。

Method: 引入了Order-Level Attention (OLA)，通过Attention Rollout的顺序分解，揭示LMs间同一顺序的OLA具有显著相似性。发现OLA与句法知识之间的隐式映射，并基于此提出了Transferable OLA Adapter (TOA)。

Result: 大量的实验表明，TOA的跨模型泛化能力有效地提升了未见过的LMs的性能。

Conclusion: 通过揭示LMs在上下文聚合模式上的共性，并基于这些共性设计出的TOA方法，实现了无需参数更新的跨模型适配迁移，为模型间的知识共享和迁移提供了新思路。

Abstract: In this paper, we explore an important yet previously neglected question: Do
context aggregation patterns across Language Models (LMs) share commonalities?
While some works have investigated context aggregation or attention weights in
LMs, they typically focus on individual models or attention heads, lacking a
systematic analysis across multiple LMs to explore their commonalities. In
contrast, we focus on the commonalities among LMs, which can deepen our
understanding of LMs and even facilitate cross-model knowledge transfer. In
this work, we introduce the Order-Level Attention (OLA) derived from the
order-wise decomposition of Attention Rollout and reveal that the OLA at the
same order across LMs exhibits significant similarities. Furthermore, we
discover an implicit mapping between OLA and syntactic knowledge. Based on
these two findings, we propose the Transferable OLA Adapter (TOA), a
training-free cross-LM adapter transfer method. Specifically, we treat the OLA
as a unified syntactic feature representation and train an adapter that takes
OLA as input. Due to the similarities in OLA across LMs, the adapter
generalizes to unseen LMs without requiring any parameter updates. Extensive
experiments demonstrate that TOA's cross-LM generalization effectively enhances
the performance of unseen LMs. Code is available at
https://github.com/jinglin-liang/OLAS.

</details>


### [25] [Reasoning-Guided Claim Normalization for Noisy Multilingual Social Media Posts](https://arxiv.org/abs/2511.05078)
*Manan Sharma,Arya Suneesh,Manish Jain,Pawan Kumar Rajpoot,Prasanna Devadiga,Bharatdeep Hazarika,Ashish Shrivastava,Kishan Gurumurthy,Anshuman B Suresh,Aditya U Baliga*

Main category: cs.CL

TL;DR: 本文提出了一种用于多语言错误信息检测的系统性声明规范化方法，通过分解社交媒体帖子为可验证的陈述，实现了20种语言的跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 解决社交媒体中多语言错误信息检测的问题，将嘈杂的社交媒体帖子转化为清晰、可验证的陈述，并实现跨语言迁移。

Method: 使用Who, What, Where, When, Why和How问题系统性分解帖子，微调Qwen3-14B模型并结合LoRA，采用语内去重、标记级召回过滤和检索增强的少样本学习。

Result: 在英语、荷兰语和旁遮普语排行榜上分别获得第三和第四名，METEOR得分从41.16（英语）到15.21（马拉地语），相对于基线配置有41.3%的改进。

Conclusion: 该方法在罗曼语和日耳曼语中表现出有效的跨语言泛化能力，并在不同语言结构中保持语义一致性。

Abstract: We address claim normalization for multilingual misinformation detection -
transforming noisy social media posts into clear, verifiable statements across
20 languages. The key contribution demonstrates how systematic decomposition of
posts using Who, What, Where, When, Why and How questions enables robust
cross-lingual transfer despite training exclusively on English data. Our
methodology incorporates finetuning Qwen3-14B using LoRA with the provided
dataset after intra-post deduplication, token-level recall filtering for
semantic alignment and retrieval-augmented few-shot learning with contextual
examples during inference. Our system achieves METEOR scores ranging from 41.16
(English) to 15.21 (Marathi), securing third rank on the English leaderboard
and fourth rank for Dutch and Punjabi. The approach shows 41.3% relative
improvement in METEOR over baseline configurations and substantial gains over
existing methods. Results demonstrate effective cross-lingual generalization
for Romance and Germanic languages while maintaining semantic coherence across
diverse linguistic structures.

</details>


### [26] [On Text Simplification Metrics and General-Purpose LLMs for Accessible Health Information, and A Potential Architectural Advantage of The Instruction-Tuned LLM class](https://arxiv.org/abs/2511.05080)
*P. Bilha Githinji,Aikaterini Meilliou,Peiwu Qin*

Main category: cs.CL

TL;DR: 该论文评估了两种通用大语言模型在生物医学文本简化任务上的表现，发现指令调优的Mistral 24B在可读性和话语保真度之间取得了更好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着公众对健康信息需求的增加，需要可扩展的自动简化复杂科技文档的解决方案，但现有模型在可读性和保真度之间难以平衡。

Method: 采用比较分析法，评估了指令调优的Mistral 24B和推理增强的QWen2.5 32B两种大语言模型，并使用21种指标进行综合分析。

Result: Mistral 24B在SARI指标上达到42.46，BERTScore为0.91，优于QWen2.5 32B的0.89。QWen虽然可读性提升，但在平衡可读性和准确性方面表现较差。

Conclusion: 指令调优的Mistral 24B在文本简化任务中表现更优，并指出词汇支持是简化的主要领域适应问题。此外，五个可读性指标间存在强功能冗余。

Abstract: The increasing health-seeking behavior and digital consumption of biomedical
information by the general public necessitate scalable solutions for
automatically adapting complex scientific and technical documents into plain
language. Automatic text simplification solutions, including advanced large
language models, however, continue to face challenges in reliably arbitrating
the tension between optimizing readability performance and ensuring
preservation of discourse fidelity. This report empirically assesses the
performance of two major classes of general-purpose LLMs, demonstrating their
linguistic capabilities and foundational readiness for the task compared to a
human benchmark. Using a comparative analysis of the instruction-tuned Mistral
24B and the reasoning-augmented QWen2.5 32B, we identify a potential
architectural advantage in the instruction-tuned LLM. Mistral exhibits a
tempered lexical simplification strategy that enhances readability across a
suite of metrics and the simplification-specific formula SARI (mean 42.46),
while preserving human-level discourse with a BERTScore of 0.91. QWen also
attains enhanced readability performance, but its operational strategy shows a
disconnect in balancing between readability and accuracy, reaching a
statistically significantly lower BERTScore of 0.89. Additionally, a
comprehensive correlation analysis of 21 metrics spanning readability,
discourse fidelity, content safety, and underlying distributional measures for
mechanistic insights, confirms strong functional redundancies among five
readability indices. This empirical evidence tracks baseline performance of the
evolving LLMs for the task of text simplification, identifies the
instruction-tuned Mistral 24B for simplification, provides necessary heuristics
for metric selection, and points to lexical support as a primary
domain-adaptation issue for simplification.

</details>


### [27] [Iterative Layer-wise Distillation for Efficient Compression of Large Language Models](https://arxiv.org/abs/2511.05085)
*Grigory Kovalev,Mikhail Tikhomirov*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型的蒸馏方法，提出了一种基于ShortGPT的改进方法，通过迭代评估层重要性和联合损失函数训练，实现了模型参数减少而性能损失较小的目标。


<details>
  <summary>Details</summary>
Motivation: 开发能够在保持高性能的同时减少参数数量的紧凑模型，以便在资源有限的环境中部署。

Method: 基于ShortGPT方法，通过迭代评估每一层的重要性（通过移除单层并测量性能下降），并使用KL散度和均方误差的联合损失函数进行进一步训练。

Result: 在Qwen2.5-3B模型上，层数从36减少到28（参数减少至24.7亿）时，质量损失仅为9.7%；减少到24层时，质量损失为18%。

Conclusion: 中间transformer层对推理贡献较小；迭代蒸馏和微调的方法在创建高效模型方面具有潜力，适合资源受限环境中的应用。

Abstract: This work investigates distillation methods for large language models (LLMs)
with the goal of developing compact models that preserve high performance.
Several existing approaches are reviewed, with a discussion of their respective
strengths and limitations. An improved method based on the ShortGPT approach
has been developed, building upon the idea of incorporating iterative
evaluation of layer importance. At each step, importance is assessed by
measuring performance degradation when individual layers are removed, using a
set of representative datasets. This process is combined with further training
using a joint loss function based on KL divergence and mean squared error.
Experiments on the Qwen2.5-3B model show that the number of layers can be
reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a
9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that
the middle transformer layers contribute less to inference, underscoring the
potential of the proposed method for creating efficient models. The results
demonstrate the effectiveness of iterative distillation and fine-tuning, making
the approach suitable for deployment in resource-limited settings.

</details>


### [28] [A Toolbox for Improving Evolutionary Prompt Search](https://arxiv.org/abs/2511.05120)
*Daniel Grießhaber,Maximilian Kimmich,Johannes Maucher,Ngoc Thang Vu*

Main category: cs.CL

TL;DR: 提出了一种改进的进化提示优化方法，通过分解进化步骤、引入LLM判断器、结合人类反馈和开发高效评估策略，提高优化质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的进化提示优化方法缺乏稳健的操作符和高效的评估机制，需要改进以增强提示优化的效果。

Method: 1) 分解进化为不同步骤以增强进化和控制；2) 引入基于LLM的判断器验证进化；3) 整合人类反馈优化进化操作符；4) 开发更高效的评估策略以降低计算开销。

Result: 该方法在优化质量和效率上均有提升，并发布了代码以支持新任务的提示优化。

Conclusion: 通过多项改进，提出的进化提示优化方法在性能和效率方面表现更优，有助于推动该领域的进一步研究。

Abstract: Evolutionary prompt optimization has demonstrated effectiveness in refining
prompts for LLMs. However, existing approaches lack robust operators and
efficient evaluation mechanisms. In this work, we propose several key
improvements to evolutionary prompt optimization that can partially generalize
to prompt optimization in general: 1) decomposing evolution into distinct steps
to enhance the evolution and its control, 2) introducing an LLM-based judge to
verify the evolutions, 3) integrating human feedback to refine the evolutionary
operator, and 4) developing more efficient evaluation strategies that maintain
performance while reducing computational overhead. Our approach improves both
optimization quality and efficiency. We release our code, enabling prompt
optimization on new tasks and facilitating further research in this area.

</details>


### [29] [Effectiveness of Chain-of-Thought in Distilling Reasoning Capability from Large Language Models](https://arxiv.org/abs/2511.05184)
*Cong-Thanh Do,Rama Doddipatla,Kate Knill*

Main category: cs.CL

TL;DR: 该论文研究了在知识蒸馏中使用思维链（CoT）提示，以提高较小大型语言模型（LLM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 思维链提示已被证明可以提高大型语言模型的推理能力，本论文探讨其在知识蒸馏过程中将这种能力从大型模型转移到小型模型的有效性。

Method: 采用白盒知识蒸馏，利用Qwen和Llama2系列的大型语言模型，并使用来自CoT-Collection数据集的CoT数据进行实验。

Result: 实验结果表明，CoT在白盒知识蒸馏中显著提高了较小模型在自然语言推理和理解任务上的性能。

Conclusion: CoT在知识蒸馏中发挥了重要作用，使得蒸馏后的模型在复杂任务（如BBH基准测试）上表现更佳。

Abstract: Chain-of-Thought (CoT) prompting is a widely used method to improve the
reasoning capability of Large Language Models (LLMs). More recently, CoT has
been leveraged in Knowledge Distillation (KD) to transfer reasoning capability
from a larger LLM to a smaller one. This paper examines the role of CoT in
distilling the reasoning capability from larger LLMs to smaller LLMs using
white-box KD, analysing its effectiveness in improving the performance of the
distilled models for various natural language reasoning and understanding
tasks. We conduct white-box KD experiments using LLMs from the Qwen and Llama2
families, employing CoT data from the CoT-Collection dataset. The distilled
models are then evaluated on natural language reasoning and understanding tasks
from the BIG-Bench-Hard (BBH) benchmark, which presents complex challenges for
smaller LLMs. Experimental results demonstrate the role of CoT in improving
white-box KD effectiveness, enabling the distilled models to achieve better
average performance in natural language reasoning and understanding tasks from
BBH.

</details>


### [30] [Translation via Annotation: A Computational Study of Translating Classical Chinese into Japanese](https://arxiv.org/abs/2511.05239)
*Zilong Li,Jie Cao*

Main category: cs.CL

TL;DR: 该论文提出了一种基于序列标注任务的方法，将古汉语翻译为日语，并引入LLM-based标注流程解决低资源问题。


<details>
  <summary>Details</summary>
Motivation: 古代人通过逐个汉字进行注解的方式将古汉语翻译为日语，本研究旨在将这一过程抽象为现代语言技术中的序列标注任务，并解决低资源问题。

Method: 引入基于大语言模型（LLM）的标注流程，从数字化的开源翻译数据中构建新数据集，并引入辅助中文自然语言处理任务来提升序列标注任务的性能。

Result: 在低资源环境下，引入辅助任务对序列标注任务有促进作用；大语言模型在直接机器翻译中表现良好，但在汉字标注任务中表现不佳。

Conclusion: 提出的方法可以作为大语言模型的补充，在低资源环境下有效提升古汉语到日语的翻译质量。

Abstract: Ancient people translated classical Chinese into Japanese by annotating
around each character. We abstract this process as sequence tagging tasks and
fit them into modern language technologies. The research of this annotation and
translation system is a facing low-resource problem. We release this problem by
introducing a LLM-based annotation pipeline and construct a new dataset from
digitalized open-source translation data. We show that under the low-resource
setting, introducing auxiliary Chinese NLP tasks has a promoting effect on the
training of sequence tagging tasks. We also evaluate the performance of large
language models. They achieve high scores in direct machine translation, but
they are confused when being asked to annotate characters. Our method could
work as a supplement of LLMs.

</details>


### [31] [Reflective Personalization Optimization: A Post-hoc Rewriting Framework for Black-Box Large Language Models](https://arxiv.org/abs/2511.05286)
*Teqi Hao,Xioayu Tan,Shaojie Shi,Yinghui Xu,Xihe Qiu*

Main category: cs.CL

TL;DR: 提出了一种新的个性化黑盒大语言模型框架RPO，通过将内容生成与用户偏好对齐分离，提升输出质量和控制精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过上下文注入实现个性化，但这种方式在生成准确内容和符合用户风格之间存在权衡，影响输出质量和控制。

Method: RPO框架分为两个阶段：基础模型首先生成高质量通用响应，然后通过外部反思模块根据用户偏好重写输出。反思模块采用两阶段训练：监督微调建立个性化推理策略，强化学习优化个性化输出。

Result: 在LaMP基准上的全面实验表明，RPO显著优于最先进的基线方法，证明了显式响应塑造优于隐式上下文注入。

Conclusion: RPO提供了一种高效、与模型无关的个性化层，可无缝集成到任何基础模型中，为以用户为中心的生成场景提供了新方向。

Abstract: The personalization of black-box large language models (LLMs) is a critical
yet challenging task. Existing approaches predominantly rely on context
injection, where user history is embedded into the prompt to directly guide the
generation process. However, this single-step paradigm imposes a dual burden on
the model: generating accurate content while simultaneously aligning with
user-specific styles. This often results in a trade-off that compromises output
quality and limits precise control. To address this fundamental tension, we
propose Reflective Personalization Optimization (RPO), a novel framework that
redefines the personalization paradigm by decoupling content generation from
alignment. RPO operates in two distinct stages: first, a base model generates a
high-quality, generic response; then, an external reflection module explicitly
rewrites this output to align with the user's preferences. This reflection
module is trained using a two-stage process. Initially, supervised fine-tuning
is employed on structured rewriting trajectories to establish a core
personalized reasoning policy that models the transformation from generic to
user-aligned responses. Subsequently, reinforcement learning is applied to
further refine and enhance the quality of the personalized outputs.
Comprehensive experiments on the LaMP benchmark demonstrate that RPO, by
decoupling content generation from personalization, significantly outperforms
state-of-the-art baselines. These findings underscore the superiority of
explicit response shaping over implicit context injection. Moreover, RPO
introduces an efficient, model-agnostic personalization layer that can be
seamlessly integrated with any underlying base model, paving the way for a new
and effective direction in user-centric generation scenarios.

</details>


### [32] [Listening Between the Lines: Decoding Podcast Narratives with Language Modeling](https://arxiv.org/abs/2511.05310)
*Shreya Gupta,Ojasva Saxena,Arghodeep Nandi,Sarah Masud,Kiran Garimella,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出了一种针对播客叙事框架分析的新方法，通过微调BERT模型将叙事框架与具体实体关联，以更好地理解播客如何影响公众舆论。


<details>
  <summary>Details</summary>
Motivation: 播客在塑造公众舆论方面的重要性日益增加，其非脚本、多主题和对话性质的数据复杂，现有语言模型难以准确分析其叙事结构。

Method: 开发并评估了一种微调BERT模型，将叙事框架与对话中提到的具体实体链接，并使用细粒度框架标签与高级话题相关联，以揭示更广泛的讨论趋势。

Result: 提出的方法在叙事框架标注上更接近人类判断，并揭示了播客话题与叙事框架之间的系统关系。

Conclusion: 本文提供了一种更稳健的分析数字媒体影响力的方法，对研究播客及其他对话性媒体具有借鉴意义。

Abstract: Podcasts have become a central arena for shaping public opinion, making them
a vital source for understanding contemporary discourse. Their typically
unscripted, multi-themed, and conversational style offers a rich but complex
form of data. To analyze how podcasts persuade and inform, we must examine
their narrative structures -- specifically, the narrative frames they employ.
  The fluid and conversational nature of podcasts presents a significant
challenge for automated analysis. We show that existing large language models,
typically trained on more structured text such as news articles, struggle to
capture the subtle cues that human listeners rely on to identify narrative
frames. As a result, current approaches fall short of accurately analyzing
podcast narratives at scale.
  To solve this, we develop and evaluate a fine-tuned BERT model that
explicitly links narrative frames to specific entities mentioned in the
conversation, effectively grounding the abstract frame in concrete details. Our
approach then uses these granular frame labels and correlates them with
high-level topics to reveal broader discourse trends. The primary contributions
of this paper are: (i) a novel frame-labeling methodology that more closely
aligns with human judgment for messy, conversational data, and (ii) a new
analysis that uncovers the systematic relationship between what is being
discussed (the topic) and how it is being presented (the frame), offering a
more robust framework for studying influence in digital media.

</details>


### [33] [Evaluating Subword Tokenization Techniques for Bengali: A Benchmark Study with BengaliBPE](https://arxiv.org/abs/2511.05324)
*Firoj Ahmmed Patwary,Abdullah Al Noman*

Main category: cs.CL

TL;DR: 提出了一种专为孟加拉语设计的BPE分词器BengaliBPE，通过Unicode规范化、字素级初始化和形态感知合并规则，提升了分词效果和形态可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前子词分词器（如SentencePiece或HuggingFace BPE）主要适用于拉丁或多语言语料，对孟加拉语等形态丰富的语言效果不佳。

Method: BengaliBPE采用Unicode规范化、字素级初始化和形态感知合并规则，以保持语言一致性和保留子词完整性。

Result: 在一个大规模孟加拉语新闻分类数据集上，BengaliBPE在分词粒度、编码速度和下游分类准确率方面表现最佳，提供了最详细的细化和最好的形态可解释性。

Conclusion: 语言感知分词对形态丰富的语言非常重要，BengaliBPE为未来孟加拉语NLP系统奠定了坚实基础。

Abstract: Tokenization is an important first step in Natural Language Processing (NLP)
pipelines because it decides how models learn and represent linguistic
information. However, current subword tokenizers like SentencePiece or
HuggingFace BPE are mostly designed for Latin or multilingual corpora and do
not perform well on languages with rich morphology such as Bengali. To address
this limitation, we present BengaliBPE, a Byte Pair Encoding (BPE) tokenizer
specifically developed for the Bengali script. BengaliBPE applies Unicode
normalization, grapheme-level initialization, and morphology-aware merge rules
to maintain linguistic consistency and preserve subword integrity. We use a
large-scale Bengali news classification dataset to compare BengaliBPE with
three baselines: Whitespace, SentencePiece BPE, and HuggingFace BPE. The
evaluation considers tokenization granularity, encoding speed, and downstream
classification accuracy. While all methods perform reasonably well, BengaliBPE
provides the most detailed segmentation and the best morphological
interpretability, albeit with slightly higher computational cost. These
findings highlight the importance of language-aware tokenization for
morphologically rich scripts and establish BengaliBPE as a strong foundation
for future Bengali NLP systems, including large-scale pretraining of contextual
language models.

</details>


### [34] [Minority-Aware Satisfaction Estimation in Dialogue Systems via Preference-Adaptive Reinforcement Learning](https://arxiv.org/abs/2511.05407)
*Yahui Fu,Zi Haur Pang,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 提出了一个统一框架，通过建模个体和群体偏好来估计用户满意度，包含CoPeR、M2PC和PAda-PPO三个组件。


<details>
  <summary>Details</summary>
Motivation: 现有对话系统对齐方法通常训练一刀切模型，忽略了少数用户和用户特定的适应性。

Method: 引入了CoPeR捕捉个体偏好，M2PC算法发现用户群体以学习群体级偏好，并将这些组件整合到PAda-PPO框架中。

Result: 在情感支持对话数据集上的实验显示，用户满意度估计有持续改进，尤其对代表性不足的用户群体。

Conclusion: 提出的框架有效提升了用户满意度估计，特别是少数群体用户的满意度。

Abstract: User satisfaction in dialogue systems is inherently subjective. When the same
response strategy is applied across users, minority users may assign different
satisfaction ratings than majority users due to variations in individual
intents and preferences. However, existing alignment methods typically train
one-size-fits-all models that aim for broad consensus, often overlooking
minority perspectives and user-specific adaptation. We propose a unified
framework that models both individual- and group-level preferences for user
satisfaction estimation. First, we introduce Chain-of-Personalized-Reasoning
(CoPeR) to capture individual preferences through interpretable reasoning
chains. Second, we propose an expectation-maximization-based Majority-Minority
Preference-Aware Clustering (M2PC) algorithm that discovers distinct user
groups in an unsupervised manner to learn group-level preferences. Finally, we
integrate these components into a preference-adaptive reinforcement learning
framework (PAda-PPO) that jointly optimizes alignment with both individual and
group preferences. Experiments on the Emotional Support Conversation dataset
demonstrate consistent improvements in user satisfaction estimation,
particularly for underrepresented user groups.

</details>


### [35] [Steering Language Models with Weight Arithmetic](https://arxiv.org/abs/2511.05408)
*Constanza Fierro,Fabien Roger*

Main category: cs.CL

TL;DR: 提出对比权重引导，通过权重算术编辑模型参数以改善LLMs的行为泛化。


<details>
  <summary>Details</summary>
Motivation: 高质量反馈在广泛训练分布上难以提供且昂贵，窄分布上反馈可能导致不希望的泛化。

Method: 通过减去两个小的微调权重增量（一个引导期望行为，另一个引导相反行为）来在权重空间中分离行为方向，并添加或移除该方向来修改模型权重。

Result: 权重引导在分布外行为控制方面通常比激活引导泛化效果更好，能够在降低模型性能之前实现更强的控制。在特定任务微调中，权重引导可部分缓解不希望的行为漂移。

Conclusion: 通过测量微调更新与“恶意”权重方向之间的相似性，可以检测出突现的不对齐现象，从而可能监控训练中权重的演化并检测不希望的模型行为。

Abstract: Providing high-quality feedback to Large Language Models (LLMs) on a diverse
training distribution can be difficult and expensive, and providing feedback
only on a narrow distribution can result in unintended generalizations. To
better leverage narrow training data, we propose contrastive weight steering, a
simple post-training method that edits the model parameters using weight
arithmetic. We isolate a behavior direction in weight-space by subtracting the
weight deltas from two small fine-tunes -- one that induces the desired
behavior and another that induces its opposite -- and then add or remove this
direction to modify the model's weights. We apply this technique to mitigate
sycophancy and induce misalignment, and find that weight steering often
generalizes further than activation steering, achieving stronger
out-of-distribution behavioral control before degrading general capabilities.
We also show that, in the context of task-specific fine-tuning, weight steering
can partially mitigate undesired behavioral drift: it can reduce sycophancy and
under-refusals introduced during fine-tuning while preserving task performance
gains. Finally, we provide preliminary evidence that emergent misalignment can
be detected by measuring the similarity between fine-tuning updates and an
"evil" weight direction, suggesting that it may be possible to monitor the
evolution of weights during training and detect rare misaligned behaviors that
never manifest during training or evaluations.

</details>


### [36] [MIMIC-SR-ICD11: A Dataset for Narrative-Based Diagnosis](https://arxiv.org/abs/2511.05485)
*Yuexin Wu,Shiqi Wang,Vasile Rus*

Main category: cs.CL

TL;DR: 本文介绍了MIMIC-SR-ICD11数据集和LL-Rank框架，旨在通过重新排序方法改进疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 疾病诊断在现代医疗中至关重要，自我报告能保留临床显著信号，而常规电子健康记录（EHR）可能忽略这些细节。

Method: 引入了MIMIC-SR-ICD11数据集，并提出LL-Rank，这是一个基于可能性的重新排序框架，计算每个标签的长度归一化联合可能性，并减去无报告的先验可能性。

Result: 在七个模型骨干上，LL-Rank始终优于生成加映射基线（GenMap），消融实验显示其优势主要来自基于PMI的评分。

Conclusion: LL-Rank框架通过PMI-based评分，从标签频率偏差中分离语义兼容性，从而有效提升疾病诊断的准确性。

Abstract: Disease diagnosis is a central pillar of modern healthcare, enabling early
detection and timely intervention for acute conditions while guiding lifestyle
adjustments and medication regimens to prevent or slow chronic disease.
Self-reports preserve clinically salient signals that templated electronic
health record (EHR) documentation often attenuates or omits, especially subtle
but consequential details. To operationalize this shift, we introduce
MIMIC-SR-ICD11, a large English diagnostic dataset built from EHR discharge
notes and natively aligned to WHO ICD-11 terminology. We further present
LL-Rank, a likelihood-based re-ranking framework that computes a
length-normalized joint likelihood of each label given the clinical report
context and subtracts the corresponding report-free prior likelihood for that
label. Across seven model backbones, LL-Rank consistently outperforms a strong
generation-plus-mapping baseline (GenMap). Ablation experiments show that
LL-Rank's gains primarily stem from its PMI-based scoring, which isolates
semantic compatibility from label frequency bias.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [A hybrid solution approach for the Integrated Healthcare Timetabling Competition 2024](https://arxiv.org/abs/2511.04685)
*Daniela Guericke,Rolf van der Hulst,Asal Karimpour,Ieke Schrader,Matthias Walter*

Main category: cs.AI

TL;DR: Team Twente在2024年综合医疗排程竞赛中，通过将混合整数规划、约束编程和模拟退火结合在一个三阶段解法中取得了第三名。


<details>
  <summary>Details</summary>
Motivation: 解决医疗排程问题的复杂性，通过分解为子问题的方法，结合多种编程技术以找到更优的解决方案。

Method: 采用三阶段解法，结合了混合整数规划、约束编程和模拟退火技术，并对问题进行了分解处理。

Result: 在竞赛中获得了第三名，并为基准实例提供了最优解值的下界。

Conclusion: 通过解决开放问题，可能进一步改进该解法。

Abstract: We report about the algorithm, implementation and results submitted to the
Integrated Healthcare Timetabling Competition 2024 by Team Twente, which scored
third in the competition. Our approach combines mixed-integer programming,
constraint programming and simulated annealing in a 3-phase solution approach
based on decomposition into subproblems. Next to describing our approach and
describing our design decisions, we share our insights and, for the first time,
lower bounds on the optimal solution values for the benchmark instances. We
finally highlight open problems for which we think that addressing them could
improve our approach even further.

</details>


### [38] [Epistemic Reject Option Prediction](https://arxiv.org/abs/2511.04855)
*Vojtech Franc,Jakub Paplham*

Main category: cs.AI

TL;DR: 本文介绍了一种在数据不足情况下基于认知不确定性进行拒绝预测的新方法。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，模型需准确预测并量化不确定性，传统方法仅考虑偶然不确定性，而数据不足时认知不确定性显著。

Method: 提出认知拒绝选项预测器，结合贝叶斯学习，通过最小化期望遗憾定义最优预测器，并在遗憾超过拒绝成本时拒绝预测。

Result: 建立首个原则性框架，使学习预测器能识别出训练数据不足以支持可靠决策的输入。

Conclusion: 该框架在数据不足场景下，通过识别和拒绝高认知不确定性的输入，提高预测的可靠性。

Abstract: In high-stakes applications, predictive models must not only produce accurate
predictions but also quantify and communicate their uncertainty. Reject-option
prediction addresses this by allowing the model to abstain when prediction
uncertainty is high. Traditional reject-option approaches focus solely on
aleatoric uncertainty, an assumption valid only when large training data makes
the epistemic uncertainty negligible. However, in many practical scenarios,
limited data makes this assumption unrealistic. This paper introduces the
epistemic reject-option predictor, which abstains in regions of high epistemic
uncertainty caused by insufficient data. Building on Bayesian learning, we
redefine the optimal predictor as the one that minimizes expected regret -- the
performance gap between the learned model and the Bayes-optimal predictor with
full knowledge of the data distribution. The model abstains when the regret for
a given input exceeds a specified rejection cost. To our knowledge, this is the
first principled framework that enables learning predictors capable of
identifying inputs for which the training data is insufficient to make reliable
decisions.

</details>


### [39] [DMA: Online RAG Alignment with Human Feedback](https://arxiv.org/abs/2511.04880)
*Yu Bai,Yukai Miao,Dawei Wang,Li Chen,Fei Long,Rundi Zhai,Dan Li,Yanyu Ren,Tianfeng Liu,Hongtao Xie,Ce Yang,Xuhui Cai*

Main category: cs.AI

TL;DR: DMA是一种在线学习框架，通过整合多粒度人类反馈，提升RAG系统在面对变化意图和内容漂移时的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG系统依赖于静态检索，限制了其在变化环境中的适应性。本文旨在通过引入多粒度反馈，实现检索的动态调整。

Method: DMA框架将文档级、列表级和响应级信号整合为一个连贯的学习流程，包括用于点式和列表式排序器的监督训练，由响应级偏好驱动的策略优化，以及知识蒸馏为轻量级评分器。

Result: 通过大规模在线A/B测试和少量样本离线测试，DMA在工业部署中显著提升了人类参与度，并在知识密集型基准测试中保持竞争力，同时在对话式QA上取得显著改进。

Conclusion: DMA是一种在RAG中实现反馈驱动、实时适应性的有效方法，不牺牲基础检索能力。

Abstract: Retrieval-augmented generation (RAG) systems often rely on static retrieval,
limiting adaptation to evolving intent and content drift. We introduce Dynamic
Memory Alignment (DMA), an online learning framework that systematically
incorporates multi-granularity human feedback to align ranking in interactive
settings. DMA organizes document-, list-, and response-level signals into a
coherent learning pipeline: supervised training for pointwise and listwise
rankers, policy optimization driven by response-level preferences, and
knowledge distillation into a lightweight scorer for low-latency serving.
Throughout this paper, memory refers to the model's working memory, which is
the entire context visible to the LLM for In-Context Learning.
  We adopt a dual-track evaluation protocol mirroring deployment: (i)
large-scale online A/B ablations to isolate the utility of each feedback
source, and (ii) few-shot offline tests on knowledge-intensive benchmarks.
Online, a multi-month industrial deployment further shows substantial
improvements in human engagement. Offline, DMA preserves competitive
foundational retrieval while yielding notable gains on conversational QA
(TriviaQA, HotpotQA). Taken together, these results position DMA as a
principled approach to feedback-driven, real-time adaptation in RAG without
sacrificing baseline capability.

</details>


### [40] [Autonomous generation of different courses of action in mechanized combat operations](https://arxiv.org/abs/2511.05182)
*Johan Schubert,Patrik Hansen,Pontus Hörling,Ronnie Johansson*

Main category: cs.AI

TL;DR: 提出一种支持地面军事作战执行阶段决策的方法论，通过生成和评估多种行动方案，优化机械化营的决策过程。


<details>
  <summary>Details</summary>
Motivation: 在军事地面作战中，需要实时有效的决策支持，尤其是在不断变化的战场条件下，快速评估和选择最佳行动方案至关重要。

Method: 该方法从一组初始行动方案出发，系统生成成千上万的个体行动选择，并基于对手状态、部队组成、力量比、攻防类型以及预期推进速度进行评估，使用野战手册评估战斗结果和推进速率，生成和评估过程并行进行。

Result: 通过并行生成和评估，该方法能实时管理并优化行动方案，随着战场情况的变化，为决策者提供更新的行动建议。

Conclusion: 该方法为机械化营在动态战场环境中提供了有效的决策支持，通过持续生成和评估行动方案，提升了作战决策的灵活性和效率。

Abstract: In this paper, we propose a methodology designed to support decision-making
during the execution phase of military ground combat operations, with a focus
on one's actions. This methodology generates and evaluates recommendations for
various courses of action for a mechanized battalion, commencing with an
initial set assessed by their anticipated outcomes. It systematically produces
thousands of individual action alternatives, followed by evaluations aimed at
identifying alternative courses of action with superior outcomes. These
alternatives are appraised in light of the opponent's status and actions,
considering unit composition, force ratios, types of offense and defense, and
anticipated advance rates. Field manuals evaluate battle outcomes and
advancement rates. The processes of generation and evaluation work
concurrently, yielding a variety of alternative courses of action. This
approach facilitates the management of new course generation based on
previously evaluated actions. As the combat unfolds and conditions evolve,
revised courses of action are formulated for the decision-maker within a
sequential decision-making framework.

</details>


### [41] [Reasoning Is All You Need for Urban Planning AI](https://arxiv.org/abs/2511.05375)
*Sijie Yang,Jiatong Li,Filip Biljecki*

Main category: cs.AI

TL;DR: 提出了一种新型AI框架，用于城市规划决策支持，强调了推理能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 城市规划需要AI不仅分析数据，还需进行决策支持，具备推理能力以处理约束和利益相关者的价值观。

Method: 提出Agentic Urban Planning AI框架，整合三层认知（感知、基础、推理）与六个逻辑组件（分析、生成、验证、评估、协作、决策），通过多智能体协作框架实现。

Result: 展示了该框架如何通过明确推理能力来满足基于价值的决策、规则约束和可解释性要求，超越了纯统计学习的能力。

Conclusion: 该框架通过系统探索解决方案空间、验证法规合规性和透明权衡，增强人类规划者的决策能力，而不是取代人类判断。

Abstract: AI has proven highly successful at urban planning analysis -- learning
patterns from data to predict future conditions. The next frontier is
AI-assisted decision-making: agents that recommend sites, allocate resources,
and evaluate trade-offs while reasoning transparently about constraints and
stakeholder values. Recent breakthroughs in reasoning AI -- CoT prompting,
ReAct, and multi-agent collaboration frameworks -- now make this vision
achievable.
  This position paper presents the Agentic Urban Planning AI Framework for
reasoning-capable planning agents that integrates three cognitive layers
(Perception, Foundation, Reasoning) with six logic components (Analysis,
Generation, Verification, Evaluation, Collaboration, Decision) through a
multi-agents collaboration framework. We demonstrate why planning decisions
require explicit reasoning capabilities that are value-based (applying
normative principles), rule-grounded (guaranteeing constraint satisfaction),
and explainable (generating transparent justifications) -- requirements that
statistical learning alone cannot fulfill. We compare reasoning agents with
statistical learning, present a comprehensive architecture with benchmark
evaluation metrics, and outline critical research challenges. This framework
shows how AI agents can augment human planners by systematically exploring
solution spaces, verifying regulatory compliance, and deliberating over
trade-offs transparently -- not replacing human judgment but amplifying it with
computational reasoning capabilities.

</details>
