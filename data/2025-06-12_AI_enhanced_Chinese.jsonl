{"id": "2506.09070", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09070", "abs": "https://arxiv.org/abs/2506.09070", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "AI": {"tldr": "STREAMINGGS\u662f\u4e00\u79cd3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5185\u5b58\u4e2d\u5fc3\u6e32\u67d3\u663e\u8457\u63d0\u5347\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "3DGS\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u96be\u4ee5\u8fbe\u523090 FPS\u7684\u5b9e\u65f6\u8981\u6c42\uff0c\u73b0\u6709\u52a0\u901f\u5668\u5ffd\u89c6\u4e86\u5185\u5b58\u6548\u7387\uff0c\u5bfc\u81f4DRAM\u6d41\u91cf\u5197\u4f59\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u6d41\u5f0f3DGS\u8bbe\u8ba1\uff0c\u4ece\u57fa\u4e8e\u74e6\u7247\u7684\u6e32\u67d3\u8f6c\u53d8\u4e3a\u5185\u5b58\u4e2d\u5fc3\u6e32\u67d3\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\u5e76\u51cf\u5c11DRAM\u6d41\u91cf\u3002", "result": "\u8bbe\u8ba1\u5728\u79fb\u52a8Ampere GPU\u4e0a\u5b9e\u73b0\u4e8645.7\u500d\u52a0\u901f\u548c62.9\u500d\u80fd\u8017\u8282\u7701\u3002", "conclusion": "STREAMINGGS\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u6548\u7387\u663e\u8457\u63d0\u5347\u4e863DGS\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2506.09075", "categories": ["cs.GR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7b80\u5355\u6846\u67b6\uff0c\u7528\u4e8e\u8fd0\u52a8\u63d2\u503c\u4efb\u52a1\uff0c\u5f3a\u8c03\u6570\u636e\u5efa\u6a21\u9009\u62e9\u5bf9\u6027\u80fd\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8fd0\u52a8\u63d2\u503c\u662f\u52a8\u753b\u5e08\u7684\u5173\u952e\u5de5\u5177\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u6a21\u578b\uff0c\u672c\u6587\u65e8\u5728\u7b80\u5316\u6a21\u578b\u5e76\u63a2\u7d22\u6570\u636e\u5efa\u6a21\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5355\u4e00Transformer\u7f16\u7801\u5668\u6846\u67b6\uff0c\u7814\u7a76\u6570\u636e\u91cf\u3001\u59ff\u6001\u8868\u793a\u548c\u901f\u5ea6\u8f93\u5165\u7279\u5f81\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6570\u636e\u91cf\u3001\u4f18\u5316\u59ff\u6001\u8868\u793a\u548c\u5f15\u5165\u901f\u5ea6\u7279\u5f81\u53ef\u63d0\u5347\u8fd0\u52a8\u63d2\u503c\u8d28\u91cf\uff0c\u6311\u6218\u4e86\u6a21\u578b\u590d\u6742\u6027\u51b3\u5b9a\u6027\u80fd\u7684\u5047\u8bbe\u3002", "conclusion": "\u6570\u636e\u5efa\u6a21\u9009\u62e9\u5bf9\u8fd0\u52a8\u63d2\u503c\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.09665", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09665", "abs": "https://arxiv.org/abs/2506.09665", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "comment": null, "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "AI": {"tldr": "\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u3001\u89c6\u9891\u5185\u90e8\u5206\u89e3\u548c\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u4ece\u6587\u672c\u63d0\u793a\u6216\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u6a21\u578b\u6750\u8d28\u3002", "motivation": "\u4e3a3D\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u6750\u8d28\uff0c\u652f\u6301\u6587\u672c\u6216\u5355\u5f20\u56fe\u50cf\u8f93\u5165\uff0c\u63d0\u5347\u5185\u5bb9\u521b\u4f5c\u6548\u7387\u3002", "method": "1. \u5fae\u8c03\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ee5\u7b26\u5408\u8f93\u5165\u51e0\u4f55\u548c\u5149\u7167\u6761\u4ef6\uff1b2. \u4ece\u751f\u6210\u89c6\u9891\u4e2d\u63d0\u53d6\u6750\u8d28\u5c5e\u6027\uff08\u57fa\u7840\u8272\u3001\u7c97\u7cd9\u5ea6\u3001\u91d1\u5c5e\u5ea6\uff09\uff1b3. \u7ed3\u5408\u53ef\u5fae\u5206\u8def\u5f84\u8ffd\u8e2a\u5668\u751f\u6210\u517c\u5bb9\u5e38\u89c1\u5de5\u5177\u7684PBR\u6750\u8d28\u3002", "result": "\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u76843D\u6a21\u578b\u6750\u8d28\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u5185\u5bb9\u521b\u4f5c\u5de5\u5177\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4e0e\u7269\u7406\u6e32\u67d3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u76843D\u6750\u8d28\u751f\u6210\u3002"}}
{"id": "2506.09909", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2506.09909", "abs": "https://arxiv.org/abs/2506.09909", "authors": ["Yijie Deng", "Lei Han", "Lu Fang"], "title": "TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model", "comment": null, "summary": "Neural rendering algorithms have revolutionized computer graphics, yet their\nimpact on real-time rendering under arbitrary lighting conditions remains\nlimited due to strict latency constraints in practical applications. The key\nchallenge lies in formulating a compact yet expressive material representation.\nTo address this, we propose TransGI, a novel neural rendering method for\nreal-time, high-fidelity global illumination. It comprises an object-centric\nneural transfer model for material representation and a radiance-sharing\nlighting system for efficient illumination. Traditional BSDF representations\nand spatial neural material representations lack expressiveness, requiring\nthousands of ray evaluations to converge to noise-free colors. Conversely,\nreal-time methods trade quality for efficiency by supporting only diffuse\nmaterials. In contrast, our object-centric neural transfer model achieves\ncompactness and expressiveness through an MLP-based decoder and vertex-attached\nlatent features, supporting glossy effects with low memory overhead. For\ndynamic, varying lighting conditions, we introduce local light probes capturing\nscene radiance, coupled with an across-probe radiance-sharing strategy for\nefficient probe generation. We implemented our method in a real-time rendering\nengine, combining compute shaders and CUDA-based neural networks. Experimental\nresults demonstrate that our method achieves real-time performance of less than\n10 ms to render a frame and significantly improved rendering quality compared\nto baseline methods.", "AI": {"tldr": "TransGI\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u9ad8\u4fdd\u771f\u5168\u5c40\u5149\u7167\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u7684\u795e\u7ecf\u4f20\u9012\u6a21\u578b\u548c\u8f90\u5c04\u5171\u4eab\u7167\u660e\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u7b97\u6cd5\u5728\u5b9e\u65f6\u6e32\u67d3\u548c\u4efb\u610f\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u6750\u6599\u8868\u793a\u7684\u7d27\u51d1\u6027\u548c\u8868\u73b0\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5bf9\u8c61\u4e2d\u5fc3\u7684\u795e\u7ecf\u4f20\u9012\u6a21\u578b\uff08MLP\u89e3\u7801\u5668\u548c\u9876\u70b9\u9644\u52a0\u6f5c\u5728\u7279\u5f81\uff09\u548c\u5c40\u90e8\u5149\u63a2\u9488\u7ed3\u5408\u8f90\u5c04\u5171\u4eab\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTransGI\u80fd\u572810\u6beb\u79d2\u5185\u5b8c\u6210\u5e27\u6e32\u67d3\uff0c\u4e14\u6e32\u67d3\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TransGI\u5728\u5b9e\u65f6\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u9ad8\u4fdd\u771f\u5168\u5c40\u5149\u7167\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perli\u0107", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\uff08LLM-as-a-qualitative-judge\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u62a5\u544a\u5206\u6790NLG\u7cfb\u7edf\u7684\u5e38\u89c1\u95ee\u9898\u7c7b\u578b\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u6539\u8fdb\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6570\u503c\u8bc4\u5206\uff0c\u7f3a\u4e4f\u5bf9\u95ee\u9898\u7684\u5177\u4f53\u5206\u6790\uff0c\u96be\u4ee5\u6307\u5bfc\u7cfb\u7edf\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a1) \u5f00\u653e\u5f0f\u9010\u5b9e\u4f8b\u95ee\u9898\u5206\u6790\uff1b2) \u76f4\u89c2\u7d2f\u79ef\u7b97\u6cd5\u805a\u7c7b\u95ee\u9898\u3002", "result": "\u57282/3\u6848\u4f8b\u4e2d\u6b63\u786e\u8bc6\u522b\u5b9e\u4f8b\u7279\u5b9a\u95ee\u9898\uff0c\u751f\u6210\u7684\u9519\u8bef\u7c7b\u578b\u62a5\u544a\u4e0e\u4eba\u5de5\u6807\u6ce8\u62a5\u544a\u76f8\u4f3c\u3002", "conclusion": "LLM-as-a-qualitative-judge\u80fd\u6709\u6548\u63d0\u4f9b\u6539\u8fdbNLG\u7cfb\u7edf\u7684\u5177\u4f53\u5efa\u8bae\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.09066", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u548c\u53ef\u62fc\u63a5\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u62fc\u63a5\u4e24\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u9002\u5e94\u4e0d\u540c\u8bbe\u5907\u7684\u8d44\u6e90\u9650\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u7684\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u4e0d\u540c\u8bbe\u5907\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u5f02\u6784\uff0c\u5355\u4e00\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u6240\u6709\u5e73\u53f0\uff0c\u4f20\u7edf\u538b\u7f29\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "ReStNet\u901a\u8fc7\u8ba1\u7b97\u5c42\u95f4\u76f8\u4f3c\u5ea6\uff08CKA\uff09\u9009\u62e9\u62fc\u63a5\u70b9\uff0c\u4fdd\u7559\u5927\u6a21\u578b\u7684\u65e9\u671f\u5c42\u548c\u5c0f\u6a21\u578b\u7684\u6df1\u5c42\uff0c\u4ec5\u5fae\u8c03\u62fc\u63a5\u5c42\uff0c\u652f\u6301\u540c\u6784\u548c\u5f02\u6784\u6a21\u578b\u62fc\u63a5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReStNet\u5728\u8fd0\u884c\u65f6\u7075\u6d3b\u5e73\u8861\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "ReStNet\u4e3a\u52a8\u6001\u9002\u5e94\u8d44\u6e90\u9650\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09997", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "DGS-LRM\u662f\u4e00\u79cd\u524d\u9988\u65b9\u6cd5\uff0c\u9996\u6b21\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u9884\u6d4b\u53ef\u53d8\u5f623D\u9ad8\u65af\u6591\u70b9\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u524d\u9988\u6a21\u578b\u591a\u9650\u4e8e\u9759\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u91cd\u5efa\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\uff0c\u4e9f\u9700\u89e3\u51b3\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u3001\u53ef\u53d8\u5f623D\u9ad8\u65af\u8868\u793a\u548c\u5927\u578bTransformer\u7f51\u7edc\uff0c\u5b9e\u73b0\u5b9e\u65f6\u52a8\u6001\u91cd\u5efa\u3002", "result": "DGS-LRM\u91cd\u5efa\u8d28\u91cf\u5ab2\u7f8e\u4f18\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u52a8\u6001\u91cd\u5efa\u65b9\u6cd5\uff0c\u652f\u6301\u957f\u7a0b3D\u8ddf\u8e2a\u3002", "conclusion": "DGS-LRM\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09175", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u77ed\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u77ed\u8bed\u51fa\u73b0\u9891\u7387\u4f4e\uff0c\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u77ed\u8bed\u7684\u6b63\u786e\u7ffb\u8bd1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u6e90\u8bed\u8a00\u5230\u76ee\u6807\u8bed\u8a00\u7684\u77ed\u8bed\u6620\u5c04\u5bf9\uff0c\u63d0\u51fa\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u4e8e\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u5728\u6d41\u5f0f\u8bed\u97f3\u7ffb\u8bd1\u6a21\u578b\u4e2d\u76f8\u5bf9\u77ed\u8bed\u5217\u8868\u504f\u7f6e\u63d0\u5347\u4e8621%\uff0c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u77ed\u8bed\u53ec\u56de\u7387\u76f8\u5bf9\u63d0\u5347\u4e8685%\u3002", "conclusion": "\u77ed\u8bed\u8bcd\u5178\u504f\u7f6e\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u77ed\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.09067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u7528\u4e8e\u4fdd\u62a4\u751f\u6210\u5f0f\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Med-VLMs\uff09\u514d\u53d7\u6709\u5bb3\u67e5\u8be2\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u9632\u5fa1\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "Med-VLMs\u5728\u751f\u6210\u533b\u5b66\u62a5\u544a\u65f6\u9762\u4e34\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8981\u62d2\u7edd\u6709\u5bb3\u67e5\u8be2\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u9632\u5fa1\uff0c\u5f71\u54cd\u6b63\u5e38\u67e5\u8be2\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5408\u6210\u4e34\u5e8a\u6f14\u793a\u7684\u63a8\u7406\u65f6\u9632\u5fa1\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6837\u5316\u7684\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u6df7\u5408\u6f14\u793a\u7b56\u7565\u4ee5\u5e73\u8861\u5b89\u5168\u4e0e\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u80fd\u6709\u6548\u9632\u5fa1\u89c6\u89c9\u548c\u6587\u672c\u8d8a\u72f1\u653b\u51fb\uff0c\u4e14\u589e\u52a0\u6f14\u793a\u9884\u7b97\u53ef\u7f13\u89e3\u8fc7\u5ea6\u9632\u5fa1\u95ee\u9898\u3002", "conclusion": "\u6df7\u5408\u6f14\u793a\u7b56\u7565\u5728\u5c11\u6837\u672c\u6f14\u793a\u9884\u7b97\u9650\u5236\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2506.09218", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc \u0160egedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u751f\u6210\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u97f3\u9891\u6ce2\u5f62\u5b66\u4e60\u4e2d\u7684\u8bcd\u6c47\u65e0\u5173\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed5\u8fc7\u5168\u8fde\u63a5\u5c42\uff08FC\uff09\u8f93\u5165\u968f\u673a\u7279\u5f81\u56fe\u6765\u63a2\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u662f\u5426\u80fd\u4ece\u8bcd\u6c47\u5b66\u4e60\u4e2d\u63d0\u53d6\u97f3\u4f4d\u89c4\u5219\uff0c\u5e76\u7814\u7a76\u7f29\u5c0f\u5168\u8fde\u63a5\u5c42\u74f6\u9888\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u8bad\u7ec3\u751f\u6210CNN\u4e8e\u97f3\u9891\u6ce2\u5f62\u6570\u636e\uff0c\u7f29\u5c0fFC\u5c42\u901a\u9053\u6570\uff0c\u5e76\u7ed5\u8fc7FC\u8f93\u5165\u968f\u673a\u7279\u5f81\u56fe\u751f\u6210\u97f3\u9891\u8f93\u51fa\u3002", "result": "\u5377\u79ef\u5c42\u80fd\u52a8\u6001\u6cdb\u5316\u97f3\u4f4d\u4f9d\u8d56\u5173\u7cfb\uff0c\u8d85\u8d8aFC\u5c42\u5b66\u4e60\u7684\u8bcd\u6c47\u7ea6\u675f\u914d\u7f6e\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u8bc1\u660e\u5377\u79ef\u5c42\u5177\u6709\u8bcd\u6c47\u65e0\u5173\u7684\u97f3\u4f4d\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09068", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "BG-HOP\u662f\u4e00\u79cd\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0c\u7528\u4e8e\u5efa\u6a213D\u4e2d\u7684\u53cc\u624b-\u7269\u4f53\u4ea4\u4e92\uff0c\u901a\u8fc7\u6269\u5c55\u5355\u624b\u751f\u6210\u5148\u9a8c\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u751f\u6210\u53cc\u624b\u4ea4\u4e92\u548c\u6293\u53d6\u5408\u6210\u7684\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u53cc\u624b-\u7269\u4f53\u4ea4\u4e92\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u6269\u5c55\u5355\u624b\u751f\u6210\u5148\u9a8c\u4ee5\u5efa\u6a21\u66f4\u590d\u6742\u7684\u4ea4\u4e92\u3002", "method": "\u6269\u5c55\u73b0\u6709\u7684\u5355\u624b\u751f\u6210\u5148\u9a8c\u6a21\u578b\uff0c\u5efa\u6a21\u53cc\u624b\u4e0e\u7269\u4f53\u7684\u8054\u5408\u5206\u5e03\u3002", "result": "\u6a21\u578b\u80fd\u591f\u751f\u6210\u53cc\u624b\u4ea4\u4e92\u52a8\u4f5c\uff0c\u5e76\u4e3a\u7ed9\u5b9a\u7269\u4f53\u5408\u6210\u6293\u53d6\u52a8\u4f5c\u3002", "conclusion": "BG-HOP\u5c55\u793a\u4e86\u5efa\u6a21\u53cc\u624b-\u7269\u4f53\u4ea4\u4e92\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2506.09251", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Transformer\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u76f8\u5173\u4efb\u52a1\u7684\u8bad\u7ec3\u5b9e\u73b0\u957f\u5ea6\u6cdb\u5316\u7684\u8fc1\u79fb\uff0c\u5373\u5728\u8f85\u52a9\u4efb\u52a1\u4e0a\u8bad\u7ec3\u540e\u80fd\u6cdb\u5316\u5230\u76ee\u6807\u4efb\u52a1\u7684\u66f4\u957f\u8f93\u5165\u3002", "motivation": "\u63a2\u7d22Transformer\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4efb\u52a1\u5173\u8054\u5b9e\u73b0\u957f\u5ea6\u6cdb\u5316\uff0c\u4ee5\u7406\u89e3\u5176\u6cdb\u5316\u80fd\u529b\u7684\u6765\u6e90\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u7b97\u6cd5\u4efb\u52a1\uff08\u5982\u7b97\u672f\u8fd0\u7b97\u3001\u5b57\u7b26\u4e32\u53d8\u6362\u3001\u8ff7\u5bab\u5bfc\u822a\uff09\u4e0a\u8bad\u7ec3\u6a21\u578b\uff0c\u7814\u7a76\u957f\u5ea6\u6cdb\u5316\u7684\u8fc1\u79fb\u73b0\u8c61\u3002", "result": "\u6a21\u578b\u80fd\u4ece\u76f8\u5173\u4efb\u52a1\u4e2d\u7ee7\u627f\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u7c7b\u4f3c\u6548\u679c\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86Transformer\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u4efb\u52a1\u95f4\u7684\u7ec4\u5408\u590d\u7528\u5b9e\u73b0\u6cdb\u5316\uff0c\u52a0\u6df1\u4e86\u5bf9\u6a21\u578b\u5904\u7406\u5206\u5e03\u5916\u8f93\u5165\u7684\u7406\u89e3\u3002"}}
{"id": "2506.09071", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "SAAF\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u5b9e\u73b0\u5efa\u7b51\u7acb\u9762\u5899\u7a97\u81ea\u52a8\u5206\u5272\uff0c\u7ed3\u5408NLP\u6280\u672f\u63d0\u5347\u8bed\u4e49\u7406\u89e3\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\u63d0\u9ad8\u81ea\u52a8\u5316\u4e0e\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63d0\u5347\u5efa\u7b51\u4fe1\u606f\u6a21\u578b\u548c\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\u6548\u7387\uff0c\u89e3\u51b3\u5899\u7a97\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u8bed\u4e49\u534f\u4f5c\u7279\u5f81\u63d0\u53d6\u673a\u5236\uff0c\u7ed3\u5408\u6587\u672c\u63cf\u8ff0\u4e0e\u56fe\u50cf\u7279\u5f81\uff1b\u5f00\u53d1\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u5728\u591a\u4e2a\u7acb\u9762\u6570\u636e\u96c6\u4e0a\uff0cSAAF\u7684mIoU\u6307\u6807\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u5206\u5272\u80fd\u529b\u548c\u6cdb\u5316\u6027\u3002", "conclusion": "SAAF\u6a21\u578b\u4e3a\u5efa\u7b51\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u53c2\u8003\uff0c\u63a2\u7d22\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5efa\u7b51\u9886\u57df\u7684\u65b0\u5e94\u7528\u8def\u5f84\u3002"}}
{"id": "2506.09259", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08SAAM\uff09\u7528\u4e8e\u8bc6\u522b\u548c\u5206\u7c7b\u6e38\u620f\u804a\u5929\u4e2d\u7684\u4eb2\u793e\u4f1a\u884c\u4e3a\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u5347\u4e867.9%\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u5c55\u793a\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u68c0\u6d4b\u6e38\u620f\u804a\u5929\u4e2d\u7684\u8d1f\u9762\u5185\u5bb9\uff0c\u800c\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u8bc6\u522b\u8d44\u6e90\u532e\u4e4f\uff0c\u4f46\u5176\u91cd\u8981\u6027\u4e0d\u4e9a\u4e8e\u6bd2\u6027\u68c0\u6d4b\u3002", "method": "\u7ed3\u5408\u65e0\u76d1\u7763\u53d1\u73b0\u4e0e\u6e38\u620f\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\uff0c\u63d0\u51faSAAM\u6a21\u578b\uff0c\u5229\u7528\u6574\u4e2a\u8bad\u7ec3\u96c6\u4f5c\u4e3a\u201c\u951a\u70b9\u201d\u63d0\u5347\u6027\u80fd\u3002", "result": "SAAM\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u6e38\u620f\u804a\u5929\u4e2d\u4eb2\u793e\u4f1a\u884c\u4e3a\u7684\u81ea\u52a8\u5206\u7c7b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7ebf\u5e73\u53f0\u4ece\u5355\u7eaf\u60e9\u7f5a\u6bd2\u6027\u8f6c\u5411\u9f13\u52b1\u79ef\u6781\u4e92\u52a8\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09079", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u6570\u636e\u96c6DarkEventInfer\u548cMixVidQA\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86VersaVid-R1\u6a21\u578b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89c6\u9891\u63a8\u7406\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u6709\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7DarkEventInfer\u548cMixVidQA\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5f00\u53d1\u4e86VersaVid-R1\u6a21\u578b\u3002", "result": "VersaVid-R1\u5728\u591a\u9879\u89c6\u9891\u7406\u89e3\u3001\u63a8\u7406\u548c\u5b57\u5e55\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65b0\u6570\u636e\u96c6\u548c\u6a21\u578b\u6210\u529f\u586b\u8865\u4e86\u89c6\u9891\u63a8\u7406\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83LLM\u751f\u6210\u7684\u81ea\u6211\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff08self-NLE\uff09\u4e0e\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u7684\u89e3\u91ca\uff0c\u5b9a\u91cf\u6d4b\u91cf\u5176\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u884c\u4e3a\u6d4b\u8bd5\u6216\u8ba1\u7b97\u5757\u8bc6\u522b\uff0c\u672a\u80fd\u6df1\u5165\u6a21\u578b\u63a8\u7406\u7684\u795e\u7ecf\u6d3b\u52a8\u5c42\u9762\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30self-NLE\u7684\u5fe0\u5b9e\u6027\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u6bd4\u8f83self-NLE\u4e0e\u6a21\u578b\u5185\u90e8\u9690\u85cf\u72b6\u6001\u7684\u89e3\u91ca\uff0c\u4ee5\u5b9a\u91cf\u6d4b\u91cf\u5fe0\u5b9e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u5bf9self-NLE\u5fe0\u5b9e\u6027\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u4e86self-NLE\u4e0e\u6a21\u578b\u63a8\u7406\u4e4b\u95f4\u7684\u76f4\u63a5\u8054\u7cfb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86self-NLE\u5fe0\u5b9e\u6027\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u751f\u6210\u66f4\u5fe0\u5b9e\u7684self-NLE\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09081", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff0c\u901a\u8fc7\u72ec\u7acb\u8bc4\u4f30\u670d\u52a1\u548c\u9ad8\u6548\u5de5\u5177\u63d0\u5347\u8bc4\u4f30\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u8bc4\u4f30\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0cFlagEvalMM\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u6a21\u578b\u63a8\u7406\u4e0e\u8bc4\u4f30\uff0c\u5229\u7528\u72ec\u7acb\u8bc4\u4f30\u670d\u52a1\u548c\u9ad8\u6548\u5de5\u5177\uff08\u5982vLLM\u3001SGLang\uff09\u63d0\u5347\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFlagEvalMM\u80fd\u51c6\u786e\u9ad8\u6548\u5730\u8bc4\u4f30\u6a21\u578b\u4f18\u7f3a\u70b9\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u7814\u7a76\u3002", "conclusion": "FlagEvalMM\u662f\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u652f\u6301\u591a\u6a21\u6001\u6a21\u578b\u7684\u5168\u9762\u8bc4\u4f30\u3002"}}
{"id": "2506.09301", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Rhetorical-Strategy-Aware RSA\u6846\u67b6\uff08$(RSA)^2$\uff09\uff0c\u7528\u4e8e\u89e3\u91ca\u6bd4\u55bb\u6027\u8bed\u8a00\uff0c\u65e0\u9700\u5efa\u6a21\u8bf4\u8bdd\u8005\u7684\u52a8\u673a\uff0c\u7ed3\u5408LLMs\u5728\u8bbd\u523a\u89e3\u91ca\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u6bd4\u55bb\u6027\u8bed\u8a00\uff08\u5982\u8bbd\u523a\u3001\u5938\u5f20\u3001\u4f4e\u8c03\u9648\u8ff0\uff09\u5728\u4eba\u7c7b\u4ea4\u6d41\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709RSA\u6846\u67b6\u65e0\u6cd5\u89e3\u91ca\u6216\u9700\u7279\u5b9a\u5efa\u6a21\u8bf4\u8bdd\u8005\u52a8\u673a\u3002", "method": "\u5f15\u5165$(RSA)^2$\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u8bf4\u8bdd\u8005\u7684\u4fee\u8f9e\u7b56\u7565\u6765\u5efa\u6a21\u6bd4\u55bb\u6027\u8bed\u8a00\u4f7f\u7528\u3002", "result": "$(RSA)^2$\u6846\u67b6\u65e0\u9700\u5efa\u6a21\u8bf4\u8bdd\u8005\u52a8\u673a\u5373\u53ef\u5b9e\u73b0\u4eba\u7c7b\u517c\u5bb9\u7684\u975e\u5b57\u9762\u89e3\u91ca\uff0c\u7ed3\u5408LLMs\u5728\u65b0\u8bbd\u523a\u6570\u636e\u96c6PragMega+\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "$(RSA)^2$\u4e3a\u6bd4\u55bb\u6027\u8bed\u8a00\u89e3\u91ca\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5728\u8bbd\u523a\u89e3\u91ca\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.09082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "AVA-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u901a\u8fc7\u89e3\u802614\u79cd\u539f\u5b50\u89c6\u89c9\u80fd\u529b\uff08AVAs\uff09\u6765\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5206\u5e03\u4e0d\u5339\u914d\u548c\u591a\u80fd\u529b\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VQA\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u4e24\u4e2a\u76f2\u70b9\uff1a\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u4e0e\u6d4b\u8bd5\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u4ee5\u53ca\u591a\u80fd\u529b\u4efb\u52a1\u96be\u4ee5\u5b9a\u4f4d\u5177\u4f53\u7f3a\u9677\u3002AVA-Bench\u901a\u8fc7\u89e3\u8026AVAs\uff0c\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u8bc4\u4f30\u3002", "method": "\u5f15\u5165AVA-Bench\uff0c\u660e\u786e\u89e3\u802614\u79cdAVAs\uff08\u5982\u5b9a\u4f4d\u3001\u6df1\u5ea6\u4f30\u8ba1\u7b49\uff09\uff0c\u5e76\u5728\u6bcf\u79cd\u80fd\u529b\u5185\u5339\u914d\u8bad\u7ec3\u548c\u6d4b\u8bd5\u5206\u5e03\uff0c\u4ee5\u7cbe\u51c6\u8bc4\u4f30VFMs\u7684\u8868\u73b0\u3002", "result": "AVA-Bench\u63ed\u793a\u4e86VFMs\u7684\u72ec\u7279\u201c\u80fd\u529b\u6307\u7eb9\u201d\uff0c\u5e76\u53d1\u73b00.5B\u53c2\u6570\u7684LLM\u5728\u8bc4\u4f30\u6548\u679c\u4e0a\u4e0e7B\u53c2\u6570\u7684LLM\u76f8\u4f3c\uff0c\u4f46GPU\u65f6\u95f4\u51cf\u5c118\u500d\u3002", "conclusion": "AVA-Bench\u4e3a\u4e0b\u4e00\u4ee3VFMs\u63d0\u4f9b\u4e86\u5168\u9762\u900f\u660e\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u4f7f\u6a21\u578b\u9009\u62e9\u4ece\u7ecf\u9a8c\u731c\u6d4b\u8f6c\u53d8\u4e3a\u5de5\u7a0b\u5316\u51b3\u7b56\u3002"}}
{"id": "2506.09315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMistral-7B\u5927\u8bed\u8a00\u6a21\u578b\u7684\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u5728\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u89e3\u91ca\u6027\u548c\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5e38\u4f34\u968f\u8bed\u8a00\u80fd\u529b\u4e0b\u964d\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u4f7f\u7528Mistral-7B\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6539\u8fdb\u914d\u5bf9\u56f0\u60d1\u5ea6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u63d0\u793a\u5fae\u8c03\u6a21\u578b\u5206\u6790AD\u8bed\u8a00\u6a21\u5f0f\u3002", "result": "\u65b0\u65b9\u6cd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.33%\uff0c\u4f18\u4e8eADReSS 2020\u57fa\u51c6\u65b9\u6cd56.35%\uff0c\u4e14\u51b3\u7b56\u8fb9\u754c\u6e05\u6670\u53ef\u89e3\u91ca\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86AD\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u6a21\u578b\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "BakuFlow\u662f\u4e00\u4e2a\u534a\u81ea\u52a8\u6807\u6ce8\u5de5\u5177\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u624b\u52a8\u4fee\u6b63\u3001\u4ea4\u4e92\u5f0f\u6570\u636e\u589e\u5f3a\u3001\u6807\u7b7e\u4f20\u64ad\u548c\u81ea\u52a8\u6807\u6ce8\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u6807\u6ce8\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\uff0c\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff0c\u73b0\u6709\u5de5\u5177\u4ecd\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "BakuFlow\u7ed3\u5408\u4e86\u53ef\u8c03\u8282\u653e\u5927\u955c\u3001\u4ea4\u4e92\u5f0f\u6570\u636e\u589e\u5f3a\u3001\u6807\u7b7e\u4f20\u64ad\u548c\u57fa\u4e8e\u6539\u8fdbYOLOE\u7684\u81ea\u52a8\u6807\u6ce8\u6a21\u5757\u3002", "result": "\u5de5\u5177\u663e\u8457\u51cf\u5c11\u4e86\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u6570\u636e\u548c\u52a8\u6001\u6570\u636e\u96c6\u7684\u6807\u6ce8\u6548\u7387\u3002", "conclusion": "BakuFlow\u4e3a\u5bf9\u8c61\u68c0\u6d4b\u548c\u8ddf\u8e2a\u4efb\u52a1\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u3002"}}
{"id": "2506.09329", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u65b9\u6cd5\uff08\u5982Lion\u3001WebR\u3001LTE\u3001BMC\u548cFollowBench\uff09\u6765\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u6db5\u76d6\u6570\u636e\u6536\u96c6\u3001\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u4e14\u6709\u6548\u5730\u4f7f\u5176\u4e0e\u4eba\u7c7b\u671f\u671b\u5bf9\u9f50\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "1. Lion\u6846\u67b6\u901a\u8fc7\u5bf9\u6297\u84b8\u998f\u4f18\u5316\u6570\u636e\u6536\u96c6\uff1b2. WebR\u4ece\u539f\u59cb\u7f51\u9875\u81ea\u52a8\u5408\u6210\u6570\u636e\uff1b3. LTE\u6846\u67b6\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316\u77e5\u8bc6\u66f4\u65b0\uff1b4. BMC\u6539\u8fdbDPO\u4ee5\u6355\u6349\u6807\u8bb0\u7ea7\u76f8\u5173\u6027\uff1b5. FollowBench\u8bc4\u4f30\u6a21\u578b\u5bf9\u590d\u6742\u7ea6\u675f\u7684\u9075\u5faa\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u63a8\u7406\u3001\u6570\u636e\u591a\u6837\u6027\u3001\u77e5\u8bc6\u66f4\u65b0\u548c\u7ea6\u675f\u9075\u5faa\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86LLM\u5bf9\u9f50\u7684\u591a\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2506.09106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u6210AI\u6a21\u578b\u4e2d\u5c5e\u6027\u504f\u79fb\u8f83\u5c0f\uff0c\u4f46\u8bc4\u4f30\u6846\u67b6\u4e2d\u7684\u5206\u7c7b\u5668\u654f\u611f\u6027\u53ef\u80fd\u5f71\u54cd\u7ed3\u679c\uff0c\u9700\u6539\u8fdb\u6807\u6ce8\u65b9\u6cd5\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u751f\u6210AI\u6a21\u578b\u7684\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u5bf9\u4ee3\u8868\u6027\u5371\u5bb3\u548c\u6b67\u89c6\u6027\u7ed3\u679c\u7684\u62c5\u5fe7\uff0c\u4f46\u65e0\u6761\u4ef6\u751f\u6210\u4e2d\u7684\u504f\u89c1\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u8bad\u7ec3\u65e0\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u5e38\u7528\u504f\u89c1\u8bc4\u4f30\u6846\u67b6\u7814\u7a76\u8bad\u7ec3\u4e0e\u751f\u6210\u5206\u5e03\u95f4\u7684\u504f\u79fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5c5e\u6027\u504f\u79fb\u8f83\u5c0f\uff0c\u4f46\u5206\u7c7b\u5668\u654f\u611f\u6027\u5728\u8bc4\u4f30\u4e2d\u663e\u8457\uff0c\u5c24\u5176\u662f\u5c5e\u6027\u503c\u5448\u8fde\u7eed\u8c31\u65f6\u3002", "conclusion": "\u9700\u6539\u8fdb\u6807\u6ce8\u5b9e\u8df5\u3001\u4e25\u683c\u5ba1\u67e5\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u8003\u8651\u5c5e\u6027\u7684\u793e\u4f1a\u590d\u6742\u6027\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30\u504f\u89c1\u3002"}}
{"id": "2506.09331", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u5177\u5907\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\u8bc4\u4f30\u5176\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7406\u89e3LLMs\u662f\u5426\u80fd\u63a8\u65ad\u4ed6\u4eba\u610f\u56fe\uff0c\u5bf9\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5229\u7528\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5728MARL\u6846\u67b6\u4e2d\u8fdb\u884c\u534f\u4f5c\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u8868\u660eLLMs\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4e00\u5b9a\u7684\u610f\u56fe\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LLMs\u5177\u5907\u6f5c\u5728\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09109", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CAIRe\uff0c\u4e00\u79cd\u8bc4\u4f30\u56fe\u50cf\u6587\u5316\u76f8\u5173\u6027\u7684\u65b0\u6307\u6807\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u8de8\u6587\u5316\u80cc\u666f\u4e0b\u7684\u504f\u89c1\u95ee\u9898\u3002", "motivation": "\u786e\u4fdd\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u516c\u5e73\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u8de8\u6587\u5316\u504f\u89c1\u65f6\u5b58\u5728\u6027\u80fd\u635f\u5931\u3001\u4e8b\u5b9e\u9519\u8bef\u6216\u5192\u72af\u6027\u8f93\u51fa\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faCAIRe\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u4e2d\u7684\u5b9e\u4f53\u548c\u6982\u5ff5\u4e0e\u77e5\u8bc6\u5e93\u5173\u8054\uff0c\u57fa\u4e8e\u4e8b\u5b9e\u4fe1\u606f\u5bf9\u6bcf\u4e2a\u6587\u5316\u6807\u7b7e\u8fdb\u884c\u72ec\u7acb\u8bc4\u5206\u3002", "result": "CAIRe\u5728\u624b\u52a8\u6784\u5efa\u7684\u6587\u5316\u663e\u8457\u4f46\u7f55\u89c1\u7684\u6570\u636e\u96c6\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa28% F1\u5206\u6570\uff0c\u5e76\u5728\u4e24\u4e2a\u6587\u5316\u901a\u7528\u6982\u5ff5\u6570\u636e\u96c6\u4e0a\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u76f8\u5173\u6027\u5206\u522b\u4e3a0.56\u548c0.66\u3002", "conclusion": "CAIRe\u80fd\u6709\u6548\u8bc4\u4f30\u56fe\u50cf\u7684\u6587\u5316\u76f8\u5173\u6027\uff0c\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e3a\u89e3\u51b3\u8de8\u6587\u5316\u504f\u89c1\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2506.09340", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO\u901a\u8fc7\u5229\u7528\u591a\u6837\u5316\u7684\u56de\u653e\u7b56\u7565\u4ece\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u68c0\u7d22\u79bb\u7b56\u7565\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u6548\u7387\uff0c\u76f8\u6bd4GRPO\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "GRPO\u65b9\u6cd5\u56e0\u4f7f\u7528\u591a\u4e2a\u540c\u7b56\u7565\u8f93\u51fa\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6570\u636e\u6548\u7387\u4f4e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51faRePO\u65b9\u6cd5\uff0c\u5229\u7528\u56de\u653e\u7f13\u51b2\u533a\u4e2d\u7684\u79bb\u7b56\u7565\u6837\u672c\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u589e\u52a0\u4e86\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRePO\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff08\u5982Qwen2.5-Math-1.5B\u63d0\u534718.4\u5206\uff09\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u4ec5\u589e\u52a015%\u3002", "conclusion": "RePO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u3002"}}
{"id": "2506.09113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0\u662f\u4e00\u6b3e\u9ad8\u6027\u80fd\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u3001\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u548c\u4f18\u5316\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u63d0\u793a\u8ddf\u968f\u3001\u8fd0\u52a8\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u4e2d\u96be\u4ee5\u5e73\u8861\u63d0\u793a\u8ddf\u968f\u3001\u8fd0\u52a8\u5408\u7406\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0cSeedance 1.0\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u6e90\u6570\u636e\u589e\u5f3a\u3001\u9ad8\u6548\u67b6\u6784\u8bbe\u8ba1\u3001\u8bad\u7ec3\u8303\u5f0f\u4f18\u5316\u53ca\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u7cbe\u7ec6\u76d1\u7763\u5fae\u8c03\u548c\u89c6\u9891\u7279\u5b9aRLHF\u3002", "result": "Seedance 1.0\u57281080p\u5206\u8fa8\u7387\u4e0b\u751f\u62105\u79d2\u89c6\u9891\u4ec5\u970041.4\u79d2\uff0c\u5177\u6709\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u751f\u6210\u548c\u5353\u8d8a\u7684\u65f6\u7a7a\u6d41\u7545\u6027\u3002", "conclusion": "Seedance 1.0\u5728\u590d\u6742\u591a\u4e3b\u4f53\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u591a\u955c\u5934\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u662f\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2506.09342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\uff08MLA\uff09\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0MLA\u7ed3\u5408\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u5728\u5185\u5b58\u548c\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u5c0f\u8bed\u8a00\u6a21\u578b\u4e2d\u6f5c\u5728\u591a\u5934\u6ce8\u610f\u529b\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u6743\u8861\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u7684\u5e94\u7528\u63d0\u4f9b\u4f18\u5316\u65b9\u6848\u3002", "method": "\u8bad\u7ec330M\u53c2\u6570\u7684GPT\u6a21\u578b\uff0c\u6bd4\u8f83\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u3001MLA\u53caMLA+RoPE\u4e09\u79cd\u67b6\u6784\uff0c\u91cd\u70b9\u5173\u6ce8\u5185\u5b58\u548c\u6027\u80fd\u6307\u6807\u3002", "result": "MLA+RoPE\uff08\u534a\u79e9\u6f5c\u5728\u7ef4\u5ea6\uff09\u51cf\u5c1145%\u7684KV\u7f13\u5b58\u5185\u5b58\uff0c\u9a8c\u8bc1\u635f\u5931\u4ec5\u589e\u52a00.3%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.4\u500d\u3002", "conclusion": "MLA+RoPE\u5728\u5c0f\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCREPA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u901a\u8fc7\u8de8\u5e27\u8868\u793a\u5bf9\u9f50\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u7528\u6237\u7ea7\u89c6\u9891\u6269\u6563\u6a21\u578b\u5fae\u8c03\u5728\u4fdd\u7559\u8bad\u7ec3\u6570\u636e\u7279\u5b9a\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\uff08\u5982REPA\uff09\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51faCREPA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5e27\u7684\u9690\u85cf\u72b6\u6001\u4e0e\u76f8\u90bb\u5e27\u7684\u5916\u90e8\u7279\u5f81\u5bf9\u9f50\uff0c\u4f18\u5316\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCREPA\u5728CogVideoX-5B\u548cHunyuan Video\u7b49\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8de8\u5e27\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "CREPA\u662f\u4e00\u79cd\u5e7f\u6cdb\u9002\u7528\u7684\u6b63\u5219\u5316\u6280\u672f\uff0c\u80fd\u6709\u6548\u6539\u8fdb\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\u3002"}}
{"id": "2506.09349", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "OmniDRCA\u662f\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u81ea\u56de\u5f52\u5efa\u6a21\u7684\u5e76\u884c\u8bed\u97f3-\u6587\u672c\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u5bf9\u6bd4\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u8bed\u97f3\u4e0e\u6587\u672c\u7684\u5e76\u884c\u5904\u7406\uff0c\u5e76\u5728\u53e3\u8bed\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u97f3\u751f\u6210\u4e0e\u6587\u672c\u751f\u6210\u7684\u534f\u540c\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0cOmniDRCA\u65e8\u5728\u901a\u8fc7\u5e76\u884c\u5efa\u6a21\u548c\u5bf9\u6bd4\u5bf9\u9f50\u63d0\u5347\u6a21\u6001\u95f4\u7684\u76f8\u4e92\u611f\u77e5\u80fd\u529b\u3002", "method": "OmniDRCA\u91c7\u7528\u8054\u5408\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u7ed3\u5408\u53cc\u5206\u8fa8\u7387\u8bed\u97f3\u8868\u793a\u548c\u5bf9\u6bd4\u8de8\u6a21\u6001\u5bf9\u9f50\u6280\u672f\uff0c\u5e76\u884c\u5904\u7406\u8bed\u97f3\u4e0e\u6587\u672c\u8868\u793a\u3002", "result": "\u5728\u53e3\u8bed\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniDRCA\u5728\u5e76\u884c\u8054\u5408\u8bed\u97f3-\u6587\u672c\u5efa\u6a21\u6a21\u578b\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5e76\u4e0e\u4ea4\u9519\u6a21\u578b\u7ade\u4e89\u3002", "conclusion": "OmniDRCA\u5c55\u793a\u4e86\u5e76\u884c\u8bed\u97f3-\u6587\u672c\u5efa\u6a21\u7684\u6f5c\u529b\uff0c\u5e76\u53ef\u80fd\u6269\u5c55\u5230\u5168\u53cc\u5de5\u4f1a\u8bdd\u573a\u666f\u3002"}}
{"id": "2506.09237", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "PatchGuard\u662f\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u5f02\u5e38\u6837\u672c\u548c\u5b9a\u4f4d\u63a9\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u8bad\u7ec3\u6570\u636e\u4ec5\u5305\u542b\u6b63\u5e38\u6837\u672c\u800c\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0cPatchGuard\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u524d\u666f\u611f\u77e5\u4f2a\u5f02\u5e38\u6837\u672c\u548cViT\u67b6\u6784\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u548c\u65b0\u9896\u635f\u5931\u51fd\u6570\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u6570\u636e\u96c6\u4e0a\uff0cPatchGuard\u5728\u5bf9\u6297\u73af\u5883\u4e0bAD\u548cAL\u6027\u80fd\u5206\u522b\u63d0\u534753.2%\u548c68.5%\u3002", "conclusion": "PatchGuard\u5728\u5bf9\u6297\u548c\u975e\u5bf9\u6297\u73af\u5883\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09351", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "DIVE\u662f\u4e00\u79cd\u591a\u6837\u6027\u589e\u5f3a\u7684\u91cd\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u526a\u548c\u91cd\u7ec4FFN\u6a21\u5757\uff0c\u9ad8\u6548\u8bad\u7ec3MoE LLMs\uff0c\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709MoE LLMs\u91cd\u6784\u65b9\u6cd5\u5ffd\u89c6\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\uff0cDIVE\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DIVE\u5305\u62ec\u9886\u57df\u4eb2\u548c\u6027\u6316\u6398\u3001\u57fa\u4e8e\u4fee\u526a\u7684\u4e13\u5bb6\u91cd\u6784\u548c\u9ad8\u6548\u518d\u8bad\u7ec3\uff0c\u5177\u4f53\u4e3aFFN\u6a21\u5757\u7684\u4fee\u526a\u4e0e\u91cd\u7ec4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDIVE\u5728\u76f8\u540c\u6fc0\u6d3b\u53c2\u6570\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u6548\u7387\u9ad8\u4e14\u7cbe\u5ea6\u635f\u5931\u5c0f\u3002", "conclusion": "DIVE\u901a\u8fc7\u589e\u5f3a\u4e13\u5bb6\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347MoE LLMs\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2506.09278", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "UFM\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u8bad\u7ec3\u5728\u5149\u6d41\u4f30\u8ba1\u548c\u5bbd\u57fa\u7ebf\u5339\u914d\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u4e13\u7528\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u901a\u7528\u7684\u56fe\u50cf\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u5149\u6d41\u4f30\u8ba1\u548c\u5bbd\u57fa\u7ebf\u5339\u914d\u4efb\u52a1\u4e2d\u4f20\u7edf\u65b9\u6cd5\u5206\u79bb\u5904\u7406\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u7edf\u4e00\u8bad\u7ec3\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7b80\u5355\u7684\u901a\u7528Transformer\u67b6\u6784\uff0c\u76f4\u63a5\u56de\u5f52(u,v)\u6d41\uff0c\u907f\u514d\u4f20\u7edf\u7c97\u5230\u7ec6\u6210\u672c\u4f53\u79ef\u7684\u590d\u6742\u6027\u3002", "result": "UFM\u5728\u5149\u6d41\u4efb\u52a1\u4e2d\u6bd4Unimatch\u51c6\u786e28%\uff0c\u5728\u5bbd\u57fa\u7ebf\u5339\u914d\u4e2d\u6bd4RoMa\u8bef\u5dee\u51cf\u5c1162%\u4e14\u901f\u5ea6\u5feb6.7\u500d\u3002", "conclusion": "\u7edf\u4e00\u8bad\u7ec3\u5728\u591a\u4e2a\u9886\u57df\u4f18\u4e8e\u4e13\u7528\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u3001\u957f\u8ddd\u79bb\u548c\u5b9e\u65f6\u5bf9\u5e94\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09359", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc4\u4f30\u751f\u6210SQL\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u5206\u6790\u4e86\u5e38\u89c1\u6a21\u5f0f\u53ca\u6311\u6218\u3002", "motivation": "\u8bc4\u4f30\u751f\u6210SQL\u7684\u8bed\u4e49\u7b49\u4ef7\u6027\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u6a21\u7cca\u67e5\u8be2\u548c\u591a\u89e3\u60c5\u51b5\u3002", "method": "\u4f7f\u7528LLMs\u8bc4\u4f30\u8bed\u4e49\u548c\u2018\u5f31\u2019\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u5206\u6790SQL\u7b49\u4ef7\u4e0e\u4e0d\u7b49\u4ef7\u6a21\u5f0f\u3002", "result": "\u603b\u7ed3\u4e86SQL\u7b49\u4ef7\u6027\u7684\u5e38\u89c1\u6a21\u5f0f\u53caLLM\u8bc4\u4f30\u7684\u6311\u6218\u3002", "conclusion": "LLMs\u5728\u8bc4\u4f30SQL\u8bed\u4e49\u7b49\u4ef7\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2506.09299", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u8282\u80fd\u7684\u7a7a\u4e2d\u5e94\u6025\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u65b9\u6848\uff0c\u91c7\u7528YOLOv4-Tiny\u6a21\u578b\u5e76\u901a\u8fc7INT8\u91cf\u5316\u4f18\u5316\uff0c\u5728\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u4f53\u79ef\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u6570\u636e\u96c6\u7f3a\u4e4f\u65e0\u4eba\u673a\u89c6\u89d2\u7684\u5e94\u6025\u56fe\u50cf\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528YOLOv4-Tiny\u6a21\u578b\uff0c\u901a\u8fc7INT8\u91cf\u5316\u4f18\u5316\uff0c\u5e76\u5728\u81ea\u5b9a\u4e49\u768410,820\u5f20\u5e94\u6025\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "\u91cf\u5316\u540e\u7684\u6a21\u578b\u4f53\u79ef\u51cf\u5c0f71%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534744%\uff0c\u6027\u80fd\u4e0eYOLOv5-small\u76f8\u5f53\u3002", "conclusion": "\u91cf\u5316YOLOv4-Tiny\u6a21\u578b\u9002\u5408\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6\u5e94\u6025\u68c0\u6d4b\u3002"}}
{"id": "2506.09367", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "COGENT\u662f\u4e00\u4e2a\u9762\u5411\u8bfe\u7a0b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9002\u5408\u5e74\u7ea7\u7684\u6559\u80b2\u5185\u5bb9\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u6311\u6218\uff0c\u5982\u8bfe\u7a0b\u5bf9\u9f50\u548c\u9605\u8bfb\u6c34\u5e73\u63a7\u5236\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u6311\u6218\uff0c\u5982\u672a\u80fd\u4e0e\u8bfe\u7a0b\u6807\u51c6\u548c\u5e74\u7ea7\u9605\u8bfb\u6c34\u5e73\u4fdd\u6301\u4e00\u81f4\uff0c\u5c24\u5176\u662f\u5728STEM\u6559\u80b2\u4e2d\u3002", "method": "COGENT\u6574\u5408\u4e86\u79d1\u5b66\u6982\u5ff5\u3001\u6838\u5fc3\u601d\u60f3\u548c\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u63a7\u5236\u6587\u672c\u957f\u5ea6\u3001\u8bcd\u6c47\u548c\u53e5\u5b50\u590d\u6742\u5ea6\u6765\u8c03\u6574\u53ef\u8bfb\u6027\uff0c\u5e76\u91c7\u7528\u201c\u57fa\u4e8e\u597d\u5947\u5fc3\u201d\u7684\u65b9\u6cd5\u63d0\u5347\u5b66\u751f\u5174\u8da3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCOGENT\u751f\u6210\u7684\u6587\u672c\u5728\u9002\u5408\u5e74\u7ea7\u7684\u5185\u5bb9\u4e0a\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u4eba\u5de5\u53c2\u8003\u6750\u6599\u3002", "conclusion": "COGENT\u4e3a\u6269\u5c55\u9ad8\u8d28\u91cf\u81ea\u9002\u5e94\u5b66\u4e60\u8d44\u6e90\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2506.09300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u6811\u8393\u6d3e5\u4e0a\u90e8\u7f72\u91cf\u5316YOLOv4-Tiny\u6a21\u578b\u7528\u4e8e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u91cf\u5316\u540e\u6a21\u578b\u5728\u901f\u5ea6\u548c\u529f\u8017\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\uff0c\u4ee5\u652f\u6301\u5b89\u5168\u5173\u952e\u7684\u5e94\u6025\u54cd\u5e94\u5e94\u7528\u3002", "method": "\u91c7\u7528TensorFlow Lite\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6280\u672f\u5c06YOLOv4-Tiny\u6a21\u578b\u91cf\u5316\u4e3aINT8\u7cbe\u5ea6\uff0c\u5e76\u5728\u6811\u8393\u6d3e5\u4e0a\u8bc4\u4f30\u5176\u68c0\u6d4b\u901f\u5ea6\u3001\u529f\u8017\u548c\u70ed\u53ef\u884c\u6027\u3002", "result": "\u91cf\u5316\u6a21\u578b\u6bcf\u5f20\u56fe\u50cf\u7684\u63a8\u7406\u65f6\u95f4\u4e3a28.2\u6beb\u79d2\uff0c\u5e73\u5747\u529f\u8017\u4e3a13.85\u74e6\uff0c\u76f8\u6bd4FP32\u7248\u672c\u663e\u8457\u964d\u4f4e\uff0c\u4e14\u5bf9\u5173\u952e\u5e94\u6025\u7c7b\u522b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u4f4e\u529f\u8017\u5d4c\u5165\u5f0fAI\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u7684\u5e94\u6025\u54cd\u5e94\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u65f6\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.09375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8bf4\u8bdd\u4eba\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u63d0\u793a\u6761\u4ef6\uff0c\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u63cf\u8ff0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u5728\u63d0\u53d6\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u8bf4\u8bdd\u4eba\u8bc6\u522b\u7cfb\u7edf\u4ec5\u80fd\u5b8c\u6210\u5206\u7c7b\u4efb\u52a1\uff0c\u65e0\u6cd5\u751f\u6210\u8be6\u7ec6\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\u6216\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u63cf\u8ff0\uff0c\u5c24\u5176\u662f\u5728\u63d0\u53d6\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\uff08\u5982\u65b9\u8a00\u3001\u6027\u522b\u3001\u5e74\u9f84\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "CoLMbo\u6574\u5408\u4e86\u8bf4\u8bdd\u4eba\u7f16\u7801\u5668\u548c\u63d0\u793a\u6761\u4ef6\uff0c\u901a\u8fc7\u7528\u6237\u5b9a\u4e49\u7684\u63d0\u793a\u52a8\u6001\u9002\u5e94\u65b0\u7684\u8bf4\u8bdd\u4eba\u7279\u5f81\uff0c\u751f\u6210\u5b9a\u5236\u5316\u7684\u63cf\u8ff0\uff0c\u5305\u62ec\u65b9\u8a00\u53d8\u4f53\u548c\u5e74\u9f84\u76f8\u5173\u7279\u5f81\u3002", "result": "CoLMbo\u4e0d\u4ec5\u63d0\u5347\u4e86\u4f20\u7edf\u8bf4\u8bdd\u4eba\u5206\u6790\u80fd\u529b\uff0c\u8fd8\u5728\u96f6\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u6570\u636e\u96c6\u3002", "conclusion": "CoLMbo\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u751f\u6210\u8be6\u7ec6\u8bf4\u8bdd\u4eba\u63cf\u8ff0\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528RGB\u56fe\u50cf\u3001\u591a\u5149\u8c31\u6570\u636e\u548cDSM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u4fe1\u606f\u611f\u77e5\u81ea\u9002\u5e94\u63a9\u7801\u7b56\u7565\u3001\u8de8\u6a21\u6001\u63a9\u7801\u673a\u5236\u548c\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u76ee\u6807\uff0c\u6355\u6349\u591a\u6a21\u6001\u95f4\u7684\u76f8\u5173\u6027\u548c\u5355\u6a21\u6001\u7279\u5f81\u7ed3\u6784\u3002", "result": "\u572815\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u768426\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982Potsdam\u548cVaihingen\u8bed\u4e49\u5206\u5272\u4efb\u52a1mIoU\u8fbe78.30%\u548c76.50%\uff0cUS3D\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1RMSE\u964d\u81f30.182\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u9065\u611f\u56fe\u50cf\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09381", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u81ea\u52a8\u533a\u5206\u4f4e\u8d28\u91cf\u4e0e\u9ad8\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\uff0c\u8bc4\u4f30\u4e8612\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u548c\u5fae\u8c03DistilBERT\u5747\u8868\u73b0\u826f\u597d\uff0c\u4f46\u540e\u8005\u8bad\u7ec3\u65f6\u95f4\u66f4\u957f\u3002", "motivation": "\u5728\u7ebf\u65b0\u95fb\u7684\u6cdb\u6ee5\u5bfc\u81f4\u4f4e\u8d28\u91cf\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u5e7f\u6cdb\u4f20\u64ad\uff0c\u7814\u7a76\u65e8\u5728\u81ea\u52a8\u533a\u5206\u5176\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u5305\u542b57,544,214\u6761\u65b0\u95fb\u6807\u9898/\u94fe\u63a5\u7684\u5e73\u8861\u6570\u636e\u96c6\uff0c\u63d0\u53d6115\u79cd\u8bed\u8a00\u7279\u5f81\uff0c\u8bc4\u4f3012\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5305\u62ec\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u4f20\u7edf\u96c6\u6210\u65b9\u6cd5\uff08\u5982bagging\u5206\u7c7b\u5668\uff09\u8868\u73b0\u826f\u597d\uff0888.1%\u51c6\u786e\u7387\uff09\uff0c\u5fae\u8c03DistilBERT\u51c6\u786e\u7387\u6700\u9ad8\uff0890.3%\uff09\uff0c\u4f46\u8bad\u7ec3\u65f6\u95f4\u66f4\u957f\u3002", "conclusion": "NLP\u7279\u5f81\u7ed3\u5408\u4f20\u7edf\u5206\u7c7b\u5668\u6216\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5747\u53ef\u6709\u6548\u533a\u5206\u65b0\u95fb\u8d28\u91cf\uff0c\u4f46\u9700\u6743\u8861\u9884\u6d4b\u6027\u80fd\u4e0e\u8bad\u7ec3\u65f6\u95f4\u3002"}}
{"id": "2506.09343", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8e\u624b\u518c\u7684\u5bb6\u7535\u64cd\u4f5c\u57fa\u51c6CheckManual\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u8f85\u52a9\u751f\u6210\u624b\u518c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u76f8\u5173\u6311\u6218\u3001\u6307\u6807\u548c\u4eff\u771f\u73af\u5883\u3002", "motivation": "\u5bb6\u7535\u7684\u6b63\u786e\u4f7f\u7528\u663e\u8457\u63d0\u5347\u751f\u6d3b\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5145\u5206\u5229\u7528\u624b\u518c\u4fe1\u606f\uff0c\u7f3a\u4e4f\u591a\u9875\u624b\u518c\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u5927\u6a21\u578b\u8f85\u52a9\u7684\u4eba\u5de5\u4fee\u8ba2\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u521b\u5efa\u57fa\u4e8eCAD\u6a21\u578b\u7684\u624b\u518c\uff0c\u5e76\u5efa\u7acb\u624b\u518c\u64cd\u4f5c\u6311\u6218\u3001\u6307\u6807\u548c\u4eff\u771f\u73af\u5883\u3002", "result": "\u63d0\u51fa\u4e86\u9996\u4e2a\u624b\u518c\u64cd\u4f5c\u89c4\u5212\u6a21\u578bManualPlan\uff0c\u4e3aCheckManual\u57fa\u51c6\u8bbe\u7acb\u57fa\u7ebf\u3002", "conclusion": "CheckManual\u4e3a\u5bb6\u7535\u64cd\u4f5c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u65b9\u5411\uff0cManualPlan\u5c55\u793a\u4e86\u624b\u518c\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.09391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793c\u8c8c\u8bed\u8a00\u4f7f\u7528\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\uff0c\u867d\u7136\u5927\u6a21\u578b\u80fd\u590d\u73b0\u8ba1\u7b97\u8bed\u7528\u5b66\u4e2d\u7684\u5173\u952e\u504f\u597d\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u8d1f\u9762\u793c\u8c8c\u7b56\u7565\u53ef\u80fd\u5bfc\u81f4\u8bef\u89e3\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u80fd\u5728\u793c\u8c8c\u8bed\u8a00\u4f7f\u7528\u4e0a\u50cf\u4eba\u7c7b\u4e00\u6837\u7075\u6d3b\u5e94\u5bf9\u4e0d\u540c\u8bed\u5883\uff0c\u5e73\u8861\u4fe1\u606f\u6027\u548c\u793e\u4ea4\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4eba\u7c7b\u548cLLM\u5728\u7ea6\u675f\u6027\u548c\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u7684\u56de\u7b54\uff0c\u5206\u6790\u5176\u793c\u8c8c\u7b56\u7565\u7684\u4f7f\u7528\u3002", "result": "\u5927\u6a21\u578b\uff08\u226570B\u53c2\u6570\uff09\u80fd\u590d\u73b0\u8ba1\u7b97\u8bed\u7528\u5b66\u504f\u597d\uff0c\u4f46\u5728\u6b63\u9762\u8bed\u5883\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u8d1f\u9762\u793c\u8c8c\u7b56\u7565\u3002\u4eba\u7c7b\u8bc4\u4f30\u8005\u66f4\u504f\u597dLLM\u7684\u5f00\u653e\u6027\u56de\u7b54\u3002", "conclusion": "LLMs\u5728\u793c\u8c8c\u7b56\u7565\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u4e0e\u4eba\u7c7b\u8bed\u7528\u7684\u5fae\u5999\u5dee\u5f02\u5f15\u53d1\u4e86\u5bf9AI\u7cfb\u7edf\u8bed\u7528\u5bf9\u9f50\u7684\u601d\u8003\u3002"}}
{"id": "2506.09345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u548c\u9884\u6d4b\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u7ade\u8d5b\u4e2d\u53d6\u5f97\u4e86Top-1 99%\u548cTop-5 100%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u4e09\u6a21\u6001\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u4f18\u5316\u6570\u636e\u589e\u5f3a\u6280\u672f\u6269\u5c55\u6570\u636e\u89c4\u6a21\uff0c\u5229\u7528RGB\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u54082D CNNs\u548cTSM\u63d0\u53d6\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u91c7\u7528SWA\u3001Ensemble\u548cTTA\u7b49\u65b9\u6cd5\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u5728\u7ade\u8d5b\u4e2d\u5b9e\u73b0\u4e86Top-1 99%\u548cTop-5 100%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u591a\u6a21\u6001\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.09393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u6811\u7684\u6982\u7387\u77e5\u8bc6\u8ffd\u8e2a\u6846\u67b6\uff08KT$^2$\uff09\uff0c\u7528\u4e8e\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u901a\u8fc7\u5c42\u6b21\u5316\u77e5\u8bc6\u6982\u5ff5\u63d0\u5347\u5b66\u751f\u8868\u73b0\u9884\u6d4b\u3002", "motivation": "\u73b0\u5b9e\u8bfe\u5802\u4e2d\u77e5\u8bc6\u8ffd\u8e2a\u5e38\u9762\u4e34\u6570\u636e\u7a00\u758f\u548c\u5728\u7ebf\u66f4\u65b0\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u91c7\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6811\u6a21\u578b\u5efa\u6a21\u5c42\u6b21\u5316\u77e5\u8bc6\u6982\u5ff5\uff0c\u901a\u8fc7EM\u7b97\u6cd5\u4f30\u8ba1\u5b66\u751f\u638c\u63e1\u7a0b\u5ea6\uff0c\u5e76\u652f\u6301\u589e\u91cf\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eKT$^2$\u5728\u4f4e\u8d44\u6e90\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "KT$^2$\u901a\u8fc7\u5229\u7528\u5c42\u6b21\u5316\u77e5\u8bc6\u6982\u5ff5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u77e5\u8bc6\u8ffd\u8e2a\u95ee\u9898\u3002"}}
{"id": "2506.09350", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u56de\u5f52\u5bf9\u6297\u540e\u8bad\u7ec3\uff08AAPT\uff09\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u8f6c\u5316\u4e3a\u5b9e\u65f6\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u5668\uff0c\u652f\u6301\u5355\u6b65\u751f\u6210\u548c\u4ea4\u4e92\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u548c\u4ea4\u4e92\u5e94\u7528\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u5bf9\u6297\u8bad\u7ec3\uff0c\u5355\u6b65\u751f\u6210\u6f5c\u5728\u5e27\uff0c\u5e76\u5229\u7528KV\u7f13\u5b58\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u901a\u8fc7\u5b66\u751f\u5f3a\u5236\u8bad\u7ec3\u51cf\u5c11\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "8B\u6a21\u578b\u5728\u5355H100\u4e0a\u5b9e\u73b024fps\u3001736x416\u5206\u8fa8\u7387\u7684\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u6216\u57288xH100\u4e0a\u652f\u63011280x720\u5206\u8fa8\u7387\u957f\u8fbe1\u5206\u949f\u7684\u89c6\u9891\u751f\u6210\u3002", "conclusion": "AAPT\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u548c\u4ea4\u4e92\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09408", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToken Constraint Decoding (TCD)\u7684\u63a8\u7406\u65f6\u7b97\u6cd5\uff0c\u901a\u8fc7\u589e\u5f3atoken\u7ea7\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTCD\u80fd\u663e\u8457\u6062\u590d\u56e0\u8f93\u5165\u566a\u58f0\u800c\u4e0b\u964d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u9009\u9898\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u8f93\u5165\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u63d0\u5347\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTCD\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236token\u7ea7\u9884\u6d4b\u5bf9\u9f50\u6765\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u5e76\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cTCD\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5bf9\u8f83\u5f31\u6a21\u578b\u6548\u679c\u66f4\u660e\u663e\uff0c\u6700\u9ad8\u63d0\u534739%\u3002", "conclusion": "TCD\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u53ef\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u4e0d\u5b8c\u7f8e\u6761\u4ef6\u4e0b\u7684\u63a8\u7406\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.09357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u6846\u67b6\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\u76842D\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f62\u72b6\u5206\u6790\u548cLDDMM\u6846\u67b6\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u6216\u6709\u9650\u7075\u6d3b\u6027\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u7406\u8bba\u624e\u5b9e\u4e14\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "\u5229\u7528LDDMM\u6846\u67b6\u548c\u5fae\u5206\u540c\u80da\u53d8\u6362\uff0c\u901a\u8fc7\u6a21\u677f\u66f2\u7ebf\u7684\u53d8\u5f62\u5b9e\u73b0\u5206\u5272\uff0c\u7ed3\u5408\u53d8\u5206\u8868\u793a\u548c\u56fe\u50cf\u68af\u5ea6\u573a\u4f18\u5316\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u56fe\u50cf\u5206\u5272\uff0c\u65b9\u6cd5\u7075\u6d3b\u4e14\u7406\u8bba\u652f\u6301\u5f3a\uff0c\u9002\u7528\u4e8e\u5c0f\u6570\u636e\u96c6\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09414", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "PGDA-KGQA\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u5f15\u5bfc\u7684\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u79cd\u6570\u636e\u589e\u5f3a\u7b56\u7565\u89e3\u51b3KGQA\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u591a\u8df3\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3KGQA\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u591a\u8df3\u63a8\u7406\u6837\u672c\u4e0d\u8db3\u4ee5\u53ca\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5bfc\u81f4\u7684\u8bed\u4e49\u5931\u771f\u95ee\u9898\u3002", "method": "\u91c7\u7528\u63d0\u793a\u5f15\u5bfc\u7684\u751f\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5355\u8df3\u4f2a\u95ee\u9898\u751f\u6210\u3001\u8bed\u4e49\u4fdd\u7559\u95ee\u9898\u91cd\u5199\u548c\u7b54\u6848\u5f15\u5bfc\u7684\u53cd\u5411\u8def\u5f84\u63a2\u7d22\uff0c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u3002", "result": "\u5728WebQSP\u548cComplexWebQuestions\u6570\u636e\u96c6\u4e0a\uff0cF1\u3001Hits@1\u548c\u51c6\u786e\u7387\u5206\u522b\u63d0\u53472.8%\u30011.2%\u30013.1%\u548c1.8%\u30011.1%\u30012.4%\u3002", "conclusion": "PGDA-KGQA\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86KGQA\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u8df3\u63a8\u7406\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2506.09363", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u64e6\u9664\u548c\u5168\u5c40-\u5c40\u90e8\u534f\u4f5c\u4fdd\u7559\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u654f\u611f\u4fe1\u606f\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\u548c\u7248\u6743\u4fb5\u6743\u98ce\u9669\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u4e0d\u5b89\u5168\u6982\u5ff5\u89c6\u4e3a\u56fa\u5b9a\u8bcd\u91cd\u590d\u64e6\u9664\uff0c\u9677\u5165\u201c\u8bcd\u6982\u5ff5\u6df1\u6e0a\u201d\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5e7f\u4e49\u6982\u5ff5\u64e6\u9664\u3002", "method": "SAGE\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u64e6\u9664\u5c06\u6982\u5ff5\u8bcd\u64e6\u9664\u8f6c\u5316\u4e3a\u6982\u5ff5\u57df\u64e6\u9664\uff0c\u5229\u7528\u5faa\u73af\u81ea\u68c0\u548c\u81ea\u64e6\u9664\u63a2\u7d22\u6982\u5ff5\u57df\u8fb9\u754c\u8868\u793a\uff1b\u540c\u65f6\u63d0\u51fa\u5168\u5c40-\u5c40\u90e8\u534f\u4f5c\u4fdd\u7559\u673a\u5236\uff0c\u7ed3\u5408\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u5bf9\u9f50\u548c\u5c40\u90e8\u9884\u6d4b\u566a\u58f0\u4fdd\u7559\uff0c\u51cf\u5c11\u65e0\u5173\u6982\u5ff5\u4fdd\u7559\u9000\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSAGE\u5728\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u751f\u6210\u65b9\u9762\u5168\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "SAGE\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u751f\u6210\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6743\u91cd\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u7684\u81ea\u52a8\u6b3a\u9a97\u68c0\u6d4b\u80fd\u529b\uff0c\u53d1\u73b0\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800cLMMs\u5728\u591a\u6a21\u6001\u7ebf\u7d22\u5229\u7528\u4e0a\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u5728\u6570\u5b57\u5316\u4e16\u754c\u4e2d\uff0c\u6b3a\u9a97\u68c0\u6d4b\u662f\u4e00\u9879\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u548cLMMs\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff08RLTD\u3001MU3D\u3001OpSpam\uff09\uff0c\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u8f85\u52a9\u7279\u5f81\u548c\u63d0\u793a\u7b56\u7565\u7684\u5f71\u54cd\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\u5728\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u800cLMMs\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6a21\u6001\u7ebf\u7d22\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2506.09369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScaleLSD\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u7684\u7ebf\u6bb5\u68c0\u6d4b\uff08LSD\uff09\uff0c\u5728\u81ea\u7136\u56fe\u50cf\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u975e\u6df1\u5ea6\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u662f\u5b66\u4e60\u4e00\u79cd\u9886\u57df\u65e0\u5173\u7684\u9c81\u68d2LSD\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u81ea\u7136\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5b9e\u73b0\u5927\u89c4\u6a21\u7ebf\u6bb5\u51e0\u4f55\u8868\u5f81\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u5e76\u7b80\u5316\u4e86\u6df1\u5ea6\u548c\u975e\u6df1\u5ea6LSD\u65b9\u6cd5\u7684\u57fa\u7840\u8bbe\u8ba1\uff0c\u63d0\u51faScaleLSD\uff0c\u5229\u7528\u8d85\u8fc71000\u4e07\u5f20\u672a\u6807\u8bb0\u7684\u771f\u5b9e\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "ScaleLSD\u5728\u96f6\u6837\u672c\u68c0\u6d4b\u6027\u80fd\u3001\u5355\u89c6\u56fe3D\u51e0\u4f55\u4f30\u8ba1\u3001\u53cc\u89c6\u56fe\u7ebf\u6bb5\u5339\u914d\u53ca\u591a\u89c6\u56fe3D\u7ebf\u6620\u5c04\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5728\u5404\u65b9\u9762\u8d85\u8d8a\u4f20\u7edf\u975e\u6df1\u5ea6\u65b9\u6cd5\u3002", "conclusion": "ScaleLSD\u663e\u8457\u6269\u5c55\u5e76\u5f3a\u5316\u4e86\u56fe\u50cf\u7ebf\u6bb5\u51e0\u4f55\u7684\u901a\u7528\u6027\uff0c\u6210\u4e3a\u9996\u4e2a\u5728\u5404\u65b9\u9762\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2506.09428", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684SFT\u65b9\u6cd5\uff0c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\uff0c\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SFT\u5bfc\u81f4\u7684\u901a\u7528\u80fd\u529b\u4e0b\u964d\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u91cd\u6784\u57fa\u7840\u6a21\u578b\u7684\u6307\u4ee4\u5206\u5e03\uff0c\u591a\u6a21\u578b\u7b5b\u9009\u6570\u636e\uff0c\u6df7\u5408\u65b0\u6570\u636e\u8fdb\u884cSFT\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5728\u4fdd\u7559\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u4e14\u6210\u672c\u4f4e\uff0c\u9002\u7528\u4e8e\u7b2c\u4e09\u65b9\u5b9e\u8df5\u8005\u3002"}}
{"id": "2506.09378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUniForward\u7684\u524d\u9988\u9ad8\u65af\u6563\u5c04\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e003D\u573a\u666f\u548c\u8bed\u4e49\u573a\u91cd\u5efa\uff0c\u4ec5\u9700\u672a\u6821\u51c6\u7684\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u5373\u53ef\u5b9e\u73b0\u5b9e\u65f6\u91cd\u5efa\u3002", "motivation": "\u7ed3\u54083D\u573a\u666f\u4e0e\u8bed\u4e49\u573a\u6709\u52a9\u4e8e\u73af\u5883\u611f\u77e5\u548c\u7406\u89e3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u76f8\u673a\u53c2\u6570\u6216\u6df1\u5ea6\u771f\u503c\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "\u901a\u8fc7\u53cc\u5206\u652f\u89e3\u8026\u89e3\u7801\u5668\u5c06\u8bed\u4e49\u7279\u5f81\u5d4c\u51653D\u9ad8\u65af\u4e2d\uff0c\u4f7f\u7528\u635f\u5931\u5f15\u5bfc\u7684\u89c6\u56fe\u91c7\u6837\u5668\u4f18\u5316\u8bad\u7ec3\uff0c\u65e0\u9700\u6df1\u5ea6\u6216\u63a9\u7801\u771f\u503c\u3002", "result": "\u6a21\u578b\u80fd\u5b9e\u65f6\u91cd\u5efa\u9ad8\u8d28\u91cf3D\u573a\u666f\u548c\u8bed\u4e49\u573a\uff0c\u652f\u6301\u4efb\u610f\u89c6\u89d2\u7684\u8bed\u4e49\u7279\u5f81\u6e32\u67d3\u548c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u3002", "conclusion": "UniForward\u57283D\u573a\u666f\u4e0e\u8bed\u4e49\u573a\u7edf\u4e00\u91cd\u5efa\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2506.09440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86GigaChat\u7cfb\u5217\u4fc4\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u7248\u672c\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86\u67b6\u6784\u3001\u9884\u8bad\u7ec3\u8fc7\u7a0b\u53ca\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5e76\u5728\u4fc4\u8bed\u548c\u82f1\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u6027\u80fd\uff0c\u4e0e\u591a\u8bed\u8a00\u6a21\u578b\u5bf9\u6bd4\u3002", "motivation": "\u7531\u4e8e\u4fc4\u8bed\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u8d44\u6e90\u9700\u6c42\u5927\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u4fc4\u8bedNLP\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u67b6\u6784\uff0c\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\uff0c\u901a\u8fc7\u5b9e\u9a8c\u4f18\u5316\u8bbe\u8ba1\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u6027\u80fd\u3002", "result": "GigaChat\u5728\u4fc4\u8bed\u548c\u82f1\u8bed\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u901a\u8fc7API\u3001Telegram\u673a\u5668\u4eba\u548cWeb\u754c\u9762\u63d0\u4f9b\u7cfb\u7edf\u6f14\u793a\uff0c\u540c\u65f6\u5f00\u6e90\u4e09\u4e2a\u6a21\u578b\u3002", "conclusion": "GigaChat\u7cfb\u5217\u6a21\u578b\u4e3a\u4fc4\u8bedNLP\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u652f\u6301\u7814\u7a76\u548c\u5de5\u4e1a\u5e94\u7528\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u3002"}}
{"id": "2506.09385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09385", "abs": "https://arxiv.org/abs/2506.09385", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "comment": null, "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u95ee\u9898\uff08OM-ReID\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u96c6ORBench\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6ReID5o\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u6709\u9650\u6a21\u6001\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u573a\u666f\u4e2d\u591a\u6a21\u6001\u67e5\u8be2\u7684\u9700\u6c42\u3002", "method": "\u6784\u5efaORBench\u6570\u636e\u96c6\uff08\u5305\u542b5\u79cd\u6a21\u6001\uff09\uff0c\u5e76\u63d0\u51faReID5o\u6846\u67b6\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u878d\u5408\u4e0e\u5bf9\u9f50\u3002", "result": "ORBench\u6570\u636e\u96c6\u5177\u6709\u591a\u6837\u6027\u4f18\u52bf\uff0cReID5o\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "ORBench\u548cReID5o\u4e3a\u591a\u6a21\u6001\u884c\u4eba\u91cd\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u60f3\u5e73\u53f0\u548c\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09450", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "UniToMBench\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408SimToM\u548cTOMBENCH\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u591a\u4ea4\u4e92\u4efb\u52a1\u8bbe\u8ba1\u548c\u52a8\u6001\u6545\u4e8b\u573a\u666f\uff0c\u7cfb\u7edf\u8bc4\u4f30\u548c\u6539\u8fdbLLMs\u7684\u5fc3\u7406\u7406\u8bba\u80fd\u529b\u3002", "motivation": "\u5fc3\u7406\u7406\u8bba\uff08ToM\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u4eba\u7c7b\u5fc3\u7406\u72b6\u6001\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "UniToMBench\u6574\u5408\u4e86\u89c6\u89d2\u91c7\u6280\u672f\u548c\u591a\u6837\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u57fa\u4e8e1,000\u591a\u4e2a\u624b\u5de5\u7f16\u5199\u7684\u60c5\u666f\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u591a\u4ea4\u4e92\u4efb\u52a1\u548c\u52a8\u6001\u6545\u4e8b\u573a\u666f\u3002", "result": "GPT-4o\u548cGPT-4o Mini\u5728\u60c5\u611f\u548c\u4fe1\u5ff5\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u51c6\u786e\u7387>80%\uff09\uff0c\u4f46\u5728\u77e5\u8bc6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "conclusion": "UniToMBench\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728ToM\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2506.09399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09399", "abs": "https://arxiv.org/abs/2506.09399", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "comment": null, "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u5148\u9a8c\u51e0\u4f55\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u66f4\u65b0\u534f\u65b9\u5dee\u77e9\u9635\u6765\u7ea0\u6b63\u4e0d\u826f\u5206\u5e03\u6837\u672c\u7684\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b50\u7a7a\u95f4\u7684\u65b9\u6cd5\u56e0\u9759\u6001\u63d0\u53d6\u4fe1\u606f\u51e0\u4f55\u800c\u65e0\u6cd5\u5904\u7406\u4e0d\u826f\u5206\u5e03\u6837\u672c\u5bfc\u81f4\u7684\u51e0\u4f55\u5931\u771f\uff0c\u9700\u8981\u52a8\u6001\u8c03\u6574\u5148\u9a8c\u51e0\u4f55\u3002", "method": "\u52a8\u6001\u66f4\u65b0\u5148\u9a8c\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u6cbf\u5b9e\u65f6\u8f93\u5165\u7279\u5f81\u65b9\u5411\u51cf\u5c11\u534f\u65b9\u5dee\uff0c\u5e76\u5728\u6b8b\u5dee\u7a7a\u95f4\u4e2d\u7ea6\u675f\u8c03\u6574\uff0c\u4fdd\u7559\u5173\u952e\u6570\u636e\u7279\u5f81\u3002", "result": "\u5728CIFAR\u548cImageNet-1k\u6570\u636e\u96c6\u4e0a\uff0c\u5305\u62ec\u81ea\u76d1\u7763DINO\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u52a8\u6001\u8c03\u6574\u5148\u9a8c\u51e0\u4f55\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u826f\u5206\u5e03\u6837\u672c\u7684\u5f71\u54cd\uff0c\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.09457", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOET\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u622a\u65ad\u504f\u597d\u548c\u975e\u504f\u597d\u54cd\u5e94\u81f3\u76f8\u540c\u957f\u5ea6\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u5bf9\u9f50\u7b97\u6cd5\uff08DAAs\uff09\u4e2d\u7684\u5956\u52b1\u751f\u6210\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u76f4\u63a5\u5bf9\u9f50\u7b97\u6cd5\uff08\u5982DPO\u548cSimPO\uff09\u5728\u8bad\u7ec3\u76ee\u6807\u4e0e\u63a8\u7406\u751f\u6210\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\uff08\u5956\u52b1\u751f\u6210\u5dee\u8ddd\uff09\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faPrefix-Oriented Equal-length Training\uff08POET\uff09\uff0c\u901a\u8fc7\u622a\u65ad\u54cd\u5e94\u81f3\u76f8\u540c\u957f\u5ea6\uff0c\u4f18\u5316DAAs\u76ee\u6807\uff0c\u4f7f\u5176\u66f4\u5173\u6ce8\u524d\u7f00\u6807\u8bb0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPOET\u5728DPO\u548cSimPO\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u5b9e\u73b0\uff0cAlpacaEval 2\u63d0\u534715.6\u5206\uff0c\u4e0b\u6e38\u4efb\u52a1\u666e\u904d\u6539\u8fdb\u3002", "conclusion": "POET\u6709\u6548\u89e3\u51b3\u4e86\u5956\u52b1\u4f18\u5316\u4e0e\u751f\u6210\u6027\u80fd\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3aDAAs\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2506.09403", "categories": ["cs.CV", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2506.09403", "abs": "https://arxiv.org/abs/2506.09403", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSegment Anything Model\uff08SAM\uff09\u7684\u6e90\u81ea\u7531\u57df\u9002\u5e94\u65b9\u6cd5\uff08SRPL-SFDA\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u4f2a\u6807\u7b7e\u8d28\u91cf\u548c\u53ef\u9760\u6027\u63d0\u5347\u76ee\u6807\u57df\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6e90\u81ea\u7531\u57df\u9002\u5e94\uff08SFDA\uff09\u5728\u76ee\u6807\u57df\u65e0\u6807\u7b7e\u6570\u636e\u4e0b\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u62a4\u6e90\u57df\u6570\u636e\u9690\u79c1\u3002", "method": "1\uff09\u4e09\u5206\u652f\u5f3a\u5ea6\u589e\u5f3a\uff08T3IE\uff09\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff1b2\uff09\u57fa\u4e8eSAM\u8f93\u51fa\u4e00\u81f4\u6027\u7684\u53ef\u9760\u4f2a\u6807\u7b7e\u9009\u62e9\uff1b3\uff09\u53ef\u9760\u6027\u611f\u77e5\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709SFDA\u65b9\u6cd5\uff0c\u63a5\u8fd1\u76ee\u6807\u57df\u6709\u76d1\u7763\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "SRPL-SFDA\u6709\u6548\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\u548cSFDA\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u7684\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2506.09495", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790YouTube\u4e0a\u81ea\u6740\u672a\u9042\u8005\u7684\u89c6\u9891\u5185\u5bb9\uff0c\u7ed3\u5408\u8ba1\u7b97\u6a21\u578b\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u63ed\u793a\u4e86\u81ea\u6740\u884c\u4e3a\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u8868\u73b0\u53ca\u5176\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u7684\u5dee\u5f02\u3002", "motivation": "\u81ea\u6740\u662f\u897f\u65b9\u56fd\u5bb6\u7684\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4e3a\u7814\u7a76\u81ea\u6740\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u8ba1\u7b97\u81ea\u4e0b\u800c\u4e0a\u3001\u6df7\u5408\u548c\u4e13\u5bb6\u81ea\u4e0a\u800c\u4e0b\u4e09\u79cd\u65b9\u6cd5\uff0c\u5206\u6790181\u4e2a\u81ea\u6740\u672a\u9042\u8005\u548c134\u4e2a\u5bf9\u7167\u8005\u7684YouTube\u9891\u9053\u6570\u636e\u3002", "result": "\u53d1\u73b0\u4e94\u4e2a\u4e0e\u81ea\u6740\u672a\u9042\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u5176\u4e2d\u4e24\u4e2a\u968f\u65f6\u95f4\u53d8\u5316\u663e\u8457\uff1b\u4e13\u5bb6\u672a\u8bc6\u522b\u7684\u5e73\u53f0\u7279\u5b9a\u6307\u6807\uff08\u5982YouTube\u53c2\u4e0e\u5ea6\uff09\u5177\u6709\u4ef7\u503c\u3002", "conclusion": "\u7efc\u5408\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5bf9\u81ea\u6740\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u8fde\u63a5\u4e86\u6570\u5b57\u884c\u4e3a\u4e0e\u4e34\u5e8a\u77e5\u8bc6\u3002"}}
{"id": "2506.09411", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09411", "abs": "https://arxiv.org/abs/2506.09411", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "comment": null, "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u59ff\u6001\u8fc1\u79fb\u7684\u5408\u6210\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u3002", "motivation": "\u5408\u6210\u6570\u636e\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5e38\u56e0\u4e0d\u81ea\u7136\u7279\u5f81\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\uff0c\u9650\u5236\u4e86\u5176\u5728\u624b\u52bf\u8bc6\u522b\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u53ef\u63a73D\u9ad8\u65af\u865a\u62df\u6a21\u578b\u8fdb\u884c\u59ff\u6001\u8fc1\u79fb\uff0c\u751f\u6210\u5408\u6210\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u3002", "result": "\u5728Toyota Smarthome\u548cNTU RGB+D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u80fd\u63d0\u5347\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u5e76\u6269\u5c55\u5c11\u6837\u672c\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u5f25\u8865\u771f\u5b9e\u6570\u636e\u4e0d\u8db3\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.09501", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u53ef\u91cd\u590d\u6027\u8106\u5f31\uff0c\u7cfb\u7edf\u914d\u7f6e\uff08\u5982GPU\u6570\u91cf\u3001\u7248\u672c\u548c\u8bc4\u4f30\u6279\u6b21\u5927\u5c0f\uff09\u4f1a\u5bfc\u81f4\u663e\u8457\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u3002", "motivation": "\u63a2\u8ba8LLM\u6027\u80fd\u8bc4\u4f30\u7684\u53ef\u91cd\u590d\u6027\u95ee\u9898\uff0c\u63ed\u793a\u6570\u503c\u7cbe\u5ea6\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u786c\u4ef6\u3001\u8f6f\u4ef6\u548c\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u6a21\u578b\u8f93\u51fa\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u63a8\u7406\u7ba1\u9053LayerCast\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u63a8\u7406\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u957f\u5ea6\u56e0\u914d\u7f6e\u5dee\u5f02\u53ef\u53d8\u53169%\u548c9,000\u4e2atoken\uff0c\u6839\u6e90\u5728\u4e8e\u6d6e\u70b9\u8fd0\u7b97\u7684\u975e\u7ed3\u5408\u6027\u3002", "conclusion": "\u6570\u503c\u7cbe\u5ea6\u5bf9LLM\u63a8\u7406\u7684\u53ef\u91cd\u590d\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e38\u88ab\u5ffd\u89c6\uff1bLayerCast\u5728\u5185\u5b58\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2506.09416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09416", "abs": "https://arxiv.org/abs/2506.09416", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Noise Conditional Variational Score Distillation", "comment": null, "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "AI": {"tldr": "NCVSD\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u84b8\u998f\u4e3a\u751f\u6210\u53bb\u566a\u5668\uff0c\u901a\u8fc7\u63ed\u793a\u65e0\u6761\u4ef6\u8bc4\u5206\u51fd\u6570\u9690\u542b\u5730\u8868\u5f81\u53bb\u566a\u540e\u9a8c\u5206\u5e03\u7684\u8bc4\u5206\u51fd\u6570\uff0c\u5b9e\u73b0\u5feb\u901f\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u84b8\u998f\u6269\u6563\u6a21\u578b\uff0c\u6784\u5efa\u751f\u6210\u53bb\u566a\u5668\uff0c\u4ee5\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u751f\u6210\u548c\u9ad8\u6837\u672c\u8d28\u91cf\u3002", "method": "\u5c06\u65e0\u6761\u4ef6\u8bc4\u5206\u51fd\u6570\u7684\u6d1e\u5bdf\u878d\u5165VSD\u6846\u67b6\uff0c\u5b66\u4e60\u751f\u6210\u53bb\u566a\u5668\uff0c\u652f\u6301\u4ece\u9ad8\u566a\u58f0\u6c34\u5e73\u5230\u4f4e\u566a\u58f0\u6c34\u5e73\u7684\u540e\u9a8c\u5206\u5e03\u91c7\u6837\u3002", "result": "NCVSD\u5728\u7c7b\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u548c\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u6559\u5e08\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4e0e\u66f4\u5927\u89c4\u6a21\u7684Consistency\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "NCVSD\u901a\u8fc7\u5feb\u901f\u751f\u6210\u548c\u591a\u6b65\u91c7\u6837\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u548c\u6837\u672c\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u7075\u6d3b\u53ef\u63a7\u7684\u91c7\u6837\u4efb\u52a1\u3002"}}
{"id": "2506.09507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u65b9\u6cd5\uff08RoPE\uff09\uff0c\u89e3\u51b3\u4e86Transformer\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u5728\u4f4d\u7f6e\u7f16\u7801\u4e0a\u7684\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u67b6\u6784\uff08model\uff09\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6Transformer\u3002", "motivation": "Transformer\u548cSSM\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u5404\u6709\u4f18\u52bf\uff0c\u4f46\u4e24\u8005\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u4e0d\u517c\u5bb9\uff0c\u5bfc\u81f4\u6df7\u5408\u67b6\u6784\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684RoPE\u65b9\u6cd5\uff0c\u4e3a\u81ea\u6ce8\u610f\u529b\u548c\u72b6\u6001\u7a7a\u95f4\u7ec4\u4ef6\u63d0\u4f9b\u4e00\u81f4\u7684\u4f4d\u7f6e\u7f16\u7801\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u6df7\u5408\u67b6\u6784model\u3002", "result": "\u57284K\u5e8f\u5217\u957f\u5ea6\u4e0b\uff0cmodel\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5206\u522b\u6bd4\u6807\u51c6Transformer\u5feb42.3%\u548c29.5%\uff0c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u63d0\u53474%\u4ee5\u4e0a\uff0c\u4e14\u6269\u5c55\u6027\u66f4\u5f3a\u3002", "conclusion": "\u7edf\u4e00\u7684\u4f4d\u7f6e\u7f16\u7801\u89e3\u51b3\u4e86\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u4f4d\u7f6e\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002"}}
{"id": "2506.09417", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09417", "abs": "https://arxiv.org/abs/2506.09417", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "title": "ODG: Occupancy Prediction Using Dual Gaussians", "comment": null, "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BEV\u548c\u7a00\u758f\u70b9\u8868\u793a\u7684\u65b0\u578b3D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5ODG\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u8bbe\u8ba1\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u67093D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0cBEV\u5bf9\u5c0f\u7269\u4f53\u8868\u73b0\u4e0d\u4f73\uff0c\u7a00\u758f\u70b9\u5bf9\u5927\u7269\u4f53\u6216\u5e73\u9762\u6548\u7387\u4f4e\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u8bbe\u8ba1\uff1a\u57fa\u4e8e\u67e5\u8be2\u7684\u7a00\u758f\u70b9\u5206\u652f\u548cBEV\u5206\u652f\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5171\u4eab\u4fe1\u606f\uff0c\u6700\u7ec8\u878d\u5408\u8f93\u51fa\u9884\u6d4b\u76843D\u5360\u7528\u3002", "result": "\u5728Occ3D-nuScenes\u548cOcc3D-Waymo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u63a8\u7406\u901f\u5ea6\u4e0e\u6700\u65b0\u9ad8\u6548\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ODG\u65b9\u6cd5\u6709\u6548\u7ed3\u5408BEV\u548c\u7a00\u758f\u70b9\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u6027\u3002"}}
{"id": "2506.09513", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "ReasonMed\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u533b\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u548c\u4f18\u5316\u8fc7\u7a0b\u6784\u5efa\uff0c\u7528\u4e8e\u63d0\u5347LLMs\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u3002\u7ed3\u5408\u8be6\u7ec6\u63a8\u7406\u548c\u7b80\u6d01\u7b54\u6848\u6458\u8981\u7684\u8bad\u7ec3\u7b56\u7565\uff0cReasonMed-7B\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u63a8\u7406\u578bLLMs\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u6784\u5efaReasonMed\u6570\u636e\u96c6\uff08370k\u9ad8\u8d28\u91cf\u6837\u672c\uff09\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u548c\u4f18\u5316\u8fc7\u7a0b\uff08\u5305\u62ecError Refiner\uff09\u63d0\u5347\u63a8\u7406\u8def\u5f84\u8d28\u91cf\uff0c\u5e76\u7814\u7a76\u6700\u4f73\u8bad\u7ec3\u7b56\u7565\uff08\u7ed3\u5408\u8be6\u7ec6\u63a8\u7406\u4e0e\u7b80\u6d01\u7b54\u6848\u6458\u8981\uff09\u3002", "result": "ReasonMed-7B\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\uff08\u63d0\u53474.17%\uff09\uff0c\u5e76\u5728PubMedQA\u4e0a\u8d85\u8fc7LLaMA3.1-70B\uff08\u63d0\u53474.60%\uff09\u3002", "conclusion": "ReasonMed\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u533b\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.09427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09427", "abs": "https://arxiv.org/abs/2506.09427", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "comment": null, "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faInterSyn\u6570\u636e\u96c6\u548cSEIR\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u56fe\u50cf-\u6587\u672c\u4ea4\u7ec7\u751f\u6210\u80fd\u529b\uff0c\u5e76\u5f15\u5165SynJudge\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u751f\u6210\u7d27\u5bc6\u4ea4\u7ec7\u7684\u56fe\u50cf-\u6587\u672c\u8f93\u51fa\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u8d28\u91cf\u548c\u6307\u4ee4\u4e30\u5bcc\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSelf-Evaluation with Iterative Refinement (SEIR)\u65b9\u6cd5\u6784\u5efaInterSyn\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1SynJudge\u8bc4\u4f30\u6a21\u578b\u3002", "result": "SEIR\u663e\u8457\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\uff0cInterSyn\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "InterSyn\u548cSynJudge\u4e3a\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2506.09542", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "KG-Infused RAG\u901a\u8fc7\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548cRAG\u7cfb\u7edf\uff0c\u5229\u7528\u8ba4\u77e5\u542f\u53d1\u7684\u6269\u6563\u6fc0\u6d3b\u673a\u5236\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2aQA\u57fa\u51c6\u4e0a\u4f18\u4e8e\u4f20\u7edfRAG\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u77e5\u8bc6\u6e90\u4e14\u7f3a\u4e4f\u8ba4\u77e5\u542f\u53d1\u7684\u77e5\u8bc6\u6fc0\u6d3b\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51faKG-Infused RAG\u6846\u67b6\uff0c\u6574\u5408KG\u5b9e\u73b0\u6269\u6563\u6fc0\u6d3b\uff0c\u901a\u8fc7\u67e5\u8be2\u6269\u5c55\u548c\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "result": "\u5728\u4e94\u4e2aQA\u57fa\u51c6\u4e0a\uff0cKG-Infused RAG\u6027\u80fd\u63d0\u53473.8%\u81f313.8%\uff0c\u4e14\u80fd\u4f5c\u4e3a\u63d2\u4ef6\u8fdb\u4e00\u6b65\u63d0\u5347Self-RAG\u6548\u679c\u3002", "conclusion": "KG-Infused RAG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u589e\u5f3a\u6a21\u5757\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u8bed\u6599\u7684RAG\u65b9\u6cd5\u3002"}}
{"id": "2506.09429", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09429", "abs": "https://arxiv.org/abs/2506.09429", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "comment": null, "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u964d\u4f4e\u7f16\u7801\u5668\u5c42\u7ef4\u5ea6\u548c\u4f7f\u7528\u84b8\u998f\u7248GPT-2\u89e3\u7801\u5668\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u8fb9\u7f18\u611f\u77e5\u589e\u5f3a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u5ffd\u89c6\u7ec6\u7c92\u5ea6\u7ed3\u6784\u7279\u5f81\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7Transformer\u67b6\u6784\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u8fb9\u7f18\u611f\u77e5\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u589e\u5f3a\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u56fe\u50cf\u63cf\u8ff0\u6027\u80fd\u3002"}}
{"id": "2506.09556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "MEDUSA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u6709\u6548\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u60c5\u611f\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\u7684\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7531\u4e8e\u4eba\u7c7b\u60c5\u611f\u7684\u4e3b\u89c2\u6027\u53ca\u5176\u5728\u81ea\u7136\u6761\u4ef6\u4e0b\u7684\u4e0d\u5747\u8861\u8868\u73b0\uff0c\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u5177\u6709\u6311\u6218\u6027\u3002", "method": "MEDUSA\u91c7\u7528\u56db\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u524d\u4e24\u9636\u6bb5\u8bad\u7ec3\u57fa\u4e8eDeepSER\u7684\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u540e\u4e24\u9636\u6bb5\u4f18\u5316\u53ef\u8bad\u7ec3\u7684\u5143\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u4e86\u591a\u79cd\u6b63\u5219\u5316\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u6280\u672f\u3002", "result": "MEDUSA\u5728Interspeech 2025\u6311\u6218\u8d5b\u7684\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "MEDUSA\u901a\u8fc7\u591a\u6a21\u6001\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u7136\u6761\u4ef6\u4e0b\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09445", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09445", "abs": "https://arxiv.org/abs/2506.09445", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "comment": null, "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTOGA\u6a21\u578b\uff0c\u7528\u4e8e\u5f31\u76d1\u7763\u4e0b\u7684\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\uff0c\u65e0\u9700\u65f6\u95f4\u6807\u6ce8\u5373\u53ef\u751f\u6210\u7b54\u6848\u53ca\u65f6\u95f4\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u5f31\u76d1\u7763\u4e0b\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u65f6\u95f4\u5b9a\u4f4d\u7684\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3002", "method": "\u901a\u8fc7\u751f\u6210\u4f2a\u6807\u7b7e\u5e76\u65bd\u52a0\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u8054\u5408\u751f\u6210\u7b54\u6848\u4e0e\u65f6\u95f4\u5b9a\u4f4d\u3002", "result": "\u5728NExT-GQA\u3001MSVD-QA\u548cActivityNet-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "TOGA\u6a21\u578b\u5728\u5f31\u76d1\u7763\u4e0b\u80fd\u6709\u6548\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u548c\u65f6\u95f4\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09558", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86Google Translate\u548cDeepL\u5728\u82f1\u8bed\u5230\u5e0c\u814a\u8bed\u7ffb\u8bd1\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u6027\u522b\u660e\u786e\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6027\u522b\u672a\u6307\u5b9a\u65f6\u96be\u4ee5\u5b9e\u73b0\u5305\u5bb9\u6027\u6216\u4e2d\u6027\u7ffb\u8bd1\u3002GPT-4o\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u6b8b\u4f59\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u5bf9\u5305\u5bb9\u6027\u8bed\u8a00\u9700\u6c42\u7684\u589e\u52a0\uff0c\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u53ef\u80fd\u5f3a\u5316\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u95ee\u9898\u5f15\u53d1\u5173\u6ce8\u3002", "method": "\u7814\u7a76\u4f7f\u7528GendEL\u6570\u636e\u96c6\uff08240\u4e2a\u53e5\u5b50\uff09\uff0c\u5206\u6790\u4e24\u79cd\u5546\u4e1aMT\u7cfb\u7edf\u548cGPT-4o\u5728\u6027\u522b\u504f\u89c1\u4e09\u4e2a\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u4e24\u79cdMT\u7cfb\u7edf\u5728\u6027\u522b\u660e\u786e\u65f6\u8868\u73b0\u826f\u597d\uff08DeepL\u4f18\u4e8eGoogle Translate\u548cGPT-4o\uff09\uff0c\u4f46\u5728\u6027\u522b\u672a\u6307\u5b9a\u65f6\u8868\u73b0\u4e0d\u4f73\u3002GPT-4o\u5728\u5927\u591a\u6570\u6a21\u7cca\u60c5\u51b5\u4e0b\u751f\u6210\u5408\u9002\u7684\u6027\u522b\u5316\u6216\u4e2d\u6027\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u5546\u4e1aMT\u7cfb\u7edf\u4ecd\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u6027\u522b\u5305\u5bb9\u6027\u7ffb\u8bd1\uff0cGPT-4o\u663e\u793a\u51fa\u6f5c\u529b\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2506.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09446", "abs": "https://arxiv.org/abs/2506.09446", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "comment": null, "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "AI": {"tldr": "HAM\u662f\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u51b2\u7a81\u6837\u672c\u589e\u5f3a\u548c\u6a21\u578b\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u591a\u6e90\u8bad\u7ec3\u4e2d\u7684\u6837\u672c\u548c\u4f18\u5316\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6e90\u8bad\u7ec3\u4e2d\u5b58\u5728\u6837\u672c\u51b2\u7a81\u548c\u4f18\u5316\u51b2\u7a81\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002HAM\u65e8\u5728\u901a\u8fc7\u65e0\u51b2\u7a81\u6837\u672c\u589e\u5f3a\u548c\u6a21\u578b\u5408\u5e76\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "HAM\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u65e0\u51b2\u7a81\u6837\u672c\uff0c\u534f\u8c03\u6a21\u578b\u66f4\u65b0\u65b9\u5411\uff0c\u5e76\u5f15\u5165\u5197\u4f59\u611f\u77e5\u7684\u5386\u53f2\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u6574\u5408\u77e5\u8bc6\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHAM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HAM\u901a\u8fc7\u6709\u6548\u6574\u5408\u591a\u6e90\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86CLIP\u6a21\u578b\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6784\u5efa\u9a6c\u5176\u987f\u8bed\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u3001\u6307\u4ee4\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u5957\u4ef6\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a8B\u53c2\u6570\u7684LLM\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u9a6c\u5176\u987f\u8bed\uff09\u5728LLM\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u6536\u96c640GB\u9a6c\u5176\u987f\u8bed\u8bed\u6599\u5e93\uff083.5B\u8bcd\uff09\u3001106k\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u6784\u5efa\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5e76\u8bad\u7ec38B\u53c2\u6570\u6a21\u578bdomestic-yak\u3002", "result": "domestic-yak\u57288B\u53c2\u6570\u8303\u56f4\u5185\u8868\u73b0\u6700\u4f18\uff0c\u6027\u80fd\u63a5\u8fd110\u500d\u5927\u6a21\u578b\uff0c\u4e14\u88ab\u6bcd\u8bed\u8005\u8ba4\u4e3a\u8bed\u6cd5\u548c\u6587\u5316\u9002\u5e94\u6027\u66f4\u4f73\u3002", "conclusion": "\u516c\u5f00\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684LLM\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2506.09460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09460", "abs": "https://arxiv.org/abs/2506.09460", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "comment": null, "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u96c6\u57df\u6cdb\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u9891\u8c31\u4e0d\u53d8\u9891\u7387\u89e3\u8026\u3001\u53cc\u901a\u9053\u6b8b\u5dee\u7f51\u7edc\u3001\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\u548c\u9891\u8c31\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u89e3\u8026\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u76ee\u6807\u57df\u5b58\u5728\u672a\u77e5\u7c7b\u522b\u65f6\u65e0\u6cd5\u5904\u7406\u57df\u504f\u79fb\u7684\u95ee\u9898\uff0c\u907f\u514d\u8d1f\u8fc1\u79fb\u548c\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7ed3\u5408SIFD\uff08\u9891\u8c31\u4e0d\u53d8\u9891\u7387\u89e3\u8026\uff09\u3001DCRN\uff08\u53cc\u901a\u9053\u6b8b\u5dee\u7f51\u7edc\uff09\u3001EDL\uff08\u8bc1\u636e\u6df1\u5ea6\u5b66\u4e60\uff09\u548cSSUD\uff08\u9891\u8c31\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\u89e3\u8026\uff09\u56db\u4e2a\u6a21\u5757\uff0c\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u8de8\u573a\u666f\u9ad8\u5149\u8c31\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u76ee\u6807\u57df\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u96c6\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09566", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Bla\u017e \u0160krlj", "Boshko Koloski", "Senja Pollak", "Nada Lavra\u010d"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7ed3\u5408\uff0c\u5206\u4e3aKG\u589e\u5f3aLLMs\u548cLLM\u589e\u5f3aKGs\u4e24\u7c7b\uff0c\u5206\u6790\u4e86\u5176\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6574\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u4ee5\u589e\u5f3aLLMs\u7684\u4e8b\u5b9e\u57fa\u7840\u548c\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u5229\u7528LLMs\u4f18\u5316KGs\u7684\u6784\u5efa\u4e0e\u67e5\u8be2\u3002", "method": "\u7cfb\u7edf\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u4e3aKG-enhanced LLMs\u548cLLM-augmented KGs\uff0c\u5e76\u5206\u6790\u5176\u6280\u672f\u7ec6\u8282\u4e0e\u6548\u679c\u3002", "result": "\u63ed\u793a\u4e86\u7ed3\u6784\u5316\u77e5\u8bc6\u6574\u5408\u7684\u4e92\u60e0\u6027\uff0c\u5f3a\u8c03\u4e86\u53ef\u6269\u5c55\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u795e\u7ecf\u7b26\u53f7\u6574\u5408\u3001\u52a8\u6001KG\u66f4\u65b0\u3001\u6570\u636e\u53ef\u9760\u6027\u548c\u4f26\u7406\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8\u66f4\u590d\u6742\u7684\u77e5\u8bc6\u4efb\u52a1\u5904\u7406\u3002"}}
{"id": "2506.09469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09469", "abs": "https://arxiv.org/abs/2506.09469", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u62d3\u6251\u611f\u77e5\u4f18\u5316\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u591a\u8f66\u8f86\u4fe1\u606f\u63d0\u53473D LiDAR\u573a\u666f\u4e2d\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u5355\u667a\u80fd\u4f53MOT\u56e0\u906e\u6321\u548c\u4f20\u611f\u5668\u6545\u969c\u7b49\u95ee\u9898\u96be\u4ee5\u5168\u9762\u611f\u77e5\u73af\u5883\uff0c\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u878d\u5408\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u68c0\u6d4b\u5230\u7684\u8fb9\u754c\u6846\u6784\u5efa\u5168\u8fde\u63a5\u56fe\u62d3\u6251\uff0c\u91c7\u7528\u56fe\u62c9\u666e\u62c9\u65af\u4f18\u5316\u6280\u672f\u5e73\u6ed1\u4f4d\u7f6e\u8bef\u5dee\u5e76\u878d\u5408\u591a\u667a\u80fd\u4f53\u68c0\u6d4b\u4fe1\u606f\uff0c\u5206\u4e24\u9636\u6bb5\u4f18\u5316\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "result": "\u5728V2V4Real\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6846\u67b6\uff08\u5982DMSTrack\u548cV2V4Real\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u540c\u548c\u4fe1\u606f\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e863D MOT\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e8f\u5217\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u5bf9\u8bb0\u5fc6\u884c\u4e3a\u6709\u6291\u5236\u4f5c\u7528\uff0c\u9ad8ID\u5e8f\u5217\u6bd4\u4f4eID\u5e8f\u5217\u66f4\u96be\u88ab\u8bb0\u5fc6\uff0c\u5c24\u5176\u662f\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u548c\u7a00\u758f\u66b4\u9732\u60c5\u51b5\u4e0b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u53ef\u80fd\u65e0\u610f\u4e2d\u8bb0\u5fc6\u6570\u636e\u5e76\u5728\u751f\u6210\u65f6\u6cc4\u9732\u9690\u79c1\u6216\u77e5\u8bc6\u4ea7\u6743\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u6f5c\u5728\u7ed3\u6784\u5982\u4f55\u8c03\u8282\u8bb0\u5fc6\u7387\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5185\u5728\u7ef4\u5ea6\uff08ID\uff09\u4f5c\u4e3a\u5e8f\u5217\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u590d\u6742\u6027\u7684\u51e0\u4f55\u4ee3\u7406\uff0c\u63a2\u8ba8\u5176\u5bf9\u8bb0\u5fc6\u884c\u4e3a\u7684\u8c03\u8282\u4f5c\u7528\u3002", "result": "\u9ad8ID\u5e8f\u5217\u6bd4\u4f4eID\u5e8f\u5217\u66f4\u96be\u88ab\u8bb0\u5fc6\uff0c\u5c24\u5176\u662f\u5728\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u548c\u7a00\u758f\u66b4\u9732\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u66b4\u9732\u548c\u7ed3\u6784\u590d\u6742\u6027\u5728\u8bb0\u5fc6\u884c\u4e3a\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\u3002"}}
{"id": "2506.09473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09473", "abs": "https://arxiv.org/abs/2506.09473", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "comment": "10 pages, 6 figures, CVPR 2025", "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a2\u7d22-\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7684\u591a\u6a21\u6001\u6f14\u793a\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5728\u56db\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6f14\u793a\u6216\u542f\u53d1\u5f0f\u9009\u62e9\u7b56\u7565\uff0c\u8986\u76d6\u4efb\u52a1\u9700\u6c42\u4e0d\u8db3\u4e14\u5ffd\u7565\u6f14\u793a\u95f4\u4ea4\u4e92\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u63a2\u7d22-\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u591a\u6a21\u6001\u6f14\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u6211\u63a2\u7d22\u4f18\u5316\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2aVQA\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672cLVLMs\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u6f14\u793a\u9009\u62e9\u4f18\u5316\u4e86LVLMs\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u672c\u6807\u6ce8\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u53bb\u504f\u65b9\u6cd5\uff08DSL\u548cPPI\uff09\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0DSL\u5728\u504f\u5dee\u51cf\u5c11\u548c\u6548\u7387\u4e0a\u901a\u5e38\u4f18\u4e8ePPI\uff0c\u4f46\u7a33\u5b9a\u6027\u8f83\u5dee\u3002", "motivation": "LLMs\u6807\u6ce8\u6587\u672c\u65f6\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u53ef\u80fd\u5f71\u54cd\u4e0b\u6e38\u7edf\u8ba1\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u6709\u6548\u7684\u53bb\u504f\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u4e86Design-based Supervised Learning (DSL)\u548cPrediction-Powered Inference (PPI)\u4e24\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u6837\u672c\u91cf\u4e0b\u7684\u8868\u73b0\u3002", "result": "DSL\u5728\u504f\u5dee\u51cf\u5c11\u548c\u6548\u7387\u4e0a\u901a\u5e38\u4f18\u4e8ePPI\uff0c\u4f46\u7a33\u5b9a\u6027\u8f83\u5dee\uff1b\u4e24\u79cd\u65b9\u6cd5\u5728\u5927\u6837\u672c\u4e0b\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u53bb\u504f\u65b9\u6cd5\u5b58\u5728\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u9700\u8981\u66f4\u591a\u7814\u7a76\u91cf\u5316\u5176\u5728\u6709\u9650\u6837\u672c\u4e0b\u7684\u6548\u7387\u3002"}}
{"id": "2506.09476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09476", "abs": "https://arxiv.org/abs/2506.09476", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "comment": null, "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Urban1960SatBench\u6570\u636e\u96c6\u548cUrban1960SatUSM\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5386\u53f2\u536b\u661f\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8d28\u91cf\u9000\u5316\u548c\u6807\u6ce8\u7f3a\u5931\u95ee\u9898\uff0c\u65e8\u5728\u4fc3\u8fdb\u5bf9\u65e9\u671f\u57ce\u5e02\u53d1\u5c55\u7684\u7406\u89e3\u3002", "motivation": "\u5386\u53f2\u536b\u661f\u56fe\u50cf\uff08\u598220\u4e16\u7eaa\u4e2d\u53f6\u7684Keyhole\u6570\u636e\uff09\u4e3a\u7814\u7a76\u65e9\u671f\u57ce\u5e02\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u4f46\u56fe\u50cf\u8d28\u91cf\u5dee\u548c\u7f3a\u4e4f\u6807\u6ce8\u963b\u788d\u4e86\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86Urban1960SatBench\u6570\u636e\u96c6\uff08\u6807\u6ce8\u7684\u5386\u53f2\u56fe\u50cf\uff09\u548cUrban1960SatUSM\u6846\u67b6\uff08\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u91c7\u7528\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u673a\u5236\u548c\u7126\u70b9\u7f6e\u4fe1\u5ea6\u635f\u5931\uff09\u3002", "result": "Urban1960SatUSM\u5728Urban1960SatSeg\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u5206\u5272\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u5386\u53f2\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5229\u7528\u73b0\u4ee3\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9a\u91cf\u7814\u7a76\u957f\u671f\u57ce\u5e02\u53d8\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.09641", "categories": ["cs.CL", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.09641", "abs": "https://arxiv.org/abs/2506.09641", "authors": ["Anna Stein", "Kevin Tang"], "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "\u6bd4\u8f83\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6982\u7387\u9884\u6d4b\u5668\u4e0e\u6734\u7d20\u5224\u522b\u5b66\u4e60\uff08NDL\uff09\u9884\u6d4b\u5668\u5728\u58f0\u5b66\u8bcd\u65f6\u957f\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0N-gram\u6a21\u578b\u4f18\u4e8eNDL\u6a21\u578b\uff0c\u4f46\u4fe1\u606f\u8bba\u516c\u5f0f\u80fd\u63d0\u5347NDL\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8NDL\u56e0\u5176\u8ba4\u77e5\u52a8\u673a\u662f\u5426\u5728\u58f0\u5b66\u8bcd\u65f6\u957f\u5efa\u6a21\u4e2d\u66f4\u6709\u6548\uff0c\u5e76\u7814\u7a76\u4fe1\u606f\u8bba\u516c\u5f0f\u5bf9NDL\u7684\u6539\u8fdb\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Buckeye\u8bed\u6599\u5e93\u6bd4\u8f83\u4e09\u79cd\u6a21\u578b\uff1a\u4fe1\u606f\u8bba\u516c\u5f0f\u589e\u5f3a\u7684NDL\u3001\u4f20\u7edfNDL\u548cN-gram\u6982\u7387\u9884\u6d4b\u5668\u3002", "result": "N-gram\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4fe1\u606f\u8bba\u516c\u5f0f\u80fd\u63d0\u5347NDL\u6027\u80fd\uff0c\u6311\u6218\u4e86NDL\u7684\u8ba4\u77e5\u4f18\u52bf\u5047\u8bbe\u3002", "conclusion": "\u9700\u7ed3\u5408\u9891\u7387\u3001\u4e0a\u4e0b\u6587\u53ef\u9884\u6d4b\u6027\u53ca\u5e73\u5747\u53ef\u9884\u6d4b\u6027\uff0c\u5e76\u878d\u5408\u4fe1\u606f\u8bba\u6307\u6807\u4e0e\u5224\u522b\u5b66\u4e60\u4fe1\u606f\u4ee5\u4f18\u5316\u58f0\u5b66\u7f29\u51cf\u5efa\u6a21\u3002"}}
{"id": "2506.09479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09479", "abs": "https://arxiv.org/abs/2506.09479", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "comment": null, "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "AI": {"tldr": "TinySplat\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u524d\u9988\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u7d27\u51d1\u76843D\u573a\u666f\u8868\u793a\uff0c\u901a\u8fc7\u6d88\u9664\u5197\u4f59\u5b9e\u73b0\u4e86100\u500d\u4ee5\u4e0a\u7684\u538b\u7f29\u6548\u679c\uff0c\u4e14\u7f16\u7801\u548c\u89e3\u7801\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u99883D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u65b9\u6cd5\u867d\u7136\u91cd\u5efa\u901f\u5ea6\u5feb\uff0c\u4f46\u5b58\u50a8\u6210\u672c\u9ad8\uff0c\u4e14\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u56e0\u67b6\u6784\u4e0d\u517c\u5bb9\u65e0\u6cd5\u9002\u7528\u3002", "method": "TinySplat\u7ed3\u5408\u4e86\u8bad\u7ec3\u65e0\u5173\u7684\u538b\u7f29\u6846\u67b6\uff0c\u5305\u62ecView-Projection Transformation\uff08VPT\uff09\u51cf\u5c11\u51e0\u4f55\u5197\u4f59\uff0cVisibility-Aware Basis Reduction\uff08VABR\uff09\u51cf\u5c11\u611f\u77e5\u5197\u4f59\uff0c\u4ee5\u53ca\u89c6\u9891\u7f16\u89e3\u7801\u5668\u51cf\u5c11\u7a7a\u95f4\u5197\u4f59\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cTinySplat\u5b9e\u73b0\u4e86100\u500d\u4ee5\u4e0a\u7684\u538b\u7f29\uff0c\u5b58\u50a8\u5927\u5c0f\u4ec5\u4e3a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76846%\uff0c\u7f16\u7801\u548c\u89e3\u7801\u65f6\u95f4\u5206\u522b\u51cf\u5c1175%\u548c99%\u3002", "conclusion": "TinySplat\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u524d\u9988\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e863D\u9ad8\u65af\u6570\u636e\u7684\u5b58\u50a8\u548c\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.09643", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09643", "abs": "https://arxiv.org/abs/2506.09643", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "\u5229\u7528\u624b\u8bed\u751f\u6210\u6280\u672f\u589e\u5f3a\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u9aa8\u67b6\u751f\u6210\u3001\u62fc\u63a5\u548c\u751f\u6210\u6a21\u578b\uff08SignGAN\u3001SignSplat\uff09\u63d0\u5347\u6570\u636e\u96c6\u591a\u6837\u6027\uff0c\u6027\u80fd\u63d0\u5347\u8fbe19%\u3002", "motivation": "\u624b\u8bed\u6570\u636e\u7a00\u7f3a\u4e14\u6536\u96c6\u56f0\u96be\uff0c\u5f71\u54cd\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u9aa8\u67b6\u751f\u6210\u3001\u62fc\u63a5\u548c\u4e24\u79cd\u751f\u6210\u6a21\u578b\uff08SignGAN\u3001SignSplat\uff09\u751f\u6210\u591a\u6837\u5316\u7684\u624b\u8bed\u6570\u636e\u3002", "result": "\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8fbe19%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6570\u636e\u96c6\uff0c\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2506.09482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09482", "abs": "https://arxiv.org/abs/2506.09482", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "comment": null, "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "AI": {"tldr": "TransDiff\u7ed3\u5408\u81ea\u56de\u5f52Transformer\u548c\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u751f\u6210\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u591a\u53c2\u8003\u81ea\u56de\u5f52\uff08MRAR\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u7ed3\u5408\u81ea\u56de\u5f52Transformer\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4ee5\u63d0\u5347\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "TransDiff\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u6846\u67b6\u7f16\u7801\u6807\u7b7e\u548c\u56fe\u50cf\u4e3a\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u5229\u7528\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u56fe\u50cf\u6837\u672c\u5206\u5e03\u3002MRAR\u901a\u8fc7\u591a\u53c2\u8003\u81ea\u56de\u5f52\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728ImageNet 256x256\u57fa\u51c6\u4e0a\uff0cTransDiff\u7684FID\u4e3a1.61\uff0cIS\u4e3a293.4\uff0c\u63a8\u7406\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002MRAR\u8fdb\u4e00\u6b65\u5c06FID\u964d\u81f31.42\u3002", "conclusion": "TransDiff\u4e3a\u56fe\u50cf\u751f\u6210\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u7ed3\u5408MRAR\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.09645", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "RAPL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6807\u6ce8\u3001\u6a21\u578b\u65e0\u5173\u7684\u56fe\u53d8\u6362\u548c\u8def\u5f84\u63a8\u7406\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u7684\u68c0\u7d22\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u4f9d\u8d56\u975e\u7ed3\u6784\u5316\u6587\u672c\u5bfc\u81f4\u7684\u89e3\u91ca\u6027\u548c\u7ed3\u6784\u5316\u63a8\u7406\u53d7\u9650\u95ee\u9898\u3002", "method": "\u63d0\u51faRAPL\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u6807\u6ce8\u7b56\u7565\u3001\u6a21\u578b\u65e0\u5173\u7684\u56fe\u53d8\u6362\u65b9\u6cd5\u548c\u8def\u5f84\u63a8\u7406\u7b56\u7565\u3002", "result": "RAPL\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd52.66%-20.34%\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0d\u540c\u89c4\u6a21LLM\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "RAPL\u901a\u8fc7\u7ed3\u6784\u5316\u68c0\u7d22\u548c\u63a8\u7406\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.09510", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.09510", "abs": "https://arxiv.org/abs/2506.09510", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "comment": null, "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u9ad8\u65af\u71b5\u6a21\u578b\u548c\u52a8\u6001\u8c03\u6574\u4f3c\u7136\u533a\u95f4\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u70b9\u4e91\u5c5e\u6027\u538b\u7f29\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\u71b5\u53c2\u6570\u7684\u672a\u5145\u5206\u5229\u7528\u4fe1\u606f\u4ee5\u53ca\u56fa\u5b9a\u4f3c\u7136\u533a\u95f4\u7684\u9650\u5236\u3002", "method": "\u5f15\u5165\u5e7f\u4e49\u9ad8\u65af\u71b5\u6a21\u578b\u63a7\u5236\u5c3e\u5f62\u72b6\uff0c\u5e76\u63d0\u51faMean Error Discriminator\u52a8\u6001\u8c03\u6574\u4f3c\u7136\u533a\u95f4\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u4e8eVAE\u7684\u70b9\u4e91\u5c5e\u6027\u538b\u7f29\u6a21\u578b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u7387\u5931\u771f\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u70b9\u4e91\u538b\u7f29\uff0c\u8fd8\u53ef\u63a8\u5e7f\u81f3\u56fe\u50cf\u548c\u89c6\u9891\u538b\u7f29\u4efb\u52a1\u3002"}}
{"id": "2506.09657", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09657", "abs": "https://arxiv.org/abs/2506.09657", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eSemEval 2025 Task 8\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u6587\u672c\u5230SQL/\u4ee3\u7801\u751f\u6210\u3001\u81ea\u6821\u6b63\u673a\u5236\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u5e76\u901a\u8fc7LLM\u534f\u8c03\u3002\u7cfb\u7edf\u5728\u6bd4\u8d5b\u4e2d\u6392\u540d\u524d13\uff0c\u51c6\u786e\u7387\u8fbe80%\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u6570\u636e\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u96c6\u6210\u6587\u672c\u5230SQL/\u4ee3\u7801\u751f\u6210\u3001\u81ea\u6821\u6b63\u3001RAG\u548c\u7aef\u5230\u7aef\u6a21\u5757\uff0c\u7531LLM\u534f\u8c03\u3002", "result": "\u6bd4\u8d5b\u6392\u540d\u524d13\uff0c\u51c6\u786e\u738780%\uff0c\u6027\u80fd\u63a5\u8fd1\u4e13\u6709LLM\u3002", "conclusion": "\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09518", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09518", "abs": "https://arxiv.org/abs/2506.09518", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "AI": {"tldr": "HAIF-GS\u662f\u4e00\u79cd\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u951a\u70b9\u9a71\u52a8\u53d8\u5f62\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5197\u4f59\u66f4\u65b0\u3001\u8fd0\u52a8\u76d1\u7763\u4e0d\u8db3\u548c\u975e\u521a\u6027\u53d8\u5f62\u5efa\u6a21\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u5728\u5355\u76ee\u89c6\u9891\u4e2d\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5197\u4f59\u66f4\u65b0\u3001\u8fd0\u52a8\u76d1\u7763\u4e0d\u8db3\u548c\u975e\u521a\u6027\u53d8\u5f62\u5efa\u6a21\u5f31\u7684\u95ee\u9898\u3002", "method": "HAIF-GS\u901a\u8fc7\u951a\u70b9\u8fc7\u6ee4\u5668\u8bc6\u522b\u8fd0\u52a8\u76f8\u5173\u533a\u57df\uff0c\u4f7f\u7528\u81ea\u76d1\u7763\u6d41\u5f15\u5bfc\u53d8\u5f62\u6a21\u5757\u548c\u591a\u7ea7\u951a\u70b9\u4f20\u64ad\u673a\u5236\u5904\u7406\u590d\u6742\u53d8\u5f62\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHAIF-GS\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u91cd\u5efa\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u52a8\u60013DGS\u65b9\u6cd5\u3002", "conclusion": "HAIF-GS\u901a\u8fc7\u7ed3\u6784\u5316\u52a8\u6001\u5efa\u6a21\u548c\u9ad8\u6548\u53d8\u5f62\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u60013D\u573a\u666f\u91cd\u5efa\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09669", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09669", "abs": "https://arxiv.org/abs/2506.09669", "authors": ["Lihu Chen", "Ga\u00ebl Varoquaux"], "title": "Query-Level Uncertainty in Large Language Models", "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u67e5\u8be2\u7ea7\u4e0d\u786e\u5b9a\u6027\u68c0\u6d4b\u77e5\u8bc6\u8fb9\u754c\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u8bad\u7ec3\u7684\u201c\u5185\u90e8\u7f6e\u4fe1\u5ea6\u201d\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u660e\u786e\u81ea\u8eab\u77e5\u8bc6\u8fb9\u754c\uff0c\u8bc6\u522b\u5df2\u77e5\u4e0e\u672a\u77e5\u67e5\u8be2\uff0c\u4ece\u800c\u652f\u6301\u81ea\u9002\u5e94\u63a8\u7406\uff08\u5982RAG\u3001\u6df1\u5ea6\u601d\u8003\u6216\u5f03\u6743\u673a\u5236\uff09\uff0c\u63a8\u52a8\u9ad8\u6548\u53ef\u4fe1AI\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u201c\u5185\u90e8\u7f6e\u4fe1\u5ea6\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c42\u548c\u4ee4\u724c\u7684\u81ea\u8bc4\u4f30\u68c0\u6d4b\u67e5\u8be2\u7ea7\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e8b\u5b9e\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5185\u90e8\u7f6e\u4fe1\u5ea6\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u9ad8\u6548\u652f\u6301RAG\u548c\u6a21\u578b\u7ea7\u8054\uff0c\u964d\u4f4e\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u68c0\u6d4b\u77e5\u8bc6\u8fb9\u754c\uff0c\u63d0\u5347\u6a21\u578b\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2506.09522", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "ReVisiT\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u7528\u89c6\u89c9\u6807\u8bb0\u6765\u5f15\u5bfc\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u6587\u672c\u751f\u6210\uff0c\u63d0\u5347\u89c6\u89c9\u4fe1\u606f\u7684\u5229\u7528\u3002", "motivation": "\u4f20\u7edfLVLM\u7684\u89e3\u7801\u7b56\u7565\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u89c6\u89c9\u65e0\u5173\u7684\u54cd\u5e94\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6216\u591a\u6b65\u63a8\u7406\u3002", "method": "ReVisiT\u5c06\u89c6\u89c9\u6807\u8bb0\u6295\u5f71\u5230\u6587\u672c\u6807\u8bb0\u5206\u5e03\u7a7a\u95f4\uff0c\u901a\u8fc7\u7ea6\u675f\u5dee\u5f02\u6700\u5c0f\u5316\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u7528\u4e8e\u4f18\u5316\u8f93\u51fa\u5206\u5e03\u3002", "result": "\u5728\u4e09\u4e2aLVLM\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReVisiT\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u76f8\u5173\u6027\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "ReVisiT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u63a8\u7406\uff0c\u5373\u53ef\u663e\u8457\u63d0\u5347LVLM\u7684\u89c6\u89c9\u4fe1\u606f\u5229\u7528\u80fd\u529b\u3002"}}
{"id": "2506.09672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09672", "abs": "https://arxiv.org/abs/2506.09672", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u65e0\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\uff08UKE\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u6027\u8bc4\u4f30\u548c\u5fae\u8c03\u5931\u8d25\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u65e0\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\uff08UKE\uff09\u5bf9\u4e8e\u66f4\u65b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5c40\u90e8\u6027\u8bc4\u4f30\u4e14\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u5f02\u5e38\u5931\u8d25\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\uff08UnKEBench-Loc\u548cAKEW-Loc\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5f71\u54cd\u5fae\u8c03\u65b9\u6cd5\u7684\u56db\u4e2a\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u5fae\u8c03\u65b9\u6cd5\uff08FT-UKE\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFT-UKE\u5728\u5355\u6b21\u548c\u6279\u91cf\u7f16\u8f91\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u52bf\u968f\u6279\u91cf\u589e\u5927\u800c\u589e\u52a0\u3002", "conclusion": "FT-UKE\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u7ed3\u6784\u5316\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bad\u7ec3\u65b9\u6848\u3002"}}
{"id": "2506.09534", "categories": ["cs.CV", "I.4.5"], "pdf": "https://arxiv.org/pdf/2506.09534", "abs": "https://arxiv.org/abs/2506.09534", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "comment": "18 pages, 8 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "AI": {"tldr": "3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8f90\u5c04\u573a\u6e32\u67d3\u6280\u672f\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u5197\u4f59\u7684\u9ad8\u65af\u57fa\u5143\uff0c\u5bfc\u81f4\u5185\u5b58\u548c\u6e32\u67d3\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5168\u5c40\u9ad8\u65af\u6df7\u5408\u7f29\u51cf\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9ad8\u65af\u57fa\u5143\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u76843DGS\u538b\u7f29\u65b9\u6cd5\u57fa\u4e8e\u542f\u53d1\u5f0f\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u7f3a\u4e4f\u5168\u5c40\u4fdd\u771f\u5ea6\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89c6\u89d2\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u4fdd\u771f\u7684\u9ad8\u65af\u57fa\u5143\u538b\u7f29\u3002", "method": "\u9996\u5148\u901a\u8fc7KD\u6811\u5206\u533a\u6700\u5c0f\u5316\u590d\u5408\u4f20\u8f93\u6563\u5ea6\uff0c\u751f\u6210\u7d27\u51d1\u7684\u51e0\u4f55\u8868\u793a\uff1b\u7136\u540e\u901a\u8fc7\u5fae\u8c03\u989c\u8272\u548c\u900f\u660e\u5ea6\u5c5e\u6027\uff0c\u5c06\u5916\u89c2\u4e0e\u51e0\u4f55\u89e3\u8026\uff0c\u4f7f\u7528\u66f4\u5c11\u7684\u9ad8\u65af\u57fa\u5143\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4ec5\u970010%\u7684\u9ad8\u65af\u57fa\u5143\u5373\u53ef\u8fbe\u5230\u4e0e\u539f\u59cb3DGS\u76f8\u5f53\u7684\u6e32\u67d3\u8d28\u91cf\uff08PSNR\u3001SSIM\u3001LPIPS\uff09\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f553DGS\u6d41\u7a0b\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u795e\u7ecf\u6e32\u67d3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09684", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09684", "abs": "https://arxiv.org/abs/2506.09684", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6846\u67b6\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u968f\u673a\u6e38\u8d70\u548c\u9006\u6a21\u578b\u5b9a\u4e49\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cfInv-Entropy\uff0c\u5e76\u5f15\u5165\u9057\u4f20\u7b97\u6cd5\u6270\u52a8\u7b56\u7565GAAP\u548c\u8bc4\u4f30\u6307\u6807TSU\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u9760\u90e8\u7f72\u9700\u8981\u6709\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08UQ\uff09\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6982\u7387\u57fa\u7840\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u5e76\u63d0\u4f9b\u5b9e\u7528\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u53cc\u968f\u673a\u6e38\u8d70\u89c6\u89d2\uff0c\u5c06\u8f93\u5165-\u8f93\u51fa\u5bf9\u5efa\u6a21\u4e3a\u4e24\u4e2a\u9a6c\u5c14\u53ef\u592b\u94fe\uff0c\u5e76\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u5b9a\u4e49\u8f6c\u79fb\u6982\u7387\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u57fa\u4e8e\u9006\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6270\u52a8\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u63d0\u51fa\u7684Inv-Entropy\u5728\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u7406\u8bba\u57fa\u7840\u624e\u5b9e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cdLLM\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.09538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09538", "abs": "https://arxiv.org/abs/2506.09538", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "comment": null, "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5bf9\u6297\u6027\u8865\u4e01\u7684\u89d2\u5ea6\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Angle-Robust Concept Learning\uff08AngleRoCL\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8865\u4e01\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u653b\u51fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86T2I\u5bf9\u6297\u6027\u8865\u4e01\u5728\u7269\u7406\u4e16\u754c\u4e2d\u4e0d\u540c\u89c6\u89d2\u4e0b\u7684\u653b\u51fb\u6548\u679c\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63ed\u793a\u6587\u672c\u5bf9\u8865\u4e01\u89d2\u5ea6\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faAngleRoCL\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u901a\u7528\u6982\u5ff5\uff08\u6587\u672c\u5d4c\u5165\uff09\u6765\u751f\u6210\u5177\u6709\u89d2\u5ea6\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6027\u8865\u4e01\uff0c\u5e76\u5c06\u5176\u878d\u5165\u6587\u672c\u63d0\u793a\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAngleRoCL\u663e\u8457\u63d0\u5347\u4e86\u8865\u4e01\u7684\u89d2\u5ea6\u9c81\u68d2\u6027\uff0c\u653b\u51fb\u6210\u529f\u7387\u5728\u6311\u6218\u6027\u89c6\u89d2\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u6c34\u5e73\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u8d85\u8fc750%\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u7269\u7406\u89d2\u5ea6\u9c81\u68d2\u8865\u4e01\u7684\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u6587\u672c\u6982\u5ff5\u4e0eT2I\u751f\u6210\u5185\u5bb9\u7269\u7406\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2506.09790", "categories": ["cs.CL", "cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "ComfyUI-R1\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210AI\u5de5\u4f5c\u6d41\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08CoT\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u663e\u8457\u63d0\u5347\u4e86\u5de5\u4f5c\u6d41\u7684\u683c\u5f0f\u6709\u6548\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "AI\u751f\u6210\u5185\u5bb9\u7684\u5de5\u4f5c\u6d41\u5b9a\u5236\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5b66\u4e60\u66f2\u7ebf\u9661\u5ced\uff0cComfyUI-R1\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u75284K\u5de5\u4f5c\u6d41\u6570\u636e\u96c6\u6784\u5efa\u957f\u94fe\u63a8\u7406\u6570\u636e\uff0c\u901a\u8fc7CoT\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec37B\u53c2\u6570\u6a21\u578b\u3002", "result": "\u6a21\u578b\u683c\u5f0f\u6709\u6548\u6027\u8fbe97%\uff0c\u5728\u8282\u70b9\u548c\u56fe\u5f62\u7ea7\u522bF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o\u548cClaude\u7cfb\u5217\u3002", "conclusion": "\u957f\u94fe\u63a8\u7406\u548c\u4ee3\u7801\u5316\u5de5\u4f5c\u6d41\u5728AI\u827a\u672f\u521b\u4f5c\u4e2d\u5177\u6709\u6f5c\u529b\uff0cComfyUI-R1\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2506.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09541", "abs": "https://arxiv.org/abs/2506.09541", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "AI": {"tldr": "3DGeoDet\u662f\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e53D\u7269\u4f53\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u548c\u9690\u5f0f3D\u51e0\u4f55\u8868\u793a\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u97003D\u4fe1\u53f7\u76d1\u7763\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u56fe\u50cf\u76843D\u7269\u4f53\u68c0\u6d4b\u4e2d\u7f3a\u4e4f3D\u51e0\u4f55\u7ebf\u7d22\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u9884\u6d4b\u6df1\u5ea6\u751f\u6210\u663e\u5f0f\uff08\u4f53\u7d20\u5360\u7528\uff09\u548c\u9690\u5f0f\uff08TSDF\uff093D\u8868\u793a\uff0c\u7ed3\u5408\u4f53\u7d20\u5360\u7528\u6ce8\u610f\u529b\u4f18\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982SUN RGB-D\u63d0\u53479.3 mAP@0.5\u3002", "conclusion": "3DGeoDet\u901a\u8fc7\u51e0\u4f55\u8868\u793a\u663e\u8457\u63d0\u53473D\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u6837\u73af\u5883\u3002"}}
{"id": "2506.09796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09796", "abs": "https://arxiv.org/abs/2506.09796", "authors": ["Andreas S\u00e4uberli", "Diego Frassinelli", "Barbara Plank"], "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u9879\u9009\u62e9\u9898\u6d4b\u8bd5\u4e2d\u7684\u53cd\u5e94\u662f\u5426\u7c7b\u4f3c\u4eba\u7c7b\u884c\u4e3a\uff0c\u4ee5\u52a0\u901f\u6d4b\u8bd5\u5f00\u53d1\u3002", "motivation": "\u6d4b\u8bd5\u5f00\u53d1\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u8bd5\u70b9\u7814\u7a76\uff0c\u5982\u679cLLMs\u80fd\u6a21\u62df\u4eba\u7c7b\u53cd\u5e94\u884c\u4e3a\uff0c\u53ef\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u7ecf\u5178\u6d4b\u8bd5\u7406\u8bba\u548c\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff0c\u8bc4\u4f3018\u79cd\u6307\u4ee4\u8c03\u4f18LLMs\u5728\u9605\u8bfb\u3001\u7f8e\u56fd\u5386\u53f2\u548c\u7ecf\u6d4e\u5b66\u79d1\u76ee\u4e2d\u7684\u53cd\u5e94\u3002", "result": "\u8f83\u5927\u6a21\u578b\u8fc7\u4e8e\u81ea\u4fe1\uff0c\u4f46\u901a\u8fc7\u6e29\u5ea6\u6821\u51c6\u540e\u53cd\u5e94\u5206\u5e03\u66f4\u63a5\u8fd1\u4eba\u7c7b\uff1bLLMs\u5728\u9605\u8bfb\u7406\u89e3\u9898\u76ee\u4e2d\u4e0e\u4eba\u7c7b\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u6574\u4f53\u76f8\u5173\u6027\u4e0d\u5f3a\u3002", "conclusion": "LLMs\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4e0d\u9002\u5408\u7528\u4e8e\u6559\u80b2\u8bc4\u4f30\u7684\u8bd5\u70b9\u7814\u7a76\u3002"}}
{"id": "2506.09553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09553", "abs": "https://arxiv.org/abs/2506.09553", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "comment": null, "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "AI": {"tldr": "GLD-Road\u662f\u4e00\u79cd\u7ed3\u5408\u5168\u5c40\u6548\u7387\u548c\u5c40\u90e8\u7cbe\u5ea6\u7684\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u63d0\u53d6\u9053\u8def\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u9053\u8def\u7f51\u7edc\u5bf9\u6d4b\u7ed8\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u707e\u5bb3\u54cd\u5e94\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u6216\u6548\u7387\u95ee\u9898\u3002", "method": "GLD-Road\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u901a\u8fc7\u5168\u5c40\u68c0\u6d4b\u9053\u8def\u8282\u70b9\u5e76\u8fde\u63a5\uff0c\u518d\u901a\u8fc7\u5c40\u90e8\u8fed\u4ee3\u4f18\u5316\u65ad\u5f00\u7684\u9053\u8def\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGLD-Road\u5728APLS\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08City-Scale\u63d0\u53471.9%\uff0cSpaceNet3\u63d0\u53470.67%\uff09\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u68c0\u7d22\u65f6\u95f4\uff08\u6bd4Sat2Graph\u5feb40%\uff0c\u6bd4RNGDet++\u5feb92%\uff09\u3002", "conclusion": "GLD-Road\u5728\u9053\u8def\u7f51\u7edc\u63d0\u53d6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u7cbe\u786e\u7684\u5e73\u8861\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09820", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT\u662f\u4e00\u4e2a\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u6559\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u9ad8\u6548\u5229\u7528\u4ee3\u7801\u89e3\u91ca\u5668\uff08CI\uff09\uff0c\u901a\u8fc7Hint-Engineering\u5408\u6210\u6570\u636e\u4f18\u5316\u6a21\u578b\u4e0eCI\u7684\u4ea4\u4e92\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982o1\u548cDeepSeek-R1\uff09\u5728\u590d\u6742\u6570\u5b66\u8fd0\u7b97\u4e2d\u6548\u7387\u4f4e\u6216\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u76f4\u63a5\u7ed3\u5408\u5916\u90e8\u8ba1\u7b97\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7Hint-Engineering\u5408\u6210\u4ee3\u7801\u96c6\u6210\u63a8\u7406\u6570\u636e\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u3001\u62d2\u7edd\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5bf91.5B\u81f332B\u53c2\u6570\u7684\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u3002", "result": "Hint-Engineering\u6a21\u578b\u5728\u4e94\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u5347\u4e8632B\u548c1.5B\u6a21\u578b\u76844%\u548c8%\u7edd\u5bf9\u6027\u80fd\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u63a8\u7406\u6240\u9700\u7684token\u6570\u91cf\u3002", "conclusion": "CoRT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LRMs\u4e0eCI\u7684\u4ea4\u4e92\u6548\u7387\uff0c\u663e\u8457\u6539\u5584\u4e86\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.09557", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09557", "abs": "https://arxiv.org/abs/2506.09557", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "AI": {"tldr": "AD^2-Bench\u662f\u9996\u4e2a\u9488\u5bf9\u6076\u52a3\u5929\u6c14\u548c\u590d\u6742\u573a\u666f\u4e0b\u81ea\u52a8\u9a7e\u9a76\u7684Chain-of-Thought\uff08CoT\uff09\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b5.4k\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u652f\u6301\u591a\u6b65\u63a8\u7406\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524dMLLMs\u51c6\u786e\u7387\u4e0d\u8db360%\u3002", "motivation": "\u73b0\u6709\u8bc4\u6d4b\u57fa\u51c6\u5ffd\u89c6\u4e86\u6076\u52a3\u5929\u6c14\u548c\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e0bCoT\u63a8\u7406\u7684\u4e25\u683c\u8bc4\u4f30\u9700\u6c42\uff0cAD^2-Bench\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efaAD^2-Bench\uff0c\u8986\u76d6\u591a\u6837\u6076\u52a3\u73af\u5883\u6570\u636e\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6807\u6ce8\u548c\u4e13\u7528\u8bc4\u4f30\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u7ea7\u3001\u70b9\u7ea7\u548c\u533a\u57df\u7ea7\u89c6\u89c9\u63d0\u793a\u7684\u63a8\u7406\u5206\u6790\u3002", "result": "\u8bc4\u6d4b\u663e\u793a\u5f53\u524dMLLMs\u5728AD^2-Bench\u4e0a\u7684\u51c6\u786e\u7387\u4f4e\u4e8e60%\uff0c\u51f8\u663e\u4e86\u5176\u6311\u6218\u6027\u548c\u5bf9\u9c81\u68d2\u6027\u63a8\u7406\u7684\u9700\u6c42\u3002", "conclusion": "AD^2-Bench\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684CoT\u63a8\u7406\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u6d4b\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86MLLMs\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2506.09827", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09827", "abs": "https://arxiv.org/abs/2506.09827", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "S\u00f6ren Auer"], "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "EmoNet-Voice\u662f\u4e00\u4e2a\u65b0\u7684\u8bed\u97f3\u60c5\u611f\u68c0\u6d4b\u8d44\u6e90\uff0c\u5305\u542b\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u572840\u79cd\u60c5\u611f\u7c7b\u522b\u4e0a\u7684\u7ec6\u7c92\u5ea6\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u6570\u636e\u96c6\u5728\u60c5\u611f\u7c92\u5ea6\u3001\u9690\u79c1\u95ee\u9898\u6216\u4f9d\u8d56\u8868\u6f14\u6570\u636e\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u5408\u6210\u97f3\u9891\u7247\u6bb5\u6a21\u62df\u7279\u5b9a\u60c5\u611f\u573a\u666f\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u4e13\u5bb6\u7684\u5f3a\u5ea6\u6807\u6ce8\uff0c\u6784\u5efaEmoNet-Voice\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1Empathic Insight Voice\u6a21\u578b\u3002", "result": "\u65b0\u6a21\u578b\u5728\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u4e0a\u8fbe\u5230\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u9ad8\u5ea6\u4e00\u81f4\u7684\u6807\u51c6\uff0c\u4e14\u9ad8\u5524\u9192\u60c5\u611f\uff08\u5982\u6124\u6012\uff09\u6bd4\u4f4e\u5524\u9192\u72b6\u6001\uff08\u5982\u4e13\u6ce8\uff09\u66f4\u6613\u68c0\u6d4b\u3002", "conclusion": "EmoNet-Voice\u4e3a\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u4e86AI\u60c5\u611f\u7406\u89e3\u80fd\u529b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09565", "abs": "https://arxiv.org/abs/2506.09565", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "comment": null, "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "AI": {"tldr": "SemanticSplat\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u548c\u8bed\u4e49\u5c5e\u6027\u7684\u524d\u9988\u65b9\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u51e0\u4f55\u3001\u5916\u89c2\u548c\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u63d0\u53d6\u548c\u51e0\u4f55\u91cd\u5efa\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u63d0\u53d6\u548c\u51e0\u4f55\u91cd\u5efa\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u5bc6\u96c6\u8f93\u5165\u89c6\u56fe\uff0c\u9650\u5236\u4e86\u5b9e\u7528\u6027\u3002", "method": "SemanticSplat\u901a\u8fc7\u878d\u5408\u591a\u7279\u5f81\u573a\uff08\u5982LSeg\u3001SAM\uff09\u548c\u6210\u672c\u4f53\u79ef\u8868\u793a\uff0c\u9884\u6d4b\u8bed\u4e49\u5404\u5411\u5f02\u6027\u9ad8\u65af\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u84b8\u998f\u6846\u67b6\u4ece\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u4e2d\u91cd\u5efa\u591a\u6a21\u6001\u8bed\u4e49\u7279\u5f81\u573a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u53ef\u63d0\u793a\u548c\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u7b493D\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SemanticSplat\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u51e0\u4f55\u3001\u5916\u89c2\u548c\u8bed\u4e49\uff0c\u5b9e\u73b0\u4e86\u66f4\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u3002"}}
{"id": "2506.09833", "categories": ["cs.CL", "I.2.1"], "pdf": "https://arxiv.org/pdf/2506.09833", "abs": "https://arxiv.org/abs/2506.09833", "authors": ["Omar Sherif", "Ali Hamdi"], "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEGPA\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e34\u5e8a\u76f8\u5173\u8fd0\u52a8\u9519\u8bef\u751f\u6210\u5408\u6210\u9aa8\u9abc\u6570\u636e\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5eb7\u590d\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5eb7\u590d\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u96be\u4ee5\u68c0\u6d4b\u7ec6\u5fae\u8fd0\u52a8\u9519\u8bef\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u3002", "method": "\u63d0\u51faError-Guided Pose Augmentation (EGPA)\u65b9\u6cd5\uff0c\u6a21\u62df\u4e34\u5e8a\u8fd0\u52a8\u9519\u8bef\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u56fe\u5377\u79ef\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cEGPA\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e27.6%\uff0c\u9519\u8bef\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534745.8%\uff0c\u6a21\u578b\u80fd\u805a\u7126\u4e34\u5e8a\u5173\u952e\u5173\u8282\u548c\u8fd0\u52a8\u9636\u6bb5\u3002", "conclusion": "EGPA\u4e3a\u4e34\u5e8a\u548c\u5bb6\u5ead\u5eb7\u590d\u4e2d\u7684\u81ea\u52a8\u8fd0\u52a8\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.09612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09612", "abs": "https://arxiv.org/abs/2506.09612", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "comment": "17 pages, 9. figures", "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZigzag Sampling with Asymmetric Prompts and Visual Sharing\u7684\u8bad\u7ec3\u65e0\u5173\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u89c6\u89c9\u6545\u4e8b\u751f\u6210\u4e2d\u7684\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u591a\u5f20\u56fe\u50cf\u65f6\u96be\u4ee5\u4fdd\u6301\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u8d44\u6e90\u5bc6\u96c6\uff0c\u8981\u4e48\u6548\u679c\u6709\u9650\u3002", "method": "\u91c7\u7528Zigzag\u91c7\u6837\u673a\u5236\uff0c\u4ea4\u66ff\u4f7f\u7528\u975e\u5bf9\u79f0\u63d0\u793a\u548c\u89c6\u89c9\u5171\u4eab\u6a21\u5757\uff0c\u4ee5\u4fdd\u7559\u4e3b\u9898\u7279\u5f81\u5e76\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u8fde\u8d2f\u4e14\u4e00\u81f4\u7684\u89c6\u89c9\u6545\u4e8b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u6545\u4e8b\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u4e00\u81f4\u6027\u589e\u5f3a\u65b9\u6848\u3002"}}
{"id": "2506.09847", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.09847", "abs": "https://arxiv.org/abs/2506.09847", "authors": ["Tomas Peterka", "Matyas Bohacek"], "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u6d4b\u65b0\u95fb\u56fe\u7247\u6765\u6e90\u76f8\u5173\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u516d\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4d\u7f6e\u548c\u65f6\u95f4\u76f8\u5173\u6027\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u68c0\u6d4b\u5a92\u4f53\u64cd\u7eb5\u7684\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u8bed\u4e49\u4e0e\u6587\u672c\u7684\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u6765\u6e90\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u64cd\u7eb5\u884c\u4e3a\u88ab\u9057\u6f0f\u3002", "method": "\u6784\u5efa\u4e86News Media Provenance Dataset\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4f4d\u7f6e\u6765\u6e90\u76f8\u5173\u6027\uff08LOR\uff09\u548c\u65f6\u95f4\u6765\u6e90\u76f8\u5173\u6027\uff08DTOR\uff09\u4e24\u9879\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u516d\u79cdLLM\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002", "result": "LOR\u4efb\u52a1\u8868\u73b0\u826f\u597d\uff0c\u4f46DTOR\u4efb\u52a1\u8868\u73b0\u8f83\u5dee\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "conclusion": "\u672a\u6765\u9700\u5f00\u53d1\u4e13\u95e8\u67b6\u6784\u4ee5\u63d0\u5347DTOR\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2506.09626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09626", "abs": "https://arxiv.org/abs/2506.09626", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "comment": "IJCNN 2025", "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faECAM\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u907f\u969c\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u5e38\u5ffd\u7565\u73af\u5883\u56e0\u7d20\u5bfc\u81f4\u78b0\u649e\uff0c\u9700\u6539\u8fdb\u907f\u969c\u80fd\u529b\u3002", "method": "\u5f15\u5165ECAM\u6a21\u5757\uff0c\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u3002", "result": "\u5728ETH/UCY\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u78b0\u649e\u7387\u964d\u4f4e40-50%\u3002", "conclusion": "ECAM\u6a21\u5757\u6709\u6548\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u907f\u969c\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09853", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "pdf": "https://arxiv.org/pdf/2506.09853", "abs": "https://arxiv.org/abs/2506.09853", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u6846\u67b6\uff0c\u901a\u8fc7\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\u53cc\u91cd\u89c6\u89d2\u4f18\u5316CoT\u63a8\u7406\uff0c\u81ea\u52a8\u589e\u5220\u63a8\u7406\u6b65\u9aa4\uff0c\u63d0\u5347\u6548\u7387\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3CoT\u63a8\u7406\u4e2d\u6b65\u9aa4\u7684\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347LLM\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56e0\u679c\u6846\u67b6\uff0c\u7ed3\u5408\u5145\u5206\u6027\u548c\u5fc5\u8981\u6027\u7684\u6982\u7387\u5206\u6790\uff0c\u91cf\u5316\u63a8\u7406\u6b65\u9aa4\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u81ea\u52a8\u4f18\u5316\u6b65\u9aa4\u3002", "result": "\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u51cf\u5c11token\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u548c\u6210\u672c\u6548\u76ca\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u5411\u3002"}}
{"id": "2506.09634", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09634", "abs": "https://arxiv.org/abs/2506.09634", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "AI": {"tldr": "HSENet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc3D\u89c6\u89c9\u7f16\u7801\u5668\u548c\u7a7a\u95f4\u538b\u7f29\u6280\u672f\uff0c\u63d0\u53473D\u533b\u5b66\u5f71\u50cf\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9488\u5bf92D\u533b\u5b66\u5f71\u50cf\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u590d\u6742\u76843D\u89e3\u5256\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef\u3002", "method": "HSENet\u91c7\u7528\u53cc3D\u89c6\u89c9\u7f16\u7801\u5668\u611f\u77e5\u5168\u5c40\u548c\u7ec6\u8282\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7Spatial Packer\u538b\u7f29\u9ad8\u5206\u8fa8\u73873D\u7a7a\u95f4\u533a\u57df\u4e3a\u7d27\u51d1\u89c6\u89c9\u6807\u8bb0\u3002", "result": "\u57283D\u89c6\u89c9\u8bed\u8a00\u68c0\u7d22\u3001\u533b\u5b66\u62a5\u544a\u751f\u6210\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cHSENet\u5747\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HSENet\u901a\u8fc7\u9ad8\u65483D\u89c6\u89c9\u7f16\u7801\u548c\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u533b\u5b66\u5f71\u50cf\u7684\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09886", "abs": "https://arxiv.org/abs/2506.09886", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u63d0\u793a\u4e0e\u54cd\u5e94\u9690\u85cf\u72b6\u6001\u5206\u5e03\u7684\u6982\u7387\u5dee\u5f02\u6765\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u7684\u65b0\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e7b\u89c9\u54cd\u5e94\u4e0e\u63d0\u793a\u7684\u504f\u5dee\u8f83\u5c0f\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u7684\u6a21\u578b\u5185\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u4ea7\u751f\u5e7b\u89c9\uff08\u4e0d\u771f\u5b9e\u6216\u65e0\u4f9d\u636e\u7684\u54cd\u5e94\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u6216\u8f85\u52a9\u6a21\u578b\uff0c\u7f3a\u4e4f\u9ad8\u6548\u4e14\u81ea\u6d3d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u63d0\u793a\u4e0e\u54cd\u5e94\u9690\u85cf\u72b6\u6001\u5206\u5e03\u7684\u8ddd\u79bb\u4f5c\u4e3a\u5e7b\u89c9\u5206\u6570\uff0c\u5e76\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u6df1\u5ea6\u6838\u51fd\u6570\u6355\u6349\u5206\u5e03\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5373\u4f7f\u4e0d\u8bad\u7ec3\u6838\u51fd\u6570\u4e5f\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u8d44\u6e90\uff0c\u6027\u80fd\u4f18\u5f02\u3002"}}
{"id": "2506.09644", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09644", "abs": "https://arxiv.org/abs/2506.09644", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "comment": null, "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "AI": {"tldr": "DGAE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u63d0\u5347\u89e3\u7801\u5668\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u9ad8\u538b\u7f29\u6bd4\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u7f16\u7801\u5668\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u4e0b\u964d\u53caGAN\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u51cf\u5c11\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8868\u793a\u3002", "method": "\u63d0\u51faDGAE\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u5f15\u5bfc\u89e3\u7801\u5668\u6062\u590d\u6f5c\u5728\u8868\u793a\u4e2d\u672a\u5b8c\u5168\u89e3\u7801\u7684\u4fe1\u606f\u4fe1\u53f7\u3002", "result": "DGAE\u5728\u9ad8\u7a7a\u95f4\u538b\u7f29\u7387\u4e0b\u6709\u6548\u7f13\u89e3\u6027\u80fd\u4e0b\u964d\uff0c\u6f5c\u5728\u7a7a\u95f4\u7f29\u5c0f2\u500d\uff0c\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u5728ImageNet-1K\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DGAE\u901a\u8fc7\u63d0\u5347\u89e3\u7801\u5668\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\uff0c\u5e76\u52a0\u901f\u6269\u6563\u6a21\u578b\u7684\u6536\u655b\u3002"}}
{"id": "2506.09890", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09890", "abs": "https://arxiv.org/abs/2506.09890", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u53d1\u5c55\u4e2d\u9010\u6e10\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u7684\u6838\u5fc3\u53c2\u6570\u7a7a\u95f4\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u62bd\u8c61\u601d\u7ef4\u3002", "motivation": "\u6311\u6218LLMs\u4ee5\u82f1\u8bed\u4e3a\u601d\u8003\u8bed\u8a00\u7684\u5047\u8bbe\uff0c\u63a2\u7d22\u5176\u591a\u8bed\u8a00\u80fd\u529b\u7684\u672c\u8d28\u3002", "method": "\u8bc6\u522b\u8bed\u8a00\u76f8\u5173\u795e\u7ecf\u5143\uff08\u5171\u4eab\u4e0e\u4e13\u5c5e\uff09\uff0c\u63d0\u51fa\u9488\u5bf9\u4e0d\u540c\u53d1\u5c55\u9636\u6bb5\u7684\u8bed\u8a00\u65e0\u5173\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5171\u4eab\u795e\u7ecf\u5143\u6bd4\u4f8b\u4e0e\u529f\u80fd\u91cd\u8981\u6027\u589e\u52a0\uff0c\u4e13\u5c5e\u795e\u7ecf\u5143\u5f71\u54cd\u529b\u51cf\u5f31\uff0c\u5f62\u6210\u6838\u5fc3\u8bed\u8a00\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\u3002", "conclusion": "LLMs\u7684\u62bd\u8c61\u601d\u7ef4\u57fa\u4e8e\u8bed\u8a00\u65e0\u5173\u53c2\u6570\u7a7a\u95f4\uff0c\u795e\u7ecf\u5143\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u652f\u6301\u591a\u8bed\u8a00\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2506.09650", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.09650", "abs": "https://arxiv.org/abs/2506.09650", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u53c2\u8003\u7684\u591a\u4eba\u7269\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u9996\u4e2a\u76f8\u5173\u6570\u636e\u96c6RHAS133\u3002\u901a\u8fc7\u63d0\u51fa\u7684HopaDIFF\u6846\u67b6\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u6761\u4ef6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u52a8\u4f5c\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4eba\u7269\u56fa\u5b9a\u52a8\u4f5c\u5e8f\u5217\uff0c\u5ffd\u89c6\u4e86\u591a\u4eba\u7269\u573a\u666f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u6587\u672c\u63cf\u8ff0\u6307\u5bfc\u591a\u4eba\u7269\u52a8\u4f5c\u5206\u5272\u3002", "method": "\u63d0\u51fa\u4e86HopaDIFF\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8de8\u8f93\u5165\u95e8\u6ce8\u610f\u529bxLSTM\u548c\u5085\u91cc\u53f6\u6761\u4ef6\uff0c\u589e\u5f3a\u6574\u4f53-\u5c40\u90e8\u957f\u7a0b\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u5728RHAS133\u6570\u636e\u96c6\u4e0a\uff0cHopaDIFF\u5728\u591a\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "HopaDIFF\u4e3a\u591a\u4eba\u7269\u52a8\u4f5c\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u6846\u67b6\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09902", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86PersonaLens\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4efb\u52a1\u5bfc\u5411AI\u52a9\u624b\u4e2a\u6027\u5316\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLM\u52a9\u624b\u5728\u4e2a\u6027\u5316\u80fd\u529b\u4e0a\u7684\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u57fa\u51c6\u672a\u80fd\u5168\u9762\u8bc4\u4f30\u4efb\u52a1\u5bfc\u5411AI\u52a9\u624b\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faPersonaLens\u57fa\u51c6\uff0c\u5305\u542b\u591a\u6837\u5316\u7528\u6237\u914d\u7f6e\u548c\u4e24\u4e2a\u57fa\u4e8eLLM\u7684\u4ee3\u7406\uff08\u7528\u6237\u4ee3\u7406\u548c\u8bc4\u5224\u4ee3\u7406\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e2a\u6027\u5316\u3001\u54cd\u5e94\u8d28\u91cf\u548c\u4efb\u52a1\u6210\u529f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dLLM\u52a9\u624b\u5728\u4e2a\u6027\u5316\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "PersonaLens\u4e3a\u63d0\u5347\u5bf9\u8bddAI\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2506.09663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09663", "abs": "https://arxiv.org/abs/2506.09663", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "comment": null, "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "AI": {"tldr": "DeGSS\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53d8\u5f623D\u9ad8\u65af\u573a\u7f16\u7801\u94f0\u63a5\u7269\u4f53\u7684\u51e0\u4f55\u3001\u5916\u89c2\u548c\u8fd0\u52a8\uff0c\u652f\u6301\u65e0\u76d1\u7763\u7684\u90e8\u4ef6\u5206\u5272\u548c\u7cbe\u786e\u5efa\u6a21\u3002", "motivation": "\u94f0\u63a5\u7269\u4f53\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u65f6\u96be\u4ee5\u6784\u5efa\u7edf\u4e00\u8868\u793a\u3002", "method": "DeGSS\u5c06\u6bcf\u4e2a\u4ea4\u4e92\u72b6\u6001\u5efa\u6a21\u4e3a\u5171\u4eab\u573a\u7684\u5e73\u6ed1\u53d8\u5f62\uff0c\u901a\u8fc7\u53d8\u5f62\u8f68\u8ff9\u5b9e\u73b0\u6e10\u8fdb\u5f0f\u90e8\u4ef6\u5206\u5272\u3002", "result": "\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DeGSS\u4e3a\u94f0\u63a5\u7269\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u8fde\u7eed\u3001\u89e3\u8026\u7684\u8868\u793a\uff0c\u652f\u6301\u90e8\u4ef6\u7ea7\u91cd\u5efa\u548c\u8fd0\u52a8\u5efa\u6a21\u3002"}}
{"id": "2506.09917", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09917", "abs": "https://arxiv.org/abs/2506.09917", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aASESUM\u7684\u65b0\u578b\u6458\u8981\u7cfb\u7edf\uff0c\u80fd\u591f\u4ece\u4ea7\u54c1\u5173\u952e\u89d2\u5ea6\u603b\u7ed3\u89c2\u70b9\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u9886\u57df\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u65b9\u9762\u3002", "motivation": "\u5728\u7ebf\u8d2d\u7269\u4e2d\uff0c\u987e\u5ba2\u96be\u4ee5\u624b\u52a8\u5904\u7406\u5927\u91cf\u8bc4\u8bba\u5e76\u603b\u7ed3\u4e3b\u8981\u89c2\u70b9\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u610f\u89c1\u6458\u8981\u7cfb\u7edf\u3002", "method": "ASESUM\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u9762\u5411\u65b9\u9762\u7684\u8bba\u70b9\u5e76\u8861\u91cf\u5176\u663e\u8457\u6027\u548c\u6709\u6548\u6027\uff0c\u603b\u7ed3\u4ea7\u54c1\u5173\u952e\u65b9\u9762\u7684\u89c2\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cASESUM\u5728\u6355\u6349\u539f\u59cb\u8bc4\u8bba\u7684\u591a\u6837\u5316\u89c2\u70b9\u65b9\u9762\u4f18\u4e8e\u65b0\u65e7\u65b9\u6cd5\u3002", "conclusion": "ASESUM\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u610f\u89c1\u6458\u8981\u7cfb\u7edf\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u9886\u57df\u5e76\u63d0\u4f9b\u6709\u8bc1\u636e\u652f\u6301\u7684\u6458\u8981\u3002"}}
{"id": "2506.09668", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09668", "abs": "https://arxiv.org/abs/2506.09668", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "comment": "Work currently under revision for IEEE TMI", "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "AI": {"tldr": "CINeMA\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u521b\u5efa\u9ad8\u5206\u8fa8\u7387\u3001\u591a\u6a21\u6001\u7684\u80ce\u513f\u548c\u65b0\u751f\u513f\u8111\u56fe\u8c31\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u7814\u7a76\u80ce\u513f\u548c\u65b0\u751f\u513f\u5927\u8111\u5feb\u901f\u53d1\u80b2\u9636\u6bb5\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u56fe\u8c31\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\uff0c\u800c\u75c5\u7406\u6570\u636e\u7a00\u7f3a\u3002", "method": "CINeMA\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u907f\u514d\u8ba1\u7b97\u5bc6\u96c6\u578b\u56fe\u50cf\u914d\u51c6\uff0c\u652f\u6301\u57fa\u4e8e\u89e3\u5256\u7279\u5f81\u7684\u7075\u6d3b\u6761\u4ef6\u3002", "result": "CINeMA\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u591a\u529f\u80fd\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u7ec4\u7ec7\u5206\u5272\u3001\u5e74\u9f84\u9884\u6d4b\u7b49\u4efb\u52a1\u3002", "conclusion": "CINeMA\u662f\u63a8\u52a8\u8111\u7814\u7a76\u7684\u6709\u529b\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u56fe\u8c31\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09942", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09942", "abs": "https://arxiv.org/abs/2506.09942", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c4\u5219\u548cLLM\u7684\u9a8c\u8bc1\u65b9\u6cd5VerIF\uff0c\u7528\u4e8e\u589e\u5f3a\u6307\u4ee4\u8ddf\u968f\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u6307\u4ee4\u8ddf\u968f\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u9a8c\u8bc1\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u6709\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u89c4\u5219\u4ee3\u7801\u9a8c\u8bc1\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982QwQ-32B\uff09\u7684\u9a8c\u8bc1\u65b9\u6cd5VerIF\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6VerInstruct\u3002", "result": "\u5728\u591a\u4e2a\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u540c\u7c7b\u6700\u4f73\uff0c\u4e14\u4e0d\u5f71\u54cd\u901a\u7528\u80fd\u529b\u3002", "conclusion": "VerIF\u53ef\u96c6\u6210\u5230\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\uff0c\u63d0\u5347\u6574\u4f53\u6a21\u578b\u6027\u80fd\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09677", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09677", "abs": "https://arxiv.org/abs/2506.09677", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "comment": null, "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u63a8\u7406\u6a21\u578b\u5728\u8bef\u5bfc\u6027\u7528\u6237\u8f93\u5165\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u4e86GaslightingBench-R\u57fa\u51c6\u4ee5\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u6a21\u578b\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u63a8\u7406\u6a21\u578b\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u7528\u6237\u8f93\u5165\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e09\u79cd\u63a8\u7406\u6a21\u578b\uff08OpenAI\u7684o4-mini\u3001Claude-3.7-Sonnet\u548cGemini-2.5-Flash\uff09\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\uff08MMMU\u3001MathVista\u548cCharXiv\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u8bbe\u8ba1\u4e86GaslightingBench-R\u57fa\u51c6\u3002", "result": "\u6a21\u578b\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\u51c6\u786e\u6027\u5e73\u5747\u4e0b\u964d25-29%\uff0c\u5728GaslightingBench-R\u4e0a\u4e0b\u964d\u8d85\u8fc753%\u3002", "conclusion": "\u63a8\u7406\u6a21\u578b\u5728\u9010\u6b65\u63a8\u7406\u548c\u4fe1\u5ff5\u575a\u6301\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u9c81\u68d2\u6027\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002"}}
{"id": "2506.09944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09944", "abs": "https://arxiv.org/abs/2506.09944", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faQRHEAD\u548cQR-RETRIEVER\uff0c\u901a\u8fc7\u805a\u7126\u67e5\u8be2\u7684\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u68c0\u7d22\u6027\u80fd\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u5934\uff08retrieval heads\uff09\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u68c0\u7d22\u6548\u7387\u3002", "method": "\u901a\u8fc7\u805a\u5408\u8f93\u5165\u67e5\u8be2\u7684\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522bQRHEAD\uff0c\u5e76\u5f00\u53d1QR-RETRIEVER\u4f5c\u4e3a\u9ad8\u6548\u68c0\u7d22\u5668\u3002", "result": "\u5728LongMemEval\u548cCLIPPER\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u8d8510%\uff0cBEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u96f6\u6837\u672c\u8868\u73b0\u4f18\u4e8eRankGPT\u3002", "conclusion": "QRHEAD\u548cQR-RETRIEVER\u4e3a\u901a\u7528\u68c0\u7d22\u5668\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u5bf9\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.09691", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u6539\u8fdb\u53cc\u7f16\u7801\u5668\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7ec4\u5408\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u56fe\u50cf\u548c\u6587\u672c\u5e76\u5339\u914d\u5bf9\u9f50\u90e8\u5206\u6765\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53cc\u7f16\u7801\u5668VLM\uff08\u5982CLIP\uff09\u5728\u7ec4\u5408\u6027\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u68c0\u7d22\u6027\u80fd\u3002\u76ee\u524d\u7814\u7a76\u591a\u5173\u6ce8\u8bad\u7ec3\u65b9\u6cd5\uff0c\u800c\u63a8\u7406\u65f6\u6280\u672f\u8f83\u5c11\u88ab\u63a2\u7d22\u3002", "method": "\u5728\u63a8\u7406\u65f6\uff0c\u5c06\u56fe\u50cf\u5206\u5272\u4e3a\u5c0f\u5757\uff0c\u63d0\u53d6\u6587\u672c\u7247\u6bb5\uff08\u5bf9\u8c61\u3001\u5c5e\u6027\u548c\u5173\u7cfb\uff09\uff0c\u7528VLM\u5bf9\u9f50\u56fe\u50cf\u5757\u4e0e\u6587\u672c\u7247\u6bb5\uff0c\u5e76\u805a\u5408\u76f8\u4f3c\u6027\u5f97\u5206\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86VLM\u7684\u7ec4\u5408\u6027\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u5c5e\u6027-\u5bf9\u8c61\u7ed1\u5b9a\u4efb\u52a1\u4e2d\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u63a8\u7406\u65f6\u6280\u672f\u5177\u6709\u6f5c\u529b\uff0c\u56fe\u50cf\u5206\u5272\u662f\u5173\u952e\u6539\u8fdb\u70b9\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u65b9\u6cd5\u3002"}}
{"id": "2506.09967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09967", "abs": "https://arxiv.org/abs/2506.09967", "authors": ["Shangshang Wang", "Julian Asilis", "\u00d6mer Faruk Akg\u00fcl", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "title": "Resa: Transparent Reasoning Models via SAEs", "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "Resa\u662f\u4e00\u79cd1.5B\u63a8\u7406\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8c03\u4f18\uff08SAE-Tuning\uff09\u65b9\u6cd5\u9ad8\u6548\u63d0\u53d6\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u548c\u65f6\u95f4\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u5730\u4ece\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u548c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u4ece\u6e90\u6a21\u578b\u4e2d\u6355\u83b7\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u6307\u5bfc\u76ee\u6807\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u65e0\u9700\u63a8\u7406\u8f68\u8ff9\u3002", "result": "SAE-Tuning\u5728\u964d\u4f4e2000\u500d\u6210\u672c\u548c450\u500d\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u7559\u4e8697%\u7684\u63a8\u7406\u6027\u80fd\uff1b\u5728\u8f7b\u91cfRL\u8bad\u7ec3\u540e\uff0c\u80fd\u4ee5\u7ea61\u7f8e\u5143\u6210\u672c\u5b9e\u73b0\u663e\u8457\u63a8\u7406\u63d0\u5347\u3002", "conclusion": "SAE-Tuning\u63d0\u53d6\u7684\u63a8\u7406\u80fd\u529b\u5177\u6709\u901a\u7528\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.09695", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09695", "abs": "https://arxiv.org/abs/2506.09695", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "comment": "11 pages, 5 figures", "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFasterSNN\u7684\u6df7\u5408\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u65e9\u671f\u8bca\u65ad\uff0c\u7ed3\u5408\u4e86\u751f\u7269\u542f\u53d1\u7684LIF\u795e\u7ecf\u5143\u3001\u533a\u57df\u81ea\u9002\u5e94\u5377\u79ef\u548c\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u65e9\u671f\u8bca\u65adAD\uff08\u5c24\u5176\u662f\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u9636\u6bb5\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u8bc4\u4f30\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u81ea\u52a8\u5316\u4f46\u80fd\u8017\u9ad8\uff0c\u800cSNNs\u9002\u5408\u5efa\u6a21AD\u7684\u7a00\u758f\u4e8b\u4ef6\u9a71\u52a8\u6a21\u5f0f\uff0c\u4f46\u8868\u8fbe\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faFasterSNN\uff0c\u7ed3\u5408LIF\u795e\u7ecf\u5143\u3001\u533a\u57df\u81ea\u9002\u5e94\u5377\u79ef\u548c\u591a\u5c3a\u5ea6\u8109\u51b2\u6ce8\u610f\u529b\uff0c\u7a00\u758f\u9ad8\u6548\u5904\u74063D MRI\u6570\u636e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFasterSNN\u8868\u73b0\u51fa\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u6548\u7387\u548c\u7a33\u5b9a\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "FasterSNN\u4e3aAD\u7b5b\u67e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09975", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09975", "abs": "https://arxiv.org/abs/2506.09975", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u793e\u4ea4\u5a92\u4f53\u4e0aAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u95ee\u9898\uff0c\u6307\u51fa\u7531\u4e8e\u6587\u672c\u77ed\u4e14\u8bed\u8a00\u975e\u6b63\u5f0f\uff0c\u68c0\u6d4b\u96be\u5ea6\u5927\u3002\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\uff0c\u53d1\u73b0\u82e5\u653b\u51fb\u8005\u4e0d\u516c\u5f00\u5176\u5fae\u8c03\u6a21\u578b\uff0c\u68c0\u6d4b\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u662f\u7f51\u7edc\u5f71\u54cd\u529b\u6d3b\u52a8\u7684\u91cd\u8981\u653b\u51fb\u8f7d\u4f53\uff0cAI\u751f\u6210\u5185\u5bb9\u53ef\u80fd\u88ab\u7528\u4e8e\u652f\u6301\u6216\u53cd\u5bf9\u7279\u5b9a\u653f\u7b56\u6216\u4e8b\u4ef6\uff0c\u56e0\u6b64\u68c0\u6d4b\u6b64\u7c7b\u5185\u5bb9\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u56e2\u961f\u4ee5\u5a01\u80c1\u8005\u7684\u89c6\u89d2\u6784\u5efa\u4e86505,159\u6761AI\u751f\u6210\u7684\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u6570\u636e\u96c6\uff0c\u6db5\u76d611\u4e2a\u4e89\u8bae\u8bdd\u9898\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5728\u4e0d\u540c\u5047\u8bbe\u4e0b\u7684\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u82e5\u653b\u51fb\u8005\u4e0d\u516c\u5f00\u5fae\u8c03\u6a21\u578b\uff0c\u68c0\u6d4b\u6548\u679c\u5927\u5e45\u4e0b\u964d\uff0c\u4eba\u7c7b\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u7ed3\u679c\u3002\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u4e86\u68c0\u6d4b\u7b97\u6cd5\u5bf9\u5fae\u8c03LLM\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5fae\u8c03LLM\u7684\u666e\u904d\u5e94\u7528\u5bf9\u68c0\u6d4b\u9886\u57df\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u5b9e\u9645\u5a01\u80c1\u3002"}}
{"id": "2506.09699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09699", "abs": "https://arxiv.org/abs/2506.09699", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander Gonz\u00e1lez Tom\u00e9", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "comment": "Technical report", "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "AI": {"tldr": "CHIP\u662f\u9996\u4e2a\u9488\u5bf9\u5de5\u4e1a\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u76846D\u59ff\u6001\u4f30\u8ba1\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u5305\u542b77,811\u5f20RGBD\u56fe\u50cf\uff0c\u5e76\u5c55\u793a\u4e86\u72ec\u7279\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u67096D\u59ff\u6001\u4f30\u8ba1\u57fa\u51c6\u5728\u771f\u5b9e\u5de5\u4e1a\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e0d\u8db3\uff0cCHIP\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "CHIP\u6570\u636e\u96c6\u5305\u542b\u4e03\u79cd\u4e0d\u540c\u6905\u5b50\uff0c\u4f7f\u7528\u4e09\u79cdRGBD\u6280\u672f\u6355\u83b7\uff0c\u5e76\u81ea\u52a8\u6807\u6ce8\u771f\u5b9e6D\u59ff\u6001\u3002", "result": "\u4e09\u79cd\u96f6\u6837\u672c6D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u5728CHIP\u4e0a\u8868\u73b0\u663e\u793a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "CHIP\u4e3a\u5de5\u4e1a\u73af\u5883\u4e2d\u76846D\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u6311\u6218\uff0c\u5e76\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2506.09983", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09983", "abs": "https://arxiv.org/abs/2506.09983", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6b65\u6307\u4ee4\u7b56\u7565\u7684\u4f9d\u8d56\u89e3\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bcd\u6027\u6807\u6ce8\u548c\u7b80\u5316\u8f93\u51fa\u683c\u5f0f\uff0c\u572817\u79cd\u8bed\u8a00\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u8bed\u8a00\u5fae\u8c03\u7684\u4f18\u52bf\u3002", "motivation": "\u6807\u51c6\u63d0\u793a\u65b9\u6cd5\u5728\u4f9d\u8d56\u89e3\u6790\u4e2d\u96be\u4ee5\u751f\u6210\u7ed3\u6784\u6709\u6548\u4e14\u51c6\u786e\u7684\u8f93\u51fa\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5206\u6b65\u6307\u4ee4\u7b56\u7565\uff0c\u5148\u8fdb\u884c\u8bcd\u6027\u6807\u6ce8\uff0c\u518d\u9884\u6d4b\u53e5\u6cd5\u5934\u548c\u4f9d\u8d56\u6807\u7b7e\uff0c\u5e76\u4f7f\u7528\u7b80\u5316\u7684CoNLL-U\u683c\u5f0f\u8f93\u51fa\u3002", "result": "\u572817\u79cd\u8bed\u8a00\u7684Universal Dependencies\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u5e7b\u89c9\u6216\u6c61\u67d3\uff1b\u591a\u8bed\u8a00\u5fae\u8c03\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u5206\u6b65\u63a8\u7406\u7b56\u7565\u5728\u57fa\u4e8eLLM\u7684\u89e3\u6790\u4e2d\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u683c\u5f0f\u4e00\u81f4\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.09718", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09718", "abs": "https://arxiv.org/abs/2506.09718", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LADH\u6570\u636e\u96c6\uff0c\u7ed3\u5408RGB\u548c\u7ea2\u5916\u89c6\u9891\u63d0\u5347\u8fdc\u7a0b\u5149\u7535\u5bb9\u79ef\u63cf\u8bb0\u672f\uff08rPPG\uff09\u5728\u957f\u671f\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3rPPG\u5728\u9ad8\u6d77\u62d4\u73af\u5883\u4e2d\u56e0\u5149\u7167\u53d8\u5316\u3001\u906e\u6321\u548c\u52a8\u6001\u9762\u90e8\u59ff\u52bf\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528240\u4e2a\u540c\u6b65RGB\u548c\u7ea2\u5916\u9762\u90e8\u89c6\u9891\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u63d0\u5347\u751f\u7406\u4fe1\u53f7\u76d1\u6d4b\u6027\u80fd\u3002", "result": "\u7ed3\u5408RGB\u548c\u7ea2\u5916\u89c6\u9891\u8f93\u5165\uff0c\u5fc3\u7387\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a4.99 BPM\u3002", "conclusion": "LADH\u6570\u636e\u96c6\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86rPPG\u5728\u957f\u671f\u5065\u5eb7\u76d1\u6d4b\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2506.09992", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09992", "abs": "https://arxiv.org/abs/2506.09992", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u585e\u5c14\u7ef4\u4e9a\u8bed\u3001\u514b\u7f57\u5730\u4e9a\u8bed\u548c\u6ce2\u65af\u5c3c\u4e9a\u8bed\u4e2d\u5904\u7406\u6709\u6bd2\u8bc4\u8bba\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u901a\u8fc7\u6dfb\u52a0\u7b80\u77ed\u4e0a\u4e0b\u6587\u7247\u6bb5\u53ef\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u5728\u7ebf\u6709\u6bd2\u8bed\u8a00\u5bf9\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u9020\u6210\u5b9e\u9645\u4f24\u5bb3\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u8bed\u8a00\u73af\u5883\u3002", "method": "\u6784\u5efa\u5e76\u624b\u52a8\u6807\u6ce8\u4e864,500\u6761YouTube\u548cTikTok\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u6a21\u578b\uff08GPT-3.5 Turbo\u3001GPT-4.1\u3001Gemini 1.5 Pro\u548cClaude 3 Opus\uff09\u5728\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5f0f\u5e73\u5747\u63d0\u5347\u53ec\u56de\u73870.12\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u53470.10\uff1bGemini\u5728\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5f0f\u4e0b\u8868\u73b0\u6700\u4f73\uff08F1=0.82\uff0c\u51c6\u786e\u7387=0.82\uff09\u3002", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\u548c\u9608\u503c\u6821\u51c6\uff0c\u53ef\u4ee5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u6709\u6548\u63d0\u5347\u6709\u6bd2\u8bed\u8a00\u68c0\u6d4b\u6548\u679c\u3002"}}
{"id": "2506.09724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09724", "abs": "https://arxiv.org/abs/2506.09724", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "title": "The Four Color Theorem for Cell Instance Segmentation", "comment": "Accepted at ICML 2025", "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56db\u8272\u5b9a\u7406\u7684\u65b0\u578b\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u56db\u8272\u7f16\u7801\u7b80\u5316\u5b9e\u4f8b\u533a\u5206\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\u89e3\u51b3\u7f16\u7801\u975e\u552f\u4e00\u6027\u95ee\u9898\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u56fe\u50cf\u4e2d\u7d27\u5bc6\u63a5\u89e6\u7ec6\u80de\u7684\u51c6\u786e\u5206\u5272\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u5c06\u7ec6\u80de\u89c6\u4e3a\u56fd\u5bb6\u3001\u7ec4\u7ec7\u89c6\u4e3a\u6d77\u6d0b\uff0c\u5f15\u5165\u56db\u8272\u7f16\u7801\u65b9\u6848\uff0c\u5c06\u5b9e\u4f8b\u5206\u5272\u8f6c\u5316\u4e3a\u4ec5\u9700\u9884\u6d4b\u56db\u7c7b\u7684\u7ea6\u675f\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u6a21\u5f0f\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u56db\u8272\u7f16\u7801\u548c\u6e10\u8fdb\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u7b80\u5316\u4e86\u7ec6\u80de\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u6548\u679c\u3002"}}
{"id": "2506.09996", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.09996", "abs": "https://arxiv.org/abs/2506.09996", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u90e8\u5206\u68c0\u6d4b\u7684\u6570\u636e\u548c\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6784\u5efaFineHarm\u6570\u636e\u96c6\u548c\u63d0\u51fa\u6d41\u5f0f\u5185\u5bb9\u76d1\u63a7\u5668\uff08SCM\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5ba1\u6838\u5668\u4e3b\u8981\u91c7\u7528\u5b8c\u5168\u68c0\u6d4b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff1b\u90e8\u5206\u68c0\u6d4b\u867d\u80fd\u51cf\u5c11\u5ef6\u8fdf\uff0c\u4f46\u56e0\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u5dee\u8ddd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u6784\u5efaFineHarm\u6570\u636e\u96c6\uff0829K\u5bf9\u63d0\u793a-\u54cd\u5e94\uff09\uff0c\u63d0\u51faSCM\u6a21\u578b\uff0c\u91c7\u7528\u54cd\u5e94\u548c\u6807\u8bb0\u7ea7\u53cc\u91cd\u76d1\u7763\u8bad\u7ec3\u3002", "result": "SCM\u4ec5\u9700\u67e5\u770b\u54cd\u5e94\u524d18%\u7684\u6807\u8bb0\u5373\u53ef\u8fbe\u5230\u4e0e\u5b8c\u5168\u68c0\u6d4b\u76f8\u5f53\u7684F1\u5206\u6570\uff080.95+\uff09\uff0c\u5e76\u80fd\u63d0\u5347\u5b89\u5168\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "SCM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6d41\u5f0f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002"}}
{"id": "2506.09735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09735", "abs": "https://arxiv.org/abs/2506.09735", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "comment": null, "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5148\u9a8c\u878d\u5408\u7f51\u7edc\uff08MPFNet\uff09\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\u4f18\u5316\u5fae\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\uff0c\u7ed3\u5408\u901a\u7528\u548c\u9ad8\u7ea7\u7279\u5f81\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\u56e0\u6301\u7eed\u65f6\u95f4\u77ed\u3001\u5f3a\u5ea6\u4f4e\u800c\u66f4\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u591a\u6e90\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51faMPFNet\uff0c\u5305\u542b\u901a\u7528\u7279\u5f81\u7f16\u7801\u5668\uff08GFE\uff09\u548c\u9ad8\u7ea7\u7279\u5f81\u7f16\u7801\u5668\uff08AFE\uff09\uff0c\u57fa\u4e8eI3D\u548c\u5750\u6807\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86MPFNet-P\u548cMPFNet-C\u4e24\u79cd\u53d8\u4f53\u3002", "result": "\u5728SMIC\u3001CASME II\u548cSAMM\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52300.811\u30010.924\u548c0.857\u7684\u51c6\u786e\u7387\uff0c\u90e8\u5206\u6570\u636e\u96c6\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "MPFNet\u901a\u8fc7\u591a\u5148\u9a8c\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09736", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09736", "abs": "https://arxiv.org/abs/2506.09736", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "comment": "Technical Report", "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u5904\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4ec5\u901a\u8fc7\u56fe\u50cf\u63cf\u8ff0\u7684\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u751a\u81f3\u4f18\u4e8eMLLMs\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u89c6\u89c9\u6270\u52a8\u6846\u67b6\uff0c\u65e0\u9700\u7b97\u6cd5\u4fee\u6539\u6216\u989d\u5916\u6570\u636e\u5373\u53ef\u589e\u5f3a\u611f\u77e5\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u89c6\u89c9\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u65e0\u6cd5\u6709\u6548\u6574\u5408\u89c6\u89c9\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\uff0c\u4fc3\u4f7f\u4f5c\u8005\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u590d\u6742\u4fee\u6539\u7684\u89c6\u89c9\u6270\u52a8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u89c6\u89c9\u6270\u52a8\u7b56\u7565\uff08\u5e72\u6270\u62fc\u63a5\u3001\u4fdd\u6301\u4e3b\u5bfc\u6027\u7684\u6df7\u5408\u3001\u968f\u673a\u65cb\u8f6c\uff09\uff0c\u53ef\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff08\u5982SFT\u3001DPO\u3001GRPO\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u8bad\u7ec3\u540e\u7684Qwen2.5-VL-7B\u6a21\u578b\u5728\u5f00\u6e907B RL\u8c03\u4f18\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u89c6\u89c9\u6270\u52a8\u5728\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u4e0d\u540c\u6270\u52a8\u7b56\u7565\u5bf9\u89c6\u89c9\u63a8\u7406\u7684\u72ec\u7279\u8d21\u732e\uff0c\u5f3a\u8c03\u4e86\u2018\u66f4\u597d\u7684\u63a8\u7406\u59cb\u4e8e\u66f4\u597d\u7684\u89c6\u89c9\u2019\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.09740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09740", "abs": "https://arxiv.org/abs/2506.09740", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "comment": null, "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aELBO-T2IAlign\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6821\u51c6\u6269\u6563\u6a21\u578b\u4e2d\u50cf\u7d20\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u96f6\u6837\u672c\u53c2\u8003\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5b9e\u9645\u4e0a\u5b58\u5728\u504f\u5dee\uff0c\u5c24\u5176\u662f\u5728\u5c0f\u5c3a\u5bf8\u3001\u906e\u6321\u6216\u7f55\u89c1\u7269\u4f53\u7c7b\u522b\u4e2d\u3002", "method": "\u4f7f\u7528\u8bc1\u636e\u4e0b\u754c\uff08ELBO\uff09\u6821\u51c6\u50cf\u7d20-\u6587\u672c\u5bf9\u9f50\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u6269\u6563\u6a21\u578b\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u6709\u6548\u6539\u5584\u4e86\u50cf\u7d20-\u6587\u672c\u5bf9\u9f50\u3002", "conclusion": "ELBO-T2IAlign\u662f\u4e00\u79cd\u7b80\u5355\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2506.09745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09745", "abs": "https://arxiv.org/abs/2506.09745", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "comment": null, "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f02\u6784\u7c7b\u522b\u96c6\u5b66\u4e60\uff08MMHCL\uff09\u7684\u5b9e\u7528\u8bbe\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u7c7b\u76f8\u4f3c\u6027\u7684\u8de8\u6a21\u6001\u878d\u5408\u6a21\u578b\uff08CSCF\uff09\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u4e2d\u7c7b\u522b\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u591a\u6a21\u6001\u6570\u636e\u7684\u7c7b\u522b\u5206\u5e03\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u8de8\u6a21\u6001\u4fe1\u606f\u8bc6\u522b\u6240\u6709\u7c7b\u522b\u3002", "method": "CSCF\u5c06\u6a21\u6001\u7279\u5b9a\u7279\u5f81\u5bf9\u9f50\u5230\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9009\u62e9\u6700\u5177\u5224\u522b\u6027\u7684\u6a21\u6001\u8fdb\u884c\u51b3\u7b56\u878d\u5408\uff0c\u5e76\u57fa\u4e8e\u7c7b\u76f8\u4f3c\u6027\u6574\u5408\u8de8\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCSCF\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86MMHCL\u4efb\u52a1\u3002", "conclusion": "CSCF\u901a\u8fc7\u7c7b\u76f8\u4f3c\u6027\u9a71\u52a8\u7684\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5f02\u6784\u7c7b\u522b\u96c6\u5b66\u4e60\u95ee\u9898\u3002"}}
{"id": "2506.09748", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09748", "abs": "https://arxiv.org/abs/2506.09748", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "comment": "8 pages, 6 figures", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u5316\u8de8\u6e90\u56fe\u50cf\u5339\u914d\u7684\u65e0\u4eba\u673a\u7edd\u5bf9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u611f\u77e5\u548c\u7ed3\u6784\u7ea6\u675f\u7684\u7c97\u5339\u914d\u6a21\u5757\u4e0e\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728GNSS\u4e0d\u53ef\u7528\u573a\u666f\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u5728GNSS\u4fe1\u53f7\u4e0d\u53ef\u7528\u65f6\u9700\u8981\u4f9d\u8d56\u89c6\u89c9\u7edd\u5bf9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u8de8\u6e90\u5dee\u5f02\u548c\u65f6\u95f4\u53d8\u5316\u5bfc\u81f4\u5339\u914d\u56f0\u96be\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u5316\u8de8\u6e90\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\uff0c\u5305\u62ec\u8bed\u4e49\u611f\u77e5\u548c\u7ed3\u6784\u7ea6\u675f\u7684\u7c97\u5339\u914d\u6a21\u5757\u4e0e\u8f7b\u91cf\u7ea7\u7ec6\u7c92\u5ea6\u5339\u914d\u6a21\u5757\uff0c\u7ed3\u5408\u56fe\u50cf\u68c0\u7d22\u6784\u5efa\u5b9a\u4f4d\u6d41\u7a0b\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u65b0\u5f15\u5165\u7684CS-UAV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728GNSS\u4e0d\u53ef\u7528\u573a\u666f\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u7edd\u5bf9\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2506.09953", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.09953", "abs": "https://arxiv.org/abs/2506.09953", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u7684\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b2017\u4e2a\u89c6\u9891\u548c5986\u4e2a\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u63a2\u7d22\u5728\u89c6\u9891\u5bf9\u8bdd\u4e2d\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u5916\u90e8\u77e5\u8bc6\u56de\u7b54\u95ee\u9898\u7684\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u89c6\u9891\u548c\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u6a21\u578b\u8bc4\u4f30\u3002", "result": "\u5c55\u793a\u4e86\u4efb\u52a1\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u4efb\u52a1\u4e3a\u7ed3\u5408\u89c6\u89c9\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.09777", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09777", "abs": "https://arxiv.org/abs/2506.09777", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "comment": null, "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "AI": {"tldr": "DarkerBB\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u76f8\u4f3c\u6027\u5206\u6570\u5728PCA\u7279\u5f81\u8138\u7a7a\u95f4\u4e2d\u8fdb\u884c\u96f6\u9636\u4f18\u5316\uff0c\u4ece\u9ed1\u76d2\u8bc6\u522b\u6a21\u578b\u4e2d\u91cd\u5efa\u5f69\u8272\u4eba\u8138\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u4ec5\u4f7f\u7528\u76f8\u4f3c\u6027\u5206\u6570\u8fdb\u884c\u6a21\u578b\u53cd\u6f14\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u4ee5\u63ed\u793a\u9ed1\u76d2\u8bc6\u522b\u6a21\u578b\u5e26\u6765\u7684\u9690\u79c1\u5a01\u80c1\u3002", "method": "\u5728PCA\u7279\u5f81\u8138\u7a7a\u95f4\u4e2d\u8fdb\u884c\u96f6\u9636\u4f18\u5316\uff0c\u4ec5\u4f9d\u8d56\u76f8\u4f3c\u6027\u5206\u6570\u91cd\u5efa\u5f69\u8272\u4eba\u8138\u56fe\u50cf\u3002", "result": "\u5728LFW\u3001AgeDB-30\u548cCFP-FP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDarkerBB\u5728\u4ec5\u4f7f\u7528\u76f8\u4f3c\u6027\u5206\u6570\u7684\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u4e14\u67e5\u8be2\u6548\u7387\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "DarkerBB\u5c55\u793a\u4e86\u5728\u4fe1\u606f\u9ad8\u5ea6\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u6709\u6548\u91cd\u5efa\u4eba\u8138\u56fe\u50cf\uff0c\u7a81\u663e\u4e86\u9ed1\u76d2\u6a21\u578b\u6f5c\u5728\u7684\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2506.09782", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09782", "abs": "https://arxiv.org/abs/2506.09782", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "comment": "20 pages", "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "AI": {"tldr": "Q-SAM2\u662f\u4e00\u79cd\u9488\u5bf9SAM2\u7684\u9ad8\u6548\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u5c42\u6821\u51c6\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u89e3\u51b3\u4e86\u91cf\u5316\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "SAM2\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6d88\u8017\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u91cf\u5316\u65b9\u6cd5\u3002", "method": "Q-SAM2\u5f15\u5165\u4e86\u7ebf\u6027\u5c42\u6821\u51c6\u65b9\u6cd5\u8fdb\u884c\u4f4e\u6bd4\u7279\u521d\u59cb\u5316\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u6291\u5236\u5f02\u5e38\u503c\uff0c\u4f7f\u7f51\u7edc\u9002\u5e94\u91cf\u5316\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQ-SAM2\u5728\u9ad8\u6548\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5c24\u5176\u57282\u6bd4\u7279\u91cf\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6821\u51c6\u6280\u672f\u5728\u540e\u8bad\u7ec3\u91cf\u5316\u4e2d\u4e5f\u6709\u6548\u3002", "conclusion": "Q-SAM2\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86SAM2\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.09784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09784", "abs": "https://arxiv.org/abs/2506.09784", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "comment": "Technical report", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "AI": {"tldr": "FreeZeV2\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u76846D\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u51e0\u4f55\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u5bf9\u65b0\u7269\u4f53\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u76846D\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u91c7\u7528\u7a00\u758f\u7279\u5f81\u63d0\u53d6\u3001\u7279\u5f81\u611f\u77e5\u8bc4\u5206\u673a\u5236\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u591a\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728BOP Benchmark\u4e0a\u8fbe\u5230\u65b0SOTA\uff0c\u901f\u5ea6\u63d0\u53478\u500d\uff0c\u51c6\u786e\u6027\u63d0\u9ad85%\uff1b\u96c6\u6210\u6a21\u578b\u540e\u51c6\u786e\u6027\u518d\u63d0\u53478%\uff0c\u901f\u5ea6\u4ecd\u5feb2.5\u500d\u3002", "conclusion": "FreeZeV2\u8bc1\u660e\u4e86\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u76846D\u59ff\u6001\u4f30\u8ba1\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09814", "abs": "https://arxiv.org/abs/2506.09814", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "comment": null, "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDreamCS\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u65e0\u914d\u5bf93D\u504f\u597d\u6570\u636e\u96c63D-MeshPref\uff0c\u5e76\u5229\u7528Cauchy-Schwarz\u6563\u5ea6\u76ee\u6807\u8bad\u7ec3\u5956\u52b1\u6a21\u578bRewardCS\uff0c\u4ee5\u63d0\u5347\u6587\u672c\u52303D\u751f\u6210\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u76843D\u8d44\u4ea7\uff0c\u4e14\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u6280\u672f\u4f9d\u8d56\u96be\u4ee5\u6536\u96c6\u7684\u914d\u5bf9\u591a\u89c6\u56fe2D\u56fe\u50cf\uff0c\u5bfc\u81f4\u51e0\u4f55\u4f2a\u5f71\u3002", "method": "\u6784\u5efa3D-MeshPref\u6570\u636e\u96c6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\u548c\u4eba\u7c7b\u8bc4\u4f30\u4fee\u6b63\uff1b\u5f00\u53d1RewardCS\u5956\u52b1\u6a21\u578b\uff0c\u91c7\u7528Cauchy-Schwarz\u6563\u5ea6\u76ee\u6807\uff1b\u63d0\u51faDreamCS\u6846\u67b6\uff0c\u6574\u5408RewardCS\u5230\u6587\u672c\u52303D\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDreamCS\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u76843D\u8d44\u4ea7\u51e0\u4f55\u51c6\u786e\u4e14\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "DreamCS\u901a\u8fc7\u76f4\u63a5\u5b66\u4e603D\u504f\u597d\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u8d28\u91cf\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\u3002"}}
{"id": "2506.09834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09834", "abs": "https://arxiv.org/abs/2506.09834", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "comment": null, "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5fae\u8868\u60c5\u6570\u636e\u96c6MMME\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u9762\u90e8\u52a8\u4f5c\u4fe1\u53f7\u3001\u4e2d\u67a2\u795e\u7ecf\u7cfb\u7edf\u4fe1\u53f7\u548c\u5916\u5468\u751f\u7406\u4fe1\u53f7\u7684\u540c\u6b65\u91c7\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u548c\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5fae\u8868\u60c5\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u4e00\u89c6\u89c9\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u751f\u7406\u6a21\u6001\u4f20\u9012\u7684\u4e30\u5bcc\u60c5\u611f\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u89c6\u89c9\u7279\u5f81\u4e0e\u751f\u7406\u4fe1\u53f7\u7684\u8de8\u6a21\u6001\u5173\u8054\u673a\u5236\uff0c\u5f00\u53d1\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u662f\u63a8\u8fdb\u5fae\u8868\u60c5\u5206\u6790\u7684\u5173\u952e\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86MMME\u6570\u636e\u96c6\uff0c\u5305\u542b634\u4e2a\u5fae\u8868\u60c5\u30012,841\u4e2a\u5b8f\u8868\u60c5\u548c2,890\u4e2a\u540c\u6b65\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u8bd5\u9a8c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u751f\u7406\u4fe1\u53f7\u663e\u8457\u63d0\u5347\u4e86\u5fae\u8868\u60c5\u7684\u8bc6\u522b\u548c\u68c0\u6d4b\u6027\u80fd\uff0cMMME\u662f\u76ee\u524d\u6a21\u6001\u591a\u6837\u6027\u6700\u5168\u9762\u7684\u5fae\u8868\u60c5\u6570\u636e\u96c6\u3002", "conclusion": "MMME\u4e3a\u63a2\u7d22\u5fae\u8868\u60c5\u7684\u795e\u7ecf\u673a\u5236\u548c\u89c6\u89c9-\u751f\u7406\u534f\u540c\u6548\u5e94\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u5fae\u8868\u60c5\u7814\u7a76\u4ece\u5355\u4e00\u89c6\u89c9\u5206\u6790\u5411\u591a\u6a21\u6001\u878d\u5408\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2506.09836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09836", "abs": "https://arxiv.org/abs/2506.09836", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "comment": null, "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "AI": {"tldr": "DynaSplat\u901a\u8fc7\u52a8\u6001-\u9759\u6001\u5206\u79bb\u548c\u5206\u5c42\u8fd0\u52a8\u5efa\u6a21\uff0c\u6269\u5c55\u4e86\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u52a8\u6001\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u76f4\u89c2\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u53d8\u5f62\u504f\u79fb\u7edf\u8ba1\u548c2D\u8fd0\u52a8\u6d41\u4e00\u81f4\u6027\u5206\u7c7b\u9759\u6001\u4e0e\u52a8\u6001\u5143\u7d20\uff0c\u91c7\u7528\u5206\u5c42\u8fd0\u52a8\u5efa\u6a21\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u4e0d\u900f\u660e\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\uff0cDynaSplat\u5728\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u66f4\u7d27\u51d1\u9ad8\u6548\u3002", "conclusion": "DynaSplat\u4e3a\u52a8\u6001\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u76f4\u89c2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09839", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09839", "abs": "https://arxiv.org/abs/2506.09839", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "title": "OctoNav: Towards Generalist Embodied Navigation", "comment": "31 pages, 25 figures", "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u5bfc\u822a\u4ee3\u7406OctoNav-R1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u81ea\u7531\u6307\u4ee4\u5b8c\u6210\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86OctoNav-Bench\u57fa\u51c6\u548cTBA-CoT\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u7814\u7a76\u4efb\u52a1\u5206\u6563\u4e14\u65b9\u6cd5\u72ec\u7acb\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u80fd\u5904\u7406\u591a\u6a21\u6001\u6307\u4ee4\u7684\u901a\u7528\u5bfc\u822a\u4ee3\u7406\u3002", "method": "\u63d0\u51faOctoNav-Bench\u57fa\u51c6\u548cTBA-CoT\u6570\u636e\u96c6\uff0c\u6784\u5efa\u57fa\u4e8eMLLMs\u7684OctoNav-R1\u6a21\u578b\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff08HTP\uff09\u3002", "result": "OctoNav-R1\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7TBA-SFT\u548cNav-GPRO\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u5bfc\u822a\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.09846", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09846", "abs": "https://arxiv.org/abs/2506.09846", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "comment": "17 pages, 10 figures, Under Review", "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u63d0\u5347\u624b\u5199\u6587\u672c\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u5b57\u7b26\u9891\u7387\u5206\u5e03\u53d8\u5316\u65f6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u624b\u5199\u6587\u672c\u8bc6\u522b\u56e0\u5b57\u7b26\u96c6\u968f\u65f6\u95f4\u6216\u533a\u57df\u53d8\u5316\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7279\u5b9a\u5b50\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528Wasserstein\u8ddd\u79bb\u5bf9\u9f50\u9884\u6d4b\u6587\u672c\u4e0e\u76ee\u6807\u5b57\u7b26\u9891\u7387\u5206\u5e03\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u5f15\u5bfc\u89e3\u7801\u65b9\u6848\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u5747\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b57\u7b26\u9891\u7387\u5206\u5e03\u53d8\u5316\u5e26\u6765\u7684\u95ee\u9898\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2506.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09849", "abs": "https://arxiv.org/abs/2506.09849", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "comment": null, "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "AI": {"tldr": "IntPhys 2\u662f\u4e00\u4e2a\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u76f4\u89c2\u7269\u7406\u7684\u7406\u89e3\u80fd\u529b\uff0c\u57fa\u4e8e\u56db\u4e2a\u6838\u5fc3\u539f\u5219\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u5dee\u8ddd\u663e\u8457\u3002", "motivation": "\u8bc4\u4f30\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u76f4\u89c2\u7269\u7406\u7684\u7406\u89e3\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u57fa\u4e8e\u8fdd\u53cd\u671f\u671b\u6846\u67b6\u8bbe\u8ba1\u6d4b\u8bd5\uff0c\u6db5\u76d6\u56db\u4e2a\u6838\u5fc3\u539f\u5219\uff08\u6c38\u4e45\u6027\u3001\u4e0d\u53ef\u53d8\u6027\u3001\u65f6\u7a7a\u8fde\u7eed\u6027\u548c\u56fa\u4f53\u6027\uff09\uff0c\u5728\u591a\u6837\u5316\u865a\u62df\u73af\u5883\u4e2d\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u63a5\u8fd1\u968f\u673a\uff0850%\uff09\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u5728\u76f4\u89c2\u7269\u7406\u7406\u89e3\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2506.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09881", "abs": "https://arxiv.org/abs/2506.09881", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "comment": null, "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVireo\u7684\u5355\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47\u9886\u57df\u901a\u7528\u8bed\u4e49\u5206\u5272\uff08OV-DGSS\uff09\uff0c\u7ed3\u5408\u4e86\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff08OVSS\uff09\u548c\u9886\u57df\u901a\u7528\u8bed\u4e49\u5206\u5272\uff08DGSS\uff09\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u4e0e\u8bed\u8a00\u7ebf\u7d22\u5bf9\u9f50\u7b49\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u548c\u9886\u57df\u901a\u7528\u8bed\u4e49\u5206\u5272\u7684\u4e92\u8865\u6027\u4fc3\u4f7f\u4e86OV-DGSS\u7684\u7814\u7a76\uff0c\u65e8\u5728\u4e3a\u672a\u89c1\u7c7b\u522b\u751f\u6210\u50cf\u7d20\u7ea7\u63a9\u7801\u5e76\u4fdd\u6301\u8de8\u9886\u57df\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7b49\u73b0\u5b9e\u573a\u666f\u3002", "method": "Vireo\u6846\u67b6\u57fa\u4e8e\u51bb\u7ed3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6VFMs\u5f15\u5165\u573a\u666f\u51e0\u4f55\u7279\u5f81\uff0c\u63d0\u51faGeoText Prompts\u3001CMPE\u548cDOV-VEH\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u4ee5\u5bf9\u9f50\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u5e76\u63d0\u5347\u6027\u80fd\u3002", "result": "Vireo\u5728\u9886\u57df\u901a\u7528\u6027\u548c\u5f00\u653e\u8bcd\u6c47\u8bc6\u522b\u65b9\u9762\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Vireo\u4e3a\u591a\u6837\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09883", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09883", "abs": "https://arxiv.org/abs/2506.09883", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u6807\u6ce8\u7684\u5fae\u8c03\u6846\u67b6Geometric Distillation\uff0c\u901a\u8fc7\u6ce8\u5165\u51e0\u4f55\u7ebf\u7d22\u63d0\u5347\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u76843D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u7a7a\u95f4\u7ed3\u6784\u7406\u89e3\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u65b9\u6cd5\u63d0\u5347\u51763D\u611f\u77e5\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4ece\u73b0\u6210\u76843D\u57fa\u7840\u6a21\u578b\uff08\u5982MASt3R\u3001VGGT\uff09\u4e2d\u63d0\u53d6\u7a00\u758f\u5bf9\u5e94\u3001\u76f8\u5bf9\u6df1\u5ea6\u5173\u7cfb\u548c\u5bc6\u96c6\u6210\u672c\u4f53\u79ef\uff0c\u6ce8\u5165\u51e0\u4f55\u7ebf\u7d22\uff0c\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002", "result": "\u57283D\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u548c3D\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u4e3a2D\u8bad\u7ec3\u7684VLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u76843D\u7406\u89e3\u8def\u5f84\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u591a\u6a21\u6001\u4efb\u52a1\u3002"}}
{"id": "2506.09885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09885", "abs": "https://arxiv.org/abs/2506.09885", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "comment": null, "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u7528\u6027\u65b0\u89c6\u89d2\u5408\u6210\uff08NVS\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u51cf\u5c113D\u77e5\u8bc6\u4f9d\u8d56\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u63a2\u7d223D\u77e5\u8bc6\u5728\u65b0\u89c6\u89d2\u5408\u6210\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u9a8c\u8bc1\u51cf\u5c11\u5176\u4f9d\u8d56\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6700\u5c0f\u53163D\u5f52\u7eb3\u504f\u5dee\u548c\u59ff\u6001\u4f9d\u8d56\u7684\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u7a00\u758f2D\u56fe\u50cf\u5b66\u4e603D\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u903c\u771f\u4e143D\u4e00\u81f4\u7684\u65b0\u89c6\u89d2\uff0c\u6027\u80fd\u4e0e\u4f9d\u8d56\u59ff\u6001\u8f93\u5165\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u51cf\u5c113D\u77e5\u8bc6\u4f9d\u8d56\u7684\u540c\u65f6\uff0c\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2506.09895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09895", "abs": "https://arxiv.org/abs/2506.09895", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "comment": "19 pages, 11 Figures, 13 Tables", "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEquiCaps\uff0c\u4e00\u79cd\u57fa\u4e8e\u80f6\u56ca\u7f51\u7edc\u7684\u81ea\u6211\u76d1\u7763\u65b9\u6cd5\uff0c\u65e0\u9700\u4e13\u7528\u9884\u6d4b\u5668\u5373\u53ef\u5b9e\u73b0\u59ff\u6001\u611f\u77e5\u8868\u793a\uff0c\u5e76\u5728\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u80f6\u56ca\u7f51\u7edc\u56fa\u6709\u7684\u59ff\u6001\u611f\u77e5\u80fd\u529b\uff0c\u907f\u514d\u4f9d\u8d56\u4e13\u7528\u9884\u6d4b\u5668\u6765\u5b9e\u73b0\u7b49\u53d8\u6027\uff0c\u4ece\u800c\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u5f15\u5165EquiCaps\uff0c\u5229\u7528\u80f6\u56ca\u7f51\u7edc\u7684\u56fa\u6709\u7279\u6027\u5b9e\u73b0\u59ff\u6001\u611f\u77e5\u81ea\u6211\u76d1\u7763\uff0c\u5e76\u901a\u8fc7\u591a\u51e0\u4f55\u53d8\u6362\u4efb\u52a1\u548c3DIEBench-T\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "result": "EquiCaps\u5728\u65cb\u8f6c\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cR\u00b2\u8fbe\u52300.78\uff0c\u5e76\u5728\u590d\u6742\u51e0\u4f55\u53d8\u6362\u4e0b\u4fdd\u6301\u7a33\u5065\u7684\u7b49\u53d8\u6027\u3002", "conclusion": "EquiCaps\u5c55\u793a\u4e86\u80f6\u56ca\u7f51\u7edc\u5728\u65e0\u9700\u9884\u6d4b\u5668\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7b49\u53d8\u6027\u7684\u6f5c\u529b\uff0c\u4e3a\u59ff\u6001\u611f\u77e5\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09897", "abs": "https://arxiv.org/abs/2506.09897", "authors": ["Tao Liu", "Zhenchao Cui"], "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "comment": null, "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faE-FPN-BS\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u4f18\u5316\u89e3\u51b3\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u9ad8\u5c42\u7279\u5f81\u672a\u8bad\u7ec3\u7684\u95ee\u9898\u3002", "motivation": "\u6807\u51c6\u6807\u7b7e\u5206\u914d\u534f\u8bae\u4e0b\uff0c\u9ad8\u5c42\u7279\u5f81\uff08P5-P6\uff09\u5e38\u56e0\u96f6\u6b63\u6837\u672c\u951a\u70b9\u800c\u672a\u88ab\u8bad\u7ec3\uff0c\u5bfc\u81f4\u8bed\u4e49\u8868\u793a\u7f3a\u5931\u548c\u4f4e\u5c42\u7279\u5f81\u7f3a\u4e4f\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "method": "\u63d0\u51faE-FPN-BS\u67b6\u6784\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u589e\u5f3a\u6a21\u5757\uff08CEM\uff09\u548c\u524d\u666f-\u80cc\u666f\u5206\u79bb\u6a21\u5757\uff08FBSM\uff09\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u68af\u5ea6\u5e73\u8861\u635f\u5931\uff08DCLoss\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u5f02\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "E-FPN-BS\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u9ad8\u5c42\u7279\u5f81\u672a\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2506.09916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09916", "abs": "https://arxiv.org/abs/2506.09916", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "comment": null, "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOnly-Style\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u56fe\u50cf\u751f\u6210\u4e2d\u5185\u5bb9\u6cc4\u6f0f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u53c2\u6570\u548c\u5c40\u90e8\u5316\u5185\u5bb9\u6cc4\u6f0f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u96be\u4ee5\u6709\u6548\u5206\u79bb\u8bed\u4e49\u5185\u5bb9\u548c\u98ce\u683c\u5143\u7d20\uff0c\u5bfc\u81f4\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\u3002", "method": "Only-Style\u901a\u8fc7\u5c40\u90e8\u5316\u5185\u5bb9\u6cc4\u6f0f\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u98ce\u683c\u5bf9\u9f50\u53c2\u6570\uff0c\u5e73\u8861\u98ce\u683c\u4e00\u81f4\u6027\u548c\u6cc4\u6f0f\u6d88\u9664\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5b9e\u4f8b\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u65e0\u5185\u5bb9\u6cc4\u6f0f\u7684\u7a33\u5065\u98ce\u683c\u4e00\u81f4\u6027\u3002", "conclusion": "Only-Style\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u89e3\u51b3\u5185\u5bb9\u6cc4\u6f0f\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2506.09919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09919", "abs": "https://arxiv.org/abs/2506.09919", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "comment": null, "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "AI": {"tldr": "MetricHMR\u662f\u4e00\u79cd\u4ece\u5355\u76ee\u56fe\u50cf\u4e2d\u6062\u590d\u5177\u6709\u7cbe\u786e\u5168\u5c40\u5e73\u79fb\u7684\u5ea6\u91cf\u4eba\u4f53\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709HMR\u65b9\u6cd5\u5728\u5c3a\u5ea6\u548c\u6df1\u5ea6\u4e0a\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709HMR\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u7684\u5c3a\u5ea6\u548c\u6df1\u5ea6\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u5728\u51e0\u4f55\u4e0a\u4e0d\u5408\u7406\u3002MetricHMR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u5ea6\u91cf\u5c3a\u5ea6\u7684\u4eba\u4f53\u7f51\u683c\u6062\u590d\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u73b0\u6709HMR\u65b9\u6cd5\u7684\u76f8\u673a\u6a21\u578b\uff0c\u5f3a\u8c03\u6807\u51c6\u900f\u89c6\u6295\u5f71\u6a21\u578b\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c04\u7ebf\u56fe\u7684\u65b0\u65b9\u6cd5\uff0c\u8054\u5408\u7f16\u7801\u8fb9\u754c\u6846\u4fe1\u606f\u3001\u76f8\u673a\u53c2\u6570\u548c\u51e0\u4f55\u7ebf\u7d22\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u5ea6\u91cfHMR\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMetricHMR\u5728\u5ea6\u91cf\u59ff\u6001\u3001\u5f62\u72b6\u548c\u5168\u5c40\u5e73\u79fb\u4f30\u8ba1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709HMR\u65b9\u6cd5\u3002", "conclusion": "MetricHMR\u901a\u8fc7\u6807\u51c6\u900f\u89c6\u6295\u5f71\u6a21\u578b\u548c\u5c04\u7ebf\u56fe\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5c3a\u5ea6\u548c\u6df1\u5ea6\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5ea6\u91cf\u4eba\u4f53\u7f51\u683c\u6062\u590d\u3002"}}
{"id": "2506.09920", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09920", "abs": "https://arxiv.org/abs/2506.09920", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "comment": null, "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u5149\u8c31\u56fe\u50cf\uff08HSI\uff09\u805a\u7c7b\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784-\u5149\u8c31\u56fe\u5377\u79ef\u7b97\u5b50\uff08SSGCO\uff09\u548c\u8bc1\u636e\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8fb9\u5b66\u4e60\uff08EGAEL\uff09\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u805a\u7c7b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528HSI\u7684\u5149\u8c31\u4fe1\u606f\uff0c\u4e14\u8d85\u50cf\u7d20\u62d3\u6251\u56fe\u7684\u4e0d\u51c6\u786e\u6027\u53ef\u80fd\u5bfc\u81f4\u7c7b\u8bed\u4e49\u6df7\u6dc6\u3002", "method": "\u63d0\u51faSSGCO\u4ee5\u540c\u65f6\u63d0\u53d6\u7a7a\u95f4\u548c\u5149\u8c31\u7279\u5f81\uff0c\u5e76\u8bbe\u8ba1EGAEL\u6a21\u5757\u81ea\u9002\u5e94\u4f18\u5316\u8fb9\u6743\u91cd\u3002\u65b9\u6cd5\u96c6\u6210\u5230\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u8868\u793a\u5b66\u4e60\u548c\u805a\u7c7b\u7684\u540c\u6b65\u8fdb\u884c\u3002", "result": "\u5728\u56db\u4e2aHSI\u6570\u636e\u96c6\u4e0a\uff0c\u805a\u7c7b\u7cbe\u5ea6\u5206\u522b\u63d0\u5347\u4e862.61%\u30016.06%\u30014.96%\u548c3.15%\u3002", "conclusion": "SSGCO\u548cEGAEL\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86HSI\u805a\u7c7b\u7684\u8868\u73b0\uff0c\u65b9\u6cd5\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.09932", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09932", "abs": "https://arxiv.org/abs/2506.09932", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "comment": "4 Pages, 5 Figures", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "AI": {"tldr": "HadaNorm\u662f\u4e00\u79cd\u65b0\u578b\u7ebf\u6027\u53d8\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u6fc0\u6d3b\u7279\u5f81\u901a\u9053\u5e76\u7ed3\u5408Hadamard\u53d8\u6362\uff0c\u6709\u6548\u51cf\u5c11\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u66f4\u6fc0\u8fdb\u7684\u6fc0\u6d3b\u91cf\u5316\uff0c\u63d0\u5347\u6269\u6563\u6a21\u578b\u7684\u91cf\u5316\u6548\u679c\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u540e\u8bad\u7ec3\u91cf\u5316\uff08PTQ\uff09\u867d\u80fd\u964d\u4f4e\u77e9\u9635\u64cd\u4f5c\u7684\u4f4d\u5bbd\uff0c\u4f46\u6807\u51c6\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5f02\u5e38\u503c\uff0c\u4e14\u9ad8\u538b\u7f29\u9700\u989d\u5916\u6743\u91cd\u548c\u6fc0\u6d3b\u53d8\u6362\u3002", "method": "\u63d0\u51faHadaNorm\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f52\u4e00\u5316\u6fc0\u6d3b\u7279\u5f81\u901a\u9053\u5e76\u5e94\u7528Hadamard\u53d8\u6362\uff0c\u51cf\u5c11\u5f02\u5e38\u503c\u5f71\u54cd\uff0c\u4ece\u800c\u652f\u6301\u66f4\u6fc0\u8fdb\u7684\u6fc0\u6d3b\u91cf\u5316\u3002", "result": "HadaNorm\u5728Transformer\u5757\u7684\u5404\u7ec4\u4ef6\u4e2d\u4e00\u81f4\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002", "conclusion": "HadaNorm\u4e3a\u6269\u6563\u6a21\u578b\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2506.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09935", "abs": "https://arxiv.org/abs/2506.09935", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "comment": "Project page: https://leo-vl.github.io", "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLEO-VL\u6a21\u578b\uff0c\u57fa\u4e8e\u9ad8\u6548\u573a\u666f\u8868\u793aCFG\uff0c\u89e3\u51b3\u4e863D-VL\u901a\u7528\u6a21\u578b\u7684\u6570\u636e\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u7406\u89e33D\u573a\u666f\u5e76\u6267\u884c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u76843D-VL\u901a\u7528\u6a21\u578b\u662f\u957f\u671f\u76ee\u6807\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u80fd\u529b\u548c\u9c81\u68d2\u6027\u4e0a\u843d\u540e\u4e8e2D\u6a21\u578b\uff0c\u4e3b\u8981\u969c\u788d\u662f\u6570\u636e\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faLEO-VL\u6a21\u578b\uff0c\u91c7\u7528CFG\uff08\u9ad8\u6548\u573a\u666f\u8868\u793a\uff09\u51cf\u5c11token\u5f00\u9500\uff0c\u5e76\u57fa\u4e8e700k\u9ad8\u8d28\u91cf3D-VL\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "LEO-VL\u5728\u591a\u4e2a3D QA\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982SQA3D\u3001MSQA\u3001Beacon3D\uff09\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "CFG\u7684\u9ad8\u6548\u6027\u3001\u4efb\u52a1\u548c\u573a\u666f\u591a\u6837\u6027\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u6570\u636e\u7b5b\u9009\u539f\u5219\u7684\u6709\u6548\u6027\u5f97\u5230\u9a8c\u8bc1\uff0c\u540c\u65f6\u63d0\u51fa\u7684SceneDPO\u589e\u5f3a\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e863D-VL\u901a\u7528\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.09943", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2506.09943", "abs": "https://arxiv.org/abs/2506.09943", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "AI": {"tldr": "CausalVQA\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\uff08VQA\uff09\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u6d4b\u8bd5\u6a21\u578b\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u56e0\u679c\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VQA\u6570\u636e\u96c6\u591a\u5173\u6ce8\u8868\u9762\u611f\u77e5\u6216\u72ed\u7a84\u7684\u7269\u7406\u63a8\u7406\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u56e0\u679c\u5173\u7cfb\u7684\u6df1\u5165\u63a2\u7a76\u3002CausalVQA\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6570\u636e\u96c6\u5305\u542b\u4e94\u79cd\u95ee\u9898\u7c7b\u578b\uff08\u53cd\u4e8b\u5b9e\u3001\u5047\u8bbe\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u63cf\u8ff0\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u8d28\u91cf\u63a7\u5236\u673a\u5236\u907f\u514d\u6a21\u578b\u5229\u7528\u7b80\u5355\u6377\u5f84\u3002", "result": "\u5f53\u524d\u524d\u6cbf\u591a\u6a21\u6001\u6a21\u578b\u5728CausalVQA\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u5728\u9884\u6d4b\u548c\u5047\u8bbe\u95ee\u9898\u4e0a\u3002", "conclusion": "CausalVQA\u63ed\u793a\u4e86\u5f53\u524d\u7cfb\u7edf\u5728\u65f6\u7a7a\u63a8\u7406\u3001\u7269\u7406\u539f\u7406\u7406\u89e3\u548c\u66ff\u4ee3\u65b9\u6848\u9884\u6d4b\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2506.09952", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09952", "abs": "https://arxiv.org/abs/2506.09952", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "comment": "Accepted to CVPR 2025", "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "AI": {"tldr": "UniPre3D\u662f\u4e00\u79cd\u7edf\u4e00\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u5c3a\u5ea6\u7684\u70b9\u4e91\u548c\u4efb\u4f55\u67b6\u6784\u76843D\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u9ad8\u65af\u57fa\u5143\u548c\u4f7f\u7528\u53ef\u5fae\u5206\u9ad8\u65af\u6e32\u67d3\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u70b9\u4e91\u6570\u636e\u7684\u5c3a\u5ea6\u591a\u6837\u6027\u5bf9\u7edf\u4e00\u8868\u793a\u5b66\u4e60\u6280\u672f\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u76ee\u524d\u7f3a\u4e4f\u9002\u7528\u4e8e\u5bf9\u8c61\u548c\u573a\u666f\u7ea7\u70b9\u4e91\u7684\u7edf\u4e00\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUniPre3D\uff0c\u9884\u6d4b\u9ad8\u65af\u57fa\u5143\u4f5c\u4e3a\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u9ad8\u65af\u6e32\u67d3\u548c2D\u7279\u5f81\u4ee5\u4f18\u5316\u51e0\u4f55\u7ed3\u6784\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5bf9\u8c61\u548c\u573a\u666f\u7ea7\u4efb\u52a1\u4e2d\u7684\u666e\u9002\u6709\u6548\u6027\u3002", "conclusion": "UniPre3D\u662f\u9996\u4e2a\u80fd\u65e0\u7f1d\u5e94\u7528\u4e8e\u4e0d\u540c\u5c3a\u5ea6\u70b9\u4e91\u7684\u7edf\u4e00\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2506.09954", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.09954", "abs": "https://arxiv.org/abs/2506.09954", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "title": "Vision Generalist Model: A Survey", "comment": "Accepted by International Journal of Computer Vision (IJCV)", "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u901a\u7528\u6a21\u578b\uff0c\u63a2\u8ba8\u4e86\u5176\u7279\u70b9\u3001\u80fd\u529b\u53ca\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u80cc\u666f\u56de\u987e\u3001\u6846\u67b6\u8bbe\u8ba1\u3001\u6027\u80fd\u63d0\u5347\u6280\u672f\u3001\u76f8\u5173\u9886\u57df\u5173\u8054\u4ee5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u901a\u7528\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6210\u529f\u6fc0\u53d1\u4e86\u5c06\u5176\u5e94\u7528\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u5174\u8da3\uff0c\u4f46\u89c6\u89c9\u4efb\u52a1\u7684\u8f93\u5165\u8f93\u51fa\u591a\u6837\u6027\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u56de\u987e\u80cc\u666f\uff08\u6570\u636e\u96c6\u3001\u4efb\u52a1\u3001\u57fa\u51c6\uff09\u3001\u5206\u6790\u73b0\u6709\u6846\u67b6\u8bbe\u8ba1\u3001\u4ecb\u7ecd\u6027\u80fd\u63d0\u5347\u6280\u672f\u3001\u63a2\u8ba8\u76f8\u5173\u9886\u57df\u5173\u8054\u3002", "result": "\u63d0\u4f9b\u4e86\u89c6\u89c9\u901a\u7528\u6a21\u578b\u7684\u5168\u9762\u6982\u8ff0\uff0c\u5305\u62ec\u5176\u5e94\u7528\u573a\u666f\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524d\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6df1\u5165\u7406\u89e3\u8be5\u9886\u57df\u7684\u6307\u5bfc\u3002"}}
{"id": "2506.09958", "categories": ["cs.CV", "cs.LG", "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)", "I.2.10; I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2506.09958", "abs": "https://arxiv.org/abs/2506.09958", "authors": ["Sushant Gautam", "Michael A. Riegler", "P\u00e5l Halvorsen"], "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "AI": {"tldr": "Kvasir-VQA-x1\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u6570\u636e\u96c6\uff0c\u65e8\u5728\u901a\u8fc7\u589e\u52a0\u4e34\u5e8a\u590d\u6742\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\u63a8\u52a8MedVQA\u9886\u57df\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709MedVQA\u6570\u636e\u96c6\u7f3a\u4e4f\u4e34\u5e8a\u590d\u6742\u6027\u548c\u89c6\u89c9\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u8fdb\u5c55\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210159,549\u4e2a\u65b0\u95ee\u7b54\u5bf9\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u589e\u5f3a\u6a21\u62df\u5e38\u89c1\u6210\u50cf\u4f2a\u5f71\uff0c\u652f\u6301\u4e24\u4e2a\u8bc4\u4f30\u8f68\u9053\u3002", "result": "Kvasir-VQA-x1\u63d0\u4f9b\u4e86\u66f4\u5177\u6311\u6218\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\u7684\u57fa\u51c6\uff0c\u4fc3\u8fdb\u591a\u6a21\u6001AI\u7cfb\u7edf\u5f00\u53d1\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u9075\u5faaFAIR\u539f\u5219\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u52a0\u901f\u4e34\u5e8aAI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2506.09965", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2506.09965", "abs": "https://arxiv.org/abs/2506.09965", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "comment": null, "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u7a7a\u95f4\u7ed8\u56fe\u64cd\u4f5c\u589e\u5f3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u4e3b\u8981\u4f9d\u8d56\u7eaf\u6587\u672c\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u5173\u7cfb\u7684\u7cbe\u786e\u51e0\u4f55\u7406\u89e3\u548c\u8fde\u7eed\u8ddf\u8e2a\u80fd\u529b\uff0c\u5bfc\u81f4\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cdrawing to reason in space\u201d\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8d4b\u4e88\u6a21\u578b\u57fa\u672c\u7ed8\u56fe\u64cd\u4f5c\uff08\u5982\u6807\u6ce8\u8fb9\u754c\u6846\u548c\u7ed8\u5236\u8f85\u52a9\u7ebf\uff09\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u89c6\u89c9\u7a7a\u95f4\u4e2d\u76f4\u63a5\u8868\u8fbe\u548c\u5206\u6790\u7a7a\u95f4\u5173\u7cfb\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a\u5408\u6210\u6570\u636e\u51b7\u542f\u52a8\u8bad\u7ec3\u3001\u53cd\u601d\u62d2\u7edd\u91c7\u6837\u548c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u6a21\u578bVILASR\u5728\u8ff7\u5bab\u5bfc\u822a\u3001\u9759\u6001\u7a7a\u95f4\u63a8\u7406\u3001\u89c6\u9891\u63a8\u7406\u548c\u591a\u89c6\u89d2\u63a8\u7406\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u5347\u4e8618.4%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u89c6\u89c9\u7ed8\u56fe\u64cd\u4f5c\u589e\u5f3aLVLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u662f\u6709\u6548\u7684\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.09969", "categories": ["cs.CV", "I.3.3; I.5"], "pdf": "https://arxiv.org/pdf/2506.09969", "abs": "https://arxiv.org/abs/2506.09969", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "comment": null, "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5f15\u5bfc\u7684\u56fe\u50cf\u5230\u7ed8\u753b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7b14\u89e6\u53c2\u6570\u548c\u987a\u5e8f\u6e32\u67d3\uff0c\u63d0\u5347\u7ed8\u753b\u7684\u827a\u672f\u6027\u548c\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7b14\u89e6\u7ed8\u753b\u7cfb\u7edf\u5728\u6355\u6349\u827a\u672f\u610f\u56fe\u548c\u539f\u5219\u65f6\u7684\u4e0d\u8db3\uff0c\u5f25\u5408\u9759\u6001\u827a\u672f\u54c1\u4e0e\u5176\u521b\u4f5c\u8fc7\u7a0b\u4e4b\u95f4\u7684\u60c5\u611f\u4e0e\u6559\u80b2\u9e3f\u6c9f\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u5f15\u5bfc\u3001\u7b14\u89e6\u53c2\u6570\u8ba1\u7b97\u548c\u987a\u5e8f\u6e32\u67d3\u7b56\u7565\uff0c\u5b9e\u73b0\u533a\u57df\u5316\u7684\u7ed8\u753b\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8f93\u5165\u56fe\u50cf\u4e0a\u5747\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u4e14\u7b14\u89e6\u8d28\u91cf\u4f18\u8d8a\u7684\u7ed8\u753b\u4f5c\u54c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u7ed8\u753b\u751f\u6210\u7684\u827a\u672f\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u827a\u672f\u521b\u4f5c\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.09980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09980", "abs": "https://arxiv.org/abs/2506.09980", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4efb\u610f\u6570\u91cf\u8bed\u4e49\u6709\u610f\u4e49\u90e8\u5206\u76843D\u5bf9\u8c61\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u5355\u4e00\u7f51\u683c\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u67093D\u5bf9\u8c61\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u751f\u6210\u5355\u4e00\u7f51\u683c\uff0c\u9650\u5236\u4e86\u90e8\u5206\u7f16\u8f91\u80fd\u529b\uff0c\u4e14\u4e0d\u540c\u5bf9\u8c61\u7684\u90e8\u5206\u6570\u91cf\u53ef\u53d8\u3002", "method": "\u91c7\u7528\u53cc\u4f53\u79ef\u6253\u5305\u7b56\u7565\uff0c\u5c06\u6240\u6709\u90e8\u5206\u7ec4\u7ec7\u5230\u4e24\u4e2a\u4e92\u8865\u7684\u4f53\u79ef\u4e2d\uff0c\u751f\u6210\u5b8c\u6574\u4e14\u4ea4\u9519\u7684\u90e8\u5206\uff0c\u6700\u7ec8\u7ec4\u88c5\u6210\u5bf9\u8c61\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u3001\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u4e4b\u524d\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u90e8\u5206\u7ea7\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u90e8\u5206\u7ea73D\u5bf9\u8c61\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u7f16\u8f91\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.09981", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.09981", "abs": "https://arxiv.org/abs/2506.09981", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "title": "ReSim: Reliable World Simulation for Autonomous Driving", "comment": "Project page: https://opendrivelab.com/ReSim", "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReSim\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u548c\u6a21\u62df\u5668\u4e2d\u7684\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u63d0\u5347\u9a7e\u9a76\u573a\u666f\u6a21\u62df\u7684\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u5f15\u5165Video2Reward\u6a21\u5757\u8bc4\u4f30\u52a8\u4f5c\u5956\u52b1\u3002", "motivation": "\u73b0\u6709\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u4ec5\u57fa\u4e8e\u771f\u5b9e\u5b89\u5168\u9a7e\u9a76\u6570\u636e\uff0c\u96be\u4ee5\u6a21\u62df\u5371\u9669\u6216\u975e\u4e13\u5bb6\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5176\u5728\u7b56\u7565\u8bc4\u4f30\u7b49\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u548c\u6a21\u62df\u5668\u4e2d\u7684\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u6784\u5efa\u53ef\u63a7\u4e16\u754c\u6a21\u578b\uff0c\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u7684\u89c6\u9891\u751f\u6210\u5668\uff0c\u5e76\u8bbe\u8ba1\u7b56\u7565\u63d0\u5347\u9884\u6d4b\u53ef\u63a7\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "result": "ReSim\u6a21\u578b\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u63d0\u534744%\uff0c\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u884c\u4e3a\u7684\u53ef\u63a7\u6027\u63d0\u5347\u8d8550%\uff0c\u5728NAVSIM\u4e0a\u7684\u89c4\u5212\u548c\u7b56\u7565\u9009\u62e9\u6027\u80fd\u5206\u522b\u63d0\u53472%\u548c25%\u3002", "conclusion": "ReSim\u901a\u8fc7\u6570\u636e\u591a\u6837\u5316\u548c\u53ef\u63a7\u6027\u63d0\u5347\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u4e16\u754c\u9a7e\u9a76\u573a\u666f\u7684\u53ef\u9760\u6a21\u62df\uff0c\u4e3a\u7b56\u7565\u8bc4\u4f30\u7b49\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.09982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09982", "abs": "https://arxiv.org/abs/2506.09982", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "AI": {"tldr": "AnimateAnyMesh\u662f\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u9a71\u52a8\u7684\u9ad8\u6548\u52a8\u753b\u751f\u6210\u6846\u67b6\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u4efb\u610f3D\u7f51\u683c\u7684\u52a8\u753b\u5316\uff0c\u901a\u8fc7DyMeshVAE\u67b6\u6784\u548cRectified Flow\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e864D\u5185\u5bb9\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d4D\u5185\u5bb9\u751f\u6210\u9762\u4e34\u5efa\u6a21\u65f6\u7a7a\u5206\u5e03\u7684\u590d\u6742\u6027\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0cAnimateAnyMesh\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528DyMeshVAE\u67b6\u6784\u5206\u79bb\u65f6\u7a7a\u7279\u5f81\u5e76\u4fdd\u7559\u5c40\u90e8\u62d3\u6251\u7ed3\u6784\uff0c\u7ed3\u5408Rectified Flow\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6587\u672c\u6761\u4ef6\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u51e0\u79d2\u5185\u751f\u6210\u8bed\u4e49\u51c6\u786e\u4e14\u65f6\u95f4\u8fde\u8d2f\u7684\u7f51\u683c\u52a8\u753b\uff0c\u8d28\u91cf\u548c\u6548\u7387\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnimateAnyMesh\u663e\u8457\u63a8\u52a8\u4e864D\u5185\u5bb9\u751f\u6210\u7684\u5b9e\u7528\u5316\u548c\u666e\u53ca\u5316\uff0c\u6240\u6709\u6570\u636e\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2506.09984", "categories": ["cs.CV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2506.09984", "abs": "https://arxiv.org/abs/2506.09984", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u57df\u7279\u5b9a\u7684\u6761\u4ef6\u7ed1\u5b9a\uff0c\u5b9e\u73b0\u591a\u6982\u5ff5\uff08\u5982\u591a\u4eba\u548c\u7269\u4f53\uff09\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u652f\u6301\u5355\u4e00\u4e3b\u4f53\u548c\u5168\u5c40\u6761\u4ef6\u6ce8\u5165\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u591a\u6982\u5ff5\u573a\u666f\u4e2d\u7684\u4eba-\u4eba\u548c\u4eba-\u7269\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5c40\u90e8\u6761\u4ef6\u7ed1\u5b9a\u5b9e\u73b0\u591a\u6982\u5ff5\u7684\u7cbe\u786e\u63a7\u5236\u3002", "method": "\u5229\u7528\u63a9\u7801\u9884\u6d4b\u5668\u81ea\u52a8\u63a8\u65ad\u5e03\u5c40\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u5c06\u5c40\u90e8\u97f3\u9891\u6761\u4ef6\u6ce8\u5165\u5bf9\u5e94\u533a\u57df\uff0c\u5b9e\u73b0\u5e03\u5c40\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u663e\u5f0f\u5e03\u5c40\u63a7\u5236\u4f18\u4e8e\u9690\u5f0f\u65b9\u6cd5\u548c\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u9ad8\u8d28\u91cf\u751f\u6210\u53ef\u63a7\u7684\u591a\u6982\u5ff5\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\uff0c\u4e3a\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.09987", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09987", "abs": "https://arxiv.org/abs/2506.09987", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "comment": null, "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MVP\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u4e16\u754c\u7406\u89e3\u80fd\u529b\uff0c\u907f\u514d\u73b0\u6709\u57fa\u51c6\u56e0\u8868\u9762\u89c6\u89c9\u6216\u6587\u672c\u7ebf\u7d22\u5bfc\u81f4\u7684\u8bc4\u5206\u81a8\u80c0\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6613\u53d7\u8868\u9762\u7ebf\u7d22\u5f71\u54cd\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u4e0d\u51c6\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165MVP\u57fa\u51c6\uff0c\u5305\u542b55K\u9ad8\u8d28\u91cf\u591a\u9009\u9898\u89c6\u9891QA\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u6700\u5c0f\u53d8\u5316\u5bf9\uff0c\u8981\u6c42\u6a21\u578b\u540c\u65f6\u6b63\u786e\u56de\u7b54\u4e24\u4e2a\u95ee\u9898\u4ee5\u907f\u514d\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u3002", "result": "\u4eba\u7c7b\u8868\u73b092.9%\uff0c\u6700\u4f73\u5f00\u6e90\u6a21\u578b40.2%\uff0c\u968f\u673a\u8868\u73b025%\u3002", "conclusion": "MVP\u57fa\u51c6\u6709\u6548\u907f\u514d\u4e86\u8868\u9762\u7ebf\u7d22\u7684\u5e72\u6270\uff0c\u4e3a\u8bc4\u4f30\u89c6\u9891\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u7406\u89e3\u80fd\u529b\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2506.09988", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09988", "abs": "https://arxiv.org/abs/2506.09988", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "comment": null, "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "AI": {"tldr": "EditInspector\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u8d28\u91cf\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5168\u9762\u8bc4\u4f30\u7f16\u8f91\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u6539\u8fdb\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u53d1\u5c55\uff0c\u6587\u672c\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4f\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u9a8c\u8bc1\u548c\u8bc4\u4f30\u8fd9\u4e9b\u7f16\u8f91\u7684\u8d28\u91cf\u3002", "method": "\u5f15\u5165EditInspector\u57fa\u51c6\uff0c\u5229\u7528\u4eba\u5de5\u6807\u6ce8\u548c\u6a21\u677f\u9a8c\u8bc1\u7f16\u8f91\uff0c\u5e76\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\u3002\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\u6539\u8fdb\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u8bc4\u4f30\u7f16\u8f91\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e38\u4ea7\u751f\u5e7b\u89c9\u63cf\u8ff0\u3002\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u5728\u4f2a\u5f71\u68c0\u6d4b\u548c\u5dee\u5f02\u63cf\u8ff0\u751f\u6210\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "EditInspector\u4e3a\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2506.09989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09989", "abs": "https://arxiv.org/abs/2506.09989", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u9884\u6d4b\u4eba\u624b\u4e0e3D\u573a\u666f\u7269\u7406\u4ea4\u4e92\u7684\u58f0\u97f3\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f3D\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u58f0\u97f3\u589e\u5f3a3D\u573a\u666f\u7684\u4ea4\u4e92\u6027\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4e2d\u4eba\u624b\u4e0e\u7269\u4f53\u7684\u4e92\u52a8\u3002", "method": "\u8bb0\u5f55\u4eba\u624b\u64cd\u4f5c3D\u573a\u666f\u4e2d\u7269\u4f53\u7684\u89c6\u9891\uff0c\u5229\u7528\u52a8\u4f5c-\u58f0\u97f3\u5bf9\u8bad\u7ec3\u6821\u6b63\u6d41\u6a21\u578b\uff0c\u5c063D\u624b\u90e8\u8f68\u8ff9\u6620\u5c04\u5230\u5bf9\u5e94\u97f3\u9891\u3002", "result": "\u751f\u6210\u7684\u97f3\u9891\u80fd\u51c6\u786e\u4f20\u8fbe\u6750\u8d28\u7279\u6027\u548c\u52a8\u4f5c\uff0c\u4eba\u7c7b\u89c2\u5bdf\u8005\u96be\u4ee5\u533a\u5206\u5176\u4e0e\u771f\u5b9e\u58f0\u97f3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u901a\u8fc7\u58f0\u97f3\u9884\u6d4b\u589e\u5f3a3D\u573a\u666f\u4ea4\u4e92\u6027\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u771f\u5b9e\u611f\u3002"}}
{"id": "2506.09993", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.09993", "abs": "https://arxiv.org/abs/2506.09993", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "title": "Text-Aware Image Restoration with Diffusion Models", "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u4fee\u590d\u4efb\u52a1TAIR\uff0c\u4e13\u6ce8\u4e8e\u540c\u65f6\u6062\u590d\u89c6\u89c9\u5185\u5bb9\u548c\u6587\u672c\u4fdd\u771f\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86TeReDiff\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u8bc6\u522b\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u5728\u81ea\u7136\u56fe\u50cf\u4fee\u590d\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6587\u672c\u533a\u57df\u91cd\u5efa\u65f6\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u7684\u6587\u672c\u6a21\u5f0f\uff08\u6587\u672c\u56fe\u50cf\u5e7b\u89c9\uff09\u3002", "method": "\u63d0\u51fa\u4e86SA-Text\u57fa\u51c6\u6570\u636e\u96c6\u548cTeReDiff\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u6587\u672c\u68c0\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u63d0\u53d6\u4e30\u5bcc\u7684\u6587\u672c\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTeReDiff\u5728\u6587\u672c\u8bc6\u522b\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TAIR\u4efb\u52a1\u548cTeReDiff\u6846\u67b6\u4e3a\u89e3\u51b3\u6587\u672c\u56fe\u50cf\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2506.09995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.09995", "abs": "https://arxiv.org/abs/2506.09995", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "title": "PlayerOne: Egocentric World Simulator", "comment": "Project page: https://playerone-hku.github.io/", "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "AI": {"tldr": "PlayerOne\u662f\u9996\u4e2a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u903c\u771f\u4e16\u754c\u6a21\u62df\u5668\uff0c\u80fd\u591f\u52a8\u6001\u751f\u6210\u4e0e\u73b0\u5b9e\u573a\u666f\u4e25\u683c\u5bf9\u9f50\u7684\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u6c89\u6d78\u5f0f\u4e14\u65e0\u9650\u5236\u7684\u52a8\u6001\u73af\u5883\u63a2\u7d22\uff0c\u63a8\u52a8\u4e16\u754c\u5efa\u6a21\u53ca\u5176\u591a\u6837\u5316\u5e94\u7528\u7684\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u6d41\u7a0b\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6587\u672c-\u89c6\u9891\u5bf9\u9884\u8bad\u7ec3\u548c\u540c\u6b65\u8fd0\u52a8-\u89c6\u9891\u6570\u636e\u5fae\u8c03\uff0c\u8bbe\u8ba1\u4e86\u90e8\u5206\u89e3\u8026\u7684\u8fd0\u52a8\u6ce8\u5165\u65b9\u6848\u548c\u8054\u5408\u91cd\u5efa\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u7cbe\u786e\u63a7\u5236\u591a\u6837\u5316\u4eba\u7c7b\u8fd0\u52a8\u548c\u4e16\u754c\u4e00\u81f4\u6027\u5efa\u6a21\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PlayerOne\u5f00\u521b\u4e86\u81ea\u6211\u4e2d\u5fc3\u73b0\u5b9e\u4e16\u754c\u6a21\u62df\u7684\u65b0\u9886\u57df\uff0c\u4e3a\u4e16\u754c\u5efa\u6a21\u53ca\u5176\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
