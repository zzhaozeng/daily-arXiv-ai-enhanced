{"id": "2505.18291", "pdf": "https://arxiv.org/pdf/2505.18291", "abs": "https://arxiv.org/abs/2505.18291", "authors": ["Zifu Wan", "Yaqi Xie", "Ce Zhang", "Zhiqiu Lin", "Zihan Wang", "Simon Stepputtis", "Deva Ramanan", "Katia Sycara"], "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ACL 2025 Main. Project page:\n  https://zifuwan.github.io/InstructPart/", "summary": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/."}
{"id": "2505.18302", "pdf": "https://arxiv.org/pdf/2505.18302", "abs": "https://arxiv.org/abs/2505.18302", "authors": ["Gefei Shen", "Yung-Hong Sun", "Yu Hen Hu", "Hongrui Jiang"], "title": "Sampling Strategies for Efficient Training of Deep Learning Object Detection Algorithms", "categories": ["cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Two sampling strategies are investigated to enhance efficiency in training a\ndeep learning object detection model. These sampling strategies are employed\nunder the assumption of Lipschitz continuity of deep learning models. The first\nstrategy is uniform sampling which seeks to obtain samples evenly yet randomly\nthrough the state space of the object dynamics. The second strategy of frame\ndifference sampling is developed to explore the temporal redundancy among\nsuccessive frames in a video. Experiment result indicates that these proposed\nsampling strategies provide a dataset that yields good training performance\nwhile requiring relatively few manually labelled samples."}
{"id": "2505.18306", "pdf": "https://arxiv.org/pdf/2505.18306", "abs": "https://arxiv.org/abs/2505.18306", "authors": ["Karly Hou", "Wanhua Li", "Hanspeter Pfister"], "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to 4D Vision Workshop @ CVPR 2025", "summary": "Recently, Gaussian Splatting methods have emerged as a desirable substitute\nfor prior Radiance Field methods for novel-view synthesis of scenes captured\nwith multi-view images or videos. In this work, we propose a novel extension to\n4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual\nlearning, we hierarchically decompose the dynamic scene into a\n\"video-segment-frame\" structure, with segments dynamically adjusted by optical\nflow. Then, instead of directly predicting the time-dependent signals, we model\nthe signal as the sum of video-constant values, segment-constant values, and\nframe-specific residuals, as inspired by the success of residual learning. This\napproach allows more flexible models that adapt to highly variable scenes. We\ndemonstrate state-of-the-art visual quality and real-time rendering on several\nestablished datasets, with the greatest improvements on complex scenes with\nlarge movements, occlusions, and fine details, where current methods degrade\nmost."}
{"id": "2505.18315", "pdf": "https://arxiv.org/pdf/2505.18315", "abs": "https://arxiv.org/abs/2505.18315", "authors": ["Mariano Rivera", "Angello Hoyos"], "title": "COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification", "categories": ["cs.CV", "cs.AI", "68T07", "I.1.2; I.4.0; I.4.10; I.4.0"], "comment": "15 pages, 12 figures. Submitted to Jou. Pattern Recognition", "summary": "We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed\nexplicitly to overcome the inefficiencies found in current CNN fine-tuning\nmethods. CoLoRA can be seen as a natural extension of the convolutional\narchitectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the\ncapabilities of our method by developing and evaluating models using the widely\nadopted CNN backbone pre-trained on ImageNet. We observed that this strategy\nresults in a stable and accurate coarse-tuning procedure. Moreover, this\nstrategy is computationally efficient and significantly reduces the number of\nparameters required for fine-tuning compared to traditional methods.\nFurthermore, our method substantially improves the speed and stability of\ntraining. Our case study focuses on classifying retinal diseases from optical\ncoherence tomography (OCT) images, specifically using the OCTMNIST dataset.\nExperimental results demonstrate that a CNN backbone fine-tuned with CoLoRA\nsurpasses nearly 1\\% in accuracy. Such a performance is comparable to the\nVision Transformer, State-space discrete, and Kolmogorov-Arnold network models."}
{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement."}
{"id": "2505.18337", "pdf": "https://arxiv.org/pdf/2505.18337", "abs": "https://arxiv.org/abs/2505.18337", "authors": ["Rajarshi Bhattacharya", "Shakeeb Murtaza", "Christian Desrosiers", "Jose Dolz", "Maguelonne Heritier", "Eric Granger"], "title": "DART$^3$: Leveraging Distance for Test Time Adaptation in Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) models are known to suffer from camera bias,\nwhere learned representations cluster according to camera viewpoints rather\nthan identity, leading to significant performance degradation under\n(inter-camera) domain shifts in real-world surveillance systems when new\ncameras are added to camera networks. State-of-the-art test-time adaptation\n(TTA) methods, largely designed for classification tasks, rely on\nclassification entropy-based objectives that fail to generalize well to ReID,\nthus making them unsuitable for tackling camera bias. In this paper, we\nintroduce DART$^3$, a TTA framework specifically designed to mitigate\ncamera-induced domain shifts in person ReID. DART$^3$ (Distance-Aware Retrieval\nTuning at Test Time) leverages a distance-based objective that aligns better\nwith image retrieval tasks like ReID by exploiting the correlation between\nnearest-neighbor distance and prediction error. Unlike prior ReID-specific\ndomain adaptation methods, DART$^3$ requires no source data, architectural\nmodifications, or retraining, and can be deployed in both fully black-box and\nhybrid settings. Empirical evaluations on multiple ReID benchmarks indicate\nthat DART$^3$ and DART$^3$ LITE, a lightweight alternative to the approach,\nconsistently outperforms state-of-the-art TTA baselines, making for a viable\noption to online learning to mitigate the adverse effects of camera bias."}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs."}
{"id": "2505.18342", "pdf": "https://arxiv.org/pdf/2505.18342", "abs": "https://arxiv.org/abs/2505.18342", "authors": ["Jack Goffinet", "Youngjo Min", "Carlo Tomasi", "David E. Carlson"], "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 13 figures", "summary": "Accurate and scalable quantification of animal pose and appearance is crucial\nfor studying behavior. Current 3D pose estimation techniques, such as keypoint-\nand mesh-based techniques, often face challenges including limited\nrepresentational detail, labor-intensive annotation requirements, and expensive\nper-frame optimization. These limitations hinder the study of subtle movements\nand can make large-scale analyses impractical. We propose Pose Splatter, a\nnovel framework leveraging shape carving and 3D Gaussian splatting to model the\ncomplete pose and appearance of laboratory animals without prior knowledge of\nanimal geometry, per-frame optimization, or manual annotations. We also propose\na novel rotation-invariant visual embedding technique for encoding pose and\nappearance, designed to be a plug-in replacement for 3D keypoint data in\ndownstream behavioral analyses. Experiments on datasets of mice, rats, and\nzebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,\nPose Splatter represents subtle variations in pose, provides better\nlow-dimensional pose embeddings over state-of-the-art as evaluated by humans,\nand generalizes to unseen data. By eliminating annotation and per-frame\noptimization bottlenecks, Pose Splatter enables analysis of large-scale,\nlongitudinal behavior needed to map genotype, neural activity, and\nmicro-behavior at unprecedented resolution."}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors."}
{"id": "2505.18197", "pdf": "https://arxiv.org/pdf/2505.18197", "abs": "https://arxiv.org/abs/2505.18197", "authors": ["Kangli Wang", "Shihao Li", "Qianxi Yi", "Wei Gao"], "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression", "categories": ["cs.GR"], "comment": "22 pages, 13 figures", "summary": "Recently, immersive media and autonomous driving applications have\nsignificantly advanced through 3D Gaussian Splatting (3DGS), which offers\nhigh-fidelity rendering and computational efficiency. Despite these advantages,\n3DGS as a display-oriented representation requires substantial storage due to\nits numerous Gaussian attributes. Current compression methods have shown\npromising results but typically neglect the compression of Gaussian spatial\npositions, creating unnecessary bitstream overhead. We conceptualize Gaussian\nprimitives as point clouds and propose leveraging point cloud compression\ntechniques for more effective storage. AI-based point cloud compression\ndemonstrates superior performance and faster inference compared to MPEG\nGeometry-based Point Cloud Compression (G-PCC). However, direct application of\nexisting models to Gaussian compression may yield suboptimal results, as\nGaussian point clouds tend to exhibit globally sparse yet locally dense\ngeometric distributions that differ from conventional point cloud\ncharacteristics. To address these challenges, we introduce GausPcgc for\nGaussian point cloud geometry compression along with a specialized training\ndataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud\ncompression into Gaussian compression pipelines, achieving superior compression\nratios. The framework complements existing Gaussian compression methods while\ndelivering significant performance improvements. All code, data, and\npre-trained models will be publicly released to facilitate further research\nadvances in this field."}
{"id": "2505.18358", "pdf": "https://arxiv.org/pdf/2505.18358", "abs": "https://arxiv.org/abs/2505.18358", "authors": ["Jianyang Gu", "Haonan Wang", "Ruoxi Jia", "Saeed Vahidian", "Vyacheslav Kungurtsev", "Wei Jiang", "Yiran Chen"], "title": "CONCORD: Concept-Informed Diffusion for Dataset Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation (DD) has witnessed significant progress in creating\nsmall datasets that encapsulate rich information from large original ones.\nParticularly, methods based on generative priors show promising performance,\nwhile maintaining computational efficiency and cross-architecture\ngeneralization. However, the generation process lacks explicit controllability\nfor each sample. Previous distillation methods primarily match the real\ndistribution from the perspective of the entire dataset, whereas overlooking\nconcept completeness at the instance level. The missing or incorrectly\nrepresented object details cannot be efficiently compensated due to the\nconstrained sample amount typical in DD settings. To this end, we propose\nincorporating the concept understanding of large language models (LLMs) to\nperform Concept-Informed Diffusion (CONCORD) for dataset distillation.\nSpecifically, distinguishable and fine-grained concepts are retrieved based on\ncategory labels to inform the denoising process and refine essential object\ndetails. By integrating these concepts, the proposed method significantly\nenhances both the controllability and interpretability of the distilled image\ngeneration, without relying on pre-trained classifiers. We demonstrate the\nefficacy of CONCORD by achieving state-of-the-art performance on ImageNet-1K\nand its subsets. The code implementation is released in\nhttps://github.com/vimar-gu/CONCORD."}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning."}
{"id": "2505.18764", "pdf": "https://arxiv.org/pdf/2505.18764", "abs": "https://arxiv.org/abs/2505.18764", "authors": ["Yitian Yuan", "Qianyue He"], "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting", "categories": ["cs.GR", "I.3.7; I.3.1"], "comment": "8 pages,2 figures", "summary": "Recent works demonstrate the advantages of hardware rasterization for 3D\nGaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized\ngraphics and fixed memory footprint. However, extending these benefits to\nbackward-pass gradient computation remains challenging due to graphics pipeline\nconstraints. We present a differentiable hardware rasterizer for 3DGS that\novercomes the memory and performance limitations of tile-based software\nrasterization. Our solution employs programmable blending for per-pixel\ngradient computation combined with a hybrid gradient reduction strategy\n(quad-level + subgroup) in fragment shaders, achieving over 10x faster backward\nrasterization versus naive atomic operations and 3x speedup over the canonical\ntile-based rasterizer. Systematic evaluation reveals 16-bit render targets\n(float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving\nhigher gradient accuracy among mixed-precision rendering formats with execution\nspeeds second only to unorm8, while float32 texture incurs severe forward pass\nperformance degradation due to suboptimal hardware optimizations. Our method\nwith float16 formats demonstrates 3.07x acceleration in full pipeline execution\n(forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset,\noutperforming the baseline tile-based renderer while preserving hardware\nrasterization's memory efficiency advantages -- incurring merely 2.67% of the\nmemory overhead required for splat sorting operations. This work presents a\nunified differentiable hardware rasterization method that simultaneously\noptimizes runtime and memory usage for 3DGS, making it particularly suitable\nfor resource-constrained devices with limited memory capacity."}
{"id": "2505.18368", "pdf": "https://arxiv.org/pdf/2505.18368", "abs": "https://arxiv.org/abs/2505.18368", "authors": ["Yike Zhang", "Jack H. Noble"], "title": "Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss", "categories": ["cs.CV"], "comment": null, "summary": "Cochlear implant surgery is a treatment for individuals with severe hearing\nloss. It involves inserting an array of electrodes inside the cochlea to\nelectrically stimulate the auditory nerve and restore hearing sensation. A\ncrucial step in this procedure is mastoidectomy, a surgical intervention that\nremoves part of the mastoid region of the temporal bone, providing a critical\npathway to the cochlea for electrode placement. Accurate prediction of the\nmastoidectomy region from preoperative imaging assists presurgical planning,\nreduces surgical risks, and improves surgical outcomes. In previous work, a\nself-supervised network was introduced to predict the mastoidectomy region\nusing only preoperative CT scans. While promising, the method suffered from\nsuboptimal robustness, limiting its practical application. To address this\nlimitation, we propose a novel weakly-supervised Mamba-based framework to\npredict accurate mastoidectomy regions directly from preoperative CT scans. Our\napproach utilizes a 3D T-Distribution loss function inspired by the Student-t\ndistribution, which effectively handles the complex geometric variability\ninherent in mastoidectomy shapes. Weak supervision is achieved using the\nsegmentation results from the prior self-supervised network to eliminate the\nneed for manual data cleaning or labeling throughout the training process. The\nproposed method is extensively evaluated against state-of-the-art approaches,\ndemonstrating superior performance in predicting accurate and clinically\nrelevant mastoidectomy regions. Our findings highlight the robustness and\nefficiency of the weakly-supervised learning framework with the proposed novel\n3D T-Distribution loss."}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment."}
{"id": "2505.18772", "pdf": "https://arxiv.org/pdf/2505.18772", "abs": "https://arxiv.org/abs/2505.18772", "authors": ["Michal Edelstein", "Hsueh-Ti Derek Liu", "Mirela Ben-Chen"], "title": "CageNet: A Meta-Framework for Learning on Wild Meshes", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 13 figures (excluding supplementary material)", "summary": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes."}
{"id": "2505.18381", "pdf": "https://arxiv.org/pdf/2505.18381", "abs": "https://arxiv.org/abs/2505.18381", "authors": ["Yike Zhang", "Eduardo Davalos Anaya", "Jack H. Noble"], "title": "Monocular Marker-free Patient-to-Image Intraoperative Registration for Cochlear Implant Surgery", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel method for monocular patient-to-image\nintraoperative registration, specifically designed to operate without any\nexternal hardware tracking equipment or fiducial point markers. Leveraging a\nsynthetic microscopy surgical scene dataset with a wide range of\ntransformations, our approach directly maps preoperative CT scans to 2D\nintraoperative surgical frames through a lightweight neural network for\nreal-time cochlear implant surgery guidance via a zero-shot learning approach.\nUnlike traditional methods, our framework seamlessly integrates with monocular\nsurgical microscopes, making it highly practical for clinical use without\nadditional hardware dependencies and requirements. Our method estimates camera\nposes, which include a rotation matrix and a translation vector, by learning\nfrom the synthetic dataset, enabling accurate and efficient intraoperative\nregistration. The proposed framework was evaluated on nine clinical cases using\na patient-specific and cross-patient validation strategy. Our results suggest\nthat our approach achieves clinically relevant accuracy in predicting 6D camera\nposes for registering 3D preoperative CT scans to 2D surgical scenes with an\nangular error within 10 degrees in most cases, while also addressing\nlimitations of traditional methods, such as reliance on external tracking\nsystems or fiducial markers."}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240", "abs": "https://arxiv.org/abs/2505.18240", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations."}
{"id": "2505.18805", "pdf": "https://arxiv.org/pdf/2505.18805", "abs": "https://arxiv.org/abs/2505.18805", "authors": ["Zhongtian Zheng", "Tao Huang", "Haozhe Su", "Xueqi Ma", "Yuefan Shen", "Tongtong Wang", "Yin Yang", "Xifeng Gao", "Zherong Pan", "Kui Wu"], "title": "DiffHairCard: Auto Hair Card Extraction with Differentiable Rendering", "categories": ["cs.GR"], "comment": null, "summary": "Hair cards remain a widely used representation for hair modeling in real-time\napplications, offering a practical trade-off between visual fidelity, memory\nusage, and performance. However, generating high-quality hair card models\nremains a challenging and labor-intensive task. This work presents an automated\npipeline for converting strand-based hair models into hair card models with a\nlimited number of cards and textures while preserving the hairstyle appearance.\nOur key idea is a novel differentiable representation where each strand is\nencoded as a projected 2D spline in the texture space, which enables efficient\noptimization with differentiable rendering and structured results respecting\nthe hair geometry. Based on this representation, we develop a novel algorithm\npipeline, where we first cluster hair strands into initial hair cards and\nproject the strands into the texture space. We then conduct a two-stage\noptimization where our first stage optimizes the texture and geometry of each\nhair card separately, and after texture reduction, our second stage conducts\njoint optimization of all the cards for fine-tuning. Put together, our method\nis evaluated on a wide range of hairstyles, including straight, wavy, curly,\nand coily hairs. To better capture the appearance of short or coily hair, we\nadditionally support hair cap and cross-card. Furthermore, our framework\nsupports seamless LoD transitions via texture sharing, balancing texture memory\nefficiency and visual quality."}
{"id": "2505.18399", "pdf": "https://arxiv.org/pdf/2505.18399", "abs": "https://arxiv.org/abs/2505.18399", "authors": ["Lin Zhao", "Yushu Wu", "Xinru Jiang", "Jianyang Gu", "Yanzhi Wang", "Xiaolin Xu", "Pu Zhao", "Xue Lin"], "title": "Taming Diffusion for Dataset Distillation with High Representativeness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The paper is accepted by ICML 2025", "summary": "Recent deep learning models demand larger datasets, driving the need for\ndataset distillation to create compact, cost-efficient datasets while\nmaintaining performance. Due to the powerful image generation capability of\ndiffusion, it has been introduced to this field for generating distilled\nimages. In this paper, we systematically investigate issues present in current\ndiffusion-based dataset distillation methods, including inaccurate distribution\nmatching, distribution deviation with random noise, and separate sampling.\nBuilding on this, we propose D^3HR, a novel diffusion-based framework to\ngenerate distilled datasets with high representativeness. Specifically, we\nadopt DDIM inversion to map the latents of the full dataset from a\nlow-normality latent domain to a high-normality Gaussian domain, preserving\ninformation and ensuring structural consistency to generate representative\nlatents for the distilled dataset. Furthermore, we propose an efficient\nsampling scheme to better align the representative latents with the\nhigh-normality Gaussian distribution. Our comprehensive experiments demonstrate\nthat D^3HR can achieve higher accuracy across different model architectures\ncompared with state-of-the-art baselines in dataset distillation. Source code:\nhttps://github.com/lin-zhao-resoLve/D3HR."}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244", "abs": "https://arxiv.org/abs/2505.18244", "authors": ["Yukin Zhang", "Qi Dong"], "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities."}
{"id": "2505.19151", "pdf": "https://arxiv.org/pdf/2505.19151", "abs": "https://arxiv.org/abs/2505.19151", "authors": ["Shenggan Cheng", "Yuanxin Wei", "Lansong Diao", "Yong Liu", "Bujiao Chen", "Lianghua Huang", "Yu Liu", "Wenyuan Yu", "Jiangsu Du", "Wei Lin", "Yang You"], "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Leveraging the diffusion transformer (DiT) architecture, models like Sora,\nCogVideoX and Wan have achieved remarkable progress in text-to-video,\nimage-to-video, and video editing tasks. Despite these advances,\ndiffusion-based video generation remains computationally intensive, especially\nfor high-resolution, long-duration videos. Prior work accelerates its inference\nby skipping computation, usually at the cost of severe quality degradation. In\nthis paper, we propose SRDiffusion, a novel framework that leverages\ncollaboration between large and small models to reduce inference cost. The\nlarge model handles high-noise steps to ensure semantic and motion fidelity\n(Sketching), while the smaller model refines visual details in low-noise steps\n(Rendering). Experimental results demonstrate that our method outperforms\nexisting approaches, over 3$\\times$ speedup for Wan with nearly no quality loss\nfor VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a\nnew direction orthogonal to existing acceleration strategies, offering a\npractical solution for scalable video generation."}
{"id": "2505.18401", "pdf": "https://arxiv.org/pdf/2505.18401", "abs": "https://arxiv.org/abs/2505.18401", "authors": ["Jiangbei Yue", "He Wang"], "title": "Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review", "categories": ["cs.CV"], "comment": "51 pages, 7 figures, Book Chapter", "summary": "Crowd behaviour analysis is essential to numerous real-world applications,\nsuch as public safety and urban planning, and therefore has been studied for\ndecades. In the last decade or so, the development of deep learning has\nsignificantly propelled the research on crowd behaviours. This chapter reviews\nrecent advances in crowd behaviour analysis using deep learning. We mainly\nreview the research in two core tasks in this field, crowd behaviour prediction\nand recognition. We broadly cover how different deep neural networks, after\nfirst being proposed in machine learning, are applied to analysing crowd\nbehaviours. This includes pure deep neural network models as well as recent\ndevelopment of methodologies combining physics with deep learning. In addition,\nrepresentative studies are discussed and compared in detail. Finally, we\ndiscuss the effectiveness of existing methods and future research directions in\nthis rapidly evolving field. This chapter aims to provide a high-level summary\nof the ongoing deep learning research in crowd behaviour analysis. It intends\nto help new researchers who just entered this field to obtain an overall\nunderstanding of the ongoing research, as well as to provide a retrospective\nanalysis for existing researchers to identify possible future directions"}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc."}
{"id": "2505.19672", "pdf": "https://arxiv.org/pdf/2505.19672", "abs": "https://arxiv.org/abs/2505.19672", "authors": ["Belcour Laurent", "Fichet Alban", "Barla Pascal"], "title": "A Fluorescent Material Model for Non-Spectral Editing & Rendering", "categories": ["cs.GR", "I.3.7"], "comment": null, "summary": "Fluorescent materials are characterized by a spectral reradiation toward\nlonger wavelengths. Recent work [Fichet et al. 2024] has shown that the\nrendering of fluorescence in a non-spectral engine is possible through the use\nof appropriate reduced reradiation matrices. But the approach has limited\nexpressivity, as it requires the storage of one reduced matrix per fluorescent\nmaterial, and only works with measured fluorescent assets.\n  In this work, we introduce an analytical approach to the editing and\nrendering of fluorescence in a non-spectral engine. It is based on a\ndecomposition of the reduced reradiation matrix, and an analytically-integrable\nGaussian-based model of the fluorescent component. The model reproduces the\nappearance of fluorescent materials accurately, especially with the addition of\na UV basis. Most importantly, it grants variations of fluorescent material\nparameters in real-time, either for the editing of fluorescent materials, or\nfor the dynamic spatial variation of fluorescence properties across object\nsurfaces. A simplified one-Gaussian fluorescence model even allows for the\nartist-friendly creation of plausible fluorescent materials from scratch,\nrequiring only a few reflectance colors as input."}
{"id": "2505.18412", "pdf": "https://arxiv.org/pdf/2505.18412", "abs": "https://arxiv.org/abs/2505.18412", "authors": ["Jessica Tang", "Ali Abedi", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering", "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 5 tables", "summary": "Exercise-based rehabilitation improves quality of life and reduces morbidity,\nmortality, and rehospitalization, though transportation constraints and staff\nshortages lead to high dropout rates from rehabilitation programs. Virtual\nplatforms enable patients to complete prescribed exercises at home, while AI\nalgorithms analyze performance, deliver feedback, and update clinicians.\nAlthough many studies have developed machine learning and deep learning models\nfor exercise quality assessment, few have explored the use of large language\nmodels (LLMs) for feedback and are limited by the lack of rehabilitation\ndatasets containing textual feedback. In this paper, we propose a new method in\nwhich exercise-specific features are extracted from the skeletal joints of\npatients performing rehabilitation exercises and fed into pre-trained LLMs.\nUsing a range of prompting techniques, such as zero-shot, few-shot,\nchain-of-thought, and role-play prompting, LLMs are leveraged to evaluate\nexercise quality and provide feedback in natural language to help patients\nimprove their movements. The method was evaluated through extensive experiments\non two publicly available rehabilitation exercise assessment datasets (UI-PRMD\nand REHAB24-6) and showed promising results in exercise assessment, reasoning,\nand feedback generation. This approach can be integrated into virtual\nrehabilitation platforms to help patients perform exercises correctly, support\nrecovery, and improve health outcomes."}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283", "abs": "https://arxiv.org/abs/2505.18283", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS."}
{"id": "2505.19713", "pdf": "https://arxiv.org/pdf/2505.19713", "abs": "https://arxiv.org/abs/2505.19713", "authors": ["Yandong Guan", "Xilin Wang", "Xingxi Ming", "Jing Zhang", "Dong Xu", "Qian Yu"], "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward", "categories": ["cs.GR"], "comment": null, "summary": "In this work, we introduce CAD-Coder, a novel framework that reformulates\ntext-to-CAD as the generation of CadQuery scripts - a Python-based, parametric\nCAD language. This representation enables direct geometric validation, a richer\nmodeling vocabulary, and seamless integration with existing LLMs. To further\nenhance code validity and geometric fidelity, we propose a two-stage learning\npipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)\nreinforcement learning with Group Reward Policy Optimization (GRPO), guided by\na CAD-specific reward comprising both a geometric reward (Chamfer Distance) and\na format reward. We also introduce a chain-of-thought (CoT) planning process to\nimprove model reasoning, and construct a large-scale, high-quality dataset of\n110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated\npipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to\ngenerate diverse, valid, and complex CAD models directly from natural language,\nadvancing the state of the art of text-to-CAD generation and geometric\nreasoning."}
{"id": "2505.18416", "pdf": "https://arxiv.org/pdf/2505.18416", "abs": "https://arxiv.org/abs/2505.18416", "authors": ["Gelareh Hajian", "Ali Abedi", "Bing Ye", "Jennifer Campos", "Alex Mihailidis"], "title": "Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment", "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Driving is a key component of independence and quality of life for older\nadults. However, cognitive decline associated with conditions such as mild\ncognitive impairment and dementia can compromise driving safety and often lead\nto premature driving cessation. Conditionally automated vehicles, which require\ndrivers to take over control when automation reaches its operational limits,\noffer a potential assistive solution. However, their effectiveness depends on\nthe driver's ability to respond to takeover requests (TORs) in a timely and\nappropriate manner. Understanding emotional responses during TORs can provide\ninsight into drivers' engagement, stress levels, and readiness to resume\ncontrol, particularly in cognitively vulnerable populations. This study\ninvestigated affective responses, measured via facial expression analysis of\nvalence and arousal, during TORs among cognitively healthy older adults and\nthose with cognitive impairment. Facial affect data were analyzed across\ndifferent road geometries and speeds to evaluate within- and between-group\ndifferences in affective states. Within-group comparisons using the Wilcoxon\nsigned-rank test revealed significant changes in valence and arousal during\nTORs for both groups. Cognitively healthy individuals showed adaptive increases\nin arousal under higher-demand conditions, while those with cognitive\nimpairment exhibited reduced arousal and more positive valence in several\nscenarios. Between-group comparisons using the Mann-Whitney U test indicated\nthat cognitively impaired individuals displayed lower arousal and higher\nvalence than controls across different TOR conditions. These findings suggest\nreduced emotional response and awareness in cognitively impaired drivers,\nhighlighting the need for adaptive vehicle systems that detect affective states\nand support safe handovers for vulnerable users."}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298", "abs": "https://arxiv.org/abs/2505.18298", "authors": ["Jinyan Su", "Claire Cardie"], "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models."}
{"id": "2505.19976", "pdf": "https://arxiv.org/pdf/2505.19976", "abs": "https://arxiv.org/abs/2505.19976", "authors": ["Naoki Agata", "Takeo Igarashi"], "title": "MAMM: Motion Control via Metric-Aligning Motion Matching", "categories": ["cs.GR"], "comment": "12 pages, SIGGRAPH 2025 (Conference Track)", "summary": "We introduce a novel method for controlling a motion sequence using an\narbitrary temporal control sequence using temporal alignment. Temporal\nalignment of motion has gained significant attention owing to its applications\nin motion control and retargeting. Traditional methods rely on either learned\nor hand-craft cross-domain mappings between frames in the original and control\ndomains, which often require large, paired, or annotated datasets and\ntime-consuming training. Our approach, named Metric-Aligning Motion Matching,\nachieves alignment by solely considering within-domain distances. It computes\ndistances among patches in each domain and seeks a matching that optimally\naligns the two within-domain distances. This framework allows for the alignment\nof a motion sequence to various types of control sequences, including sketches,\nlabels, audio, and another motion sequence, all without the need for manually\ndefined mappings or training with annotated data. We demonstrate the\neffectiveness of our approach through applications in efficient motion control,\nshowcasing its potential in practical scenarios."}
{"id": "2505.18423", "pdf": "https://arxiv.org/pdf/2505.18423", "abs": "https://arxiv.org/abs/2505.18423", "authors": ["Afshin Bozorgpour", "Sina Ghorbani Kolahi", "Reza Azad", "Ilker Hacihaliloglu", "Dorit Merhof"], "title": "CENet: Context Enhancement Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Provisionally accepted at MICCAI-2025", "summary": "Medical image segmentation, particularly in multi-domain scenarios, requires\nprecise preservation of anatomical structures across diverse representations.\nWhile deep learning has advanced this field, existing models often struggle\nwith accurate boundary representation, variability in organ morphology, and\ninformation loss during downsampling, limiting their accuracy and robustness.\nTo address these challenges, we propose the Context Enhancement Network\n(CENet), a novel segmentation framework featuring two key innovations. First,\nthe Dual Selective Enhancement Block (DSEB) integrated into skip connections\nenhances boundary details and improves the detection of smaller organs in a\ncontext-aware manner. Second, the Context Feature Attention Module (CFAM) in\nthe decoder employs a multi-scale design to maintain spatial integrity, reduce\nfeature redundancy, and mitigate overly enhanced representations. Extensive\nevaluations on both radiology and dermoscopic datasets demonstrate that CENet\noutperforms state-of-the-art (SOTA) methods in multi-organ segmentation and\nboundary detail preservation, offering a robust and accurate solution for\ncomplex medical image analysis tasks. The code is publicly available at\nhttps://github.com/xmindflow/cenet."}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322", "abs": "https://arxiv.org/abs/2505.18322", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base."}
{"id": "2505.18168", "pdf": "https://arxiv.org/pdf/2505.18168", "abs": "https://arxiv.org/abs/2505.18168", "authors": ["Feifan Wang", "Tengfei Song", "Minggui He", "Chang Su", "Zhanglin Wu", "Hao Yang", "Wenming Zheng", "Osamu Yoshie"], "title": "Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Facial emotion perception in the vision large language model (VLLM) is\ncrucial for achieving natural human-machine interaction. However, creating\nhigh-quality annotations for both coarse- and fine-grained facial emotion\nanalysis demands costly expertise. The lack of such high-quality instruction\ndata limits the performance of VLLMs in facial emotion perception. To address\nthis, we propose a self-verification approach with emotion knowledge\nenhancement (SEKE), which generates high-quality instruction data for\nmulti-grained emotion analysis cost-effectively using closed-source VLLM. This\napproach integrates prior human knowledge to VLLM inference, guided by the\ninherent correlations between three grained levels of emotion descriptions,\ni.e., discrete expression, valence-arousal, and action unit, to reliably\ngenerate comprehensive annotations. A self-verification strategy with\nUncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to\nefficiently extract more accurate VLLM predictions, further improving\nannotation reliability. Consequently, we construct a facial emotion instruction\ndataset (FEID) containing three comprehensive descriptions, which provides\ncoarse- and fine-grained emotional information for effective model training.\nAdditionally, we introduce a facial emotion analysis benchmark (FEAB) to\nmeasure the VLLM's corresponding ability. Our method significantly outperforms\nstate-of-the-art methods on three downstream facial emotion analysis tasks."}
{"id": "2505.18434", "pdf": "https://arxiv.org/pdf/2505.18434", "abs": "https://arxiv.org/abs/2505.18434", "authors": ["Yuliang Cai", "Jesse Thomason", "Mohammad Rostami"], "title": "TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 3 figures", "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated strong\nperformance across a range of downstream tasks. However, CLIP is still limited\nin negation understanding: the ability to recognize the absence or exclusion of\na concept. Existing methods address the problem by using a large language model\n(LLM) to generate large-scale data of image captions containing negation for\nfurther fine-tuning CLIP. However, these methods are both time- and\ncompute-intensive, and their evaluations are typically restricted to image-text\nmatching tasks. To expand the horizon, we (1) introduce a training-time\nnegation data generation pipeline such that negation captions are generated\nduring the training stage, which only increases 2.5% extra training time, and\n(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image\ngeneration models on prompts containing negation, assessing model's ability to\nproduce semantically accurate images. We show that our proposed method,\nTNG-CLIP, achieves SOTA performance on diverse negation benchmarks of\nimage-to-text matching, text-to-image retrieval, and image generation."}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331", "abs": "https://arxiv.org/abs/2505.18331", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA"}
{"id": "2505.19086", "pdf": "https://arxiv.org/pdf/2505.19086", "abs": "https://arxiv.org/abs/2505.19086", "authors": ["Chen Tessler", "Yifeng Jiang", "Erwin Coumans", "Zhengyi Luo", "Gal Chechik", "Xue Bin Peng"], "title": "MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Humans interact with their world while leveraging precise full-body control\nto achieve versatile goals. This versatility allows them to solve long-horizon,\nunderspecified problems, such as placing a cup in a sink, by seamlessly\nsequencing actions like approaching the cup, grasping, transporting it, and\nfinally placing it in the sink. Such goal-driven control can enable new\nprocedural tools for animation systems, enabling users to define partial\nobjectives while the system naturally ``fills in'' the intermediate motions.\nHowever, while current methods for whole-body dexterous manipulation in\nphysics-based animation achieve success in specific interaction tasks, they\ntypically employ control paradigms (e.g., detailed kinematic motion tracking,\ncontinuous object trajectory following, or direct VR teleoperation) that offer\nlimited versatility for high-level goal specification across the entire coupled\nhuman-object system. To bridge this gap, we present MaskedManipulator, a\nunified and generative policy developed through a two-stage learning approach.\nFirst, our system trains a tracking controller to physically reconstruct\ncomplex human-object interactions from large-scale human mocap datasets. This\ntracking controller is then distilled into MaskedManipulator, which provides\nusers with intuitive control over both the character's body and the manipulated\nobject. As a result, MaskedManipulator enables users to specify complex\nloco-manipulation tasks through intuitive high-level objectives (e.g., target\nobject poses, key character stances), and MaskedManipulator then synthesizes\nthe necessary full-body actions for a physically simulated humanoid to achieve\nthese goals, paving the way for more interactive and life-like virtual\ncharacters."}
{"id": "2505.18445", "pdf": "https://arxiv.org/pdf/2505.18445", "abs": "https://arxiv.org/abs/2505.18445", "authors": ["Yiren Song", "Cheng Liu", "Mike Zheng Shou"], "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have advanced image stylization significantly, yet two core\nchallenges persist: (1) maintaining consistent stylization in complex scenes,\nparticularly identity, composition, and fine details, and (2) preventing style\ndegradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional\nstylization consistency highlights the performance gap between open-source\nmethods and proprietary models. To bridge this gap, we propose\n\\textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale\nDiffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context\nconsistency learning framework trained on aligned image pairs for robust\ngeneralization; (2) a two-stage progressive learning strategy decoupling style\nlearning from consistency preservation to mitigate style degradation; and (3) a\nfully plug-and-play design compatible with arbitrary style LoRAs under the Flux\nframework. Extensive experiments show that OmniConsistency significantly\nenhances visual coherence and aesthetic quality, achieving performance\ncomparable to commercial state-of-the-art model GPT-4o."}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343", "abs": "https://arxiv.org/abs/2505.18343", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "Model Editing with Graph-Based External Memory", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning."}
{"id": "2505.19306", "pdf": "https://arxiv.org/pdf/2505.19306", "abs": "https://arxiv.org/abs/2505.19306", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image."}
{"id": "2505.18446", "pdf": "https://arxiv.org/pdf/2505.18446", "abs": "https://arxiv.org/abs/2505.18446", "authors": ["Hojun Son", "Asma Almutairi", "Arpan Kusari"], "title": "Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Context bias refers to the association between the foreground objects and\nbackground during the object detection training process. Various methods have\nbeen proposed to minimize the context bias when applying the trained model to\nan unseen domain, known as domain adaptation for object detection (DAOD). But a\nprincipled approach to understand why the context bias occurs and how to remove\nit has been missing.\n  In this work, we provide a causal view of the context bias, pointing towards\nthe pooling operation in the convolution network architecture as the possible\nsource of this bias. We present an alternative, Mask Pooling, which uses an\nadditional input of foreground masks, to separate the pooling process in the\nrespective foreground and background regions and show that this process leads\nthe trained model to detect objects in a more robust manner under different\ndomains. We also provide a benchmark designed to create an ultimate test for\nDAOD, using foregrounds in the presence of absolute random backgrounds, to\nanalyze the robustness of the intended trained models. Through these\nexperiments, we hope to provide a principled approach for minimizing context\nbias under domain shift."}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356", "abs": "https://arxiv.org/abs/2505.18356", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start."}
{"id": "2505.20129", "pdf": "https://arxiv.org/pdf/2505.20129", "abs": "https://arxiv.org/abs/2505.20129", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications."}
{"id": "2505.18465", "pdf": "https://arxiv.org/pdf/2505.18465", "abs": "https://arxiv.org/abs/2505.18465", "authors": ["Ruize Yang", "Ann Kennedy", "R. James Cotton"], "title": "BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Advances in markerless motion capture are expanding access to biomechanical\nmovement analysis, making it feasible to obtain high-quality movement data from\noutpatient clinics, inpatient hospitals, therapy, and even home. Expanding\naccess to movement data in these diverse contexts makes the challenge of\nperforming downstream analytics all the more acute. Creating separate bespoke\nanalysis code for all the tasks end users might want is both intractable and\ndoes not take advantage of the common features of human movement underlying\nthem all. Recent studies have shown that fine-tuning language models to accept\ntokenized movement as an additional modality enables successful descriptive\ncaptioning of movement. Here, we explore whether such a multimodal\nmotion-language model can answer detailed, clinically meaningful questions\nabout movement. We collected over 30 hours of biomechanics from nearly 500\nparticipants, many with movement impairments from a variety of etiologies,\nperforming a range of movements used in clinical outcomes assessments. After\ntokenizing these movement trajectories, we created a multimodal dataset of\nmotion-related questions and answers spanning a range of tasks. We developed\nBiomechGPT, a multimodal biomechanics-language model, on this dataset. Our\nresults show that BiomechGPT demonstrates high performance across a range of\ntasks such as activity recognition, identifying movement impairments,\ndiagnosis, scoring clinical outcomes, and measuring walking. BiomechGPT\nprovides an important step towards a foundation model for rehabilitation\nmovement data."}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363", "abs": "https://arxiv.org/abs/2505.18363", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes."}
{"id": "2505.20271", "pdf": "https://arxiv.org/pdf/2505.20271", "abs": "https://arxiv.org/abs/2505.20271", "authors": ["Yu Xu", "Fan Tang", "You Wu", "Lin Gao", "Oliver Deussen", "Hongbin Yan", "Jintao Li", "Juan Cao", "Tong-Yee Lee"], "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection."}
{"id": "2505.18469", "pdf": "https://arxiv.org/pdf/2505.18469", "abs": "https://arxiv.org/abs/2505.18469", "authors": ["Jingkai Wang", "Wu Miao", "Jue Gong", "Zheng Chen", "Xing Liu", "Hong Gu", "Yutong Liu", "Yulun Zhang"], "title": "HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Face restoration has achieved remarkable advancements through the years of\ndevelopment. However, ensuring that restored facial images exhibit high\nfidelity, preserve authentic features, and avoid introducing artifacts or\nbiases remains a significant challenge. This highlights the need for models\nthat are more \"honest\" in their reconstruction from low-quality inputs,\naccurately reflecting original characteristics. In this work, we propose\nHonestFace, a novel approach designed to restore faces with a strong emphasis\non such honesty, particularly concerning identity consistency and texture\nrealism. To achieve this, HonestFace incorporates several key components.\nFirst, we propose an identity embedder to effectively capture and preserve\ncrucial identity features from both the low-quality input and multiple\nreference faces. Second, a masked face alignment method is presented to enhance\nfine-grained details and textural authenticity, thereby preventing the\ngeneration of patterned or overly synthetic textures and improving overall\nclarity. Furthermore, we present a new landmark-based evaluation metric. Based\non affine transformation principles, this metric improves the accuracy compared\nto conventional L2 distance calculations for facial feature alignment.\nLeveraging these contributions within a one-step diffusion model framework,\nHonestFace delivers exceptional restoration results in terms of facial fidelity\nand realism. Extensive experiments demonstrate that our approach surpasses\nexisting state-of-the-art methods, achieving superior performance in both\nvisual quality and quantitative assessments. The code and pre-trained models\nwill be made publicly available at https://github.com/jkwang28/HonestFace ."}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374", "abs": "https://arxiv.org/abs/2505.18374", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research."}
{"id": "2505.18477", "pdf": "https://arxiv.org/pdf/2505.18477", "abs": "https://arxiv.org/abs/2505.18477", "authors": ["Fukun Liu", "Adam T. Greer", "Gengchen Mai", "Jin Sun"], "title": "ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations", "categories": ["cs.CV"], "comment": null, "summary": "Plankton are small drifting organisms found throughout the world's oceans.\nOne component of this plankton community is the zooplankton, which includes\ngelatinous animals and crustaceans (e.g. shrimp), as well as the early life\nstages (i.e., eggs and larvae) of many commercially important fishes. Being\nable to monitor zooplankton abundances accurately and understand how\npopulations change in relation to ocean conditions is invaluable to marine\nscience research, with important implications for future marine seafood\nproductivity. While new imaging technologies generate massive amounts of video\ndata of zooplankton, analyzing them using general-purpose computer vision tools\ndeveloped for general objects turns out to be highly challenging due to the\nhigh similarity in appearance between the zooplankton and its background (e.g.,\nmarine snow). In this work, we present the ZooplanktonBench, a benchmark\ndataset containing images and videos of zooplankton associated with rich\ngeospatial metadata (e.g., geographic coordinates, depth, etc.) in various\nwater ecosystems. ZooplanktonBench defines a collection of tasks to detect,\nclassify, and track zooplankton in challenging settings, including highly\ncluttered environments, living vs non-living classification, objects with\nsimilar shapes, and relatively small objects. Our dataset presents unique\nchallenges and opportunities for state-of-the-art computer vision systems to\nevolve and improve visual understanding in a dynamic environment with huge\nvariations and be geo-aware."}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383", "abs": "https://arxiv.org/abs/2505.18383", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development."}
{"id": "2505.18479", "pdf": "https://arxiv.org/pdf/2505.18479", "abs": "https://arxiv.org/abs/2505.18479", "authors": ["Li-Syun Hsiung", "Jun-Kai Tu", "Kuan-Wu Chu", "Yu-Hsuan Chiu", "Yan-Tsung Peng", "Sheng-Luen Chung", "Gee-Sern Jison Hsu"], "title": "Syn3DTxt: Embedding 3D Cues for Scene Text Generation", "categories": ["cs.CV"], "comment": "CVPR workshop 2025: SyntaGen", "summary": "This study aims to investigate the challenge of insufficient\nthree-dimensional context in synthetic datasets for scene text rendering.\nAlthough recent advances in diffusion models and related techniques have\nimproved certain aspects of scene text generation, most existing approaches\ncontinue to rely on 2D data, sourcing authentic training examples from movie\nposters and book covers, which limits their ability to capture the complex\ninteractions among spatial layout and visual effects in real-world scenes. In\nparticular, traditional 2D datasets do not provide the necessary geometric cues\nfor accurately embedding text into diverse backgrounds. To address this\nlimitation, we propose a novel standard for constructing synthetic datasets\nthat incorporates surface normals to enrich three-dimensional scene\ncharacteristic. By adding surface normals to conventional 2D data, our approach\naims to enhance the representation of spatial relationships and provide a more\nrobust foundation for future scene text rendering methods. Extensive\nexperiments demonstrate that datasets built under this new standard offer\nimproved geometric context, facilitating further advancements in text rendering\nunder complex 3D-spatial conditions."}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405", "abs": "https://arxiv.org/abs/2505.18405", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance.Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data."}
{"id": "2505.18503", "pdf": "https://arxiv.org/pdf/2505.18503", "abs": "https://arxiv.org/abs/2505.18503", "authors": ["Aofei Chang", "Le Huang", "Alex James Boyd", "Parminder Bhatia", "Taha Kass-Hout", "Cao Xiao", "Fenglong Ma"], "title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "categories": ["cs.CV"], "comment": "Accepted to ACL2025 (main)", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs."}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411", "abs": "https://arxiv.org/abs/2505.18411", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench"}
{"id": "2505.18521", "pdf": "https://arxiv.org/pdf/2505.18521", "abs": "https://arxiv.org/abs/2505.18521", "authors": ["Yiheng Li", "Feng Liang", "Dan Kondratyuk", "Masayoshi Tomizuka", "Kurt Keutzer", "Chenfeng Xu"], "title": "Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility", "categories": ["cs.CV"], "comment": null, "summary": "The substantial training cost of diffusion models hinders their deployment.\nImmiscible Diffusion recently showed that reducing diffusion trajectory mixing\nin the noise space via linear assignment accelerates training by simplifying\ndenoising. To extend immiscible diffusion beyond the inefficient linear\nassignment under high batch sizes and high dimensions, we refine this concept\nto a broader miscibility reduction at any layer and by any implementation.\nSpecifically, we empirically demonstrate the bijective nature of the denoising\nprocess with respect to immiscible diffusion, ensuring its preservation of\ngenerative diversity. Moreover, we provide thorough analysis and show\nstep-by-step how immiscibility eases denoising and improves efficiency.\nExtending beyond linear assignment, we propose a family of implementations\nincluding K-nearest neighbor (KNN) noise selection and image scaling to reduce\nmiscibility, achieving up to >4x faster training across diverse models and\ntasks including unconditional/conditional generation, image editing, and\nrobotics planning. Furthermore, our analysis of immiscibility offers a novel\nperspective on how optimal transport (OT) enhances diffusion training. By\nidentifying trajectory miscibility as a fundamental bottleneck, we believe this\nwork establishes a potentially new direction for future research into\nhigh-efficiency diffusion training. The code is available at\nhttps://github.com/yhli123/Immiscible-Diffusion."}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426", "abs": "https://arxiv.org/abs/2505.18426", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies."}
{"id": "2505.18525", "pdf": "https://arxiv.org/pdf/2505.18525", "abs": "https://arxiv.org/abs/2505.18525", "authors": ["Haoyu Yang", "Yuxiang Cai", "Jintao Chen", "Xuhong Zhang", "Wenhui Lei", "Xiaoming Shi", "Jianwei Yin", "Yankai Jiang"], "title": "TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D medical image segmentation is vital for clinical diagnosis and treatment\nbut is challenged by high-dimensional data and complex spatial dependencies.\nTraditional single-modality networks, such as CNNs and Transformers, are often\nlimited by computational inefficiency and constrained contextual modeling in 3D\nsettings. We introduce a novel multimodal framework that leverages Mamba and\nKolmogorov-Arnold Networks (KAN) as an efficient backbone for long-sequence\nmodeling. Our approach features three key innovations: First, an EGSC (Enhanced\nGated Spatial Convolution) module captures spatial information when unfolding\n3D images into 1D sequences. Second, we extend Group-Rational KAN (GR-KAN), a\nKolmogorov-Arnold Networks variant with rational basis functions, into\n3D-Group-Rational KAN (3D-GR-KAN) for 3D medical imaging - its first\napplication in this domain - enabling superior feature representation tailored\nto volumetric data. Third, a dual-branch text-driven strategy leverages CLIP's\ntext embeddings: one branch swaps one-hot labels for semantic vectors to\npreserve inter-organ semantic relationships, while the other aligns images with\ndetailed organ descriptions to enhance semantic alignment. Experiments on the\nMedical Segmentation Decathlon (MSD) and KiTS23 datasets show our method\nachieving state-of-the-art performance, surpassing existing approaches in\naccuracy and efficiency. This work highlights the power of combining advanced\nsequence modeling, extended network architectures, and vision-language synergy\nto push forward 3D medical image segmentation, delivering a scalable solution\nfor clinical use. The source code is openly available at\nhttps://github.com/yhy-whu/TK-Mamba."}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436", "abs": "https://arxiv.org/abs/2505.18436", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies."}
{"id": "2505.18561", "pdf": "https://arxiv.org/pdf/2505.18561", "abs": "https://arxiv.org/abs/2505.18561", "authors": ["Shiu-hong Kao", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts", "categories": ["cs.CV"], "comment": "Project page: https://cse.hkust.edu.hk/~skao/thinkvideo.html", "summary": "Reasoning Video Object Segmentation is a challenging task, which generates a\nmask sequence from an input video and an implicit, complex text query. Existing\nworks probe into the problem by finetuning Multimodal Large Language Models\n(MLLM) for segmentation-based output, while still falling short in difficult\ncases on videos given temporally-sensitive queries, primarily due to the\nfailure to integrate temporal and spatial information. In this paper, we\npropose ThinkVideo, a novel framework which leverages the zero-shot\nChain-of-Thought (CoT) capability of MLLM to address these challenges.\nSpecifically, ThinkVideo utilizes the CoT prompts to extract object\nselectivities associated with particular keyframes, then bridging the reasoning\nimage segmentation model and SAM2 video processor to output mask sequences. The\nThinkVideo framework is training-free and compatible with closed-source MLLMs,\nwhich can be applied to Reasoning Video Instance Segmentation. We further\nextend the framework for online video streams, where the CoT is used to update\nthe object of interest when a better target starts to emerge and becomes\nvisible. We conduct extensive experiments on video object segmentation with\nexplicit and implicit queries. The results show that ThinkVideo significantly\noutperforms previous works in both cases, qualitatively and quantitatively."}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440", "abs": "https://arxiv.org/abs/2505.18440", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "title": "Efficient Long CoT Reasoning in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps."}
{"id": "2505.18582", "pdf": "https://arxiv.org/pdf/2505.18582", "abs": "https://arxiv.org/abs/2505.18582", "authors": ["Dongyang Jin", "Chao Fan", "Jingzhe Ma", "Jingkai Zhou", "Weihua Chen", "Shiqi Yu"], "title": "On Denoising Walking Videos for Gait Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "8pages, 4 figures", "summary": "To capture individual gait patterns, excluding identity-irrelevant cues in\nwalking videos, such as clothing texture and color, remains a persistent\nchallenge for vision-based gait recognition. Traditional silhouette- and\npose-based methods, though theoretically effective at removing such\ndistractions, often fall short of high accuracy due to their sparse and less\ninformative inputs. Emerging end-to-end methods address this by directly\ndenoising RGB videos using human priors. Building on this trend, we propose\nDenoisingGait, a novel gait denoising method. Inspired by the philosophy that\n\"what I cannot create, I do not understand\", we turn to generative diffusion\nmodels, uncovering how they partially filter out irrelevant factors for gait\nunderstanding. Additionally, we introduce a geometry-driven Feature Matching\nmodule, which, combined with background removal via human silhouettes,\ncondenses the multi-channel diffusion features at each foreground pixel into a\ntwo-channel direction vector. Specifically, the proposed within- and\ncross-frame matching respectively capture the local vectorized structures of\ngait appearance and motion, producing a novel flow-like gait representation\ntermed Gait Feature Field, which further reduces residual noise in diffusion\nfeatures. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate\nthat DenoisingGait achieves a new SoTA performance in most cases for both\nwithin- and cross-domain evaluations. Code is available at\nhttps://github.com/ShiqiYu/OpenGait."}
{"id": "2505.18450", "pdf": "https://arxiv.org/pdf/2505.18450", "abs": "https://arxiv.org/abs/2505.18450", "authors": ["Ainulla Khan", "Yamada Moyuru", "Srinidhi Akella"], "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments."}
{"id": "2505.18584", "pdf": "https://arxiv.org/pdf/2505.18584", "abs": "https://arxiv.org/abs/2505.18584", "authors": ["Chaofan Gan", "Yuanpeng Tu", "Xi Chen", "Tieyuan Chen", "Yuxi Li", "Mehrtash Harandi", "Weiyao Lin"], "title": "Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.)."}
{"id": "2505.18452", "pdf": "https://arxiv.org/pdf/2505.18452", "abs": "https://arxiv.org/abs/2505.18452", "authors": ["Heyuan Huang", "Alexandra DeLucia", "Vijay Murari Tiyyala", "Mark Dredze"], "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation."}
{"id": "2505.18586", "pdf": "https://arxiv.org/pdf/2505.18586", "abs": "https://arxiv.org/abs/2505.18586", "authors": ["Chengxi Min", "Wei Wang", "Yahui Liu", "Weixin Ye", "Enver Sangineto", "Qi Wang", "Yao Zhao"], "title": "Guiding the Experts: Semantic Priors for Efficient and Focused MoE Routing", "categories": ["cs.CV"], "comment": null, "summary": "Mixture-of-Experts (MoE) models have emerged as a promising direction for\nscaling vision architectures efficiently. Among them, Soft MoE improves\ntraining stability by assigning each token to all experts via continuous\ndispatch weights. However, current designs overlook the semantic structure\nwhich is implicitly encoded in these weights, resulting in suboptimal expert\nrouting. In this paper, we discover that dispatch weights in Soft MoE\ninherently exhibit segmentation-like patterns but are not explicitly aligned\nwith semantic regions. Motivated by this observation, we propose a\nforeground-guided enhancement strategy. Specifically, we introduce a spatially\naware auxiliary loss that encourages expert activation to align with semantic\nforeground regions. To further reinforce this supervision, we integrate a\nlightweight LayerScale mechanism that improves information flow and stabilizes\noptimization in skip connections. Our method necessitates only minor\narchitectural adjustments and can be seamlessly integrated into prevailing Soft\nMoE frameworks. Comprehensive experiments on ImageNet-1K and multiple\nsmaller-scale classification benchmarks not only showcase consistent\nperformance enhancements but also reveal more interpretable expert routing\nmechanisms."}
{"id": "2505.18454", "pdf": "https://arxiv.org/pdf/2505.18454", "abs": "https://arxiv.org/abs/2505.18454", "authors": ["Zhenrui Yue", "Bowen Jin", "Huimin Zeng", "Honglei Zhuang", "Zhen Qin", "Jinsung Yoon", "Lanyu Shang", "Jiawei Han", "Dong Wang"], "title": "Hybrid Latent Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning."}
{"id": "2505.18587", "pdf": "https://arxiv.org/pdf/2505.18587", "abs": "https://arxiv.org/abs/2505.18587", "authors": ["Pavan C Shekar", "Pawan Soni", "Vivek Kanhangad"], "title": "HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures, 1 table. Preliminary results on FaceForensics++\n  dataset. First approach to use hyperspectral reconstruction for deepfake\n  detection", "summary": "Deepfakes pose a significant threat to digital media security, with current\ndetection methods struggling to generalize across different manipulation\ntechniques and datasets. While recent approaches combine CNN-based\narchitectures with Vision Transformers or leverage multi-modal learning, they\nremain limited by the inherent constraints of RGB data. We introduce HyperFake,\na novel deepfake detection pipeline that reconstructs 31-channel hyperspectral\ndata from standard RGB videos, revealing hidden manipulation traces invisible\nto conventional methods. Using an improved MST++ architecture, HyperFake\nenhances hyperspectral reconstruction, while a spectral attention mechanism\nselects the most critical spectral features for deepfake detection. The refined\nspectral data is then processed by an EfficientNet-based classifier optimized\nfor spectral analysis, enabling more accurate and generalizable detection\nacross different deepfake styles and datasets, all without the need for\nexpensive hyperspectral cameras. To the best of our knowledge, this is the\nfirst approach to leverage hyperspectral imaging reconstruction for deepfake\ndetection, opening new possibilities for detecting increasingly sophisticated\nmanipulations."}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456", "abs": "https://arxiv.org/abs/2505.18456", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "Anchored Diffusion Language Model", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches"}
{"id": "2505.18594", "pdf": "https://arxiv.org/pdf/2505.18594", "abs": "https://arxiv.org/abs/2505.18594", "authors": ["GuangHao Meng", "Sunan He", "Jinpeng Wang", "Tao Dai", "Letian Zhang", "Jieming Zhu", "Qing Li", "Gang Wang", "Rui Zhang", "Yong Jiang"], "title": "EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models", "categories": ["cs.CV", "cs.IR"], "comment": "9 pages, 6 figures", "summary": "Vision-language retrieval (VLR) has attracted significant attention in both\nacademia and industry, which involves using text (or images) as queries to\nretrieve corresponding images (or text). However, existing methods often\nneglect the rich visual semantics knowledge of entities, thus leading to\nincorrect retrieval results. To address this problem, we propose the Entity\nVisual Description enhanced CLIP (EvdCLIP), designed to leverage the visual\nknowledge of entities to enrich queries. Specifically, since humans recognize\nentities through visual cues, we employ a large language model (LLM) to\ngenerate Entity Visual Descriptions (EVDs) as alignment cues to complement\ntextual data. These EVDs are then integrated into raw queries to create\nvisually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced\nqueries may introduce noise or low-quality expansions, we develop a novel,\ntrainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW\nutilizes EVD knowledge and the generative capabilities of the language model to\neffectively rewrite queries. With our specialized training strategy, EaRW can\ngenerate high-quality and low-noise EVD-enhanced queries. Extensive\nquantitative and qualitative experiments on image-text retrieval benchmarks\nvalidate the superiority of EvdCLIP on vision-language retrieval tasks."}
{"id": "2505.18466", "pdf": "https://arxiv.org/pdf/2505.18466", "abs": "https://arxiv.org/abs/2505.18466", "authors": ["Mamnuya Rinki", "Chahat Raj", "Anjishnu Mukherjee", "Ziwei Zhu"], "title": "Measuring South Asian Biases in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings."}
{"id": "2505.18600", "pdf": "https://arxiv.org/pdf/2505.18600", "abs": "https://arxiv.org/abs/2505.18600", "authors": ["Bryan Sangwoo Kim", "Jeongsol Kim", "Jong Chul Ye"], "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity."}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486", "abs": "https://arxiv.org/abs/2505.18486", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects."}
{"id": "2505.18605", "pdf": "https://arxiv.org/pdf/2505.18605", "abs": "https://arxiv.org/abs/2505.18605", "authors": ["Xiaohuan Pei", "Tao Huang", "YanXiang Ma", "Chang Xu"], "title": "Rethinking Causal Mask Attention for Vision-Language Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference."}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497", "abs": "https://arxiv.org/abs/2505.18497", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms."}
{"id": "2505.18608", "pdf": "https://arxiv.org/pdf/2505.18608", "abs": "https://arxiv.org/abs/2505.18608", "authors": ["Yuetong Fang", "Deming Zhou", "Ziqing Wang", "Hongwei Ren", "ZeCui Zeng", "Lusong Li", "Shibo Zhou", "Renjing Xu"], "title": "Spiking Transformers Need High Frequency Information", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Transformers offer an energy-efficient alternative to conventional\ndeep learning by transmitting information solely through binary (0/1) spikes.\nHowever, there remains a substantial performance gap compared to artificial\nneural networks. A common belief is that their binary and sparse activation\ntransmission leads to information loss, thus degrading feature representation\nand accuracy. In this work, however, we reveal for the first time that spiking\nneurons preferentially propagate low-frequency information. We hypothesize that\nthe rapid dissipation of high-frequency components is the primary cause of\nperformance degradation. For example, on Cifar-100, adopting Avg-Pooling\n(low-pass) for token mixing lowers performance to 76.73%; interestingly,\nreplacing it with Max-Pooling (high-pass) pushes the top-1 accuracy to 79.12%,\nsurpassing the well-tuned Spikformer baseline by 0.97%. Accordingly, we\nintroduce Max-Former that restores high-frequency signals through two\nfrequency-enhancing operators: extra Max-Pooling in patch embedding and\nDepth-Wise Convolution in place of self-attention. Notably, our Max-Former\n(63.99 M) hits the top-1 accuracy of 82.39% on ImageNet, showing a +7.58%\nimprovement over Spikformer with comparable model size (74.81%, 66.34 M). We\nhope this simple yet effective solution inspires future research to explore the\ndistinctive nature of spiking neural networks, beyond the established practice\nin standard deep learning."}
{"id": "2505.18522", "pdf": "https://arxiv.org/pdf/2505.18522", "abs": "https://arxiv.org/abs/2505.18522", "authors": ["Xin Lu", "Yanyan Zhao", "Si Wei", "Shijin Wang", "Bing Qin", "Ting Liu"], "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs."}
{"id": "2505.18612", "pdf": "https://arxiv.org/pdf/2505.18612", "abs": "https://arxiv.org/abs/2505.18612", "authors": ["Weizhi Zhong", "Huan Yang", "Zheng Liu", "Huiguo He", "Zijian He", "Xuesong Niu", "Di Zhang", "Guanbin Li"], "title": "Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter", "categories": ["cs.CV"], "comment": "Project page: https://weizhi-zhong.github.io/Mod-Adapter", "summary": "Personalized text-to-image generation aims to synthesize images of\nuser-provided concepts in diverse contexts. Despite recent progress in\nmulti-concept personalization, most are limited to object concepts and struggle\nto customize abstract concepts (e.g., pose, lighting). Some methods have begun\nexploring multi-concept personalization supporting abstract concepts, but they\nrequire test-time fine-tuning for each new concept, which is time-consuming and\nprone to overfitting on limited training images. In this work, we propose a\nnovel tuning-free method for multi-concept personalization that can effectively\ncustomize both object and abstract concepts without test-time fine-tuning. Our\nmethod builds upon the modulation mechanism in pretrained Diffusion\nTransformers (DiTs) model, leveraging the localized and semantically meaningful\nproperties of the modulation space. Specifically, we propose a novel module,\nMod-Adapter, to predict concept-specific modulation direction for the\nmodulation process of concept-related text tokens. It incorporates\nvision-language cross-attention for extracting concept visual features, and\nMixture-of-Experts (MoE) layers that adaptively map the concept features into\nthe modulation space. Furthermore, to mitigate the training difficulty caused\nby the large gap between the concept image space and the modulation space, we\nintroduce a VLM-guided pretraining strategy that leverages the strong image\nunderstanding capabilities of vision-language models to provide semantic\nsupervision signals. For a comprehensive comparison, we extend a standard\nbenchmark by incorporating abstract concepts. Our method achieves\nstate-of-the-art performance in multi-concept personalization, supported by\nquantitative, qualitative, and human evaluations."}
{"id": "2505.18524", "pdf": "https://arxiv.org/pdf/2505.18524", "abs": "https://arxiv.org/abs/2505.18524", "authors": ["Guowei Xu", "Mert Yuksekgonul", "Carlos Guestrin", "James Zou"], "title": "metaTextGrad: Automatically optimizing language model optimizers", "categories": ["cs.CL"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline."}
{"id": "2505.18634", "pdf": "https://arxiv.org/pdf/2505.18634", "abs": "https://arxiv.org/abs/2505.18634", "authors": ["NH Wanigasingha", "ES Sithpahan", "MKA Ariyaratne", "PRS De Silva"], "title": "SerendibCoins: Exploring The Sri Lankan Coins Dataset", "categories": ["cs.CV"], "comment": "20 pages", "summary": "The recognition and classification of coins are essential in numerous\nfinancial and automated systems. This study introduces a comprehensive Sri\nLankan coin image dataset and evaluates its impact on machine learning model\naccuracy for coin classification. We experiment with traditional machine\nlearning classifiers K-Nearest Neighbors (KNN), Support Vector Machines (SVM),\nand Random Forest as well as a custom Convolutional Neural Network (CNN) to\nbenchmark performance at different levels of classification. Our results show\nthat SVM outperforms KNN and Random Forest in traditional classification\napproaches, while the CNN model achieves near-perfect classification accuracy\nwith minimal misclassifications. The dataset demonstrates significant potential\nin enhancing automated coin recognition systems, offering a robust foundation\nfor future research in regional currency classification and deep learning\napplications."}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2505.18649", "pdf": "https://arxiv.org/pdf/2505.18649", "abs": "https://arxiv.org/abs/2505.18649", "authors": ["Shiyun Xie", "Zhiru Wang", "Yinghao Zhu", "Xu Wang", "Chengwei Pan", "Xiwang Dong"], "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis\n(NVS) with its real-time rendering capabilities and superior quality. However,\nit encounters challenges for high-resolution novel view synthesis (HRNVS) due\nto the coarse nature of primitives derived from low-resolution input views. To\naddress this issue, we propose SuperGS, an expansion of Scaffold-GS designed\nwith a two-stage coarse-to-fine training framework. In the low-resolution\nstage, we introduce a latent feature field to represent the low-resolution\nscene, which serves as both the initialization and foundational information for\nsuper-resolution optimization. In the high-resolution stage, we propose a\nmulti-view consistent densification strategy that backprojects high-resolution\ndepth maps based on error maps and employs a multi-view voting mechanism,\nmitigating ambiguities caused by multi-view inconsistencies in the pseudo\nlabels provided by 2D prior models while avoiding Gaussian redundancy.\nFurthermore, we model uncertainty through variational feature learning and use\nit to guide further scene representation refinement and adjust the supervisory\neffect of pseudo-labels, ensuring consistent and detailed scene reconstruction.\nExtensive experiments demonstrate that SuperGS outperforms state-of-the-art\nHRNVS methods on both forward-facing and 360-degree datasets."}
{"id": "2505.18542", "pdf": "https://arxiv.org/pdf/2505.18542", "abs": "https://arxiv.org/abs/2505.18542", "authors": ["Chen Yang", "Ruping Xu", "Ruizhe Li", "Bin Cao", "Jing Fan"], "title": "Business as \\textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, \\textbf{BPRF}, which contains 50 business process documents\nwith 326 explicitly labeled business rules across multiple domains. Each rule\nis represented as a <Condition, Action> pair, and we annotate logical\ndependencies between rules (sequential, conditional, or parallel). We also\npropose \\textbf{ExIde}, a framework for automatic business rule extraction and\ndependency relationship identification using large language models (LLMs). We\nevaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,\nbenchmarking performance on both rule extraction and dependency classification\ntasks of current LLMs. Our results demonstrate the effectiveness of ExIde in\nextracting structured business rules and analyzing their interdependencies for\ncurrent SOTA LLMs, paving the way for more automated and interpretable business\nprocess automation."}
{"id": "2505.18650", "pdf": "https://arxiv.org/pdf/2505.18650", "abs": "https://arxiv.org/abs/2505.18650", "authors": ["Xiaodong Wang", "Peixi Peng"], "title": "ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Real-world driving requires people to observe the current environment,\nanticipate the future, and make appropriate driving decisions. This requirement\nis aligned well with the capabilities of world models, which understand the\nenvironment and predict the future. However, recent world models in autonomous\ndriving are built explicitly, where they could predict the future by\ncontrollable driving video generation. We argue that driving world models\nshould have two additional abilities: action control and action prediction.\nFollowing this line, previous methods are limited because they predict the\nvideo requires given actions of the same length as the video and ignore the\ndynamical action laws. To address these issues, we propose ProphetDWM, a novel\nend-to-end driving world model that jointly predicts future videos and actions.\nOur world model has an action module to learn latent action from the present to\nthe future period by giving the action sequence and observations. And a\ndiffusion-model-based transition module to learn the state distribution. The\nmodel is jointly trained by learning latent actions given finite states and\npredicting action and video. The joint learning connects the action dynamics\nand states and enables long-term future prediction. We evaluate our method in\nvideo generation and action prediction tasks on the Nuscenes dataset. Compared\nto the state-of-the-art methods, our method achieves the best video consistency\nand best action prediction accuracy, while also enabling high-quality long-term\nvideo and action generation."}
{"id": "2505.18548", "pdf": "https://arxiv.org/pdf/2505.18548", "abs": "https://arxiv.org/abs/2505.18548", "authors": ["Sanwoo Lee", "Kun Liang", "Yunfang Wu"], "title": "Composable Cross-prompt Essay Scoring by Merging Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency."}
{"id": "2505.18652", "pdf": "https://arxiv.org/pdf/2505.18652", "abs": "https://arxiv.org/abs/2505.18652", "authors": ["Yicheng Lin", "Yunlong Jiang", "Xujia Jiao", "Bin Han"], "title": "Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU", "categories": ["cs.CV"], "comment": "8 pages, 6 gifures", "summary": "Robust long-term visual localization in complex industrial environments is\ncritical for mobile robotic systems. Existing approaches face limitations:\nhandcrafted features are illumination-sensitive, learned features are\ncomputationally intensive, and semantic- or marker-based methods are\nenvironmentally constrained. Handcrafted and learned features share similar\nrepresentations but differ functionally. Handcrafted features are optimized for\ncontinuous tracking, while learned features excel in wide-baseline matching.\nTheir complementarity calls for integration rather than replacement. Building\non this, we propose a hierarchical localization framework. It leverages\nreal-time handcrafted feature extraction for relative pose estimation. In\nparallel, it employs selective learned keypoint detection on optimized\nkeyframes for absolute positioning. This design enables CPU-efficient,\nlong-term visual localization. Experiments systematically progress through\nthree validation phases: Initially establishing feature complementarity through\ncomparative analysis, followed by computational latency profiling across\nalgorithm stages on CPU platforms. Final evaluation under photometric\nvariations (including seasonal transitions and diurnal cycles) demonstrates 47%\naverage error reduction with significantly improved localization consistency.\nThe code implementation is publicly available at\nhttps://github.com/linyicheng1/ORB_SLAM3_localization."}
{"id": "2505.18549", "pdf": "https://arxiv.org/pdf/2505.18549", "abs": "https://arxiv.org/abs/2505.18549", "authors": ["Baraa Hikal", "Mohamed Basem", "Islam Oshallah", "Ali Hamdi"], "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors", "categories": ["cs.CL"], "comment": null, "summary": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors."}
{"id": "2505.18660", "pdf": "https://arxiv.org/pdf/2505.18660", "abs": "https://arxiv.org/abs/2505.18660", "authors": ["Zhenglin Huang", "Tianxiao Li", "Xiangtai Li", "Haiquan Wen", "Yiwei He", "Jiangning Zhang", "Hao Fei", "Xi Yang", "Xiaowei Huang", "Bei Peng", "Guangliang Cheng"], "title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly."}
{"id": "2505.18555", "pdf": "https://arxiv.org/pdf/2505.18555", "abs": "https://arxiv.org/abs/2505.18555", "authors": ["Yiyang Feng", "Yichen Wang", "Shaobo Cui", "Boi Faltings", "Mina Lee", "Jiawei Zhou"], "title": "Unraveling Misinformation Propagation in LLM Reasoning", "categories": ["cs.CL"], "comment": "24 pages, 14 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation."}
{"id": "2505.18663", "pdf": "https://arxiv.org/pdf/2505.18663", "abs": "https://arxiv.org/abs/2505.18663", "authors": ["Zhiteng Li", "Hanxuan Li", "Junyi Wu", "Kai Liu", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "DVD-Quant: Data-free Video Diffusion Transformers Quantization", "categories": ["cs.CV"], "comment": "Code and models will be available at\n  \\url{https://github.com/lhxcs/DVD-Quant}", "summary": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art\narchitecture for video generation, yet their computational and memory demands\nhinder practical deployment. While post-training quantization (PTQ) presents a\npromising approach to accelerate Video DiT models, existing methods suffer from\ntwo critical limitations: (1) dependence on lengthy, computation-heavy\ncalibration procedures, and (2) considerable performance deterioration after\nquantization. To address these challenges, we propose DVD-Quant, a novel\nData-free quantization framework for Video DiTs. Our approach integrates three\nkey innovations: (1) Progressive Bounded Quantization (PBQ) and (2)\nAuto-scaling Rotated Quantization (ARQ) for calibration data-free quantization\nerror reduction, as well as (3) $\\delta$-Guided Bit Switching ($\\delta$-GBS)\nfor adaptive bit-width allocation. Extensive experiments across multiple video\ngeneration benchmarks demonstrate that DVD-Quant achieves an approximately\n2$\\times$ speedup over full-precision baselines on HunyuanVideo while\nmaintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ\nfor Video DiTs without compromising video quality. Code and models will be\navailable at https://github.com/lhxcs/DVD-Quant."}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556", "abs": "https://arxiv.org/abs/2505.18556", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668", "abs": "https://arxiv.org/abs/2505.18668", "authors": ["Zhen Li", "Yukai Guo", "Duan Li", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "categories": ["cs.CV", "cs.CL"], "comment": "63 pages, submitted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557", "abs": "https://arxiv.org/abs/2505.18557", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Wenjia Zhang", "Yun Chen", "Guanhua Chen"], "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks."}
{"id": "2505.18674", "pdf": "https://arxiv.org/pdf/2505.18674", "abs": "https://arxiv.org/abs/2505.18674", "authors": ["Peng Xiao", "Hongbo Zhao", "Yijun Wang", "Jianxin Lin"], "title": "Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Restoring real-world degraded images, such as old photographs or\nlow-resolution images, presents a significant challenge due to the complex,\nmixed degradations they exhibit, such as scratches, color fading, and noise.\nRecent data-driven approaches have struggled with two main challenges:\nachieving high-fidelity restoration and providing object-level control over\ncolorization. While diffusion models have shown promise in generating\nhigh-quality images with specific controls, they often fail to fully preserve\nimage details during restoration. In this work, we propose an internal\ndetail-preserving diffusion model for high-fidelity restoration of real-world\ndegraded images. Our method utilizes a pre-trained Stable Diffusion model as a\ngenerative prior, eliminating the need to train a model from scratch. Central\nto our approach is the Internal Image Detail Enhancement (IIDE) technique,\nwhich directs the diffusion model to preserve essential structural and textural\ninformation while mitigating degradation effects. The process starts by mapping\nthe input image into a latent space, where we inject the diffusion denoising\nprocess with degradation operations that simulate the effects of various\ndegradation factors. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models in both qualitative\nassessments and perceptual quantitative evaluations. Additionally, our approach\nsupports text-guided restoration, enabling object-level colorization control\nthat mimics the expertise of professional photo editing."}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562", "abs": "https://arxiv.org/abs/2505.18562", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies."}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675", "abs": "https://arxiv.org/abs/2505.18675", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581", "abs": "https://arxiv.org/abs/2505.18581", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG."}
{"id": "2505.18679", "pdf": "https://arxiv.org/pdf/2505.18679", "abs": "https://arxiv.org/abs/2505.18679", "authors": ["Bin Ren", "Yawei Li", "Xu Zheng", "Yuqian Fu", "Danda Pani Paudel", "Ming-Hsuan Yang", "Luc Van Gool", "Nicu Sebe"], "title": "Manifold-aware Representation Learning for Degradation-agnostic Image Restoration", "categories": ["cs.CV"], "comment": "ALl-in-One Image Restoration, low-level vision", "summary": "Image Restoration (IR) aims to recover high quality images from degraded\ninputs affected by various corruptions such as noise, blur, haze, rain, and low\nlight conditions. Despite recent advances, most existing approaches treat IR as\na direct mapping problem, relying on shared representations across degradation\ntypes without modeling their structural diversity. In this work, we present\nMIRAGE, a unified and lightweight framework for all in one IR that explicitly\ndecomposes the input feature space into three semantically aligned parallel\nbranches, each processed by a specialized module attention for global context,\nconvolution for local textures, and MLP for channel-wise statistics. This\nmodular decomposition significantly improves generalization and efficiency\nacross diverse degradations. Furthermore, we introduce a cross layer\ncontrastive learning scheme that aligns shallow and latent features to enhance\nthe discriminability of shared representations. To better capture the\nunderlying geometry of feature representations, we perform contrastive learning\nin a Symmetric Positive Definite (SPD) manifold space rather than the\nconventional Euclidean space. Extensive experiments show that MIRAGE not only\nachieves new state of the art performance across a variety of degradation types\nbut also offers a scalable solution for challenging all-in-one IR scenarios.\nOur code and models will be publicly available at\nhttps://amazingren.github.io/MIRAGE/."}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588", "abs": "https://arxiv.org/abs/2505.18588", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "title": "Safety Alignment via Constrained Knowledge Unlearning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing."}
{"id": "2505.18686", "pdf": "https://arxiv.org/pdf/2505.18686", "abs": "https://arxiv.org/abs/2505.18686", "authors": ["Yang Liu", "Silin Cheng", "Xinwei He", "Sebastien Ourselin", "Lei Tan", "Gen Luo"], "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Weakly supervised referring expression comprehension(WREC) and\nsegmentation(WRES) aim to learn object grounding based on a given expression\nusing weak supervision signals like image-text pairs. While these tasks have\ntraditionally been modeled separately, we argue that they can benefit from\njoint learning in a multi-task framework. To this end, we propose WeakMCN, a\nnovel multi-task collaborative network that effectively combines WREC and WRES\nwith a dual-branch architecture. Specifically, the WREC branch is formulated as\nanchor-based contrastive learning, which also acts as a teacher to supervise\nthe WRES branch. In WeakMCN, we propose two innovative designs to facilitate\nmulti-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and\nCollaborative Consistency Module(CCM). DVFE dynamically combines various\npre-trained visual knowledge to meet different task requirements, while CCM\npromotes cross-task consistency from the perspective of optimization. Extensive\nexperimental results on three popular REC and RES benchmarks, i.e., RefCOCO,\nRefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN\nover state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on\nRefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also\nvalidate the strong generalization ability of WeakMCN in both semi-supervised\nREC and RES settings against existing methods, e.g., +8.94% for semi-REC and\n+7.71% for semi-RES on 1% RefCOCO. The code is publicly available at\nhttps://github.com/MRUIL/WeakMCN."}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596", "abs": "https://arxiv.org/abs/2505.18596", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release."}
{"id": "2505.18699", "pdf": "https://arxiv.org/pdf/2505.18699", "abs": "https://arxiv.org/abs/2505.18699", "authors": ["Peixuan Zhang", "Shuchen Weng", "Chengxuan Zhu", "Binghao Tang", "Zijian Jia", "Si Li", "Boxin Shi"], "title": "Affective Image Editing: Shaping Emotional Factors via Text Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "In daily life, images as common affective stimuli have widespread\napplications. Despite significant progress in text-driven image editing, there\nis limited work focusing on understanding users' emotional requests. In this\npaper, we introduce AIEdiT for Affective Image Editing using Text descriptions,\nwhich evokes specific emotions by adaptively shaping multiple emotional factors\nacross the entire images. To represent universal emotional priors, we build the\ncontinuous emotional spectrum and extract nuanced emotional requests. To\nmanipulate emotional factors, we design the emotional mapper to translate\nvisually-abstract emotional requests to visually-concrete semantic\nrepresentations. To ensure that editing results evoke specific emotions, we\nintroduce an MLLM to supervise the model training. During inference, we\nstrategically distort visual elements and subsequently shape corresponding\nemotional factors to edit images according to users' instructions.\nAdditionally, we introduce a large-scale dataset that includes the\nemotion-aligned text and image pair set for training and evaluation. Extensive\nexperiments demonstrate that AIEdiT achieves superior performance, effectively\nreflecting users' emotional requests."}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601", "abs": "https://arxiv.org/abs/2505.18601", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "title": "Flex-Judge: Think Once, Judge Anywhere", "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge."}
{"id": "2505.18700", "pdf": "https://arxiv.org/pdf/2505.18700", "abs": "https://arxiv.org/abs/2505.18700", "authors": ["Chun Wang", "Xiaoran Pan", "Zihao Pan", "Haofan Wang", "Yiren Song"], "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE."}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609", "abs": "https://arxiv.org/abs/2505.18609", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages."}
{"id": "2505.18725", "pdf": "https://arxiv.org/pdf/2505.18725", "abs": "https://arxiv.org/abs/2505.18725", "authors": ["Mahmudul Hasan"], "title": "Deep Learning for Breast Cancer Detection: Comparative Analysis of ConvNeXT and EfficientNet", "categories": ["cs.CV"], "comment": null, "summary": "Breast cancer is the most commonly occurring cancer worldwide. This cancer\ncaused 670,000 deaths globally in 2022, as reported by the WHO. Yet since\nhealth officials began routine mammography screening in age groups deemed at\nrisk in the 1980s, breast cancer mortality has decreased by 40% in high-income\nnations. Every day, a greater and greater number of people are receiving a\nbreast cancer diagnosis. Reducing cancer-related deaths requires early\ndetection and treatment. This paper compares two convolutional neural networks\ncalled ConvNeXT and EfficientNet to predict the likelihood of cancer in\nmammograms from screening exams. Preprocessing of the images, classification,\nand performance evaluation are main parts of the whole procedure. Several\nevaluation metrics were used to compare and evaluate the performance of the\nmodels. The result shows that ConvNeXT generates better results with a 94.33%\nAUC score, 93.36% accuracy, and 95.13% F-score compared to EfficientNet with a\n92.34% AUC score, 91.47% accuracy, and 93.06% F-score on RSNA screening\nmammography breast cancer dataset."}
{"id": "2505.18610", "pdf": "https://arxiv.org/pdf/2505.18610", "abs": "https://arxiv.org/abs/2505.18610", "authors": ["Tengxuan Liu", "Shiyao Li", "Jiayi Yang", "Tianchen Zhao", "Feng Zhou", "Xiaohui Song", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ."}
{"id": "2505.18727", "pdf": "https://arxiv.org/pdf/2505.18727", "abs": "https://arxiv.org/abs/2505.18727", "authors": ["Xiaohe Li", "Pengfei Li", "Zide Fan", "Ying Geng", "Fangli Mou", "Haohua Wu", "Yunping Ge"], "title": "FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view multi-object tracking (MVMOT) has found widespread applications in\nintelligent transportation, surveillance systems, and urban management.\nHowever, existing studies rarely address genuinely free-viewpoint MVMOT\nsystems, which could significantly enhance the flexibility and scalability of\ncooperative tracking systems. To bridge this gap, we first construct the\nMulti-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone\nswarms across diverse real-world scenarios, initially establishing the first\nbenchmark for multi-object tracking in arbitrary multi-view environment.\nBuilding upon this foundation, we propose \\textbf{FusionTrack}, an end-to-end\nframework that reasonably integrates tracking and re-identification to leverage\nmulti-view information for robust trajectory association. Extensive experiments\non our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves\nstate-of-the-art performance in both single-view and multi-view tracking."}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614", "abs": "https://arxiv.org/abs/2505.18614", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation."}
{"id": "2505.18730", "pdf": "https://arxiv.org/pdf/2505.18730", "abs": "https://arxiv.org/abs/2505.18730", "authors": ["Wenchao Zhang", "Jiahe Tian", "Runze He", "Jizhong Han", "Jiao Dai", "Miaomiao Feng", "Wei Mi", "Xiaodan Zhang"], "title": "Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/smile365317/ABP", "summary": "Recent text-to-image (T2I) generation models have advanced significantly,\nenabling the creation of high-fidelity images from textual prompts. However,\nexisting evaluation benchmarks primarily focus on the explicit alignment\nbetween generated images and prompts, neglecting the alignment with real-world\nknowledge beyond prompts. To address this gap, we introduce Align Beyond\nPrompts (ABP), a comprehensive benchmark designed to measure the alignment of\ngenerated images with real-world knowledge that extends beyond the explicit\nuser prompts. ABP comprises over 2,000 meticulously crafted prompts, covering\nreal-world knowledge across six distinct scenarios. We further introduce\nABPScore, a metric that utilizes existing Multimodal Large Language Models\n(MLLMs) to assess the alignment between generated images and world knowledge\nbeyond prompts, which demonstrates strong correlations with human judgments.\nThrough a comprehensive evaluation of 8 popular T2I models using ABP, we find\nthat even state-of-the-art models, such as GPT-4o, face limitations in\nintegrating simple real-world knowledge into generated images. To mitigate this\nissue, we introduce a training-free strategy within ABP, named Inference-Time\nKnowledge Injection (ITKI). By applying this strategy to optimize 200\nchallenging samples, we achieved an improvement of approximately 43% in\nABPScore. The dataset and code are available in\nhttps://github.com/smile365317/ABP."}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630", "abs": "https://arxiv.org/abs/2505.18630", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task."}
{"id": "2505.18736", "pdf": "https://arxiv.org/pdf/2505.18736", "abs": "https://arxiv.org/abs/2505.18736", "authors": ["Junyong Kang", "Seohyun Lim", "Kyungjune Baek", "Hyunjung Shim"], "title": "Rethinking Direct Preference Optimization in Diffusion Models", "categories": ["cs.CV"], "comment": "21 pages, 12 figures, preprint", "summary": "Aligning text-to-image (T2I) diffusion models with human preferences has\nemerged as a critical research challenge. While recent advances in this area\nhave extended preference optimization techniques from large language models\n(LLMs) to the diffusion setting, they often struggle with limited exploration.\nIn this work, we propose a novel and orthogonal approach to enhancing\ndiffusion-based preference optimization. First, we introduce a stable reference\nmodel update strategy that relaxes the frozen reference model, encouraging\nexploration while maintaining a stable optimization anchor through reference\nmodel regularization. Second, we present a timestep-aware training strategy\nthat mitigates the reward scale imbalance problem across timesteps. Our method\ncan be integrated into various preference optimization algorithms. Experimental\nresults show that our approach improves the performance of state-of-the-art\nmethods on human preference evaluation benchmarks."}
{"id": "2505.18638", "pdf": "https://arxiv.org/pdf/2505.18638", "abs": "https://arxiv.org/abs/2505.18638", "authors": ["Md. Tanzib Hosain", "Rajan Das Gupta", "Md. Kishor Morol"], "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "categories": ["cs.CL"], "comment": "24 pages, 20 figures", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation."}
{"id": "2505.18741", "pdf": "https://arxiv.org/pdf/2505.18741", "abs": "https://arxiv.org/abs/2505.18741", "authors": ["Han Li", "Hu Han", "S. Kevin Zhou"], "title": "MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages,8 figures", "summary": "Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled\nimage classification and prevalence diversity (abundant vs. sparse) in\nlong-tailed image classification. Similarly, medical images in universal lesion\ndetection (ULD) exhibit substantial variations in image quality, encompassing\nattributes such as clarity and label correctness. How to effectively leverage\ntraining images with diverse qualities becomes a problem in learning deep\nmodels. Conventional training mechanisms, such as self-paced curriculum\nlearning (SCL) and online hard example mining (OHEM), relieve this problem by\nreweighting images with high loss values. Despite their success, these methods\nstill confront two challenges: (i) the loss-based measure of sample hardness is\nimprecise, preventing optimum handling of different cases, and (ii) there\nexists under-utilization in SCL or over-utilization OHEM with the identified\nhard samples. To address these issues, this paper revisits the minibatch\nsampling (MBS), a technique widely used in deep network training but largely\nunexplored concerning the handling of diverse-quality training samples. We\ndiscover that the samples within a minibatch influence each other during\ntraining; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)\nmethod to optimize the use of training samples with diverse qualities. MoMBS\nintroduces a measure that takes both loss and uncertainty into account to\nsurpass a sole reliance on loss and allows for a more refined categorization of\nhigh-loss samples by distinguishing them as either poorly labeled and under\nrepresented or well represented and overfitted. We prioritize under represented\nsamples as the main gradient contributors in a minibatch and keep them from the\nnegative influences of poorly labeled or overfitted samples with a mixed-order\nminibatch sampling design."}
{"id": "2505.18642", "pdf": "https://arxiv.org/pdf/2505.18642", "abs": "https://arxiv.org/abs/2505.18642", "authors": ["Xiao Chen", "Sihang Zhou", "Ke Liang", "Xiaoyu Sun", "Xinwang Liu"], "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks."}
{"id": "2505.18745", "pdf": "https://arxiv.org/pdf/2505.18745", "abs": "https://arxiv.org/abs/2505.18745", "authors": ["Umar Marikkar", "Syed Sameed Husain", "Muhammad Awais", "Sara Atito"], "title": "C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Immunohistochemical (IHC) images reveal detailed information about structures\nand functions at the subcellular level. However, unlike natural images, IHC\ndatasets pose challenges for deep learning models due to their inconsistencies\nin channel count and configuration, stemming from varying staining protocols\nacross laboratories and studies. Existing approaches build channel-adaptive\nmodels, which unfortunately fail to support out-of-distribution (OOD)\nevaluation across IHC datasets and cannot be applied in a true zero-shot\nsetting with mismatched channel counts. To address this, we introduce a\nstructured view of cellular image channels by grouping them into either context\nor concept, where we treat the context channels as a reference to the concept\nchannels in the image. We leverage this context-concept principle to develop\nChannel Conditioned Cell Representations (C3R), a framework designed for\nunified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold\nframework comprising a channel-adaptive encoder architecture and a masked\nknowledge distillation training strategy, both built around the context-concept\nprinciple. We find that C3R outperforms existing benchmarks on both ID and OOD\ntasks, while a trivial implementation of our core idea also outperforms the\nchannel-adaptive methods reported on the CHAMMI benchmark. Our method opens a\nnew pathway for cross-dataset generalization between IHC datasets, without\nrequiring dataset-specific adaptation or retraining."}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651", "abs": "https://arxiv.org/abs/2505.18651", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "title": "On the Emergence of Linear Analogies in Word Embeddings", "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al."}
{"id": "2505.18757", "pdf": "https://arxiv.org/pdf/2505.18757", "abs": "https://arxiv.org/abs/2505.18757", "authors": ["Duo Li", "Zuhao Yang", "Shijian Lu"], "title": "ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models", "categories": ["cs.CV"], "comment": "21 pages, 7 figures", "summary": "The representation of visual inputs of large vision-language models (LVLMs)\nusually involves substantially more tokens than that of textual inputs, leading\nto significant computational overhead. Several recent studies strive to\nmitigate this issue by either conducting token compression to prune redundant\nvisual tokens or guiding them to bypass certain computational stages. While\nmost existing work exploits token importance as the redundancy indicator, our\nstudy reveals that two largely neglected factors, namely, the diversity of\nretained visual tokens and their task relevance, often offer more robust\ncriteria in token pruning. To this end, we design ToDRE, a two-stage and\ntraining-free token compression framework that achieves superior performance by\npruning Tokens based on token Diversity and token-task RElevance. Instead of\npruning redundant tokens, ToDRE introduces a greedy k-center algorithm to\nselect and retain a small subset of diverse visual tokens after the vision\nencoder. Additionally, ToDRE addresses the \"information migration\" by further\neliminating task-irrelevant visual tokens within the decoder of large language\nmodel (LLM). Extensive experiments show that ToDRE effectively reduces 90% of\nvisual tokens after vision encoder and adaptively prunes all visual tokens\nwithin certain LLM's decoder layers, leading to a 2.6x speed-up in total\ninference time while maintaining 95.1% of model performance and excellent\ncompatibility with efficient attention operators."}
{"id": "2505.18653", "pdf": "https://arxiv.org/pdf/2505.18653", "abs": "https://arxiv.org/abs/2505.18653", "authors": ["Murathan Kurfal", "Shorouq Zahra", "Joakim Nivre", "Gabriele Messori"], "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change", "categories": ["cs.CL"], "comment": "Accepted to ClimateNLP 2025@ACL", "summary": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange."}
{"id": "2505.18766", "pdf": "https://arxiv.org/pdf/2505.18766", "abs": "https://arxiv.org/abs/2505.18766", "authors": ["Yanjie Li", "Wenxuan Zhang", "Xinqi Lyu", "Yihao Liu", "Bin Xiao"], "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations", "categories": ["cs.CV", "cs.AI"], "comment": "submitted to NIPS2025", "summary": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion."}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658", "abs": "https://arxiv.org/abs/2505.18658", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research."}
{"id": "2505.18770", "pdf": "https://arxiv.org/pdf/2505.18770", "abs": "https://arxiv.org/abs/2505.18770", "authors": ["Yuedi Zhang", "Shuanghao Bai", "Wanqi Zhou", "Zhirong Luan", "Badong Chen"], "title": "Dual-Path Stable Soft Prompt Generation for Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Domain generalization (DG) aims to learn a model using data from one or\nmultiple related but distinct source domains that can generalize well to unseen\nout-of-distribution target domains. Inspired by the success of large\npre-trained vision-language models (VLMs), prompt tuning has emerged as an\neffective generalization strategy. However, it often struggles to capture\ndomain-specific features due to its reliance on manually or fixed prompt\ninputs. Recently, some prompt generation methods have addressed this limitation\nby dynamically generating instance-specific and domain-specific prompts for\neach input, enriching domain information and demonstrating potential for\nenhanced generalization. Through further investigation, we identify a notable\nissue in existing prompt generation methods: the same input often yields\nsignificantly different and suboptimal prompts across different random seeds, a\nphenomenon we term Prompt Variability. To address this, we introduce negative\nlearning into the prompt generation process and propose Dual-Path Stable Soft\nPrompt Generation (DPSPG), a transformer-based framework designed to improve\nboth the stability and generalization of prompts. Specifically, DPSPG\nincorporates a complementary prompt generator to produce negative prompts,\nthereby reducing the risk of introducing misleading information. Both\ntheoretical and empirical analyses demonstrate that negative learning leads to\nmore robust and effective prompts by increasing the effective margin and\nreducing the upper bound of the gradient norm. Extensive experiments on five DG\nbenchmark datasets show that DPSPG consistently outperforms state-of-the-art\nmethods while maintaining prompt stability."}
{"id": "2505.18673", "pdf": "https://arxiv.org/pdf/2505.18673", "abs": "https://arxiv.org/abs/2505.18673", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Xiuying Chen", "Jieyu Zhao", "Meng Jiang", "Xiangliang Zhang"], "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls."}
{"id": "2505.18775", "pdf": "https://arxiv.org/pdf/2505.18775", "abs": "https://arxiv.org/abs/2505.18775", "authors": ["Jiayu Wang", "Yang Jiao", "Yue Yu", "Tianwen Qian", "Shaoxiang Chen", "Jingjing Chen", "Yu-Gang Jiang"], "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs), such as the\nimpressive GPT-4o-Native, have demonstrated remarkable proficiency in following\ngeneral-purpose instructions for image generation. However, current benchmarks\noften lack the necessary breadth and depth to fully evaluate the diverse\ncapabilities of these models. To overcome this limitation, we introduce\nOmniGenBench, a novel and comprehensive benchmark meticulously designed to\nassess the instruction-following abilities of state-of-the-art LMMs across both\nperception-centric and cognition-centric dimensions. Our OmniGenBench includes\n57 diverse sub-tasks grounded in real-world scenarios, systematically\ncategorized according to the specific model capabilities they demand. For\nrigorous evaluation, we further employ a dual-mode protocol. This protocol\nutilizes off-the-shelf visual parsing tools for perception-centric tasks and a\npowerful LLM-based judger for cognition-centric tasks to assess the alignment\nbetween generated images and user instructions. Using OmniGenBench, we evaluate\nmainstream generative models, including prevalent models like GPT-4o,\nGemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses\nof their performance.Code and data are available at\nhttps://github.com/emilia113/OmniGenBench."}
{"id": "2505.18677", "pdf": "https://arxiv.org/pdf/2505.18677", "abs": "https://arxiv.org/abs/2505.18677", "authors": ["Eric Chamoun", "Nedjma Ousidhoum", "Michael Schlichtkrull", "Andreas Vlachos"], "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts", "categories": ["cs.CL"], "comment": null, "summary": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation."}
{"id": "2505.18787", "pdf": "https://arxiv.org/pdf/2505.18787", "abs": "https://arxiv.org/abs/2505.18787", "authors": ["Hong-Hanh Nguyen-Le", "Van-Tuan Tran", "Dinh-Thuc Nguyen", "Nhien-An Le-Khac"], "title": "Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted at 34th International Joint Conference on Artificial\n  Intelligence (IJCAI-25)", "summary": "Deepfake (DF) detectors face significant challenges when deployed in\nreal-world environments, particularly when encountering test samples deviated\nfrom training data through either postprocessing manipulations or distribution\nshifts. We demonstrate postprocessing techniques can completely obscure\ngeneration artifacts presented in DF samples, leading to performance\ndegradation of DF detectors. To address these challenges, we propose Think\nTwice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation\nmethod that enhances the adaptability of detectors during inference without\nrequiring access to source training data or labels. Our key idea is to enable\nthe model to explore alternative options through an Uncertainty-aware Negative\nLearning objective rather than solely relying on its initial predictions as\ncommonly seen in entropy minimization (EM)-based approaches. We also introduce\nan Uncertain Sample Prioritization strategy and Gradients Masking technique to\nimprove the adaptation by focusing on important samples and model parameters.\nOur theoretical analysis demonstrates that the proposed negative learning\nobjective exhibits complementary behavior to EM, facilitating better adaptation\ncapability. Empirically, our method achieves state-of-the-art results compared\nto existing test-time adaptation (TTA) approaches and significantly enhances\nthe resilience and generalization of DF detectors during inference. Code is\navailable\n\\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}."}
{"id": "2505.18683", "pdf": "https://arxiv.org/pdf/2505.18683", "abs": "https://arxiv.org/abs/2505.18683", "authors": ["Raphal Merx", "Hanna Suominen", "Lois Hong", "Nick Thieberger", "Trevor Cohn", "Ekaterina Vylomova"], "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B."}
{"id": "2505.18809", "pdf": "https://arxiv.org/pdf/2505.18809", "abs": "https://arxiv.org/abs/2505.18809", "authors": ["Wenhao Sun", "Rong-Cheng Tu", "Yifu Ding", "Zhao Jin", "Jingyi Liao", "Shunyu Liu", "Dacheng Tao"], "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention", "categories": ["cs.CV"], "comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA", "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."}
{"id": "2505.18685", "pdf": "https://arxiv.org/pdf/2505.18685", "abs": "https://arxiv.org/abs/2505.18685", "authors": ["Zhihao Zhang", "Yiran Zhang", "Xiyue Zhou", "Liting Huang", "Imran Razzak", "Preslav Nakov", "Usman Naseem"], "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels."}
{"id": "2505.18812", "pdf": "https://arxiv.org/pdf/2505.18812", "abs": "https://arxiv.org/abs/2505.18812", "authors": ["Ye Sun", "Hao Zhang", "Henghui Ding", "Tiehua Zhang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Achieving fine-grained spatio-temporal understanding in videos remains a\nmajor challenge for current Video Large Multimodal Models (Video LMMs).\nAddressing this challenge requires mastering two core capabilities: video\nreferring understanding, which captures the semantics of video regions, and\nvideo grounding, which segments object regions based on natural language\ndescriptions. However, most existing approaches tackle these tasks in\nisolation, limiting progress toward unified, referentially grounded video\ninteraction. We identify a key bottleneck in the lack of high-quality, unified\nvideo instruction data and a comprehensive benchmark for evaluating\nreferentially grounded video chat. To address these challenges, we contribute\nin three core aspects: dataset, model, and benchmark. First, we introduce\nSAMA-239K, a large-scale dataset comprising 15K videos specifically curated to\nenable joint learning of video referring understanding, grounding, and\nmulti-turn video chat. Second, we propose the SAMA model, which incorporates a\nversatile spatio-temporal context aggregator and a Segment Anything Model to\njointly enhance fine-grained video comprehension and precise grounding\ncapabilities. Finally, we establish SAMA-Bench, a meticulously designed\nbenchmark consisting of 5,067 questions from 522 videos, to comprehensively\nevaluate the integrated capabilities of Video LMMs in multi-turn,\nspatio-temporal referring understanding and grounded dialogue. Extensive\nexperiments and benchmarking results show that SAMA not only achieves strong\nperformance on SAMA-Bench but also sets a new state-of-the-art on general\ngrounding benchmarks, while maintaining highly competitive performance on\nstandard visual understanding benchmarks."}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688", "abs": "https://arxiv.org/abs/2505.18688", "authors": ["Aleksandr Tsymbalov"], "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning."}
{"id": "2505.18816", "pdf": "https://arxiv.org/pdf/2505.18816", "abs": "https://arxiv.org/abs/2505.18816", "authors": ["Yiqing Shen", "Chenjia Li", "Fei Xiong", "Jeong-O Jeong", "Tianpeng Wang", "Michael Latman", "Mathias Unberath"], "title": "Reasoning Segmentation for Images and Videos: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning Segmentation (RS) aims to delineate objects based on implicit text\nqueries, the interpretation of which requires reasoning and knowledge\nintegration. Unlike the traditional formulation of segmentation problems that\nrelies on fixed semantic categories or explicit prompting, RS bridges the gap\nbetween visual perception and human-like reasoning capabilities, facilitating\nmore intuitive human-AI interaction through natural language. Our work presents\nthe first comprehensive survey of RS for image and video processing, examining\n26 state-of-the-art methods together with a review of the corresponding\nevaluation metrics, as well as 29 datasets and benchmarks. We also explore\nexisting applications of RS across diverse domains and identify their potential\nextensions. Finally, we identify current research gaps and highlight promising\nfuture directions."}
{"id": "2505.18690", "pdf": "https://arxiv.org/pdf/2505.18690", "abs": "https://arxiv.org/abs/2505.18690", "authors": ["Guoxiu He", "Xin Song", "Futing Wang", "Aixin Sun"], "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.05212", "summary": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative."}
{"id": "2505.18819", "pdf": "https://arxiv.org/pdf/2505.18819", "abs": "https://arxiv.org/abs/2505.18819", "authors": ["Guofeng Mei", "Bin Ren", "Juan Liu", "Luigi Riz", "Xiaoshui Huang", "Xu Zheng", "Yongshun Gong", "Ming-Hsuan Yang", "Nicu Sebe", "Fabio Poiesi"], "title": "Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding", "categories": ["cs.CV"], "comment": "10 pages, tokenizer", "summary": "Vision-language models like CLIP can offer a promising foundation for 3D\nscene understanding when extended with 3D tokenizers. However, standard\napproaches, such as k-nearest neighbor or radius-based tokenization, struggle\nwith cross-domain generalization due to sensitivity to dataset-specific spatial\nscales. We present a universal 3D tokenizer designed for scale-invariant\nrepresentation learning with a frozen CLIP backbone. We show that combining\nsuperpoint-based grouping with coordinate scale normalization consistently\noutperforms conventional methods through extensive experimental analysis.\nSpecifically, we introduce S4Token, a tokenization pipeline that produces\nsemantically-informed tokens regardless of scene scale. Our tokenizer is\ntrained without annotations using masked point modeling and clustering-based\nobjectives, along with cross-modal distillation to align 3D tokens with 2D\nmulti-view image features. For dense prediction tasks, we propose a\nsuperpoint-level feature propagation module to recover point-level detail from\nsparse tokens."}
{"id": "2505.18703", "pdf": "https://arxiv.org/pdf/2505.18703", "abs": "https://arxiv.org/abs/2505.18703", "authors": ["Gaurav Negi", "Dhairya Dalal", "Omnia Zayed", "Paul Buitelaar"], "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models."}
{"id": "2505.18823", "pdf": "https://arxiv.org/pdf/2505.18823", "abs": "https://arxiv.org/abs/2505.18823", "authors": ["Libin Lan", "Yanxin Li", "Xiaojuan Liu", "Juan Zhou", "Jianxun Zhang", "Nannan Huang", "Yudong Zhang"], "title": "MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "13 pages, 7 figures, 7 tables", "summary": "Both CNN-based and Transformer-based methods have achieved remarkable success\nin medical image segmentation tasks. However, CNN-based methods struggle to\neffectively capture global contextual information due to the inherent\nlimitations of convolution operations. Meanwhile, Transformer-based methods\nsuffer from insufficient local feature modeling and face challenges related to\nthe high computational complexity caused by the self-attention mechanism. To\naddress these limitations, we propose a novel hybrid CNN-Transformer\narchitecture, named MSLAU-Net, which integrates the strengths of both\nparadigms. The proposed MSLAU-Net incorporates two key ideas. First, it\nintroduces Multi-Scale Linear Attention, designed to efficiently extract\nmulti-scale features from medical images while modeling long-range dependencies\nwith low computational complexity. Second, it adopts a top-down feature\naggregation mechanism, which performs multi-level feature aggregation and\nrestores spatial resolution using a lightweight structure. Extensive\nexperiments conducted on benchmark datasets covering three imaging modalities\ndemonstrate that the proposed MSLAU-Net outperforms other state-of-the-art\nmethods on nearly all evaluation metrics, validating the superiority,\neffectiveness, and robustness of our approach. Our code is available at\nhttps://github.com/Monsoon49/MSLAU-Net."}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708", "abs": "https://arxiv.org/abs/2505.18708", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "title": "A General Knowledge Injection Framework for ICD Coding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD."}
{"id": "2505.18832", "pdf": "https://arxiv.org/pdf/2505.18832", "abs": "https://arxiv.org/abs/2505.18832", "authors": ["Arman Zarei", "Samyadeep Basu", "Keivan Rezaei", "Zihao Lin", "Sayan Nag", "Soheil Feizi"], "title": "Localizing Knowledge in Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Understanding how knowledge is distributed across the layers of generative\nmodels is crucial for improving interpretability, controllability, and\nadaptation. While prior work has explored knowledge localization in UNet-based\narchitectures, Diffusion Transformer (DiT)-based models remain underexplored in\nthis context. In this paper, we propose a model- and knowledge-agnostic method\nto localize where specific types of knowledge are encoded within the DiT\nblocks. We evaluate our method on state-of-the-art DiT-based models, including\nPixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show\nthat the identified blocks are both interpretable and causally linked to the\nexpression of knowledge in generated outputs. Building on these insights, we\napply our localization framework to two key applications: model personalization\nand knowledge unlearning. In both settings, our localized fine-tuning approach\nenables efficient and targeted updates, reducing computational cost, improving\ntask-specific performance, and better preserving general model behavior with\nminimal interference to unrelated or surrounding content. Overall, our findings\noffer new insights into the internal structure of DiTs and introduce a\npractical pathway for more interpretable, efficient, and controllable model\nediting."}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709", "abs": "https://arxiv.org/abs/2505.18709", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations."}
{"id": "2505.18855", "pdf": "https://arxiv.org/pdf/2505.18855", "abs": "https://arxiv.org/abs/2505.18855", "authors": ["Peiqi Wang", "ShengYun Peng", "Xuewen Zhang", "Hanchao Yu", "Yibo Yang", "Lifu Huang", "Fujun Liu", "Qifan Wang"], "title": "Inference Compute-Optimal Video Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025", "summary": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors."}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720", "abs": "https://arxiv.org/abs/2505.18720", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}."}
{"id": "2505.18869", "pdf": "https://arxiv.org/pdf/2505.18869", "abs": "https://arxiv.org/abs/2505.18869", "authors": ["Ankan Dash", "Jingyi Gu", "Guiling Wang", "Chen Chen"], "title": "Eye-See-You: Reverse Pass-Through VR and Head Avatars", "categories": ["cs.CV"], "comment": "34th International Joint Conference on Artificial Intelligence, IJCAI\n  2025", "summary": "Virtual Reality (VR) headsets, while integral to the evolving digital\necosystem, present a critical challenge: the occlusion of users' eyes and\nportions of their faces, which hinders visual communication and may contribute\nto social isolation. To address this, we introduce RevAvatar, an innovative\nframework that leverages AI methodologies to enable reverse pass-through\ntechnology, fundamentally transforming VR headset design and interaction\nparadigms. RevAvatar integrates state-of-the-art generative models and\nmultimodal AI techniques to reconstruct high-fidelity 2D facial images and\ngenerate accurate 3D head avatars from partially observed eye and lower-face\nregions. This framework represents a significant advancement in AI4Tech by\nenabling seamless interaction between virtual and physical environments,\nfostering immersive experiences such as VR meetings and social engagements.\nAdditionally, we present VR-Face, a novel dataset comprising 200,000 samples\ndesigned to emulate diverse VR-specific conditions, including occlusions,\nlighting variations, and distortions. By addressing fundamental limitations in\ncurrent VR systems, RevAvatar exemplifies the transformative synergy between AI\nand next-generation technologies, offering a robust platform for enhancing\nhuman connection and interaction in virtual environments."}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744", "abs": "https://arxiv.org/abs/2505.18744", "authors": ["Tao Liu", "Hongying Zan", "Yifan Li", "Dixuan Zhang", "Lulu Kong", "Haixin Liu", "Jiaming Hou", "Aoze Zheng", "Rui Li", "Yiming Qiao", "Zewei Luo", "Qi Wang", "Zhiqiang Zhang", "Jiaxi Li", "Supeng Liu", "Kunli Zhang", "Min Peng"], "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges", "categories": ["cs.CL"], "comment": "22 pages, 10 figures", "summary": "Text-to-SQL is a fundamental task in natural language processing that seeks\nto translate natural language questions into meaningful and executable SQL\nqueries. While existing datasets are extensive and primarily focus on business\nscenarios and operational logic, they frequently lack coverage of\ndomain-specific knowledge and complex mathematical reasoning. To address this\ngap, we present a novel dataset tailored for complex reasoning and\nchain-of-thought analysis in SQL inference, encompassing physical, arithmetic,\ncommonsense, and hypothetical reasoning. The dataset consists of 4,038 English\nquestions, each paired with a unique SQL query and accompanied by 12,114\nstep-by-step reasoning annotations, spanning 45 databases across diverse\ndomains. Experimental results demonstrate that LogicCat substantially increases\nthe difficulty for state-of-the-art models, with the highest execution accuracy\nreaching only 14.96%. Incorporating our chain-of-thought annotations boosts\nperformance to 33.96%. Benchmarking leading public methods on Spider and BIRD\nfurther underscores the unique challenges presented by LogicCat, highlighting\nthe significant opportunities for advancing research in robust,\nreasoning-driven text-to-SQL systems. We have released our dataset code at\nhttps://github.com/Ffunkytao/LogicCat."}
{"id": "2505.18875", "pdf": "https://arxiv.org/pdf/2505.18875", "abs": "https://arxiv.org/abs/2505.18875", "authors": ["Shuo Yang", "Haocheng Xi", "Yilong Zhao", "Muyang Li", "Jintao Zhang", "Han Cai", "Yujun Lin", "Xiuyu Li", "Chenfeng Xu", "Kelly Peng", "Jianfei Chen", "Song Han", "Kurt Keutzer", "Ion Stoica"], "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) are essential for video generation but suffer\nfrom significant latency due to the quadratic complexity of attention. By\ncomputing only critical tokens, sparse attention reduces computational costs\nand offers a promising acceleration approach. However, we identify that\nexisting methods fail to approach optimal generation quality under the same\ncomputation budget for two reasons: (1) Inaccurate critical token\nidentification: current methods cluster tokens based on position rather than\nsemantics, leading to imprecise aggregated representations. (2) Excessive\ncomputation waste: critical tokens are scattered among non-critical ones,\nleading to wasted computation on GPUs, which are optimized for processing\ncontiguous tokens. In this paper, we propose SVG2, a training-free framework\nthat maximizes identification accuracy and minimizes computation waste,\nachieving a Pareto frontier trade-off between generation quality and\nefficiency. The core of SVG2 is semantic-aware permutation, which clusters and\nreorders tokens based on semantic similarity using k-means. This approach\nensures both a precise cluster representation, improving identification\naccuracy, and a densified layout of critical tokens, enabling efficient\ncomputation without padding. Additionally, SVG2 integrates top-p dynamic budget\ncontrol and customized kernel implementations, achieving up to 2.30x and 1.89x\nspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan\n2.1, respectively."}
{"id": "2505.18752", "pdf": "https://arxiv.org/pdf/2505.18752", "abs": "https://arxiv.org/abs/2505.18752", "authors": ["Haolin Yang", "Hakaze Cho", "Yiqiao Zhong", "Naoya Inoue"], "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning", "categories": ["cs.CL"], "comment": "45 pages, 49 figures", "summary": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms."}
{"id": "2505.18880", "pdf": "https://arxiv.org/pdf/2505.18880", "abs": "https://arxiv.org/abs/2505.18880", "authors": ["Weihan Xu", "Yimeng Ma", "Jingyue Huang", "Yang Li", "Wenye Ma", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Paul Pu Liang", "Hao-Wen Dong"], "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Short videos are an effective tool for promoting contents and improving\nknowledge accessibility. While existing extractive video summarization methods\nstruggle to produce a coherent narrative, existing abstractive methods cannot\n`quote' from the input videos, i.e., inserting short video clips in their\noutputs. In this work, we explore novel video editing models for generating\nshorts that feature a coherent narrative with embedded video insertions\nextracted from a long input video. We propose a novel retrieval-embedded\ngeneration framework that allows a large language model to quote multimodal\nresources while maintaining a coherent narrative. Our proposed REGen system\nfirst generates the output story script with quote placeholders using a\nfinetuned large language model, and then uses a novel retrieval model to\nreplace the quote placeholders by selecting a video clip that best supports the\nnarrative from a pool of candidate quotable video clips. We examine the\nproposed method on the task of documentary teaser generation, where short\ninterview insertions are commonly used to support the narrative of a\ndocumentary. Our objective evaluations show that the proposed method can\neffectively insert short video clips while maintaining a coherent narrative. In\na subjective survey, we show that our proposed method outperforms existing\nabstractive and extractive approaches in terms of coherence, alignment, and\nrealism in teaser generation."}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754", "abs": "https://arxiv.org/abs/2505.18754", "authors": ["Elsen Ronando", "Sozo Inoue"], "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "categories": ["cs.CL", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios."}
{"id": "2505.18881", "pdf": "https://arxiv.org/pdf/2505.18881", "abs": "https://arxiv.org/abs/2505.18881", "authors": ["Dicong Qiu", "Jiadi You", "Zeying Gong", "Ronghe Qiu", "Hui Xiong", "Junwei Liang"], "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Preprint. 21 pages", "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available."}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761", "abs": "https://arxiv.org/abs/2505.18761", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions."}
{"id": "2505.18899", "pdf": "https://arxiv.org/pdf/2505.18899", "abs": "https://arxiv.org/abs/2505.18899", "authors": ["Andrea Ramazzina", "Vittorio Giammarino", "Matteo El-Hariry", "Mario Bijelic"], "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO."}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762", "abs": "https://arxiv.org/abs/2505.18762", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs."}
{"id": "2505.18915", "pdf": "https://arxiv.org/pdf/2505.18915", "abs": "https://arxiv.org/abs/2505.18915", "authors": ["Yixiong Chen", "Wenjie Xiao", "Pedro R. A. S. Bassi", "Xinze Zhou", "Sezgin Er", "Ibrahim Ethem Hamamci", "Zongwei Zhou", "Alan Yuille"], "title": "Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical Benchmark for Tumor-centric Visual Question Answering", "categories": ["cs.CV"], "comment": "NeurIPS 2025 datasets&benchmarks track submission", "summary": "Vision-Language Models (VLMs) have shown promise in various 2D visual tasks,\nyet their readiness for 3D clinical diagnosis remains unclear due to stringent\ndemands for recognition precision, reasoning ability, and domain knowledge. To\nsystematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic\nvisual question answering (VQA) benchmark targeting abdominal tumors in CT\nscans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets,\nwith 395K expert-level questions spanning four categories: Recognition,\nMeasurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces\nunique challenges, including small tumor detection and clinical reasoning\nacross 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin,\nCT-CHAT), we find current models perform adequately on measurement tasks but\nstruggle with lesion recognition and reasoning, and are still not meeting\nclinical needs. Two key insights emerge: (1) large-scale multimodal pretraining\nplays a crucial role in DeepTumorVQA testing performance, making RadFM stand\nout among all VLMs. (2) Our dataset exposes critical differences in VLM\ncomponents, where proper image preprocessing and design of vision modules\nsignificantly affect 3D perception. To facilitate medical multimodal research,\nwe have released DeepTumorVQA as a rigorous benchmark:\nhttps://github.com/Schuture/DeepTumorVQA."}
{"id": "2505.18774", "pdf": "https://arxiv.org/pdf/2505.18774", "abs": "https://arxiv.org/abs/2505.18774", "authors": ["Mengqi Zhang", "Zisheng Zhou", "Xiaotian Ye", "Qiang Liu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Disentangling Knowledge Representations for Large Language Model Editing", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance."}
{"id": "2505.18924", "pdf": "https://arxiv.org/pdf/2505.18924", "abs": "https://arxiv.org/abs/2505.18924", "authors": ["Chenxi Li", "Nuo Chen", "Fengyun Tan", "Yantong Chen", "Bochun Yuan", "Tianrui Li", "Chongshou Li"], "title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point CLoud Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation."}
{"id": "2505.18778", "pdf": "https://arxiv.org/pdf/2505.18778", "abs": "https://arxiv.org/abs/2505.18778", "authors": ["Benjamin Bennetzen", "Peter Buus Steffensen", "Hans Httel", "Nikolaj Rossander Kristensen", "Andreas Tor Mortensen"], "title": "A generalised editor calculus (Short Paper)", "categories": ["cs.CL", "F.2.2, I.2.7"], "comment": "7 pages, 21 figures", "summary": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points"}
{"id": "2505.18925", "pdf": "https://arxiv.org/pdf/2505.18925", "abs": "https://arxiv.org/abs/2505.18925", "authors": ["Ross Greer", "Alisha Ukani", "Katherine Izhikevich", "Earlence Fernandes", "Stefan Savage", "Alex C. Snoeren"], "title": "Words as Geometric Features: Estimating Homography using Optical Character Recognition as Compressed Image Representation", "categories": ["cs.CV"], "comment": null, "summary": "Document alignment and registration play a crucial role in numerous\nreal-world applications, such as automated form processing, anomaly detection,\nand workflow automation. Traditional methods for document alignment rely on\nimage-based features like keypoints, edges, and textures to estimate geometric\ntransformations, such as homographies. However, these approaches often require\naccess to the original document images, which may not always be available due\nto privacy, storage, or transmission constraints. This paper introduces a novel\napproach that leverages Optical Character Recognition (OCR) outputs as features\nfor homography estimation. By utilizing the spatial positions and textual\ncontent of OCR-detected words, our method enables document alignment without\nrelying on pixel-level image data. This technique is particularly valuable in\nscenarios where only OCR outputs are accessible. Furthermore, the method is\nrobust to OCR noise, incorporating RANSAC to handle outliers and inaccuracies\nin the OCR data. On a set of test documents, we demonstrate that our OCR-based\napproach even performs more accurately than traditional image-based methods,\noffering a more efficient and scalable solution for document registration\ntasks. The proposed method facilitates applications in document processing, all\nwhile reducing reliance on high-dimensional image data."}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799", "abs": "https://arxiv.org/abs/2505.18799", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment."}
{"id": "2505.18930", "pdf": "https://arxiv.org/pdf/2505.18930", "abs": "https://arxiv.org/abs/2505.18930", "authors": ["Yanben Shen", "Timilehin T. Ayanlade", "Venkata Naresh Boddepalli", "Mojdeh Saadati", "Ashlyn Rairdin", "Zi K. Deng", "Muhammad Arbab Arshad", "Aditya Balu", "Daren Mueller", "Asheesh K Singh", "Wesley Everman", "Nirav Merchant", "Baskar Ganapathysubramanian", "Meaghan Anderson", "Soumik Sarkar", "Arti Singh"], "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early identification of weeds is essential for effective management and\ncontrol, and there is growing interest in automating the process using computer\nvision techniques coupled with AI methods. However, challenges associated with\ntraining AI-based weed identification models, such as limited expert-verified\ndata and complexity and variability in morphological features, have hindered\nprogress. To address these issues, we present WeedNet, the first global-scale\nweed identification model capable of recognizing an extensive set of weed\nspecies, including noxious and invasive plant species. WeedNet is an end-to-end\nreal-time weed identification pipeline and uses self-supervised learning,\nfine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%\naccuracy across 1,593 weed species, with 41% species achieving 100% accuracy.\nUsing a fine-tuning strategy and a Global-to-Local approach, the local Iowa\nWeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most\nclasses exceeded a 90% mean accuracy per class. Testing across intra-species\ndissimilarity (developmental stages) and inter-species similarity (look-alike\nspecies) suggests that diversity in the images collected, spanning all the\ngrowth stages and distinguishable plant characteristics, is crucial in driving\nmodel performance. The generalizability and adaptability of the Global WeedNet\nmodel enable it to function as a foundational model, with the Global-to-Local\nstrategy allowing fine-tuning for region-specific weed communities. Additional\nvalidation of drone- and ground-rover-based images highlights the potential of\nWeedNet for integration into robotic platforms. Furthermore, integration with\nAI for conversational use provides intelligent agricultural and ecological\nconservation consulting tools for farmers, agronomists, researchers, land\nmanagers, and government agencies across diverse landscapes."}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842", "abs": "https://arxiv.org/abs/2505.18842", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch."}
{"id": "2505.18932", "pdf": "https://arxiv.org/pdf/2505.18932", "abs": "https://arxiv.org/abs/2505.18932", "authors": ["Hyunho Ha", "Lei Xiao", "Christian Richardt", "Thu Nguyen-Phuoc", "Changil Kim", "Min H. Kim", "Douglas Lanman", "Numair Khan"], "title": "Geometry-guided Online 3D Video Synthesis with Multi-View Temporal Consistency", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project website:\n  https://nkhan2.github.io/projects/geometry-guided-2025/index.html", "summary": "We introduce a novel geometry-guided online video view synthesis method with\nenhanced view and temporal consistency. Traditional approaches achieve\nhigh-quality synthesis from dense multi-view camera setups but require\nsignificant computational resources. In contrast, selective-input methods\nreduce this cost but often compromise quality, leading to multi-view and\ntemporal inconsistencies such as flickering artifacts. Our method addresses\nthis challenge to deliver efficient, high-quality novel-view synthesis with\nview and temporal consistency. The key innovation of our approach lies in using\nglobal geometry to guide an image-based rendering pipeline. To accomplish this,\nwe progressively refine depth maps using color difference masks across time.\nThese depth maps are then accumulated through truncated signed distance fields\nin the synthesized view's image space. This depth representation is view and\ntemporally consistent, and is used to guide a pre-trained blending network that\nfuses multiple forward-rendered input-view images. Thus, the network is\nencouraged to output geometrically consistent synthesis results across multiple\nviews and time. Our approach achieves consistent, high-quality video synthesis,\nwhile running efficiently in an online manner."}
{"id": "2505.18845", "pdf": "https://arxiv.org/pdf/2505.18845", "abs": "https://arxiv.org/abs/2505.18845", "authors": ["Sagar Sapkota", "Mohammad Saqib Hasan", "Mubarak Shah", "Santu Karmaker"], "title": "Multi-Party Conversational Agents: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs."}
{"id": "2505.18945", "pdf": "https://arxiv.org/pdf/2505.18945", "abs": "https://arxiv.org/abs/2505.18945", "authors": ["Jintao Sun", "Hu Zhang", "Gangyi Ding", "Zhedong Zheng"], "title": "Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 4 figures", "summary": "Modern end-to-end autonomous driving systems suffer from a critical\nlimitation: their planners lack mechanisms to enforce temporal consistency\nbetween predicted trajectories and evolving scene dynamics. This absence of\nself-supervision allows early prediction errors to compound catastrophically\nover time. We introduce Echo Planning, a novel self-correcting framework that\nestablishes a closed-loop Current - Future - Current (CFC) cycle to harmonize\ntrajectory prediction with scene coherence. Our key insight is that plausible\nfuture trajectories must be bi-directionally consistent, ie, not only generated\nfrom current observations but also capable of reconstructing them. The CFC\nmechanism first predicts future trajectories from the Bird's-Eye-View (BEV)\nscene representation, then inversely maps these trajectories back to estimate\nthe current BEV state. By enforcing consistency between the original and\nreconstructed BEV representations through a cycle loss, the framework\nintrinsically penalizes physically implausible or misaligned trajectories.\nExperiments on nuScenes demonstrate state-of-the-art performance, reducing L2\nerror by 0.04 m and collision rate by 0.12% compared to one-shot planners.\nCrucially, our method requires no additional supervision, leveraging the CFC\ncycle as an inductive bias for robust planning. This work offers a deployable\nsolution for safety-critical autonomous systems."}
{"id": "2505.18853", "pdf": "https://arxiv.org/pdf/2505.18853", "abs": "https://arxiv.org/abs/2505.18853", "authors": ["Alexander Shabalin", "Viacheslav Meshchaninov", "Dmitry Vetrov"], "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation", "categories": ["cs.CL"], "comment": "17 pages, 2 figures, 8 tables", "summary": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie."}
{"id": "2505.18947", "pdf": "https://arxiv.org/pdf/2505.18947", "abs": "https://arxiv.org/abs/2505.18947", "authors": ["Zhenhao Zhang", "Ye Shi", "Lingxiao Yang", "Suting Ni", "Qi Ye", "Jingya Wang"], "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is\ncritical for applications ranging from immersive AR/VR to dexterous robotics.\nExisting methods struggle with generalization, performing well on closed-set\nobjects and predefined tasks but failing to handle unseen objects or\nopen-vocabulary instructions. We introduce OpenHOI, the first framework for\nopen-world HOI synthesis, capable of generating long-horizon manipulation\nsequences for novel objects guided by free-form language commands. Our approach\nintegrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint\naffordance grounding and semantic task decomposition, enabling precise\nlocalization of interaction regions (e.g., handles, buttons) and breakdown of\ncomplex instructions (e.g., \"Find a water bottle and take a sip\") into\nexecutable sub-tasks. To synthesize physically plausible interactions, we\npropose an affordance-driven diffusion model paired with a training-free\nphysics refinement stage that minimizes penetration and optimizes affordance\nalignment. Evaluations across diverse scenarios demonstrate OpenHOI's\nsuperiority over state-of-the-art methods in generalizing to novel object\ncategories, multi-stage tasks, and complex language instructions. Our project\npage at \\href{https://openhoi.github.io}"}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859", "abs": "https://arxiv.org/abs/2505.18859", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task."}
{"id": "2505.18956", "pdf": "https://arxiv.org/pdf/2505.18956", "abs": "https://arxiv.org/abs/2505.18956", "authors": ["Yining Pan", "Qiongjie Cui", "Xulei Yang", "Na Zhao"], "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted at the 2025 International Conference on Machine Learning\n  (ICML)", "summary": "LiDAR-based 3D panoptic segmentation often struggles with the inherent\nsparsity of data from LiDAR sensors, which makes it challenging to accurately\nrecognize distant or small objects. Recently, a few studies have sought to\novercome this challenge by integrating LiDAR inputs with camera images,\nleveraging the rich and dense texture information provided by the latter. While\nthese approaches have shown promising results, they still face challenges, such\nas misalignment during data augmentation and the reliance on post-processing\nsteps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel\nmulti-modal 3D panoptic segmentation framework. In IAL, we first introduce a\nmodality-synchronized data augmentation strategy, PieAug, to ensure alignment\nbetween LiDAR and image inputs from the start. Next, we adopt a transformer\ndecoder to directly predict panoptic segmentation results. To effectively fuse\nLiDAR and image features into tokens for the decoder, we design a\nGeometric-guided Token Fusion (GTF) module. Additionally, we leverage the\ncomplementary strengths of each modality as priors for query initialization\nthrough a Prior-based Query Generation (PQG) module, enhancing the decoder's\nability to generate accurate instance masks. Our IAL framework achieves\nstate-of-the-art performance compared to previous multi-modal 3D panoptic\nsegmentation methods on two widely used benchmarks. Code and models are\npublicly available at <https://github.com/IMPL-Lab/IAL.git>."}
{"id": "2505.18864", "pdf": "https://arxiv.org/pdf/2505.18864", "abs": "https://arxiv.org/abs/2505.18864", "authors": ["Binhao Ma", "Hanqing Guo", "Zhengping Jay Luo", "Rui Duan"], "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs."}
{"id": "2505.18958", "pdf": "https://arxiv.org/pdf/2505.18958", "abs": "https://arxiv.org/abs/2505.18958", "authors": ["Jiong Wu", "Yang Xing", "Boxiao Yu", "Wei Shao", "Kuang Gong"], "title": "CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Most publicly available medical segmentation datasets are only partially\nlabeled, with annotations provided for a subset of anatomical structures. When\nmultiple datasets are combined for training, this incomplete annotation poses\nchallenges, as it limits the model's ability to learn shared anatomical\nrepresentations among datasets. Furthermore, vision-only frameworks often fail\nto capture complex anatomical relationships and task-specific distinctions,\nleading to reduced segmentation accuracy and poor generalizability to unseen\ndatasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven\nSegmentation Network (CDPDNet), which combined a self-supervised vision\ntransformer with CLIP-based text embedding and introduced task-specific text\nprompts to tackle these challenges. Specifically, the framework was constructed\nupon a convolutional neural network (CNN) and incorporated DINOv2 to extract\nboth fine-grained and global visual features, which were then fused using a\nmulti-head cross-attention module to overcome the limited long-range modeling\ncapability of CNNs. In addition, CLIP-derived text embeddings were projected\ninto the visual space to help model complex relationships among organs and\ntumors. To further address the partial label challenge and enhance inter-task\ndiscriminative capability, a Text-based Task Prompt Generation (TTPG) module\nthat generated task-specific prompts was designed to guide the segmentation.\nExtensive experiments on multiple medical imaging datasets demonstrated that\nCDPDNet consistently outperformed existing state-of-the-art segmentation\nmethods. Code and pretrained model are available at:\nhttps://github.com/wujiong-hub/CDPDNet.git."}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867", "abs": "https://arxiv.org/abs/2505.18867", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing."}
{"id": "2505.18963", "pdf": "https://arxiv.org/pdf/2505.18963", "abs": "https://arxiv.org/abs/2505.18963", "authors": ["Jeffrey A. Chan-Santiago", "Praveen Tirupattur", "Gaurav Kumar Nayak", "Gaowen Liu", "Mubarak Shah"], "title": "MGD$^3$: Mode-Guided Dataset Distillation using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation has emerged as an effective strategy, significantly\nreducing training costs and facilitating more efficient model deployment.\nRecent advances have leveraged generative models to distill datasets by\ncapturing the underlying data distribution. Unfortunately, existing methods\nrequire model fine-tuning with distillation losses to encourage diversity and\nrepresentativeness. However, these methods do not guarantee sample diversity,\nlimiting their performance. We propose a mode-guided diffusion model leveraging\na pre-trained diffusion model without the need to fine-tune with distillation\nlosses. Our approach addresses dataset diversity in three stages: Mode\nDiscovery to identify distinct data modes, Mode Guidance to enhance intra-class\ndiversity, and Stop Guidance to mitigate artifacts in synthetic samples that\naffect performance. Our approach outperforms state-of-the-art methods,\nachieving accuracy gains of 4.4%, 2.9%, 1.6%, and 1.6% on ImageNette, ImageIDC,\nImageNet-100, and ImageNet-1K, respectively. Our method eliminates the need for\nfine-tuning diffusion models with distillation losses, significantly reducing\ncomputational costs. Our code is available on the project webpage:\nhttps://jachansantiago.github.io/mode-guided-distillation/"}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878", "abs": "https://arxiv.org/abs/2505.18878", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition."}
{"id": "2505.18986", "pdf": "https://arxiv.org/pdf/2505.18986", "abs": "https://arxiv.org/abs/2505.18986", "authors": ["Zhiwei Lin", "Yongtao Wang"], "title": "VL-SAM-V2: Open-World Object Detection with General and Specific Query Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Current perception models have achieved remarkable success by leveraging\nlarge-scale labeled datasets, but still face challenges in open-world\nenvironments with novel objects. To address this limitation, researchers\nintroduce open-set perception models to detect or segment arbitrary test-time\nuser-input categories. However, open-set models rely on human involvement to\nprovide predefined object categories as input during inference. More recently,\nresearchers have framed a more realistic and challenging task known as\nopen-ended perception that aims to discover unseen objects without requiring\nany category-level input from humans at inference time. Nevertheless,\nopen-ended models suffer from low performance compared to open-set models. In\nthis paper, we present VL-SAM-V2, an open-world object detection framework that\nis capable of discovering unseen objects while achieving favorable performance.\nTo achieve this, we combine queries from open-set and open-ended models and\npropose a general and specific query fusion module to allow different queries\nto interact. By adjusting queries from open-set models, we enable VL-SAM-V2 to\nbe evaluated in the open-set or open-ended mode. In addition, to learn more\ndiverse queries, we introduce ranked learnable queries to match queries with\nproposals from open-ended models by sorting. Moreover, we design a denoising\npoint training strategy to facilitate the training process. Experimental\nresults on LVIS show that our method surpasses the previous open-set and\nopen-ended methods, especially on rare objects."}
{"id": "2505.18903", "pdf": "https://arxiv.org/pdf/2505.18903", "abs": "https://arxiv.org/abs/2505.18903", "authors": ["Valentin Barriere", "Nahuel Gomez", "Leo Hemamou", "Sofia Callejas", "Brian Ravenet"], "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos", "categories": ["cs.CL"], "comment": null, "summary": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic"}
{"id": "2505.18988", "pdf": "https://arxiv.org/pdf/2505.18988", "abs": "https://arxiv.org/abs/2505.18988", "authors": ["Varun Jain", "Zongwei Wu", "Quan Zou", "Louis Florentin", "Henrik Turbell", "Sandeep Siddhartha", "Radu Timofte", "others"], "title": "NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive review of the 1st Challenge on Video\nQuality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR\n2025, and highlights the problem statement, datasets, proposed solutions, and\nresults. The aim of this challenge was to design a Video Quality Enhancement\n(VQE) model to enhance video quality in video conferencing scenarios by (a)\nimproving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing\nsharpness - giving a professional studio-like effect. Participants were given a\ndifferentiable Video Quality Assessment (VQA) model, training, and test videos.\nA total of 91 participants registered for the challenge. We received 10 valid\nsubmissions that were evaluated in a crowdsourced framework."}
{"id": "2505.18905", "pdf": "https://arxiv.org/pdf/2505.18905", "abs": "https://arxiv.org/abs/2505.18905", "authors": ["Kweku Andoh Yamoah", "Jackson Weako", "Emmanuel J. Dorley"], "title": "Building a Functional Machine Translation Corpus for Kpelle", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages."}
{"id": "2505.18989", "pdf": "https://arxiv.org/pdf/2505.18989", "abs": "https://arxiv.org/abs/2505.18989", "authors": ["Catalina Tan", "Yipeng Hu", "Shaheer U. Saeed"], "title": "SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours", "categories": ["cs.CV"], "comment": "Accepted at Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Accurate tumour segmentation is vital for various targeted diagnostic and\ntherapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.\nManual delineation is extremely labour-intensive, requiring substantial expert\ntime. Fully-supervised machine learning models aim to automate such\nlocalisation tasks, but require a large number of costly and often subjective\n3D voxel-level labels for training. The high-variance and subjectivity in such\nlabels impacts model generalisability, even when large datasets are available.\nHistopathology labels may offer more objective labels but the infeasibility of\nacquiring pixel-level annotations to develop tumour localisation methods based\non histology remains challenging in-vivo. In this work, we propose a novel\nweakly-supervised semantic segmentation framework called SPARS (Self-Play\nAdversarial Reinforcement Learning for Segmentation), which utilises an object\npresence classifier, trained on a small number of image-level binary cancer\npresence labels, to localise cancerous regions on CT scans. Such binary labels\nof patient-level cancer presence can be sourced more feasibly from biopsies and\nhistopathology reports, enabling a more objective cancer localisation on\nmedical images. Evaluating with real patient data, we observed that SPARS\nyielded a mean dice score of $77.3 \\pm 9.4$, which outperformed other\nweakly-supervised methods by large margins. This performance was comparable\nwith recent fully-supervised methods that require voxel-level annotations. Our\nresults demonstrate the potential of using SPARS to reduce the need for\nextensive human-annotated labels to detect cancer in real-world healthcare\nsettings."}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906", "abs": "https://arxiv.org/abs/2505.18906", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems."}
{"id": "2505.18991", "pdf": "https://arxiv.org/pdf/2505.18991", "abs": "https://arxiv.org/abs/2505.18991", "authors": ["Hancong Jin", "Zihan Cao", "Liangjian Deng"], "title": "Kernel Space Diffusion Model for Efficient Remote Sensing Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a fundamental task in remote sensing that integrates\nhigh-resolution panchromatic imagery (PAN) with low-resolution multispectral\nimagery (LRMS) to produce an enhanced image with both high spatial and spectral\nresolution. Despite significant progress in deep learning-based approaches,\nexisting methods often fail to capture the global priors inherent in remote\nsensing data distributions. Diffusion-based models have recently emerged as\npromising solutions due to their powerful distribution mapping capabilities;\nhowever, they suffer from significant inference latency, which limits their\npractical applicability. In this work, we propose the Kernel Space Diffusion\nModel (KSDiff), a novel approach that leverages diffusion processes in a latent\nspace to generate convolutional kernels enriched with global contextual\ninformation, thereby improving pansharpening quality while enabling faster\ninference. Specifically, KSDiff constructs these kernels through the\nintegration of a low-rank core tensor generator and a unified factor generator,\norchestrated by a structure-aware multi-head attention mechanism. We further\nintroduce a two-stage training strategy tailored for pansharpening, enabling\nKSDiff to serve as a framework for enhancing existing pansharpening\narchitectures. Experiments on three widely used datasets, including\nWorldView-3, GaoFen-2, and QuickBird, demonstrate the superior performance of\nKSDiff both qualitatively and quantitatively. Code will be released upon\npossible acceptance."}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916", "abs": "https://arxiv.org/abs/2505.18916", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, a multilingual dataset for Rumour Stance\nClassification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond\nexisting stance classification datasets by covering more languages (9), linking\nexamples to more fact-checked claims (2.1k), and including complex annotations\nfrom multiple annotators to account for intra- and inter-annotator variability.\nAnnotations were made by at least three native speakers per language, totalling\naround 405 hours of annotation and 8,150 dollars in compensation. Experiments\non SCRum-9 show that it is a challenging benchmark for both state-of-the-art\nLLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating\nfuture work in this area."}
{"id": "2505.18992", "pdf": "https://arxiv.org/pdf/2505.18992", "abs": "https://arxiv.org/abs/2505.18992", "authors": ["Tianchen Deng", "Wenhua Wu", "Junjie He", "Yue Pan", "Xirui Jiang", "Shenghai Yuan", "Danwei Wang", "Hesheng Wang", "Weidong Chen"], "title": "VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has recently shown promising results in dense visual\nSLAM. However, existing 3DGS-based SLAM methods are all constrained to\nsmall-room scenarios and struggle with memory explosion in large-scale scenes\nand long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based\nlarge-scale RGBD SLAM framework for both indoor and outdoor scenarios. We\ndesign a novel voxel-based progressive 3D Gaussian mapping method with multiple\nsubmaps for compact and accurate scene representation in large-scale and\nlong-sequence scenes. This allows us to scale up to arbitrary scenes and\nimproves robustness (even under pose drifts). In addition, we propose a 2D-3D\nfusion camera tracking method to achieve robust and accurate camera tracking in\nboth indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D\nGaussian loop closure method to eliminate pose drift. We further propose a\nsubmap fusion method with online distillation to achieve global consistency in\nlarge-scale scenes when detecting a loop. Experiments on various indoor and\noutdoor datasets demonstrate the superiority and generalizability of the\nproposed framework. The code will be open source on\nhttps://github.com/dtc111111/vpgs-slam."}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927", "abs": "https://arxiv.org/abs/2505.18927", "authors": ["Amel Muminovic"], "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010", "abs": "https://arxiv.org/abs/2505.19010", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has become a critical research area because integrating\ntext and image data can significantly improve performance in tasks such as\nclassification, retrieval, and scene understanding. However, despite progress\nwith pre-trained models, current approaches are limited by inadequate\ncross-modal interactions and static fusion strategies that do not fully exploit\nthe complementary nature of different modalities. To address these\nshortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that\nleverages dual-path encoding, co-attention with dimension-wise gating, and\nadvanced expert fusion. Our approach begins by projecting text and image\nfeatures into a common embedding space, where a dedicated co-attention\nmechanism enables simultaneous, fine-grained interactions between modalities.\nThis mechanism is further enhanced by a dimension-wise gating network that\nadaptively regulates the feature contributions at the channel level, ensuring\nthat only the most relevant information is emphasized. In parallel, dual-path\nencoders refine the representations by processing cross-modal information\nseparately before an additional cross-attention layer further aligns\nmodalities. The refined features are then aggregated via an expert fusion\nmodule that combines learned gating and self-attention to produce a robust,\nunified representation. We validate our approach on the MIMIC and SemEval\nMemotion 1.0, where experimental results demonstrate significant improvements\nin cross-modal alignment and state-of-the-art performance, underscoring the\npotential of our model for a wide range of multi-modal applications."}
{"id": "2505.18943", "pdf": "https://arxiv.org/pdf/2505.18943", "abs": "https://arxiv.org/abs/2505.18943", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses user mental states (e.g., intent,\nemotion), (2) a Domain Agent refines these hypotheses using cultural norms and\nethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind."}
{"id": "2505.19015", "pdf": "https://arxiv.org/pdf/2505.19015", "abs": "https://arxiv.org/abs/2505.19015", "authors": ["Jingping Liu", "Ziyan Liu", "Zhedong Cen", "Yan Zhou", "Yinan Zou", "Weiyan Zhang", "Haiyun Jiang", "Tong Ruan"], "title": "Can Multimodal Large Language Models Understand Spatial Relations?", "categories": ["cs.CV", "cs.MM"], "comment": "13 pages, 19 figures", "summary": "Spatial relation reasoning is a crucial task for multimodal large language\nmodels (MLLMs) to understand the objective world. However, current benchmarks\nhave issues like relying on bounding boxes, ignoring perspective substitutions,\nor allowing questions to be answered using only the model's prior knowledge\nwithout image understanding. To address these issues, we introduce SpatialMQA,\na human-annotated spatial relation reasoning benchmark based on COCO2017, which\nenables MLLMs to focus more on understanding images in the objective world. To\nensure data quality, we design a well-tailored annotation procedure, resulting\nin SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of\nclosed- and open-source MLLMs are implemented and the results indicate that the\ncurrent state-of-the-art MLLM achieves only 48.14% accuracy, far below the\nhuman-level accuracy of 98.40%. Extensive experimental analyses are also\nconducted, suggesting the future research directions. The benchmark and codes\nare available at https://github.com/ziyan-xiaoyu/SpatialMQA.git."}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949", "abs": "https://arxiv.org/abs/2505.18949", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "title": "The Price of Format: Diversity Collapse in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning."}
{"id": "2505.19022", "pdf": "https://arxiv.org/pdf/2505.19022", "abs": "https://arxiv.org/abs/2505.19022", "authors": ["Zihao Liu", "Xiaoyu Wu", "Wenna Li", "Linlin Yang"], "title": "Rethinking Metrics and Benchmarks of Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Anomaly Detection (VAD), which aims to detect anomalies that deviate\nfrom expectation, has attracted increasing attention in recent years. Existing\nadvancements in VAD primarily focus on model architectures and training\nstrategies, while devoting insufficient attention to evaluation metrics and\nbenchmarks. In this paper, we rethink VAD evaluation protocols through\ncomprehensive experimental analyses, revealing three critical limitations in\ncurrent practices: 1) existing metrics are significantly influenced by single\nannotation bias; 2) current metrics fail to reward early detection of\nanomalies; 3) available benchmarks lack the capability to evaluate scene\noverfitting. To address these limitations, we propose three novel evaluation\nmethods: first, we establish averaged AUC/AP metrics over multi-round\nannotations to mitigate single annotation bias; second, we develop a\nLatency-aware Average Precision (LaAP) metric that rewards early and accurate\nanomaly detection; and finally, we introduce two hard normal benchmarks\n(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene\noverfitting. We report performance comparisons of ten state-of-the-art VAD\napproaches using our proposed evaluation methods, providing novel perspectives\nfor future VAD model development."}
{"id": "2505.18951", "pdf": "https://arxiv.org/pdf/2505.18951", "abs": "https://arxiv.org/abs/2505.18951", "authors": ["Saman Sarker Joy"], "title": "BnMMLU: Measuring Massive Multitask Language Understanding in Bengali", "categories": ["cs.CL"], "comment": "18 pages, 9 figures, 5 tables; Code & dataset available at\n  https://github.com/samanjoy2/bnmmlu", "summary": "The Massive Multitask Language Understanding (MMLU) benchmark has been widely\nused to evaluate language models across various domains. However, existing MMLU\ndatasets primarily focus on high-resource languages such as English, which\nleaves low-resource languages like Bengali underrepresented. In this paper, we\nintroduce BnMMLU, a benchmark to evaluate the multitask language understanding\ncapabilities of Bengali in language models. The dataset spans 23 domains,\nincluding science, humanities, mathematics and general knowledge and is\nstructured in a multiple-choice format to assess factual knowledge,\napplication-based problem-solving and reasoning abilities of language models.\nIt consists of 138,949 question-option pairs. We benchmark several proprietary\nand open-source large language models (LLMs) on the BnMMLU test set.\nAdditionally, we annotate the test set with three cognitive categories-factual\nknowledge, procedural application and reasoning-to gain deeper insights into\nmodel strengths and weaknesses across various cognitive tasks. The results\nreveal significant performance gaps, highlighting the need for improved\npre-training and fine-tuning strategies tailored to Bengali data. We release\nthe dataset and benchmark results to facilitate further research in this area."}
{"id": "2505.19023", "pdf": "https://arxiv.org/pdf/2505.19023", "abs": "https://arxiv.org/abs/2505.19023", "authors": ["Huda Alghoraibi", "Nuha Alqurashi", "Sarah Alotaibi", "Renad Alkhudaydi", "Bdoor Aldajani", "Lubna Alqurashi", "Jood Batweel", "Maha A. Thafar"], "title": "A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "23 pages, 5 figures", "summary": "Monkeypox is a viral disease characterized by distinctive skin lesions and\nhas been reported in many countries. The recent global outbreak has emphasized\nthe urgent need for scalable, accessible, and accurate diagnostic solutions to\nsupport public health responses.\n  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare\nsystem specifically designed to detect Monkeypox from skin lesion images using\nadvanced deep learning techniques. Our system consists of three main\ncomponents. First, we trained and evaluated several pretrained models using\ntransfer learning on publicly available skin lesion datasets to identify the\nmost effective models. For binary classification (Monkeypox vs. non-Monkeypox),\nthe Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16\nachieved the highest performance, each with an accuracy and F1-score of 97.8%.\nFor multiclass classification, which contains images of patients with Monkeypox\nand five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,\nand healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1\nscores of 92.24% and 92.19%, respectively. The best-performing and most\nlightweight model, MobileViT, was deployed within the mobile application. The\nsecond component is a cross-platform smartphone application that enables users\nto detect Monkeypox through image analysis, track symptoms, and receive\nrecommendations for nearby healthcare centers based on their location. The\nthird component is a real-time monitoring dashboard designed for health\nauthorities to support them in tracking cases, analyzing symptom trends,\nguiding public health interventions, and taking proactive measures.\n  This system is fundamental in developing responsive healthcare infrastructure\nwithin smart cities. Our solution, ITMAINN, is part of revolutionizing public\nhealth management."}
{"id": "2505.18953", "pdf": "https://arxiv.org/pdf/2505.18953", "abs": "https://arxiv.org/abs/2505.18953", "authors": ["Divij Chawla", "Ashita Bhutada", "Do Duc Anh", "Abhinav Raghunathan", "Vinod SP", "Cathy Guo", "Dar Win Liew", "Prannaya Gupta", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "title": "Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?", "categories": ["cs.CL"], "comment": null, "summary": "We evaluate the credibility of leading AI models in assessing investment risk\nappetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and\nopen-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720\nuser profiles constructed with 16 risk-relevant features across 10 countries\nand both genders. We observe significant variance across models in score\ndistributions and demographic sensitivity. For example, GPT-4o assigns higher\nrisk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show\nopposite gender tendencies in risk classification. While some models (e.g.,\nGPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk\nranges, none maintain consistent performance across regions and demographics.\nOur findings highlight the need for rigorous, standardized evaluations of AI\nsystems in regulated financial contexts to prevent bias, opacity, and\ninconsistency in real-world deployment."}
{"id": "2505.19028", "pdf": "https://arxiv.org/pdf/2505.19028", "abs": "https://arxiv.org/abs/2505.19028", "authors": ["Minzhi Lin", "Tianchi Xie", "Mengchen Liu", "Yilin Ye", "Changjian Chen", "Shixia Liu"], "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA."}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962", "abs": "https://arxiv.org/abs/2505.18962", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space.Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage."}
{"id": "2505.19031", "pdf": "https://arxiv.org/pdf/2505.19031", "abs": "https://arxiv.org/abs/2505.19031", "authors": ["Xikai Yang", "Juzheng Miao", "Yuchen Yuan", "Jiaze Wang", "Qi Dou", "Jinpeng Li", "Pheng-Ann Heng"], "title": "Medical Large Vision Language Models with Multi-Image Visual Ability", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Medical large vision-language models (LVLMs) have demonstrated promising\nperformance across various single-image question answering (QA) benchmarks, yet\ntheir capability in processing multi-image clinical scenarios remains\nunderexplored. Unlike single image based tasks, medical tasks involving\nmultiple images often demand sophisticated visual understanding capabilities,\nsuch as temporal reasoning and cross-modal analysis, which are poorly supported\nby current medical LVLMs. To bridge this critical gap, we present the Med-MIM\ninstruction dataset, comprising 83.2K medical multi-image QA pairs that span\nfour types of multi-image visual abilities (temporal understanding, reasoning,\ncomparison, co-reference). Using this dataset, we fine-tune Mantis and\nLLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and\nMed-Mantis, both optimized for multi-image analysis. Additionally, we develop\nthe Med-MIM benchmark to comprehensively evaluate the medical multi-image\nunderstanding capabilities of LVLMs. We assess eight popular LVLMs, including\nour two models, on the Med-MIM benchmark. Experimental results show that both\nMed-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and\nheld-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM\ninstruction dataset effectively enhances LVLMs' multi-image understanding\ncapabilities in the medical domain."}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970", "abs": "https://arxiv.org/abs/2505.18970", "authors": ["Bowen Wei", "Ziwei Zhu"], "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications."}
{"id": "2505.19049", "pdf": "https://arxiv.org/pdf/2505.19049", "abs": "https://arxiv.org/abs/2505.19049", "authors": ["Lu Wang", "Xishuai Peng", "S. Kevin Zhou"], "title": "Disentangled Human Body Representation Based on Unsupervised Semantic-Aware Learning", "categories": ["cs.CV"], "comment": "8 pages", "summary": "In recent years, more and more attention has been paid to the learning of 3D\nhuman representation. However, the complexity of lots of hand-defined human\nbody constraints and the absence of supervision data limit that the existing\nworks controllably and accurately represent the human body in views of\nsemantics and representation ability. In this paper, we propose a human body\nrepresentation with controllable fine-grained semantics and high precison of\nreconstruction in an unsupervised learning framework. In particularly, we\ndesign a whole-aware skeleton-grouped disentangle strategy to learn a\ncorrespondence between geometric semantical measurement of body and latent\ncodes, which facilitates the control of shape and posture of human body by\nmodifying latent coding paramerers. With the help of skeleton-grouped\nwhole-aware encoder and unsupervised disentanglement losses, our representation\nmodel is learned by an unsupervised manner. Besides, a based-template residual\nlearning scheme is injected into the encoder to ease of learning human body\nlatent parameter in complicated body shape and pose spaces. Because of the\ngeometrically meaningful latent codes, it can be used in a wide range of\napplications, from human body pose transfer to bilinear latent code\ninterpolation. Further more, a part-aware decoder is utlized to promote the\nlearning of controllable fine-grained semantics. The experimental results on\npublic 3D human datasets show that the method has the ability of precise\nreconstruction."}
{"id": "2505.18971", "pdf": "https://arxiv.org/pdf/2505.18971", "abs": "https://arxiv.org/abs/2505.18971", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Ashutosh Balasubramaniam", "Tejas Anvekar", "Vivek Gupta"], "title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion."}
{"id": "2505.19057", "pdf": "https://arxiv.org/pdf/2505.19057", "abs": "https://arxiv.org/abs/2505.19057", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "Less is More: Efficient Point Cloud Reconstruction via Multi-Head Decoders", "categories": ["cs.CV"], "comment": null, "summary": "We challenge the common assumption that deeper decoder architectures always\nyield better performance in point cloud reconstruction. Our analysis reveals\nthat, beyond a certain depth, increasing decoder complexity leads to\noverfitting and degraded generalization. Additionally, we propose a novel\nmulti-head decoder architecture that exploits the inherent redundancy in point\nclouds by reconstructing complete shapes from multiple independent heads, each\noperating on a distinct subset of points. The final output is obtained by\nconcatenating the predictions from all heads, enhancing both diversity and\nfidelity. Extensive experiments on ModelNet40 and ShapeNetPart demonstrate that\nour approach achieves consistent improvements across key metrics--including\nChamfer Distance (CD), Hausdorff Distance (HD), Earth Mover's Distance (EMD),\nand F1-score--outperforming standard single-head baselines. Our findings\nhighlight that output diversity and architectural design can be more critical\nthan depth alone for effective and efficient point cloud reconstruction."}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973", "abs": "https://arxiv.org/abs/2505.18973", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail."}
{"id": "2505.19063", "pdf": "https://arxiv.org/pdf/2505.19063", "abs": "https://arxiv.org/abs/2505.19063", "authors": ["Xin Ma", "Yaohui Wang", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Training-free Stylized Text-to-Image Generation with Fast Inference", "categories": ["cs.CV"], "comment": "Project Page: https://maxin-cn.github.io/omnipainter_project", "summary": "Although diffusion models exhibit impressive generative capabilities,\nexisting methods for stylized image generation based on these models often\nrequire textual inversion or fine-tuning with style images, which is\ntime-consuming and limits the practical applicability of large-scale diffusion\nmodels. To address these challenges, we propose a novel stylized image\ngeneration method leveraging a pre-trained large-scale diffusion model without\nrequiring fine-tuning or any additional optimization, termed as OmniPainter.\nSpecifically, we exploit the self-consistency property of latent consistency\nmodels to extract the representative style statistics from reference style\nimages to guide the stylization process. Additionally, we then introduce the\nnorm mixture of self-attention, which enables the model to query the most\nrelevant style patterns from these statistics for the intermediate output\ncontent features. This mechanism also ensures that the stylized results align\nclosely with the distribution of the reference style images. Our qualitative\nand quantitative experimental results demonstrate that the proposed method\noutperforms state-of-the-art approaches."}
{"id": "2505.18978", "pdf": "https://arxiv.org/pdf/2505.18978", "abs": "https://arxiv.org/abs/2505.18978", "authors": ["Miguel Angel Pealoza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Michelle Bruno Hernandez", "Miguel Angel Alvarado Gonzalez", "Sandra Malagon"], "title": "AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models", "categories": ["cs.CL", "68", "I.2"], "comment": "36 pages, 5 figures", "summary": "Existing mathematical reasoning benchmarks are predominantly English only or\ntranslation-based, which can introduce semantic drift and mask languagespecific\nreasoning errors. To address this, we present AI4Math, a benchmark of 105\noriginal university level math problems natively authored in Spanish. The\ndataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,\nNumber Theory, Combinatorics, and Logic), and each problem is accompanied by a\nstep by step human solution. We evaluate six large language models GPT 4o, GPT\n4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under\nfour configurations: zero shot and chain of thought, each in Spanish and\nEnglish. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve\nover 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most\nmodels show no significant performance drop between languages, with GPT 4o even\nperforming better on Spanish problems in the zero shot setting. Geometry,\nCombinatorics, and Probability questions remain persistently challenging for\nall models. These results highlight the need for native-language benchmarks and\ndomain-specific evaluations to reveal reasoning failures not captured by\nstandard metrics."}
{"id": "2505.19065", "pdf": "https://arxiv.org/pdf/2505.19065", "abs": "https://arxiv.org/abs/2505.19065", "authors": ["Jiashuo Chang", "Zhengyi Li", "Jianxun Lou", "Zhen Qiu", "Hanhe Lin"], "title": "MMP-2K: A Benchmark Multi-Labeled Macro Photography Image Quality Assessment Database", "categories": ["cs.CV"], "comment": "Accepted to the IEEE International Conference on Image Processing,\n  IEEE ICIP 2025", "summary": "Macro photography (MP) is a specialized field of photography that captures\nobjects at an extremely close range, revealing tiny details. Although an\naccurate macro photography image quality assessment (MPIQA) metric can benefit\nmacro photograph capturing, which is vital in some domains such as scientific\nresearch and medical applications, the lack of MPIQA data limits the\ndevelopment of MPIQA metrics. To address this limitation, we conducted a\nlarge-scale MPIQA study. Specifically, to ensure diversity both in content and\nquality, we sampled 2,000 MP images from 15,700 MP images, collected from three\npublic image websites. For each MP image, 17 (out of 21 after outlier removal)\nquality ratings and a detailed quality report of distortion magnitudes, types,\nand positions are gathered by a lab study. The images, quality ratings, and\nquality reports form our novel multi-labeled MPIQA database, MMP-2k.\nExperimental results showed that the state-of-the-art generic IQA metrics\nunderperform on MP images. The database and supplementary materials are\navailable at https://github.com/Future-IQA/MMP-2k."}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995", "abs": "https://arxiv.org/abs/2505.18995", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs."}
{"id": "2505.19076", "pdf": "https://arxiv.org/pdf/2505.19076", "abs": "https://arxiv.org/abs/2505.19076", "authors": ["Muye Huang", "Lingling Zhang", "Jie Ma", "Han Lai", "Fangzhi Xu", "Yifei Li", "Wenjun Wu", "Yaqiang Wu", "Jun Liu"], "title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "categories": ["cs.CV"], "comment": "23 pages, 9 figures", "summary": "Charts are high-density visualization carriers for complex data, serving as a\ncrucial medium for information extraction and analysis. Automated chart\nunderstanding poses significant challenges to existing multimodal large\nlanguage models (MLLMs) due to the need for precise and complex visual\nreasoning. Current step-by-step reasoning models primarily focus on text-based\nlogical reasoning for chart understanding. However, they struggle to refine or\ncorrect their reasoning when errors stem from flawed visual understanding, as\nthey lack the ability to leverage multimodal interaction for deeper\ncomprehension. Inspired by human cognitive behavior, we propose ChartSketcher,\na multimodal feedback-driven step-by-step reasoning method designed to address\nthese limitations. ChartSketcher is a chart understanding model that employs\nSketch-CoT, enabling MLLMs to annotate intermediate reasoning steps directly\nonto charts using a programmatic sketching library, iteratively feeding these\nvisual annotations back into the reasoning process. This mechanism enables the\nmodel to visually ground its reasoning and refine its understanding over\nmultiple steps. We employ a two-stage training strategy: a cold start phase to\nlearn sketch-based reasoning patterns, followed by off-policy reinforcement\nlearning to enhance reflection and generalization. Experiments demonstrate that\nChartSketcher achieves promising performance on chart understanding benchmarks\nand general vision tasks, providing an interactive and interpretable approach\nto chart comprehension."}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000", "abs": "https://arxiv.org/abs/2505.19000", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability."}
{"id": "2505.19081", "pdf": "https://arxiv.org/pdf/2505.19081", "abs": "https://arxiv.org/abs/2505.19081", "authors": ["Ruiyang Xia", "Dawei Zhou", "Decheng Liu", "Lin Yuan", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "Towards Generalized Proactive Defense against Face Swappingwith Contour-Hybrid Watermark", "categories": ["cs.CV"], "comment": "16 pages, 11 figures, under review", "summary": "Face swapping, recognized as a privacy and security concern, has prompted\nconsiderable defensive research. With the advancements in AI-generated content,\nthe discrepancies between the real and swapped faces have become nuanced.\nConsidering the difficulty of forged traces detection, we shift the focus to\nthe face swapping purpose and proactively embed elaborate watermarks against\nunknown face swapping techniques. Given that the constant purpose is to swap\nthe original face identity while preserving the background, we concentrate on\nthe regions surrounding the face to ensure robust watermark generation, while\nembedding the contour texture and face identity information to achieve\nprogressive image determination. The watermark is located in the facial contour\nand contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our\napproach generalizes face swapping detection without requiring any swapping\ntechniques during training and the storage of large-scale messages in advance.\nExperiments conducted across 8 face swapping techniques demonstrate the\nsuperiority of our approach compared with state-of-the-art passive and\nproactive detectors while achieving a favorable balance between the image\nquality and watermark robustness."}
{"id": "2505.19018", "pdf": "https://arxiv.org/pdf/2505.19018", "abs": "https://arxiv.org/abs/2505.19018", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "Md. Rajib Hossain", "Md. Saifur Rahman", "A. B. M. Shawkat Ali"], "title": "CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural\nlanguage processing, offering fine-grained insights into opinions expressed in\ntext. While existing research has largely focused on resource-rich languages\nlike English which leveraging large annotated datasets, pre-trained models, and\nlanguage-specific tools. These resources are often unavailable for low-resource\nlanguages such as Bengali. The ABSA task in Bengali remains poorly explored and\nis further complicated by its unique linguistic characteristics and a lack of\nannotated data, pre-trained models, and optimized hyperparameters. To address\nthese challenges, this research propose CrosGrpsABS, a novel hybrid framework\nthat leverages bidirectional cross-attention between syntactic and semantic\ngraphs to enhance aspect-level sentiment classification. The CrosGrpsABS\ncombines transformerbased contextual embeddings with graph convolutional\nnetworks, built upon rule-based syntactic dependency parsing and semantic\nsimilarity computations. By employing bidirectional crossattention, the model\neffectively fuses local syntactic structure with global semantic context,\nresulting in improved sentiment classification performance across both low- and\nhigh-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali\nABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The\nCrosGrpsABS consistently outperforms existing approaches, achieving notable\nimprovements, including a 0.93% F1-score increase for the Restaurant domain and\na 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark."}
{"id": "2505.19084", "pdf": "https://arxiv.org/pdf/2505.19084", "abs": "https://arxiv.org/abs/2505.19084", "authors": ["Yifeng Xu", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/VIPL-GENUN/Jodi", "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi."}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051", "abs": "https://arxiv.org/abs/2505.19051", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "title": "Efficient Data Selection at Scale via Influence Distillation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection."}
{"id": "2505.19089", "pdf": "https://arxiv.org/pdf/2505.19089", "abs": "https://arxiv.org/abs/2505.19089", "authors": ["Xuejie Liu", "Anji Liu", "Guy Van den Broeck", "Yitao Liang"], "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation", "categories": ["cs.CV"], "comment": null, "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056", "abs": "https://arxiv.org/abs/2505.19056", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance."}
{"id": "2505.19094", "pdf": "https://arxiv.org/pdf/2505.19094", "abs": "https://arxiv.org/abs/2505.19094", "authors": ["Chuming Shen", "Wei Wei", "Xiaoye Qu", "Yu Cheng"], "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text\ndomain through stable reinforcement learning (RL). Recently, in the multimodal\ndomain, works have begun to directly apply RL to generate R1-like free-form\nreasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks\nshare an intrinsically different nature from textual tasks, which heavily rely\non the understanding of the input image to solve the problem. Therefore, such\nfree-form reasoning faces two critical limitations in the VQA task: (1)\nExtended reasoning chains diffuse visual focus away from task-critical regions,\ndegrading answer accuracy. (2) Unverifiable intermediate steps amplify\npolicy-gradient variance and computational costs overhead. To address these\nissues, in this paper, we introduce SATORI ($\\textbf{S}patially$\n$\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with\n$\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three\nverifiable stages, including global image captioning, region localization, and\nanswer prediction, each supplying explicit reward signals. Furthermore, we also\nintroduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and\nbounding-boxes to facilitate training. Experiments demonstrate consistent\nperformance improvements across seven VQA benchmarks, achieving up to $15.7\\%$\nimprovement in accuracy in accuracy compared to the R1-like baseline. Our\nanalysis of the attention map confirms enhanced focus on critical regions,\nwhich brings improvements in accuracy. Our code is available at\nhttps://github.com/justairr/SATORI-R1."}
{"id": "2505.19060", "pdf": "https://arxiv.org/pdf/2505.19060", "abs": "https://arxiv.org/abs/2505.19060", "authors": ["Roman Vashurin", "Maiya Goloburda", "Preslav Nakov", "Maxim Panov"], "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools across various\napplications, making it more important than ever to ensure the quality and the\ntrustworthiness of their outputs. This has led to growing interest in\nuncertainty quantification (UQ) methods for assessing the reliability of LLM\noutputs. Many existing UQ techniques rely on token probabilities, which\ninadvertently introduces a bias with respect to the length of the output. While\nsome methods attempt to account for this, we demonstrate that such biases\npersist even in length-normalized approaches. To address the problem, here we\npropose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing\nprocedure that regresses uncertainty scores on output length and uses the\nresiduals as corrected, length-invariant estimates. Our method is post-hoc,\nmodel-agnostic, and applicable to a range of UQ measures. Through extensive\nevaluation on machine translation, summarization, and question-answering tasks,\nwe demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally\nlength-normalized UQ methods uncertainty estimates across multiple metrics and\nmodels."}
{"id": "2505.19110", "pdf": "https://arxiv.org/pdf/2505.19110", "abs": "https://arxiv.org/abs/2505.19110", "authors": ["Vishwa Mohan Singh", "Alberto Gaston Villagran Asiares", "Luisa Sophie Schuhmacher", "Kate Rendall", "Simon Weibrod", "David Rgamer", "Inga Krte"], "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at MIDL 2025", "summary": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the\nstructural connectivity of the brain, but presents challenges in effective\nrepresentation and interpretation in deep learning models. In this work, we\npropose a novel 2D representation of DTI tractography that encodes tract-level\nfractional anisotropy (FA) values into a 9x9 grayscale image. This\nrepresentation is processed through a Beta-Total Correlation Variational\nAutoencoder with a Spatial Broadcast Decoder to learn a disentangled and\ninterpretable latent embedding. We evaluate the quality of this embedding using\nsupervised and unsupervised representation learning strategies, including\nauxiliary classification, triplet loss, and SimCLR-based contrastive learning.\nCompared to the 1D Group deep neural network (DNN) baselines, our approach\nimproves the F1 score in a downstream sex classification task by 15.74% and\nshows a better disentanglement than the 3D representation."}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073", "abs": "https://arxiv.org/abs/2505.19073", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods."}
{"id": "2505.19111", "pdf": "https://arxiv.org/pdf/2505.19111", "abs": "https://arxiv.org/abs/2505.19111", "authors": ["Yaping He", "Jianfeng Cai", "Qicong Hu", "Peiqing Wang"], "title": "Remote Sensing Image Classification with Decoupled Knowledge Distillation", "categories": ["cs.CV"], "comment": "7", "summary": "To address the challenges posed by the large number of parameters in existing\nremote sensing image classification models, which hinder deployment on\nresource-constrained devices, this paper proposes a lightweight classification\nmethod based on knowledge distillation. Specifically, G-GhostNet is adopted as\nthe backbone network, leveraging feature reuse to reduce redundant parameters\nand significantly improve inference efficiency. In addition, a decoupled\nknowledge distillation strategy is employed, which separates target and\nnon-target classes to effectively enhance classification accuracy. Experimental\nresults on the RSOD and AID datasets demonstrate that, compared with the\nhigh-parameter VGG-16 model, the proposed method achieves nearly equivalent\nTop-1 accuracy while reducing the number of parameters by 6.24 times. This\napproach strikes an excellent balance between model size and classification\nperformance, offering an efficient solution for deployment on resource-limited\ndevices."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.19114", "pdf": "https://arxiv.org/pdf/2505.19114", "abs": "https://arxiv.org/abs/2505.19114", "authors": ["Hui Zhang", "Dexiang Hong", "Maoke Yang", "Yutao Chen", "Zhao Zhang", "Jie Shao", "Xinglong Wu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for Creative Graphic Design", "categories": ["cs.CV"], "comment": null, "summary": "Graphic design plays a vital role in visual communication across advertising,\nmarketing, and multimedia entertainment. Prior work has explored automated\ngraphic design generation using diffusion models, aiming to streamline creative\nworkflows and democratize design capabilities. However, complex graphic design\nscenarios require accurately adhering to design intent specified by multiple\nheterogeneous user-provided elements (\\eg images, layouts, and texts), which\npose multi-condition control challenges for existing methods. Specifically,\nprevious single-condition control models demonstrate effectiveness only within\ntheir specialized domains but fail to generalize to other conditions, while\nexisting multi-condition methods often lack fine-grained control over each\nsub-condition and compromise overall compositional harmony. To address these\nlimitations, we introduce CreatiDesign, a systematic solution for automated\ngraphic design covering both model architecture and dataset construction.\nFirst, we design a unified multi-condition driven architecture that enables\nflexible and precise integration of heterogeneous design elements with minimal\narchitectural modifications to the base diffusion model. Furthermore, to ensure\nthat each condition precisely controls its designated image region and to avoid\ninterference between conditions, we propose a multimodal attention mask\nmechanism. Additionally, we develop a fully automated pipeline for constructing\ngraphic design datasets, and introduce a new dataset with 400K samples\nfeaturing multi-condition annotations, along with a comprehensive benchmark.\nExperimental results show that CreatiDesign outperforms existing models by a\nclear margin in faithfully adhering to user intent."}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100", "abs": "https://arxiv.org/abs/2505.19100", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models."}
{"id": "2505.19120", "pdf": "https://arxiv.org/pdf/2505.19120", "abs": "https://arxiv.org/abs/2505.19120", "authors": ["Xiaoyang Liu", "Bolin Qiu", "Jiezhang Cao", "Zheng Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "Freqformer: Image-Demoiring Transformer via Efficient Frequency Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Image demoir\\'eing remains a challenging task due to the complex interplay\nbetween texture corruption and color distortions caused by moir\\'e patterns.\nExisting methods, especially those relying on direct image-to-image\nrestoration, often fail to disentangle these intertwined artifacts effectively.\nWhile wavelet-based frequency-aware approaches offer a promising direction,\ntheir potential remains underexplored. In this paper, we present Freqformer, a\nTransformer-based framework specifically designed for image demoir\\'eing\nthrough targeted frequency separation. Our method performs an effective\nfrequency decomposition that explicitly splits moir\\'e patterns into\nhigh-frequency spatially-localized textures and low-frequency scale-robust\ncolor distortions, which are then handled by a dual-branch architecture\ntailored to their distinct characteristics. We further propose a learnable\nFrequency Composition Transform (FCT) module to adaptively fuse the\nfrequency-specific outputs, enabling consistent and high-fidelity\nreconstruction. To better aggregate the spatial dependencies and the\ninter-channel complementary information, we introduce a Spatial-Aware Channel\nAttention (SA-CA) module that refines moir\\'e-sensitive regions without\nincurring high computational cost. Extensive experiments on various\ndemoir\\'eing benchmarks demonstrate that Freqformer achieves state-of-the-art\nperformance with a compact model size. The code is publicly available at\nhttps://github.com/xyLiu339/Freqformer."}
{"id": "2505.19103", "pdf": "https://arxiv.org/pdf/2505.19103", "abs": "https://arxiv.org/abs/2505.19103", "authors": ["Iddo Yosha", "Dorin Shteyman", "Yossi Adi"], "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress."}
{"id": "2505.19122", "pdf": "https://arxiv.org/pdf/2505.19122", "abs": "https://arxiv.org/abs/2505.19122", "authors": ["Eric Tillman Bill", "Cristian Perez Jensen", "Sotiris Anagnostidis", "Dimitri von Rtte"], "title": "Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Denoising diffusion models exhibit remarkable generative capabilities, but\nremain challenging to train due to their inherent stochasticity, where\nhigh-variance gradient estimates lead to slow convergence. Previous works have\nshown that magnitude preservation helps with stabilizing training in the U-net\narchitecture. This work explores whether this effect extends to the Diffusion\nTransformer (DiT) architecture. As such, we propose a magnitude-preserving\ndesign that stabilizes training without normalization layers. Motivated by the\ngoal of maintaining activation magnitudes, we additionally introduce rotation\nmodulation, which is a novel conditioning method using learned rotations\ninstead of traditional scaling or shifting. Through empirical evaluations and\nablation studies on small-scale models, we show that magnitude-preserving\nstrategies significantly improve performance, notably reducing FID scores by\n$\\sim$12.8%. Further, we show that rotation modulation combined with scaling is\ncompetitive with AdaLN, while requiring $\\sim$5.4% fewer parameters. This work\nprovides insights into conditioning strategies and magnitude control. We will\npublicly release the implementation of our method."}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108", "abs": "https://arxiv.org/abs/2505.19108", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios."}
{"id": "2505.19125", "pdf": "https://arxiv.org/pdf/2505.19125", "abs": "https://arxiv.org/abs/2505.19125", "authors": ["Yuqi Liu", "Qin Jin", "Tianyuan Qu", "Xuan Liu", "Yang Du", "Bei Yu", "Jiaya Jia"], "title": "RTime-QA: A Benchmark for Atomic Temporal Event Understanding in Large Multi-modal Models", "categories": ["cs.CV"], "comment": null, "summary": "Understanding accurate atomic temporal event is essential for video\ncomprehension. However, current video-language benchmarks often fall short to\nevaluate Large Multi-modal Models' (LMMs) temporal event understanding\ncapabilities, as they can be effectively addressed using image-language models.\nIn this paper, we introduce RTime-QA, a novel benchmark specifically designed\nto assess the atomic temporal event understanding ability of LMMs. RTime-QA\ncomprises 822 high-quality, carefully-curated video-text questions, each\nmeticulously annotated by human experts. Each question features a video\ndepicting an atomic temporal event, paired with both correct answers and\ntemporal negative descriptions, specifically designed to evaluate temporal\nunderstanding. To advance LMMs' temporal event understanding ability, we\nfurther introduce RTime-IT, a 14k instruction-tuning dataset that employs a\nsimilar annotation process as RTime-QA. Extensive experimental analysis\ndemonstrates that RTime-QA presents a significant challenge for LMMs: the\nstate-of-the-art model Qwen2-VL achieves only 34.6 on strict-ACC metric,\nsubstantially lagging behind human performance. Furthermore, our experiments\nreveal that RTime-IT effectively enhance LMMs' capacity in temporal\nunderstanding. By fine-tuning on RTime-IT, our Qwen2-VL achieves 65.9 on\nRTime-QA."}
{"id": "2505.19112", "pdf": "https://arxiv.org/pdf/2505.19112", "abs": "https://arxiv.org/abs/2505.19112", "authors": ["Zheng Chu", "Huiming Fan", "Jingchang Chen", "Qianyu Wang", "Mingda Yang", "Jiafeng Liang", "Zhongjie Wang", "Hao Li", "Guo Tang", "Ming Liu", "Bing Qin"], "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Although large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they still face challenges in knowledge-intensive multi-hop\nreasoning. Recent work explores iterative retrieval to address complex\nproblems. However, the lack of intermediate guidance often results in\ninaccurate retrieval and flawed intermediate reasoning, leading to incorrect\nreasoning. To address these, we propose Self-Critique Guided Iterative\nReasoning (SiGIR), which uses self-critique feedback to guide the iterative\nreasoning process. Specifically, through end-to-end training, we enable the\nmodel to iteratively address complex problems via question decomposition.\nAdditionally, the model is able to self-evaluate its intermediate reasoning\nsteps. During iterative reasoning, the model engages in branching exploration\nand employs self-evaluation to guide the selection of promising reasoning\ntrajectories. Extensive experiments on three multi-hop reasoning datasets\ndemonstrate the effectiveness of our proposed method, surpassing the previous\nSOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future\nresearch. Our code, data, and models are available at Github:\nhttps://github.com/zchuz/SiGIR-MHQA."}
{"id": "2505.19138", "pdf": "https://arxiv.org/pdf/2505.19138", "abs": "https://arxiv.org/abs/2505.19138", "authors": ["Myeongseok Nam", "Wongi Park", "Minsol Kim", "Hyejin Hur", "Soomok Lee"], "title": "Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal infrared Novel-view Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR)\nimaging has gained attention in novel-view synthesis, showing real-time\nrendering. However, novel-view synthesis with thermal infrared images suffers\nfrom transmission effects, emissivity, and low resolution, leading to floaters\nand blur effects in rendered images. To address these problems, we introduce\nVeta-GS, which leverages a view-dependent deformation field and a Thermal\nFeature Extractor (TFE) to precisely capture subtle thermal variations and\nmaintain robustness. Specifically, we design view-dependent deformation field\nthat leverages camera position and viewing direction, which capture thermal\nvariations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and\nMonoSSIM loss, which consider appearance, edge, and frequency to maintain\nrobustness. Extensive experiments on the TI-NSD benchmark show that our method\nachieves better performance over existing methods."}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116", "abs": "https://arxiv.org/abs/2505.19116", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "title": "Controlling Language Confusion in Multilingual LLMs", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon\nwhere responses are partially or entirely generated in unintended languages.\nThis can critically impact user experience in low-resource settings. We\nhypothesize that conventional supervised fine-tuning exacerbates this issue\nbecause the softmax objective focuses probability mass only on the single\ncorrect token but does not explicitly penalize cross-lingual mixing.\nInterestingly, by observing loss trajectories during the pretraining phase, we\nobserve that models fail to learn to distinguish between monolingual and\nlanguage-confused text. Additionally, we find that ORPO, which adds penalties\nfor unwanted output styles to standard SFT, effectively suppresses\nlanguage-confused generations even at high decoding temperatures without\ndegrading overall model performance. Our findings suggest that incorporating\nappropriate penalty terms can mitigate language confusion in low-resource\nsettings with limited data."}
{"id": "2505.19139", "pdf": "https://arxiv.org/pdf/2505.19139", "abs": "https://arxiv.org/abs/2505.19139", "authors": ["Feiran Liu", "Yuzhe Zhang", "Xinyi Huang", "Yinan Peng", "Xinfeng Li", "Lixu Wang", "Yutong Shen", "Ranjie Duan", "Simeng Qin", "Xiaojun Jia", "Qingsong Wen", "Wei Dong"], "title": "The Eye of Sherlock Holmes: Uncovering User Private Attribute Profiling via Vision-Language Model Agentic Framework", "categories": ["cs.CV"], "comment": null, "summary": "Our research reveals a new privacy risk associated with the vision-language\nmodel (VLM) agentic framework: the ability to infer sensitive attributes (e.g.,\nage and health information) and even abstract ones (e.g., personality and\nsocial traits) from a set of personal images, which we term \"image private\nattribute profiling.\" This threat is particularly severe given that modern apps\ncan easily access users' photo albums, and inference from image sets enables\nmodels to exploit inter-image relations for more sophisticated profiling.\nHowever, two main challenges hinder our understanding of how well VLMs can\nprofile an individual from a few personal photos: (1) the lack of benchmark\ndatasets with multi-image annotations for private attributes, and (2) the\nlimited ability of current multimodal large language models (MLLMs) to infer\nabstract attributes from large image collections. In this work, we construct\nPAPI, the largest dataset for studying private attribute profiling in personal\nimages, comprising 2,510 images from 251 individuals with 3,012 annotated\nprivacy attributes. We also propose HolmesEye, a hybrid agentic framework that\ncombines VLMs and LLMs to enhance privacy inference. HolmesEye uses VLMs to\nextract both intra-image and inter-image information and LLMs to guide the\ninference process as well as consolidate the results through forensic analysis,\novercoming existing limitations in long-context visual reasoning. Experiments\nreveal that HolmesEye achieves a 10.8% improvement in average accuracy over\nstate-of-the-art baselines and surpasses human-level performance by 15.0% in\npredicting abstract attributes. This work highlights the urgency of addressing\nprivacy risks in image-based profiling and offers both a new dataset and an\nadvanced framework to guide future research in this area."}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121", "abs": "https://arxiv.org/abs/2505.19121", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models."}
{"id": "2505.19148", "pdf": "https://arxiv.org/pdf/2505.19148", "abs": "https://arxiv.org/abs/2505.19148", "authors": ["Shengdong Han", "Shangdong Yang", "Xin Zhang", "Yuxuan Li", "Xiang Li", "Jian Yang", "Ming-Ming Cheng", "Yimian Dai"], "title": "DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing", "categories": ["cs.CV"], "comment": null, "summary": "Resolving closely-spaced small targets in dense clusters presents a\nsignificant challenge in infrared imaging, as the overlapping signals hinder\nprecise determination of their quantity, sub-pixel positions, and radiation\nintensities. While deep learning has advanced the field of infrared small\ntarget detection, its application to closely-spaced infrared small targets has\nnot yet been explored. This gap exists primarily due to the complexity of\nseparating superimposed characteristics and the lack of an open-source\ninfrastructure. In this work, we propose the Dynamic Iterative Shrinkage\nThresholding Network (DISTA-Net), which reconceptualizes traditional sparse\nreconstruction within a dynamic framework. DISTA-Net adaptively generates\nconvolution weights and thresholding parameters to tailor the reconstruction\nprocess in real time. To the best of our knowledge, DISTA-Net is the first deep\nlearning model designed specifically for the unmixing of closely-spaced\ninfrared small targets, achieving superior sub-pixel detection accuracy.\nMoreover, we have established the first open-source ecosystem to foster further\nresearch in this field. This ecosystem comprises three key components: (1)\nCSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom\nevaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source\ntoolkit featuring DISTA-Net and other models. Our code and dataset are\navailable at https://github.com/GrokCV/GrokCSO."}
{"id": "2505.19126", "pdf": "https://arxiv.org/pdf/2505.19126", "abs": "https://arxiv.org/abs/2505.19126", "authors": ["Wenyang Luo", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-Rong Wen"], "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has\nsignificantly advanced complex reasoning tasks. However, their capabilities in\nmultilingual complex reasoning remain underexplored, with existing efforts\nlargely focused on simpler tasks like MGSM. To address this gap, we introduce\nMMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality\nmath problems across 10 typologically diverse languages. Using MMATH, we\nobserve that even advanced models like DeepSeek R1 exhibit substantial\nperformance disparities across languages and suffer from a critical off-target\nissue-generating responses in unintended languages. To address this, we explore\nstrategies including prompting and training, demonstrating that reasoning in\nEnglish and answering in target languages can simultaneously enhance\nperformance and preserve target-language consistency. Our findings offer new\ninsights and practical strategies for advancing the multilingual reasoning\ncapabilities of large language models. Our code and data could be found at\nhttps://github.com/RUCAIBox/MMATH."}
{"id": "2505.19149", "pdf": "https://arxiv.org/pdf/2505.19149", "abs": "https://arxiv.org/abs/2505.19149", "authors": ["Shuyu Wang", "Weiqi Li", "Qian Wang", "Shijie Zhao", "Jian Zhang"], "title": "MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nimage editing techniques, driving increasing demand for diverse and\nfine-grained edits. Despite these advances, existing image editing methods\nstill face challenges in achieving high precision and semantic accuracy in\ncomplex scenarios. Recent studies address this issue by incorporating\nmultimodal large language models (MLLMs) into image editing pipelines. However,\ncurrent MLLM-based methods mainly rely on interpreting textual instructions,\nleaving the intrinsic visual understanding of large models largely unexplored,\nthus resulting in insufficient alignment between textual semantics and visual\noutcomes. To overcome these limitations, we propose MIND-Edit, an end-to-end\nimage-editing framework integrating pretrained diffusion model with MLLM.\nMIND-Edit introduces two complementary strategies: (1) a text instruction\noptimization strategy that clarifies ambiguous user instructions based on\nsemantic reasoning from the MLLM, and (2) an MLLM insight-driven editing\nstrategy that explicitly leverages the intrinsic visual understanding\ncapability of the MLLM to infer editing intent and guide the diffusion process\nvia generated visual embeddings. Furthermore, we propose a joint training\napproach to effectively integrate both strategies, allowing them to reinforce\neach other for more accurate instruction interpretation and visually coherent\nedits aligned with user intent. Extensive experiments demonstrate that\nMIND-Edit outperforms state-of-the-art image editing methods in both\nquantitative metrics and visual quality, particularly under complex and\nchallenging scenarios."}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128", "abs": "https://arxiv.org/abs/2505.19128", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent."}
{"id": "2505.19154", "pdf": "https://arxiv.org/pdf/2505.19154", "abs": "https://arxiv.org/abs/2505.19154", "authors": ["Q. G. Duan", "Benyun Zhao", "Mingqiao Han Yijun Huang", "Ben M. Chen"], "title": "FHGS: Feature-Homogenized Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently\nachieved notable advances. Although 3DGS related methods have efficient\nrendering capabilities, they fail to address the inherent contradiction between\nthe anisotropic color representation of gaussian primitives and the isotropic\nrequirements of semantic features, leading to insufficient cross-view feature\nconsistency. To overcome the limitation, we proposes $\\textit{FHGS}$\n(Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework\ninspired by physical models, which can achieve high-precision mapping of\narbitrary 2D features from pre-trained models to 3D scenes while preserving the\nreal-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$\nintroduces the following innovations: Firstly, a universal feature fusion\narchitecture is proposed, enabling robust embedding of large-scale pre-trained\nmodels' semantic features (e.g., SAM, CLIP) into sparse 3D structures.\nSecondly, a non-differentiable feature fusion mechanism is introduced, which\nenables semantic features to exhibit viewpoint independent isotropic\ndistributions. This fundamentally balances the anisotropic rendering of\ngaussian primitives and the isotropic expression of features; Thirdly, a\ndual-driven optimization strategy inspired by electric potential fields is\nproposed, which combines external supervision from semantic feature fields with\ninternal primitive clustering guidance. This mechanism enables synergistic\noptimization of global semantic alignment and local structural consistency.\nMore interactive results can be accessed on: https://fhgs.cuastro.org/."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.19155", "pdf": "https://arxiv.org/pdf/2505.19155", "abs": "https://arxiv.org/abs/2505.19155", "authors": ["Xuan Zhang", "Cunxiao Du", "Sicheng Yu", "Jiawei Wu", "Fengzhuo Zhang", "Wei Gao", "Qian Liu"], "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Due to the auto-regressive nature of current video large language models\n(Video-LLMs), the inference latency increases as the input sequence length\ngrows, posing challenges for the efficient processing of video sequences that\nare usually very long. We observe that during decoding, the attention scores of\nmost tokens in Video-LLMs tend to be sparse and concentrated, with only certain\ntokens requiring comprehensive full attention. Based on this insight, we\nintroduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two\ndistinct modules: one leveraging sparse top-K attention and the other employing\ndense full attention. These modules collaborate to accelerate Video-LLMs\nwithout loss. The fast (sparse) model speculatively decodes multiple tokens,\nwhile the slow (dense) model verifies them in parallel. StD is a tuning-free,\nplug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in\nvideo processing. It maintains model performance while enabling a seamless\ntransition from a standard Video-LLM to a sparse Video-LLM with minimal code\nmodifications."}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163", "abs": "https://arxiv.org/abs/2505.19163", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community."}
{"id": "2505.19159", "pdf": "https://arxiv.org/pdf/2505.19159", "abs": "https://arxiv.org/abs/2505.19159", "authors": ["Yuze Wang", "Mariana Belgiu", "Haiyang Wu", "Dandan Zhong", "Yangyang Cao", "Chao Tao"], "title": "A Joint Learning Framework with Feature Reconstruction and Prediction for Incomplete Satellite Image Time Series in Agricultural Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Satellite Image Time Series (SITS) is crucial for agricultural semantic\nsegmentation. However, Cloud contamination introduces time gaps in SITS,\ndisrupting temporal dependencies and causing feature shifts, leading to\ndegraded performance of models trained on complete SITS. Existing methods\ntypically address this by reconstructing the entire SITS before prediction or\nusing data augmentation to simulate missing data. Yet, full reconstruction may\nintroduce noise and redundancy, while the data-augmented model can only handle\nlimited missing patterns, leading to poor generalization. We propose a joint\nlearning framework with feature reconstruction and prediction to address\nincomplete SITS more effectively. During training, we simulate data-missing\nscenarios using temporal masks. The two tasks are guided by both ground-truth\nlabels and the teacher model trained on complete SITS. The prediction task\nconstrains the model from selectively reconstructing critical features from\nmasked inputs that align with the teacher's temporal feature representations.\nIt reduces unnecessary reconstruction and limits noise propagation. By\nintegrating reconstructed features into the prediction task, the model avoids\nlearning shortcuts and maintains its ability to handle varied missing patterns\nand complete SITS. Experiments on SITS from Hunan Province, Western France, and\nCatalonia show that our method improves mean F1-scores by 6.93% in cropland\nextraction and 7.09% in crop classification over baselines. It also generalizes\nwell across satellite sensors, including Sentinel-2 and PlanetScope, under\nvarying temporal missing rates and model backbones."}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176", "abs": "https://arxiv.org/abs/2505.19176", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge."}
{"id": "2505.19161", "pdf": "https://arxiv.org/pdf/2505.19161", "abs": "https://arxiv.org/abs/2505.19161", "authors": ["Jialun Pei", "Diandian Guo", "Donghui Yang", "Zhixi Li", "Yuxin Feng", "Long Ma", "Bo Du", "Pheng-Ann Heng"], "title": "Benchmarking Laparoscopic Surgical Image Restoration and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "In laparoscopic surgery, a clear and high-quality visual field is critical\nfor surgeons to make accurate intraoperative decisions. However, persistent\nvisual degradation, including smoke generated by energy devices, lens fogging\nfrom thermal gradients, and lens contamination due to blood or tissue fluid\nsplashes during surgical procedures, severely impair visual clarity. These\ndegenerations can seriously hinder surgical workflow and pose risks to patient\nsafety. To systematically investigate and address various forms of surgical\nscene degradation, we introduce a real-world open-source surgical image\nrestoration dataset covering laparoscopic environments, called SurgClean, which\ninvolves multi-type image restoration tasks, e.g., desmoking, defogging, and\ndesplashing. SurgClean comprises 1,020 images with diverse degradation types\nand corresponding paired reference labels. Based on SurgClean, we establish a\nstandardized evaluation benchmark and provide performance for 22 representative\ngeneric task-specific image restoration approaches, including 12 generic and 10\ntask-specific image restoration approaches. Experimental results reveal\nsubstantial performance gaps relative to clinical requirements, highlighting a\ncritical opportunity for algorithm advancements in intelligent surgical\nrestoration. Furthermore, we explore the degradation discrepancies between\nsurgical and natural scenes from structural perception and semantic\nunderstanding perspectives, providing fundamental insights for domain-specific\nimage restoration research. Our work aims to empower the capabilities of\nrestoration algorithms to increase surgical environments and improve the\nefficiency of clinical procedures."}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184", "abs": "https://arxiv.org/abs/2505.19184", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "title": "Two LLMs debate, both are certain they've won", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings."}
{"id": "2505.19166", "pdf": "https://arxiv.org/pdf/2505.19166", "abs": "https://arxiv.org/abs/2505.19166", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required.\n  JEDI is model-agnostic and applicable to architectures such as Stable\nDiffusion 1.5 and 3.5, consistently improving prompt alignment and\ndisentanglement in complex scenes. Additionally, JEDI provides a lightweight,\nCLIP-free disentanglement score derived from internal attention distributions,\noffering a principled benchmark for compositional alignment under test-time\nconditions. We will publicly release the implementation of our method."}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187", "abs": "https://arxiv.org/abs/2505.19187", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints."}
{"id": "2505.19169", "pdf": "https://arxiv.org/pdf/2505.19169", "abs": "https://arxiv.org/abs/2505.19169", "authors": ["Ryosei Hara", "Wataru Ikeda", "Masashi Hatano", "Mariko Isogawa"], "title": "EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction", "categories": ["cs.CV"], "comment": "IEEE International Conference on Image Processing 2025", "summary": "Reconstructing 3D hand mesh is challenging but an important task for\nhuman-computer interaction and AR/VR applications. In particular, RGB and/or\ndepth cameras have been widely used in this task. However, methods using these\nconventional cameras face challenges in low-light environments and during\nmotion blur. Thus, to address these limitations, event cameras have been\nattracting attention in recent years for their high dynamic range and high\ntemporal resolution. Despite their advantages, event cameras are sensitive to\nbackground noise or camera motion, which has limited existing studies to static\nbackgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel\nmethod for event-based 3D hand mesh reconstruction in an egocentric view. Our\napproach introduces a Hand Segmentation Module that extracts hand regions,\neffectively mitigating the influence of dynamic background events. We evaluated\nour approach and demonstrated its effectiveness on the N-HOT3D dataset,\nimproving MPJPE by approximately more than 4.5 cm (43%)."}
{"id": "2505.19191", "pdf": "https://arxiv.org/pdf/2505.19191", "abs": "https://arxiv.org/abs/2505.19191", "authors": ["Nursulu Sagimbayeva", "Ruveyda Betl Baheci", "Ingmar Weber"], "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection", "categories": ["cs.CL"], "comment": "8 pages, 6 figures. Accepted for publication in the Proceedings of\n  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at\n  ICWSM-2025", "summary": "Inconsistent political statements represent a form of misinformation. They\nerode public trust and pose challenges to accountability, when left unnoticed.\nDetecting inconsistencies automatically could support journalists in asking\nclarification questions, thereby helping to keep politicians accountable. We\npropose the Inconsistency detection task and develop a scale of inconsistency\ntypes to prompt NLP-research in this direction. To provide a resource for\ndetecting inconsistencies in a political domain, we present a dataset of 698\nhuman-annotated pairs of political statements with explanations of the\nannotators' reasoning for 237 samples. The statements mainly come from voting\nassistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,\nreflecting real-world political issues. We benchmark Large Language Models\n(LLMs) on our dataset and show that in general, they are as good as humans at\ndetecting inconsistencies, and might be even better than individual humans at\npredicting the crowd-annotated ground-truth. However, when it comes to\nidentifying fine-grained inconsistency types, none of the model have reached\nthe upper bound of performance (due to natural labeling variation), thus\nleaving room for improvement. We make our dataset and code publicly available."}
{"id": "2505.19175", "pdf": "https://arxiv.org/pdf/2505.19175", "abs": "https://arxiv.org/abs/2505.19175", "authors": ["Jan Held", "Renaud Vandeghen", "Adrien Deliege", "Abdullah Hamdi", "Silvio Giancola", "Anthony Cioppa", "Andrea Vedaldi", "Bernard Ghanem", "Andrea Tagliasacchi", "Marc Van Droogenbroeck"], "title": "Triangle Splatting for Real-Time Radiance Field Rendering", "categories": ["cs.CV"], "comment": "18 pages, 13 figures, 10 tables", "summary": "The field of computer graphics was revolutionized by models such as Neural\nRadiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant\nrepresentation for photogrammetry. In this paper, we argue for a triangle\ncomeback. We develop a differentiable renderer that directly optimizes\ntriangles via end-to-end gradients. We achieve this by rendering each triangle\nas differentiable splats, combining the efficiency of triangles with the\nadaptive density of representations based on independent primitives. Compared\nto popular 2D and 3D Gaussian Splatting methods, our approach achieves higher\nvisual fidelity, faster convergence, and increased rendering throughput. On the\nMip-NeRF360 dataset, our method outperforms concurrent non-volumetric\nprimitives in visual fidelity and achieves higher perceptual quality than the\nstate-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible\nwith standard graphics stacks and GPU hardware, and highly efficient: for the\n\\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using\nan off-the-shelf mesh renderer. These results highlight the efficiency and\neffectiveness of triangle-based representations for high-quality novel view\nsynthesis. Triangles bring us closer to mesh-based optimization by combining\nclassical computer graphics with modern differentiable rendering frameworks.\nThe project page is https://trianglesplatting.github.io/"}
{"id": "2505.19201", "pdf": "https://arxiv.org/pdf/2505.19201", "abs": "https://arxiv.org/abs/2505.19201", "authors": ["Yunhai Hu", "Tianhua Xia", "Zining Liu", "Rahul Raman", "Xingyu Liu", "Bo Bao", "Eric Sather", "Vithursan Thangarasa", "Sai Qian Zhang"], "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD) has emerged as a powerful method for accelerating\nautoregressive generation in large language models (LLMs), yet its integration\ninto vision-language models (VLMs) remains underexplored. We introduce DREAM, a\nnovel speculative decoding framework tailored for VLMs that combines three key\ninnovations: (1) a cross-attention-based mechanism to inject intermediate\nfeatures from the target model into the draft model for improved alignment, (2)\nadaptive intermediate feature selection based on attention entropy to guide\nefficient draft model training, and (3) visual token compression to reduce\ndraft model latency. DREAM enables efficient, accurate, and parallel multimodal\ndecoding with significant throughput improvement. Experiments across a diverse\nset of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,\ndemonstrate up to 3.6x speedup over conventional decoding and significantly\noutperform prior SD baselines in both inference throughput and speculative\ndraft acceptance length across a broad range of multimodal benchmarks. The code\nis publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git"}
{"id": "2505.19178", "pdf": "https://arxiv.org/pdf/2505.19178", "abs": "https://arxiv.org/abs/2505.19178", "authors": ["Akhila Yaragoppa", "Siddharth"], "title": "Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Understanding the emotional impact of videos is crucial for applications in\ncontent creation, advertising, and Human-Computer Interaction (HCI).\nTraditional affective computing methods rely on self-reported emotions, facial\nexpression analysis, and biosensing data, yet they often overlook the role of\nvisual saliency -- the naturally attention-grabbing regions within a video. In\nthis study, we utilize deep learning to introduce a novel saliency-based\napproach to emotion prediction by extracting two key features: saliency area\nand number of salient regions. Using the HD2S saliency model and OpenFace\nfacial action unit analysis, we examine the relationship between video saliency\nand viewer emotions. Our findings reveal three key insights: (1) Videos with\nmultiple salient regions tend to elicit high-valence, low-arousal emotions, (2)\nVideos with a single dominant salient region are more likely to induce\nlow-valence, high-arousal responses, and (3) Self-reported emotions often\nmisalign with facial expression-based emotion detection, suggesting limitations\nin subjective reporting. By leveraging saliency-driven insights, this work\nprovides a computationally efficient and interpretable alternative for emotion\nmodeling, with implications for content creation, personalized media\nexperiences, and affective computing research."}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206", "abs": "https://arxiv.org/abs/2505.19206", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems."}
{"id": "2505.19186", "pdf": "https://arxiv.org/pdf/2505.19186", "abs": "https://arxiv.org/abs/2505.19186", "authors": ["Rushiraj Gadhvi", "Priyansh Desai", "Siddharth"], "title": "PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Automated pose correction remains a significant challenge in AI-driven\nfitness systems, despite extensive research in activity recognition. This work\npresents PosePilot, a novel system that integrates pose recognition with\nreal-time personalized corrective feedback, overcoming the limitations of\ntraditional fitness solutions. Using Yoga, a discipline requiring precise\nspatio-temporal alignment as a case study, we demonstrate PosePilot's ability\nto analyze complex physical movements. Designed for deployment on edge devices,\nPosePilot can be extended to various at-home and outdoor exercises. We employ a\nVanilla LSTM, allowing the system to capture temporal dependencies for pose\nrecognition. Additionally, a BiLSTM with multi-head Attention enhances the\nmodel's ability to process motion contexts, selectively focusing on key limb\nangles for accurate error detection while maintaining computational efficiency.\nAs part of this work, we introduce a high-quality video dataset used for\nevaluating our models. Most importantly, PosePilot provides instant corrective\nfeedback at every stage of a movement, ensuring precise posture adjustments\nthroughout the exercise routine. The proposed approach 1) performs automatic\nhuman posture recognition, 2) provides personalized posture correction feedback\nat each instant which is crucial in Yoga, and 3) offers a lightweight and\nrobust posture correction model feasible for deploying on edge devices in\nreal-world environments."}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209", "abs": "https://arxiv.org/abs/2505.19209", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines."}
{"id": "2505.19196", "pdf": "https://arxiv.org/pdf/2505.19196", "abs": "https://arxiv.org/abs/2505.19196", "authors": ["Xinyao Liao", "Wei Wei", "Xiaoye Qu", "Yu Cheng"], "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image (T2I) diffusion model fine-tuning leverage\nreinforcement learning (RL) to align generated images with learnable reward\nfunctions. The existing approaches reformulate denoising as a Markov decision\nprocess for RL-driven optimization. However, they suffer from reward sparsity,\nreceiving only a single delayed reward per generated trajectory. This flaw\nhinders precise step-level attribution of denoising actions, undermines\ntraining efficiency. To address this, we propose a simple yet effective credit\nassignment framework that dynamically distributes dense rewards across\ndenoising steps. Specifically, we track changes in cosine similarity between\nintermediate and final images to quantify each step's contribution on\nprogressively reducing the distance to the final image. Our approach avoids\nadditional auxiliary neural networks for step-level preference modeling and\ninstead uses reward shaping to highlight denoising phases that have a greater\nimpact on image quality. Our method achieves 1.25 to 2 times higher sample\nefficiency and better generalization across four human preference reward\nfunctions, without compromising the original optimal policy."}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212", "abs": "https://arxiv.org/abs/2505.19212", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Schlkopf", "Zhijing Jin"], "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim."}
{"id": "2505.19208", "pdf": "https://arxiv.org/pdf/2505.19208", "abs": "https://arxiv.org/abs/2505.19208", "authors": ["Tyler Ward", "Aaron Moseley", "Abdullah-Al-Zubaer Imran"], "title": "Domain and Task-Focused Example Selection for Data-Efficient Contrastive Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation is one of the most important tasks in the medical imaging\npipeline as it influences a number of image-based decisions. To be effective,\nfully supervised segmentation approaches require large amounts of manually\nannotated training data. However, the pixel-level annotation process is\nexpensive, time-consuming, and error-prone, hindering progress and making it\nchallenging to perform effective segmentations. Therefore, models must learn\nefficiently from limited labeled data. Self-supervised learning (SSL),\nparticularly contrastive learning via pre-training on unlabeled data and\nfine-tuning on limited annotations, can facilitate such limited labeled image\nsegmentation. To this end, we propose a novel self-supervised contrastive\nlearning framework for medical image segmentation, leveraging inherent\nrelationships of different images, dubbed PolyCL. Without requiring any\npixel-level annotations or unreasonable data augmentations, our PolyCL learns\nand transfers context-aware discriminant features useful for segmentation from\nan innovative surrogate, in a task-related manner. Additionally, we integrate\nthe Segment Anything Model (SAM) into our framework in two novel ways: as a\npost-processing refinement module that improves the accuracy of predicted masks\nusing bounding box prompts derived from coarse outputs, and as a propagation\nmechanism via SAM 2 that generates volumetric segmentations from a single\nannotated 2D slice. Experimental evaluations on three public computed\ntomography (CT) datasets demonstrate that PolyCL outperforms fully-supervised\nand self-supervised baselines in both low-data and cross-domain scenarios. Our\ncode is available at https://github.com/tbwa233/PolyCL."}
{"id": "2505.19217", "pdf": "https://arxiv.org/pdf/2505.19217", "abs": "https://arxiv.org/abs/2505.19217", "authors": ["Weize Chen", "Jiarui Yuan", "Tailin Jin", "Ning Ding", "Huimin Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training", "categories": ["cs.CL"], "comment": "under review", "summary": "Recent large language models (LLMs) exhibit impressive reasoning but often\nover-think, generating excessively long responses that hinder efficiency. We\nintroduce DIET ( DIfficulty-AwarE Training), a framework that systematically\ncuts these \"token calories\" by integrating on-the-fly problem difficulty into\nthe reinforcement learning (RL) process. DIET dynamically adapts token\ncompression strategies by modulating token penalty strength and conditioning\ntarget lengths on estimated task difficulty, to optimize the\nperformance-efficiency trade-off. We also theoretically analyze the pitfalls of\nnaive reward weighting in group-normalized RL algorithms like GRPO, and propose\nAdvantage Weighting technique, which enables stable and effective\nimplementation of these difficulty-aware objectives. Experimental results\ndemonstrate that DIET significantly reduces token counts while simultaneously\nimproving reasoning performance. Beyond raw token reduction, we show two\ncrucial benefits largely overlooked by prior work: (1) DIET leads to superior\ninference scaling. By maintaining high per-sample quality with fewer tokens, it\nenables better scaling performance via majority voting with more samples under\nfixed computational budgets, an area where other methods falter. (2) DIET\nenhances the natural positive correlation between response length and problem\ndifficulty, ensuring verbosity is appropriately allocated, unlike many existing\ncompression methods that disrupt this relationship. Our analyses provide a\nprincipled and effective framework for developing more efficient, practical,\nand high-performing LLMs."}
{"id": "2505.19210", "pdf": "https://arxiv.org/pdf/2505.19210", "abs": "https://arxiv.org/abs/2505.19210", "authors": ["Xiang Li", "Rongrong Wang", "Qing Qu"], "title": "Towards Understanding the Mechanisms of Classifier-Free Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-free guidance (CFG) is a core technique powering state-of-the-art\nimage generation systems, yet its underlying mechanisms remain poorly\nunderstood. In this work, we begin by analyzing CFG in a simplified linear\ndiffusion model, where we show its behavior closely resembles that observed in\nthe nonlinear case. Our analysis reveals that linear CFG improves generation\nquality via three distinct components: (i) a mean-shift term that approximately\nsteers samples in the direction of class means, (ii) a positive Contrastive\nPrincipal Components (CPC) term that amplifies class-specific features, and\n(iii) a negative CPC term that suppresses generic features prevalent in\nunconditional data. We then verify that these insights in real-world, nonlinear\ndiffusion models: over a broad range of noise levels, linear CFG resembles the\nbehavior of its nonlinear counterpart. Although the two eventually diverge at\nlow noise levels, we discuss how the insights from the linear analysis still\nshed light on the CFG's mechanism in the nonlinear regime."}
{"id": "2505.19236", "pdf": "https://arxiv.org/pdf/2505.19236", "abs": "https://arxiv.org/abs/2505.19236", "authors": ["Qian Cao", "Xiting Wang", "Yuzhuo Yuan", "Yahui Liu", "Fang Luo", "Ruihua Song"], "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator", "categories": ["cs.CL"], "comment": null, "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research."}
{"id": "2505.19218", "pdf": "https://arxiv.org/pdf/2505.19218", "abs": "https://arxiv.org/abs/2505.19218", "authors": ["Jingwei Wu", "Zhewei Huang", "Chang Liu"], "title": "Advancing Video Self-Supervised Learning via Image Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "In the past decade, image foundation models (IFMs) have achieved\nunprecedented progress. However, the potential of directly using IFMs for video\nself-supervised representation learning has largely been overlooked. In this\nstudy, we propose an advancing video self-supervised learning (AdViSe)\napproach, aimed at significantly reducing the training overhead of video\nrepresentation models using pre-trained IFMs. Specifically, we first introduce\ntemporal modeling modules (ResNet3D) to IFMs, constructing a video\nrepresentation model. We then employ a video self-supervised learning approach,\nplayback rate perception, to train temporal modules while freezing the IFM\ncomponents. Experiments on UCF101 demonstrate that AdViSe achieves performance\ncomparable to state-of-the-art methods while reducing training time by\n$3.4\\times$ and GPU memory usage by $8.2\\times$. This study offers fresh\ninsights into low-cost video self-supervised learning based on pre-trained\nIFMs. Code is available at https://github.com/JingwWu/advise-video-ssl."}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Ptz", "Benjamin Paaen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research."}
{"id": "2505.19233", "pdf": "https://arxiv.org/pdf/2505.19233", "abs": "https://arxiv.org/abs/2505.19233", "authors": ["Aniruddha Mukherjee", "Spriha Dubey", "Somdyuti Paul"], "title": "RAISE: Realness Assessment for Image Synthesis and Evaluation", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nphotorealistic visual content, offering practical substitutes for real images\nand videos in scenarios where acquiring real data is difficult or expensive.\nHowever, reliably substituting real visual content with AI-generated\ncounterparts requires robust assessment of the perceived realness of\nAI-generated visual content, a challenging task due to its inherent subjective\nnature. To address this, we conducted a comprehensive human study evaluating\nthe perceptual realness of both real and AI-generated images, resulting in a\nnew dataset, containing images paired with subjective realness scores,\nintroduced as RAISE in this paper. Further, we develop and train multiple\nmodels on RAISE to establish baselines for realness prediction. Our\nexperimental results demonstrate that features derived from deep foundation\nvision models can effectively capture the subjective realness. RAISE thus\nprovides a valuable resource for developing robust, objective models of\nperceptual realness assessment."}
{"id": "2505.19250", "pdf": "https://arxiv.org/pdf/2505.19250", "abs": "https://arxiv.org/abs/2505.19250", "authors": ["Yi Wang", "Junxiao Liu", "Shimao Zhang", "Jiajun Chen", "Shujian Huang"], "title": "PATS: Process-Level Adaptive Thinking Mode Switching", "categories": ["cs.CL"], "comment": null, "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs."}
{"id": "2505.19239", "pdf": "https://arxiv.org/pdf/2505.19239", "abs": "https://arxiv.org/abs/2505.19239", "authors": ["Chen Shi", "Shaoshuai Shi", "Kehua Sheng", "Bo Zhang", "Li Jiang"], "title": "DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Data-driven learning has advanced autonomous driving, yet task-specific\nmodels struggle with out-of-distribution scenarios due to their narrow\noptimization objectives and reliance on costly annotated data. We present\nDriveX, a self-supervised world model that learns generalizable scene dynamics\nand holistic representations (geometric, semantic, and motion) from large-scale\ndriving videos. DriveX introduces Omni Scene Modeling (OSM), a module that\nunifies multimodal supervision-3D point cloud forecasting, 2D semantic\nrepresentation, and image generation-to capture comprehensive scene evolution.\nTo simplify learning complex dynamics, we propose a decoupled latent world\nmodeling strategy that separates world representation learning from future\nstate decoding, augmented by dynamic-aware ray sampling to enhance motion\nmodeling. For downstream adaptation, we design Future Spatial Attention (FSA),\na unified paradigm that dynamically aggregates spatiotemporal features from\nDriveX's predictions to enhance task-specific inference. Extensive experiments\ndemonstrate DriveX's effectiveness: it achieves significant improvements in 3D\nfuture point cloud prediction over prior work, while attaining state-of-the-art\nresults on diverse tasks including occupancy prediction, flow estimation, and\nend-to-end driving. These results validate DriveX's capability as a\ngeneral-purpose world model, paving the way for robust and unified autonomous\ndriving frameworks."}
{"id": "2505.19254", "pdf": "https://arxiv.org/pdf/2505.19254", "abs": "https://arxiv.org/abs/2505.19254", "authors": ["Rafa Powiata", "Marcin Micha Miroczuk", "Sawomir Dadas", "Magorzata Grbowiec", "Micha Perekiewicz"], "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Industry Track", "summary": "Consumers often face inconsistent product quality, particularly when\nidentical products vary between markets, a situation known as the dual quality\nproblem. To identify and address this issue, automated techniques are needed.\nThis paper explores how natural language processing (NLP) can aid in detecting\nsuch discrepancies and presents the full process of developing a solution.\nFirst, we describe in detail the creation of a new Polish-language dataset with\n1,957 reviews, 540 highlighting dual quality issues. We then discuss\nexperiments with various approaches like SetFit with sentence-transformers,\ntransformer-based encoders, and LLMs, including error analysis and robustness\nverification. Additionally, we evaluate multilingual transfer using a subset of\nopinions in English, French, and German. The paper concludes with insights on\ndeployment and practical applications."}
{"id": "2505.19242", "pdf": "https://arxiv.org/pdf/2505.19242", "abs": "https://arxiv.org/abs/2505.19242", "authors": ["Alaa Dalaq", "Muzammil Behzad"], "title": "Deformable Attentive Visual Enhancement for Referring Segmentation Using Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a fundamental task in computer vision, aimed at\npartitioning an image into semantically meaningful regions. Referring image\nsegmentation extends this task by using natural language expressions to\nlocalize specific objects, requiring effective integration of visual and\nlinguistic information. In this work, we propose SegVLM, a vision-language\nmodel that incorporates architectural improvements to enhance segmentation\naccuracy and cross-modal alignment. The model integrates squeeze-and-excitation\n(SE) blocks for dynamic feature recalibration, deformable convolutions for\ngeometric adaptability, and residual connections for deep feature learning. We\nalso introduce a novel referring-aware fusion (RAF) loss that balances\nregion-level alignment, boundary precision, and class imbalance. Extensive\nexperiments and ablation studies demonstrate that each component contributes to\nconsistent performance improvements. SegVLM also shows strong generalization\nacross diverse datasets and referring expression scenarios."}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286", "abs": "https://arxiv.org/abs/2505.19286", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance."}
{"id": "2505.19256", "pdf": "https://arxiv.org/pdf/2505.19256", "abs": "https://arxiv.org/abs/2505.19256", "authors": ["Vivek Gopalakrishnan", "Neel Dey", "Polina Golland"], "title": "PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Determining the 3D pose of a patient from a limited set of 2D X-ray images is\na critical task in interventional settings. While preoperative volumetric\nimaging (e.g., CT and MRI) provides precise 3D localization and visualization\nof anatomical targets, these modalities cannot be acquired during procedures,\nwhere fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance\ninto intraoperative procedures, we present PolyPose, a simple and robust method\nfor deformable 2D/3D registration. PolyPose parameterizes complex 3D\ndeformation fields as a composition of rigid transforms, leveraging the\nbiological constraint that individual bones do not bend in typical motion.\nUnlike existing methods that either assume no inter-joint movement or fail\noutright in this under-determined setting, our polyrigid formulation enforces\nanatomically plausible priors that respect the piecewise rigid nature of human\nmovement. This approach eliminates the need for expensive deformation\nregularizers that require patient- and procedure-specific hyperparameter\noptimization. Across extensive experiments on diverse datasets from orthopedic\nsurgery and radiotherapy, we show that this strong inductive bias enables\nPolyPose to successfully align the patient's preoperative volume to as few as\ntwo X-ray images, thereby providing crucial 3D guidance in challenging\nsparse-view and limited-angle settings where current registration methods fail."}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293", "abs": "https://arxiv.org/abs/2505.19293", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs."}
{"id": "2505.19261", "pdf": "https://arxiv.org/pdf/2505.19261", "abs": "https://arxiv.org/abs/2505.19261", "authors": ["Yu Zhang", "Jialei Zhou", "Xinchen Li", "Qi Zhang", "Zhongwei Wan", "Tianyu Wang", "Duoqian Miao", "Changwei Wang", "Longbing Cao"], "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages", "summary": "Current text-to-image diffusion generation typically employs complete-text\nconditioning. Due to the intricate syntax, diffusion transformers (DiTs)\ninherently suffer from a comprehension defect of complete-text captions.\nOne-fly complete-text input either overlooks critical semantic details or\ncauses semantic confusion by simultaneously modeling diverse semantic primitive\ntypes. To mitigate this defect of DiTs, we propose a novel split-text\nconditioning framework named DiT-ST. This framework converts a complete-text\ncaption into a split-text caption, a collection of simplified sentences, to\nexplicitly express various semantic primitives and their interconnections. The\nsplit-text caption is then injected into different denoising stages of DiT-ST\nin a hierarchical and incremental manner. Specifically, DiT-ST leverages Large\nLanguage Models to parse captions, extracting diverse primitives and\nhierarchically sorting out and constructing these primitives into a split-text\ninput. Moreover, we partition the diffusion denoising process according to its\ndifferential sensitivities to diverse semantic primitive types and determine\nthe appropriate timesteps to incrementally inject tokens of diverse semantic\nprimitive types into input tokens via cross-attention. In this way, DiT-ST\nenhances the representation learning of specific semantic primitive types\nacross different stages. Extensive experiments validate the effectiveness of\nour proposed DiT-ST in mitigating the complete-text comprehension defect."}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299", "abs": "https://arxiv.org/abs/2505.19299", "authors": ["Lingjun Zhao", "Hal Daum III"], "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%."}
{"id": "2505.19264", "pdf": "https://arxiv.org/pdf/2505.19264", "abs": "https://arxiv.org/abs/2505.19264", "authors": ["Guangan Chen", "Anh Minh Truong", "Hanhe Lin", "Michiel Vlaminck", "Wilfried Philips", "Hiep Luong"], "title": "Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis in 360$^\\circ$ scenes from extremely sparse input views\nis essential for applications like virtual reality and augmented reality. This\npaper presents a novel framework for novel view synthesis in extremely\nsparse-view cases. As typical structure-from-motion methods are unable to\nestimate camera poses in extremely sparse-view cases, we apply DUSt3R to\nestimate camera poses and generate a dense point cloud. Using the poses of\nestimated cameras, we densely sample additional views from the upper hemisphere\nspace of the scenes, from which we render synthetic images together with the\npoint cloud. Training 3D Gaussian Splatting model on a combination of reference\nimages from sparse views and densely sampled synthetic images allows a larger\nscene coverage in 3D space, addressing the overfitting challenge due to the\nlimited input in sparse-view cases. Retraining a diffusion-based image\nenhancement model on our created dataset, we further improve the quality of the\npoint-cloud-rendered images by removing artifacts. We compare our framework\nwith benchmark methods in cases of only four input views, demonstrating\nsignificant improvement in novel view synthesis under extremely sparse-view\nconditions for 360$^\\circ$ scenes."}
{"id": "2505.19300", "pdf": "https://arxiv.org/pdf/2505.19300", "abs": "https://arxiv.org/abs/2505.19300", "authors": ["Junnan Liu", "Linhao Luo", "Thuy-Trang Vu", "Gholamreza Haffari"], "title": "SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in large language models (LLMs) demonstrate their impressive\nreasoning capabilities. However, the reasoning confined to internal parametric\nspace limits LLMs' access to real-time information and understanding of the\nphysical world. To overcome this constraint, we introduce SituatedThinker, a\nnovel framework that enables LLMs to ground their reasoning in real-world\ncontexts through situated thinking, which adaptively combines both internal\nknowledge and external information with predefined interfaces. By utilizing\nreinforcement learning, SituatedThinker incentivizes deliberate reasoning with\nthe real world to acquire information and feedback, allowing LLMs to surpass\ntheir knowledge boundaries and enhance reasoning. Experimental results\ndemonstrate significant performance improvements on multi-hop\nquestion-answering and mathematical reasoning benchmarks. Furthermore,\nSituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,\nTableQA, and text-based games, showcasing the generalizable real-world grounded\nreasoning capability. Our codes are available at\nhttps://github.com/jnanliu/SituatedThinker."}
{"id": "2505.19291", "pdf": "https://arxiv.org/pdf/2505.19291", "abs": "https://arxiv.org/abs/2505.19291", "authors": ["Kazi Mahathir Rahman", "Showrin Rahman", "Sharmin Sultana Srishty"], "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis", "categories": ["cs.CV", "cs.AI", "68T05, 68T07, 68U10 68T05, 68T07, 68U10 68T05, 68T07, 68U10", "I.2.6; I.2.7; I.2.10; I.5.1; I.4.9"], "comment": "14 pages, 26 figures. Submitted to arXiv for dissemination. Intended\n  for future submission to a Generative AI conference", "summary": "Text-embedded image generation plays a critical role in industries such as\ngraphic design, advertising, and digital content creation. Text-to-Image\ngeneration methods leveraging diffusion models, such as TextDiffuser-2, have\ndemonstrated promising results in producing images with embedded text.\nTextDiffuser-2 effectively generates bounding box layouts that guide the\nrendering of visual text, achieving high fidelity and coherence. However,\nexisting approaches often rely on resource-intensive processes and are limited\nin their ability to run efficiently on both CPU and GPU platforms. To address\nthese challenges, we propose a novel two-stage pipeline that integrates\nreinforcement learning (RL) for rapid and optimized text layout generation with\na diffusion-based image synthesis model. Our RL-based approach significantly\naccelerates the bounding box prediction step while reducing overlaps, allowing\nthe system to run efficiently on both CPUs and GPUs. Extensive evaluations\ndemonstrate that our framework maintains or surpasses TextDiffuser-2's quality\nin text placement and image synthesis, with markedly faster runtime and\nincreased flexibility. Extensive evaluations demonstrate that our framework\nmaintains or surpasses TextDiffuser-2's quality in text placement and image\nsynthesis, with markedly faster runtime and increased flexibility. Our approach\nhas been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore\nmetrics close to state-of-the-art models, while being 97.64% more faster and\nrequiring only 2MB of memory to run."}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345", "abs": "https://arxiv.org/abs/2505.19345", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework."}
{"id": "2505.19297", "pdf": "https://arxiv.org/pdf/2505.19297", "abs": "https://arxiv.org/abs/2505.19297", "authors": ["Valerii Startsev", "Alexander Ustyuzhanin", "Alexey Kirillov", "Dmitry Baranchuk", "Sergey Kastryulin"], "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public."}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354", "abs": "https://arxiv.org/abs/2505.19354", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public."}
{"id": "2505.19319", "pdf": "https://arxiv.org/pdf/2505.19319", "abs": "https://arxiv.org/abs/2505.19319", "authors": ["Qiang Hu", "Qimei Wang", "Jia Chen", "Xuantao Ji", "Qiang Li", "Zhiwei Wang"], "title": "Holistic White-light Polyp Classification via Alignment-free Dense Distillation of Auxiliary Optical Chromoendoscopy", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025. Code and models:\n  https://github.com/Huster-Hq/ADD", "summary": "White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main\ncolonoscopic modalities for polyp classification. While NBI, as optical\nchromoendoscopy, offers valuable vascular details, WLI remains the most common\nand often the only available modality in resource-limited settings. However,\nWLI-based methods typically underperform, limiting their clinical\napplicability. Existing approaches transfer knowledge from NBI to WLI through\nglobal feature alignment but often rely on cropped lesion regions, which are\nsusceptible to detection errors and neglect contextual and subtle diagnostic\ncues. To address this, this paper proposes a novel holistic classification\nframework that leverages full-image diagnosis without requiring polyp\nlocalization. The key innovation lies in the Alignment-free Dense Distillation\n(ADD) module, which enables fine-grained cross-domain knowledge distillation\nregardless of misalignment between WLI and NBI images. Without resorting to\nexplicit image alignment, ADD learns pixel-wise cross-domain affinities to\nestablish correspondences between feature maps, guiding the distillation along\nthe most relevant pixel connections. To further enhance distillation\nreliability, ADD incorporates Class Activation Mapping (CAM) to filter\ncross-domain affinities, ensuring the distillation path connects only those\nsemantically consistent regions with equal contributions to polyp diagnosis.\nExtensive results on public and in-house datasets show that our method achieves\nstate-of-the-art performance, relatively outperforming the other approaches by\nat least 2.5% and 16.2% in AUC, respectively. Code is available at:\nhttps://github.com/Huster-Hq/ADD."}
{"id": "2505.19355", "pdf": "https://arxiv.org/pdf/2505.19355", "abs": "https://arxiv.org/abs/2505.19355", "authors": ["Lin Tian", "Marian-Andrei Rizoiu"], "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence."}
{"id": "2505.19328", "pdf": "https://arxiv.org/pdf/2505.19328", "abs": "https://arxiv.org/abs/2505.19328", "authors": ["Manuela Gonzlez-Gonzlez", "Soufiane Belharbi", "Muhammad Osama Zeeshan", "Masoumeh Sharafi", "Muhammad Haseeb Aslam", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon L Bacon", "Eric Granger"], "title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change", "categories": ["cs.CV", "cs.LG"], "comment": "41 pages, 13 figures, under review", "summary": "Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can\nplay a critical role in the personalization and effectiveness of digital\nbehaviour change interventions. These subtle and conflicting emotions are\nmanifested by a discord between multiple modalities, such as facial and vocal\nexpressions, and body language. Although experts can be trained to identify\nA/H, integrating them into digital interventions is costly and less effective.\nAutomatic learning systems provide a cost-effective alternative that can adapt\nto individual users, and operate seamlessly within real-time, and\nresource-limited environments. However, there are currently no datasets\navailable for the design of ML models to recognize A/H. This paper introduces a\nfirst Behavioural Ambivalence/Hesitancy (BAH) dataset collected for\nsubject-based multimodal recognition of A/H in videos. It contains videos from\n224 participants captured across 9 provinces in Canada, with different age, and\nethnicity. Through our web platform, we recruited participants to answer 7\nquestions, some of which were designed to elicit A/H while recording themselves\nvia webcam with microphone. BAH amounts to 1,118 videos for a total duration of\n8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp\nsegments to indicate where A/H occurs, and provide frame- and video-level\nannotations with the A/H cues. Video transcripts and their timestamps are also\nincluded, along with cropped and aligned faces in each frame, and a variety of\nparticipants meta-data. We include results baselines for BAH at frame- and\nvideo-level recognition in multi-modal setups, in addition to zero-shot\nprediction, and for personalization using unsupervised domain adaptation. The\nlimited performance of baseline models highlights the challenges of recognizing\nA/H in real-world videos. The data, code, and pretrained weights are available."}
{"id": "2505.19360", "pdf": "https://arxiv.org/pdf/2505.19360", "abs": "https://arxiv.org/abs/2505.19360", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "ChartLens: Fine-grained Visual Attribution in Charts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main)", "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%."}
{"id": "2505.19352", "pdf": "https://arxiv.org/pdf/2505.19352", "abs": "https://arxiv.org/abs/2505.19352", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Yanning Shen"], "title": "Beyond Editing Pairs: Fine-Grained Instructional Image Editing via Multi-Scale Learnable Regions", "categories": ["cs.CV"], "comment": null, "summary": "Current text-driven image editing methods typically follow one of two\ndirections: relying on large-scale, high-quality editing pair datasets to\nimprove editing precision and diversity, or exploring alternative dataset-free\ntechniques. However, constructing large-scale editing datasets requires\ncarefully designed pipelines, is time-consuming, and often results in\nunrealistic samples or unwanted artifacts. Meanwhile, dataset-free methods may\nsuffer from limited instruction comprehension and restricted editing\ncapabilities. Faced with these challenges, the present work develops a novel\nparadigm for instruction-driven image editing that leverages widely available\nand enormous text-image pairs, instead of relying on editing pair datasets. Our\napproach introduces a multi-scale learnable region to localize and guide the\nediting process. By treating the alignment between images and their textual\ndescriptions as supervision and learning to generate task-specific editing\nregions, our method achieves high-fidelity, precise, and instruction-consistent\nimage editing. Extensive experiments demonstrate that the proposed approach\nattains state-of-the-art performance across various tasks and benchmarks, while\nexhibiting strong adaptability to various types of generative models."}
{"id": "2505.19376", "pdf": "https://arxiv.org/pdf/2505.19376", "abs": "https://arxiv.org/abs/2505.19376", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "categories": ["cs.CL"], "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "summary": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses."}
{"id": "2505.19373", "pdf": "https://arxiv.org/pdf/2505.19373", "abs": "https://arxiv.org/abs/2505.19373", "authors": ["Niloufar Alipour Talemi", "Hossein Kashiani", "Hossein R. Nowdeh", "Fatemeh Afghah"], "title": "DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2025)", "summary": "Prompt learning has emerged as a powerful paradigm for adapting\nvision-language models such as CLIP to downstream tasks. However, existing\nmethods often overfit to seen data, leading to significant performance\ndegradation when generalizing to novel classes or unseen domains. To address\nthis limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning\nframework that integrates two complementary regularization strategies to\nenhance generalization. First, our Cross-Interactive Regularization (CIR)\nfosters cross-modal alignment by enabling cooperative learning between prompted\nand frozen encoders. Within CIR, a saliency-aware masking strategy guides the\nimage encoder to prioritize semantically critical image regions, reducing\nreliance on less informative patches. Second, we introduce a directional\nregularization strategy that aligns visual embeddings with class-wise prototype\nfeatures in a directional manner to prioritize consistency in feature\norientation over strict proximity. This approach ensures robust generalization\nby leveraging stable prototype directions derived from class-mean statistics.\nExtensive evaluations on 11 diverse image classification benchmarks demonstrate\nthat DiSa consistently outperforms state-of-the-art prompt learning methods\nacross various settings, including base-to-novel generalization, cross-dataset\ntransfer, domain generalization, and few-shot learning."}
{"id": "2505.19384", "pdf": "https://arxiv.org/pdf/2505.19384", "abs": "https://arxiv.org/abs/2505.19384", "authors": ["Seokgi Lee", "Jungjun Kim"], "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures", "summary": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure."}
{"id": "2505.19377", "pdf": "https://arxiv.org/pdf/2505.19377", "abs": "https://arxiv.org/abs/2505.19377", "authors": ["Zichong Meng", "Zeyu Han", "Xiaogang Peng", "Yiming Xie", "Huaizu Jiang"], "title": "Absolute Coordinates Make Motion Generation Easy", "categories": ["cs.CV"], "comment": "Preprint", "summary": "State-of-the-art text-to-motion generation models rely on the\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\nwhich encodes motion relative to the pelvis and to the previous frame with\nbuilt-in redundancy. While this design simplifies training for earlier\ngeneration models, it introduces critical limitations for diffusion models and\nhinders applicability to downstream tasks. In this work, we revisit the motion\nrepresentation and propose a radically simplified and long-abandoned\nalternative for text-to-motion generation: absolute joint coordinates in global\nspace. Through systematic analysis of design choices, we show that this\nformulation achieves significantly higher motion fidelity, improved text\nalignment, and strong scalability, even with a simple Transformer backbone and\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\nsupports downstream tasks such as text-driven motion control and\ntemporal/spatial editing without additional task-specific reengineering and\ncostly classifier guidance generation from control signals. Finally, we\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\nin motion from text, laying a strong foundation for future research and\nmotion-related applications."}
{"id": "2505.19388", "pdf": "https://arxiv.org/pdf/2505.19388", "abs": "https://arxiv.org/abs/2505.19388", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "summary": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube."}
{"id": "2505.19385", "pdf": "https://arxiv.org/pdf/2505.19385", "abs": "https://arxiv.org/abs/2505.19385", "authors": ["Jiaqi Guo", "Santiago Lopez-Tapia", "Aggelos K. Katsaggelos"], "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 2025 IEEE International Conference on Image\n  Processing (Oral)", "summary": "Limited Angle Computed Tomography (LACT) often faces significant challenges\ndue to missing angular information. Unlike previous methods that operate in the\nimage domain, we propose a new method that focuses on sinogram inpainting. We\nleverage MR-SDEs, a variant of diffusion models that characterize the diffusion\nprocess with mean-reverting stochastic differential equations, to fill in\nmissing angular data at the projection level. Furthermore, by combining\ndistillation with constraining the output of the model using the pseudo-inverse\nof the inpainting matrix, the diffusion process is accelerated and done in a\nstep, enabling efficient and accurate sinogram completion. A subsequent\npost-processing module back-projects the inpainted sinogram into the image\ndomain and further refines the reconstruction, effectively suppressing\nartifacts while preserving critical structural details. Quantitative\nexperimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in both perceptual and fidelity quality, offering\na promising solution for LACT reconstruction in scientific and clinical\napplications."}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392", "abs": "https://arxiv.org/abs/2505.19392", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias."}
{"id": "2505.19386", "pdf": "https://arxiv.org/pdf/2505.19386", "abs": "https://arxiv.org/abs/2505.19386", "authors": ["Nate Gillman", "Charles Herrmann", "Michael Freeman", "Daksh Aggarwal", "Evan Luo", "Deqing Sun", "Chen Sun"], "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://force-prompting.github.io/", "summary": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page."}
{"id": "2505.19405", "pdf": "https://arxiv.org/pdf/2505.19405", "abs": "https://arxiv.org/abs/2505.19405", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "categories": ["cs.CL", "cs.CR"], "comment": "18 pages, 1 figure", "summary": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems."}
{"id": "2505.19398", "pdf": "https://arxiv.org/pdf/2505.19398", "abs": "https://arxiv.org/abs/2505.19398", "authors": ["Yiwei Xie", "Ping Liu", "Zheng Zhang"], "title": "Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) models have demonstrated impressive capabilities in\ngenerating high-quality and diverse visual content from natural language\nprompts. However, uncontrolled reproduction of sensitive, copyrighted, or\nharmful imagery poses serious ethical, legal, and safety challenges. To address\nthese concerns, the concept erasure paradigm has emerged as a promising\ndirection, enabling the selective removal of specific semantic concepts from\ngenerative models while preserving their overall utility. This survey provides\na comprehensive overview and in-depth synthesis of concept erasure techniques\nin T2I diffusion models. We systematically categorize existing approaches along\nthree key dimensions: intervention level, which identifies specific model\ncomponents targeted for concept removal; optimization structure, referring to\nthe algorithmic strategies employed to achieve suppression; and semantic scope,\nconcerning the complexity and nature of the concepts addressed. This\nmulti-dimensional taxonomy enables clear, structured comparisons across diverse\nmethodologies, highlighting fundamental trade-offs between erasure specificity,\ngeneralization, and computational complexity. We further discuss current\nevaluation benchmarks, standardized metrics, and practical datasets,\nemphasizing gaps that limit comprehensive assessment, particularly regarding\nrobustness and practical effectiveness. Finally, we outline major challenges\nand promising future directions, including disentanglement of concept\nrepresentations, adaptive and incremental erasure strategies, adversarial\nrobustness, and new generative architectures. This survey aims to guide\nresearchers toward safer, more ethically aligned generative models, providing\nfoundational knowledge and actionable recommendations to advance responsible\ndevelopment in generative AI."}
{"id": "2505.19410", "pdf": "https://arxiv.org/pdf/2505.19410", "abs": "https://arxiv.org/abs/2505.19410", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability."}
{"id": "2505.19415", "pdf": "https://arxiv.org/pdf/2505.19415", "abs": "https://arxiv.org/abs/2505.19415", "authors": ["Hang Hua", "Ziyun Zeng", "Yizhi Song", "Yunlong Tang", "Liu He", "Daniel Aliaga", "Wei Xiong", "Jiebo Luo"], "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design. We will\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\nand accelerate future innovations in multi-modal image generation."}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426", "abs": "https://arxiv.org/abs/2505.19426", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "title": "The Role of Diversity in In-Context Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection."}
{"id": "2505.19420", "pdf": "https://arxiv.org/pdf/2505.19420", "abs": "https://arxiv.org/abs/2505.19420", "authors": ["Wenhua Wu", "Chenpeng Su", "Siting Zhu", "Tianchen Deng", "Zhe Liu", "Hesheng Wang"], "title": "ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based\nSimultaneous Localization and Mapping (SLAM) methods have demonstrated\nexceptional localization precision and remarkable dense mapping performance.\nHowever, dynamic objects introduce critical challenges by disrupting scene\nconsistency, leading to tracking drift and mapping artifacts. Existing methods\nthat employ semantic segmentation or object detection for dynamic\nidentification and filtering typically rely on predefined categorical priors,\nwhile discarding dynamic scene information crucial for robotic applications\nsuch as dynamic obstacle avoidance and environmental interaction. To overcome\nthese challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework\nbased on Gaussian splitting. We design an adaptive dynamic identification\nmechanism grounded in scene consistency analysis, comparing geometric and\ntextural discrepancies between real-time observations and historical maps. Ours\nrequires no predefined semantic category priors and adaptively discovers scene\ndynamics. Precise dynamic object recognition effectively mitigates interference\nfrom moving targets during localization. Furthermore, we propose a\ndynamic-static separation mapping strategy that constructs a temporal Gaussian\nmodel to achieve online incremental dynamic modeling. Experiments conducted on\nmultiple dynamic datasets demonstrate our method's flexible and accurate\ndynamic segmentation capabilities, along with state-of-the-art performance in\nboth localization and mapping."}
{"id": "2505.19428", "pdf": "https://arxiv.org/pdf/2505.19428", "abs": "https://arxiv.org/abs/2505.19428", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "categories": ["cs.CL"], "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "summary": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL."}
{"id": "2505.19421", "pdf": "https://arxiv.org/pdf/2505.19421", "abs": "https://arxiv.org/abs/2505.19421", "authors": ["Bardia Safaei", "Vibashan VS", "Vishal M. Patel"], "title": "Certainty and Uncertainty Guided Active Domain Adaptation", "categories": ["cs.CV"], "comment": "Accepted at IEEE ICIP 2025", "summary": "Active Domain Adaptation (ADA) adapts models to target domains by selectively\nlabeling a few target samples. Existing ADA methods prioritize uncertain\nsamples but overlook confident ones, which often match ground-truth. We find\nthat incorporating confident predictions into the labeled set before active\nsampling reduces the search space and improves adaptation. To address this, we\npropose a collaborative framework that labels uncertain samples while treating\nhighly confident predictions as ground truth. Our method combines Gaussian\nProcess-based Active Sampling (GPAS) for identifying uncertain samples and\nPseudo-Label-based Certain Sampling (PLCS) for confident ones, progressively\nenhancing adaptation. PLCS refines the search space, and GPAS reduces the\ndomain gap, boosting the proportion of confident samples. Extensive experiments\non Office-Home and DomainNet show that our approach outperforms\nstate-of-the-art ADA methods."}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429", "abs": "https://arxiv.org/abs/2505.19429", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "categories": ["cs.CL"], "comment": null, "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media."}
{"id": "2505.19422", "pdf": "https://arxiv.org/pdf/2505.19422", "abs": "https://arxiv.org/abs/2505.19422", "authors": ["Jiru Deng", "Tengjin Weng", "Tianyu Yang", "Wenhan Luo", "Zhiheng Li", "Wenhao Jiang"], "title": "LlamaSeg: Image Segmentation via Autoregressive Mask Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present LlamaSeg, a visual autoregressive framework that unifies multiple\nimage segmentation tasks via natural language instructions. We reformulate\nimage segmentation as a visual generation problem, representing masks as\n\"visual\" tokens and employing a LLaMA-style Transformer to predict them\ndirectly from image inputs. By adhering to the next-token prediction paradigm,\nour approach naturally integrates segmentation tasks into autoregressive\narchitectures. To support large-scale training, we introduce a data annotation\npipeline and construct the SA-OVRS dataset, which contains 2M segmentation\nmasks annotated with over 5,800 open-vocabulary labels or diverse textual\ndescriptions, covering a wide spectrum of real-world scenarios. This enables\nour model to localize objects in images based on text prompts and to generate\nfine-grained masks. To more accurately evaluate the quality of masks produced\nby visual generative models, we further propose a composite metric that\ncombines Intersection over Union (IoU) with Average Hausdorff Distance (AHD),\noffering a more precise assessment of contour fidelity. Experimental results\ndemonstrate that our method surpasses existing generative models across\nmultiple datasets and yields more detailed segmentation masks."}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430", "abs": "https://arxiv.org/abs/2505.19430", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research."}
{"id": "2505.19425", "pdf": "https://arxiv.org/pdf/2505.19425", "abs": "https://arxiv.org/abs/2505.19425", "authors": ["Yuhao He", "Jinyu Tian", "Haiwei Wu", "Jianqing Li"], "title": "Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The rapid advancement of diffusion models has enhanced their image inpainting\nand editing capabilities but also introduced significant societal risks.\nAdversaries can exploit user images from social media to generate misleading or\nharmful content. While adversarial perturbations can disrupt inpainting, global\nperturbation-based methods fail in mask-guided editing tasks due to spatial\nconstraints. To address these challenges, we propose Structure Disruption\nAttack (SDA), a powerful protection framework for safeguarding sensitive image\nregions against inpainting-based editing. Building upon the contour-focused\nnature of self-attention mechanisms of diffusion models, SDA optimizes\nperturbations by disrupting queries in self-attention during the initial\ndenoising step to destroy the contour generation process. This targeted\ninterference directly disrupts the structural generation capability of\ndiffusion models, effectively preventing them from producing coherent images.\nWe validate our motivation through visualization techniques and extensive\nexperiments on public datasets, demonstrating that SDA achieves\nstate-of-the-art (SOTA) protection performance while maintaining strong\nrobustness."}
{"id": "2505.19435", "pdf": "https://arxiv.org/pdf/2505.19435", "abs": "https://arxiv.org/abs/2505.19435", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "categories": ["cs.CL"], "comment": null, "summary": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%."}
{"id": "2505.19434", "pdf": "https://arxiv.org/pdf/2505.19434", "abs": "https://arxiv.org/abs/2505.19434", "authors": ["X. Feng", "D. Zhang", "S. Hu", "X. Li", "M. Wu", "J. Zhang", "X. Chen", "K. Huang"], "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICML25!", "summary": "Effectively modeling and utilizing spatiotemporal features from RGB and other\nmodalities (\\eg, depth, thermal, and event data, denoted as X) is the core of\nRGB-X tracker design. Existing methods often employ two parallel branches to\nseparately process the RGB and X input streams, requiring the model to\nsimultaneously handle two dispersed feature spaces, which complicates both the\nmodel structure and computation process. More critically, intra-modality\nspatial modeling within each dispersed space incurs substantial computational\noverhead, limiting resources for inter-modality spatial modeling and temporal\nmodeling. To address this, we propose a novel tracker, CSTrack, which focuses\non modeling Compact Spatiotemporal features to achieve simple yet effective\ntracking. Specifically, we first introduce an innovative Spatial Compact Module\nthat integrates the RGB-X dual input streams into a compact spatial feature,\nenabling thorough intra- and inter-modality spatial modeling. Additionally, we\ndesign an efficient Temporal Compact Module that compactly represents temporal\nfeatures by constructing the refined target distribution heatmap. Extensive\nexperiments validate the effectiveness of our compact spatiotemporal modeling\nmethod, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.\nThe code and models will be released at:\nhttps://github.com/XiaokunFeng/CSTrack."}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439", "abs": "https://arxiv.org/abs/2505.19439", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses."}
{"id": "2505.19455", "pdf": "https://arxiv.org/pdf/2505.19455", "abs": "https://arxiv.org/abs/2505.19455", "authors": ["Xu Li", "Fan Lyu"], "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning."}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440", "abs": "https://arxiv.org/abs/2505.19440", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models."}
{"id": "2505.19479", "pdf": "https://arxiv.org/pdf/2505.19479", "abs": "https://arxiv.org/abs/2505.19479", "authors": ["Lakshmi Aishwarya Malladi", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach", "categories": ["cs.CV", "cs.LG"], "comment": "Conference at ASEE 2025", "summary": "Over 8,024 wildfire incidents have been documented in 2024 alone, affecting\nthousands of fatalities and significant damage to infrastructure and\necosystems. Wildfires in the United States have inflicted devastating losses.\nWildfires are becoming more frequent and intense, which highlights how urgently\nefficient warning systems are needed to avoid disastrous outcomes. The goal of\nthis study is to enhance the accuracy of wildfire detection by using\nConvolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE\ndataset, which includes several kinds of wildfire and non-wildfire images, was\nemployed in the study. Low-resolution images, dataset imbalance, and the\nnecessity for real-time applicability are some of the main challenges. These\nproblems were resolved by enriching the dataset using data augmentation\ntechniques and optimizing the VGG16 model for binary classification. The model\nproduced a low false negative rate, which is essential for reducing unexplored\nfires, despite dataset boundaries. In order to help authorities execute fast\nresponses, this work shows that deep learning models such as VGG16 can offer a\nreliable, automated approach for early wildfire recognition. For the purpose of\nreducing the impact of wildfires, our future work will concentrate on\nconnecting to systems with real-time surveillance networks and enlarging the\ndataset to cover more varied fire situations."}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472", "abs": "https://arxiv.org/abs/2505.19472", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU)."}
{"id": "2505.19487", "pdf": "https://arxiv.org/pdf/2505.19487", "abs": "https://arxiv.org/abs/2505.19487", "authors": ["Zhuoheng Gao", "Yihao Li", "Jiyao Zhang", "Rui Zhao", "Tong Wu", "Hao Tang", "Zhaofei Yu", "Hao Dong", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams", "categories": ["cs.CV"], "comment": null, "summary": "Conventional frame-based cameras often struggle with stereo depth estimation\nin rapidly changing scenes. In contrast, bio-inspired spike cameras emit\nasynchronous events at microsecond-level resolution, providing an alternative\nsensing modality. However, existing methods lack specialized stereo algorithms\nand benchmarks tailored to the spike data. To address this gap, we propose\nSpikeStereoNet, a brain-inspired framework and the first to estimate stereo\ndepth directly from raw spike streams. The model fuses raw spike streams from\ntwo viewpoints and iteratively refines depth estimation through a recurrent\nspiking neural network (RSNN) update module. To benchmark our approach, we\nintroduce a large-scale synthetic spike stream dataset and a real-world stereo\nspike dataset with dense depth annotations. SpikeStereoNet outperforms existing\nmethods on both datasets by leveraging spike streams' ability to capture subtle\nedges and intensity shifts in challenging regions such as textureless surfaces\nand extreme lighting conditions. Furthermore, our framework exhibits strong\ndata efficiency, maintaining high accuracy even with substantially reduced\ntraining data. The source code and datasets will be publicly available."}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475", "abs": "https://arxiv.org/abs/2505.19475", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation."}
{"id": "2505.19492", "pdf": "https://arxiv.org/pdf/2505.19492", "abs": "https://arxiv.org/abs/2505.19492", "authors": ["Chuang Wang", "Haitao Zhou", "Ling Luo", "Qian Yu"], "title": "ViewCraft3D: High-Fidelity and View-Consistent 3D Vector Graphics Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "3D vector graphics play a crucial role in various applications including 3D\nshape retrieval, conceptual design, and virtual reality interactions due to\ntheir ability to capture essential structural information with minimal\nrepresentation. While recent approaches have shown promise in generating 3D\nvector graphics, they often suffer from lengthy processing times and struggle\nto maintain view consistency. To address these limitations, we propose\nViewCraft3D (VC3D), an efficient method that leverages 3D priors to generate 3D\nvector graphics. Specifically, our approach begins with 3D object analysis,\nemploys a geometric extraction algorithm to fit 3D vector graphics to the\nunderlying structure, and applies view-consistent refinement process to enhance\nvisual quality. Our comprehensive experiments demonstrate that VC3D outperforms\nprevious methods in both qualitative and quantitative evaluations, while\nsignificantly reducing computational overhead. The resulting 3D sketches\nmaintain view consistency and effectively capture the essential characteristics\nof the original objects."}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484", "abs": "https://arxiv.org/abs/2505.19484", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning."}
{"id": "2505.19495", "pdf": "https://arxiv.org/pdf/2505.19495", "abs": "https://arxiv.org/abs/2505.19495", "authors": ["Wei Li", "Dezhao Luo", "Dongbao Yang", "Zhenhang Li", "Weiping Wang", "Yu Zhou"], "title": "The Role of Video Generation in Enhancing Data-Limited Action Understanding", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Video action understanding tasks in real-world scenarios always suffer data\nlimitations. In this paper, we address the data-limited action understanding\nproblem by bridging data scarcity. We propose a novel method that employs a\ntext-to-video diffusion transformer to generate annotated data for model\ntraining. This paradigm enables the generation of realistic annotated data on\nan infinite scale without human intervention. We proposed the information\nenhancement strategy and the uncertainty-based label smoothing tailored to\ngenerate sample training. Through quantitative and qualitative analysis, we\nobserved that real samples generally contain a richer level of information than\ngenerated samples. Based on this observation, the information enhancement\nstrategy is proposed to enhance the informative content of the generated\nsamples from two aspects: the environments and the characters. Furthermore, we\nobserved that some low-quality generated samples might negatively affect model\ntraining. To address this, we devised the uncertainty-based label smoothing\nstrategy to increase the smoothing of these samples, thus reducing their\nimpact. We demonstrate the effectiveness of the proposed method on four\ndatasets across five tasks and achieve state-of-the-art performance for\nzero-shot action recognition."}
{"id": "2505.19494", "pdf": "https://arxiv.org/pdf/2505.19494", "abs": "https://arxiv.org/abs/2505.19494", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana"}
{"id": "2505.19498", "pdf": "https://arxiv.org/pdf/2505.19498", "abs": "https://arxiv.org/abs/2505.19498", "authors": ["Nanxing Hu", "Xiaoyue Duan", "Jinchao Zhang", "Guoliang Kang"], "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts."}
{"id": "2505.19510", "pdf": "https://arxiv.org/pdf/2505.19510", "abs": "https://arxiv.org/abs/2505.19510", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench."}
{"id": "2505.19500", "pdf": "https://arxiv.org/pdf/2505.19500", "abs": "https://arxiv.org/abs/2505.19500", "authors": ["Shogo Sato", "Masaru Tsuchida", "Mariko Yamaguchi", "Takuhiro Kaneko", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Objective, Absolute and Hue-aware Metrics for Intrinsic Image Decomposition on Real-World Scenes: A Proof of Concept", "categories": ["cs.CV"], "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Intrinsic image decomposition (IID) is the task of separating an image into\nalbedo and shade. In real-world scenes, it is difficult to quantitatively\nassess IID quality due to the unavailability of ground truth. The existing\nmethod provides the relative reflection intensities based on human-judged\nannotations. However, these annotations have challenges in subjectivity,\nrelative evaluation, and hue non-assessment. To address these, we propose a\nconcept of quantitative evaluation with a calculated albedo from a\nhyperspectral imaging and light detection and ranging (LiDAR) intensity.\nAdditionally, we introduce an optional albedo densification approach based on\nspectral similarity. This paper conducted a concept verification in a\nlaboratory environment, and suggested the feasibility of an objective,\nabsolute, and hue-aware assessment. (This paper is accepted by IEEE ICIP 2025.\n)"}
{"id": "2505.19511", "pdf": "https://arxiv.org/pdf/2505.19511", "abs": "https://arxiv.org/abs/2505.19511", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs."}
{"id": "2505.19503", "pdf": "https://arxiv.org/pdf/2505.19503", "abs": "https://arxiv.org/abs/2505.19503", "authors": ["Sanghyun Kim", "Deunsol Jung", "Minsu Cho"], "title": "Locality-Aware Zero-Shot Human-Object Interaction Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025; Code is available at:\n  https://github.com/OreoChocolate/LAIN", "summary": "Recent methods for zero-shot Human-Object Interaction (HOI) detection\ntypically leverage the generalization ability of large Vision-Language Model\n(VLM), i.e., CLIP, on unseen categories, showing impressive results on various\nzero-shot settings. However, existing methods struggle to adapt CLIP\nrepresentations for human-object pairs, as CLIP tends to overlook fine-grained\ninformation necessary for distinguishing interactions. To address this issue,\nwe devise, LAIN, a novel zero-shot HOI detection framework enhancing the\nlocality and interaction awareness of CLIP representations. The locality\nawareness, which involves capturing fine-grained details and the spatial\nstructure of individual objects, is achieved by aggregating the information and\nspatial priors of adjacent neighborhood patches. The interaction awareness,\nwhich involves identifying whether and how a human is interacting with an\nobject, is achieved by capturing the interaction pattern between the human and\nthe object. By infusing locality and interaction awareness into CLIP\nrepresentation, LAIN captures detailed information about the human-object\npairs. Our extensive experiments on existing benchmarks show that LAIN\noutperforms previous methods on various zero-shot settings, demonstrating the\nimportance of locality and interaction awareness for effective zero-shot HOI\ndetection."}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514", "abs": "https://arxiv.org/abs/2505.19514", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows."}
{"id": "2505.19507", "pdf": "https://arxiv.org/pdf/2505.19507", "abs": "https://arxiv.org/abs/2505.19507", "authors": ["Chenyu Lu", "Shiliang Sun", "Jing Zhao", "Nan Zhang", "Tengfei Song", "Hao Yang"], "title": "Multimodal Machine Translation with Visual Scene Graph Pruning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal machine translation (MMT) seeks to address the challenges posed by\nlinguistic polysemy and ambiguity in translation tasks by incorporating visual\ninformation. A key bottleneck in current MMT research is the effective\nutilization of visual data. Previous approaches have focused on extracting\nglobal or region-level image features and using attention or gating mechanisms\nfor multimodal information fusion. However, these methods have not adequately\ntackled the issue of visual information redundancy in MMT, nor have they\nproposed effective solutions. In this paper, we introduce a novel\napproach--multimodal machine translation with visual Scene Graph Pruning (PSG),\nwhich leverages language scene graph information to guide the pruning of\nredundant nodes in visual scene graphs, thereby reducing noise in downstream\ntranslation tasks. Through extensive comparative experiments with\nstate-of-the-art methods and ablation studies, we demonstrate the effectiveness\nof the PSG model. Our results also highlight the promising potential of visual\ninformation pruning in advancing the field of MMT."}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515", "abs": "https://arxiv.org/abs/2505.19515", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts."}
{"id": "2505.19518", "pdf": "https://arxiv.org/pdf/2505.19518", "abs": "https://arxiv.org/abs/2505.19518", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "Intra-operative data captured during image-guided surgery lacks sub-surface\ninformation, where key regions of interest, such as vessels and tumors, reside.\nImage-to-physical registration enables the fusion of pre-operative information\nand intra-operative data, typically represented as a point cloud. However, this\nregistration process struggles due to partial visibility of the intra-operative\npoint cloud. In this research, we propose a patient-specific point cloud\ncompletion approach to assist with the registration process. Specifically, we\nleverage VN-OccNet to generate a complete liver surface from a partial\nintra-operative point cloud. The network is trained in a patient-specific\nmanner, where simulated deformations from the pre-operative model are used to\ntrain the model. First, we conduct an in-depth analysis of VN-OccNet's\nrotation-equivariant property and its effectiveness in recovering complete\nsurfaces from partial intra-operative surfaces. Next, we integrate the\ncompleted intra-operative surface into the Go-ICP registration algorithm to\ndemonstrate its utility in improving initial rigid registration outcomes. Our\nresults highlight the promise of this patient-specific completion approach in\nmitigating the challenges posed by partial intra-operative visibility. The\nrotation equivariant and surface generation capabilities of VN-OccNet hold\nstrong promise for developing robust registration frameworks for variations of\nthe intra-operative point cloud."}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528", "abs": "https://arxiv.org/abs/2505.19528", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness."}
{"id": "2505.19519", "pdf": "https://arxiv.org/pdf/2505.19519", "abs": "https://arxiv.org/abs/2505.19519", "authors": ["Gihoon Kim", "Hyungjin Park", "Taesup Kim"], "title": "Regularized Personalization of Text-to-Image Diffusion Models without Distributional Drift", "categories": ["cs.CV"], "comment": null, "summary": "Personalization using text-to-image diffusion models involves adapting a\npretrained model to novel subjects with only a few image examples. This task\npresents a fundamental challenge, as the model must not only learn the new\nsubject effectively but also preserve its ability to generate diverse and\ncoherent outputs across a wide range of prompts. In other words, successful\npersonalization requires integrating new concepts without forgetting previously\nlearned generative capabilities. Forgetting denotes unintended distributional\ndrift, where the model's output distribution deviates from that of the original\npretrained model. In this paper, we provide an analysis of this issue and\nidentify a mismatch between standard training objectives and the goals of\npersonalization. To address this, we propose a new training objective based on\na Lipschitz-bounded formulation that explicitly constrains deviation from the\npretrained distribution. Our method provides improved control over\ndistributional drift and performs well even in data-scarce scenarios.\nExperimental results demonstrate that our approach consistently outperforms\nexisting personalization methods, achieving higher CLIP-T, CLIP-I, and DINO\nscores."}
{"id": "2505.19529", "pdf": "https://arxiv.org/pdf/2505.19529", "abs": "https://arxiv.org/abs/2505.19529", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models."}
{"id": "2505.19522", "pdf": "https://arxiv.org/pdf/2505.19522", "abs": "https://arxiv.org/abs/2505.19522", "authors": ["Jiyu Hu", "Haijiang Zeng", "Zhen Tian"], "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments."}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538", "abs": "https://arxiv.org/abs/2505.19538", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems."}
{"id": "2505.19535", "pdf": "https://arxiv.org/pdf/2505.19535", "abs": "https://arxiv.org/abs/2505.19535", "authors": ["Juntong Wang", "Jiarui Wang", "Huiyu Duan", "Guangtao Zhai", "Xiongkuo Min"], "title": "TDVE-Assessor: Benchmarking and Evaluating the Quality of Text-Driven Video Editing with LMMs", "categories": ["cs.CV"], "comment": "25 pages, 14 figures, 8 tables", "summary": "Text-driven video editing is rapidly advancing, yet its rigorous evaluation\nremains challenging due to the absence of dedicated video quality assessment\n(VQA) models capable of discerning the nuances of editing quality. To address\nthis critical gap, we introduce TDVE-DB, a large-scale benchmark dataset for\ntext-driven video editing. TDVE-DB consists of 3,857 edited videos generated\nfrom 12 diverse models across 8 editing categories, and is annotated with\n173,565 human subjective ratings along three crucial dimensions, i.e., edited\nvideo quality, editing alignment, and structural consistency. Based on TDVE-DB,\nwe first conduct a comprehensive evaluation for the 12 state-of-the-art editing\nmodels revealing the strengths and weaknesses of current video techniques, and\nthen benchmark existing VQA methods in the context of text-driven video editing\nevaluation. Building on these insights, we propose TDVE-Assessor, a novel VQA\nmodel specifically designed for text-driven video editing assessment.\nTDVE-Assessor integrates both spatial and temporal video features into a large\nlanguage model (LLM) for rich contextual understanding to provide comprehensive\nquality assessment. Extensive experiments demonstrate that TDVE-Assessor\nsubstantially outperforms existing VQA models on TDVE-DB across all three\nevaluation dimensions, setting a new state-of-the-art. Both TDVE-DB and\nTDVE-Assessor will be released upon the publication."}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548", "abs": "https://arxiv.org/abs/2505.19548", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "title": "How Syntax Specialization Emerges in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536", "abs": "https://arxiv.org/abs/2505.19536", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.19549", "pdf": "https://arxiv.org/pdf/2505.19549", "abs": "https://arxiv.org/abs/2505.19549", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings."}
{"id": "2505.19546", "pdf": "https://arxiv.org/pdf/2505.19546", "abs": "https://arxiv.org/abs/2505.19546", "authors": ["Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Mehrdad Noori", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Farzad Beizaee", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC."}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572", "abs": "https://arxiv.org/abs/2505.19572", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "title": "DocMEdit: Towards Document-Level Model Editing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods."}
{"id": "2505.19554", "pdf": "https://arxiv.org/pdf/2505.19554", "abs": "https://arxiv.org/abs/2505.19554", "authors": ["Jiongchao Jin", "Shengchu Zhao", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Time consumption and the complexity of manual layout design make automated\nlayout generation a critical task, especially for multiple applications across\ndifferent mobile devices. Existing graph-based layout generation approaches\nsuffer from limited generative capability, often resulting in unreasonable and\nincompatible outputs. Meanwhile, vision based generative models tend to\noverlook the original structural information, leading to component\nintersections and overlaps. To address these challenges, we propose an\nAggregation Structural Representation (ASR) module that integrates graph\nnetworks with large language models (LLMs) to preserve structural information\nwhile enhancing generative capability. This novel pipeline utilizes graph\nfeatures as hierarchical prior knowledge, replacing the traditional Vision\nTransformer (ViT) module in multimodal large language models (MLLM) to predict\nfull layout information for the first time. Moreover, the intermediate graph\nmatrix used as input for the LLM is human editable, enabling progressive, human\ncentric design generation. A comprehensive evaluation on the RICO dataset\ndemonstrates the strong performance of ASR, both quantitatively using mean\nIntersection over Union (mIoU), and qualitatively through a crowdsourced user\nstudy. Additionally, sampling on relational features ensures diverse layout\ngeneration, further enhancing the adaptability and creativity of the proposed\napproach."}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586", "abs": "https://arxiv.org/abs/2505.19586", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding."}
{"id": "2505.19564", "pdf": "https://arxiv.org/pdf/2505.19564", "abs": "https://arxiv.org/abs/2505.19564", "authors": ["Haofan Ren", "Zunjie Zhu", "Xiang Chen", "Ming Lu", "Rongfeng Lu", "Chenggang Yan"], "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, IJCAI 2025", "summary": "Neural fields are now the central focus of research in 3D vision and computer\ngraphics. Existing methods mainly focus on various scene representations, such\nas neural points and 3D Gaussians. However, few works have studied the\nrendering process to enhance the neural fields. In this work, we propose a\nplug-in method named K-Buffers that leverages multiple buffers to improve the\nrendering performance. Our method first renders K buffers from scene\nrepresentations and constructs K pixel-wise feature maps. Then, We introduce a\nK-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally,\nwe adopt a feature decoder to generate the rendering image. We also introduce\nan acceleration strategy to improve rendering speed and quality. We apply our\nmethod to well-known radiance field baselines, including neural point fields\nand 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our\nmethod effectively enhances the rendering performance of neural point fields\nand 3DGS."}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591", "abs": "https://arxiv.org/abs/2505.19591", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Multi-Agent Collaboration via Evolving Orchestration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution."}
{"id": "2505.19565", "pdf": "https://arxiv.org/pdf/2505.19565", "abs": "https://arxiv.org/abs/2505.19565", "authors": ["George Karantaidis", "Athanasios Pantsios", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic aperture radar automatic target recognition (SAR-ATR) systems have\nrapidly evolved to tackle incremental recognition challenges in operational\nsettings. Data scarcity remains a major hurdle that conventional SAR-ATR\ntechniques struggle to address. To cope with this challenge, we propose a\nfew-shot class-incremental learning (FSCIL) framework based on a dual-branch\narchitecture that focuses on local feature extraction and leverages the\ndiscrete Fourier transform and global filters to capture long-term spatial\ndependencies. This incorporates a lightweight cross-attention mechanism that\nfuses domain-specific features with global dependencies to ensure robust\nfeature interaction, while maintaining computational efficiency by introducing\nminimal scale-shift parameters. The framework combines focal loss for class\ndistinction under imbalance and center loss for compact intra-class\ndistributions to enhance class separation boundaries. Experimental results on\nthe MSTAR benchmark dataset demonstrate that the proposed framework\nconsistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting\nto its effectiveness in real-world scenarios."}
{"id": "2505.19598", "pdf": "https://arxiv.org/pdf/2505.19598", "abs": "https://arxiv.org/abs/2505.19598", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment."}
{"id": "2505.19569", "pdf": "https://arxiv.org/pdf/2505.19569", "abs": "https://arxiv.org/abs/2505.19569", "authors": ["Jianghang Lin", "Yue Hu", "Jiangtao Shen", "Yunhang Shen", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open vocabulary image segmentation tackles the challenge of recognizing\ndynamically adjustable, predefined novel categories at inference time by\nleveraging vision-language alignment. However, existing paradigms typically\nperform class-agnostic region segmentation followed by category matching, which\ndeviates from the human visual system's process of recognizing objects based on\nsemantic concepts, leading to poor alignment between region segmentation and\ntarget concepts. To bridge this gap, we propose a novel Cognition-Inspired\nFramework for open vocabulary image segmentation that emulates the human visual\nrecognition process: first forming a conceptual understanding of an object,\nthen perceiving its spatial extent. The framework consists of three core\ncomponents: (1) A Generative Vision-Language Model (G-VLM) that mimics human\ncognition by generating object concepts to provide semantic guidance for region\nsegmentation. (2) A Concept-Aware Visual Enhancer Module that fuses textual\nconcept features with global visual representations, enabling adaptive visual\nperception based on target concepts. (3) A Cognition-Inspired Decoder that\nintegrates local instance features with G-VLM-provided semantic cues, allowing\nselective classification over a subset of relevant categories. Extensive\nexperiments demonstrate that our framework achieves significant improvements,\nreaching $27.2$ PQ, $17.0$ mAP, and $35.3$ mIoU on A-150. It further attains\n$56.2$, $28.2$, $15.4$, $59.2$, $18.7$, and $95.8$ mIoU on Cityscapes,\nMapillary Vistas, A-847, PC-59, PC-459, and PAS-20, respectively. In addition,\nour framework supports vocabulary-free segmentation, offering enhanced\nflexibility in recognizing unseen categories. Code will be public."}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599", "abs": "https://arxiv.org/abs/2505.19599", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output."}
{"id": "2505.19571", "pdf": "https://arxiv.org/pdf/2505.19571", "abs": "https://arxiv.org/abs/2505.19571", "authors": ["Hu Xiaobin", "Liang Yujie", "Luo Donghao", "Peng Xu", "Zhang Jiangning", "Zhu Junwei", "Wang Chengjie", "Fu Yanwei"], "title": "VTBench: Comprehensive Benchmark Suite Towards Real-World Virtual Try-on Models", "categories": ["cs.CV"], "comment": "Project Websit: \\url{https://github.com/HUuxiaobin/VTBench}", "summary": "While virtual try-on has achieved significant progress, evaluating these\nmodels towards real-world scenarios remains a challenge. A comprehensive\nbenchmark is essential for three key reasons:(1) Current metrics inadequately\nreflect human perception, particularly in unpaired try-on settings;(2)Most\nexisting test sets are limited to indoor scenarios, lacking complexity for\nreal-world evaluation; and (3) An ideal system should guide future advancements\nin virtual try-on generation. To address these needs, we introduce VTBench, a\nhierarchical benchmark suite that systematically decomposes virtual image\ntry-on into hierarchical, disentangled dimensions, each equipped with tailored\ntest sets and evaluation criteria. VTBench exhibits three key advantages:1)\nMulti-Dimensional Evaluation Framework: The benchmark encompasses five critical\ndimensions for virtual try-on generation (e.g., overall image quality, texture\npreservation, complex background consistency, cross-category size adaptability,\nand hand-occlusion handling). Granular evaluation metrics of corresponding test\nsets pinpoint model capabilities and limitations across diverse, challenging\nscenarios.2) Human Alignment: Human preference annotations are provided for\neach test set, ensuring the benchmark's alignment with perceptual quality\nacross all evaluation dimensions. (3) Valuable Insights: Beyond standard indoor\nsettings, we analyze model performance variations across dimensions and\ninvestigate the disparity between indoor and real-world try-on scenarios. To\nfoster the field of virtual try-on towards challenging real-world scenario,\nVTBench will be open-sourced, including all test sets, evaluation protocols,\ngenerated results, and human annotations."}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604", "abs": "https://arxiv.org/abs/2505.19604", "authors": ["Ahan Prasannakumar Shetty"], "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems."}
{"id": "2505.19582", "pdf": "https://arxiv.org/pdf/2505.19582", "abs": "https://arxiv.org/abs/2505.19582", "authors": ["Kaiqing Lin", "Zhiyuan Yan", "Ke-Yue Zhang", "Li Hao", "Yue Zhou", "Yuzhen Lin", "Weixiang Li", "Taiping Yao", "Shouhong Ding", "Bin Li"], "title": "Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes", "categories": ["cs.CV"], "comment": null, "summary": "Securing personal identity against deepfake attacks is increasingly critical\nin the digital age, especially for celebrities and political figures whose\nfaces are easily accessible and frequently targeted. Most existing deepfake\ndetection methods focus on general-purpose scenarios and often ignore the\nvaluable prior knowledge of known facial identities, e.g., \"VIP individuals\"\nwhose authentic facial data are already available. In this paper, we propose\n\\textbf{VIPGuard}, a unified multimodal framework designed to capture\nfine-grained and comprehensive facial representations of a given identity,\ncompare them against potentially fake or similar-looking faces, and reason over\nthese comparisons to make accurate and explainable predictions. Specifically,\nour framework consists of three main stages. First, fine-tune a multimodal\nlarge language model (MLLM) to learn detailed and structural facial attributes.\nSecond, we perform identity-level discriminative learning to enable the model\nto distinguish subtle differences between highly similar faces, including real\nand fake variations. Finally, we introduce user-specific customization, where\nwe model the unique characteristics of the target face identity and perform\nsemantic reasoning via MLLM to enable personalized and explainable deepfake\ndetection. Our framework shows clear advantages over previous detection works,\nwhere traditional detectors mainly rely on low-level visual cues and provide no\nhuman-understandable explanations, while other MLLM-based models often lack a\ndetailed understanding of specific face identities. To facilitate the\nevaluation of our method, we built a comprehensive identity-aware benchmark\ncalled \\textbf{VIPBench} for personalized deepfake detection, involving the\nlatest 7 face-swapping and 7 entire face synthesis techniques for generation."}
{"id": "2505.19606", "pdf": "https://arxiv.org/pdf/2505.19606", "abs": "https://arxiv.org/abs/2505.19606", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies."}
{"id": "2505.19585", "pdf": "https://arxiv.org/pdf/2505.19585", "abs": "https://arxiv.org/abs/2505.19585", "authors": ["Jiameng Li", "Teodora Popordanoska", "Sebastian G. Gruber", "Frederik Maes", "Matthew B. Blaschko"], "title": "Beyond Segmentation: Confidence-Aware and Debiased Estimation of Ratio-based Biomarkers", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Ratio-based biomarkers -- such as the proportion of necrotic tissue within a\ntumor -- are widely used in clinical practice to support diagnosis, prognosis\nand treatment planning. These biomarkers are typically estimated from soft\nsegmentation outputs by computing region-wise ratios. Despite the high-stakes\nnature of clinical decision making, existing methods provide only point\nestimates, offering no measure of uncertainty. In this work, we propose a\nunified \\textit{confidence-aware} framework for estimating ratio-based\nbiomarkers. We conduct a systematic analysis of error propagation in the\nsegmentation-to-biomarker pipeline and identify model miscalibration as the\ndominant source of uncertainty. To mitigate this, we incorporate a lightweight,\npost-hoc calibration module that can be applied using internal hospital data\nwithout retraining. We leverage a tunable parameter $Q$ to control the\nconfidence level of the derived bounds, allowing adaptation towards clinical\npractice. Extensive experiments show that our method produces statistically\nsound confidence intervals, with tunable confidence levels, enabling more\ntrustworthy application of predictive biomarkers in clinical workflows."}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628", "abs": "https://arxiv.org/abs/2505.19628", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench."}
{"id": "2505.19603", "pdf": "https://arxiv.org/pdf/2505.19603", "abs": "https://arxiv.org/abs/2505.19603", "authors": ["Ho Hin Lee", "Quan Liu", "Shunxing Bao", "Yuankai Huo", "Bennett A. Landman"], "title": "Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages", "summary": "In contrast to vision transformers, which model long-range dependencies\nthrough global self-attention, large kernel convolutions provide a more\nefficient and scalable alternative, particularly in high-resolution 3D\nvolumetric settings. However, naively increasing kernel size often leads to\noptimization instability and degradation in performance. Motivated by the\nspatial bias observed in effective receptive fields (ERFs), we hypothesize that\ndifferent kernel elements converge at variable rates during training. To\nsupport this, we derive a theoretical connection between element-wise gradients\nand first-order optimization, showing that structurally re-parameterized\nconvolution blocks inherently induce spatially varying learning rates. Building\non this insight, we introduce Rep3D, a 3D convolutional framework that\nincorporates a learnable spatial prior into large kernel training. A\nlightweight two-stage modulation network generates a receptive-biased scaling\nmask, adaptively re-weighting kernel updates and enabling local-to-global\nconvergence behavior. Rep3D adopts a plain encoder design with large depthwise\nconvolutions, avoiding the architectural complexity of multi-branch\ncompositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks\nand demonstrate consistent improvements over state-of-the-art baselines,\nincluding transformer-based and fixed-prior re-parameterization methods. By\nunifying spatial inductive bias with optimization-aware learning, Rep3D offers\nan interpretable, and scalable solution for 3D medical image analysis. The\nsource code is publicly available at https://github.com/leeh43/Rep3D."}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630", "abs": "https://arxiv.org/abs/2505.19630", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL"}
{"id": "2505.19610", "pdf": "https://arxiv.org/pdf/2505.19610", "abs": "https://arxiv.org/abs/2505.19610", "authors": ["Jiaxin Song", "Yixu Wang", "Jie Li", "Rui Yu", "Yan Teng", "Xingjun Ma", "Yingchun Wang"], "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit impressive performance, yet the\nintegration of powerful vision encoders has significantly broadened their\nattack surface, rendering them increasingly susceptible to jailbreak attacks.\nHowever, lacking well-defined attack objectives, existing jailbreak methods\noften struggle with gradient-based strategies prone to local optima and lacking\nprecise directional guidance, and typically decouple visual and textual\nmodalities, thereby limiting their effectiveness by neglecting crucial\ncross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)\nframework, we posit that VLMs encode safety-relevant information within their\ninternal fusion-layer representations, revealing an implicit safety decision\nboundary in the latent space. This motivates exploiting boundary to steer model\nbehavior. Accordingly, we propose JailBound, a novel latent space jailbreak\nframework comprising two stages: (1) Safety Boundary Probing, which addresses\nthe guidance issue by approximating decision boundary within fusion layer's\nlatent space, thereby identifying optimal perturbation directions towards the\ntarget region; and (2) Safety Boundary Crossing, which overcomes the\nlimitations of decoupled approaches by jointly optimizing adversarial\nperturbations across both image and text inputs. This latter stage employs an\ninnovative mechanism to steer the model's internal state towards\npolicy-violating outputs while maintaining cross-modal semantic consistency.\nExtensive experiments on six diverse VLMs demonstrate JailBound's efficacy,\nachieves 94.32% white-box and 67.28% black-box attack success averagely, which\nare 6.17% and 21.13% higher than SOTA methods, respectively. Our findings\nexpose a overlooked safety risk in VLMs and highlight the urgent need for more\nrobust defenses. Warning: This paper contains potentially sensitive, harmful\nand offensive content."}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631", "abs": "https://arxiv.org/abs/2505.19631", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA"}
{"id": "2505.19611", "pdf": "https://arxiv.org/pdf/2505.19611", "abs": "https://arxiv.org/abs/2505.19611", "authors": ["Ruolin Shen", "Xiaozhong Ji", "Kai WU", "Jiangning Zhang", "Yijun He", "HaiHua Yang", "Xiaobin Hu", "Xiaoyu Sun"], "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Website: \\url{https://github.com/HUuxiaobin/VRRF}", "summary": "Current multi-modal models exhibit a notable misalignment with the human\nvisual system when identifying objects that are visually assimilated into the\nbackground. Our observations reveal that these multi-modal models cannot\ndistinguish concealed objects, demonstrating an inability to emulate human\ncognitive processes which effectively utilize foreground-background similarity\nprinciples for visual analysis. To analyze this hidden human-model visual\nthinking discrepancy, we build a visual system that mimicks human visual\ncamouflaged perception to progressively and iteratively `refocus' visual\nconcealed content. The refocus is a progressive guidance mechanism enabling\nmodels to logically localize objects in visual images through stepwise\nreasoning. The localization process of concealed objects requires hierarchical\nattention shifting with dynamic adjustment and refinement of prior cognitive\nknowledge. In this paper, we propose a visual refocus reinforcement framework\nvia the policy optimization algorithm to encourage multi-modal models to think\nand refocus more before answering, and achieve excellent reasoning abilities to\nalign and even surpass human camouflaged perception systems. Our extensive\nexperiments on camouflaged perception successfully demonstrate the emergence of\nrefocus visual phenomena, characterized by multiple reasoning tokens and\ndynamic adjustment of the detection box. Besides, experimental results on both\ncamouflaged object classification and detection tasks exhibit significantly\nsuperior performance compared to Supervised Fine-Tuning (SFT) baselines."}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634", "abs": "https://arxiv.org/abs/2505.19634", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios."}
{"id": "2505.19613", "pdf": "https://arxiv.org/pdf/2505.19613", "abs": "https://arxiv.org/abs/2505.19613", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization."}
{"id": "2505.19640", "pdf": "https://arxiv.org/pdf/2505.19640", "abs": "https://arxiv.org/abs/2505.19640", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling."}
{"id": "2505.19618", "pdf": "https://arxiv.org/pdf/2505.19618", "abs": "https://arxiv.org/abs/2505.19618", "authors": ["Hanze Liu", "Jiahong Fu", "Qi Xie", "Deyu Meng"], "title": "Rotation-Equivariant Self-Supervised Method in Image Denoising", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Self-supervised image denoising methods have garnered significant research\nattention in recent years, for this kind of method reduces the requirement of\nlarge training datasets. Compared to supervised methods, self-supervised\nmethods rely more on the prior embedded in deep networks themselves. As a\nresult, most of the self-supervised methods are designed with Convolution\nNeural Networks (CNNs) architectures, which well capture one of the most\nimportant image prior, translation equivariant prior. Inspired by the great\nsuccess achieved by the introduction of translational equivariance, in this\npaper, we explore the way to further incorporate another important image prior.\nSpecifically, we first apply high-accuracy rotation equivariant convolution to\nself-supervised image denoising. Through rigorous theoretical analysis, we have\nproved that simply replacing all the convolution layers with rotation\nequivariant convolution layers would modify the network into its rotation\nequivariant version. To the best of our knowledge, this is the first time that\nrotation equivariant image prior is introduced to self-supervised image\ndenoising at the network architecture level with a comprehensive theoretical\nanalysis of equivariance errors, which offers a new perspective to the field of\nself-supervised image denoising. Moreover, to further improve the performance,\nwe design a new mask mechanism to fusion the output of rotation equivariant\nnetwork and vanilla CNN-based network, and construct an adaptive rotation\nequivariant framework. Through extensive experiments on three typical methods,\nwe have demonstrated the effectiveness of the proposed method."}
{"id": "2505.19647", "pdf": "https://arxiv.org/pdf/2505.19647", "abs": "https://arxiv.org/abs/2505.19647", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG."}
{"id": "2505.19624", "pdf": "https://arxiv.org/pdf/2505.19624", "abs": "https://arxiv.org/abs/2505.19624", "authors": ["Pusheng Xu", "Xia Gong", "Xiaolan Chen", "Weiyi Zhang", "Jiancheng Yang", "Bingjie Yan", "Meng Yuan", "Yalin Zheng", "Mingguang He", "Danli Shi"], "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care."}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660", "abs": "https://arxiv.org/abs/2505.19660", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI"}
{"id": "2505.19638", "pdf": "https://arxiv.org/pdf/2505.19638", "abs": "https://arxiv.org/abs/2505.19638", "authors": ["Ming Meng", "Qi Dong", "Jiajie Li", "Zhe Zhu", "Xingyu Wang", "Zhaoxin Fan", "Wei Zhao", "Wenjun Wu"], "title": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation."}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667", "abs": "https://arxiv.org/abs/2505.19667", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions."}
{"id": "2505.19650", "pdf": "https://arxiv.org/pdf/2505.19650", "abs": "https://arxiv.org/abs/2505.19650", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Yahui Liu", "Hongzhi Zhang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yu Tian", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "26 pages, project page: https://friedrichor.github.io/projects/UNITE", "summary": "Multimodal information retrieval (MIR) faces inherent challenges due to the\nheterogeneity of data sources and the complexity of cross-modal alignment.\nWhile previous studies have identified modal gaps in feature spaces, a\nsystematic approach to address these challenges remains unexplored. In this\nwork, we introduce UNITE, a universal framework that tackles these challenges\nthrough two critical yet underexplored aspects: data curation and\nmodality-aware training configurations. Our work provides the first\ncomprehensive analysis of how modality-specific data properties influence\ndownstream task performance across diverse scenarios. Moreover, we propose\nModal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive\nrelationships among the instances of different modalities. Our framework\nachieves state-of-the-art results on multiple multimodal retrieval benchmarks,\noutperforming existing methods by notable margins. Through extensive\nexperiments, we demonstrate that strategic modality curation and tailored\ntraining protocols are pivotal for robust cross-modal representation learning.\nThis work not only advances MIR performance but also provides a foundational\nblueprint for future research in multimodal systems. Our project is available\nat https://friedrichor.github.io/projects/UNITE."}
{"id": "2505.19670", "pdf": "https://arxiv.org/pdf/2505.19670", "abs": "https://arxiv.org/abs/2505.19670", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples."}
{"id": "2505.19656", "pdf": "https://arxiv.org/pdf/2505.19656", "abs": "https://arxiv.org/abs/2505.19656", "authors": ["Tianren Ma", "Xiaosong Zhang", "Boyu Yang", "Junlan Feng", "Qixiang Ye"], "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation", "categories": ["cs.CV"], "comment": "Preprint, under development", "summary": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency."}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674", "abs": "https://arxiv.org/abs/2505.19674", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations."}
{"id": "2505.19659", "pdf": "https://arxiv.org/pdf/2505.19659", "abs": "https://arxiv.org/abs/2505.19659", "authors": ["Piyush Tiwary", "Kinjawl Bhattacharyya", "Prathosh A. P"], "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at ICML 2025", "summary": "Medical image segmentation models often struggle to generalize across\ndifferent domains due to various reasons. Domain Generalization (DG) methods\novercome this either through representation learning or data augmentation\n(DAug). While representation learning methods seek domain-invariant features,\nthey often rely on ad-hoc techniques and lack formal guarantees. DAug methods,\nwhich enrich model representations through synthetic samples, have shown\ncomparable or superior performance to representation learning approaches. We\npropose LangDAug, a novel $\\textbf{Lang}$evin $\\textbf{D}$ata\n$\\textbf{Aug}$mentation for multi-source domain generalization in 2D medical\nimage segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via\ncontrastive divergence to traverse between source domains, generating\nintermediate samples through Langevin dynamics. Theoretical analysis shows that\nLangDAug induces a regularization effect, and for GLMs, it upper-bounds the\nRademacher complexity by the intrinsic dimensionality of the data manifold.\nThrough extensive experiments on Fundus segmentation and 2D MRI prostate\nsegmentation benchmarks, we show that LangDAug outperforms state-of-the-art\ndomain generalization methods and effectively complements existing\ndomain-randomization approaches. The codebase for our method is available at\nhttps://github.com/backpropagator/LangDAug."}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675", "abs": "https://arxiv.org/abs/2505.19675", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github."}
{"id": "2505.19668", "pdf": "https://arxiv.org/pdf/2505.19668", "abs": "https://arxiv.org/abs/2505.19668", "authors": ["Tengda Huang", "Yu Zhang", "Tianren Li", "Yufu Qu", "Fulin Liu", "Zhenzhong Wei"], "title": "Burst Image Super-Resolution via Multi-Cross Attention Encoding and Multi-Scan State-Space Decoding", "categories": ["cs.CV"], "comment": "32 pages, 13 figures, submitted to 'Image and Vision Computing'", "summary": "Multi-image super-resolution (MISR) can achieve higher image quality than\nsingle-image super-resolution (SISR) by aggregating sub-pixel information from\nmultiple spatially shifted frames. Among MISR tasks, burst super-resolution\n(BurstSR) has gained significant attention due to its wide range of\napplications. Recent methods have increasingly adopted Transformers over\nconvolutional neural networks (CNNs) in super-resolution tasks, due to their\nsuperior ability to capture both local and global context. However, most\nexisting approaches still rely on fixed and narrow attention windows that\nrestrict the perception of features beyond the local field. This limitation\nhampers alignment and feature aggregation, both of which are crucial for\nhigh-quality super-resolution. To address these limitations, we propose a novel\nfeature extractor that incorporates two newly designed attention mechanisms:\noverlapping cross-window attention and cross-frame attention, enabling more\nprecise and efficient extraction of sub-pixel information across multiple\nframes. Furthermore, we introduce a Multi-scan State-Space Module with the\ncross-frame attention mechanism to enhance feature aggregation. Extensive\nexperiments on both synthetic and real-world benchmarks demonstrate the\nsuperiority of our approach. Additional evaluations on ISO 12233 resolution\ntest charts further confirm its enhanced super-resolution performance."}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678", "abs": "https://arxiv.org/abs/2505.19678", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency."}
{"id": "2505.19684", "pdf": "https://arxiv.org/pdf/2505.19684", "abs": "https://arxiv.org/abs/2505.19684", "authors": ["Bingrui Sima", "Linhua Cong", "Wenxuan Wang", "Kun He"], "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks."}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679", "abs": "https://arxiv.org/abs/2505.19679", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points."}
{"id": "2505.19692", "pdf": "https://arxiv.org/pdf/2505.19692", "abs": "https://arxiv.org/abs/2505.19692", "authors": ["Wenchao Sun", "Xuewu Lin", "Keyu Chen", "Zixiang Pei", "Yining Shi", "Chuang Zhang", "Sifa Zheng"], "title": "DriveCamSim: Generalizable Camera Simulation via Explicit Camera Modeling for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Camera sensor simulation serves as a critical role for autonomous driving\n(AD), e.g. evaluating vision-based AD algorithms. While existing approaches\nhave leveraged generative models for controllable image/video generation, they\nremain constrained to generating multi-view video sequences with fixed camera\nviewpoints and video frequency, significantly limiting their downstream\napplications. To address this, we present a generalizable camera simulation\nframework DriveCamSim, whose core innovation lies in the proposed Explicit\nCamera Modeling (ECM) mechanism. Instead of implicit interaction through\nvanilla attention, ECM establishes explicit pixel-wise correspondences across\nmulti-view and multi-frame dimensions, decoupling the model from overfitting to\nthe specific camera configurations (intrinsic/extrinsic parameters, number of\nviews) and temporal sampling rates presented in the training data. For\ncontrollable generation, we identify the issue of information loss inherent in\nexisting conditional encoding and injection pipelines, proposing an\ninformation-preserving control mechanism. This control mechanism not only\nimproves conditional controllability, but also can be extended to be\nidentity-aware to enhance temporal consistency in foreground object rendering.\nWith above designs, our model demonstrates superior performance in both visual\nquality and controllability, as well as generalization capability across\nspatial-level (camera parameters variations) and temporal-level (video frame\nrate variations), enabling flexible user-customizable camera simulation\ntailored to diverse application scenarios. Code will be avaliable at\nhttps://github.com/swc-17/DriveCamSim for facilitating future research."}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700", "abs": "https://arxiv.org/abs/2505.19700", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models."}
{"id": "2505.19694", "pdf": "https://arxiv.org/pdf/2505.19694", "abs": "https://arxiv.org/abs/2505.19694", "authors": ["Wen Yin", "Yong Wang", "Guiduo Duan", "Dongyang Zhang", "Xin Hu", "Yuan-Fang Li", "Tao He"], "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Visual Emotion Recognition (VER) is a critical yet challenging task aimed at\ninferring emotional states of individuals based on visual cues. However,\nexisting works focus on single domains, e.g., realistic images or stickers,\nlimiting VER models' cross-domain generalizability. To fill this gap, we\nintroduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)\ntask, which aims to generalize visual emotion recognition from the source\ndomain (e.g., realistic images) to the low-resource target domain (e.g.,\nstickers) in an unsupervised manner. Compared to the conventional unsupervised\ndomain adaptation problems, UCDVER presents two key challenges: a significant\nemotional expression variability and an affective distribution shift. To\nmitigate these issues, we propose the Knowledge-aligned\nCounterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,\nKCDP leverages a VLM to align emotional representations in a shared knowledge\nspace and guides diffusion models for improved visual affective perception.\nFurthermore, a Counterfactual-Enhanced Language-image Emotional Alignment\n(CLIEA) method generates high-quality pseudo-labels for the target domain.\nExtensive experiments demonstrate that our model surpasses SOTA models in both\nperceptibility and generalization, e.g., gaining 12% improvements over the SOTA\nVER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver."}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706", "abs": "https://arxiv.org/abs/2505.19706", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency."}
{"id": "2505.19696", "pdf": "https://arxiv.org/pdf/2505.19696", "abs": "https://arxiv.org/abs/2505.19696", "authors": ["Mohamed Amine Kerkouri", "Marouane Tliba", "Aladine Chetouani", "Nour Aburaed", "Alessandro Bruno"], "title": "Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Under review", "summary": "This position paper argues that Mean Opinion Score (MOS), while historically\nfoundational, is no longer sufficient as the sole supervisory signal for\nmultimedia quality assessment models. MOS reduces rich, context-sensitive human\njudgments to a single scalar, obscuring semantic failures, user intent, and the\nrationale behind quality decisions. We contend that modern quality assessment\nmodels must integrate three interdependent capabilities: (1) context-awareness,\nto adapt evaluations to task-specific goals and viewing conditions; (2)\nreasoning, to produce interpretable, evidence-grounded justifications for\nquality judgments; and (3) multimodality, to align perceptual and semantic cues\nusing vision-language models. We critique the limitations of current\nMOS-centric benchmarks and propose a roadmap for reform: richer datasets with\ncontextual metadata and expert rationales, and new evaluation metrics that\nassess semantic alignment, reasoning fidelity, and contextual sensitivity. By\nreframing quality assessment as a contextual, explainable, and multimodal\nmodeling task, we aim to catalyze a shift toward more robust, human-aligned,\nand trustworthy evaluation systems."}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT."}
{"id": "2505.19702", "pdf": "https://arxiv.org/pdf/2505.19702", "abs": "https://arxiv.org/abs/2505.19702", "authors": ["Minheng Ni", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Wangmeng Zuo", "Lijuan Wang"], "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large language models have significantly improved textual\nreasoning through the effective use of Chain-of-Thought (CoT) and reinforcement\nlearning. However, extending these successes to vision-language tasks remains\nchallenging due to inherent limitations in text-only CoT, such as visual\nhallucinations and insufficient multimodal integration. In this paper, we\nintroduce Point-RFT, a multimodal reasoning framework explicitly designed to\nleverage visually grounded CoT reasoning for visual document understanding. Our\napproach consists of two stages: First, we conduct format finetuning using a\ncurated dataset of 71K diverse visual reasoning problems, each annotated with\ndetailed, step-by-step rationales explicitly grounded to corresponding visual\nelements. Second, we employ reinforcement finetuning targeting visual document\nunderstanding. On ChartQA, our approach improves accuracy from 70.88%\n(format-finetuned baseline) to 90.04%, surpassing the 83.92% accuracy achieved\nby reinforcement finetuning relying solely on text-based CoT. The result shows\nthat our grounded CoT is more effective for multimodal reasoning compared with\nthe text-only CoT. Moreover, Point-RFT exhibits superior generalization\ncapability across several out-of-domain visual document reasoning benchmarks,\nincluding CharXiv, PlotQA, IconQA, TabMWP, etc., and highlights its potential\nin complex real-world scenarios."}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715", "abs": "https://arxiv.org/abs/2505.19715", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "title": "Graceful Forgetting in Generative Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance."}
{"id": "2505.19707", "pdf": "https://arxiv.org/pdf/2505.19707", "abs": "https://arxiv.org/abs/2505.19707", "authors": ["Rong-Cheng Tu", "Zhao Jin", "Jingyi Liao", "Xiao Luo", "Yingjie Wang", "Li Shen", "Dacheng Tao"], "title": "MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Existing Zero-Shot Composed Image Retrieval (ZS-CIR) methods typically train\nadapters that convert reference images into pseudo-text tokens, which are\nconcatenated with the modifying text and processed by frozen text encoders in\npretrained VLMs or LLMs. While this design leverages the strengths of large\npretrained models, it only supervises the adapter to produce encoder-compatible\ntokens that loosely preserve visual semantics. Crucially, it does not directly\noptimize the composed query representation to capture the full intent of the\ncomposition or to align with the target semantics, thereby limiting retrieval\nperformance, particularly in cases involving fine-grained or complex visual\ntransformations. To address this problem, we propose MLLM-Guided VLM\nFine-Tuning with Joint Inference (MVFT-JI), a novel approach that leverages a\npretrained multimodal large language model (MLLM) to construct two\ncomplementary training tasks using only unlabeled images: target text retrieval\ntaskand text-to-image retrieval task. By jointly optimizing these tasks, our\nmethod enables the VLM to inherently acquire robust compositional retrieval\ncapabilities, supported by the provided theoretical justifications and\nempirical validation. Furthermore, during inference, we further prompt the MLLM\nto generate target texts from composed queries and compute retrieval scores by\nintegrating similarities between (i) the composed query and candidate images,\nand (ii) the MLLM-generated target text and candidate images. This strategy\neffectively combines the VLM's semantic alignment strengths with the MLLM's\nreasoning capabilities."}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722", "abs": "https://arxiv.org/abs/2505.19722", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework."}
{"id": "2505.19733", "pdf": "https://arxiv.org/pdf/2505.19733", "abs": "https://arxiv.org/abs/2505.19733", "authors": ["Alou Diakite", "Cheng Li", "Lei Xie", "Yuanjing Feng", "Ruoyou Wu", "Jianzhong He", "Hairong Zheng", "Shanshan Wang"], "title": "Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation", "categories": ["cs.CV", "cs.CE"], "comment": null, "summary": "Accurately delineating the visual pathway (VP) is crucial for understanding\nthe human visual system and diagnosing related disorders. Exploring\nmulti-parametric MR imaging data has been identified as an important way to\ndelineate VP. However, due to the complex cross-sequence relationships,\nexisting methods cannot effectively model the complementary information from\ndifferent MRI sequences. In addition, these existing methods heavily rely on\nlarge training data with labels, which is labor-intensive and time-consuming to\nobtain. In this work, we propose a novel semi-supervised multi-parametric\nfeature decomposition framework for VP delineation. Specifically, a\ncorrelation-constrained feature decomposition (CFD) is designed to handle the\ncomplex cross-sequence relationships by capturing the unique characteristics of\neach MRI sequence and easing the multi-parametric information fusion process.\nFurthermore, a consistency-based sample enhancement (CSE) module is developed\nto address the limited labeled data issue, by generating and promoting\nmeaningful edge information from unlabeled data. We validate our framework\nusing two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM)\ndataset. Experimental results demonstrate the superiority of our approach in\nterms of delineation performance when compared to seven state-of-the-art\napproaches."}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743", "abs": "https://arxiv.org/abs/2505.19743", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs."}
{"id": "2505.19742", "pdf": "https://arxiv.org/pdf/2505.19742", "abs": "https://arxiv.org/abs/2505.19742", "authors": ["Jue Gong", "Tingyu Yang", "Jingkai Wang", "Zheng Chen", "Xing Liu", "Hong Gu", "Yulun Zhang", "Xiaokang Yang"], "title": "HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance", "categories": ["cs.CV"], "comment": "9 pages, 8 figures. The code and model will be available at\n  https://github.com/gobunu/HAODiff", "summary": "Human-centered images often suffer from severe generic degradation during\ntransmission and are prone to human motion blur (HMB), making restoration\nchallenging. Existing research lacks sufficient focus on these issues, as both\nproblems often coexist in practice. To address this, we design a degradation\npipeline that simulates the coexistence of HMB and generic noise, generating\nsynthetic degraded data to train our proposed HAODiff, a human-aware one-step\ndiffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG),\nwhich leverages high-quality images, residual noise (LQ minus HQ), and HMB\nsegmentation masks as training targets. It produces a positive-negative prompt\npair for classifier-free guidance (CFG) in a single diffusion step. The\nresulting adaptive dual prompts let HAODiff exploit CFG more effectively,\nboosting robustness against diverse degradations. For fair evaluation, we\nintroduce MPII-Test, a benchmark rich in combined noise and HMB cases.\nExtensive experiments show that our HAODiff surpasses existing state-of-the-art\n(SOTA) methods in terms of both quantitative metrics and visual quality on\nsynthetic and real-world datasets, including our introduced MPII-Test. Code is\navailable at: https://github.com/gobunu/HAODiff."}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754", "abs": "https://arxiv.org/abs/2505.19754", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG."}
{"id": "2505.19746", "pdf": "https://arxiv.org/pdf/2505.19746", "abs": "https://arxiv.org/abs/2505.19746", "authors": ["Jakov Samardija", "Donik Vrnak", "Sven Lonari"], "title": "Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of acute cellular rejection (ACR) in endomyocardial\nbiopsies is essential for effective management of heart transplant patients.\nHowever, the rarity of high-grade rejection cases (3R) presents a significant\nchallenge for training robust deep learning models. This work addresses the\nclass imbalance problem by leveraging synthetic data generation using StyleGAN\nto augment the limited number of real 3R images. Prior to GAN training,\nhistogram equalization was applied to standardize image appearance and improve\nthe consistency of tissue representation. StyleGAN was trained on available 3R\nbiopsy patches and subsequently used to generate 10,000 realistic synthetic\nimages. These were combined with real 0R samples, that is samples without\nrejection, in various configurations to train ResNet-18 classifiers for binary\nrejection classification.\n  Three classifier variants were evaluated: one trained on real 0R and\nsynthetic 3R images, another using both synthetic and additional real samples,\nand a third trained solely on real data. All models were tested on an\nindependent set of real biopsy images. Results demonstrate that synthetic data\nimproves classification performance, particularly when used in combination with\nreal samples. The highest-performing model, which used both real and synthetic\nimages, achieved strong precision and recall for both classes. These findings\nunderscore the value of hybrid training strategies and highlight the potential\nof GAN-based data augmentation in biomedical image analysis, especially in\ndomains constrained by limited annotated datasets."}
{"id": "2505.19756", "pdf": "https://arxiv.org/pdf/2505.19756", "abs": "https://arxiv.org/abs/2505.19756", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "title": "Efficient Reasoning via Chain of Unconscious Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT"}
{"id": "2505.19750", "pdf": "https://arxiv.org/pdf/2505.19750", "abs": "https://arxiv.org/abs/2505.19750", "authors": ["Huaiyuan Zhang", "Hang Chen", "Yu Cheng", "Shunyi Wu", "Linghao Sun", "Linao Han", "Zeyu Shi", "Lei Qi"], "title": "SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect", "categories": ["cs.CV"], "comment": null, "summary": "In this technical report, we present our solution to the CVPR 2025 Visual\nAnomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt &\nDetect: Robust Anomaly Detection in Real-World Applications. In real-world\nindustrial anomaly detection, it is crucial to accurately identify anomalies\nwith physical complexity, such as transparent or reflective surfaces,\nocclusions, and low-contrast contaminations. The recently proposed MVTec AD 2\ndataset significantly narrows the gap between publicly available benchmarks and\nanomalies found in real-world industrial environments. To address the\nchallenges posed by this dataset--such as complex and varying lighting\nconditions and real anomalies with large scale differences--we propose a fully\ntraining-free anomaly detection and segmentation method based on feature\nextraction using the DINOv2 model named SuperAD. Our method carefully selects a\nsmall number of normal reference images and constructs a memory bank by\nleveraging the strong representational power of DINOv2. Anomalies are then\nsegmented by performing nearest neighbor matching between test image features\nand the memory bank. Our method achieves competitive results on both test sets\nof the MVTec AD 2 dataset."}
{"id": "2505.19766", "pdf": "https://arxiv.org/pdf/2505.19766", "abs": "https://arxiv.org/abs/2505.19766", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol."}
{"id": "2505.19751", "pdf": "https://arxiv.org/pdf/2505.19751", "abs": "https://arxiv.org/abs/2505.19751", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldo", "Moussab Bennehar", "Dzmitry Tsishkou", "Cline Loscos", "Dsir Sidib"], "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online."}
{"id": "2505.19768", "pdf": "https://arxiv.org/pdf/2505.19768", "abs": "https://arxiv.org/abs/2505.19768", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased."}
{"id": "2505.19793", "pdf": "https://arxiv.org/pdf/2505.19793", "abs": "https://arxiv.org/abs/2505.19793", "authors": ["Li Fang", "Hao Zhu", "Longlong Chen", "Fei Hu", "Long Ye", "Zhan Ma"], "title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent advancements in generalizable novel view synthesis have achieved\nimpressive quality through interpolation between nearby views. However,\nrendering high-resolution images remains computationally intensive due to the\nneed for dense sampling of all rays. Recognizing that natural scenes are\ntypically piecewise smooth and sampling all rays is often redundant, we propose\na novel depth-guided bundle sampling strategy to accelerate rendering. By\ngrouping adjacent rays into a bundle and sampling them collectively, a shared\nrepresentation is generated for decoding all rays within the bundle. To further\noptimize efficiency, our adaptive sampling strategy dynamically allocates\nsamples based on depth confidence, concentrating more samples in complex\nregions while reducing them in smoother areas. When applied to ENeRF, our\nmethod achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on\nthe DTU dataset. Extensive experiments on synthetic and real-world datasets\ndemonstrate state-of-the-art rendering quality and up to 2x faster rendering\ncompared to existing generalizable methods. Code is available at\nhttps://github.com/KLMAV-CUC/GDB-NeRF."}
{"id": "2505.19773", "pdf": "https://arxiv.org/pdf/2505.19773", "abs": "https://arxiv.org/abs/2505.19773", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "categories": ["cs.CL", "cs.CR"], "comment": "Accepted by ACL 2025", "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms."}
{"id": "2505.19795", "pdf": "https://arxiv.org/pdf/2505.19795", "abs": "https://arxiv.org/abs/2505.19795", "authors": ["Sajjad Shahabodini", "Mobina Mansoori", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P."}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776", "abs": "https://arxiv.org/abs/2505.19776", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts."}
{"id": "2505.19799", "pdf": "https://arxiv.org/pdf/2505.19799", "abs": "https://arxiv.org/abs/2505.19799", "authors": ["Yulu Bai", "Jiahong Fu", "Qi Xie", "Deyu Meng"], "title": "A Regularization-Guided Equivariant Approach for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Equivariant and invariant deep learning models have been developed to exploit\nintrinsic symmetries in data, demonstrating significant effectiveness in\ncertain scenarios. However, these methods often suffer from limited\nrepresentation accuracy and rely on strict symmetry assumptions that may not\nhold in practice. These limitations pose a significant drawback for image\nrestoration tasks, which demands high accuracy and precise symmetry\nrepresentation. To address these challenges, we propose a rotation-equivariant\nregularization strategy that adaptively enforces the appropriate symmetry\nconstraints on the data while preserving the network's representational\naccuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance\nrotation equivariance, which innovatively extends the insights of\ndata-augmentation-based and equivariant-based methodologies. This is achieved\nthrough self-supervised learning and the spatial rotation and cyclic channel\nshift of feature maps deduce in the equivariant framework. Our approach firstly\nenables a non-strictly equivariant network suitable for image restoration,\nproviding a simple and adaptive mechanism for adjusting equivariance based on\ntask. Extensive experiments across three low-level tasks demonstrate the\nsuperior accuracy and generalization capability of our method, outperforming\nstate-of-the-art approaches."}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797", "abs": "https://arxiv.org/abs/2505.19797", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers"}
{"id": "2505.19805", "pdf": "https://arxiv.org/pdf/2505.19805", "abs": "https://arxiv.org/abs/2505.19805", "authors": ["Jrmy Scanvic", "Quentin Barthlemy", "Julin Tachella"], "title": "Translation-Equivariance of Normalization Layers and Aliasing in Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "The design of convolutional neural architectures that are exactly equivariant\nto continuous translations is an active field of research. It promises to\nbenefit scientific computing, notably by making existing imaging systems more\nphysically accurate. Most efforts focus on the design of downsampling/pooling\nlayers, upsampling layers and activation functions, but little attention is\ndedicated to normalization layers. In this work, we present a novel theoretical\nframework for understanding the equivariance of normalization layers to\ndiscrete shifts and continuous translations. We also determine necessary and\nsufficient conditions for normalization layers to be equivariant in terms of\nthe dimensions they operate on. Using real feature maps from ResNet-18 and\nImageNet, we test those theoretical results empirically and find that they are\nconsistent with our predictions."}
{"id": "2505.19800", "pdf": "https://arxiv.org/pdf/2505.19800", "abs": "https://arxiv.org/abs/2505.19800", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community."}
{"id": "2505.19812", "pdf": "https://arxiv.org/pdf/2505.19812", "abs": "https://arxiv.org/abs/2505.19812", "authors": ["Zehong Ma", "Shiliang Zhang", "Longhui Wei", "Qi Tian"], "title": "Efficient Multi-modal Long Context Learning for Training-free Adaptation", "categories": ["cs.CV"], "comment": "Accepted to ICML2025", "summary": "Traditional approaches to adapting multi-modal large language models (MLLMs)\nto new tasks have relied heavily on fine-tuning. This paper introduces\nEfficient Multi-Modal Long Context Learning (EMLoC), a novel training-free\nalternative that embeds demonstration examples directly into the model input.\nEMLoC offers a more efficient, flexible, and scalable solution for task\nadaptation. Because extremely lengthy inputs introduce prohibitive\ncomputational and memory overhead, EMLoC contributes a chunk-wise compression\nmechanism combined with layer-wise adaptive pruning. It condenses long-context\nmultimodal inputs into compact, task-specific memory representations. By\nadaptively pruning tokens at each layer under a Jensen-Shannon divergence\nconstraint, our method achieves a dramatic reduction in inference complexity\nwithout sacrificing performance. This approach is the first to seamlessly\nintegrate compression and pruning techniques for multi-modal long-context\nlearning, offering a scalable and efficient solution for real-world\napplications. Extensive experiments on diverse vision-language benchmarks\ndemonstrate that EMLoC achieves performance on par with or superior to naive\nlong-context approaches. Our results highlight the potential of EMLoC as a\ngroundbreaking framework for efficient and flexible adaptation of multi-modal\nmodels in resource-constrained environments. Codes are publicly available at\nhttps://github.com/Zehong-Ma/EMLoC."}
{"id": "2505.19804", "pdf": "https://arxiv.org/pdf/2505.19804", "abs": "https://arxiv.org/abs/2505.19804", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation."}
{"id": "2505.19813", "pdf": "https://arxiv.org/pdf/2505.19813", "abs": "https://arxiv.org/abs/2505.19813", "authors": ["You Wang", "Li Fang", "Hao Zhu", "Fei Hu", "Long Ye", "Zhan Ma"], "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Neural Radiance Fields (NeRF) have transformed novel view synthesis by\nmodeling scene-specific volumetric representations directly from images. While\ngeneralizable NeRF models can generate novel views across unknown scenes by\nlearning latent ray representations, their performance heavily depends on a\nlarge number of multi-view observations. However, with limited input views,\nthese methods experience significant degradation in rendering quality. To\naddress this limitation, we propose GoLF-NRT: a Global and Local feature\nFusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable\nneural rendering from few input views by leveraging a 3D transformer with\nefficient sparse attention to capture global scene context. In parallel, it\nintegrates local geometric features extracted along the epipolar line, enabling\nhigh-quality scene reconstruction from as few as 1 to 3 input views.\nFurthermore, we introduce an adaptive sampling strategy based on attention\nweights and kernel regression, improving the accuracy of transformer-based\nneural rendering. Extensive experiments on public datasets show that GoLF-NRT\nachieves state-of-the-art performance across varying numbers of input views,\nhighlighting the effectiveness and superiority of our approach. Code is\navailable at https://github.com/KLMAV-CUC/GoLF-NRT."}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806", "abs": "https://arxiv.org/abs/2505.19806", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness."}
{"id": "2505.19846", "pdf": "https://arxiv.org/pdf/2505.19846", "abs": "https://arxiv.org/abs/2505.19846", "authors": ["Nagito Saito", "Shintaro Ito", "Koichi Ito", "Takafumi Aoki"], "title": "Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "Semantic segmentation is a fundamental task in medical image analysis and\nautonomous driving and has a problem with the high cost of annotating the\nlabels required in training. To address this problem, semantic segmentation\nmethods based on semi-supervised learning with a small number of labeled data\nhave been proposed. For example, one approach is to train a semantic\nsegmentation model using images with annotated labels and pseudo labels. In\nthis approach, the accuracy of the semantic segmentation model depends on the\nquality of the pseudo labels, and the quality of the pseudo labels depends on\nthe performance of the model to be trained and the amount of data with\nannotated labels. In this paper, we generate pseudo labels using zero-shot\nannotation with the Segment Anything Model (SAM) and Contrastive Language-Image\nPretraining (CLIP), improve the accuracy of the pseudo labels using the Unified\nDual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels\nto train a semantic segmentation model. The effectiveness of the proposed\nmethod is demonstrated through the experiments using the public datasets:\nPASCAL and MS COCO."}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815", "abs": "https://arxiv.org/abs/2505.19815", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques."}
{"id": "2505.19853", "pdf": "https://arxiv.org/pdf/2505.19853", "abs": "https://arxiv.org/abs/2505.19853", "authors": ["Miaoyu Li", "Qin Chao", "Boyang Li"], "title": "Two Causally Related Needles in a Video Haystack", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs."}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838", "abs": "https://arxiv.org/abs/2505.19838", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes."}
{"id": "2505.19854", "pdf": "https://arxiv.org/pdf/2505.19854", "abs": "https://arxiv.org/abs/2505.19854", "authors": ["Natsuki Takama", "Shintaro Ito", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "Gaussian Splatting (GS) has gained attention as a fast and effective method\nfor novel view synthesis. It has also been applied to 3D reconstruction using\nmulti-view images and can achieve fast and accurate 3D reconstruction. However,\nGS assumes that the input contains a large number of multi-view images, and\ntherefore, the reconstruction accuracy significantly decreases when only a\nlimited number of input images are available. One of the main reasons is the\ninsufficient number of 3D points in the sparse point cloud obtained through\nStructure from Motion (SfM), which results in a poor initialization for\noptimizing the Gaussian primitives. We propose a new 3D reconstruction method,\ncalled Sparse2DGS, to enhance 2DGS in reconstructing objects using only three\nimages. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along\nwith COLMAP MVS to generate highly accurate and dense 3D point clouds, which\nare then used to initialize 2D Gaussians. Through experiments on the DTU\ndataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of\nobjects using just three images."}
{"id": "2505.19848", "pdf": "https://arxiv.org/pdf/2505.19848", "abs": "https://arxiv.org/abs/2505.19848", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "title": "Improving Multilingual Math Reasoning for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model."}
{"id": "2505.19858", "pdf": "https://arxiv.org/pdf/2505.19858", "abs": "https://arxiv.org/abs/2505.19858", "authors": ["Zixiang Zhao", "Haowen Bai", "Bingxin Ke", "Yukun Cui", "Lilun Deng", "Yulun Zhang", "Kai Zhang", "Konrad Schindler"], "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "The real world is dynamic, yet most image fusion methods process static\nframes independently, ignoring temporal correlations in videos and leading to\nflickering and temporal inconsistency. To address this, we propose Unified\nVideo Fusion (UniVF), a novel framework for temporally coherent video fusion\nthat leverages multi-frame learning and optical flow-based feature warping for\ninformative, temporally coherent video fusion. To support its development, we\nalso introduce Video Fusion Benchmark (VF-Bench), the first comprehensive\nbenchmark covering four video fusion tasks: multi-exposure, multi-focus,\ninfrared-visible, and medical fusion. VF-Bench provides high-quality,\nwell-aligned video pairs obtained through synthetic data generation and\nrigorous curation from existing datasets, with a unified evaluation protocol\nthat jointly assesses the spatial quality and temporal consistency of video\nfusion. Extensive experiments show that UniVF achieves state-of-the-art results\nacross all tasks on VF-Bench. Project page: https://vfbench.github.io."}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851", "abs": "https://arxiv.org/abs/2505.19851", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead."}
{"id": "2505.19863", "pdf": "https://arxiv.org/pdf/2505.19863", "abs": "https://arxiv.org/abs/2505.19863", "authors": ["Lukas Meyer", "Andrei-Timotei Ardelean", "Tim Weyrich", "Marc Stamminger"], "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields", "categories": ["cs.CV", "cs.LG"], "comment": "for project website, see https://meyerls.github.io/fruit_nerfpp", "summary": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods."}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862", "abs": "https://arxiv.org/abs/2505.19862", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL."}
{"id": "2505.19868", "pdf": "https://arxiv.org/pdf/2505.19868", "abs": "https://arxiv.org/abs/2505.19868", "authors": ["Junhong Lee", "Seungwook Kim", "Minsu Cho"], "title": "Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects."}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912", "abs": "https://arxiv.org/abs/2505.19912", "authors": ["Javier Marn"], "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining."}
{"id": "2505.19873", "pdf": "https://arxiv.org/pdf/2505.19873", "abs": "https://arxiv.org/abs/2505.19873", "authors": ["Yanqi Cheng", "Tieyong Zeng", "Pietro Lio", "Carola-Bibiane Schnlieb", "Angelica I Aviles-Rivero"], "title": "Deep Spectral Prior", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "We introduce Deep Spectral Prior (DSP), a new formulation of Deep Image Prior\n(DIP) that redefines image reconstruction as a frequency-domain alignment\nproblem. Unlike traditional DIP, which relies on pixel-wise loss and early\nstopping to mitigate overfitting, DSP directly matches Fourier coefficients\nbetween the network output and observed measurements. This shift introduces an\nexplicit inductive bias towards spectral coherence, aligning with the known\nfrequency structure of images and the spectral bias of convolutional neural\nnetworks. We provide a rigorous theoretical framework demonstrating that DSP\nacts as an implicit spectral regulariser, suppressing high-frequency noise by\ndesign and eliminating the need for early stopping. Our analysis spans four\ncore dimensions establishing smooth convergence dynamics, local stability, and\nfavourable bias-variance tradeoffs. We further show that DSP naturally projects\nreconstructions onto a frequency-consistent manifold, enhancing\ninterpretability and robustness. These theoretical guarantees are supported by\nempirical results across denoising, inpainting, and super-resolution tasks,\nwhere DSP consistently outperforms classical DIP and other unsupervised\nbaselines."}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914", "abs": "https://arxiv.org/abs/2505.19914", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io."}
{"id": "2505.19874", "pdf": "https://arxiv.org/pdf/2505.19874", "abs": "https://arxiv.org/abs/2505.19874", "authors": ["Yi Wu", "Lingting Zhu", "Shengju Qian", "Lei Liu", "Wandi Qiao", "Lequan Yu", "Bin Li"], "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In the current research landscape, multimodal autoregressive (AR) models have\nshown exceptional capabilities across various domains, including visual\nunderstanding and generation. However, complex tasks such as style-aligned\ntext-to-image generation present significant challenges, particularly in data\nacquisition. In analogy to instruction-following tuning for image editing of AR\nmodels, style-aligned generation requires a reference style image and prompt,\nresulting in a text-image-to-image triplet where the output shares the style\nand semantics of the input. However, acquiring large volumes of such triplet\ndata with specific styles is considerably more challenging than obtaining\nconventional text-to-image data used for training generative models. To address\nthis issue, we propose StyleAR, an innovative approach that combines a\nspecially designed data curation method with our proposed AR models to\neffectively utilize text-to-image binary data for style-aligned text-to-image\ngeneration. Our method synthesizes target stylized data using a reference style\nimage and prompt, but only incorporates the target stylized image as the image\nmodality to create high-quality binary data. To facilitate binary data\ntraining, we introduce a CLIP image encoder with a perceiver resampler that\ntranslates the image input into style tokens aligned with multimodal tokens in\nAR models and implement a style-enhanced token technique to prevent content\nleakage which is a common issue in previous work. Furthermore, we mix raw\nimages drawn from large-scale text-image datasets with stylized images to\nenhance StyleAR's ability to extract richer stylistic features and ensure style\nconsistency. Extensive qualitative and quantitative experiments demonstrate our\nsuperior performance."}
{"id": "2505.19937", "pdf": "https://arxiv.org/pdf/2505.19937", "abs": "https://arxiv.org/abs/2505.19937", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks."}
{"id": "2505.19877", "pdf": "https://arxiv.org/pdf/2505.19877", "abs": "https://arxiv.org/abs/2505.19877", "authors": ["Chao Huang", "Benfeng Wang", "Jie Wen", "Chengliang Liu", "Wei Wang", "Li Shen", "Xiaochun Cao"], "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "Recent advancements in reasoning capability of Multimodal Large Language\nModels (MLLMs) demonstrate its effectiveness in tackling complex visual tasks.\nHowever, existing MLLM-based Video Anomaly Detection (VAD) methods remain\nlimited to shallow anomaly descriptions without deep reasoning. In this paper,\nwe propose a new task named Video Anomaly Reasoning (VAR), which aims to enable\ndeep analysis and understanding of anomalies in the video by requiring MLLMs to\nthink explicitly before answering. To this end, we propose Vad-R1, an\nend-to-end MLLM-based framework for VAR. Specifically, we design a\nPerception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human\nprocess of recognizing anomalies, guiding the MLLM to reason anomaly\nstep-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a\ndedicated dataset for VAR. Furthermore, we propose an improved reinforcement\nlearning algorithm AVA-GRPO, which explicitly incentivizes the anomaly\nreasoning capability of MLLMs through a self-verification mechanism with\nlimited annotations. Experimental results demonstrate that Vad-R1 achieves\nsuperior performance, outperforming both open-source and proprietary models on\nVAD and VAR tasks. Codes and datasets will be released at\nhttps://github.com/wbfwonderful/Vad-R1."}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959", "abs": "https://arxiv.org/abs/2505.19959", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial."}
{"id": "2505.19883", "pdf": "https://arxiv.org/pdf/2505.19883", "abs": "https://arxiv.org/abs/2505.19883", "authors": ["Shintaro Ito", "Natsuki Takama", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization", "categories": ["cs.CV"], "comment": "Accepted to ICIP2025", "summary": "The use of multi-view images acquired by a 360-degree camera can reconstruct\na 3D space with a wide area. There are 3D reconstruction methods from\nequirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis\n(NVS) methods. On the other hand, it is necessary to overcome the large\ndistortion caused by the projection model of a 360-degree camera when\nequirectangular images are used. In 3DGS-based methods, the large distortion of\nthe 360-degree camera model generates extremely large 3D Gaussians, resulting\nin poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based\non 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering\naccuracy improvement techniques: geometric regularization, scale\nregularization, and distortion-aware weights and a mask to suppress the effects\nof obstacles in equirectangular images. Through experiments on public datasets,\nwe demonstrate that ErpGS can render novel view images more accurately than\nconventional methods."}
{"id": "2505.19970", "pdf": "https://arxiv.org/pdf/2505.19970", "abs": "https://arxiv.org/abs/2505.19970", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness."}
{"id": "2505.19889", "pdf": "https://arxiv.org/pdf/2505.19889", "abs": "https://arxiv.org/abs/2505.19889", "authors": ["David Schneider", "Zdravko Marinov", "Rafael Baur", "Zeyun Zhong", "Rodi Dger", "Rainer Stiefelhagen"], "title": "OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection", "categories": ["cs.CV", "I.2.10; I.5.4"], "comment": null, "summary": "Current video-based fall detection research mostly relies on small, staged\ndatasets with significant domain biases concerning background, lighting, and\ncamera setup resulting in unknown real-world performance. We introduce\nOmniFall, unifying eight public fall detection datasets (roughly 14 h of\nrecordings, roughly 42 h of multiview data, 101 subjects, 29 camera views)\nunder a consistent ten-class taxonomy with standardized evaluation protocols.\nOur benchmark provides complete video segmentation labels and enables fair\ncross-dataset comparison previously impossible with incompatible annotation\nschemes. For real-world evaluation we curate OOPS-Fall from genuine accident\nvideos and establish a staged-to-wild protocol measuring generalization from\ncontrolled to uncontrolled environments. Experiments with frozen pre-trained\nbackbones such as I3D or VideoMAE reveal significant performance gaps between\nin-distribution and in-the-wild scenarios, highlighting critical challenges in\ndeveloping robust fall detection systems. OmniFall Dataset at\nhttps://huggingface.co/datasets/simplexsigil2/omnifall , Code at\nhttps://github.com/simplexsigil/omnifall-experiments"}
{"id": "2505.19971", "pdf": "https://arxiv.org/pdf/2505.19971", "abs": "https://arxiv.org/abs/2505.19971", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "categories": ["cs.CL"], "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "summary": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation."}
{"id": "2505.19895", "pdf": "https://arxiv.org/pdf/2505.19895", "abs": "https://arxiv.org/abs/2505.19895", "authors": ["Afrah Shaahid", "Muzammil Behzad"], "title": "Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Underwater images are often affected by complex degradations such as light\nabsorption, scattering, color casts, and artifacts, making enhancement critical\nfor effective object detection, recognition, and scene understanding in aquatic\nenvironments. Existing methods, especially diffusion-based approaches,\ntypically rely on synthetic paired datasets due to the scarcity of real\nunderwater references, introducing bias and limiting generalization.\nFurthermore, fine-tuning these models can degrade learned priors, resulting in\nunrealistic enhancements due to domain shifts. To address these challenges, we\npropose UDAN-CLIP, an image-to-image diffusion framework pre-trained on\nsynthetic underwater datasets and enhanced with a customized classifier based\non vision-language model, a spatial attention module, and a novel\nCLIP-Diffusion loss. The classifier preserves natural in-air priors and\nsemantically guides the diffusion process, while the spatial attention module\nfocuses on correcting localized degradations such as haze and low contrast. The\nproposed CLIP-Diffusion loss further strengthens visual-textual alignment and\nhelps maintain semantic consistency during enhancement. The proposed\ncontributions empower our UDAN-CLIP model to perform more effective underwater\nimage enhancement, producing results that are not only visually compelling but\nalso more realistic and detail-preserving. These improvements are consistently\nvalidated through both quantitative metrics and qualitative visual comparisons,\ndemonstrating the model's ability to correct distortions and restore natural\nappearance in challenging underwater conditions."}
{"id": "2505.19978", "pdf": "https://arxiv.org/pdf/2505.19978", "abs": "https://arxiv.org/abs/2505.19978", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "summary": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations."}
{"id": "2505.19901", "pdf": "https://arxiv.org/pdf/2505.19901", "abs": "https://arxiv.org/abs/2505.19901", "authors": ["Peng Liu", "Xiaoming Ren", "Fengkai Liu", "Qingsong Xie", "Quanlong Zheng", "Yanhao Zhang", "Haonan Lu", "Yujiu Yang"], "title": "Dynamic-I2V: Exploring Image-to-Video Generaion Models via Multimodal LLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in image-to-video (I2V) generation have shown promising\nperformance in conventional scenarios. However, these methods still encounter\nsignificant challenges when dealing with complex scenes that require a deep\nunderstanding of nuanced motion and intricate object-action relationships. To\naddress these challenges, we present Dynamic-I2V, an innovative framework that\nintegrates Multimodal Large Language Models (MLLMs) to jointly encode visual\nand textual conditions for a diffusion transformer (DiT) architecture. By\nleveraging the advanced multimodal understanding capabilities of MLLMs, our\nmodel significantly improves motion controllability and temporal coherence in\nsynthesized videos. The inherent multimodality of Dynamic-I2V further enables\nflexible support for diverse conditional inputs, extending its applicability to\nvarious downstream generation tasks. Through systematic analysis, we identify a\ncritical limitation in current I2V benchmarks: a significant bias towards\nfavoring low-dynamic videos, stemming from an inadequate balance between motion\ncomplexity and visual quality metrics. To resolve this evaluation gap, we\npropose DIVE - a novel assessment benchmark specifically designed for\ncomprehensive dynamic quality measurement in I2V generation. In conclusion,\nextensive quantitative and qualitative experiments confirm that Dynamic-I2V\nattains state-of-the-art performance in image-to-video generation, particularly\nrevealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,\ncontrollability, and quality, respectively, as assessed by the DIVE metric in\ncomparison to existing methods."}
{"id": "2505.19987", "pdf": "https://arxiv.org/pdf/2505.19987", "abs": "https://arxiv.org/abs/2505.19987", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts."}
{"id": "2505.19911", "pdf": "https://arxiv.org/pdf/2505.19911", "abs": "https://arxiv.org/abs/2505.19911", "authors": ["Xiaosen Wang", "Shaokang Wang", "Zhijin Ge", "Yuyang Luo", "Shudong Zhang"], "title": "Attention! You Vision Language Model Could Be Maliciously Manipulated", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have achieved remarkable success in\nunderstanding complex real-world scenarios and supporting data-driven\ndecision-making processes. However, VLMs exhibit significant vulnerability\nagainst adversarial examples, either text or image, which can lead to various\nadversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In\nthis work, we empirically and theoretically demonstrate that VLMs are\nparticularly susceptible to image-based adversarial examples, where\nimperceptible perturbations can precisely manipulate each output token. To this\nend, we propose a novel attack called Vision-language model Manipulation Attack\n(VMA), which integrates first-order and second-order momentum optimization\ntechniques with a differentiable transformation mechanism to effectively\noptimize the adversarial perturbation. Notably, VMA can be a double-edged\nsword: it can be leveraged to implement various attacks, such as jailbreaking,\nhijacking, privacy breaches, Denial-of-Service, and the generation of sponge\nexamples, etc, while simultaneously enabling the injection of watermarks for\ncopyright protection. Extensive empirical evaluations substantiate the efficacy\nand generalizability of VMA across diverse scenarios and datasets."}
{"id": "2505.20006", "pdf": "https://arxiv.org/pdf/2505.20006", "abs": "https://arxiv.org/abs/2505.20006", "authors": ["Raphal Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "categories": ["cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR."}
{"id": "2505.19919", "pdf": "https://arxiv.org/pdf/2505.19919", "abs": "https://arxiv.org/abs/2505.19919", "authors": ["Chen Sang", "Yeqiang Qian", "Jiale Zhang", "Chunxiang Wang", "Ming Yang"], "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time", "categories": ["cs.CV"], "comment": "Project homepage: https://weathermagician.github.io", "summary": "For tasks such as urban digital twins, VR/AR/game scene design, or creating\nsynthetic films, the traditional industrial approach often involves manually\nmodeling scenes and using various rendering engines to complete the rendering\nprocess. This approach typically requires high labor costs and hardware\ndemands, and can result in poor quality when replicating complex real-world\nscenes. A more efficient approach is to use data from captured real-world\nscenes, then apply reconstruction and rendering algorithms to quickly recreate\nthe authentic scene. However, current algorithms are unable to effectively\nreconstruct and render real-world weather effects. To address this, we propose\na framework based on gaussian splatting, that can reconstruct real scenes and\nrender them under synthesized 4D weather effects. Our work can simulate various\ncommon weather effects by applying Gaussians modeling and rendering techniques.\nIt supports continuous dynamic weather changes and can easily control the\ndetails of the effects. Additionally, our work has low hardware requirements\nand achieves real-time rendering performance. The result demos can be accessed\non our project homepage: weathermagician.github.io"}
{"id": "2505.20013", "pdf": "https://arxiv.org/pdf/2505.20013", "abs": "https://arxiv.org/abs/2505.20013", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents."}
{"id": "2505.19920", "pdf": "https://arxiv.org/pdf/2505.19920", "abs": "https://arxiv.org/abs/2505.19920", "authors": ["Sebastian Gro", "Stefan Heindorf", "Philipp Terhrst"], "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical."}
{"id": "2505.20014", "pdf": "https://arxiv.org/pdf/2505.20014", "abs": "https://arxiv.org/abs/2505.20014", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications."}
{"id": "2505.19928", "pdf": "https://arxiv.org/pdf/2505.19928", "abs": "https://arxiv.org/abs/2505.19928", "authors": ["Gabriele Lagani", "Fabrizio Falchi", "Claudio Gennaro", "Giuseppe Amato"], "title": "CA3D: Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce a deep learning solution for video activity\nrecognition that leverages an innovative combination of convolutional layers\nwith a linear-complexity attention mechanism. Moreover, we introduce a novel\nquantization mechanism to further improve the efficiency of our model during\nboth training and inference. Our model maintains a reduced computational cost,\nwhile preserving robust learning and generalization capabilities. Our approach\naddresses the issues related to the high computing requirements of current\nmodels, with the goal of achieving competitive accuracy on consumer and edge\ndevices, enabling smart home and smart healthcare applications where efficiency\nand privacy issues are of concern. We experimentally validate our model on\ndifferent established and publicly available video activity recognition\nbenchmarks, improving accuracy over alternative models at a competitive\ncomputing cost."}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015", "abs": "https://arxiv.org/abs/2505.20015", "authors": ["Ramon Ferrer-i-Cancho"], "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression."}
{"id": "2505.19938", "pdf": "https://arxiv.org/pdf/2505.19938", "abs": "https://arxiv.org/abs/2505.19938", "authors": ["Wenrui Li", "Penghong Wang", "Xingtao Wang", "Wangmeng Zuo", "Xiaopeng Fan", "Yonghong Tian"], "title": "Multi-Timescale Motion-Decoupled Spiking Transformer for Audio-Visual Zero-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by IEEE TCSVT", "summary": "Audio-visual zero-shot learning (ZSL) has been extensively researched for its\ncapability to classify video data from unseen classes during training.\nNevertheless, current methodologies often struggle with background scene biases\nand inadequate motion detail. This paper proposes a novel dual-stream\nMulti-Timescale Motion-Decoupled Spiking Transformer (MDST++), which decouples\ncontextual semantic information and sparse dynamic motion information. The\nrecurrent joint learning unit is proposed to extract contextual semantic\ninformation and capture joint knowledge across various modalities to understand\nthe environment of actions. By converting RGB images to events, our method\ncaptures motion information more accurately and mitigates background scene\nbiases. Moreover, we introduce a discrepancy analysis block to model audio\nmotion information. To enhance the robustness of SNNs in extracting temporal\nand motion cues, we dynamically adjust the threshold of Leaky\nIntegrate-and-Fire neurons based on global motion and contextual semantic\ninformation. Our experiments validate the effectiveness of MDST++,\ndemonstrating their consistent superiority over state-of-the-art methods on\nmainstream benchmarks. Additionally, incorporating motion and multi-timescale\ninformation significantly improves HM and ZSL accuracy by 26.2\\% and 39.9\\%."}
{"id": "2505.20016", "pdf": "https://arxiv.org/pdf/2505.20016", "abs": "https://arxiv.org/abs/2505.20016", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.20023", "pdf": "https://arxiv.org/pdf/2505.20023", "abs": "https://arxiv.org/abs/2505.20023", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories."}
{"id": "2505.19948", "pdf": "https://arxiv.org/pdf/2505.19948", "abs": "https://arxiv.org/abs/2505.19948", "authors": ["Gokul Adethya", "Bhanu Pratyush Mantha", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology."}
{"id": "2505.20045", "pdf": "https://arxiv.org/pdf/2505.20045", "abs": "https://arxiv.org/abs/2505.20045", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs."}
{"id": "2505.19952", "pdf": "https://arxiv.org/pdf/2505.19952", "abs": "https://arxiv.org/abs/2505.19952", "authors": ["Rong-Cheng Tu", "Wenhao Sun", "Hanzhe You", "Yingjie Wang", "Jiaxing Huang", "Li Shen", "Dacheng Tao"], "title": "Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images\ngiven a compositional query, consisting of a reference image and a modifying\ntext-without relying on annotated training data. Existing approaches often\ngenerate a synthetic target text using large language models (LLMs) to serve as\nan intermediate anchor between the compositional query and the target image.\nModels are then trained to align the compositional query with the generated\ntext, and separately align images with their corresponding texts using\ncontrastive learning. However, this reliance on intermediate text introduces\nerror propagation, as inaccuracies in query-to-text and text-to-image mappings\naccumulate, ultimately degrading retrieval performance. To address these\nproblems, we propose a novel framework by employing a Multimodal Reasoning\nAgent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries\nby directly constructing triplets, <reference image, modification text, target\nimage>, using only unlabeled image data. By training on these synthetic\ntriplets, our model learns to capture the relationships between compositional\nqueries and candidate images directly. Extensive experiments on three standard\nCIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ\ndataset, our method improves Average R@10 by at least 7.5\\% over existing\nbaselines; on CIRR, it boosts R@1 by 9.6\\%; and on CIRCO, it increases mAP@5 by\n9.5\\%."}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047", "abs": "https://arxiv.org/abs/2505.20047", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline."}
{"id": "2505.19958", "pdf": "https://arxiv.org/pdf/2505.19958", "abs": "https://arxiv.org/abs/2505.19958", "authors": ["Yong Liu", "Jinshan Pan", "Yinchuan Li", "Qingji Dong", "Chao Zhu", "Yu Guo", "Fei Wang"], "title": "UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space", "categories": ["cs.CV"], "comment": "Under review, 10 pages, 7 figures", "summary": "Diffusion models have shown great potential in generating realistic image\ndetail. However, adapting these models to video super-resolution (VSR) remains\nchallenging due to their inherent stochasticity and lack of temporal modeling.\nIn this paper, we propose UltraVSR, a novel framework that enables\nultra-realistic and temporal-coherent VSR through an efficient one-step\ndiffusion space. A central component of UltraVSR is the Degradation-aware\nRestoration Schedule (DRS), which estimates a degradation factor from the\nlow-resolution input and transforms iterative denoising process into a\nsingle-step reconstruction from from low-resolution to high-resolution videos.\nThis design eliminates randomness from diffusion noise and significantly speeds\nup inference. To ensure temporal consistency, we propose a lightweight yet\neffective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution\nunit and an RTS-attention unit. By partially shifting feature components along\nthe temporal dimension, these two units collaboratively facilitate effective\nfeature propagation, fusion, and alignment across neighboring frames, without\nrelying on explicit temporal layers. The RTS module is integrated into a\npretrained text-to-image diffusion model and is further enhanced through\nSpatio-temporal Joint Distillation (SJD), which improves temporal coherence\nwhile preserving realistic details. Additionally, we introduce a Temporally\nAsynchronous Inference (TAI) strategy to capture long-range temporal\ndependencies under limited memory constraints. Extensive experiments show that\nUltraVSR achieves state-of-the-art performance, both qualitatively and\nquantitatively, in a single sampling step."}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072", "abs": "https://arxiv.org/abs/2505.20072", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "title": "Incentivizing Reasoning from Weak Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR."}
{"id": "2505.19972", "pdf": "https://arxiv.org/pdf/2505.19972", "abs": "https://arxiv.org/abs/2505.19972", "authors": ["Kanglei Zhou", "Hubert P. H. Shum", "Frederick W. B. Li", "Xingxing Zhang", "Xiaohui Liang"], "title": "PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Image Processing", "summary": "Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative\nperformance of actions in long videos. However, existing methods face\nchallenges due to domain shifts between the pre-trained large-scale action\nrecognition backbones and the specific AQA task, thereby hindering their\nperformance. This arises since fine-tuning resource-intensive backbones on\nsmall AQA datasets is impractical. We address this by identifying two levels of\ndomain shift: task-level, regarding differences in task objectives, and\nfeature-level, regarding differences in important features. For feature-level\nshifts, which are more detrimental, we propose Progressive Hierarchical\nInstruction (PHI) with two strategies. First, Gap Minimization Flow (GMF)\nleverages flow matching to progressively learn a fast flow path that reduces\nthe domain gap between initial and desired features across shallow to deep\nlayers. Additionally, a temporally-enhanced attention module captures\nlong-range dependencies essential for AQA. Second, List-wise Contrastive\nRegularization (LCR) facilitates coarse-to-fine alignment by comprehensively\ncomparing batch pairs to learn fine-grained cues while mitigating domain shift.\nIntegrating these modules, PHI offers an effective solution. Experiments\ndemonstrate that PHI achieves state-of-the-art performance on three\nrepresentative long-term AQA datasets, proving its superiority in addressing\nthe domain shift for long-term AQA."}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081", "abs": "https://arxiv.org/abs/2505.20081", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "title": "Inference-time Alignment in Continuous Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA"}
{"id": "2505.19985", "pdf": "https://arxiv.org/pdf/2505.19985", "abs": "https://arxiv.org/abs/2505.19985", "authors": ["Jianqiao Zheng", "Xueqian Li", "Hemanth Saratchandran", "Simon Lucey"], "title": "Structured Initialization for Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) inherently encode strong inductive\nbiases, enabling effective generalization on small-scale datasets. In this\npaper, we propose integrating this inductive bias into ViTs, not through an\narchitectural intervention but solely through initialization. The motivation\nhere is to have a ViT that can enjoy strong CNN-like performance when data\nassets are small, but can still scale to ViT-like performance as the data\nexpands. Our approach is motivated by our empirical results that random impulse\nfilters can achieve commensurate performance to learned filters within a CNN.\nWe improve upon current ViT initialization strategies, which typically rely on\nempirical heuristics such as using attention weights from pretrained models or\nfocusing on the distribution of attention weights without enforcing structures.\nEmpirical results demonstrate that our method significantly outperforms\nstandard ViT initialization across numerous small and medium-scale benchmarks,\nincluding Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while\nmaintaining comparative performance on large-scale datasets such as\nImageNet-1K. Moreover, our initialization strategy can be easily integrated\ninto various transformer-based architectures such as Swin Transformer and\nMLP-Mixer with consistent improvements in performance."}
{"id": "2505.20088", "pdf": "https://arxiv.org/pdf/2505.20088", "abs": "https://arxiv.org/abs/2505.20088", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "title": "Multi-Domain Explainability of Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs."}
{"id": "2505.19990", "pdf": "https://arxiv.org/pdf/2505.19990", "abs": "https://arxiv.org/abs/2505.19990", "authors": ["Jack Hong", "Shilin Yan", "Zehao Xiao", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Henghui Ding"], "title": "Progressive Scaling Visual Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a progressive scaling training strategy for visual\nobject tracking, systematically analyzing the influence of training data\nvolume, model size, and input resolution on tracking performance. Our empirical\nstudy reveals that while scaling each factor leads to significant improvements\nin tracking accuracy, naive training suffers from suboptimal optimization and\nlimited iterative refinement. To address this issue, we introduce DT-Training,\na progressive scaling framework that integrates small teacher transfer and\ndual-branch alignment to maximize model potential. The resulting scaled tracker\nconsistently outperforms state-of-the-art methods across multiple benchmarks,\ndemonstrating strong generalization and transferability of the proposed method.\nFurthermore, we validate the broader applicability of our approach to\nadditional tasks, underscoring its versatility beyond tracking."}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096", "abs": "https://arxiv.org/abs/2505.20096", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG."}
{"id": "2505.20001", "pdf": "https://arxiv.org/pdf/2505.20001", "abs": "https://arxiv.org/abs/2505.20001", "authors": ["Shihao Li", "Chenglong Li", "Aihua Zheng", "Andong Lu", "Jin Tang", "Jixin Ma"], "title": "NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object re-identification (ReID) aims to extract identity features\nacross heterogeneous spectral modalities to enable accurate recognition and\nretrieval in complex real-world scenarios. However, most existing methods rely\non implicit feature fusion structures, making it difficult to model\nfine-grained recognition strategies under varying challenging conditions.\nBenefiting from the powerful semantic understanding capabilities of Multi-modal\nLarge Language Models (MLLMs), the visual appearance of an object can be\neffectively translated into descriptive text. In this paper, we propose a\nreliable multi-modal caption generation method based on attribute confidence,\nwhich significantly reduces the unknown recognition rate of MLLMs in\nmulti-modal semantic generation and improves the quality of generated text.\nAdditionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture\nof Experts via Text-Modulation for Multi-modal Object Re-Identification.\nSpecifically, we decouple the recognition problem into semantic and structural\nexpert branches to separately capture modality-specific appearance and\nintrinsic structure. For semantic recognition, we propose the Text-Modulated\nSemantic-sampling Experts (TMSE), which leverages randomly sampled high-quality\nsemantic texts to modulate expert-specific sampling of multi-modal features and\nmining intra-modality fine-grained semantic cues. Then, to recognize\ncoarse-grained structure features, we propose the Context-Shared\nStructure-aware Experts (CSSE) that focuses on capturing the holistic object\nstructure across modalities and maintains inter-modality structural consistency\nthrough a soft routing mechanism. Finally, we propose the Multi-Modal Feature\nAggregation (MMFA), which adopts a unified feature fusion strategy to simply\nand effectively integrate semantic and structural expert outputs into the final\nidentity representations."}
{"id": "2505.20097", "pdf": "https://arxiv.org/pdf/2505.20097", "abs": "https://arxiv.org/abs/2505.20097", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "categories": ["cs.CL"], "comment": "15 pages", "summary": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness"}
{"id": "2505.20021", "pdf": "https://arxiv.org/pdf/2505.20021", "abs": "https://arxiv.org/abs/2505.20021", "authors": ["Hyunsik Chae", "Seungwoo Yoon", "Jaden Park", "Chloe Yewon Chun", "Yongin Cho", "Mu Cai", "Yong Jae Lee", "Ernest K. Ryu"], "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "69 pages, 16 figures", "summary": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks."}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099", "abs": "https://arxiv.org/abs/2505.20099", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities."}
{"id": "2505.20024", "pdf": "https://arxiv.org/pdf/2505.20024", "abs": "https://arxiv.org/abs/2505.20024", "authors": ["Xueyi Liu", "Zuodong Zhong", "Yuxin Guo", "Yun-Fu Liu", "Zhiguo Su", "Qichao Zhang", "Junli Wang", "Yinfeng Gao", "Yupeng Zheng", "Qiao Lin", "Huiyong Chen", "Dongbin Zhao"], "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T40(Primary), 68T45, 68T50(Secondary)", "I.2.9; I.2.10; I.5.1"], "comment": "18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan", "summary": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan."}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101", "abs": "https://arxiv.org/abs/2505.20101", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications."}
{"id": "2505.20032", "pdf": "https://arxiv.org/pdf/2505.20032", "abs": "https://arxiv.org/abs/2505.20032", "authors": ["Fotios Lygerakis", "Ozan zdenizci", "Elmar Rckert"], "title": "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Tactile sensing provides local essential information that is complementary to\nvisual perception, such as texture, compliance, and force. Despite recent\nadvances in visuotactile representation learning, challenges remain in fusing\nthese modalities and generalizing across tasks and environments without heavy\nreliance on pre-trained vision-language models. Moreover, existing methods do\nnot study positional encodings, thereby overlooking the multi-scale spatial\nreasoning needed to capture fine-grained visuotactile correlations. We\nintroduce ViTaPEs, a transformer-based framework that robustly integrates\nvisual and tactile input data to learn task-agnostic representations for\nvisuotactile perception. Our approach exploits a novel multi-scale positional\nencoding scheme to capture intra-modal structures, while simultaneously\nmodeling cross-modal cues. Unlike prior work, we provide provable guarantees in\nvisuotactile fusion, showing that our encodings are injective,\nrigid-motion-equivariant, and information-preserving, validating these\nproperties empirically. Experiments on multiple large-scale real-world datasets\nshow that ViTaPEs not only surpasses state-of-the-art baselines across various\nrecognition tasks but also demonstrates zero-shot generalization to unseen,\nout-of-domain scenarios. We further demonstrate the transfer-learning strength\nof ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art\nbaselines in predicting grasp success. Project page:\nhttps://sites.google.com/view/vitapes"}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109", "abs": "https://arxiv.org/abs/2505.20109", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment."}
{"id": "2505.20033", "pdf": "https://arxiv.org/pdf/2505.20033", "abs": "https://arxiv.org/abs/2505.20033", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Maurice Kraus", "Felix Friedrich", "Huu Nguyen", "Krishna Kalyan", "Kourosh Nadi", "Kristian Kersting", "Sren Auer"], "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions."}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112", "abs": "https://arxiv.org/abs/2505.20112", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness."}
{"id": "2505.20041", "pdf": "https://arxiv.org/pdf/2505.20041", "abs": "https://arxiv.org/abs/2505.20041", "authors": ["Jianxin Huang", "Jiahang Li", "Sergey Vityazev", "Alexander Dvorkovich", "Rui Fan"], "title": "DepthMatch: Semi-Supervised RGB-D Scene Parsing through Depth-Guided Regularization", "categories": ["cs.CV"], "comment": "5 pages, 2 figures, accepted by IEEE Signal Processing Letters", "summary": "RGB-D scene parsing methods effectively capture both semantic and geometric\nfeatures of the environment, demonstrating great potential under challenging\nconditions such as extreme weather and low lighting. However, existing RGB-D\nscene parsing methods predominantly rely on supervised training strategies,\nwhich require a large amount of manually annotated pixel-level labels that are\nboth time-consuming and costly. To overcome these limitations, we introduce\nDepthMatch, a semi-supervised learning framework that is specifically designed\nfor RGB-D scene parsing. To make full use of unlabeled data, we propose\ncomplementary patch mix-up augmentation to explore the latent relationships\nbetween texture and spatial features in RGB-D image pairs. We also design a\nlightweight spatial prior injector to replace traditional complex fusion\nmodules, improving the efficiency of heterogeneous feature fusion. Furthermore,\nwe introduce depth-guided boundary loss to enhance the model's boundary\nprediction capabilities. Experimental results demonstrate that DepthMatch\nexhibits high applicability in both indoor and outdoor scenes, achieving\nstate-of-the-art results on the NYUv2 dataset and ranking first on the KITTI\nSemantics benchmark."}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113", "abs": "https://arxiv.org/abs/2505.20113", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences."}
{"id": "2505.20049", "pdf": "https://arxiv.org/pdf/2505.20049", "abs": "https://arxiv.org/abs/2505.20049", "authors": ["Hongsong Wang", "Ao Sun", "Jie Gui", "Liang Wang"], "title": "Data-Free Class-Incremental Gesture Recognition with Prototype-Guided Pseudo Feature Replay", "categories": ["cs.CV"], "comment": "Code is on https://github.com/sunao-101/PGPFR-3/", "summary": "Gesture recognition is an important research area in the field of computer\nvision. Most gesture recognition efforts focus on close-set scenarios, thereby\nlimiting the capacity to effectively handle unseen or novel gestures. We aim to\naddress class-incremental gesture recognition, which entails the ability to\naccommodate new and previously unseen gestures over time. Specifically, we\nintroduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for\ndata-free class-incremental gesture recognition. This framework comprises four\ncomponents: Pseudo Feature Generation with Batch Prototypes (PFGBP),\nVariational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy\n(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle\nthe issue of catastrophic forgetting, the PFGBP dynamically generates a\ndiversity of pseudo features in an online manner, leveraging class prototypes\nof old classes along with batch class prototypes of new classes. Furthermore,\nthe VPR enforces consistency between the classifier's weights and the\nprototypes of old classes, leveraging class prototypes and covariance matrices\nto enhance robustness and generalization capabilities. The TCE mitigates the\nimpact of domain differences of the classifier caused by pseudo features.\nFinally, the CCRT training strategy is designed to prevent overfitting to new\nclasses and ensure the stability of features extracted from old classes.\nExtensive experiments conducted on two widely used gesture recognition\ndatasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach\noutperforms existing state-of-the-art methods by 11.8\\% and 12.8\\% in terms of\nmean global accuracy, respectively. The code is available on\nhttps://github.com/sunao-101/PGPFR-3/."}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118", "abs": "https://arxiv.org/abs/2505.20118", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Rttger", "Terry Ruas", "Bela Gipp"], "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "categories": ["cs.CL", "cs.CR"], "comment": "8 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053", "abs": "https://arxiv.org/abs/2505.20053", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.20128", "pdf": "https://arxiv.org/pdf/2505.20128", "abs": "https://arxiv.org/abs/2505.20128", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "categories": ["cs.CL"], "comment": "Working in process", "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work."}
{"id": "2505.20056", "pdf": "https://arxiv.org/pdf/2505.20056", "abs": "https://arxiv.org/abs/2505.20056", "authors": ["Hongsong Wang", "Yin Zhu", "Qiuxia Lai", "Yang Zhang", "Guo-Sen Xie", "Xin Geng"], "title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation", "categories": ["cs.CV"], "comment": "This project page is available at:\n  https://mucunzhuzhu.github.io/PAMD-page/", "summary": "Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/."}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133", "abs": "https://arxiv.org/abs/2505.20133", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines."}
{"id": "2505.20058", "pdf": "https://arxiv.org/pdf/2505.20058", "abs": "https://arxiv.org/abs/2505.20058", "authors": ["Yihong Lin", "Xianjia Wu", "Xilai Wang", "Jianqiao Hu", "Songju Lei", "Xiandong Li", "Wenxiong Kang"], "title": "M3DHMR: Monocular 3D Hand Mesh Recovery", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Monocular 3D hand mesh recovery is challenging due to high degrees of freedom\nof hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are\neither inefficient or less straightforward for predicting the position of 3D\nmesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh\nRecovery (M3DHMR) to directly estimate the positions of hand mesh vertices.\nM3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral\ndecoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region\nof Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the\nweights based on the vertex positions and extract the vertex features in both\nspatial and channel dimensions. On the other hand, ROI Layer utilizes the\nphysical information and refines mesh vertices in each predefined hand region\nseparately. Extensive experiments on popular dataset FreiHAND demonstrate that\nM3DHMR significantly outperforms state-of-the-art real-time methods."}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144", "abs": "https://arxiv.org/abs/2505.20144", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition."}
{"id": "2505.20100", "pdf": "https://arxiv.org/pdf/2505.20100", "abs": "https://arxiv.org/abs/2505.20100", "authors": ["Fengyuan Sun", "Leqi Shen", "Hui Chen", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon."}
{"id": "2505.20154", "pdf": "https://arxiv.org/pdf/2505.20154", "abs": "https://arxiv.org/abs/2505.20154", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models", "categories": ["cs.CL"], "comment": "20 pages, 2 figures, 15 tables", "summary": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs."}
{"id": "2505.20106", "pdf": "https://arxiv.org/pdf/2505.20106", "abs": "https://arxiv.org/abs/2505.20106", "authors": ["Zuyao Chen", "Jinlin Wu", "Zhen Lei", "Chang Wen Chen"], "title": "From Data to Modeling: Fully Open-vocabulary Scene Graph Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present OvSGTR, a novel transformer-based framework for fully\nopen-vocabulary scene graph generation that overcomes the limitations of\ntraditional closed-set models. Conventional methods restrict both object and\nrelationship recognition to a fixed vocabulary, hindering their applicability\nto real-world scenarios where novel concepts frequently emerge. In contrast,\nour approach jointly predicts objects (nodes) and their inter-relationships\n(edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture\nfeaturing a frozen image backbone and text encoder to extract high-quality\nvisual and semantic features, which are then fused via a transformer decoder\nfor end-to-end scene graph prediction. To enrich the model's understanding of\ncomplex visual relations, we propose a relation-aware pre-training strategy\nthat synthesizes scene graph annotations in a weakly supervised manner.\nSpecifically, we investigate three pipelines--scene parser-based, LLM-based,\nand multimodal LLM-based--to generate transferable supervision signals with\nminimal manual annotation. Furthermore, we address the common issue of\ncatastrophic forgetting in open-vocabulary settings by incorporating a\nvisual-concept retention mechanism coupled with a knowledge distillation\nstrategy, ensuring that the model retains rich semantic cues during\nfine-tuning. Extensive experiments on the VG150 benchmark demonstrate that\nOvSGTR achieves state-of-the-art performance across multiple settings,\nincluding closed-set, open-vocabulary object detection-based, relation-based,\nand fully open-vocabulary scenarios. Our results highlight the promise of\nlarge-scale relation-aware pre-training and transformer architectures for\nadvancing scene graph generation towards more generalized and reliable visual\nunderstanding."}
{"id": "2505.20155", "pdf": "https://arxiv.org/pdf/2505.20155", "abs": "https://arxiv.org/abs/2505.20155", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s."}
{"id": "2505.20122", "pdf": "https://arxiv.org/pdf/2505.20122", "abs": "https://arxiv.org/abs/2505.20122", "authors": ["Anh Thai", "Stefan Stojanov", "Zixuan Huang", "Bikram Boote", "James M. Rehg"], "title": "MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces MEBench, a novel benchmark for evaluating mutual\nexclusivity (ME) bias, a cognitive phenomenon observed in children during word\nlearning. Unlike traditional ME tasks, MEBench further incorporates spatial\nreasoning to create more challenging and realistic evaluation settings. We\nassess the performance of state-of-the-art vision-language models (VLMs) on\nthis benchmark using novel evaluation metrics that capture key aspects of\nME-based reasoning. To facilitate controlled experimentation, we also present a\nflexible and scalable data generation pipeline that supports the construction\nof diverse annotated scenes."}
{"id": "2505.20163", "pdf": "https://arxiv.org/pdf/2505.20163", "abs": "https://arxiv.org/abs/2505.20163", "authors": ["Moreno La Quatra", "Alkis Koudounas", "Valerio Mario Salerno", "Sabato Marco Siniscalchi"], "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition"}
{"id": "2505.20124", "pdf": "https://arxiv.org/pdf/2505.20124", "abs": "https://arxiv.org/abs/2505.20124", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Hongzhi Zhang", "Shi Feng", "Daling Wang", "Linhao Yu", "Xingguang Ji", "Yu Tian", "Qi Wang", "Fuzheng Zhang"], "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos", "categories": ["cs.CV", "cs.DB", "cs.MM"], "comment": "Accepted to CVPR 2025 Main. Project page:\n  https://friedrichor.github.io/projects/TUNA", "summary": "Videos are unique in their integration of temporal elements, including\ncamera, scene, action, and attribute, along with their dynamic relationships\nover time. However, existing benchmarks for video understanding often treat\nthese properties separately or narrowly focus on specific aspects, overlooking\nthe holistic nature of video content. To address this, we introduce TUNA, a\ntemporal-oriented benchmark for fine-grained understanding on dense dynamic\nvideos, with two complementary tasks: captioning and QA. Our TUNA features\ndiverse video scenarios and dynamics, assisted by interpretable and robust\nevaluation criteria. We evaluate several leading models on our benchmark,\nproviding fine-grained performance assessments across various dimensions. This\nevaluation reveals key challenges in video temporal understanding, such as\nlimited action description, inadequate multi-subject understanding, and\ninsensitivity to camera motion, offering valuable insights for improving video\nunderstanding models. The data and code are available at\nhttps://friedrichor.github.io/projects/TUNA."}
{"id": "2505.20164", "pdf": "https://arxiv.org/pdf/2505.20164", "abs": "https://arxiv.org/abs/2505.20164", "authors": ["Dairu Liu", "Ziyue Wang", "Minyuan Ruan", "Fuwen Luo", "Chi Chen", "Peng Li", "Yang Liu"], "title": "Visual Abstract Thinking Empowers Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition."}
{"id": "2505.20126", "pdf": "https://arxiv.org/pdf/2505.20126", "abs": "https://arxiv.org/abs/2505.20126", "authors": ["Shintaro Ito", "Natsuki Takama", "Toshiki Watanabe", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in radiance field rendering, exemplified by Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly\nprogressed 3D modeling and reconstruction. The use of multiple 360-degree\nomnidirectional images for these tasks is increasingly favored due to\nadvantages in data acquisition and comprehensive scene capture. However, the\ninherent geometric distortions in common omnidirectional representations, such\nas equirectangular projection (particularly severe in polar regions and varying\nwith latitude), pose substantial challenges to achieving high-fidelity 3D\nreconstructions. Current datasets, while valuable, often lack the specific\nfocus, scene composition, and ground truth granularity required to\nsystematically benchmark and drive progress in overcoming these\nomnidirectional-specific challenges. To address this critical gap, we introduce\nOmnidirectional Blender 3D (OB3D), a new synthetic dataset curated for\nadvancing 3D reconstruction from multiple omnidirectional images. OB3D features\ndiverse and complex 3D scenes generated from Blender 3D projects, with a\ndeliberate emphasis on challenging scenarios. The dataset provides\ncomprehensive ground truth, including omnidirectional RGB images, precise\nomnidirectional camera parameters, and pixel-aligned equirectangular maps for\ndepth and normals, alongside evaluation metrics. By offering a controlled yet\nchallenging environment, OB3Daims to facilitate the rigorous evaluation of\nexisting methods and prompt the development of new techniques to enhance the\naccuracy and reliability of 3D reconstruction from omnidirectional images."}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176", "abs": "https://arxiv.org/abs/2505.20176", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms."}
{"id": "2505.20129", "pdf": "https://arxiv.org/pdf/2505.20129", "abs": "https://arxiv.org/abs/2505.20129", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications."}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184", "abs": "https://arxiv.org/abs/2505.20184", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "title": "THiNK: Can Large Language Models Think-aloud?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository."}
{"id": "2505.20147", "pdf": "https://arxiv.org/pdf/2505.20147", "abs": "https://arxiv.org/abs/2505.20147", "authors": ["Jin Wang", "Yao Lai", "Aoxue Li", "Shifeng Zhang", "Jiacheng Sun", "Ning Kang", "Chengyue Wu", "Zhenguo Li", "Ping Luo"], "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities", "categories": ["cs.CV"], "comment": "37 pages, 12 figures", "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning."}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195", "abs": "https://arxiv.org/abs/2505.20195", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152", "abs": "https://arxiv.org/abs/2505.20152", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.20199", "pdf": "https://arxiv.org/pdf/2505.20199", "abs": "https://arxiv.org/abs/2505.20199", "authors": ["Pengxiang Li", "Shilin Yan", "Joey Tsai", "Renrui Zhang", "Ruichuan An", "Ziyu Guo", "Xiaowei Gao"], "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "categories": ["cs.CL"], "comment": "Project page: https://github.com/pixeli99/A-CFG", "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation."}
{"id": "2505.20156", "pdf": "https://arxiv.org/pdf/2505.20156", "abs": "https://arxiv.org/abs/2505.20156", "authors": ["Yi Chen", "Sen Liang", "Zixiang Zhou", "Ziyao Huang", "Yifeng Ma", "Junshu Tang", "Qin Lin", "Yuan Zhou", "Qinglin Lu"], "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios."}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201", "abs": "https://arxiv.org/abs/2505.20201", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "categories": ["cs.CL"], "comment": "33 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations."}
{"id": "2505.20171", "pdf": "https://arxiv.org/pdf/2505.20171", "abs": "https://arxiv.org/abs/2505.20171", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "title": "Long-Context State-Space Video World Models", "categories": ["cs.CV"], "comment": "Project website: https://ryanpo.com/ssm_wm", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications."}
{"id": "2505.20209", "pdf": "https://arxiv.org/pdf/2505.20209", "abs": "https://arxiv.org/abs/2505.20209", "authors": ["Joe Stacey", "Lisa Alazraki", "Aran Ubhi", "Beyza Ermis", "Aaron Mueller", "Marek Rei"], "title": "How to Improve the Robustness of Closed-Source Models on NLI", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward."}
{"id": "2505.20202", "pdf": "https://arxiv.org/pdf/2505.20202", "abs": "https://arxiv.org/abs/2505.20202", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "categories": ["cs.CV"], "comment": "35 pages, 9 figures", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice."}
{"id": "2505.20215", "pdf": "https://arxiv.org/pdf/2505.20215", "abs": "https://arxiv.org/abs/2505.20215", "authors": ["Paolo Gajo", "Domenic Rosati", "Hassan Sajjad", "Alberto Barrn-Cedeo"], "title": "Dependency Parsing is More Parameter-Efficient with Normalization", "categories": ["cs.CL"], "comment": null, "summary": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1"}
{"id": "2505.20236", "pdf": "https://arxiv.org/pdf/2505.20236", "abs": "https://arxiv.org/abs/2505.20236", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Junjue Wang", "Naoto Yokoya"], "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems."}
{"id": "2505.20225", "pdf": "https://arxiv.org/pdf/2505.20225", "abs": "https://arxiv.org/abs/2505.20225", "authors": ["Hao Kang", "Zichun Yu", "Chenyan Xiong"], "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE."}
{"id": "2505.20255", "pdf": "https://arxiv.org/pdf/2505.20255", "abs": "https://arxiv.org/abs/2505.20255", "authors": ["Muyao Niu", "Mingdeng Cao", "Yifan Zhan", "Qingtian Zhu", "Mingze Ma", "Jiancheng Zhao", "Yanhong Zeng", "Zhihang Zhong", "Xiao Sun", "Yinqiang Zheng"], "title": "AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models", "categories": ["cs.CV"], "comment": "Github: https://github.com/MyNiuuu/AniCrafter", "summary": "Recent advances in video diffusion models have significantly improved\ncharacter animation techniques. However, current approaches rely on basic\nstructural conditions such as DWPose or SMPL-X to animate character images,\nlimiting their effectiveness in open-domain scenarios with dynamic backgrounds\nor challenging human poses. In this paper, we introduce $\\textbf{AniCrafter}$,\na diffusion-based human-centric animation model that can seamlessly integrate\nand animate a given character into open-domain dynamic backgrounds while\nfollowing given human motion sequences. Built on cutting-edge Image-to-Video\n(I2V) diffusion architectures, our model incorporates an innovative\n\"avatar-background\" conditioning mechanism that reframes open-domain\nhuman-centric animation as a restoration task, enabling more stable and\nversatile animation outputs. Experimental results demonstrate the superior\nperformance of our method. Codes will be available at\nhttps://github.com/MyNiuuu/AniCrafter."}
{"id": "2505.20231", "pdf": "https://arxiv.org/pdf/2505.20231", "abs": "https://arxiv.org/abs/2505.20231", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "title": "Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Existing Task-Oriented Dialogue (TOD) systems primarily focus on\nsingle-session dialogues, limiting their effectiveness in long-term memory\naugmentation. To address this challenge, we introduce a MS-TOD dataset, the\nfirst multi-session TOD dataset designed to retain long-term memory across\nsessions, enabling fewer turns and more efficient task completion. This defines\na new benchmark task for evaluating long-term memory in multi-session TOD.\nBased on this new dataset, we propose a Memory-Active Policy (MAP) that\nimproves multi-session dialogue efficiency through a two-stage approach. 1)\nMemory-Guided Dialogue Planning retrieves intent-aligned history, identifies\nkey QA units via a memory judger, refines them by removing redundant questions,\nand generates responses based on the reconstructed memory. 2) Proactive\nResponse Strategy detects and correct errors or omissions, ensuring efficient\nand accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on\nresponse quality and effectiveness of the proactive strategy. Experiments on\nMS-TOD demonstrate that MAP significantly improves task success and turn\nefficiency in multi-session scenarios, while maintaining competitive\nperformance on conventional single-session tasks."}
{"id": "2505.20256", "pdf": "https://arxiv.org/pdf/2505.20256", "abs": "https://arxiv.org/abs/2505.20256", "authors": ["Hao Zhong", "Muzhi Zhu", "Zongze Du", "Zheng Huang", "Canyu Zhao", "Mingyu Liu", "Wen Wang", "Hao Chen", "Chunhua Shen"], "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration", "categories": ["cs.CV"], "comment": "Project page: https://aim-uofa.github.io/OmniR1", "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels."}
{"id": "2505.20237", "pdf": "https://arxiv.org/pdf/2505.20237", "abs": "https://arxiv.org/abs/2505.20237", "authors": ["Yasmin Moslem"], "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models."}
{"id": "2505.20267", "pdf": "https://arxiv.org/pdf/2505.20267", "abs": "https://arxiv.org/abs/2505.20267", "authors": ["Changjian Jiang", "Kerui Ren", "Linning Xu", "Jiong Chen", "Jiangmiao Pang", "Yu Zhang", "Bo Dai", "Mulin Yu"], "title": "HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes", "categories": ["cs.CV"], "comment": null, "summary": "High fidelity 3D reconstruction and rendering hinge on capturing precise\ngeometry while preserving photo realistic detail. Most existing methods either\nfuse these goals into a single cumbersome model or adopt hybrid schemes whose\nuniform primitives lead to a trade off between efficiency and fidelity. In this\npaper, we introduce HaloGS, a dual representation that loosely couples coarse\ntriangles for geometry with Gaussian primitives for appearance, motivated by\nthe lightweight classic geometry representations and their proven efficiency in\nreal world applications. Our design yields a compact yet expressive model\ncapable of photo realistic rendering across both indoor and outdoor\nenvironments, seamlessly adapting to varying levels of scene complexity.\nExperiments on multiple benchmark datasets demonstrate that our method yields\nboth compact, accurate geometry and high fidelity renderings, especially in\nchallenging scenarios where robust geometric structure make a clear difference."}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243", "abs": "https://arxiv.org/abs/2505.20243", "authors": ["Bhawna Piryani", "Abdelrahman Abdullah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization."}
{"id": "2505.20270", "pdf": "https://arxiv.org/pdf/2505.20270", "abs": "https://arxiv.org/abs/2505.20270", "authors": ["Jinsheng Quan", "Chunshi Wang", "Yawei Luo"], "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation", "categories": ["cs.CV"], "comment": null, "summary": "This paper aims to model the dynamics of 3D Gaussians from visual\nobservations to support temporal extrapolation. Existing dynamic 3D\nreconstruction methods often struggle to effectively learn underlying dynamics\nor rely heavily on manually defined physical priors, which limits their\nextrapolation capabilities. To address this issue, we propose a novel dynamic\n3D Gaussian Splatting prior-free motion extrapolation framework based on\nparticle dynamics systems. The core advantage of our method lies in its ability\nto learn differential equations that describe the dynamics of 3D Gaussians, and\nfollow them during future frame extrapolation. Instead of simply fitting to the\nobserved visual frame sequence, we aim to more effectively model the gaussian\nparticle dynamics system. To this end, we introduce a dynamics latent state\nvector into the standard Gaussian kernel and design a dynamics latent space\nencoder to extract initial state. Subsequently, we introduce a Neural\nODEs-based dynamics module that models the temporal evolution of Gaussian in\ndynamics latent space. Finally, a Gaussian kernel space decoder is used to\ndecode latent state at the specific time step into the deformation.\nExperimental results demonstrate that the proposed method achieves comparable\nrendering quality with existing approaches in reconstruction tasks, and\nsignificantly outperforms them in future frame extrapolation. Our code is\navailable at https://github.com/QuanJinSheng/ParticleGS."}
{"id": "2505.20245", "pdf": "https://arxiv.org/pdf/2505.20245", "abs": "https://arxiv.org/abs/2505.20245", "authors": ["Rui Li", "Quanyu Dai", "Zeyu Zhang", "Xu Chen", "Zhenhua Dong", "Ji-Rong Wen"], "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains."}
{"id": "2505.20271", "pdf": "https://arxiv.org/pdf/2505.20271", "abs": "https://arxiv.org/abs/2505.20271", "authors": ["Yu Xu", "Fan Tang", "You Wu", "Lin Gao", "Oliver Deussen", "Hongbin Yan", "Jintao Li", "Juan Cao", "Tong-Yee Lee"], "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection."}
{"id": "2505.20249", "pdf": "https://arxiv.org/pdf/2505.20249", "abs": "https://arxiv.org/abs/2505.20249", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters."}
{"id": "2505.20272", "pdf": "https://arxiv.org/pdf/2505.20272", "abs": "https://arxiv.org/abs/2505.20272", "authors": ["Meng Cao", "Haoze Zhao", "Can Zhang", "Xiaojun Chang", "Ian Reid", "Xiaodan Liang"], "title": "Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive general\ncapabilities across a wide range of multi-modal tasks. However, the reasoning\nprocesses of LVLMs often suffer from unreliable outputs and limited\ninterpretability. To address this, grounded visual reasoning has emerged as a\npromising paradigm that enforces responses anchored on salient visual evidence\nregions. However, existing approaches typically rely on costly supervision such\nas bounding box annotations, chain-of-thought rationale or external tool calls,\nlimiting their scalability. In this work, we propose Ground-R1, a reinforcement\nlearning framework that enables grounded visual reasoning without requiring\nexplicit evidence or rationale annotations. Ground-R1 consists of a grounding\nphase that generates evidence region rollouts based on format constraints, and\nan answering phase that produces responses guided by both answer correctness\nand format adherence rewards. Extensive experiments across multiple visual\nreasoning benchmarks manifest that Ground-R1 achieves superior performance and\nexhibits emergent cognitive behaviors such as uncertainty awareness, spatial\nperception, and iterative refinement, offering a scalable and interpretable\nalternative to existing approaches."}
{"id": "2505.20258", "pdf": "https://arxiv.org/pdf/2505.20258", "abs": "https://arxiv.org/abs/2505.20258", "authors": ["Siye Wu", "Jian Xie", "Yikai Zhang", "Aili Chen", "Kai Zhang", "Yu Su", "Yanghua Xiao"], "title": "ARM: Adaptive Reasoning Model", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage."}
{"id": "2505.20275", "pdf": "https://arxiv.org/pdf/2505.20275", "abs": "https://arxiv.org/abs/2505.20275", "authors": ["Yang Ye", "Xianyi He", "Zongjian Li", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Bohan Hou", "Li Yuan"], "title": "ImgEdit: A Unified Image Editing Dataset and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative models have enabled high-fidelity\ntext-to-image generation. However, open-source image-editing models still lag\nbehind their proprietary counterparts, primarily due to limited high-quality\ndata and insufficient benchmarks. To overcome these limitations, we introduce\nImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2\nmillion carefully curated edit pairs, which contain both novel and complex\nsingle-turn edits, as well as challenging multi-turn tasks. To ensure the data\nquality, we employ a multi-stage pipeline that integrates a cutting-edge\nvision-language model, a detection model, a segmentation model, alongside\ntask-specific in-painting procedures and strict post-processing. ImgEdit\nsurpasses existing datasets in both task novelty and data quality. Using\nImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to\nprocess the reference image and editing prompt, which outperforms existing\nopen-source models on multiple tasks, highlighting the value of ImgEdit and\nmodel design. For comprehensive evaluation, we introduce ImgEdit-Bench, a\nbenchmark designed to evaluate image editing performance in terms of\ninstruction adherence, editing quality, and detail preservation. It includes a\nbasic testsuite, a challenging single-turn suite, and a dedicated multi-turn\nsuite. We evaluate both open-source and proprietary models, as well as\nImgEdit-E1, providing deep analysis and actionable insights into the current\nbehavior of image-editing models. The source data are publicly available on\nhttps://github.com/PKU-YuanGroup/ImgEdit."}
{"id": "2505.20264", "pdf": "https://arxiv.org/pdf/2505.20264", "abs": "https://arxiv.org/abs/2505.20264", "authors": ["Dong Nguyen", "Esther Ploeger"], "title": "We Need to Measure Data Diversity in NLP -- Better and Broader", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279", "abs": "https://arxiv.org/abs/2505.20279", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276", "abs": "https://arxiv.org/abs/2505.20276", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "title": "Does quantization affect models' performance on long-context tasks?", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English."}
{"id": "2505.20283", "pdf": "https://arxiv.org/pdf/2505.20283", "abs": "https://arxiv.org/abs/2505.20283", "authors": ["Guangzhao He", "Chen Geng", "Shangzhe Wu", "Jiajun Wu"], "title": "Category-Agnostic Neural Object Rigging", "categories": ["cs.CV", "I.2.10"], "comment": "Accepted to CVPR 2025. Project Page: https://guangzhaohe.com/canor", "summary": "The motion of deformable 4D objects lies in a low-dimensional manifold. To\nbetter capture the low dimensionality and enable better controllability,\ntraditional methods have devised several heuristic-based methods, i.e.,\nrigging, for manipulating dynamic objects in an intuitive fashion. However,\nsuch representations are not scalable due to the need for expert knowledge of\nspecific categories. Instead, we study the automatic exploration of such\nlow-dimensional structures in a purely data-driven manner. Specifically, we\ndesign a novel representation that encodes deformable 4D objects into a sparse\nset of spatially grounded blobs and an instance-aware feature volume to\ndisentangle the pose and instance information of the 3D shape. With such a\nrepresentation, we can manipulate the pose of 3D objects intuitively by\nmodifying the parameters of the blobs, while preserving rich instance-specific\ninformation. We evaluate the proposed method on a variety of object categories\nand demonstrate the effectiveness of the proposed framework. Project page:\nhttps://guangzhaohe.com/canor"}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277", "abs": "https://arxiv.org/abs/2505.20277", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter."}
{"id": "2505.20287", "pdf": "https://arxiv.org/pdf/2505.20287", "abs": "https://arxiv.org/abs/2505.20287", "authors": ["Zhongwei Zhang", "Fuchen Long", "Zhaofan Qiu", "Yingwei Pan", "Wu Liu", "Ting Yao", "Tao Mei"], "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation", "categories": ["cs.CV", "cs.MM"], "comment": "CVPR 2025. Project page: https://zhw-zhang.github.io/MotionPro-page/", "summary": "Animating images with interactive motion control has garnered popularity for\nimage-to-video (I2V) generation. Modern approaches typically rely on large\nGaussian kernels to extend motion trajectories as condition without explicitly\ndefining movement region, leading to coarse motion control and failing to\ndisentangle object and camera moving. To alleviate these, we present MotionPro,\na precise motion controller that novelly leverages region-wise trajectory and\nmotion mask to regulate fine-grained motion synthesis and identify target\nmotion category (i.e., object or camera moving), respectively. Technically,\nMotionPro first estimates the flow maps on each training video via a tracking\nmodel, and then samples the region-wise trajectories to simulate inference\nscenario. Instead of extending flow through large Gaussian kernels, our\nregion-wise trajectory approach enables more precise control by directly\nutilizing trajectories within local regions, thereby effectively characterizing\nfine-grained movements. A motion mask is simultaneously derived from the\npredicted flow maps to capture the holistic motion dynamics of the movement\nregions. To pursue natural motion control, MotionPro further strengthens video\ndenoising by incorporating both region-wise trajectories and motion mask\nthrough feature modulation. More remarkably, we meticulously construct a\nbenchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for\nthe evaluation of both fine-grained and object-level I2V motion control.\nExtensive experiments conducted on WebVid-10M and MC-Bench demonstrate the\neffectiveness of MotionPro. Please refer to our project page for more results:\nhttps://zhw-zhang.github.io/MotionPro-page/."}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282", "abs": "https://arxiv.org/abs/2505.20282", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "title": "One-shot Entropy Minimization", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em."}
{"id": "2505.20288", "pdf": "https://arxiv.org/pdf/2505.20288", "abs": "https://arxiv.org/abs/2505.20288", "authors": ["Guangting Zheng", "Yehao Li", "Yingwei Pan", "Jiajun Deng", "Ting Yao", "Yanyong Zhang", "Tao Mei"], "title": "Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots", "categories": ["cs.CV", "cs.MM"], "comment": "ICML 2025. Source code is available at\n  https://github.com/HiDream-ai/himar", "summary": "Autoregressive models have emerged as a powerful generative paradigm for\nvisual generation. The current de-facto standard of next token prediction\ncommonly operates over a single-scale sequence of dense image tokens, and is\nincapable of utilizing global context especially for early tokens prediction.\nIn this paper, we introduce a new autoregressive design to model a hierarchy\nfrom a few low-resolution image tokens to the typical dense image tokens, and\ndelve into a thorough hierarchical dependency across multi-scale image tokens.\nTechnically, we present a Hierarchical Masked Autoregressive models (Hi-MAR)\nthat pivot on low-resolution image tokens to trigger hierarchical\nautoregressive modeling in a multi-phase manner. Hi-MAR learns to predict a few\nimage tokens in low resolution, functioning as intermediary pivots to reflect\nglobal structure, in the first phase. Such pivots act as the additional\nguidance to strengthen the next autoregressive modeling phase by shaping global\nstructural awareness of typical dense image tokens. A new Diffusion Transformer\nhead is further devised to amplify the global context among all tokens for mask\ntoken prediction. Extensive evaluations on both class-conditional and\ntext-to-image generation tasks demonstrate that Hi-MAR outperforms typical AR\nbaselines, while requiring fewer computational costs. Code is available at\nhttps://github.com/HiDream-ai/himar."}
{"id": "2505.20285", "pdf": "https://arxiv.org/pdf/2505.20285", "abs": "https://arxiv.org/abs/2505.20285", "authors": ["Weiqi Wu", "Xin Guan", "Shen Huang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jiuxin Cao", "Hai Zhao", "Jingren Zhou"], "title": "MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MASKSEARCH. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMASKSEARCH significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks."}
{"id": "2505.20289", "pdf": "https://arxiv.org/pdf/2505.20289", "abs": "https://arxiv.org/abs/2505.20289", "authors": ["Zeyi Huang", "Yuyang Ji", "Anirudh Sundara Rajan", "Zefan Cai", "Wen Xiao", "Junjie Hu", "Yong Jae Lee"], "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection", "categories": ["cs.CV"], "comment": null, "summary": "We introduce VisTA, a new reinforcement learning framework that empowers\nvisual agents to dynamically explore, select, and combine tools from a diverse\nlibrary based on empirical performance. Existing methods for tool-augmented\nreasoning either rely on training-free prompting or large-scale fine-tuning;\nboth lack active tool exploration and typically assume limited tool diversity,\nand fine-tuning methods additionally demand extensive human supervision. In\ncontrast, VisTA leverages end-to-end reinforcement learning to iteratively\nrefine sophisticated, query-specific tool selection strategies, using task\noutcomes as feedback signals. Through Group Relative Policy Optimization\n(GRPO), our framework enables an agent to autonomously discover effective\ntool-selection pathways without requiring explicit reasoning supervision.\nExperiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate\nthat VisTA achieves substantial performance gains over training-free baselines,\nespecially on out-of-distribution examples. These results highlight VisTA's\nability to enhance generalization, adaptively utilize diverse tools, and pave\nthe way for flexible, experience-driven visual reasoning systems."}
{"id": "2505.20293", "pdf": "https://arxiv.org/pdf/2505.20293", "abs": "https://arxiv.org/abs/2505.20293", "authors": ["Yifan Sun", "Danding Wang", "Qiang Sheng", "Juan Cao", "Jintao Li"], "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Concept-based explainable approaches have emerged as a promising method in\nexplainable AI because they can interpret models in a way that aligns with\nhuman reasoning. However, their adaption in the text domain remains limited.\nMost existing methods rely on predefined concept annotations and cannot\ndiscover unseen concepts, while other methods that extract concepts without\nsupervision often produce explanations that are not intuitively comprehensible\nto humans, potentially diminishing user trust. These methods fall short of\ndiscovering comprehensible concepts automatically. To address this issue, we\npropose \\textbf{ECO-Concept}, an intrinsically interpretable framework to\ndiscover comprehensible concepts with no concept annotations. ECO-Concept first\nutilizes an object-centric architecture to extract semantic concepts\nautomatically. Then the comprehensibility of the extracted concepts is\nevaluated by large language models. Finally, the evaluation result guides the\nsubsequent model fine-tuning to obtain more understandable explanations.\nExperiments show that our method achieves superior performance across diverse\ntasks. Further concept evaluations validate that the concepts learned by\nECO-Concept surpassed current counterparts in comprehensibility."}
{"id": "2505.20291", "pdf": "https://arxiv.org/pdf/2505.20291", "abs": "https://arxiv.org/abs/2505.20291", "authors": ["Di Wu", "Yixin Wan", "Kai-Wei Chang"], "title": "Visualized Text-to-Image Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve."}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295", "abs": "https://arxiv.org/abs/2505.20295", "authors": ["Michael Kirchhof", "Luca Fger", "Adam Goliski", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties."}
{"id": "2505.20292", "pdf": "https://arxiv.org/pdf/2505.20292", "abs": "https://arxiv.org/abs/2505.20292", "authors": ["Shenghai Yuan", "Xianyi He", "Yufan Deng", "Yang Ye", "Jinfa Huang", "Bin Lin", "Chongyang Ma", "Jiebo Luo", "Li Yuan"], "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Code and Dataset: https://github.com/PKU-YuanGroup/OpenS2V-Nexus", "summary": "Subject-to-Video (S2V) generation aims to create videos that faithfully\nincorporate reference content, providing enhanced flexibility in the production\nof videos. To establish the infrastructure for S2V generation, we propose\nOpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and\n(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V\nbenchmarks inherited from VBench that focus on global and coarse-grained\nassessment of generated videos, OpenS2V-Eval focuses on the model's ability to\ngenerate subject-consistent videos with natural subject appearance and identity\nfidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven\nmajor categories of S2V, which incorporate both real and synthetic test data.\nFurthermore, to accurately align human preferences with S2V benchmarks, we\npropose three automatic metrics, NexusScore, NaturalScore and GmeScore, to\nseparately quantify subject consistency, naturalness, and text relevance in\ngenerated videos. Building on this, we conduct a comprehensive evaluation of 16\nrepresentative S2V models, highlighting their strengths and weaknesses across\ndifferent content. Moreover, we create the first open-source large-scale S2V\ngeneration dataset OpenS2V-5M, which consists of five million high-quality 720P\nsubject-text-video triples. Specifically, we ensure subject-information\ndiversity in our dataset by (1) segmenting subjects and building pairing\ninformation via cross-video associations and (2) prompting GPT-Image-1 on raw\nframes to synthesize multi-view representations. Through OpenS2V-Nexus, we\ndeliver a robust infrastructure to accelerate future S2V generation research."}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296", "abs": "https://arxiv.org/abs/2505.20296", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "title": "Reasoning LLMs are Wandering Solution Explorers", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself."}
{"id": "2505.20294", "pdf": "https://arxiv.org/pdf/2505.20294", "abs": "https://arxiv.org/abs/2505.20294", "authors": ["Xiao Chen", "Tai Wang", "Quanyi Li", "Tao Huang", "Jiangmiao Pang", "Tianfan Xue"], "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project page: https://xiao-chen.tech/gleam/", "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298", "abs": "https://arxiv.org/abs/2505.20298", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
{"id": "2505.20297", "pdf": "https://arxiv.org/pdf/2505.20297", "abs": "https://arxiv.org/abs/2505.20297", "authors": ["Qinyu Zhao", "Jaskirat Singh", "Ming Xu", "Akshay Asthana", "Stephen Gould", "Liang Zheng"], "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA", "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality."}
{"id": "2309.03824", "pdf": "https://arxiv.org/pdf/2309.03824", "abs": "https://arxiv.org/abs/2309.03824", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Yang Liu"], "title": "Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low Rank Decomposition (LRD) is a model compression technique applied to the\nweight tensors of deep learning models in order to reduce the number of\ntrainable parameters and computational complexity. However, due to high number\nof new layers added to the architecture after applying LRD, it may not lead to\na high training/inference acceleration if the decomposition ranks are not small\nenough. The issue is that using small ranks increases the risk of significant\naccuracy drop after decomposition. In this paper, we propose two techniques for\naccelerating low rank decomposed models without requiring to use small ranks\nfor decomposition. These methods include rank optimization and sequential\nfreezing of decomposed layers. We perform experiments on both convolutional and\ntransformer-based models. Experiments show that these techniques can improve\nthe model throughput up to 60% during training and 37% during inference when\ncombined together while preserving the accuracy close to that of the original\nmodels"}
{"id": "2505.18175", "pdf": "https://arxiv.org/pdf/2505.18175", "abs": "https://arxiv.org/abs/2505.18175", "authors": ["Natia Kukhilava", "Tatia Tsmindashvili", "Rapael Kalandadze", "Anchit Gupta", "Sofio Katamadze", "Franois Brmond", "Laura M. Ferrari", "Philipp Mller", "Benedikt Emanuel Wirth"], "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a\ngrowing research area in recent years. Analyzing 216 papers published between\n2018 and 2023, we uncover that the field lacks a unified evaluation protocol,\nwhich is essential to fairly define the state of the art, compare new\napproaches and to track the field's progress. We report the main\ninconsistencies between the used evaluation protocols, which are related to\nground truth definition, evaluation metric selection, data splitting types\n(e.g., subject-dependent or subject-independent) and the use of different\ndatasets. Capitalizing on this state-of-the-art research, we propose a unified\nevaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which\nenables an easy and efficient evaluation of new methods and datasets. EEGain is\na novel open source software framework, offering the capability to compare -\nand thus define - state-of-the-art results. EEGain includes standardized\nmethods for data pre-processing, data splitting, evaluation metrics, and the\nability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,\nMAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In\naddition, we have assessed and validated EEGain using these six datasets on the\nfour most common publicly available methods (EEGNet, DeepConvNet,\nShallowConvNet, TSception). This is a significant step to make research on\nEEG-ER more reproducible and comparable, thereby accelerating the overall\nprogress of the field."}
{"id": "2309.03965", "pdf": "https://arxiv.org/pdf/2309.03965", "abs": "https://arxiv.org/abs/2309.03965", "authors": ["Omar Mohamed Awad", "Habib Hajimolahoseini", "Michael Lim", "Gurpreet Gosal", "Walid Ahmed", "Yang Liu", "Gordon Deng"], "title": "Improving Resnet-9 Generalization Trained on Small Datasets", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper presents our proposed approach that won the first prize at the\nICLR competition on Hardware Aware Efficient Training. The challenge is to\nachieve the highest possible accuracy in an image classification task in less\nthan 10 minutes. The training is done on a small dataset of 5000 images picked\nrandomly from CIFAR-10 dataset. The evaluation is performed by the competition\norganizers on a secret dataset with 1000 images of the same size. Our approach\nincludes applying a series of technique for improving the generalization of\nResNet-9 including: sharpness aware optimization, label smoothing, gradient\ncentralization, input patch whitening as well as metalearning based training.\nOur experiments show that the ResNet-9 can achieve the accuracy of 88% while\ntrained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets"}
{"id": "2505.18184", "pdf": "https://arxiv.org/pdf/2505.18184", "abs": "https://arxiv.org/abs/2505.18184", "authors": ["Hania Ghouse", "Juveria Tanveen", "Abdul Muqtadir Ahmed", "Uma N. Dulhare"], "title": "AI- Enhanced Stethoscope in Remote Diagnostics for Cardiopulmonary Diseases", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "The increase in cardiac and pulmonary diseases presents an alarming and\npervasive health challenge on a global scale responsible for unexpected and\npremature mortalities. In spite of how serious these conditions are, existing\nmethods of detection and treatment encounter challenges, particularly in\nachieving timely diagnosis for effective medical intervention. Manual screening\nprocesses commonly used for primary detection of cardiac and respiratory\nproblems face inherent limitations, increased by a scarcity of skilled medical\npractitioners in remote or under-resourced areas. To address this, our study\nintroduces an innovative yet efficient model which integrates AI for diagnosing\nlung and heart conditions concurrently using the auscultation sounds. Unlike\nthe already high-priced digital stethoscope, our proposed model has been\nparticularly designed to deploy on low-cost embedded devices and thus ensure\napplicability in under-developed regions that actually face an issue of\naccessing medical care. Our proposed model incorporates MFCC feature extraction\nand engineering techniques to ensure that the signal is well analyzed for\naccurate diagnostics through the hybrid model combining Gated Recurrent Unit\nwith CNN in processing audio signals recorded from the low-cost stethoscope.\nBeyond its diagnostic capabilities, the model generates digital audio records\nthat facilitate in classifying six pulmonary and five cardiovascular diseases.\nHence, the integration of a cost effective stethoscope with an efficient AI\nempowered model deployed on a web app providing real-time analysis, represents\na transformative step towards standardized healthcare"}
{"id": "2311.03426", "pdf": "https://arxiv.org/pdf/2311.03426", "abs": "https://arxiv.org/abs/2311.03426", "authors": ["Farnoosh Javadi", "Walid Ahmed", "Habib Hajimolahoseini", "Foozhan Ataiefard", "Mohammad Hassanpour", "Saina Asani", "Austin Wen", "Omar Mohamed Awad", "Kangling Liu", "Yang Liu"], "title": "GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Massive transformer-based models face several challenges, including slow and\ncomputationally intensive pre-training and over-parametrization. This paper\naddresses these challenges by proposing a versatile method called GQKVA, which\ngeneralizes query, key, and value grouping techniques. GQKVA is designed to\nspeed up transformer pre-training while reducing the model size. Our\nexperiments with various GQKVA variants highlight a clear trade-off between\nperformance and model size, allowing for customized choices based on resource\nand time limitations. Our findings also indicate that the conventional\nmulti-head attention approach is not always the best choice, as there are\nlighter and faster alternatives available. We tested our method on ViT, which\nachieved an approximate 0.3% increase in accuracy while reducing the model size\nby about 4% in the task of image classification. Additionally, our most\naggressive model reduction experiment resulted in a reduction of approximately\n15% in model size, with only around a 1% drop in accuracy."}
{"id": "2505.18194", "pdf": "https://arxiv.org/pdf/2505.18194", "abs": "https://arxiv.org/abs/2505.18194", "authors": ["Yubo Peng", "Luping Xiang", "Bingxin Zhang", "Kun Yang"], "title": "Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications", "categories": ["eess.SP", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional single-modal sensing systems-based solely on either radio\nfrequency (RF) or visual data-struggle to cope with the demands of complex and\ndynamic environments. Furthermore, single-device systems are constrained by\nlimited perspectives and insufficient spatial coverage, which impairs their\neffectiveness in urban or non-line-of-sight scenarios. To overcome these\nchallenges, we propose a novel large language model (LLM)-driven distributed\nintegrated multimodal sensing and semantic communication (LLM-DiSAC) framework.\nSpecifically, our system consists of multiple collaborative sensing devices\nequipped with RF and camera modules, working together with an aggregation\ncenter to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC\ndevelops an RF-vision fusion network (RVFN), which employs specialized feature\nextractors for RF and visual data, followed by a cross-attention module for\neffective multimodal integration. Second, a LLM-based semantic transmission\nnetwork (LSTN) is proposed to enhance communication efficiency, where the\nLLM-based decoder leverages known channel parameters, such as transceiver\ndistance and signal-to-noise ratio (SNR), to mitigate semantic distortion.\nThird, at the aggregation center, a transformer-based aggregation model (TRAM)\nwith an adaptive aggregation attention mechanism is developed to fuse\ndistributed features and enhance sensing accuracy. To preserve data privacy, a\ntwo-stage distributed learning strategy is introduced, allowing local model\ntraining at the device level and centralized aggregation model training using\nintermediate features. Finally, evaluations on a synthetic multi-view RF-visual\ndataset generated by the Genesis simulation engine show that LLM-DiSAC achieves\na good performance."}
{"id": "2406.12634", "pdf": "https://arxiv.org/pdf/2406.12634", "abs": "https://arxiv.org/abs/2406.12634", "authors": ["Andreea Iana", "Fabian David Schmidt", "Goran Glava", "Heiko Paulheim"], "title": "News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; H.3.3"], "comment": "Accepted at the 47th European Conference on Information Retrieval\n  (ECIR 2025) Appendix A is provided only in the arXiv version", "summary": "Rapidly growing numbers of multilingual news consumers pose an increasing\nchallenge to news recommender systems in terms of providing customized\nrecommendations. First, existing neural news recommenders, even when powered by\nmultilingual language models (LMs), suffer substantial performance losses in\nzero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of\nfine-tuning the backbone LM of a neural recommender on task-specific data is\ncomputationally expensive and infeasible in few-shot recommendation and\ncold-start setups, where data is scarce or completely unavailable. In this\nwork, we propose a news-adapted sentence encoder (NaSE), domain-specialized\nfrom a pretrained massively multilingual sentence encoder (SE). To this end, we\nconstruct and leverage PolyNews and PolyNewsParallel, two multilingual\nnews-specific corpora. With the news-adapted multilingual SE in place, we test\nthe effectiveness of (i.e., question the need for) supervised fine-tuning for\nnews recommendation, and propose a simple and strong baseline based on (i)\nfrozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE\nachieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot\nnews recommendation."}
{"id": "2505.18365", "pdf": "https://arxiv.org/pdf/2505.18365", "abs": "https://arxiv.org/abs/2505.18365", "authors": ["Zhangxing Bian", "Shuwen Wei", "Xiao Liang", "Yuan-Chiao Lu", "Samuel W. Remedios", "Fangxu Xing", "Jonghye Woo", "Dzung L. Pham", "Aaron Carass", "Philip V. Bayly", "Jiachen Zhuo", "Ahmed Alshareef", "Jerry L. Prince"], "title": "Brightness-Invariant Tracking Estimation in Tagged MRI", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IPMI 2025", "summary": "Magnetic resonance (MR) tagging is an imaging technique for noninvasively\ntracking tissue motion in vivo by creating a visible pattern of magnetization\nsaturation (tags) that deforms with the tissue. Due to longitudinal relaxation\nand progression to steady-state, the tags and tissue brightnesses change over\ntime, which makes tracking with optical flow methods error-prone. Although\nFourier methods can alleviate these problems, they are also sensitive to\nbrightness changes as well as spectral spreading due to motion. To address\nthese problems, we introduce the brightness-invariant tracking estimation\n(BRITE) technique for tagged MRI. BRITE disentangles the anatomy from the tag\npattern in the observed tagged image sequence and simultaneously estimates the\nLagrangian motion. The inherent ill-posedness of this problem is addressed by\nleveraging the expressive power of denoising diffusion probabilistic models to\nrepresent the probabilistic distribution of the underlying anatomy and the\nflexibility of physics-informed neural networks to estimate\nbiologically-plausible motion. A set of tagged MR images of a gel phantom was\nacquired with various tag periods and imaging flip angles to demonstrate the\nimpact of brightness variations and to validate our method. The results show\nthat BRITE achieves more accurate motion and strain estimates as compared to\nother state of the art methods, while also being resistant to tag fading."}
{"id": "2407.20266", "pdf": "https://arxiv.org/pdf/2407.20266", "abs": "https://arxiv.org/abs/2407.20266", "authors": ["Habib Hajimolahoseini", "Walid Ahmed", "Austin Wen", "Yang Liu"], "title": "Accelerating the Low-Rank Decomposed Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Tensor decomposition is a mathematically supported technique for data\ncompression. It consists of applying some kind of a Low Rank Decomposition\ntechnique on the tensors or matrices in order to reduce the redundancy of the\ndata. However, it is not a popular technique for compressing the AI models duo\nto the high number of new layers added to the architecture after decomposition.\nAlthough the number of parameters could shrink significantly, it could result\nin the model be more than twice deeper which could add some latency to the\ntraining or inference. In this paper, we present a comprehensive study about\nhow to modify low rank decomposition technique in AI models so that we could\nbenefit from both high accuracy and low memory consumption as well as speeding\nup the training and inference"}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.16849", "pdf": "https://arxiv.org/pdf/2505.16849", "abs": "https://arxiv.org/abs/2505.16849", "authors": ["Martin Bckling", "Heiko Paulheim", "Andreea Iana"], "title": "Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks", "categories": ["cs.IR", "cs.CL", "H.3.3; I.2.7"], "comment": "Accepted at the Information Retrieval's Role in RAG Systems (IR-RAG\n  2025) in conjunction with SIGIR 2025", "summary": "Large Language Models (LLMs) have showcased impressive reasoning abilities,\nbut often suffer from hallucinations or outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) remedies these shortcomings by\ngrounding LLM responses in structured external information from a knowledge\nbase. However, many KG-based RAG approaches struggle with (i) aligning KG and\ntextual representations, (ii) balancing retrieval accuracy and efficiency, and\n(iii) adapting to dynamically updated KGs. In this work, we introduce\nWalk&Retrieve, a simple yet effective KG-based framework that leverages\nwalk-based graph traversal and knowledge verbalization for corpus generation\nfor zero-shot RAG. Built around efficient KG walks, our method does not require\nfine-tuning on domain-specific data, enabling seamless adaptation to KG\nupdates, reducing computational overhead, and allowing integration with any\noff-the-shelf backbone LLM. Despite its simplicity, Walk&Retrieve performs\ncompetitively, often outperforming existing RAG systems in response accuracy\nand hallucination reduction. Moreover, it demonstrates lower query latency and\nrobust scalability to large KGs, highlighting the potential of lightweight\nretrieval strategies as strong baselines for future RAG research."}
{"id": "2505.18424", "pdf": "https://arxiv.org/pdf/2505.18424", "abs": "https://arxiv.org/abs/2505.18424", "authors": ["Tianyi Ren", "Juampablo E. Heras Rivera", "Hitender Oswal", "Yutong Pan", "William Henry", "Jacob Ruzevick", "Mehmet Kurt"], "title": "How We Won the ISLES'24 Challenge by Preprocessing", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Stroke is among the top three causes of death worldwide, and accurate\nidentification of stroke lesion boundaries is critical for diagnosis and\ntreatment. Supervised deep learning methods have emerged as the leading\nsolution for stroke lesion segmentation but require large, diverse, and\nannotated datasets. The ISLES'24 challenge addresses this need by providing\nlongitudinal stroke imaging data, including CT scans taken on arrival to the\nhospital and follow-up MRI taken 2-9 days from initial arrival, with\nannotations derived from follow-up MRI. Importantly, models submitted to the\nISLES'24 challenge are evaluated using only CT inputs, requiring prediction of\nlesion progression that may not be visible in CT scans for segmentation. Our\nwinning solution shows that a carefully designed preprocessing pipeline\nincluding deep-learning-based skull stripping and custom intensity windowing is\nbeneficial for accurate segmentation. Combined with a standard large residual\nnnU-Net architecture for segmentation, this approach achieves a mean test Dice\nof 28.5 with a standard deviation of 21.27."}
{"id": "2505.18212", "pdf": "https://arxiv.org/pdf/2505.18212", "abs": "https://arxiv.org/abs/2505.18212", "authors": ["Barbara Puccio", "Federico Castagna", "Allan Tucker", "Pierangelo Veltri"], "title": "Towards medical AI misalignment: a preliminary study", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Despite their staggering capabilities as assistant tools, often exceeding\nhuman performances, Large Language Models (LLMs) are still prone to jailbreak\nattempts from malevolent users. Although red teaming practices have already\nidentified and helped to address several such jailbreak techniques, one\nparticular sturdy approach involving role-playing (which we named `Goofy Game')\nseems effective against most of the current LLMs safeguards. This can result in\nthe provision of unsafe content, which, although not harmful per se, might lead\nto dangerous consequences if delivered in a setting such as the medical domain.\nIn this preliminary and exploratory study, we provide an initial analysis of\nhow, even without technical knowledge of the internal architecture and\nparameters of generative AI models, a malicious user could construct a\nrole-playing prompt capable of coercing an LLM into producing incorrect (and\npotentially harmful) clinical suggestions. We aim to illustrate a specific\nvulnerability scenario, providing insights that can support future advancements\nin the field."}
{"id": "2505.18487", "pdf": "https://arxiv.org/pdf/2505.18487", "abs": "https://arxiv.org/abs/2505.18487", "authors": ["Junlin Wang", "Zhiyun Lin"], "title": "Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "A preprint version", "summary": "Learning effective visual representations for robotic manipulation remains a\nfundamental challenge due to the complex body dynamics involved in action\nexecution. In this paper, we study how visual representations that carry\nbody-relevant cues can enable efficient policy learning for downstream robotic\nmanipulation tasks. We present $\\textbf{I}$nter-token $\\textbf{Con}$trast\n($\\textbf{ICon}$), a contrastive learning method applied to the token-level\nrepresentations of Vision Transformers (ViTs). ICon enforces a separation in\nthe feature space between agent-specific and environment-specific tokens,\nresulting in agent-centric visual representations that embed body-specific\ninductive biases. This framework can be seamlessly integrated into end-to-end\npolicy learning by incorporating the contrastive loss as an auxiliary\nobjective. Our experiments show that ICon not only improves policy performance\nacross various manipulation tasks but also facilitates policy transfer across\ndifferent robots. The project website: https://github.com/HenryWJL/icon"}
{"id": "2505.18221", "pdf": "https://arxiv.org/pdf/2505.18221", "abs": "https://arxiv.org/abs/2505.18221", "authors": ["Sharad Duwal", "Mir Nafis Sharear Shopnil", "Abhishek Tyagi", "Adiba Mahbub Proma"], "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods."}
{"id": "2505.18531", "pdf": "https://arxiv.org/pdf/2505.18531", "abs": "https://arxiv.org/abs/2505.18531", "authors": ["Jiayi Zhou", "Jiaming Ji", "Boyuan Chen", "Jiapeng Sun", "Wenqi Chen", "Donghai Hong", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference", "categories": ["cs.AI", "cs.CV"], "comment": "9 pages, 8 figures", "summary": "Training multi-modal large language models (MLLMs) that align with human\nintentions is a long-term challenge. Traditional score-only reward models for\nalignment suffer from low accuracy, weak generalization, and poor\ninterpretability, blocking the progress of alignment methods, e.g.,\nreinforcement learning from human feedback (RLHF). Generative reward models\n(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate\npair-wise responses, but their pair-wise paradigm makes it hard to generalize\nto learnable rewards. We introduce Generative RLHF-V, a novel alignment\nframework that integrates GRMs with multi-modal RLHF. We propose a two-stage\npipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL\nguides GRMs to actively capture human intention, then predict the correct\npair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which\nenhances multi-modal RL scoring precision by grouped responses comparison.\nExperimental results demonstrate that, besides out-of-distribution\ngeneralization of RM discrimination, our framework improves 4 MLLMs'\nperformance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only\n$5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear\nimprovement with an increasing number of candidate responses. Our code and\nmodels can be found at https://generative-rlhf-v.github.io."}
{"id": "2505.18232", "pdf": "https://arxiv.org/pdf/2505.18232", "abs": "https://arxiv.org/abs/2505.18232", "authors": ["Mingkuan Feng", "Jinyang Wu", "Siyuan Liu", "Shuai Zhang", "Hongjian Fang", "Ruihan Jin", "Feihu Che", "Pengpeng Shao", "Zhengqi Wen", "Jianhua Tao"], "title": "ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The deployment of Large language models (LLMs) in many fields is largely\nhindered by their high computational and memory costs. Recent studies suggest\nthat LLMs exhibit sparsity, which can be used for pruning. Previous pruning\nmethods typically follow a prune-then-finetune paradigm. Since the pruned parts\nstill contain valuable information, statically removing them without updating\nthe remaining parameters often results in irreversible performance degradation,\nrequiring costly recovery fine-tuning (RFT) to maintain performance. To address\nthis, we propose a novel paradigm: first apply regularization, then prune.\nBased on this paradigm, we propose ELDeR: Getting Efficient LLMs through\nData-Driven Regularized Layer-wise Pruning. We multiply the output of each\ntransformer layer by an initial weight, then we iteratively learn the weights\nof each transformer layer by using a small amount of data in a simple way.\nAfter that, we apply regularization to the difference between the output and\ninput of the layers with smaller weights, forcing the information to be\ntransferred to the remaining layers. Compared with direct pruning, ELDeR\nreduces the information loss caused by direct parameter removal, thus better\npreserving the model's language modeling ability. Experimental results show\nthat ELDeR achieves superior performance compared with powerful layer-wise\nstructured pruning methods, while greatly reducing RFT computational costs.\nSince ELDeR is a layer-wise pruning method, its end-to-end acceleration effect\nis obvious, making it a promising technique for efficient LLMs."}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2505.18246", "pdf": "https://arxiv.org/pdf/2505.18246", "abs": "https://arxiv.org/abs/2505.18246", "authors": ["Yusuf Yildiz", "Goran Nenadic", "Meghna Jani", "David A. Jenkins"], "title": "Will Large Language Models Transform Clinical Prediction?", "categories": ["cs.CY", "cs.CL"], "comment": "Submitted to: BMC Diagnostic and Prognostic Research", "summary": "Background: Large language models (LLMs) are attracting increasing interest\nin healthcare. Their ability to summarise large datasets effectively, answer\nquestions accurately, and generate synthesised text is widely recognised. These\ncapabilities are already finding applications in healthcare. Body: This\ncommentary discusses LLMs usage in the clinical prediction context and\nhighlight potential benefits and existing challenges. In these early stages,\nthe focus should be on extending the methodology, specifically on validation,\nfairness and bias evaluation, survival analysis and development of regulations.\nConclusion: We conclude that further work and domain-specific considerations\nneed to be made for full integration into the clinical prediction workflows."}
{"id": "2505.18546", "pdf": "https://arxiv.org/pdf/2505.18546", "abs": "https://arxiv.org/abs/2505.18546", "authors": ["Dristi Datta", "Manoranjan Paul", "Manzur Murshed", "Shyh Wei Teng", "Leigh M. Schmidtke"], "title": "ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Soil organic carbon (SOC) is a critical indicator of soil health, but its\naccurate estimation from satellite imagery is hindered in vegetated regions due\nto spectral contamination from plant cover, which obscures soil reflectance and\nreduces model reliability. This study proposes the Reflectance Transformation\nGenerative Adversarial Network (ReflectGAN), a novel paired GAN-based framework\ndesigned to reconstruct accurate bare soil reflectance from vegetated soil\nsatellite observations. By learning the spectral transformation between\nvegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC\nestimation under mixed land cover conditions. Using the LUCAS 2018 dataset and\ncorresponding Landsat 8 imagery, we trained multiple learning-based models on\nboth original and ReflectGAN-reconstructed reflectance inputs. Models trained\non ReflectGAN outputs consistently outperformed those using existing vegetation\ncorrection methods. For example, the best-performing model (RF) achieved an\n$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the\nReflectGAN-generated signals, representing a 35\\% increase in $R^2$, a 43\\%\nreduction in RMSE, and a 43\\% improvement in RPD compared to the best existing\nmethod (PMM-SU). The performance of the models with ReflectGAN is also better\ncompared to their counterparts when applied to another dataset, i.e.,\nSentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to\nimprove SOC estimation accuracy in vegetated landscapes, supporting more\nreliable soil monitoring."}
{"id": "2505.18279", "pdf": "https://arxiv.org/pdf/2505.18279", "abs": "https://arxiv.org/abs/2505.18279", "authors": ["Alireza Rezazadeh", "Zichao Li", "Ange Lou", "Yuying Zhao", "Wei Wei", "Yujia Bao"], "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations."}
{"id": "2505.18547", "pdf": "https://arxiv.org/pdf/2505.18547", "abs": "https://arxiv.org/abs/2505.18547", "authors": ["Min Cheng", "Fatemeh Doudi", "Dileep Kalathil", "Mohammad Ghavamzadeh", "Panganamala R. Kumar"], "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement learning (RL) algorithms have been used recently to align\ndiffusion models with downstream objectives such as aesthetic quality and\ntext-image consistency by fine-tuning them to maximize a single reward function\nunder a fixed KL regularization. However, this approach is inherently\nrestrictive in practice, where alignment must balance multiple, often\nconflicting objectives. Moreover, user preferences vary across prompts,\nindividuals, and deployment contexts, with varying tolerances for deviation\nfrom a pre-trained base model. We address the problem of inference-time\nmulti-preference alignment: given a set of basis reward functions and a\nreference KL regularization strength, can we design a fine-tuning procedure so\nthat, at inference time, it can generate images aligned with any user-specified\nlinear combination of rewards and regularization, without requiring additional\nfine-tuning? We propose Diffusion Blend, a novel approach to solve\ninference-time multi-preference alignment by blending backward diffusion\nprocesses associated with fine-tuned models, and we instantiate this approach\nwith two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL\nregularization control. Extensive experiments show that Diffusion Blend\nalgorithms consistently outperform relevant baselines and closely match or\nexceed the performance of individually fine-tuned models, enabling efficient,\nuser-driven alignment at inference-time. The code is available at\nhttps://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025."}
{"id": "2505.18350", "pdf": "https://arxiv.org/pdf/2505.18350", "abs": "https://arxiv.org/abs/2505.18350", "authors": ["Waleed Reda", "Abhinav Jangda", "Krishna Chintalapudi"], "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models."}
{"id": "2505.18568", "pdf": "https://arxiv.org/pdf/2505.18568", "abs": "https://arxiv.org/abs/2505.18568", "authors": ["Zhikang Chen", "Abudukelimu Wuerkaixi", "Sen Cui", "Haoxuan Li", "Ding Li", "Jingfeng Zhang", "Bo Han", "Gang Niu", "Houfang Liu", "Yi Yang", "Sifan Yang", "Changshui Zhang", "Tianling Ren"], "title": "Learning without Isolation: Pathway Protection for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages", "summary": "Deep networks are prone to catastrophic forgetting during sequential task\nlearning, i.e., losing the knowledge about old tasks upon learning new tasks.\nTo this end, continual learning(CL) has emerged, whose existing methods focus\nmostly on regulating or protecting the parameters associated with the previous\ntasks. However, parameter protection is often impractical, since the size of\nparameters for storing the old-task knowledge increases linearly with the\nnumber of tasks, otherwise it is hard to preserve the parameters related to the\nold-task knowledge. In this work, we bring a dual opinion from neuroscience and\nphysics to CL: in the whole networks, the pathways matter more than the\nparameters when concerning the knowledge acquired from the old tasks. Following\nthis opinion, we propose a novel CL framework, learning without isolation(LwI),\nwhere model fusion is formulated as graph matching and the pathways occupied by\nthe old tasks are protected without being isolated. Thanks to the sparsity of\nactivation channels in a deep network, LwI can adaptively allocate available\npathways for a new task, realizing pathway protection and addressing\ncatastrophic forgetting in a parameter-efficient manner. Experiments on popular\nbenchmark datasets demonstrate the superiority of the proposed LwI."}
{"id": "2505.18366", "pdf": "https://arxiv.org/pdf/2505.18366", "abs": "https://arxiv.org/abs/2505.18366", "authors": ["Hansa Meghwani", "Amit Agarwal", "Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Srikant Panda"], "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "H.3.3; I.2.6; I.2.7"], "comment": "Accepted to ACL 2025", "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications."}
{"id": "2505.18603", "pdf": "https://arxiv.org/pdf/2505.18603", "abs": "https://arxiv.org/abs/2505.18603", "authors": ["Ye Mo", "Zirui Shao", "Kai Ye", "Xianwei Mao", "Bo Zhang", "Hangdi Xing", "Peng Ye", "Gang Huang", "Kehan Chen", "Zhou Huan", "Zixu Yan", "Sheng Zhou"], "title": "Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\ndocument understanding. However, the information-dense nature of document\nimages still poses challenges, as most queries depend on only a few relevant\nregions, with the rest being redundant. Existing one-pass MLLMs process entire\ndocument images without considering query relevance, often failing to focus on\ncritical regions and producing unfaithful responses. Inspired by the human\ncoarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a\nsimple-yet-effective mechanism that integrates human-style visual reasoning\ninto MLLM without modifying its architecture. Our method allows the model to\nautonomously select the set of regions (boxes) most relevant to the query, and\nthen focus attention on them for further understanding. We first design a fully\nautomatic pipeline, integrating a commercial MLLM with a layout analyzer, to\ngenerate 249k training samples with intermediate visual reasoning supervision.\nThen we incorporate two enabling tasks that improve box identification and\nbox-query reasoning, which together enhance document understanding. Extensive\nexperiments on seven benchmarks with four popular models show that Doc-CoB\nsignificantly improves performance, demonstrating its effectiveness and wide\napplicability. All code, data, and models will be released publicly."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.18625", "pdf": "https://arxiv.org/pdf/2505.18625", "abs": "https://arxiv.org/abs/2505.18625", "authors": ["Shivam Kumar Jha S", "Jaya NN Iyer"], "title": "Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra", "categories": ["math.AG", "cs.CV", "14T90, 14-04"], "comment": null, "summary": "This paper proposes a tropical geometry-based edge detection framework that\nreformulates convolution and gradient computations using min-plus and max-plus\nalgebra. The tropical formulation emphasizes dominant intensity variations,\ncontributing to sharper and more continuous edge representations. Three\nvariants are explored: an adaptive threshold-based method, a multi-kernel\nmin-plus method, and a max-plus method emphasizing structural continuity. The\nframework integrates multi-scale processing, Hessian filtering, and wavelet\nshrinkage to enhance edge transitions while maintaining computational\nefficiency. Experiments on MATLAB built-in grayscale and color images suggest\nthat tropical formulations integrated with classical operators, such as Canny\nand LoG, can improve boundary detection in low-contrast and textured regions.\nQuantitative evaluation using standard edge metrics indicates favorable edge\nclarity and structural coherence. These results highlight the potential of\ntropical algebra as a scalable and noise-aware formulation for edge detection\nin practical image analysis tasks."}
{"id": "2505.18451", "pdf": "https://arxiv.org/pdf/2505.18451", "abs": "https://arxiv.org/abs/2505.18451", "authors": ["Toshiaki Koike-Akino", "Jing Liu", "Ye Wang"], "title": "$$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 4 figures", "summary": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly."}
{"id": "2505.18664", "pdf": "https://arxiv.org/pdf/2505.18664", "abs": "https://arxiv.org/abs/2505.18664", "authors": ["Evgeny Ugolkov", "Xupeng He", "Hyung Kwak", "Hussein Hoteit"], "title": "Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "31 pages, 15 figures", "summary": "We present a memory-efficient algorithm for significantly enhancing the\nquality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks\nusing a generative model. The proposed model achieves a 16x increase in\nresolution and corrects inaccuracies in segmentation caused by the overlapping\nX-ray attenuation in micro-CT measurements across different minerals. The\ngenerative model employed is a 3D Octree-based convolutional Wasserstein\ngenerative adversarial network with gradient penalty. To address the challenge\nof high memory consumption inherent in standard 3D convolutional layers, we\nimplemented an Octree structure within the 3D progressive growing generator\nmodel. This enabled the use of memory-efficient 3D Octree-based convolutional\nlayers. The approach is pivotal in overcoming the long-standing memory\nbottleneck in volumetric deep learning, making it possible to reach 16x\nsuper-resolution in 3D, a scale that is challenging to attain due to cubic\nmemory scaling. For training, we utilized segmented 3D low-resolution micro-CT\nimages along with unpaired segmented complementary 2D high-resolution laser\nscanning microscope images. Post-training, resolution improved from 7 to 0.44\nmicro-m/voxel with accurate segmentation of constituent minerals. Validated on\nBerea sandstone, this framework demonstrates substantial improvements in pore\ncharacterization and mineral differentiation, offering a robust solution to one\nof the primary computational limitations in modern geoscientific imaging."}
{"id": "2505.18458", "pdf": "https://arxiv.org/pdf/2505.18458", "abs": "https://arxiv.org/abs/2505.18458", "authors": ["Xuanhe Zhou", "Junxuan He", "Wei Zhou", "Haodong Chen", "Zirui Tang", "Haoyu Zhao", "Xin Tong", "Guoliang Li", "Youmin Chen", "Jun Zhou", "Zhaojun Sun", "Binyuan Hui", "Shuo Wang", "Conghui He", "Zhiyuan Liu", "Jingren Zhou", "Fan Wu"], "title": "A Survey of LLM $\\times$ DATA", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Please refer to the paper list at:\n  https://github.com/weAIDB/awsome-data-llm", "summary": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."}
{"id": "2505.18772", "pdf": "https://arxiv.org/pdf/2505.18772", "abs": "https://arxiv.org/abs/2505.18772", "authors": ["Michal Edelstein", "Hsueh-Ti Derek Liu", "Mirela Ben-Chen"], "title": "CageNet: A Meta-Framework for Learning on Wild Meshes", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 13 figures (excluding supplementary material)", "summary": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes."}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464", "abs": "https://arxiv.org/abs/2505.18464", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies."}
{"id": "2505.18825", "pdf": "https://arxiv.org/pdf/2505.18825", "abs": "https://arxiv.org/abs/2505.18825", "authors": ["Nicholas M. Boffi", "Michael S. Albergo", "Eric Vanden-Eijnden"], "title": "How to build a consistency model: Learning flow maps via self-distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Building on the framework proposed in Boffi et al. (2024), we present a\nsystematic approach for learning flow maps associated with flow and diffusion\nmodels. Flow map-based models, commonly known as consistency models, encompass\nrecent efforts to improve the efficiency of generative models based on\nsolutions to differential equations. By exploiting a relationship between the\nvelocity field underlying a continuous-time flow and the instantaneous rate of\nchange of the flow map, we show how to convert existing distillation schemes\ninto direct training algorithms via self-distillation, eliminating the need for\npre-trained models. We empirically evaluate several instantiations of our\nframework, finding that high-dimensional tasks like image synthesis benefit\nfrom objective functions that avoid temporal and spatial derivatives of the\nflow map, while lower-dimensional tasks can benefit from objectives\nincorporating higher-order derivatives to capture sharp features."}
{"id": "2505.18467", "pdf": "https://arxiv.org/pdf/2505.18467", "abs": "https://arxiv.org/abs/2505.18467", "authors": ["Unggi Lee", "Jaeyong Lee", "Jiyeong Bae", "Yeil Jeong", "Junbo Koh", "Gyeonggeon Lee", "Gunho Lee", "Taekyung Ahn", "Hyeoncheol Kim"], "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 5 figures, 4 tables", "summary": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations."}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842", "abs": "https://arxiv.org/abs/2505.18842", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch."}
{"id": "2505.18488", "pdf": "https://arxiv.org/pdf/2505.18488", "abs": "https://arxiv.org/abs/2505.18488", "authors": ["Yanxiang Zhang", "Zheng Xu", "Shanshan Wu", "Yuanbo Zhang", "Daniel Ramage"], "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ACL Industry", "summary": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing."}
{"id": "2505.18884", "pdf": "https://arxiv.org/pdf/2505.18884", "abs": "https://arxiv.org/abs/2505.18884", "authors": ["Borna Khodabandeh", "Amirabbas Afzali", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi", "Sanjay Lall", "Sajjad Amini", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.OC"], "comment": null, "summary": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."}
{"id": "2505.18502", "pdf": "https://arxiv.org/pdf/2505.18502", "abs": "https://arxiv.org/abs/2505.18502", "authors": ["Guodong Du", "Xuanning Zhou", "Junlin Li", "Zhuo Li", "Zesheng Shi", "Wanyu Lin", "Ho-Kin Tang", "Xiucheng Li", "Fangming Liu", "Wenya Wang", "Min Zhang", "Jing Li"], "title": "Knowledge Grafting of Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM."}
{"id": "2505.18902", "pdf": "https://arxiv.org/pdf/2505.18902", "abs": "https://arxiv.org/abs/2505.18902", "authors": ["Laura Baracaldo", "Blythe King", "Haoran Yan", "Yizi Lin", "Nina Miolane", "Mengyang Gu"], "title": "Unsupervised cell segmentation by fast Gaussian Processes", "categories": ["stat.AP", "cs.CV"], "comment": null, "summary": "Cell boundary information is crucial for analyzing cell behaviors from\ntime-lapse microscopy videos. Existing supervised cell segmentation tools, such\nas ImageJ, require tuning various parameters and rely on restrictive\nassumptions about the shape of the objects. While recent supervised\nsegmentation tools based on convolutional neural networks enhance accuracy,\nthey depend on high-quality labelled images, making them unsuitable for\nsegmenting new types of objects not in the database. We developed a novel\nunsupervised cell segmentation algorithm based on fast Gaussian processes for\nnoisy microscopy images without the need for parameter tuning or restrictive\nassumptions about the shape of the object. We derived robust thresholding\ncriteria adaptive for heterogeneous images containing distinct brightness at\ndifferent parts to separate objects from the background, and employed watershed\nsegmentation to distinguish touching cell objects. Both simulated studies and\nreal-data analysis of large microscopy images demonstrate the scalability and\naccuracy of our approach compared with the alternatives."}
{"id": "2505.18512", "pdf": "https://arxiv.org/pdf/2505.18512", "abs": "https://arxiv.org/abs/2505.18512", "authors": ["Soyoung Yoon", "Gyuwan Kim", "Gyu-Hwung Cho", "Seung-won Hwang"], "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages, 3 figures. The first two authors contributed equally.\n  Author order is randomly determined via coin toss", "summary": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models."}
{"id": "2505.18983", "pdf": "https://arxiv.org/pdf/2505.18983", "abs": "https://arxiv.org/abs/2505.18983", "authors": ["Haotian Sun", "Yitong Li", "Yuchen Zhuang", "Niao He", "Hanjun Dai", "Bo Dai"], "title": "AmorLIP: Efficient Language-Image Pretraining via Amortization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has demonstrated strong\nzero-shot performance across diverse downstream text-image tasks. Existing CLIP\nmethods typically optimize a contrastive objective using negative samples drawn\nfrom each minibatch. To achieve robust representation learning, these methods\nrequire extremely large batch sizes and escalate computational demands to\nhundreds or even thousands of GPUs. Prior approaches to mitigate this issue\noften compromise downstream performance, prolong training duration, or face\nscalability challenges with very large datasets. To overcome these limitations,\nwe propose AmorLIP, an efficient CLIP pretraining framework that amortizes\nexpensive computations involved in contrastive learning through lightweight\nneural networks, which substantially improves training efficiency and\nperformance. Leveraging insights from a spectral factorization of energy-based\nmodels, we introduce novel amortization objectives along with practical\ntechniques to improve training stability. Extensive experiments across 38\ndownstream tasks demonstrate the superior zero-shot classification and\nretrieval capabilities of AmorLIP, consistently outperforming standard CLIP\nbaselines with substantial relative improvements of up to 12.24%."}
{"id": "2505.18545", "pdf": "https://arxiv.org/pdf/2505.18545", "abs": "https://arxiv.org/abs/2505.18545", "authors": ["An Vo", "Mohammad Reza Taesiri", "Daeyoung Kim", "Anh Totti Nguyen"], "title": "B-score: Detecting biases in large language models using response history", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025 (Main track)", "summary": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985", "abs": "https://arxiv.org/abs/2505.18985", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "title": "STRICT: Stress Test of Rendering Images Containing Text", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.18573", "pdf": "https://arxiv.org/pdf/2505.18573", "abs": "https://arxiv.org/abs/2505.18573", "authors": ["Mengqi Liao", "Xiangyu Xi", "Ruinian Chen", "Jia Leng", "Yangen Hu", "Ke Zeng", "Shuai Liu", "Huaiyu Wan"], "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) excel in complex tasks, which has\ndrawn significant attention to reinforcement learning (RL) for LLMs. However,\nexisting approaches allocate an equal number of rollouts to all questions\nduring the RL process, which is inefficient. This inefficiency stems from the\nfact that training on simple questions yields limited gains, whereas more\nrollouts are needed for challenging questions to sample correct answers.\nFurthermore, while RL improves response precision, it limits the model's\nexploration ability, potentially resulting in a performance cap below that of\nthe base model prior to RL. To address these issues, we propose a mechanism for\ndynamically allocating rollout budgets based on the difficulty of the problems,\nenabling more efficient RL training. Additionally, we introduce an adaptive\ndynamic temperature adjustment strategy to maintain the entropy at a stable\nlevel, thereby encouraging sufficient exploration. This enables LLMs to improve\nresponse precision while preserving their exploratory ability to uncover\npotential correct pathways. The code and data is available on:\nhttps://github.com/LiaoMengqi/E3-RL4LLMs"}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000", "abs": "https://arxiv.org/abs/2505.19000", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability."}
{"id": "2505.18585", "pdf": "https://arxiv.org/pdf/2505.18585", "abs": "https://arxiv.org/abs/2505.18585", "authors": ["Yedi Zhang", "Sun Yi Emma", "Annabelle Lee Jia En", "Annabelle Lee Jia En", "Jin Song Dong"], "title": "RvLLM: LLM Runtime Verification with Domain Knowledge", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "12 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs."}
{"id": "2505.19017", "pdf": "https://arxiv.org/pdf/2505.19017", "abs": "https://arxiv.org/abs/2505.19017", "authors": ["Yaxuan Li", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "WorldEval: World Model as Real-World Robot Policies Evaluator", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "The project page is available at https://worldeval.github.io", "summary": "The field of robotics has made significant strides toward developing\ngeneralist robot manipulation policies. However, evaluating these policies in\nreal-world scenarios remains time-consuming and challenging, particularly as\nthe number of tasks scales and environmental conditions change. In this work,\nwe demonstrate that world models can serve as a scalable, reproducible, and\nreliable proxy for real-world robot policy evaluation. A key challenge is\ngenerating accurate policy videos from world models that faithfully reflect the\nrobot actions. We observe that directly inputting robot actions or using\nhigh-dimensional encoding methods often fails to generate action-following\nvideos. To address this, we propose Policy2Vec, a simple yet effective approach\nto turn a video generation model into a world simulator that follows latent\naction to generate the robot video. We then introduce WorldEval, an automated\npipeline designed to evaluate real-world robot policies entirely online.\nWorldEval effectively ranks various robot policies and individual checkpoints\nwithin a single policy, and functions as a safety detector to prevent dangerous\nactions by newly developed robot models. Through comprehensive paired\nevaluations of manipulation policies in real-world environments, we demonstrate\na strong correlation between policy performance in WorldEval and real-world\nscenarios. Furthermore, our method significantly outperforms popular methods\nsuch as real-to-sim approach."}
{"id": "2505.18644", "pdf": "https://arxiv.org/pdf/2505.18644", "abs": "https://arxiv.org/abs/2505.18644", "authors": ["Jingran Xie", "Xiang Li", "Hui Wang", "Yue Yu", "Yang Xiang", "Xixin Wu", "Zhiyong Wu"], "title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted by Interspeech 2025", "summary": "Large language models (LLMs) have shown remarkable generalization across\ntasks, leading to increased interest in integrating speech with LLMs. These\nspeech LLMs (SLLMs) typically use supervised fine-tuning to align speech with\ntext-based LLMs. However, the lack of annotated speech data across a wide range\nof tasks hinders alignment efficiency, resulting in poor generalization. To\naddress these issues, we propose a novel multi-task 'behavior imitation' method\nwith speech-text interleaving, called MTBI, which relies solely on paired\nspeech and transcripts. By ensuring the LLM decoder generates equivalent\nresponses to paired speech and text, we achieve a more generalized SLLM.\nInterleaving is used to further enhance alignment efficiency. We introduce a\nsimple benchmark to evaluate prompt and task generalization across different\nmodels. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs\non both prompt and task generalization, while requiring less supervised speech\ndata."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.18646", "pdf": "https://arxiv.org/pdf/2505.18646", "abs": "https://arxiv.org/abs/2505.18646", "authors": ["Siwei Liu", "Jinyuan Fang", "Han Zhou", "Yingxu Wang", "Zaiqiao Meng"], "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) have demonstrated effectiveness in code\ngeneration tasks. To enable LLMs to address more complex coding challenges,\nexisting research has focused on crafting multi-agent systems with agentic\nworkflows, where complex coding tasks are decomposed into sub-tasks, assigned\nto specialized agents. Despite their effectiveness, current approaches heavily\nrely on hand-crafted agentic workflows, with both agent topologies and prompts\nmanually designed, which limits their ability to automatically adapt to\ndifferent types of coding problems. To address these limitations and enable\nautomated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving\n\\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that\nautomatically generates and optimises multi-agent workflows. Extensive\nexperiments on three coding benchmark datasets, including the challenging\nLiveCodeBench, demonstrate that our SEW can automatically design agentic\nworkflows and optimise them through self-evolution, bringing up to 33\\%\nimprovement on LiveCodeBench compared to using the backbone LLM only.\nFurthermore, by investigating different representation schemes of workflow, we\nprovide insights into the optimal way to encode workflow information with text."}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100", "abs": "https://arxiv.org/abs/2505.19100", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668", "abs": "https://arxiv.org/abs/2505.18668", "authors": ["Zhen Li", "Yukai Guo", "Duan Li", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "categories": ["cs.CV", "cs.CL"], "comment": "63 pages, submitted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675", "abs": "https://arxiv.org/abs/2505.18675", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.19151", "pdf": "https://arxiv.org/pdf/2505.19151", "abs": "https://arxiv.org/abs/2505.19151", "authors": ["Shenggan Cheng", "Yuanxin Wei", "Lansong Diao", "Yong Liu", "Bujiao Chen", "Lianghua Huang", "Yu Liu", "Wenyuan Yu", "Jiangsu Du", "Wei Lin", "Yang You"], "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Leveraging the diffusion transformer (DiT) architecture, models like Sora,\nCogVideoX and Wan have achieved remarkable progress in text-to-video,\nimage-to-video, and video editing tasks. Despite these advances,\ndiffusion-based video generation remains computationally intensive, especially\nfor high-resolution, long-duration videos. Prior work accelerates its inference\nby skipping computation, usually at the cost of severe quality degradation. In\nthis paper, we propose SRDiffusion, a novel framework that leverages\ncollaboration between large and small models to reduce inference cost. The\nlarge model handles high-noise steps to ensure semantic and motion fidelity\n(Sketching), while the smaller model refines visual details in low-noise steps\n(Rendering). Experimental results demonstrate that our method outperforms\nexisting approaches, over 3$\\times$ speedup for Wan with nearly no quality loss\nfor VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a\nnew direction orthogonal to existing acceleration strategies, offering a\npractical solution for scalable video generation."}
{"id": "2505.18680", "pdf": "https://arxiv.org/pdf/2505.18680", "abs": "https://arxiv.org/abs/2505.18680", "authors": ["Yuanhe Zhang", "Xinyue Wang", "Haoran Gao", "Zhenhong Zhou", "Fanyu Meng", "Yuyao Zhang", "Sen Su"], "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks."}
{"id": "2505.19190", "pdf": "https://arxiv.org/pdf/2505.19190", "abs": "https://arxiv.org/abs/2505.19190", "authors": ["Jiayi Xin", "Sukwon Yun", "Jie Peng", "Inyoung Choi", "Jenna L. Ballard", "Tianlong Chen", "Qi Long"], "title": "I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Poster", "summary": "Modality fusion is a cornerstone of multimodal learning, enabling information\nintegration from diverse data sources. However, vanilla fusion methods are\nlimited by (1) inability to account for heterogeneous interactions between\nmodalities and (2) lack of interpretability in uncovering the multimodal\ninteractions inherent in the data. To this end, we propose I2MoE (Interpretable\nMultimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework\ndesigned to enhance modality fusion by explicitly modeling diverse multimodal\ninteractions, as well as providing interpretation on a local and global level.\nFirst, I2MoE utilizes different interaction experts with weakly supervised\ninteraction losses to learn multimodal interactions in a data-driven way.\nSecond, I2MoE deploys a reweighting model that assigns importance scores for\nthe output of each interaction expert, which offers sample-level and\ndataset-level interpretation. Extensive evaluation of medical and general\nmultimodal datasets shows that I2MoE is flexible enough to be combined with\ndifferent fusion techniques, consistently improves task performance, and\nprovides interpretation across various real-world scenarios. Code is available\nat https://github.com/Raina-Xin/I2MoE."}
{"id": "2505.18713", "pdf": "https://arxiv.org/pdf/2505.18713", "abs": "https://arxiv.org/abs/2505.18713", "authors": ["Guodong Du", "Zitao Fang", "Jing Li", "Junlin Li", "Runhua Jiang", "Shuyang Yu", "Yifei Guo", "Yangneng Chen", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Honghai Liu", "Min Zhang"], "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ACL2025 Main", "summary": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning."}
{"id": "2505.19195", "pdf": "https://arxiv.org/pdf/2505.19195", "abs": "https://arxiv.org/abs/2505.19195", "authors": ["Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lian-Ming Wu", "Xiaosong Wang"], "title": "CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Accurate prediction of major adverse cardiovascular events recurrence risk in\nacute myocardial infarction patients based on postoperative cardiac MRI and\nassociated clinical notes is crucial for precision treatment and personalized\nintervention. Existing methods primarily focus on risk stratification\ncapability while overlooking the need for intermediate robust reasoning and\nmodel interpretability in clinical practice. Moreover, end-to-end risk\nprediction using LLM/VLM faces significant challenges due to data limitations\nand modeling complexity. To bridge this gap, we propose CardioCoT, a novel\ntwo-stage hierarchical reasoning-enhanced survival analysis framework designed\nto enhance both model interpretability and predictive performance. In the first\nstage, we employ an evidence-augmented self-refinement mechanism to guide\nLLM/VLMs in generating robust hierarchical reasoning trajectories based on\nassociated radiological findings. In the second stage, we integrate the\nreasoning trajectories with imaging data for risk model training and\nprediction. CardioCoT demonstrates superior performance in MACE recurrence risk\nprediction while providing interpretable reasoning processes, offering valuable\ninsights for clinical decision-making."}
{"id": "2505.18722", "pdf": "https://arxiv.org/pdf/2505.18722", "abs": "https://arxiv.org/abs/2505.18722", "authors": ["Terry Yi Zhong", "Esther Janse", "Cristian Tejedor-Garcia", "Louis ten Bosch", "Martha Larson"], "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted for Interspeech 2025 (Camera-Ready)", "summary": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance."}
{"id": "2505.19225", "pdf": "https://arxiv.org/pdf/2505.19225", "abs": "https://arxiv.org/abs/2505.19225", "authors": ["Chenglong Ma", "Yuanfeng Ji", "Jin Ye", "Zilong Li", "Chenhui Wang", "Junzhi Ning", "Wei Li", "Lihao Liu", "Qiushan Guo", "Tianbin Li", "Junjun He", "Hongming Shan"], "title": "MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Advanced autoregressive models have reshaped multimodal AI. However, their\ntransformative potential in medical imaging remains largely untapped due to the\nabsence of a unified visual tokenizer -- one capable of capturing fine-grained\nvisual structures for faithful image reconstruction and realistic image\nsynthesis, as well as rich semantics for accurate diagnosis and image\ninterpretation. To this end, we present MedITok, the first unified tokenizer\ntailored for medical images, encoding both low-level structural details and\nhigh-level clinical semantics within a unified latent space. To balance these\ncompeting objectives, we introduce a novel two-stage training framework: a\nvisual representation alignment stage that cold-starts the tokenizer\nreconstruction learning with a visual semantic constraint, followed by a\ntextual semantic representation alignment stage that infuses detailed clinical\nsemantics into the latent space. Trained on the meticulously collected\nlarge-scale dataset with over 30 million medical images and 2 million\nimage-caption pairs, MedITok achieves state-of-the-art performance on more than\n30 datasets across 9 imaging modalities and 4 different tasks. By providing a\nunified token space for autoregressive modeling, MedITok supports a wide range\nof tasks in clinical diagnostics and generative healthcare applications. Model\nand code will be made publicly available at:\nhttps://github.com/Masaaki-75/meditok."}
{"id": "2505.18789", "pdf": "https://arxiv.org/pdf/2505.18789", "abs": "https://arxiv.org/abs/2505.18789", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Boris Ginsburg"], "title": "From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Post-processing is crucial for the automatic evaluation of LLMs in\nfill-in-the-middle (FIM) code generation due to the frequent presence of\nextraneous code in raw outputs. This extraneous generation suggests a lack of\nawareness regarding output boundaries, requiring truncation for effective\nevaluation. The determination of an optimal truncation strategy, however, often\nproves intricate, particularly when the scope includes several programming\nlanguages. This study investigates the necessity of post-processing\ninstruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning\nsignificantly enhances FIM code generation, enabling LLMs to generate code that\nseamlessly integrates with the surrounding context. Evaluating our fine-tuned\n\\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and\nSAFIM benchmarks demonstrates improved performances without post-processing,\nespecially when the \\emph{middle} consist of complete lines. However,\npost-processing of the LLM outputs remains necessary when the \\emph{middle} is\na random span of code."}
{"id": "2505.19235", "pdf": "https://arxiv.org/pdf/2505.19235", "abs": "https://arxiv.org/abs/2505.19235", "authors": ["Qinsi Wang", "Hancheng Ye", "Ming-Yu Chung", "Yudong Liu", "Yueqian Lin", "Martin Kuo", "Mingyuan Ma", "Jianyi Zhang", "Yiran Chen"], "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high\ninference costs in time and memory. Token sparsity mitigates inefficiencies in\ntoken usage, while neuron sparsity reduces high-dimensional computations, both\noffering promising solutions to enhance efficiency. Recently, these two\nsparsity paradigms have evolved largely in parallel, fostering the prevailing\nassumption that they function independently. However, a fundamental yet\nunderexplored question remains: Do they truly operate in isolation, or is there\na deeper underlying interplay that has yet to be uncovered? In this paper, we\nconduct the first comprehensive investigation into this question. By\nintroducing and analyzing the matching mechanism between Core Neurons and Core\nTokens, we found that key neurons and tokens for inference mutually influence\nand reinforce each other. Building on this insight, we propose CoreMatching, a\nco-adaptive sparse inference framework, which leverages the synergy between\ntoken and neuron sparsity to enhance inference efficiency. Through theoretical\nanalysis and efficiency evaluations, we demonstrate that the proposed method\nsurpasses state-of-the-art baselines on ten image understanding tasks and three\nhardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs\nreduction and a 10x overall speedup. Code is released at\nhttps://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."}
{"id": "2505.18822", "pdf": "https://arxiv.org/pdf/2505.18822", "abs": "https://arxiv.org/abs/2505.18822", "authors": ["Shijue Huang", "Hongru Wang", "Wanjun Zhong", "Zhaochen Su", "Jiazhan Feng", "Bowen Cao", "Yi R. Fung"], "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs."}
{"id": "2505.19249", "pdf": "https://arxiv.org/pdf/2505.19249", "abs": "https://arxiv.org/abs/2505.19249", "authors": ["Mir Sazzat Hossain", "Khan Muhammad Bin Asad", "Payaswini Saikia", "Adrita Khan", "Md Akil Raihan Iftee", "Rakibul Hasan Rajib", "Arshad Momen", "Md Ashraful Amin", "Amin Ahsan Ali", "AKM Mahbubur Rahman"], "title": "RGC-Bent: A Novel Dataset for Bent Radio Galaxy Classification", "categories": ["astro-ph.GA", "cs.CV"], "comment": "6 pages, 3 figures, 2 tables, Accepted In ICIP 2025", "summary": "We introduce a novel machine learning dataset tailored for the classification\nof bent radio active galactic nuclei (AGN) in astronomical observations. Bent\nradio AGN, distinguished by their curved jet structures, provide critical\ninsights into galaxy cluster dynamics, interactions within the intracluster\nmedium, and the broader physics of AGN. Despite their astrophysical\nsignificance, the classification of bent radio AGN remains a challenge due to\nthe scarcity of specialized datasets and benchmarks. To address this, we\npresent a dataset, derived from a well-recognized radio astronomy survey, that\nis designed to support the classification of NAT (Narrow-Angle Tail) and WAT\n(Wide-Angle Tail) categories, along with detailed data processing steps. We\nfurther evaluate the performance of state-of-the-art deep learning models on\nthe dataset, including Convolutional Neural Networks (CNNs), and\ntransformer-based architectures. Our results demonstrate the effectiveness of\nadvanced machine learning models in classifying bent radio AGN, with ConvNeXT\nachieving the highest F1-scores for both NAT and WAT sources. By sharing this\ndataset and benchmarks, we aim to facilitate the advancement of research in AGN\nclassification, galaxy cluster environments and galaxy evolution."}
{"id": "2505.18830", "pdf": "https://arxiv.org/pdf/2505.18830", "abs": "https://arxiv.org/abs/2505.18830", "authors": ["Wenlong Deng", "Yi Ren", "Muchen Li", "Danica J. Sutherland", "Xiaoxiao Li", "Christos Thrampoulidis"], "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters."}
{"id": "2505.19306", "pdf": "https://arxiv.org/pdf/2505.19306", "abs": "https://arxiv.org/abs/2505.19306", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image."}
{"id": "2505.18847", "pdf": "https://arxiv.org/pdf/2505.18847", "abs": "https://arxiv.org/abs/2505.18847", "authors": ["William Han", "Chaojing Duan", "Zhepeng Cen", "Yihang Yao", "Xiaoyu Song", "Atharva Mhaskar", "Dylan Leong", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework", "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 2 figures, 8 tables", "summary": "Recent advances have increasingly applied large language models (LLMs) to\nelectrocardiogram (ECG) interpretation, giving rise to\nElectrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual\nquery, an ELM autoregressively generates a free-form textual response. Unlike\ntraditional classification-based systems, ELMs emulate expert cardiac\nelectrophysiologists by issuing diagnoses, analyzing waveform morphology,\nidentifying contributing factors, and proposing patient-specific action plans.\nTo realize this potential, researchers are curating instruction-tuning datasets\nthat pair ECGs with textual dialogues and are training ELMs on these resources.\nYet before scaling ELMs further, there is a fundamental question yet to be\nexplored: What is the most effective ECG input representation? In recent works,\nthree candidate representations have emerged-raw time-series signals, rendered\nimages, and discretized symbolic sequences. We present the first comprehensive\nbenchmark of these modalities across 6 public datasets and 5 evaluation\nmetrics. We find symbolic representations achieve the greatest number of\nstatistically significant wins over both signal and image inputs. We further\nablate the LLM backbone, ECG duration, and token budget, and we evaluate\nrobustness to signal perturbations. We hope that our findings offer clear\nguidance for selecting input representations when developing the next\ngeneration of ELMs."}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354", "abs": "https://arxiv.org/abs/2505.19354", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public."}
{"id": "2505.18855", "pdf": "https://arxiv.org/pdf/2505.18855", "abs": "https://arxiv.org/abs/2505.18855", "authors": ["Peiqi Wang", "ShengYun Peng", "Xuewen Zhang", "Hanchao Yu", "Yibo Yang", "Lifu Huang", "Fujun Liu", "Qifan Wang"], "title": "Inference Compute-Optimal Video Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025", "summary": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors."}
{"id": "2505.19361", "pdf": "https://arxiv.org/pdf/2505.19361", "abs": "https://arxiv.org/abs/2505.19361", "authors": ["Mario Leiva", "Noel Ngu", "Joshua Shay Kricheli", "Aditya Taparia", "Ransalu Senanayake", "Paulo Shakarian", "Nathaniel Bastian", "John Corcoran", "Gerardo Simari"], "title": "Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.LO"], "comment": null, "summary": "The deployment of pre-trained perception models in novel environments often\nleads to performance degradation due to distributional shifts. Although recent\nartificial intelligence approaches for metacognition use logical rules to\ncharacterize and filter model errors, improving precision often comes at the\ncost of reduced recall. This paper addresses the hypothesis that leveraging\nmultiple pre-trained models can mitigate this recall reduction. We formulate\nthe challenge of identifying and managing conflicting predictions from various\nmodels as a consistency-based abduction problem. The input predictions and the\nlearned error detection rules derived from each model are encoded in a logic\nprogram. We then seek an abductive explanation--a subset of model\npredictions--that maximizes prediction coverage while ensuring the rate of\nlogical inconsistencies (derived from domain constraints) remains below a\nspecified threshold. We propose two algorithms for this knowledge\nrepresentation task: an exact method based on Integer Programming (IP) and an\nefficient Heuristic Search (HS). Through extensive experiments on a simulated\naerial imagery dataset featuring controlled, complex distributional shifts, we\ndemonstrate that our abduction-based framework outperforms individual models\nand standard ensemble baselines, achieving, for instance, average relative\nimprovements of approximately 13.6% in F1-score and 16.6% in accuracy across 15\ndiverse test datasets when compared to the best individual model. Our results\nvalidate the use of consistency-based abduction as an effective mechanism to\nrobustly integrate knowledge from multiple imperfect reasoners in challenging,\nnovel scenarios."}
{"id": "2505.18929", "pdf": "https://arxiv.org/pdf/2505.18929", "abs": "https://arxiv.org/abs/2505.18929", "authors": ["Wenda Zhang"], "title": "Meta-aware Learning in text-to-SQL Large Language Model", "categories": ["cs.AI", "cs.CL"], "comment": "Keywords: text-to-SQL LLM, fine-tuning, meta-aware leanring,\n  metadata, chain-of-thought, BigQuery SQL, business database", "summary": "The advancements of Large language models (LLMs) have provided great\nopportunities to text-to-SQL tasks to overcome the main challenges to\nunderstand complex domain information and complex database structures in\nbusiness applications. In this paper, we propose a meta-aware learning\nframework to integrate domain knowledge, database schema, chain-of-thought\nreasoning processes, and metadata relationships to improve the SQL generation\nquality. The proposed framework includes four learning strategies: schema-based\nlearning, Chain-of-Thought (CoT) learning, knowledge-enhanced learning, and key\ninformation tokenization. This approach provides a comprehensive understanding\nof database structure and metadata information towards LLM through fine-tuning\nto improve its performance on SQL generation within business domains. Through\ntwo experimental studies, we have demonstrated the superiority of the proposed\nmethods in execution accuracy, multi-task SQL generation capability, and\nreduction of catastrophic forgetting."}
{"id": "2505.19381", "pdf": "https://arxiv.org/pdf/2505.19381", "abs": "https://arxiv.org/abs/2505.19381", "authors": ["Anqing Jiang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Jijun Wang", "Jinghao Chai", "Qian Cao", "Yuweng Heng", "Hao Jiang", "Zongzheng Zhang", "Xianda Guo", "Hao Sun", "Hao Zhao"], "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "4pages", "summary": "Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS."}
{"id": "2505.18931", "pdf": "https://arxiv.org/pdf/2505.18931", "abs": "https://arxiv.org/abs/2505.18931", "authors": ["Ryan Saklad", "Aman Chadha", "Oleg Pavlov", "Raha Moraffah"], "title": "Can Large Language Models Infer Causal Relationships from Real-World Text?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning."}
{"id": "2505.19404", "pdf": "https://arxiv.org/pdf/2505.19404", "abs": "https://arxiv.org/abs/2505.19404", "authors": ["Yuta Ono", "Hiroshi Nakamura", "Hideki Takase"], "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "6 pages. Accepted at COMPSAC 2025", "summary": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations."}
{"id": "2505.18933", "pdf": "https://arxiv.org/pdf/2505.18933", "abs": "https://arxiv.org/abs/2505.18933", "authors": ["Haitian Zhong", "Yuhuan Liu", "Ziyang Xu", "Guofan Liu", "Qiang Liu", "Shu Wu", "Zhe Zhao", "Liang Wang", "Tieniu Tan"], "title": "REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing", "categories": ["cs.AI", "cs.CL"], "comment": "15 pages, 4 figures", "summary": "Large language model editing methods frequently suffer from overfitting,\nwherein factual updates can propagate beyond their intended scope,\noveremphasizing the edited target even when it's contextually inappropriate. To\naddress this challenge, we introduce REACT (Representation Extraction And\nControllable Tuning), a unified two-phase framework designed for precise and\ncontrollable knowledge editing. In the initial phase, we utilize tailored\nstimuli to extract latent factual representations and apply Principal Component\nAnalysis with a simple learnbale linear transformation to compute a directional\n\"belief shift\" vector for each instance. In the second phase, we apply\ncontrollable perturbations to hidden states using the obtained vector with a\nmagnitude scalar, gated by a pre-trained classifier that permits edits only\nwhen contextually necessary. Relevant experiments on EVOKE benchmarks\ndemonstrate that REACT significantly reduces overfitting across nearly all\nevaluation metrics, and experiments on COUNTERFACT and MQuAKE shows that our\nmethod preserves balanced basic editing performance (reliability, locality, and\ngenerality) under diverse editing scenarios."}
{"id": "2505.19447", "pdf": "https://arxiv.org/pdf/2505.19447", "abs": "https://arxiv.org/abs/2505.19447", "authors": ["Hengtong Shen", "Haiyan Gu", "Haitao Li", "Yi Yang", "Agen qiu"], "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works."}
{"id": "2505.18942", "pdf": "https://arxiv.org/pdf/2505.18942", "abs": "https://arxiv.org/abs/2505.18942", "authors": ["Honglin Bao", "Siyang Wu", "Jiwoong Choi", "Yingrong Mao", "James A. Evans"], "title": "Language Models Surface the Unwritten Code of Science and Society", "categories": ["cs.CY", "cs.CL", "cs.DL"], "comment": null, "summary": "This paper calls on the research community not only to investigate how human\nbiases are inherited by large language models (LLMs) but also to explore how\nthese biases in LLMs can be leveraged to make society's \"unwritten code\" - such\nas implicit stereotypes and heuristics - visible and accessible for critique.\nWe introduce a conceptual framework through a case study in science: uncovering\nhidden rules in peer review - the factors that reviewers care about but rarely\nstate explicitly due to normative scientific expectations. The idea of the\nframework is to push LLMs to speak out their heuristics through generating\nself-consistent hypotheses - why one paper appeared stronger in reviewer\nscoring - among paired papers submitted to 45 computer science conferences,\nwhile iteratively searching deeper hypotheses from remaining pairs where\nexisting hypotheses cannot explain. We observed that LLMs' normative priors\nabout the internal characteristics of good science extracted from their\nself-talk, e.g. theoretical rigor, were systematically updated toward\nposteriors that emphasize storytelling about external connections, such as how\nthe work is positioned and connected within and across literatures. This shift\nreveals the primacy of scientific myths about intrinsic properties driving\nscientific excellence rather than extrinsic contextualization and storytelling\nthat influence conceptions of relevance and significance. Human reviewers tend\nto explicitly reward aspects that moderately align with LLMs' normative priors\n(correlation = 0.49) but avoid articulating contextualization and storytelling\nposteriors in their review comments (correlation = -0.14), despite giving\nimplicit reward to them with positive scores. We discuss the broad\napplicability of the framework, leveraging LLMs as diagnostic tools to surface\nthe tacit codes underlying human society, enabling more precisely targeted\nresponsible AI."}
{"id": "2505.19469", "pdf": "https://arxiv.org/pdf/2505.19469", "abs": "https://arxiv.org/abs/2505.19469", "authors": ["Mingzhuo Li", "Guang Li", "Jiafeng Mao", "Takahiro Ogawa", "Miki Haseyama"], "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICIP 2025", "summary": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985", "abs": "https://arxiv.org/abs/2505.18985", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "title": "STRICT: Stress Test of Rendering Images Containing Text", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.19587", "pdf": "https://arxiv.org/pdf/2505.19587", "abs": "https://arxiv.org/abs/2505.19587", "authors": ["Shadi Alijani", "Homayoun Najjaran"], "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010", "abs": "https://arxiv.org/abs/2505.19010", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has become a critical research area because integrating\ntext and image data can significantly improve performance in tasks such as\nclassification, retrieval, and scene understanding. However, despite progress\nwith pre-trained models, current approaches are limited by inadequate\ncross-modal interactions and static fusion strategies that do not fully exploit\nthe complementary nature of different modalities. To address these\nshortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that\nleverages dual-path encoding, co-attention with dimension-wise gating, and\nadvanced expert fusion. Our approach begins by projecting text and image\nfeatures into a common embedding space, where a dedicated co-attention\nmechanism enables simultaneous, fine-grained interactions between modalities.\nThis mechanism is further enhanced by a dimension-wise gating network that\nadaptively regulates the feature contributions at the channel level, ensuring\nthat only the most relevant information is emphasized. In parallel, dual-path\nencoders refine the representations by processing cross-modal information\nseparately before an additional cross-attention layer further aligns\nmodalities. The refined features are then aggregated via an expert fusion\nmodule that combines learned gating and self-attention to produce a robust,\nunified representation. We validate our approach on the MIMIC and SemEval\nMemotion 1.0, where experimental results demonstrate significant improvements\nin cross-modal alignment and state-of-the-art performance, underscoring the\npotential of our model for a wide range of multi-modal applications."}
{"id": "2505.19614", "pdf": "https://arxiv.org/pdf/2505.19614", "abs": "https://arxiv.org/abs/2505.19614", "authors": ["Sanghyuk Chun"], "title": "Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal learning has seen remarkable progress, particularly with the\nemergence of large-scale pre-training across various modalities. However, most\ncurrent approaches are built on the assumption of a deterministic, one-to-one\nalignment between modalities. This oversimplifies real-world multimodal\nrelationships, where their nature is inherently many-to-many. This phenomenon,\nnamed multiplicity, is not a side-effect of noise or annotation error, but an\ninevitable outcome of semantic abstraction, representational asymmetry, and\ntask-dependent ambiguity in multimodal tasks. This position paper argues that\nmultiplicity is a fundamental bottleneck that manifests across all stages of\nthe multimodal learning pipeline: from data construction to training and\nevaluation. This paper examines the causes and consequences of multiplicity,\nand highlights how multiplicity introduces training uncertainty, unreliable\nevaluation, and low dataset quality. This position calls for new research\ndirections on multimodal learning: novel multiplicity-aware learning frameworks\nand dataset construction protocols considering multiplicity."}
{"id": "2505.19025", "pdf": "https://arxiv.org/pdf/2505.19025", "abs": "https://arxiv.org/abs/2505.19025", "authors": ["Mushtari Sadia", "Zhenning Yang", "Yunming Xiao", "Ang Chen", "Amrita Roy Chowdhury"], "title": "SQUiD: Synthesizing Relational Databases from Unstructured Text", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Relational databases are central to modern data management, yet most data\nexists in unstructured forms like text documents. To bridge this gap, we\nleverage large language models (LLMs) to automatically synthesize a relational\ndatabase by generating its schema and populating its tables from raw text. We\nintroduce SQUiD, a novel neurosymbolic framework that decomposes this task into\nfour stages, each with specialized techniques. Our experiments show that SQUiD\nconsistently outperforms baselines across diverse datasets."}
{"id": "2505.19616", "pdf": "https://arxiv.org/pdf/2505.19616", "abs": "https://arxiv.org/abs/2505.19616", "authors": ["Rui Cai", "Bangzheng Li", "Xiaofei Wen", "Muhao Chen", "Zhe Zhao"], "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks."}
{"id": "2505.19037", "pdf": "https://arxiv.org/pdf/2505.19037", "abs": "https://arxiv.org/abs/2505.19037", "authors": ["Ke-Han Lu", "Chun-Yi Kuan", "Hung-yi Lee"], "title": "Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models", "categories": ["eess.AS", "cs.CL"], "comment": "Accecpted by Interspeech 2025;\n  https://github.com/kehanlu/Speech-IFEval", "summary": "We introduce Speech-IFeval, an evaluation framework designed to assess\ninstruction-following capabilities and quantify catastrophic forgetting in\nspeech-aware language models (SLMs). Recent SLMs integrate speech perception\nwith large language models (LLMs), often degrading textual capabilities due to\nspeech-centric training. Existing benchmarks conflate speech perception with\ninstruction-following, hindering evaluation of these distinct skills. To\naddress this gap, we provide a benchmark for diagnosing the\ninstruction-following abilities of SLMs. Our findings show that most SLMs\nstruggle with even basic instructions, performing far worse than text-based\nLLMs. Additionally, these models are highly sensitive to prompt variations,\noften yielding inconsistent and unreliable outputs. We highlight core\nchallenges and provide insights to guide future research, emphasizing the need\nfor evaluation beyond task-level metrics."}
{"id": "2505.19662", "pdf": "https://arxiv.org/pdf/2505.19662", "abs": "https://arxiv.org/abs/2505.19662", "authors": ["Atsunori Moteki", "Shoichi Masui", "Fan Yang", "Yueqi Song", "Yonatan Bisk", "Graham Neubig", "Ikuo Kusajima", "Yasuto Watanabe", "Hiroyuki Ishida", "Jun Takahashi", "Shan Jiang"], "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, 4 tables", "summary": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/."}
{"id": "2505.19075", "pdf": "https://arxiv.org/pdf/2505.19075", "abs": "https://arxiv.org/abs/2505.19075", "authors": ["Jaemin Kim", "Hangeol Chang", "Hyunmin Hwang", "Choonghan Kim", "Jong Chul Ye"], "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "22 pages", "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms \\add{existing baseline fine-tuning methods using the\nLlama3.2 model}. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR"}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678", "abs": "https://arxiv.org/abs/2505.19678", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency."}
{"id": "2505.19155", "pdf": "https://arxiv.org/pdf/2505.19155", "abs": "https://arxiv.org/abs/2505.19155", "authors": ["Xuan Zhang", "Cunxiao Du", "Sicheng Yu", "Jiawei Wu", "Fengzhuo Zhang", "Wei Gao", "Qian Liu"], "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Due to the auto-regressive nature of current video large language models\n(Video-LLMs), the inference latency increases as the input sequence length\ngrows, posing challenges for the efficient processing of video sequences that\nare usually very long. We observe that during decoding, the attention scores of\nmost tokens in Video-LLMs tend to be sparse and concentrated, with only certain\ntokens requiring comprehensive full attention. Based on this insight, we\nintroduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two\ndistinct modules: one leveraging sparse top-K attention and the other employing\ndense full attention. These modules collaborate to accelerate Video-LLMs\nwithout loss. The fast (sparse) model speculatively decodes multiple tokens,\nwhile the slow (dense) model verifies them in parallel. StD is a tuning-free,\nplug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in\nvideo processing. It maintains model performance while enabling a seamless\ntransition from a standard Video-LLM to a sparse Video-LLM with minimal code\nmodifications."}
{"id": "2505.19779", "pdf": "https://arxiv.org/pdf/2505.19779", "abs": "https://arxiv.org/abs/2505.19779", "authors": ["Mobina Mansoori", "Sajjad Shahabodini", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning."}
{"id": "2505.19234", "pdf": "https://arxiv.org/pdf/2505.19234", "abs": "https://arxiv.org/abs/2505.19234", "authors": ["Jialong Zhou", "Lichao Wang", "Xiao Yang"], "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration face critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm, learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization."}
{"id": "2505.19802", "pdf": "https://arxiv.org/pdf/2505.19802", "abs": "https://arxiv.org/abs/2505.19802", "authors": ["Zhiyu Wang", "Yang Liu", "Hatice Gunes"], "title": "GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation."}
{"id": "2505.19277", "pdf": "https://arxiv.org/pdf/2505.19277", "abs": "https://arxiv.org/abs/2505.19277", "authors": ["Ibukun Olatunji", "Mark Sheppard"], "title": "Next Token Prediction Is a Dead End for Creativity", "categories": ["cs.AI", "cs.CL", "J.5; I.2.0; I.2.7"], "comment": "10 pages including references", "summary": "This paper argues that token prediction is fundamentally misaligned with real\ncreativity. While next-token models have enabled impressive advances in\nlanguage generation, their architecture favours surface-level coherence over\nspontaneity, originality, and improvisational risk. We use battle rap as a case\nstudy to expose the limitations of predictive systems, demonstrating that they\ncannot truly engage in adversarial or emotionally resonant exchanges. By\nreframing creativity as an interactive process rather than a predictive output,\nwe offer a vision for AI systems that are more expressive, responsive, and\naligned with human creative practice."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897", "abs": "https://arxiv.org/abs/2505.19897", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.19294", "pdf": "https://arxiv.org/pdf/2505.19294", "abs": "https://arxiv.org/abs/2505.19294", "authors": ["Ziyang Ma", "Xiquan Li", "Yakun Song", "Wenxi Chen", "Chenpeng Du", "Jian Wu", "Yuanzhe Chen", "Zhuo Chen", "Yuping Wang", "Yuxuan Wang", "Xie Chen"], "title": "Towards Reliable Large Audio Language Model", "categories": ["cs.SD", "cs.CL", "cs.HC", "cs.MM", "eess.AS"], "comment": "ACL 2025 Findings", "summary": "Recent advancements in large audio language models (LALMs) have demonstrated\nimpressive results and promising prospects in universal understanding and\nreasoning across speech, music, and general sound. However, these models still\nlack the ability to recognize their knowledge boundaries and refuse to answer\nquestions they don't know proactively. While there have been successful\nattempts to enhance the reliability of LLMs, reliable LALMs remain largely\nunexplored. In this paper, we systematically investigate various approaches\ntowards reliable LALMs, including training-free methods such as multi-modal\nchain-of-thought (MCoT), and training-based methods such as supervised\nfine-tuning (SFT). Besides, we identify the limitations of previous evaluation\nmetrics and propose a new metric, the Reliability Gain Index (RGI), to assess\nthe effectiveness of different reliable methods. Our findings suggest that both\ntraining-free and training-based methods enhance the reliability of LALMs to\ndifferent extents. Moreover, we find that awareness of reliability is a \"meta\nability\", which can be transferred across different audio modalities, although\nsignificant structural and content differences exist among sound, music, and\nspeech."}
{"id": "2505.19983", "pdf": "https://arxiv.org/pdf/2505.19983", "abs": "https://arxiv.org/abs/2505.19983", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "comment": "submitted to IEEE journal", "summary": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB."}
{"id": "2505.19302", "pdf": "https://arxiv.org/pdf/2505.19302", "abs": "https://arxiv.org/abs/2505.19302", "authors": ["Kapil Vaidya", "Abishek Sankararaman", "Jialin Ding", "Chuan Lei", "Xiao Qin", "Balakrishnan Narayanaswamy", "Tim Kraska"], "title": "ODIN: A NL2SQL Recommender to Handle Schema Ambiguity", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "NL2SQL (natural language to SQL) systems translate natural language into SQL\nqueries, allowing users with no technical background to interact with databases\nand create tools like reports or visualizations. While recent advancements in\nlarge language models (LLMs) have significantly improved NL2SQL accuracy,\nschema ambiguity remains a major challenge in enterprise environments with\ncomplex schemas, where multiple tables and columns with semantically similar\nnames often co-exist. To address schema ambiguity, we introduce ODIN, a NL2SQL\nrecommendation engine. Instead of producing a single SQL query given a natural\nlanguage question, ODIN generates a set of potential SQL queries by accounting\nfor different interpretations of ambiguous schema components. ODIN dynamically\nadjusts the number of suggestions based on the level of ambiguity, and ODIN\nlearns from user feedback to personalize future SQL query recommendations. Our\nevaluation shows that ODIN improves the likelihood of generating the correct\nSQL query by 1.5-2$\\times$ compared to baselines."}
{"id": "2505.19995", "pdf": "https://arxiv.org/pdf/2505.19995", "abs": "https://arxiv.org/abs/2505.19995", "authors": ["Marcel Aach", "Cyril Blanc", "Andreas Lintermann", "Kurt De Grave"], "title": "Optimizing edge AI models on HPC systems with the edge in the loop", "categories": ["cs.DC", "cs.CV", "I.2.6; D.1.3; I.2.8; I.5.1"], "comment": "13 pages, accepted for oral presentation at Computational Aspects of\n  Deep Learning 2025 (at ISC 2025)", "summary": "Artificial intelligence and machine learning models deployed on edge devices,\ne.g., for quality control in Additive Manufacturing (AM), are frequently small\nin size. Such models usually have to deliver highly accurate results within a\nshort time frame. Methods that are commonly employed in literature start out\nwith larger trained models and try to reduce their memory and latency footprint\nby structural pruning, knowledge distillation, or quantization. It is, however,\nalso possible to leverage hardware-aware Neural Architecture Search (NAS), an\napproach that seeks to systematically explore the architecture space to find\noptimized configurations. In this study, a hardware-aware NAS workflow is\nintroduced that couples an edge device located in Belgium with a powerful\nHigh-Performance Computing system in Germany, to train possible architecture\ncandidates as fast as possible while performing real-time latency measurements\non the target hardware. The approach is verified on a use case in the AM\ndomain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster\ninference speed while simultaneously enhancing model quality by a factor of\n~1.35, compared to a human-designed baseline."}
{"id": "2505.19353", "pdf": "https://arxiv.org/pdf/2505.19353", "abs": "https://arxiv.org/abs/2505.19353", "authors": ["Camilo Chacn Sartori"], "title": "Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.SE"], "comment": "preprint", "summary": "With the rise of generative AI (GenAI), Large Language Models are\nincreasingly employed for code generation, becoming active co-authors alongside\nhuman programmers. Focusing specifically on this application domain, this paper\narticulates distinct ``Architectures of Error'' to ground an epistemic\ndistinction between human and machine code generation. Examined through their\nshared vulnerability to error, this distinction reveals fundamentally different\ncausal origins: human-cognitive versus artificial-stochastic. To develop this\nframework and substantiate the distinction, the analysis draws critically upon\nDennett's mechanistic functionalism and Rescher's methodological pragmatism. I\nargue that a systematic differentiation of these error profiles raises critical\nphilosophical questions concerning semantic coherence, security robustness,\nepistemic limits, and control mechanisms in human-AI collaborative software\ndevelopment. The paper also utilizes Floridi's levels of abstraction to provide\na nuanced understanding of how these error dimensions interact and may evolve\nwith technological advancements. This analysis aims to offer philosophers a\nstructured framework for understanding GenAI's unique epistemological\nchallenges, shaped by these architectural foundations, while also providing\nsoftware engineers a basis for more critically informed engagement."}
{"id": "2505.20038", "pdf": "https://arxiv.org/pdf/2505.20038", "abs": "https://arxiv.org/abs/2505.20038", "authors": ["Chang Liu", "Haomin Zhang", "Shiyu Xia", "Zihao Chen", "Chaofan Ding", "Xin Yue", "Huizhe Chen", "Xinhan Di"], "title": "Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "4 pages, 1 figure, accepted by CVPR 2025 MMFM Workshop", "summary": "Generating high-quality piano audio from video requires precise\nsynchronization between visual cues and musical output, ensuring accurate\nsemantic and temporal alignment.However, existing evaluation datasets do not\nfully capture the intricate synchronization required for piano music\ngeneration. A comprehensive benchmark is essential for two primary reasons: (1)\nexisting metrics fail to reflect the complexity of video-to-piano music\ninteractions, and (2) a dedicated benchmark dataset can provide valuable\ninsights to accelerate progress in high-quality piano music generation. To\naddress these challenges, we introduce the CoP Benchmark Dataset-a fully\nopen-sourced, multimodal benchmark designed specifically for video-guided piano\nmusic generation. The proposed Chain-of-Perform (CoP) benchmark offers several\ncompelling features: (1) detailed multimodal annotations, enabling precise\nsemantic and temporal alignment between video content and piano audio via\nstep-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework\nfor rigorous assessment of both general-purpose and specialized video-to-piano\ngeneration tasks; and (3) full open-sourcing of the dataset, annotations, and\nevaluation protocols. The dataset is publicly available at\nhttps://github.com/acappemin/Video-to-Audio-and-Piano, with a continuously\nupdated leaderboard to promote ongoing research in this domain."}
{"id": "2505.19356", "pdf": "https://arxiv.org/pdf/2505.19356", "abs": "https://arxiv.org/abs/2505.19356", "authors": ["Kidist Amde Mekonnen", "Yosef Worku Alemneh", "Maarten de Rijke"], "title": "Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "68T50 (Primary), 68T05 (Secondary)", "H.3.3; H.3.1; I.2.7"], "comment": "10 pages (excluding references and appendix), 10 figures. Accepted to\n  ACL 2025 Findings. Public release includes dataset, code, and trained models:\n  https://github.com/kidist-amde/amharic-ir-benchmarks", "summary": "Neural retrieval methods using transformer-based pre-trained language models\nhave advanced multilingual and cross-lingual retrieval. However, their\neffectiveness for low-resource, morphologically rich languages such as Amharic\nremains underexplored due to data scarcity and suboptimal tokenization. We\naddress this gap by introducing Amharic-specific dense retrieval models based\non pre-trained Amharic BERT and RoBERTa backbones. Our proposed\nRoBERTa-Base-Amharic-Embed model (110M parameters) achieves a 17.6% relative\nimprovement in MRR@10 and a 9.86% gain in Recall@10 over the strongest\nmultilingual baseline, Arctic Embed 2.0 (568M parameters). More compact\nvariants, such as RoBERTa-Medium-Amharic-Embed (42M), remain competitive while\nbeing over 13x smaller. Additionally, we train a ColBERT-based late interaction\nretrieval model that achieves the highest MRR@10 score (0.843) among all\nevaluated models. We benchmark our proposed models against both sparse and\ndense retrieval baselines to systematically assess retrieval effectiveness in\nAmharic. Our analysis highlights key challenges in low-resource settings and\nunderscores the importance of language-specific adaptation. To foster future\nresearch in low-resource IR, we publicly release our dataset, codebase, and\ntrained models at https://github.com/kidist-amde/amharic-ir-benchmarks."}
{"id": "2505.20107", "pdf": "https://arxiv.org/pdf/2505.20107", "abs": "https://arxiv.org/abs/2505.20107", "authors": ["Ziyi Zhang", "Li Shen", "Deheng Ye", "Yong Luo", "Huangxuan Zhao", "Lefei Zhang"], "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency."}
{"id": "2505.19436", "pdf": "https://arxiv.org/pdf/2505.19436", "abs": "https://arxiv.org/abs/2505.19436", "authors": ["Ye Ye"], "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents", "categories": ["cs.AI", "cs.CL", "I.2.6; I.2.8; H.3.3"], "comment": "Under review. 9 pages main content, 15 pages appendix, 5 figures", "summary": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings."}
{"id": "2505.20123", "pdf": "https://arxiv.org/pdf/2505.20123", "abs": "https://arxiv.org/abs/2505.20123", "authors": ["Huijie Zhang", "Zijian Huang", "Siyi Chen", "Jinfan Zhou", "Zekai Zhang", "Peng Wang", "Qing Qu"], "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance", "categories": ["cs.LG", "cs.CV"], "comment": "41 pages, 14 figures", "summary": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models."}
{"id": "2505.19443", "pdf": "https://arxiv.org/pdf/2505.19443", "abs": "https://arxiv.org/abs/2505.19443", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "35 Pages, 8 Figures, 6 Tables", "summary": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle."}
{"id": "2505.20149", "pdf": "https://arxiv.org/pdf/2505.20149", "abs": "https://arxiv.org/abs/2505.20149", "authors": ["Cheng-Yu Tai", "Ching-Wen Chen", "Chi-Chin Wu", "Bo-Chen Chiu", "Cheng-Hung", "Lin", "Cheng-Kai Lu", "Jia-Kang Wang", "Tzu-Lun Huang"], "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline."}
{"id": "2505.19457", "pdf": "https://arxiv.org/pdf/2505.19457", "abs": "https://arxiv.org/abs/2505.19457", "authors": ["Guilong Lu", "Xuntao Guo", "Rongjunchen Zhang", "Wenqiao Zhu", "Ji Liu"], "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "Project Page: https://hithink-research.github.io/BizFinBench/", "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench."}
{"id": "2505.20232", "pdf": "https://arxiv.org/pdf/2505.20232", "abs": "https://arxiv.org/abs/2505.20232", "authors": ["Pranav Poudel", "Aavash Chhetri", "Prashnna Gyawali", "Georgios Leontidis", "Binod Bhattarai"], "title": "Multimodal Federated Learning With Missing Modalities through Feature Imputation Network", "categories": ["cs.LG", "cs.CV"], "comment": "MIUA 2025", "summary": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen"}
{"id": "2505.19504", "pdf": "https://arxiv.org/pdf/2505.19504", "abs": "https://arxiv.org/abs/2505.19504", "authors": ["Pingzhi Li", "Zhen Tan", "Huaizhi Qu", "Huan Liu", "Tianlong Chen"], "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Code is available at https://github.com/UNITES-Lab/DOGe", "summary": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation."}
{"id": "2505.20274", "pdf": "https://arxiv.org/pdf/2505.20274", "abs": "https://arxiv.org/abs/2505.20274", "authors": ["Kejing Lu", "Chuan Xiao", "Yoshiharu Ishikawa"], "title": "Probabilistic Kernel Function for Fast Angle Testing", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB", "cs.DS"], "comment": null, "summary": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536", "abs": "https://arxiv.org/abs/2505.19536", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277", "abs": "https://arxiv.org/abs/2505.20277", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter."}
{"id": "2505.19563", "pdf": "https://arxiv.org/pdf/2505.19563", "abs": "https://arxiv.org/abs/2505.19563", "authors": ["Shi-Yu Tian", "Zhi Zhou", "Wei Dong", "Ming Yang", "Kun-Yang Yu", "Zi-Jian Cheng", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights", "categories": ["cs.AI", "cs.CL"], "comment": "Paper under review, code and dataset are all available", "summary": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298", "abs": "https://arxiv.org/abs/2505.20298", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
{"id": "2505.19578", "pdf": "https://arxiv.org/pdf/2505.19578", "abs": "https://arxiv.org/abs/2505.19578", "authors": ["Dan Peng", "Zhihui Fu", "Zewen Ye", "Zhuoran Song", "Jun Wang"], "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy."}
{"id": "2505.19590", "pdf": "https://arxiv.org/pdf/2505.19590", "abs": "https://arxiv.org/abs/2505.19590", "authors": ["Xuandong Zhao", "Zhewei Kang", "Aosong Feng", "Sergey Levine", "Dawn Song"], "title": "Learning to Reason without External Rewards", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor"}
{"id": "2505.19601", "pdf": "https://arxiv.org/pdf/2505.19601", "abs": "https://arxiv.org/abs/2505.19601", "authors": ["Yeongmin Kim", "Heesun Bae", "Byeonghu Na", "Il-Chul Moon"], "title": "Preference Optimization by Estimating the Ratio of the Data Distribution", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2."}
{"id": "2505.19621", "pdf": "https://arxiv.org/pdf/2505.19621", "abs": "https://arxiv.org/abs/2505.19621", "authors": ["George Kour", "Itay Nakash", "Ateret Anaby-Tavor", "Michal Shmueli-Scheuer"], "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS"}
{"id": "2505.19641", "pdf": "https://arxiv.org/pdf/2505.19641", "abs": "https://arxiv.org/abs/2505.19641", "authors": ["Junteng Liu", "Yuanxiang Fan", "Zhuo Jiang", "Han Ding", "Yongyi Hu", "Chi Zhang", "Yiqi Shi", "Shitong Weng", "Aili Chen", "Shiqi Chen", "Yunan Huang", "Mozhi Zhang", "Pengyu Zhao", "Junjie Yan", "Junxian He"], "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic."}
{"id": "2505.19683", "pdf": "https://arxiv.org/pdf/2505.19683", "abs": "https://arxiv.org/abs/2505.19683", "authors": ["Pengfei Cao", "Tianyi Men", "Wencan Liu", "Jingwen Zhang", "Xuzhao Li", "Xixun Lin", "Dianbo Sui", "Yanan Cao", "Kang Liu", "Jun Zhao"], "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field."}
{"id": "2505.19752", "pdf": "https://arxiv.org/pdf/2505.19752", "abs": "https://arxiv.org/abs/2505.19752", "authors": ["Hengli Li", "Yuxuan Wang", "Song-Chun Zhu", "Ying Nian Wu", "Zilong Zheng"], "title": "Discrete Markov Bridge", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches."}
{"id": "2505.19757", "pdf": "https://arxiv.org/pdf/2505.19757", "abs": "https://arxiv.org/abs/2505.19757", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments."}
{"id": "2505.19770", "pdf": "https://arxiv.org/pdf/2505.19770", "abs": "https://arxiv.org/abs/2505.19770", "authors": ["Ruizhe Shi", "Minhak Song", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO", "categories": ["cs.LG", "cs.CL"], "comment": "30 pages, 5 figures", "summary": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred."}
{"id": "2505.19866", "pdf": "https://arxiv.org/pdf/2505.19866", "abs": "https://arxiv.org/abs/2505.19866", "authors": ["Feng Xiong", "Hongling Xu", "Yifei Wang", "Runxi Cheng", "Yong Wang", "Xiangxiang Chu"], "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget."}
{"id": "2505.19893", "pdf": "https://arxiv.org/pdf/2505.19893", "abs": "https://arxiv.org/abs/2505.19893", "authors": ["Melis Ilayda Bal", "Volkan Cevher", "Michael Muehlebach"], "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation."}
{"id": "2505.19896", "pdf": "https://arxiv.org/pdf/2505.19896", "abs": "https://arxiv.org/abs/2505.19896", "authors": ["Alejandro Carrasco", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program", "categories": ["cs.AI", "astro-ph.IM", "cs.CL"], "comment": "Non revised version for paper going to be published in Journal of\n  Advances in Space Research", "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases"}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897", "abs": "https://arxiv.org/abs/2505.19897", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.19954", "pdf": "https://arxiv.org/pdf/2505.19954", "abs": "https://arxiv.org/abs/2505.19954", "authors": ["Andrew Zamai", "Nathanael Fijalkow", "Boris Mansencal", "Laurent Simon", "Eloi Navet", "Pierrick Coupe"], "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions."}
{"id": "2505.19955", "pdf": "https://arxiv.org/pdf/2505.19955", "abs": "https://arxiv.org/abs/2505.19955", "authors": ["Hui Chen", "Miao Xiong", "Yujie Lu", "Wei Han", "Ailin Deng", "Yufei He", "Jiaying Wu", "Yibo Li", "Yue Liu", "Bryan Hooi"], "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "40 pages, 7 figures", "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."}
{"id": "2505.19956", "pdf": "https://arxiv.org/pdf/2505.19956", "abs": "https://arxiv.org/abs/2505.19956", "authors": ["Jihyung Lee", "Jin-Seop Lee", "Jaehoon Lee", "YunSeok Choi", "Jee-Hyong Lee"], "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased."}
{"id": "2505.19964", "pdf": "https://arxiv.org/pdf/2505.19964", "abs": "https://arxiv.org/abs/2505.19964", "authors": ["Eric Zhao", "Jessica Dai", "Pranjal Awasthi"], "title": "The Limits of Preference Data for Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.GT"], "comment": null, "summary": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors."}
{"id": "2505.19997", "pdf": "https://arxiv.org/pdf/2505.19997", "abs": "https://arxiv.org/abs/2505.19997", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Mengze Li", "Yumeng Zhu", "Ang Li", "Kun Kuang", "Fei Wu"], "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents", "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy."}
{"id": "2505.20027", "pdf": "https://arxiv.org/pdf/2505.20027", "abs": "https://arxiv.org/abs/2505.20027", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Mounika Marreddy", "Maneesh Singh", "Manish Gupta", "Bapi S. Raju"], "title": "Multi-modal brain encoding models for multi-modal stimuli", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.LG", "eess.AS", "eess.IV"], "comment": "26 pages, 15 figures, The Thirteenth International Conference on\n  Learning Representations, ICLR-2025, Singapore.\n  https://openreview.net/pdf?id=0dELcFHig2", "summary": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain."}
{"id": "2505.20046", "pdf": "https://arxiv.org/pdf/2505.20046", "abs": "https://arxiv.org/abs/2505.20046", "authors": ["Le Zhang", "Bo Wang", "Xipeng Qiu", "Siva Reddy", "Aishwarya Agrawal"], "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking."}
{"id": "2505.20050", "pdf": "https://arxiv.org/pdf/2505.20050", "abs": "https://arxiv.org/abs/2505.20050", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Gabriele Ciravegna", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "MVP: Multi-source Voice Pathology detection", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Voice disorders significantly impact patient quality of life, yet\nnon-invasive automated diagnosis remains under-explored due to both the\nscarcity of pathological voice data, and the variability in recording sources.\nThis work introduces MVP (Multi-source Voice Pathology detection), a novel\napproach that leverages transformers operating directly on raw voice signals.\nWe explore three fusion strategies to combine sentence reading and sustained\nvowel recordings: waveform concatenation, intermediate feature fusion, and\ndecision-level combination. Empirical validation across the German, Portuguese,\nand Italian languages shows that intermediate feature fusion using transformers\nbest captures the complementary characteristics of both recording types. Our\napproach achieves up to +13% AUC improvement over single-source methods."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053", "abs": "https://arxiv.org/abs/2505.20053", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.20063", "pdf": "https://arxiv.org/pdf/2505.20063", "abs": "https://arxiv.org/abs/2505.20063", "authors": ["Dana Arad", "Aaron Mueller", "Yonatan Belinkov"], "title": "SAEs Are Good for Steering -- If You Select the Right Features", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods."}
{"id": "2505.20087", "pdf": "https://arxiv.org/pdf/2505.20087", "abs": "https://arxiv.org/abs/2505.20087", "authors": ["Makesh Narsimhan Sreedhar", "Traian Rebedea", "Christopher Parisien"], "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems."}
{"id": "2505.20103", "pdf": "https://arxiv.org/pdf/2505.20103", "abs": "https://arxiv.org/abs/2505.20103", "authors": ["Xiangyu Li", "Jingqiang Chen"], "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment", "categories": ["cs.DL", "cs.CL"], "comment": "15 pages, 7 figures", "summary": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers."}
{"id": "2505.20139", "pdf": "https://arxiv.org/pdf/2505.20139", "abs": "https://arxiv.org/abs/2505.20139", "authors": ["Jialin Yang", "Dongfu Jiang", "Lipeng He", "Sherman Siu", "Yuxuan Zhang", "Disen Liao", "Zhuofeng Li", "Huaye Zeng", "Yiming Jia", "Haozhe Wang", "Benjamin Schneider", "Chi Ruan", "Wentao Ma", "Zhiheng Lyu", "Yifei Wang", "Yi Lu", "Quy Duc Do", "Ziyan Jiang", "Ping Nie", "Wenhu Chen"], "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "16 pages, 9 figures, 13 tables", "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152", "abs": "https://arxiv.org/abs/2505.20152", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.20161", "pdf": "https://arxiv.org/pdf/2505.20161", "abs": "https://arxiv.org/abs/2505.20161", "authors": ["Jaehun Jung", "Seungju Han", "Ximing Lu", "Skyler Hallinan", "David Acuna", "Shrimai Prabhumoye", "Mostafa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Yejin Choi"], "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks."}
{"id": "2505.20166", "pdf": "https://arxiv.org/pdf/2505.20166", "abs": "https://arxiv.org/abs/2505.20166", "authors": ["Chun-Yi Kuan", "Hung-yi Lee"], "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Project Website: https://kuan2jiu99.github.io/Balsa", "summary": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\nimportant textual capabilities such as instruction-following are lost after\ntraining on audio data. In some cases, models may even hallucinate sounds that\nare not present in the input audio, raising concerns about their reliability.\nSecond, achieving cross-modal alignment between audio and language typically\nrelies on large collections of task-specific question-answer pairs for\ninstruction tuning, making the process resource-intensive. To address these\nissues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose\ncaption-style alignment data. We refer to this process as bootstrapping\naudio-language alignment via synthetic data generation from backbone LLMs\n(BALSa). Building on BALSa, we introduce LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method designed\nto improve ALLMs' ability to distinguish between present and absent sounds. We\nfurther extend BALSa to multi-audio scenarios, where the model either explains\nthe differences between audio inputs or produces a unified caption that\ndescribes them all, thereby enhancing audio-language alignment. Experimental\nresults indicate that our method effectively mitigates audio hallucinations\nwhile reliably maintaining strong performance in audio understanding,\nreasoning, and instruction-following skills. Moreover, incorporating\nmulti-audio training further enhances the model's comprehension and reasoning\ncapabilities. Overall, BALSa offers an efficient and scalable approach to the\ndevelopment of ALLMs."}
{"id": "2505.20246", "pdf": "https://arxiv.org/pdf/2505.20246", "abs": "https://arxiv.org/abs/2505.20246", "authors": ["Jiahao Qiu", "Fulian Xiao", "Yimin Wang", "Yuchen Mao", "Yijia Chen", "Xinzhe Juan", "Siran Wang", "Xuan Qi", "Tongcheng Zhang", "Zixin Yao", "Jiacheng Guo", "Yifu Lu", "Charles Argon", "Jundi Cui", "Daixin Chen", "Junran Zhou", "Shuyao Zhou", "Zhanpeng Zhou", "Ling Yang", "Shilong Liu", "Hongru Wang", "Kaixuan Huang", "Xun Jiang", "Yuming Cao", "Yue Chen", "Yunfei Chen", "Zhengyi Chen", "Ruowei Dai", "Mengqiu Deng", "Jiye Fu", "Yunting Gu", "Zijie Guan", "Zirui Huang", "Xiaoyan Ji", "Yumeng Jiang", "Delong Kong", "Haolong Li", "Jiaqi Li", "Ruipeng Li", "Tianze Li", "Zhuoran Li", "Haixia Lian", "Mengyue Lin", "Xudong Liu", "Jiayi Lu", "Jinghan Lu", "Wanyu Luo", "Ziyue Luo", "Zihao Pu", "Zhi Qiao", "Ruihuan Ren", "Liang Wan", "Ruixiang Wang", "Tianhui Wang", "Yang Wang", "Zeyu Wang", "Zihua Wang", "Yujia Wu", "Zhaoyi Wu", "Hao Xin", "Weiao Xing", "Ruojun Xiong", "Weijie Xu", "Yao Shu", "Xiao Yao", "Xiaorui Yang", "Yuchen Yang", "Nan Yi", "Jiadong Yu", "Yangyuxuan Yu", "Huiting Zeng", "Danni Zhang", "Yunjie Zhang", "Zhaoyu Zhang", "Zhiheng Zhang", "Xiaofeng Zheng", "Peirong Zhou", "Linyan Zhong", "Xiaoyin Zong", "Ying Zhao", "Zhenxin Chen", "Lin Ding", "Xiaoyu Gao", "Bingbing Gong", "Yichao Li", "Yang Liao", "Guang Ma", "Tianyuan Ma", "Xinrui Sun", "Tianyi Wang", "Han Xia", "Ruobing Xian", "Gen Ye", "Tengfei Yu", "Wentao Zhang", "Yuxi Wang", "Xi Gao", "Mengdi Wang"], "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent", "categories": ["cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning."}
{"id": "2505.20251", "pdf": "https://arxiv.org/pdf/2505.20251", "abs": "https://arxiv.org/abs/2505.20251", "authors": ["Sophia Hager", "Aleem Khan", "Andrew Wang", "Nicholas Andrews"], "title": "Learning Extrapolative Sequence Transformations from Markov Chains", "categories": ["cs.LG", "cs.CL"], "comment": "To be published at the Forty-Second International Conference on\n  Machine Learning", "summary": "Most successful applications of deep learning involve similar training and\ntest conditions. However, tasks such as biological sequence design involve\nsearching for sequences that improve desirable properties beyond previously\nknown values, which requires novel hypotheses that \\emph{extrapolate} beyond\ntraining data. In these settings, extrapolation may be achieved by using random\nsearch methods such as Markov chain Monte Carlo (MCMC), which, given an initial\nstate, sample local transformations to approximate a target density that\nrewards states with the desired properties. However, even with a well-designed\nproposal, MCMC may struggle to explore large structured state spaces\nefficiently. Rather than relying on stochastic search, it would be desirable to\nhave a model that greedily optimizes the properties of interest, successfully\nextrapolating in as few steps as possible. We propose to learn such a model\nfrom the Markov chains resulting from MCMC search. Specifically, our approach\nuses selected states from Markov chains as a source of training data for an\nautoregressive model, which is then able to efficiently generate novel\nsequences that extrapolate along the sequence-level properties of interest. The\nproposed approach is validated on three problems: protein sequence design, text\nsentiment control, and text anonymization. We find that the autoregressive\nmodel can extrapolate as well or better than MCMC, but with the additional\nbenefits of scalability and significantly higher sample efficiency."}
{"id": "2505.20254", "pdf": "https://arxiv.org/pdf/2505.20254", "abs": "https://arxiv.org/abs/2505.20254", "authors": ["Xiangchen Song", "Aashiq Muhamed", "Yujia Zheng", "Lingjing Kong", "Zeyu Tang", "Mona T. Diab", "Virginia Smith", "Kun Zhang"], "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI."}
{"id": "2505.20259", "pdf": "https://arxiv.org/pdf/2505.20259", "abs": "https://arxiv.org/abs/2505.20259", "authors": ["Haoyu Wang", "Zeyu Qin", "Yifei Zhao", "Chao Du", "Min Lin", "Xueqian Wang", "Tianyu Pang"], "title": "Lifelong Safety Alignment for Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment."}
{"id": "2505.20278", "pdf": "https://arxiv.org/pdf/2505.20278", "abs": "https://arxiv.org/abs/2505.20278", "authors": ["Hoyeon Chang", "Jinho Park", "Hanseul Cho", "Sohee Yang", "Miyoung Ko", "Hyeonbin Hwang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6"], "comment": null, "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279", "abs": "https://arxiv.org/abs/2505.20279", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.20291", "pdf": "https://arxiv.org/pdf/2505.20291", "abs": "https://arxiv.org/abs/2505.20291", "authors": ["Di Wu", "Yixin Wan", "Kai-Wei Chang"], "title": "Visualized Text-to-Image Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve."}
{"id": "2505.20297", "pdf": "https://arxiv.org/pdf/2505.20297", "abs": "https://arxiv.org/abs/2505.20297", "authors": ["Qinyu Zhao", "Jaskirat Singh", "Ming Xu", "Akshay Asthana", "Stephen Gould", "Liang Zheng"], "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA", "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality."}
